
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 310 papers. February 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Февраль 2025</span> | <span id="title-articles-count">310 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-01.html">⬅️ <span id="prev-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-03.html">➡️ <span id="next-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Февраль 2025', 'en': 'February 2025', 'zh': '2月2025年'};
        let feedDateNext = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let feedDatePrev = {'ru': '01.2025', 'en': '01/2025', 'zh': '1月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.02737', 'title': 'SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model', 'url': 'https://huggingface.co/papers/2502.02737', 'abstract': 'While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art "small" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.', 'score': 80, 'issue_id': 2066, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'c78fe4c39300443d', 'authors': ['Loubna Ben Allal', 'Anton Lozhkov', 'Elie Bakouch', 'Gabriel Martín Blázquez', 'Guilherme Penedo', 'Lewis Tunstall', 'Andrés Marafioti', 'Hynek Kydlíček', 'Agustín Piqueres Lajarín', 'Vaibhav Srivastav', 'Joshua Lochner', 'Caleb Fahlgren', 'Xuan-Son Nguyen', 'Clémentine Fourrier', 'Ben Burtenshaw', 'Hugo Larcher', 'Haojun Zhao', 'Cyril Zakka', 'Mathieu Morlon', 'Colin Raffel', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['HuggingFaceTB'], 'pdf_title_img': 'assets/pdf/title_img/2502.02737.jpg', 'data': {'categories': ['#open_source', '#dataset', '#low_resource', '#training', '#small_models'], 'emoji': '🤏', 'ru': {'title': 'Большие возможности в маленьком пакете: SmolLM2 - компактная языковая модель с впечатляющей производительностью', 'desc': "Статья описывает разработку SmolLM2 - современной 'маленькой' языковой модели с 1,7 миллиардами параметров. Модель обучалась на ~11 триллионах токенов данных с использованием многоэтапного процесса, сочетающего веб-тексты со специализированными данными по математике, коду и выполнению инструкций. Авторы также представили новые специализированные наборы данных и провели эксперименты для оптимизации процесса обучения. В результате SmolLM2 превзошла другие современные малые языковые модели, такие как Qwen2.5-1.5B и Llama3.2-1B."}, 'en': {'title': 'SmolLM2: Efficient Language Modeling for Resource-Constrained Environments', 'desc': 'This paper presents SmolLM2, a compact language model with 1.7 billion parameters designed to be efficient for deployment in resource-limited environments. The model is trained on an extensive dataset of approximately 11 trillion tokens, utilizing a multi-stage training approach that incorporates diverse data sources, including web text and specialized datasets for math and coding. The authors introduce new datasets to enhance training quality and perform systematic evaluations to optimize dataset mixing based on performance feedback. SmolLM2 demonstrates superior performance compared to other recent small language models, and the authors provide access to the model and datasets for further research.'}, 'zh': {'title': '小型语言模型的强大突破', 'desc': '本论文介绍了SmolLM2的开发，这是一个具有17亿参数的小型语言模型。为了实现强大的性能，我们在约11万亿个数据上进行了过度训练，采用了多阶段训练过程，结合了网络文本、数学、代码和指令跟随数据。我们还引入了新的专用数据集，以解决现有数据集规模小或质量低的问题。最终，我们证明SmolLM2在性能上超越了其他近期的小型语言模型，如Qwen2.5-1.5B和Llama3.2-1B。'}}}, {'id': 'https://huggingface.co/papers/2502.01506', 'title': 'TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets', 'url': 'https://huggingface.co/papers/2502.01506', 'abstract': 'The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.', 'score': 26, 'issue_id': 2063, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'f5ec0450054af574', 'authors': ['Yuzhe Yang', 'Yifei Zhang', 'Minghao Wu', 'Kaidi Zhang', 'Yunmiao Zhang', 'Honghai Yu', 'Yan Hu', 'Benyou Wang'], 'affiliations': ['Nanjing University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.01506.jpg', 'data': {'categories': ['#multimodal', '#agents'], 'emoji': '📊', 'ru': {'title': 'LLM-агенты раскрывают тайны социально-экономической динамики', 'desc': 'Статья представляет новый фреймворк TwinMarket для моделирования социально-экономических систем с использованием больших языковых моделей (LLM). Авторы применяют LLM-агентов для более реалистичного моделирования человеческого поведения, учитывая когнитивные искажения и эмоциональные факторы. В экспериментах на симулированном фондовом рынке демонстрируется, как индивидуальные действия приводят к групповому поведению и эмергентным явлениям. Этот подход позволяет лучше понять взаимосвязь между индивидуальным принятием решений и коллективными социально-экономическими паттернами.'}, 'en': {'title': 'Harnessing LLMs for Realistic Socio-Economic Simulations', 'desc': 'This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns.'}, 'zh': {'title': '利用大型语言模型模拟社会经济系统的涌现现象', 'desc': '本研究探讨了社会涌现现象，传统的基于规则的代理模型（ABM）难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。我们提出了一种新的多代理框架TwinMarket，利用大型语言模型（LLM）来模拟社会经济系统。通过模拟股票市场环境的实验，我们展示了个体行为如何通过互动和反馈机制引发集体动态，导致金融泡沫和经济衰退等涌现现象。该方法为个体决策与集体社会经济模式之间的复杂关系提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2502.03373', 'title': 'Demystifying Long Chain-of-Thought Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2502.03373', 'abstract': 'Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.', 'score': 20, 'issue_id': 2064, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a1d00a6c8452131a', 'authors': ['Edward Yeo', 'Yuxuan Tong', 'Morry Niu', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'IN.AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03373.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая секреты длинных цепочек рассуждений в ИИ', 'desc': 'Статья исследует механизмы длинных цепочек рассуждений (CoT) в больших языковых моделях (LLM). Авторы выявляют ключевые факторы, влияющие на способность моделей генерировать длинные CoT траектории через эксперименты с обучением с подкреплением (RL) и тонкой настройкой. Исследование показывает важность масштабирования вычислительных ресурсов, формирования наград и использования веб-данных для улучшения рассуждений. Результаты предоставляют практические рекомендации по оптимизации стратегий обучения для усиления длинных CoT рассуждений в LLM.'}, 'en': {'title': 'Unlocking Reasoning Power in Large Language Models', 'desc': 'This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs.'}, 'zh': {'title': '优化训练策略，提升长推理链能力', 'desc': '本研究探讨了大型语言模型（LLMs）中长推理链（CoTs）的生成机制，揭示了影响模型生成长推理链的关键因素。我们发现，虽然监督微调（SFT）不是绝对必要的，但它可以简化训练过程并提高效率。随着训练计算能力的增加，推理能力有可能出现，但其发展并不总是保证，因此奖励设计对于稳定推理链的长度增长至关重要。最后，我们的研究为优化训练策略以增强LLMs中的长推理链提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2502.03387', 'title': 'LIMO: Less is More for Reasoning', 'url': 'https://huggingface.co/papers/2502.03387', 'abstract': 'We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models\' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model\'s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.', 'score': 18, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ad1fa98bc3904527', 'authors': ['Yixin Ye', 'Zhen Huang', 'Yang Xiao', 'Ethan Chern', 'Shijie Xia', 'Pengfei Liu'], 'affiliations': ['SJTU, SII, GAIR'], 'pdf_title_img': 'assets/pdf/title_img/2502.03387.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Меньше значит больше: революция в обучении языковых моделей сложным рассуждениям', 'desc': 'Исследователи обнаружили, что сложные математические рассуждения в больших языковых моделях можно вызвать с помощью удивительно малого количества примеров. Их модель LIMO достигла впечатляющих результатов на математических тестах, используя всего 817 обучающих образцов, что значительно меньше, чем у предыдущих подходов. LIMO также продемонстрировала исключительную способность к обобщению вне распределения, превзойдя модели, обученные на гораздо большем объеме данных. На основе этих результатов авторы предлагают гипотезу LIMO, согласно которой сложные рассуждения могут возникать через минимальные, но точно организованные демонстрации когнитивных процессов в предварительно обученных моделях.'}, 'en': {'title': 'Less Data, More Reasoning: The LIMO Hypothesis', 'desc': 'This paper reveals a surprising finding about how large language models can perform complex reasoning tasks. It shows that instead of needing a lot of training data, a model called LIMO can achieve high accuracy in mathematical reasoning with only a small number of examples. LIMO outperforms previous models that used much more data, indicating that less can be more when it comes to training for reasoning tasks. The authors introduce the Less-Is-More Reasoning Hypothesis, suggesting that a well-prepared model can effectively learn complex reasoning from minimal, well-structured examples.'}, 'zh': {'title': '少即是多，推理能力的新发现', 'desc': '本文提出了一项重要发现，挑战了我们对大型语言模型中复杂推理能力产生机制的理解。传统观点认为，复杂推理任务需要大量的训练数据，但我们证明只需少量示例即可有效引发复杂的数学推理能力。我们的模型LIMO在数学推理方面表现出前所未有的性能，使用仅817个训练样本，分别在AIME和MATH上达到了57.1%和94.8%的准确率。我们提出的“少即是多推理假设”表明，在基础模型中，经过充分编码的领域知识可以通过精心设计的少量示例来激发复杂推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.02339', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'url': 'https://huggingface.co/papers/2502.02339', 'abstract': "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.", 'score': 9, 'issue_id': 2063, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '3f3413717efb32f6', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Ruihan Jin', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Beijing', 'Department of Automation, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.02339.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#multimodal', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'AStar: Эффективное структурированное мышление для мультимодальных ИИ', 'desc': 'Статья представляет новый подход к улучшению визуального рассуждения мультимодальных больших языковых моделей (MLLM). Авторы предлагают метод AStar, использующий автоматизированное структурированное мышление на основе поиска Монте-Карло по дереву (MCTS). AStar автоматически извлекает высокоуровневые паттерны рассуждений из ограниченных данных и интегрирует внутренние способности модели с внешними указаниями. Эксперименты показывают, что AStar достигает точности 54.0% на бенчмарке MathVerse, превосходя GPT-4o при высокой эффективности использования данных и вычислений.'}, 'en': {'title': 'AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking', 'desc': "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."}, 'zh': {'title': 'AStar：高效的多模态推理新范式', 'desc': '多模态大型语言模型（MLLMs）在复杂视觉推理方面表现出色，但仍面临挑战。尽管最近的研究尝试通过引入结构化思维和教师指导来增强推理能力，但在性能和效率之间的平衡仍然困难。本文提出了一种名为AStar的自动化结构化思维范式，利用蒙特卡洛树搜索（MCTS）从有限数据中自动推导高层次的认知推理模式。AStar通过统一的推理框架，结合模型的内部推理能力和外部推理指导，实现高效推理，显著提高了准确性和数据利用效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01105', 'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2502.01105', 'abstract': "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.", 'score': 6, 'issue_id': 2067, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b4eb829c549c6a2e', 'authors': ['Yiren Song', 'Danze Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.01105.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#diffusion', '#cv', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'LayerTracer: ИИ-дизайнер векторной графики', 'desc': 'LayerTracer - это новый подход к созданию многослойных SVG изображений, основанный на диффузионном трансформере. Он имитирует процесс работы дизайнера, генерируя пошаговые растровые чертежи, а затем векторизуя их по слоям. Метод использует условный диффузионный механизм для кодирования референсных изображений в латентные токены. Эксперименты показывают превосходство LayerTracer над существующими методами в качестве генерации и редактируемости векторной графики.'}, 'en': {'title': 'LayerTracer: Bridging the Gap in Layered SVG Generation', 'desc': 'This paper introduces LayerTracer, a new framework that improves the generation of layered SVGs by learning from how designers create them. It uses a two-phase process: first, it generates rasterized blueprints that mimic human design steps, and then it converts these into clean, editable SVGs while removing duplicate paths. The framework employs a conditional diffusion mechanism to ensure that the generated images maintain their structure and quality. Experiments show that LayerTracer outperforms existing methods in both the quality of the generated designs and their ease of editing, aligning better with professional design practices.'}, 'zh': {'title': 'LayerTracer：智能生成可编辑的分层SVG图形', 'desc': '本文提出了一种名为LayerTracer的框架，旨在生成认知对齐的分层SVG图形。该方法通过学习设计师的分层SVG创建过程，利用一个新颖的顺序设计操作数据集。LayerTracer分为两个阶段：首先，基于文本的扩散变换器生成多阶段的光栅化构建蓝图；其次，通过路径去重实现分层矢量化，生成干净且可编辑的SVG文件。实验结果表明，LayerTracer在生成质量和可编辑性方面优于基于优化和神经网络的基线方法，有效地将AI生成的矢量图与专业设计认知对齐。'}}}, {'id': 'https://huggingface.co/papers/2502.02671', 'title': 'On Teacher Hacking in Language Model Distillation', 'url': 'https://huggingface.co/papers/2502.02671', 'abstract': "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.", 'score': 5, 'issue_id': 2072, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'defca87e9bf06d0b', 'authors': ['Daniil Tiapkin', 'Daniele Calandriello', 'Johan Ferret', 'Sarah Perrin', 'Nino Vieillard', 'Alexandre Ramé', 'Mathieu Blondel'], 'affiliations': ['Ecole 1CMAP, France; Polytechnique', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.02671.jpg', 'data': {'categories': ['#alignment', '#optimization', '#rlhf', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': "Борьба с 'teacher hacking': ключ к robust языковым моделям", 'desc': "Статья исследует феномен 'teacher hacking' при дистилляции знаний в языковых моделях. Авторы предлагают экспериментальную установку с оракулом, учителем и учеником для изучения этого явления. Результаты показывают, что 'teacher hacking' возникает при использовании фиксированного офлайн-датасета, но может быть смягчен с помощью онлайн-генерации данных. Исследование подчеркивает важность разнообразия данных для предотвращения 'hacking' и построения надежных языковых моделей."}, 'en': {'title': 'Preventing Teacher Hacking: The Key Role of Data Diversity in Distillation', 'desc': "This paper explores the concept of 'teacher hacking' in the context of knowledge distillation for language models (LMs). Teacher hacking occurs when a student LM overly optimizes based on an imperfect teacher LM, leading to poor performance on the actual task. The authors conducted experiments using an oracle LM as the ground truth, a teacher LM distilled from it, and a student LM distilled from the teacher. They found that using diverse online data can prevent teacher hacking, highlighting the importance of data diversity in the distillation process."}, 'zh': {'title': '防止教师黑客，提升语言模型的蒸馏效果', 'desc': '本文探讨了语言模型（LM）在知识蒸馏阶段可能出现的“教师黑客”现象。教师黑客是指学生模型在模仿教师模型时，过度优化导致性能下降的情况。这种现象与古德哈特法则相符，可能源于教师模型对真实分布的不完美近似。我们的实验表明，使用固定的离线数据集进行蒸馏时，教师黑客现象会发生，而采用在线数据生成技术则能有效缓解这一问题，数据多样性是防止教师黑客的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2502.01618', 'title': 'A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods', 'url': 'https://huggingface.co/papers/2502.01618', 'abstract': 'Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at https://probabilistic-inference-scaling.github.io.', 'score': 5, 'issue_id': 2065, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'c9971916eb027101', 'authors': ['Isha Puri', 'Shivchander Sudalairaj', 'Guangxuan Xu', 'Kai Xu', 'Akash Srivastava'], 'affiliations': ['MIT CSAIL', 'Red Hat AI Innovation'], 'pdf_title_img': 'assets/pdf/title_img/2502.01618.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#inference'], 'emoji': '🎲', 'ru': {'title': 'Вероятностный подход к масштабированию вывода LLM', 'desc': 'Статья представляет новый подход к масштабированию вычислений во время вывода для больших языковых моделей (LLM). Вместо оптимизации с помощью моделей вознаграждения, авторы рассматривают задачу как вероятностный вывод, используя методы Монте-Карло на основе частиц. Эмпирическая оценка показывает, что предложенный метод имеет в 4-16 раз лучшую скорость масштабирования по сравнению с детерминированными аналогами на сложных задачах математических рассуждений. Исследование демонстрирует, как небольшие модели могут достичь точности крупных моделей при меньшем количестве прогонов.'}, 'en': {'title': 'Revolutionizing Inference: Probabilistic Scaling for LLMs', 'desc': 'This paper addresses the limitations of scaling large language models (LLMs) by focusing on improving inference time rather than just increasing model size or data. The authors propose a new approach that treats inference-time scaling as a probabilistic inference task, using sampling techniques to better explore the state distribution. By applying particle-based Monte Carlo methods, their method shows significant improvements in scaling rates compared to traditional deterministic search methods. The results indicate that their approach can achieve higher accuracy with fewer rollouts, demonstrating a promising direction for enhancing LLM performance.'}, 'zh': {'title': '推理时间扩展的新方法：概率推理与粒子采样结合', 'desc': '大型语言模型（LLMs）通过增加模型规模和数据量取得了显著的性能提升。然而，最近的研究表明，这种方法的收益递减，促使我们考虑在推理时增加计算量。现有的推理时间扩展方法通常将任务视为搜索问题，容易受到奖励模型的近似误差影响而导致奖励操控。本文提出了一种新的推理时间扩展方法，通过适应基于粒子的蒙特卡洛方法，将推理时间扩展视为概率推理任务，从而在各种数学推理任务中实现了更好的扩展率。'}}}, {'id': 'https://huggingface.co/papers/2502.01154', 'title': 'Jailbreaking with Universal Multi-Prompts', 'url': 'https://huggingface.co/papers/2502.01154', 'abstract': 'Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.', 'score': 3, 'issue_id': 2068, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'aa9860c81d83ac21', 'authors': ['Yu-Ling Hsu', 'Hsuan Su', 'Shang-Tse Chen'], 'affiliations': ['National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01154.jpg', 'data': {'categories': ['#security', '#rl', '#data', '#optimization', '#transfer_learning', '#training', '#ethics'], 'emoji': '🔓', 'ru': {'title': 'Универсальный взлом языковых моделей: новый метод JUMP', 'desc': 'Статья описывает новый метод под названием JUMP для универсального взлома (jailbreak) больших языковых моделей с помощью мульти-промптов. Авторы также представляют адаптированную версию этого метода для защиты, называемую DUMP. Экспериментальные результаты показывают, что предложенный подход превосходит существующие техники по оптимизации универсальных мульти-промптов. Исследование затрагивает важную тему этических проблем и новых типов атак на языковые модели.'}, 'en': {'title': 'JUMP: Universal Multi-Prompts for Jailbreaking LLMs', 'desc': 'This paper presents JUMP, a novel method for jailbreaking large language models (LLMs) using universal multi-prompts. Unlike traditional prompting techniques that focus on specific adversarial inputs, JUMP aims to create a universal attacker that can adapt to various unseen tasks, reducing computational costs. Additionally, the authors propose a defense mechanism called DUMP, which leverages the same principles to protect against such attacks. Experimental results indicate that JUMP significantly outperforms existing methods in optimizing these universal multi-prompts.'}, 'zh': {'title': '通用多提示：破解与防御的创新方法', 'desc': '大型语言模型（LLMs）近年来迅速发展，改变了许多应用，显著提高了便利性和生产力。然而，随着其强大能力的提升，伦理问题和新型攻击（如越狱攻击）也随之出现。大多数提示技术专注于优化单个案例的对抗输入，这在处理大数据集时会导致更高的计算成本。本文介绍了一种名为JUMP的方法，旨在使用通用多提示对LLMs进行越狱，同时我们还提出了防御方法DUMP，实验结果表明我们的方法在优化通用多提示方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2502.03275', 'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning', 'url': 'https://huggingface.co/papers/2502.03275', 'abstract': 'Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.', 'score': 3, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'f94d674e0f57dcf9', 'authors': ['DiJia Su', 'Hanlin Zhu', 'Yingchen Xu', 'Jiantao Jiao', 'Yuandong Tian', 'Qinqing Zheng'], 'affiliations': ['Meta AI', 'UC Berkeley', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2502.03275.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#optimization', '#training', '#benchmark', '#math'], 'emoji': '🧠', 'ru': {'title': 'Гибридное представление рассуждений: эффективность через абстракцию', 'desc': 'Данная статья предлагает гибридный подход к представлению процесса рассуждений в больших языковых моделях (LLM). Авторы используют латентные дискретные токены, генерируемые VQ-VAE, для частичной абстракции начальных шагов рассуждения, что значительно сокращает длину входных данных. Метод применяется как при обучении модели с нуля, так и при дообучении существующих LLM на гибридных данных с расширенным словарем. Предложенный подход превосходит базовые методы в различных тестах на логические и математические рассуждения.'}, 'en': {'title': 'Streamlining Reasoning with Hybrid Token Representations', 'desc': 'This paper presents a new method to improve reasoning in Large Language Models (LLMs) by using a hybrid representation of reasoning processes. Instead of relying solely on lengthy text inputs, the authors introduce latent discrete tokens generated by VQ-VAE to simplify the reasoning steps. This approach reduces the input length and computational resources needed while maintaining effective reasoning capabilities. The proposed method shows superior performance in training and fine-tuning scenarios for logical and mathematical reasoning tasks compared to traditional methods.'}, 'zh': {'title': '优化推理过程，提升模型效率', 'desc': '本文探讨了大型语言模型（LLMs）在推理和规划中的应用，特别是在链式思维（CoT）数据训练时的表现。我们提出了一种混合表示法，通过使用VQ-VAE生成的潜在离散标记，部分抽象化初始推理步骤，从而显著减少推理过程的长度。我们在两个场景中探索了潜在追踪抽象的使用：一是从头开始训练模型解决钥匙寻找迷宫问题，二是对LLMs进行微调以处理逻辑和数学推理问题。我们的训练方法通过随机混合潜在标记和文本标记，促进了对新潜在标记的快速适应，且在多个基准测试中表现优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2502.02928', 'title': 'Large Language Model Guided Self-Debugging Code Generation', 'url': 'https://huggingface.co/papers/2502.02928', 'abstract': 'Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.', 'score': 2, 'issue_id': 2075, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'dc7aaadeeee7e1e7', 'authors': ['Muntasir Adnan', 'Zhiwei Xu', 'Carlos C. N. Kuhn'], 'affiliations': ['Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2502.02928.jpg', 'data': {'categories': ['#agents', '#plp', '#training'], 'emoji': '🐍', 'ru': {'title': 'PyCapsule: Эффективная генерация Python-кода с самоотладкой', 'desc': 'PyCapsule - это новая система для автоматической генерации кода на Python, использующая двухагентный конвейер и модули самоотладки. Система включает в себя сложный вывод промптов, итеративную обработку ошибок и тестирование примеров, что обеспечивает высокую стабильность, безопасность и корректность генерации. PyCapsule демонстрирует значительное улучшение успешности на различных бенчмарках по сравнению с современными методами. Однако наблюдается снижение нормализованной успешности при увеличении попыток самоотладки, возможно, из-за ограниченной и зашумленной обратной связи об ошибках.'}, 'en': {'title': 'Revolutionizing Python Code Generation with PyCapsule', 'desc': 'This paper introduces PyCapsule, a new framework designed to enhance automated code generation, particularly for Python. It employs a two-agent pipeline that focuses on efficient self-debugging and robust error handling, addressing common issues in existing methods. The framework utilizes advanced prompt inference and iterative testing to improve the stability and correctness of generated code. Empirical results show that PyCapsule outperforms current state-of-the-art techniques in various benchmarks, highlighting its potential for more efficient AI-driven programming solutions.'}, 'zh': {'title': 'PyCapsule：高效的自动化代码生成框架', 'desc': '自动化代码生成在智能计算机编程和系统部署中变得越来越重要。然而，现有的方法在计算效率上常常面临挑战，并且缺乏强大的代码解析和错误修正机制。本文提出了一种新颖的框架PyCapsule，采用简单而有效的双代理管道和高效的自我调试模块来生成Python代码。PyCapsule通过复杂的提示推理、迭代错误处理和案例测试，确保了高生成稳定性、安全性和正确性。'}}}, {'id': 'https://huggingface.co/papers/2502.02421', 'title': 'Activation-Informed Merging of Large Language Models', 'url': 'https://huggingface.co/papers/2502.02421', 'abstract': 'Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.', 'score': 1, 'issue_id': 2079, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '90e80efaaef789ec', 'authors': ['Amin Heyrani Nobari', 'Kaveh Alimohammadi', 'Ali ArjomandBigdeli', 'Akash Srivastava', 'Faez Ahmed', 'Navid Azizan'], 'affiliations': ['Massachusetts Institute of Technology', 'RedHat AI Innovation & MIT-IBM Watson AI Lab', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02421.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'AIM: Умное слияние языковых моделей для повышения эффективности', 'desc': 'Статья представляет новый метод объединения языковых моделей под названием Activation-Informed Merging (AIM). AIM использует информацию из пространства активаций моделей для улучшения производительности и устойчивости объединенной модели. Метод применим к любому существующему способу слияния моделей и использует принципы непрерывного обучения и сжатия моделей. Эмпирические результаты показывают значительное улучшение производительности объединенных моделей на различных бенчмарках, с увеличением до 40%.'}, 'en': {'title': 'Boosting Model Performance with Activation-Informed Merging', 'desc': 'This paper presents Activation-Informed Merging (AIM), a novel technique for merging large language models (LLMs) that leverages activation space information to improve model performance. AIM enhances the merging process by selectively prioritizing essential weights from the base models, which helps maintain robustness and efficiency. The method is flexible and can be integrated with existing merging techniques, making it widely applicable. Empirical results show that AIM can lead to significant performance improvements, achieving up to a 40% increase in benchmark scores for merged models.'}, 'zh': {'title': '激活信息合并：提升模型合并性能的新方法', 'desc': '模型合并是一种将多个微调的大型语言模型（LLMs）的参数和嵌入结合起来的方法，能够在保持计算效率的同时提升模型在各种任务上的表现。本文提出了一种名为激活信息合并（AIM）的技术，它将LLMs的激活空间信息整合到合并过程中，以提高性能和鲁棒性。AIM旨在作为一种灵活的补充解决方案，适用于任何现有的合并方法，并通过持续学习和模型压缩的原则来保留基础模型中的关键权重。通过使用与任务无关的校准集，AIM在合并过程中优先考虑重要权重，实验证明AIM在多个基准测试中显著提升了合并模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.00306', 'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2502.00306', 'abstract': "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.", 'score': 1, 'issue_id': 2076, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '4987f380f5ddb7af', 'authors': ['Ali Naseh', 'Yuefeng Peng', 'Anshuman Suri', 'Harsh Chaudhari', 'Alina Oprea', 'Amir Houmansadr'], 'affiliations': ['Northeastern University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.00306.jpg', 'data': {'categories': ['#inference', '#rag', '#leakage', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Незаметная атака на RAG-системы: как выявить документы в базе знаний', 'desc': "Статья представляет новый метод атаки на системы генерации текста с извлечением информации (RAG). Авторы предлагают технику под названием 'Interrogation Attack', которая позволяет определить, содержится ли конкретный документ в базе знаний RAG-системы. Метод основан на создании естественных запросов, на которые можно ответить только при наличии целевого документа. Эксперименты показывают, что атака эффективнее существующих методов и трудно обнаружима стандартными детекторами."}, 'en': {'title': 'Stealthy Inference: Unveiling Membership in RAG Systems', 'desc': 'This paper introduces a new method called Interrogation Attack (IA) for membership inference in Retrieval-Augmented Generation (RAG) systems. RAG allows Large Language Models (LLMs) to generate responses using external knowledge without changing their internal parameters, but this can be exploited by adversaries. The IA technique uses natural-text queries that can only be answered if a specific document is present, making it harder to detect than previous methods. The authors demonstrate that their approach is more effective and stealthy, achieving better performance with fewer queries and lower costs compared to existing techniques.'}, 'zh': {'title': '隐蔽的会员推断攻击：RAG系统的新挑战', 'desc': '本论文介绍了一种新的会员推断技术，称为审问攻击（Interrogation Attack, IA），旨在针对RAG数据存储中的文档进行攻击。该方法通过构造自然语言查询，仅在目标文档存在时才能得到答案，从而实现有效的推断。与现有方法相比，我们的攻击在仅使用30个查询的情况下，成功率提高了2倍，同时保持隐蔽性。我们的研究表明，IA在多种RAG配置下的表现优于以往的推断攻击，且每个文档的推断成本低于0.02美元。'}}}, {'id': 'https://huggingface.co/papers/2502.00226', 'title': 'HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems', 'url': 'https://huggingface.co/papers/2502.00226', 'abstract': 'Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.', 'score': 0, 'issue_id': 2079, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'c9615b5d00a42037', 'authors': ['Jun Xing', 'Mayur Bhatia', 'Sahil Phulwani', 'Darshan Suresh', 'Rafik Matta'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00226.jpg', 'data': {'categories': ['#benchmark', '#science', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый бенчмарк раскрывает потенциал LLM в реальной разработке ПО', 'desc': 'Статья представляет новый бенчмарк для оценки применимости больших языковых моделей (LLM) в реальных задачах разработки программного обеспечения. HackerRank-ASTRA Benchmark включает проектные задачи кодирования, имитирующие реальные сценарии, и оценивает согласованность модели через 32 запуска. Исследование показало, что три ведущие модели достигли сравнимых средних оценок в 75%. Модель Claude-3.5-Sonnet-1022 продемонстрировала наивысшую согласованность и низкую вариативность результатов.'}, 'en': {'title': 'Benchmarking LLMs for Real-World Coding Consistency', 'desc': 'This paper evaluates the effectiveness of large language models (LLMs) in real-world software development tasks using the HackerRank-ASTRA Benchmark. Unlike previous benchmarks that focus on isolated coding problems, this benchmark introduces project-based scenarios that require multi-file handling and assesses model consistency through extensive testing. The study analyzes the performance of top models, revealing that while they achieved similar average scores, one model, Claude-3.5-Sonnet-1022, stood out for its high consistency and low variability in results. This research emphasizes the importance of rigorous evaluation methods to ensure LLMs are reliable for practical applications in coding.'}, 'zh': {'title': '评估大型语言模型在软件开发中的真实应用性', 'desc': '这篇论文评估了大型语言模型（LLMs）在实际软件开发任务中的适用性。现有的基准测试通常只关注单一的编码问题或特定库，忽视了多文件、基于项目的场景，并缺乏对一致性的严格评估。HackerRank-ASTRA基准引入了模拟真实场景的项目基础编码问题，并通过32次运行评估模型的一致性。初步评估显示，Claude-3.5-Sonnet-1022在问题一致性方面表现最佳，具有较低的变异性，突显了其在实际软件开发任务中的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2502.10389', 'title': 'Region-Adaptive Sampling for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.10389', 'abstract': "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.", 'score': 44, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '8068f45b7fd0c2ee', 'authors': ['Ziming Liu', 'Yifan Yang', 'Chengruidong Zhang', 'Yiqi Zhang', 'Lili Qiu', 'Yang You', 'Yuqing Yang'], 'affiliations': ['Microsoft Research', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.10389.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#diffusion', '#architecture', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных трансформеров без потери качества', 'desc': 'Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод основан на динамическом распределении различных коэффициентов сэмплирования для разных областей изображения, опираясь на фокус внимания модели Diffusion Transformer. RAS обновляет только те области, на которых в данный момент сфокусирована модель, используя для остальных областей кэшированный шум из предыдущего шага. Эксперименты показали, что RAS позволяет достичь ускорения до 2.51x на различных моделях при минимальном снижении качества генерации.'}, 'en': {'title': 'Accelerating Diffusion Transformers with RAS for Real-Time Generation', 'desc': "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."}, 'zh': {'title': '提升扩散模型的实时性能', 'desc': '扩散模型（DMs）在生成任务中已成为首选，但其依赖多个顺序前向传递限制了实时性能。我们提出了一种新的无训练采样策略RAS，利用扩散变换器（DiTs）的灵活性，根据模型的关注点动态分配图像区域的采样比例。RAS只更新当前关注的区域，而其他区域则使用上一步的缓存噪声，从而提高了效率。我们的实验表明，RAS在生成质量几乎不下降的情况下，能够实现显著的加速。'}}}, {'id': 'https://huggingface.co/papers/2502.09992', 'title': 'Large Language Diffusion Models', 'url': 'https://huggingface.co/papers/2502.09992', 'abstract': 'Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.', 'score': 35, 'issue_id': 2242, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '5117e8f17ba51f92', 'authors': ['Shen Nie', 'Fengqi Zhu', 'Zebin You', 'Xiaolu Zhang', 'Jingyang Ou', 'Jun Hu', 'Jun Zhou', 'Yankai Lin', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.09992.jpg', 'data': {'categories': ['#architecture', '#optimization', '#benchmark', '#diffusion', '#training'], 'emoji': '🌊', 'ru': {'title': 'Диффузионные модели бросают вызов авторегрессии в обработке языка', 'desc': "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернативную авторегрессионным моделям. LLaDA использует процесс маскирования данных и обратный процесс для предсказания замаскированных токенов, оптимизируя границу правдоподобия. Модель демонстрирует сильную масштабируемость и превосходит базовые авторегрессионные модели в различных тестах. LLaDA показывает впечатляющие результаты в задачах обучения в контексте и следования инструкциям, а также решает проблему 'проклятия обращения'."}, 'en': {'title': 'LLaDA: A New Era for Language Models Beyond Autoregression', 'desc': 'This paper introduces LLaDA, a novel diffusion model that challenges the dominance of autoregressive models (ARMs) in large language models (LLMs). LLaDA employs a forward data masking process and a reverse process, utilizing a Transformer architecture to predict masked tokens effectively. By optimizing a likelihood bound, it offers a robust generative framework for probabilistic inference. The results show that LLaDA not only scales well but also competes with leading LLMs in tasks like in-context learning and instruction-following, suggesting that diffusion models can serve as a strong alternative to traditional ARMs.'}, 'zh': {'title': '扩散模型：自回归模型的新挑战', 'desc': '自回归模型（ARMs）被广泛认为是大型语言模型（LLMs）的基石。本文提出了LLaDA，这是一种从头开始训练的扩散模型，采用预训练和监督微调（SFT）的方法。LLaDA通过前向数据掩蔽过程和反向过程建模分布，并使用普通Transformer预测被掩蔽的标记。研究表明，LLaDA在多个基准测试中表现出强大的可扩展性，超越了自构建的ARMs基线，证明了扩散模型作为ARMs的可行替代方案。'}}}, {'id': 'https://huggingface.co/papers/2502.10248', 'title': 'Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model', 'url': 'https://huggingface.co/papers/2502.10248', 'abstract': "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.", 'score': 33, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '356bc046cc5f59e5', 'authors': ['Guoqing Ma', 'Haoyang Huang', 'Kun Yan', 'Liangyu Chen', 'Nan Duan', 'Shengming Yin', 'Changyi Wan', 'Ranchen Ming', 'Xiaoniu Song', 'Xing Chen', 'Yu Zhou', 'Deshan Sun', 'Deyu Zhou', 'Jian Zhou', 'Kaijun Tan', 'Kang An', 'Mei Chen', 'Wei Ji', 'Qiling Wu', 'Wen Sun', 'Xin Han', 'Yanan Wei', 'Zheng Ge', 'Aojie Li', 'Bin Wang', 'Bizhu Huang', 'Bo Wang', 'Brian Li', 'Changxing Miao', 'Chen Xu', 'Chenfei Wu', 'Chenguang Yu', 'Dapeng Shi', 'Dingyuan Hu', 'Enle Liu', 'Gang Yu', 'Ge Yang', 'Guanzhe Huang', 'Gulin Yan', 'Haiyang Feng', 'Hao Nie', 'Haonan Jia', 'Hanpeng Hu', 'Hanqi Chen', 'Haolong Yan', 'Heng Wang', 'Hongcheng Guo', 'Huilin Xiong', 'Huixin Xiong', 'Jiahao Gong', 'Jianchang Wu', 'Jiaoren Wu', 'Jie Wu', 'Jie Yang', 'Jiashuai Liu', 'Jiashuo Li', 'Jingyang Zhang', 'Junjing Guo', 'Junzhe Lin', 'Kaixiang Li', 'Lei Liu', 'Lei Xia', 'Liang Zhao', 'Liguo Tan', 'Liwen Huang', 'Liying Shi', 'Ming Li', 'Mingliang Li', 'Muhua Cheng', 'Na Wang', 'Qiaohui Chen', 'Qinglin He', 'Qiuyan Liang', 'Quan Sun', 'Ran Sun', 'Rui Wang', 'Shaoliang Pang', 'Shiliang Yang', 'Sitong Liu', 'Siqi Liu', 'Shuli Gao', 'Tiancheng Cao', 'Tianyu Wang', 'Weipeng Ming', 'Wenqing He', 'Xu Zhao', 'Xuelin Zhang', 'Xianfang Zeng', 'Xiaojia Liu', 'Xuan Yang', 'Yaqi Dai', 'Yanbo Yu', 'Yang Li', 'Yineng Deng', 'Yingming Wang', 'Yilei Wang', 'Yuanwei Lu', 'Yu Chen', 'Yu Luo', 'Yuchu Luo', 'Yuhe Yin', 'Yuheng Feng', 'Yuxiang Yang', 'Zecheng Tang', 'Zekai Zhang', 'Zidong Yang', 'Binxing Jiao', 'Jiansheng Chen', 'Jing Li', 'Shuchang Zhou', 'Xiangyu Zhang', 'Xinhao Zhang', 'Yibo Zhu', 'Heung-Yeung Shum', 'Daxin Jiang'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2502.10248.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#training', '#open_source', '#diffusion', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: от текста к реалистичным роликам', 'desc': 'Представлена модель Step-Video-T2V - предобученная нейросеть для генерации видео по текстовому описанию с 30 миллиардами параметров. Модель использует глубокий вариационный автоэнкодер Video-VAE для сжатия видео и два двуязычных текстовых энкодера для обработки запросов на английском и китайском языках. Для улучшения качества генерируемых видео применяется подход Video-DPO на основе предпочтений. Модель демонстрирует передовое качество генерации видео по сравнению с открытыми и коммерческими аналогами.'}, 'en': {'title': 'Revolutionizing Video Generation with Step-Video-T2V', 'desc': 'Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models.'}, 'zh': {'title': '创新视频生成，赋能内容创作者', 'desc': '本文介绍了一种名为Step-Video-T2V的先进文本到视频预训练模型，具有300亿参数，能够生成最长204帧的视频。我们设计了一种深度压缩变分自编码器（Video-VAE），在视频生成任务中实现了16x16的空间压缩和8x的时间压缩，同时保持了卓越的视频重建质量。用户提示通过双语文本编码器进行编码，以处理英语和中文。我们还讨论了当前扩散模型范式的局限性，并概述了视频基础模型的未来方向。'}}}, {'id': 'https://huggingface.co/papers/2502.09696', 'title': 'ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models', 'url': 'https://huggingface.co/papers/2502.09696', 'abstract': 'Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.', 'score': 23, 'issue_id': 2241, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '00f4f8e85ea27717', 'authors': ['Jonathan Roberts', 'Mohammad Reza Taesiri', 'Ansh Sharma', 'Akash Gupta', 'Samuel Roberts', 'Ioana Croitoru', 'Simion-Vlad Bogolin', 'Jialu Tang', 'Florian Langer', 'Vyas Raina', 'Vatsal Raina', 'Hanyi Xiong', 'Vishaal Udandarao', 'Jingyi Lu', 'Shiyang Chen', 'Sam Purkis', 'Tianshuo Yan', 'Wenye Lin', 'Gyungin Shin', 'Qiaochu Yang', 'Anh Totti Nguyen', 'Kai Han', 'Samuel Albanie'], 'affiliations': ['Auburn University', 'Independent Researcher', 'The University of Hong Kong', 'University of Alberta', 'University of Cambridge', 'University of Oxford', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.09696.jpg', 'data': {'categories': ['#cv', '#benchmark', '#interpretability', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'ZeroBench: невозможный визуальный тест для мультимодальных моделей', 'desc': 'В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимодальных языковых моделей (LMM). Бенчмарк состоит из 100 вручную отобранных вопросов и 334 менее сложных подвопросов. Авторы оценили 20 LMM на ZeroBench, и все они показали результат 0%. Целью бенчмарка является создание сложного теста, который останется актуальным дольше, чем существующие визуальные бенчмарки.'}, 'en': {'title': 'ZeroBench: Raising the Bar for Visual Reasoning in LMMs', 'desc': 'This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding.'}, 'zh': {'title': 'ZeroBench：推动视觉理解的新基准', 'desc': '大型多模态模型（LMMs）在图像理解方面存在显著不足，甚至在某些方面的空间认知能力不如小孩或动物。尽管如此，它们在许多流行的视觉基准测试中得分很高，但随着模型进步的加速，这些基准的挑战性迅速降低。为了解决这个问题，我们提出了ZeroBench，这是一个轻量级的视觉推理基准，当前的前沿LMMs完全无法解决。ZeroBench包含100个手动策划的问题和334个较简单的子问题，我们对20个LMMs进行了评估，结果均为0.0%，并对错误进行了严格分析。'}}}, {'id': 'https://huggingface.co/papers/2502.10391', 'title': 'MM-RLHF: The Next Step Forward in Multimodal LLM Alignment', 'url': 'https://huggingface.co/papers/2502.10391', 'abstract': 'Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.', 'score': 20, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': 'c47a89fda79a1a4b', 'authors': ['Yi-Fan Zhang', 'Tao Yu', 'Haochen Tian', 'Chaoyou Fu', 'Peiyan Li', 'Jianshu Zeng', 'Wulin Xie', 'Yang Shi', 'Huanyu Zhang', 'Junkang Wu', 'Xue Wang', 'Yibo Hu', 'Bin Wen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Di Zhang', 'Liang Wang', 'Rong Jin', 'Tieniu Tan'], 'affiliations': ['Alibaba', 'CASIA', 'KuaiShou', 'Meta AI', 'NJU', 'PKU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2502.10391.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#training', '#interpretability', '#open_source', '#rlhf', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями', 'desc': 'Исследователи представили MM-RLHF - набор данных из 120 тысяч размеченных пар сравнения предпочтений для мультимодальных языковых моделей. На его основе они разработали новые методы обучения с подкреплением, включая модель вознаграждения на основе критики и динамическое масштабирование вознаграждений. Эксперименты показали значительное улучшение способностей модели LLaVA-ov-7B в диалогах и безопасности после дообучения с использованием предложенных методов. Авторы открыли доступ к набору данных, моделям и коду для воспроизведения результатов.'}, 'en': {'title': 'Enhancing MLLM Alignment with Human Preferences', 'desc': 'This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics.'}, 'zh': {'title': '提升多模态模型与人类偏好的对齐', 'desc': '尽管多模态大型语言模型（MLLMs）取得了显著进展，但大多数最先进的模型尚未与人类偏好进行充分对齐。为了解决这一问题，我们引入了MM-RLHF数据集，包含12万个细粒度的人类标注偏好比较对，显著提升了现有资源的规模和质量。我们提出了基于批评的奖励模型，能够在评分前生成模型输出的批评，从而提供更具可解释性和信息量的反馈。此外，我们还提出了动态奖励缩放方法，根据奖励信号调整每个样本的损失权重，以优化高质量比较对的使用。'}}}, {'id': 'https://huggingface.co/papers/2502.09935', 'title': 'Precise Parameter Localization for Textual Generation in Diffusion Models', 'url': 'https://huggingface.co/papers/2502.09935', 'abstract': "Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.", 'score': 9, 'issue_id': 2245, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '1c3ce78b0c6424d2', 'authors': ['Łukasz Staniszewski', 'Bartosz Cywiński', 'Franziska Boenisch', 'Kamil Deja', 'Adam Dziedzic'], 'affiliations': ['CISPA Helmholtz Center for Information Security', 'IDEAS NCBR', 'Warsaw University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.09935.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#synthetic', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Локализация текста в диффузионных моделях для повышения эффективности генерации', 'desc': 'Статья представляет новый подход к улучшению генерации текста в диффузионных моделях для создания изображений. Авторы обнаружили, что менее 1% параметров модели, расположенных в слоях внимания, отвечают за текстовый контент. На основе этого наблюдения они предлагают методы повышения эффективности и качества генерации текста, включая точечную настройку локализованных слоев и редактирование текста в сгенерированных изображениях. Подход применим к различным архитектурам диффузионных моделей и текстовым энкодерам.'}, 'en': {'title': 'Targeting Attention for Enhanced Text Generation in Diffusion Models', 'desc': "This paper explores how diffusion models can create realistic images that include high-quality text. It reveals that less than 1% of the model's parameters, specifically in the attention layers, are crucial for generating text within these images. By focusing on these specific layers, the authors enhance the efficiency and performance of text generation in diffusion models. They also present applications such as improving text generation, editing text in images, and preventing toxic text generation, demonstrating the broad applicability of their approach across different model architectures."}, 'zh': {'title': '局部化注意力层提升文本生成能力', 'desc': '本论文介绍了一种新颖的扩散模型，能够合成高质量的照片级图像，并集成文本生成。研究发现，扩散模型中只有不到1%的参数，主要集中在注意力层，影响图像中的文本内容生成。基于这一观察，作者通过针对交叉和联合注意力层，提升了文本生成的效率和性能。论文还展示了如何利用这些局部化的层来编辑生成图像中的文本内容，并防止生成有害文本。'}}}, {'id': 'https://huggingface.co/papers/2502.09955', 'title': 'Diverse Inference and Verification for Advanced Reasoning', 'url': 'https://huggingface.co/papers/2502.09955', 'abstract': "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.", 'score': 9, 'issue_id': 2242, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '10eaccfc7377f600', 'authors': ['Iddo Drori', 'Gaston Longhitano', 'Mao Mao', 'Seunghwan Hyun', 'Yuke Zhang', 'Sungjun Park', 'Zachary Meeks', 'Xin-Yu Zhang', 'Ben Segev', 'Howard Yong', 'Nakul Verma', 'Avi Shporer', 'Alon Amit', 'Madeleine Udell'], 'affiliations': ['Boston University', 'Columbia University', 'Google', 'Intuit', 'Massachusetts Institute of Technology', 'NotBadMath.AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09955.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#agents', '#rl', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Усиление LLM для решения сложных задач: многомодельный подход', 'desc': "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложных математических и логических задач. Авторы применяют комбинацию нескольких моделей и методов, включая верификацию решений и отбор лучших ответов. Подход значительно повышает точность на задачах Международной математической олимпиады, Humanity's Last Exam и Abstraction and Reasoning Corpus. Исследователи используют симуляции, обучение с подкреплением и мета-обучение для адаптации представлений агентов и улучшения обобщающей способности."}, 'en': {'title': 'Boosting LLMs: From 33% to 77% Accuracy in Complex Problem Solving!', 'desc': "This paper discusses advancements in reasoning large language models (LLMs) like OpenAI's models and DeepSeek in tackling complex mathematical and coding challenges. The authors propose a diverse inference strategy that integrates various models and methods during testing to enhance performance on difficult tasks such as IMO problems and ARC puzzles. They demonstrate that their method significantly boosts accuracy, achieving a 77.8% success rate on IMO combinatorics and solving 80% of previously unsolved ARC puzzles. Additionally, they employ techniques like reinforcement learning and meta-learning to improve the adaptability and generalization of their models, ensuring their approach is both reliable and scalable."}, 'zh': {'title': '提升推理模型在高级数学问题上的准确性', 'desc': '本文探讨了推理型大语言模型在解决高级数学和编程任务中的挑战，特别是国际数学奥林匹克（IMO）组合问题、抽象与推理语料库（ARC）难题和人类最后考试（HLE）问题。我们提出了一种多模型和多方法结合的推理方法，在测试时进行多样化推理。通过自动验证IMO问题和ARC难题的解答正确性，我们显著提高了这些问题的解答准确率。我们的研究方法可靠、稳健且可扩展，旨在推动可重复研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.07780', 'title': 'DarwinLM: Evolutionary Structured Pruning of Large Language Models', 'url': 'https://huggingface.co/papers/2502.07780', 'abstract': 'Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \\sysname, a method for training-aware structured pruning. \\sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \\sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.', 'score': 7, 'issue_id': 2251, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'b53421574afe0c8a', 'authors': ['Shengkun Tang', 'Oliver Sieberling', 'Eldar Kurtic', 'Zhiqiang Shen', 'Dan Alistarh'], 'affiliations': ['Department of Machine Learning, MBZUAI, Abu Dhabi, UAE', 'ETH Zurich, Zurich, Switzerland', 'ISTA, Vienna, Austria', 'Red Hat AI, Boston, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07780.jpg', 'data': {'categories': ['#inference', '#small_models', '#training', '#optimization'], 'emoji': '🧬', 'ru': {'title': 'Эволюционная оптимизация языковых моделей', 'desc': 'В статье рассматривается проблема высокой вычислительной стоимости больших языковых моделей (LLM) и предлагается метод структурированной обрезки для их оптимизации. Метод \x1710\x187 использует эволюционный подход, создавая и отбирая наиболее эффективные подмодели. Важной частью метода является обучение после обрезки, что позволяет улучшить производительность моделей. Эксперименты показывают, что \x1710\x187 превосходит существующие методы, требуя меньше данных для обучения.'}, 'en': {'title': 'Efficient Pruning for Powerful Language Models', 'desc': 'This paper presents a new method called \textit{sysname} for structured pruning of large language models (LLMs) to improve their efficiency in natural language processing tasks. The method uses an evolutionary search process to create multiple model variations, selecting the best-performing ones while incorporating a lightweight training process to enhance post-compression performance. By focusing on the varying sensitivities of different model components, \textit{sysname} achieves effective model compression without sacrificing accuracy. The results show that \textit{sysname} outperforms existing methods, requiring significantly less training data while maintaining high performance on various LLMs.'}, 'zh': {'title': '训练感知的结构化剪枝方法', 'desc': '大型语言模型（LLMs）在自然语言处理任务中取得了显著成功，但其巨大的计算成本限制了其广泛应用，尤其是在实时应用中。结构化剪枝是一种有效的解决方案，通过压缩模型并直接提供端到端的速度提升。不同模型组件对剪枝的敏感性不同，因此需要非均匀的模型压缩方法。我们提出了一种名为\textit{sysname}的训练感知结构化剪枝方法，通过进化搜索过程生成多个后代模型，并在每一代中选择最适合的模型进行生存。'}}}, {'id': 'https://huggingface.co/papers/2502.09411', 'title': 'ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation', 'url': 'https://huggingface.co/papers/2502.09411', 'abstract': 'Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG', 'score': 7, 'issue_id': 2251, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'a3c0f020b9cc5226', 'authors': ['Rotem Shalev-Arkushin', 'Rinon Gal', 'Amit H. Bermano', 'Ohad Fried'], 'affiliations': ['NVIDIA', 'Reichman University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09411.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#cv', '#rag', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'ImageRAG: Улучшение генерации изображений с помощью извлечения и дополнения', 'desc': 'Статья представляет метод ImageRAG, который использует технологию извлечения и дополнения генерации (RAG) для улучшения работы диффузионных моделей при создании редких или невиданных концепций. ImageRAG динамически извлекает релевантные изображения на основе текстового запроса и использует их в качестве контекста для управления процессом генерации. В отличие от предыдущих подходов, ImageRAG не требует специального обучения для работы с извлеченными изображениями, а использует возможности существующих моделей обусловливания изображений. Метод показывает значительное улучшение в генерации редких и детализированных концепций с использованием различных базовых моделей.'}, 'en': {'title': 'Enhancing Image Generation with Dynamic Retrieval', 'desc': 'This paper introduces ImageRAG, a novel method that enhances image generation by integrating Retrieval-Augmented Generation (RAG) techniques. Unlike previous methods that required specific training for retrieval-based generation, ImageRAG utilizes existing image conditioning models to dynamically retrieve relevant images based on text prompts. This approach allows for improved synthesis of rare and fine-grained visual concepts by providing contextual guidance during the generation process. The adaptability of ImageRAG across various model types demonstrates its effectiveness in producing high-quality and diverse visual content.'}, 'zh': {'title': 'ImageRAG：提升稀有概念生成的创新方法', 'desc': '扩散模型可以生成高质量和多样化的视觉内容，但在生成稀有或未见过的概念时存在困难。为了解决这个问题，我们探索了结合检索增强生成（RAG）与图像生成模型的方法。我们提出了ImageRAG，这是一种根据给定文本提示动态检索相关图像的方法，并将这些图像作为上下文来指导生成过程。与之前需要专门训练的检索生成模型不同，ImageRAG利用现有图像条件模型的能力，无需特定的RAG训练，适应性强，能够在不同模型类型中应用。'}}}, {'id': 'https://huggingface.co/papers/2502.10235', 'title': 'AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.10235', 'abstract': 'Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.', 'score': 7, 'issue_id': 2248, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': 'dbd38216ecfcb531', 'authors': ['Abdelhakim Benechehab', 'Vasilii Feofanov', 'Giuseppe Paolo', 'Albert Thomas', 'Maurizio Filippone', 'Balázs Kégl'], 'affiliations': ['Department of Data Science, EURECOM', 'Huawei Noahs Ark Lab, Paris, France', 'Statistics Program, KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2502.10235.jpg', 'data': {'categories': ['#data', '#synthetic', '#dataset', '#inference', '#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'AdaPTS: Адаптация одномерных моделей для многомерного прогнозирования временных рядов', 'desc': 'Исследование представляет AdaPTS - фреймворк, использующий адаптеры для применения предобученных моделей-основ (foundation models) в задачах многомерного прогнозирования временных рядов. Адаптеры проецируют многомерные входные данные в латентное пространство, позволяя эффективно использовать одномерные модели для многомерных задач. Эксперименты на синтетических и реальных данных показали значительное улучшение точности прогнозирования и количественной оценки неопределенности по сравнению с базовыми методами. Фреймворк AdaPTS представляет собой модульное и масштабируемое решение для использования моделей временных рядов в многомерных контекстах.'}, 'en': {'title': 'Enhancing Multivariate Time Series Forecasting with Adapters', 'desc': 'This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications.'}, 'zh': {'title': '适配器：多变量时间序列预测的新解决方案', 'desc': '预训练基础模型在单变量时间序列预测任务中表现出色，但在处理特征间复杂依赖关系和量化预测不确定性方面仍面临挑战。本研究通过引入适配器来解决这些关键限制，适配器是一种特征空间转换，能够有效利用预训练的单变量时间序列模型进行多变量任务。适配器通过将多变量输入投影到合适的潜在空间，并独立地对每个维度应用基础模型，从而实现功能。实验结果表明，适配器在预测准确性和不确定性量化方面显著优于基线方法，展示了其在多变量时间序列应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.07586', 'title': "We Can't Understand AI Using our Existing Vocabulary", 'url': 'https://huggingface.co/papers/2502.07586', 'abstract': 'This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they\'re reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.', 'score': 6, 'issue_id': 2247, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '0435a4b5389f3dcb', 'authors': ['John Hewitt', 'Robert Geirhos', 'Been Kim'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.07586.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#interpretability'], 'emoji': '🗣️', 'ru': {'title': 'Неологизмы как мост между человеком и ИИ', 'desc': 'В статье утверждается, что для понимания искусственного интеллекта недостаточно существующего человеческого словаря. Авторы предлагают разрабатывать неологизмы - новые слова для точных человеческих и машинных концепций. Интерпретируемость рассматривается как проблема коммуникации между людьми и машинами. Создание общего языка через неологизмы может решить эту проблему, позволяя лучше контролировать и понимать ИИ.'}, 'en': {'title': 'Creating New Words for Better AI Communication', 'desc': 'This paper discusses the need for new words, or neologisms, to better communicate with artificial intelligence (AI). It argues that humans and machines have different ways of understanding concepts, which makes it hard for us to explain our ideas to machines. By creating a shared language with these new terms, we can improve how we control and interpret machine behavior. The authors provide examples of neologisms that help manage AI responses, showing that a richer vocabulary can enhance our interaction with AI systems.'}, 'zh': {'title': '通过新词汇理解人工智能', 'desc': '这篇论文认为，要理解人工智能，我们不能仅依赖现有的人类词汇。我们应该努力开发新词汇，以准确表达我们想教给机器的人类概念或我们需要学习的机器概念。人类和机器的概念不同，因此可解释性可以看作是一个沟通问题：人类必须能够引用和控制机器概念，并将人类概念传达给机器。通过开发新词汇创建一个共享的人机语言，可以解决这个沟通问题。'}}}, {'id': 'https://huggingface.co/papers/2502.09741', 'title': 'FoNE: Precise Single-Token Number Embeddings via Fourier Features', 'url': 'https://huggingface.co/papers/2502.09741', 'abstract': "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.", 'score': 6, 'issue_id': 2241, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'adb30f7d3d01ce3a', 'authors': ['Tianyi Zhou', 'Deqing Fu', 'Mahdi Soltanolkotabi', 'Robin Jia', 'Vatsal Sharan'], 'affiliations': ['Department of Computer Science University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.09741.jpg', 'data': {'categories': ['#architecture', '#training', '#data', '#optimization'], 'emoji': '🔢', 'ru': {'title': 'FoNE: революция в обработке чисел для языковых моделей', 'desc': 'Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). FoNE представляет числа в виде единых токенов с использованием фурье-подобных признаков, что позволяет эффективно захватывать числовые значения без фрагментации. Этот компактный способ представления ускоряет как обучение, так и вывод модели. По сравнению с традиционными методами, FoNE демонстрирует более высокую точность в различных числовых задачах, включая сложение, вычитание и умножение, при меньших вычислительных затратах.'}, 'en': {'title': 'Revolutionizing Number Representation in LLMs with FoNE', 'desc': 'This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication.'}, 'zh': {'title': '傅里叶数字嵌入：高效的数字表示方法', 'desc': '大型语言模型（LLMs）通常使用多个标记来表示数字，这导致模型在理解数值时需要聚合这些标记，降低了训练和推理的效率。我们提出了一种新方法，称为傅里叶数字嵌入（FoNE），它将数字直接映射到嵌入空间，使用傅里叶特征进行编码。FoNE将每个数字编码为一个单一标记，显著减少了计算开销，并在加法、减法和乘法等数值任务中实现了更高的准确性。与传统的子词和数字嵌入相比，FoNE在6位十进制加法中所需的数据量减少了64倍，同时每个数字使用的标记数量也减少了3倍到6倍。'}}}, {'id': 'https://huggingface.co/papers/2502.10140', 'title': 'Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages', 'url': 'https://huggingface.co/papers/2502.10140', 'abstract': 'Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.', 'score': 4, 'issue_id': 2251, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '0d0292edb1900dd5', 'authors': ['Daniil Gurgurov', 'Ivan Vykopal', 'Josef van Genabith', 'Simon Ostermann'], 'affiliations': ['Brno University of Technology', 'German Research Center for Artificial Intelligence (DFKI)', 'Kempelen Institute of Intelligent Technologies (KInIT)', 'University of Saarland'], 'pdf_title_img': 'assets/pdf/title_img/2502.10140.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#small_models', '#multilingual', '#low_resource'], 'emoji': '🌍', 'ru': {'title': 'Эффективная адаптация многоязычных моделей для языков с ограниченными ресурсами', 'desc': 'Исследование посвящено адаптации многоязычных моделей (mLMs) для языков с ограниченными ресурсами (LRLs) с использованием эффективных по параметрам методов на основе адаптеров. Авторы сравнивают три архитектуры адаптеров: Sequential Bottleneck, Invertible Bottleneck и Low-Rank Adaptation, оценивая их эффективность на различных задачах обработки естественного языка. Результаты показывают, что адаптация на небольших наборах данных (до 1 ГБ неструктурированного текста или несколько МБ структурированных знаний) улучшает производительность моделей. Исследование также демонстрирует, что для LRLs более эффективны меньшие mLMs, чем крупные языковые модели типа LLaMA-3 или GPT-4.'}, 'en': {'title': 'Empowering Low-Resource Languages with Efficient Multilingual Models', 'desc': 'This paper addresses the challenges of processing low-resource languages (LRLs) in natural language processing (NLP) by exploring the use of smaller multilingual models (mLMs) like mBERT and XLM-R. It investigates parameter-efficient adapter-based methods to adapt these mLMs to LRLs, focusing on three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. The study demonstrates that even small adaptation datasets can significantly enhance performance in both intrinsic and extrinsic NLP tasks. Results indicate that Sequential Bottleneck adapters are particularly effective for language modeling, while Invertible Bottleneck adapters perform better on downstream tasks, highlighting the advantages of adapter-based methods over full fine-tuning.'}, 'zh': {'title': '小型多语言模型助力低资源语言处理', 'desc': '低资源语言（LRLs）在自然语言处理（NLP）中面临数据不足的重大挑战。当前的先进大型语言模型（LLMs）在处理LRLs时仍然存在困难，而较小的多语言模型（mLMs）如mBERT和XLM-R由于其容量更适合低训练数据量，展现出更大的潜力。本文系统研究了基于参数高效适配器的方法，评估了三种架构：顺序瓶颈、可逆瓶颈和低秩适配。研究表明，使用小型适配数据集可以在语言建模和下游任务中取得显著提升，尤其是顺序瓶颈适配器在语言建模方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2502.08130', 'title': 'Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models', 'url': 'https://huggingface.co/papers/2502.08130', 'abstract': "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to 4.4 on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. 2.5, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.", 'score': 3, 'issue_id': 2255, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '2270a63ed413aee6', 'authors': ['Sonam Gupta', 'Yatin Nandwani', 'Asaf Yehudai', 'Dinesh Khandelwal', 'Dinesh Raghu', 'Sachindra Joshi'], 'affiliations': ['IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.08130.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#benchmark', '#plp', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'S3FT: Улучшение обобщающей способности языковых моделей при дообучении', 'desc': 'Данная статья представляет новый метод дообучения больших языковых моделей (LLM) под названием Selective Self-to-Supervised Fine-Tuning (S3FT). S3FT использует множество правильных ответов модели на запрос, что позволяет улучшить производительность по сравнению со стандартным методом дообучения с учителем (SFT). Эксперименты показали, что S3FT снижает падение производительности на различных бенчмарках вдвое по сравнению с SFT. Метод S3FT демонстрирует лучшую обобщающую способность при сохранении высокой производительности на целевых задачах.'}, 'en': {'title': 'Enhancing Generalization in Fine-Tuning with S3FT', 'desc': "This paper presents Selective Self-to-Supervised Fine-Tuning (S3FT), a novel approach to fine-tuning Large Language Models (LLMs) that enhances their performance while mitigating overfitting. S3FT improves generalization by utilizing multiple valid responses to a query, allowing the model to learn from its correct outputs rather than just the training data. By identifying and incorporating these correct responses during the fine-tuning process, S3FT reduces the model's tendency to specialize too narrowly on specific tasks. Experimental results demonstrate that S3FT outperforms standard supervised fine-tuning (SFT) and significantly improves generalization across various benchmarks, including mathematical reasoning and programming tasks."}, 'zh': {'title': '选择性自监督微调：提升模型泛化能力的创新方法', 'desc': '本文提出了一种新的微调方法，称为选择性自监督微调（S3FT），旨在提高大型语言模型（LLMs）在特定任务上的表现，同时改善模型的泛化能力。传统的监督微调（SFT）常常导致模型过拟合，即模型对训练数据或任务过于专注，失去对新数据的适应能力。S3FT通过利用多个有效响应来减少模型的专门化，首先识别训练集中正确的模型响应，然后结合这些响应和真实答案进行微调。实验结果表明，S3FT在数学推理、Python编程和阅读理解等任务上表现优于标准的SFT，显著提高了模型的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.10392', 'title': 'Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding', 'url': 'https://huggingface.co/papers/2502.10392', 'abstract': 'In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on ScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. The code is available at https://github.com/GWxuan/TSP3D{https://github.com/GWxuan/TSP3D}.', 'score': 3, 'issue_id': 2252, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '364eaebf9db5bd4a', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10392.jpg', 'data': {'categories': ['#3d', '#architecture', '#training', '#cv'], 'emoji': '🏆', 'ru': {'title': 'Эффективная 3D визуальная локализация с помощью многоуровневой сверточной архитектуры', 'desc': 'В статье предлагается эффективная многоуровневая сверточная архитектура для 3D визуальной локализации. Авторы вводят текстово-управляемое прореживание (TGP) и дополнение на основе завершения (CBA) для эффективного объединения 3D-представления сцены и текстовых признаков. Метод достигает высокой скорости вывода, превосходя предыдущий самый быстрый метод на 100% по FPS. Также достигается наилучшая точность по сравнению с двухэтапными методами на наборах данных ScanRefer, NR3D и SR3D.'}, 'en': {'title': 'Efficient 3D Visual Grounding with Multi-Level Convolution', 'desc': 'This paper introduces a new multi-level convolution architecture designed for 3D visual grounding, which is the task of linking 3D scenes with textual descriptions. Traditional methods struggle with real-time performance due to their complex two-stage or point-based designs. The authors leverage a multi-level fully sparse convolution approach, enhancing the interaction between 3D scene representations and text features through innovative techniques like text-guided pruning (TGP) and completion-based addition (CBA). Their method not only improves inference speed, achieving a 100% increase in frames per second (FPS) compared to the fastest existing methods, but also enhances accuracy on benchmark datasets, outperforming previous models.'}, 'zh': {'title': '高效融合3D场景与文本特征的视觉定位新方法', 'desc': '本文提出了一种高效的多层卷积架构，用于3D视觉定位。传统方法由于采用两阶段或基于点的架构，难以满足实时推理的要求。我们借鉴了多层稀疏卷积架构在3D物体检测中的成功，构建了新的3D视觉定位框架。通过文本引导修剪和基于补全的添加，我们有效地融合了3D场景表示和文本特征，显著提高了推理速度和准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.09638', 'title': 'Jailbreaking to Jailbreak', 'url': 'https://huggingface.co/papers/2502.09638', 'abstract': 'Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J_2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.', 'score': 3, 'issue_id': 2242, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '3c2ed560e12b971a', 'authors': ['Jeremy Kritz', 'Vaughn Robinson', 'Robert Vacareanu', 'Bijan Varjavand', 'Michael Choi', 'Bobby Gogov', 'Scale Red Team', 'Summer Yue', 'Willow E. Primack', 'Zifan Wang'], 'affiliations': ['Scale'], 'pdf_title_img': 'assets/pdf/title_img/2502.09638.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#training', '#security', '#rlhf'], 'emoji': '🕵️', 'ru': {'title': 'LLM против LLM: новый фронт в безопасности ИИ', 'desc': "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красной команды'. Авторы демонстрируют, как взломанная модель (J_2) может систематически оценивать и атаковать другие модели, достигая высокого уровня успеха. Эксперименты показывают, что Sonnet 3.5 и Gemini 1.5 pro превосходят другие LLM в роли J_2, достигая 93.0% и 91.0% успешности атак соответственно. Исследование подчеркивает уязвимость существующих механизмов защиты и предлагает новый метод для улучшения безопасности искусственного интеллекта."}, 'en': {'title': 'Jailbreaking the Safeguards: A New Approach to LLM Red Teaming', 'desc': 'This paper discusses a new method for testing the safety of Large Language Models (LLMs) by using a jailbroken version of the model itself, referred to as J_2 attackers. These J_2 attackers can evaluate and exploit vulnerabilities in other LLMs, achieving high success rates in bypassing safeguards. The approach allows the jailbroken LLM to learn from its previous attempts, improving its effectiveness in red teaming strategies. The authors emphasize the importance of understanding this jailbreaking-to-jailbreak phenomenon as a potential risk in AI safety.'}, 'zh': {'title': '破解自我保护的红队策略', 'desc': '本论文提出了一种新的方法，利用大型语言模型（LLM）作为红队成员，来评估和改进拒绝训练模型的安全性。我们称这些被破解的LLM为J_2攻击者，它们能够通过上下文学习从之前的失败中提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5在攻击成功率上优于其他LLM，分别达到了93.0%和91.0%。我们的研究不仅提供了一种可扩展的红队策略，还揭示了破解自身保护机制的潜在风险。'}}}, {'id': 'https://huggingface.co/papers/2502.10177', 'title': 'STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning', 'url': 'https://huggingface.co/papers/2502.10177', 'abstract': 'A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.', 'score': 3, 'issue_id': 2240, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '1b17b668b26c2264', 'authors': ['Mingcong Lei', 'Yiming Zhao', 'Ge Wang', 'Zhixin Mai', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China', 'Harbin Engineering University, China', 'Infused Synapse AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.10177.jpg', 'data': {'categories': ['#games', '#agents', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Пространственно-временная память повышает эффективность воплощенных агентов', 'desc': 'Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения задач агентами с воплощенным интеллектом. STMA включает в себя модуль пространственно-временной памяти, динамический граф знаний и механизм планировщика-критика. Эксперименты в среде TextWorld показали значительное улучшение успешности и средней оценки по сравнению с современными моделями. Результаты подчеркивают эффективность пространственно-временной памяти для улучшения возможностей памяти воплощенных агентов.'}, 'en': {'title': 'Enhancing Agent Intelligence with Spatio-Temporal Memory', 'desc': 'The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence.'}, 'zh': {'title': '时空记忆智能体：提升智能体决策与适应能力的关键', 'desc': '本文提出了一种新的框架，称为时空记忆智能体（STMA），旨在提高智能体在动态环境中执行长期任务的能力。STMA集成了时空记忆模块、动态知识图谱和规划-评估机制，以增强任务规划和执行的效果。通过在TextWorld环境中进行32个任务的评估，STMA在成功率和平均得分上分别提高了31.25%和24.7%。实验结果表明，时空记忆在提升智能体的记忆能力方面具有显著效果。'}}}, {'id': 'https://huggingface.co/papers/2502.07856', 'title': 'MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers', 'url': 'https://huggingface.co/papers/2502.07856', 'abstract': 'In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.', 'score': 2, 'issue_id': 2244, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '2bb6766a68f50cdc', 'authors': ['Ao Li', 'Wei Fang', 'Hongbo Zhao', 'Le Lu', 'Ge Yang', 'Minfeng Xu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Laboratory, Hangzhou, China', 'Institute of Automation, Chinese Academy of Sciences (CASIA)', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07856.jpg', 'data': {'categories': ['#training', '#cv', '#data', '#diffusion', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение управляемой генерации изображений без потери качества', 'desc': 'Статья представляет новый алгоритм MRS (MR Sampler) для ускорения процесса семплирования в Mean Reverting (MR) Diffusion моделях. Авторы решают обратное стохастическое дифференциальное уравнение и обыкновенное дифференциальное уравнение потока вероятности, связанные с MR Diffusion, и выводят полуаналитические решения. Этот подход не требует дополнительного обучения и поддерживает все основные параметризации, включая предсказание шума, данных и скорости. Эксперименты показывают, что MR Sampler сохраняет высокое качество семплирования при ускорении в 10-20 раз для десяти различных задач восстановления изображений.'}, 'en': {'title': 'Accelerating Controllable Generation with MR Sampler', 'desc': 'This paper introduces a new algorithm called MRS (MR Sampler) to improve the efficiency of sampling in Mean Reverting (MR) Diffusion models. Unlike traditional methods that modify the score function, MRS directly addresses the stochastic differential equation structure, simplifying the integration of image conditions. The proposed method achieves high-quality sample generation with significantly fewer function evaluations, enhancing the speed of the sampling process. Experimental results show that MRS can produce samples 10 to 20 times faster while maintaining quality across various image restoration tasks.'}, 'zh': {'title': '加速可控生成的MR采样器', 'desc': '在扩散模型的应用中，可控生成具有重要的实际意义，但也面临挑战。当前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归扩散（MR Diffusion）则直接修改随机微分方程（SDE）的结构，使得图像条件的结合更加简单自然。本文提出了一种新算法MRS（MR采样器），旨在减少MR扩散的采样函数评估次数（NFEs），并通过解决与MR扩散相关的反向时间SDE和概率流常微分方程（PF-ODE）来获得高质量样本。实验表明，MR采样器在十种不同的图像恢复任务中保持高采样质量，并实现了10到20倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2502.10362', 'title': 'CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages', 'url': 'https://huggingface.co/papers/2502.10362', 'abstract': 'CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.', 'score': 1, 'issue_id': 2253, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '01dc60f8f9db0ad7', 'authors': ['Shangda Wu', 'Zhancheng Guo', 'Ruibin Yuan', 'Junyan Jiang', 'Seungheon Doh', 'Gus Xia', 'Juhan Nam', 'Xiaobing Li', 'Feng Yu', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.10362.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#multilingual', '#rag', '#low_resource', '#data', '#benchmark', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'Единая платформа для мультимодального и многоязычного анализа музыки', 'desc': 'CLaMP 3 - это унифицированная система для решения задач кросс-модальной и кросс-языковой генерализации в области извлечения музыкальной информации. Она использует контрастное обучение для объединения основных музыкальных модальностей (ноты, сигналы исполнения, аудиозаписи) с многоязычным текстом в едином пространстве представлений. Система включает многоязычный текстовый энкодер, адаптируемый к новым языкам, и демонстрирует сильную кросс-языковую генерализацию. Эксперименты показывают, что CLaMP 3 достигает наилучших результатов в различных задачах MIR, значительно превосходя предыдущие сильные базовые модели.'}, 'en': {'title': 'Bridging Music and Language with CLaMP 3', 'desc': "CLaMP 3 is a new framework designed to improve how we retrieve music information across different formats and languages. It uses contrastive learning to connect various music types, like sheet music and audio, with text in multiple languages, allowing for better searches even when the formats don't match. The framework includes a multilingual text encoder that can adapt to new languages, showing its ability to generalize across different linguistic contexts. Additionally, it introduces a large dataset and benchmark to support further research in music information retrieval, achieving top performance in various tasks."}, 'zh': {'title': '跨模态音乐检索的新突破', 'desc': 'CLaMP 3 是一个统一框架，旨在解决音乐信息检索中的跨模态和跨语言泛化挑战。它通过对比学习，将乐谱、表演信号和音频录音等主要音乐模态与多语言文本对齐，形成共享表示空间，从而实现通过文本作为桥梁的检索。该框架具有适应未见语言的多语言文本编码器，展现出强大的跨语言泛化能力。通过增强检索生成，我们创建了 M4-RAG 数据集，包含 231 万对音乐-文本对，并发布了 WikiMT-X 基准，推动未来研究。'}}}, {'id': 'https://huggingface.co/papers/2502.08769', 'title': 'Cluster and Predict Latents Patches for Improved Masked Image Modeling', 'url': 'https://huggingface.co/papers/2502.08769', 'abstract': 'Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models.', 'score': 1, 'issue_id': 2250, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '8fd9852310af51f5', 'authors': ['Timothée Darcet', 'Federico Baldassarre', 'Maxime Oquab', 'Julien Mairal', 'Piotr Bojanowski'], 'affiliations': ['CNRS', 'Grenoble INP', 'Inria', 'LJK', 'Meta', 'Univ. Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2502.08769.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#architecture', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'CAPI: Прорыв в самообучаемом представлении изображений', 'desc': 'Статья представляет CAPI - новую систему для самообучаемого представления изображений на основе маскированного моделирования изображений (MIM). CAPI использует прогнозирование латентных кластеризаций и новую функцию потерь на основе кластеризации. Модель достигает точности 83.8% на ImageNet и 32.1% mIoU на ADE20K с использованием простых линейных проб. CAPI значительно превосходит предыдущие методы MIM и приближается к производительности современного state-of-the-art метода DINOv2.'}, 'en': {'title': 'CAPI: Clustering for Superior Masked Image Modeling', 'desc': 'This paper presents CAPI, a new framework for Masked Image Modeling (MIM) that enhances self-supervised learning by focusing on predicting latent clusterings. The authors analyze various aspects of MIM, including target representations and loss functions, to develop a clustering-based loss that is stable during training. CAPI utilizes a Vision Transformer (ViT-L) backbone, achieving impressive results with 83.8% accuracy on ImageNet and 32.1% mean Intersection over Union (mIoU) on ADE20K. The framework significantly outperforms previous MIM methods and approaches the performance of the leading model, DINOv2, while the authors provide all code and models for further research.'}, 'zh': {'title': 'CAPI：提升自监督学习的新方法', 'desc': '本文提出了一种新的纯遮罩图像建模框架CAPI，旨在提升自监督表示学习的效果。我们系统分析了目标表示、损失函数和架构，提出了一种基于聚类的损失函数，使得训练过程更加稳定。CAPI在ViT-L骨干网络上实现了83.8%的ImageNet准确率和32.1%的ADE20K mIoU，显著超越了之前的MIM方法。我们将所有代码和模型公开，促进研究的进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2502.09980', 'title': 'V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.09980', 'abstract': 'Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .', 'score': 1, 'issue_id': 2244, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '57343c782d806dc0', 'authors': ['Hsu-kuang Chiu', 'Ryo Hachiuma', 'Chien-Yi Wang', 'Stephen F. Smith', 'Yu-Chiang Frank Wang', 'Min-Hung Chen'], 'affiliations': ['Carnegie Mellon University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.09980.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#reasoning', '#science', '#agi', '#games', '#optimization', '#dataset', '#agents'], 'emoji': '🚗', 'ru': {'title': 'LLM на службе кооперативного автономного вождения', 'desc': 'Статья представляет новый подход к кооперативному автономному вождению с использованием больших языковых моделей (LLM). Авторы предлагают датасет и бенчмарк V2V-QA для обмена информацией между автомобилями через вопросно-ответную систему. Их метод V2V-LLM использует LLM для объединения данных восприятия от нескольких подключенных автономных транспортных средств и ответа на вопросы, связанные с вождением. Эксперименты показывают, что V2V-LLM превосходит другие базовые методы и открывает новое направление исследований для повышения безопасности будущих систем автономного вождения.'}, 'en': {'title': 'Enhancing Cooperative Driving with Language Models', 'desc': 'This paper introduces a new approach to enhance cooperative autonomous driving by integrating Large Language Models (LLMs) with vehicle-to-vehicle (V2V) communication. The proposed method, called Vehicle-to-Vehicle Large Language Model (V2V-LLM), allows connected autonomous vehicles (CAVs) to share and fuse perception data, enabling them to answer driving-related questions effectively. The authors present a new dataset, Vehicle-to-Vehicle Question-Answering (V2V-QA), to benchmark this integration and demonstrate its effectiveness in improving planning and safety. Experimental results indicate that V2V-LLM outperforms existing methods, paving the way for a unified model architecture in cooperative driving systems.'}, 'zh': {'title': '协作自动驾驶的新方向：车辆间问答模型', 'desc': '当前的自动驾驶车辆主要依赖各自的传感器来理解周围场景和规划未来轨迹，但当传感器出现故障或被遮挡时，这种方法可能不可靠。为了解决这个问题，提出了通过车与车（V2V）通信的协作感知方法，但这些方法主要集中在检测和跟踪上。本文提出了一种新颖的问题设置，将大型语言模型（LLM）集成到协作自动驾驶中，并引入了车辆间问答（V2V-QA）数据集和基准测试。我们的实验结果表明，V2V-LLM能够有效融合多个连接的自动驾驶车辆的感知信息，并在协作自动驾驶中执行多种任务，提升未来自动驾驶系统的安全性。'}}}, {'id': 'https://huggingface.co/papers/2502.10173', 'title': 'Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model', 'url': 'https://huggingface.co/papers/2502.10173', 'abstract': 'Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.', 'score': 0, 'issue_id': 2247, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '84102aa522298331', 'authors': ['Bo Ni', 'Markus J. Buehler'], 'affiliations': ['Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA, USA', 'Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, Cambridge, MA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.10173.jpg', 'data': {'categories': ['#architecture', '#agents', '#dataset'], 'emoji': '🧬', 'ru': {'title': 'VibeGen: ИИ-дизайн белков с заданной динамикой', 'desc': 'VibeGen - это генеративная ИИ-система для проектирования белков с заданными динамическими свойствами. Она использует двойную архитектуру с моделью-дизайнером, генерирующей последовательности белков, и моделью-предиктором, оценивающей их динамическую точность. VibeGen способна создавать de novo белки с заданными нормальными модами колебаний, что подтверждается полноатомным молекулярным моделированием. Эта система открывает новые возможности для инженерии биомолекул с заданными динамическими и функциональными свойствами.'}, 'en': {'title': 'Unlocking Dynamic Protein Design with VibeGen', 'desc': 'This paper presents VibeGen, a generative AI framework designed for creating proteins with specific dynamic properties. It utilizes a dual-model architecture that includes a protein designer to generate sequences based on desired vibrational modes and a protein predictor to assess their dynamic accuracy. The framework successfully integrates protein dynamics into the design process, allowing for the creation of novel protein sequences that do not resemble existing natural proteins. This innovation opens new avenues for engineering proteins with tailored functions and dynamics, which could significantly impact fields like enzyme design and biomaterials.'}, 'zh': {'title': '动态驱动的蛋白质设计新方法', 'desc': '这篇论文介绍了一种名为VibeGen的生成性人工智能框架，用于设计具有特定动态特性的蛋白质。VibeGen结合了蛋白质设计器和蛋白质预测器，前者根据指定的振动模式生成序列候选，后者评估其动态准确性。通过全原子分子模拟验证，设计的蛋白质能够准确再现预定的正常模式振幅，并采用多种稳定的、功能相关的结构。该方法不仅扩展了可设计蛋白质的空间，还为灵活酶、动态支架和生物材料的理性设计提供了新的途径。'}}}, {'id': 'https://huggingface.co/papers/2502.12900', 'title': 'Soundwave: Less is More for Speech-Text Alignment in LLMs', 'url': 'https://huggingface.co/papers/2502.12900', 'abstract': 'Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.', 'score': 62, 'issue_id': 2289, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '95780ecdf251cffd', 'authors': ['Yuhao Zhang', 'Zhiheng Liu', 'Fan Bu', 'Ruiyu Zhang', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.12900.jpg', 'data': {'categories': ['#training', '#audio', '#transfer_learning', '#open_source', '#optimization', '#data', '#architecture'], 'emoji': '🎙️', 'ru': {'title': 'Эффективное обучение речевых моделей с минимумом данных', 'desc': 'Исследователи представили Soundwave - новую модель обработки речи, которая решает проблемы разрыва пространства представлений и несоответствия длины последовательностей между речью и текстом. Модель использует эффективную стратегию обучения и инновационную архитектуру. Soundwave превосходит передовую модель Qwen2-Audio в задачах перевода речи и тестах AIR-Bench, используя всего 1/50 часть обучающих данных. При этом модель сохраняет свой интеллект во время диалога.'}, 'en': {'title': 'Soundwave: Efficient Speech Translation with Minimal Data', 'desc': 'This paper introduces Soundwave, a new model designed for speech translation that requires significantly less training data compared to existing large language models. It addresses two key challenges: the differences in how speech and text are represented and the varying lengths of sequences in speech data. By employing an efficient training strategy and a unique architecture, Soundwave achieves superior performance on speech tasks while using only 2% of the data needed by its competitors. The findings indicate that Soundwave maintains high conversational intelligence, making it a promising approach for data-efficient speech processing.'}, 'zh': {'title': '高效训练，语音翻译新突破！', 'desc': '现有的端到端语音大型语言模型通常依赖于大规模标注数据进行训练，而数据高效训练尚未深入探讨。我们关注语音和文本之间的两个基本问题：表示空间差距和序列长度不一致。我们提出了Soundwave，它利用高效的训练策略和新颖的架构来解决这些问题。结果表明，Soundwave在语音翻译和AIR-Bench语音任务中表现优于先进的Qwen2-Audio，仅使用了五十分之一的训练数据。'}}}, {'id': 'https://huggingface.co/papers/2502.13063', 'title': 'Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity', 'url': 'https://huggingface.co/papers/2502.13063', 'abstract': 'A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches allow to reduce the amount of compute in existing language models. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.', 'score': 51, 'issue_id': 2293, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'bd5537cf011da83d', 'authors': ['Yuri Kuratov', 'Mikhail Arkhipov', 'Aydar Bulatov', 'Mikhail Burtsev'], 'affiliations': ['AIRI, Moscow, Russia', 'Independent Researcher, Amsterdam, Netherlands', 'London Institute for Mathematical Sciences, London, UK', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.13063.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '🗜️', 'ru': {'title': 'Раскрывая потенциал сверхсжатия токенов в языковых моделях', 'desc': 'Статья исследует возможности сжатия последовательностей токенов в более короткие последовательности векторов для использования в языковых моделях. Авторы показывают, что существующие методы достигают коэффициента сжатия не выше 10, хотя теоретически возможно гораздо большее сжатие. Используя оптимизацию для каждого образца, исследователи демонстрируют возможность сжатия до 1500 раз. Результаты указывают на значительный разрыв между теоретической ёмкостью входных эмбеддингов и их практическим использованием в современных нейронных сетях.'}, 'en': {'title': 'Unlocking Compression: From x10 to x1500 in Token Sequences', 'desc': 'This paper investigates the compression of token sequences into shorter real-valued vectors for use in language models. It reveals that while current methods achieve a maximum lossless compression ratio of about x10, a new optimization approach can reach ratios as high as x1500. The study emphasizes that the limits of compression are influenced more by the uncertainty in the data rather than the input length itself. This finding indicates a significant disparity between the theoretical potential of embeddings and their actual performance, suggesting opportunities for further optimization in model architecture.'}, 'zh': {'title': '压缩潜力：从10倍到1500倍的飞跃', 'desc': '这篇论文探讨了将序列的标记压缩为更短的实值向量序列的问题，以便用作输入，而不是使用标记嵌入或键值缓存。尽管现有的编码器模型非常强大，但无损压缩的最大比率通常不超过10倍。研究表明，通过每个样本的优化程序替代编码器，可以实现高达1500倍的压缩比，显示出现有解决方案与可实际达到的解决方案之间的巨大差距。此外，压缩的限制主要由需要减少的不确定性决定，而不是输入的长度，这为模型设计的优化提供了重要的空间。'}}}, {'id': 'https://huggingface.co/papers/2502.11564', 'title': 'Continuous Diffusion Model for Language Modeling', 'url': 'https://huggingface.co/papers/2502.11564', 'abstract': 'Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.', 'score': 41, 'issue_id': 2287, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'f5889d3a88d24b7c', 'authors': ['Jaehyeong Jo', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2502.11564.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#dataset', '#diffusion', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'Непрерывная диффузия на статистическом многообразии для языкового моделирования', 'desc': 'Статья представляет новый подход к моделированию естественного языка с использованием непрерывных диффузионных моделей. Авторы устанавливают связь между дискретной диффузией и непрерывным потоком на статистическом многообразии. Предложенный метод обобщает предыдущие дискретные диффузионные модели и вводит безсимуляционную схему обучения. Эксперименты показывают, что данный подход превосходит существующие дискретные диффузионные модели и приближается к производительности авторегрессионных моделей в задачах языкового моделирования.'}, 'en': {'title': 'Revolutionizing Language Modeling with Continuous Diffusion', 'desc': 'This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models.'}, 'zh': {'title': '连续扩散模型：提升离散数据建模的性能', 'desc': '扩散模型作为一种新兴的替代自回归模型的方法，在建模离散分类数据方面展现了良好的前景。现有的连续扩散模型在处理离散数据时性能有限，且二者之间的联系不明确，限制了扩散模型的发展。本文提出了一种新的连续扩散模型，结合了基础分类分布的几何特性，并建立了离散扩散与连续流动之间的联系。通过全面的实验，我们的方法在语言建模基准测试中超越了现有的离散扩散模型，接近自回归模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.11079', 'title': 'Phantom: Subject-consistent video generation via cross-modal alignment', 'url': 'https://huggingface.co/papers/2502.11079', 'abstract': 'The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.', 'score': 40, 'issue_id': 2286, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'e443a635e58b164f', 'authors': ['Lijie Liu', 'Tianxiang Ma', 'Bingchuan Li', 'Zhuowei Chen', 'Jiawei Liu', 'Qian He', 'Xinglong Wu'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.11079.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Phantom: баланс текста и изображения для создания согласованного видео', 'desc': 'Эта статья представляет Phantom - унифицированную систему генерации видео на основе текстовых и визуальных подсказок. Авторы предлагают новый подход к созданию видео с сохранением характеристик субъектов из опорных изображений. Phantom переосмысливает модель совместной обработки текста и изображений, обучаясь на триплетах текст-изображение-видео. Особое внимание уделяется сохранению идентичности людей при генерации видео.'}, 'en': {'title': 'Phantom: Consistent Video Generation from Text and Images', 'desc': 'This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology.'}, 'zh': {'title': '实现主题一致性的视频生成', 'desc': '这篇论文介绍了一种新的视频生成模型，称为Phantom，旨在实现主题一致性的视频生成。该模型通过提取参考图像中的主题元素，并结合文本指令生成视频。Phantom框架能够处理单一和多个主题的参考，强调文本和图像的双模态提示的平衡。研究者们通过文本-图像-视频三元组数据来学习跨模态对齐，从而提升人类生成视频的主题一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.13131', 'title': 'Rethinking Diverse Human Preference Learning through Principal Component Analysis', 'url': 'https://huggingface.co/papers/2502.13131', 'abstract': 'Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.', 'score': 32, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'b3376cde29e0b44f', 'authors': ['Feng Luo', 'Rui Yang', 'Hao Sun', 'Chunyuan Deng', 'Jiarui Yao', 'Jingyan Shen', 'Huan Zhang', 'Hanjie Chen'], 'affiliations': ['Columbia University', 'Rice University', 'University of Cambridge', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.13131.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training', '#dataset', '#interpretability'], 'emoji': '🧩', 'ru': {'title': 'Разложение предпочтений для персонализированного обучения языковых моделей', 'desc': 'Статья представляет новый подход к извлечению разнообразных человеческих предпочтений из бинарных сравнений без необходимости в детальных аннотациях - Decomposed Reward Models (DRMs). Метод использует векторное представление предпочтений и анализ методом главных компонент (PCA) для выявления ортогональных базисных векторов, отражающих различные аспекты предпочтений. DRMs позволяют гибко комбинировать разложенные награды для адаптации к потребностям разных пользователей, предлагая интерпретируемую и масштабируемую альтернативу традиционным моделям вознаграждения. Эксперименты показывают, что DRMs эффективно извлекают значимые измерения предпочтений и адаптируются к новым пользователям без дополнительного обучения.'}, 'en': {'title': 'Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI', 'desc': 'This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety.'}, 'zh': {'title': '分解奖励模型：个性化AI的新方法', 'desc': '理解人类偏好对于改善基础模型和构建个性化AI系统至关重要。传统的奖励模型难以捕捉偏好的多样性和复杂性。我们提出了一种新方法，称为分解奖励模型（DRMs），它通过二元比较提取人类偏好，而无需细粒度的注释。DRMs利用主成分分析（PCA）分析偏好向量，能够灵活组合以满足不同用户需求，提供了一种可解释且可扩展的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2502.13145', 'title': 'Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation', 'url': 'https://huggingface.co/papers/2502.13145', 'abstract': "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba", 'score': 26, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '6810113d41bfd26d', 'authors': ['Bencheng Liao', 'Hongyuan Tao', 'Qian Zhang', 'Tianheng Cheng', 'Yingyue Li', 'Haoran Yin', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.13145.jpg', 'data': {'categories': ['#open_source', '#optimization', '#transfer_learning', '#architecture', '#training', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'mmMamba: эффективные мультимодальные модели с линейной сложностью', 'desc': 'Статья представляет mmMamba - фреймворк для разработки мультимодальных моделей с линейной сложностью на основе state space models. Авторы предлагают метод прогрессивной дистилляции знаний от существующих мультимодальных языковых моделей (MLLM) к линейным архитектурам без использования предобученных RNN-моделей или энкодеров изображений. Предложенный подход включает стратегию инициализации Mamba из обученного Transformer и трехэтапный рецепт дистилляции. Результаты показывают, что mmMamba достигает конкурентоспособной производительности по сравнению с существующими моделями, обеспечивая значительное ускорение и экономию памяти.'}, 'en': {'title': 'Efficient Multimodal Models with mmMamba', 'desc': 'The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment.'}, 'zh': {'title': 'mmMamba：高效的多模态模型架构', 'desc': '最近的多模态大型语言模型（MLLMs）在性能上取得了显著进展，但由于其计算复杂度呈平方增长、对键值缓存的需求增加以及依赖于独立的视觉编码器，面临部署挑战。我们提出了mmMamba框架，通过从现有的MLLMs进行渐进蒸馏，开发线性复杂度的本地多模态状态空间模型，使用适度的学术计算资源。该方法允许将训练好的仅解码器MLLMs直接转换为线性复杂度架构，而无需预训练的基于RNN的LLM或视觉编码器。我们的蒸馏策略有效地将知识从Transformer转移到Mamba，同时保留多模态能力，并支持灵活的混合架构，以实现可定制的效率与性能权衡。'}}}, {'id': 'https://huggingface.co/papers/2502.13143', 'title': 'SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation', 'url': 'https://huggingface.co/papers/2502.13143', 'abstract': "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.", 'score': 26, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '7aab95a55286d8b2', 'authors': ['Zekun Qi', 'Wenyao Zhang', 'Yufei Ding', 'Runpei Dong', 'Xinqiang Yu', 'Jingwen Li', 'Lingyun Xu', 'Baoyu Li', 'Xialin He', 'Guofan Fan', 'Jiazhao Zhang', 'Jiawei He', 'Jiayuan Gu', 'Xin Jin', 'Kaisheng Ma', 'Zhizheng Zhang', 'He Wang', 'Li Yi'], 'affiliations': ['Eastern Institute of Technology', 'Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'ShanghaiTech University', 'Tsinghua University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.13143.jpg', 'data': {'categories': ['#3d', '#games', '#dataset', '#reasoning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Семантическая ориентация: новый подход к пространственному интеллекту роботов', 'desc': 'Статья представляет концепцию семантической ориентации для улучшения пространственного интеллекта роботов. Авторы создали датасет OrienText300K с 3D моделями, аннотированными семантическими ориентациями на естественном языке. Интеграция семантической ориентации в систему с визуально-языковой моделью (VLM) позволяет роботам генерировать действия с учетом позиционных и ориентационных ограничений. Эксперименты показали значительное улучшение возможностей роботов в манипуляциях с объектами.'}, 'en': {'title': 'Empowering Robots with Semantic Orientation for Enhanced Manipulation', 'desc': 'This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation.'}, 'zh': {'title': '用自然语言提升机器人的空间智能', 'desc': '空间智能是具身人工智能的重要组成部分，帮助机器人理解和与环境互动。尽管最近的进展提高了视觉语言模型（VLMs）对物体位置和关系的感知能力，但它们仍然缺乏精确理解物体方向的能力，这对于细致操作任务至关重要。为了解决这个问题，我们提出了语义方向的概念，使用自然语言以无参考框架的方式定义物体方向。通过构建OrienText300K数据集，我们将几何理解与功能语义联系起来，从而提升机器人在操作中的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.13130', 'title': 'Magma: A Foundation Model for Multimodal AI Agents', 'url': 'https://huggingface.co/papers/2502.13130', 'abstract': 'We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.', 'score': 25, 'issue_id': 2288, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '1851b242ac65c88f', 'authors': ['Jianwei Yang', 'Reuben Tan', 'Qianhui Wu', 'Ruijie Zheng', 'Baolin Peng', 'Yongyuan Liang', 'Yu Gu', 'Mu Cai', 'Seonghyeon Ye', 'Joel Jang', 'Yuquan Deng', 'Lars Liden', 'Jianfeng Gao'], 'affiliations': ['KAIST', 'Microsoft Research', 'University of Maryland', 'University of Washington', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2502.13130.jpg', 'data': {'categories': ['#cv', '#robotics', '#agi', '#multimodal', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Magma: мультимодальный ИИ-агент для цифрового и физического мира', 'desc': 'Статья представляет Magma - новую мультимодальную модель искусственного интеллекта, способную выполнять агентные задачи как в цифровом, так и в физическом мире. Модель расширяет возможности существующих vision-language моделей, добавляя способность планировать и действовать в визуально-пространственном мире. Magma обучена на больших гетерогенных наборах данных, включая изображения, видео и робототехнические данные, с использованием специальных методов разметки Set-of-Mark и Trace-of-Mark. Эксперименты показывают, что Magma достигает новых передовых результатов в задачах навигации по пользовательскому интерфейсу и манипуляции роботами.'}, 'en': {'title': 'Magma: Bridging Digital and Physical Worlds with Multimodal Intelligence', 'desc': 'Magma is a foundation model designed for multimodal AI tasks that operate in both digital and physical environments. It enhances traditional vision-language models by integrating spatial-temporal intelligence, allowing it to plan and execute actions in real-world scenarios. The model is pretrained on diverse datasets, utilizing Set-of-Mark (SoM) for action grounding and Trace-of-Mark (ToM) for action planning, which together improve its ability to understand and interact with visual-spatial elements. Magma achieves state-of-the-art performance in UI navigation and robotic manipulation, surpassing specialized models and competing effectively with larger multimodal models.'}, 'zh': {'title': 'Magma：多模态智能的未来', 'desc': 'Magma是一个基础模型，能够处理数字和物理世界中的多模态人工智能任务。它不仅保留了视觉-语言模型的理解能力，还具备在视觉空间中规划和行动的能力。Magma通过大量异构数据集进行预训练，这些数据集包括图像、视频和机器人数据，使用Set-of-Mark和Trace-of-Mark进行动作标定。实验表明，Magma在用户界面导航和机器人操作任务上创造了新的最先进结果，超越了专门为这些任务设计的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.12464', 'title': 'SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models', 'url': 'https://huggingface.co/papers/2502.12464', 'abstract': 'Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model\'s capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.', 'score': 25, 'issue_id': 2288, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '5edcf5af6c8edecb', 'authors': ['Seanie Lee', 'Dong Bok Lee', 'Dominik Wagner', 'Minki Kang', 'Haebin Seong', 'Tobias Bocklet', 'Juho Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'Technische Hochschule Nürnberg Georg Simon Ohm'], 'pdf_title_img': 'assets/pdf/title_img/2502.12464.jpg', 'data': {'categories': ['#security', '#benchmark', '#training', '#inference', '#optimization'], 'emoji': '🛡️', 'ru': {'title': 'Умная маршрутизация для эффективной защиты языковых моделей', 'desc': "Статья предлагает метод SafeRoute для повышения эффективности моделей безопасности в крупных языковых моделях (LLM). SafeRoute использует бинарный маршрутизатор для разделения входных данных на 'простые' и 'сложные'. Сложные примеры обрабатываются большой моделью безопасности, а простые - меньшей дистиллированной моделью. Этот подход позволяет сохранить точность крупной модели при значительном снижении вычислительных затрат. Эксперименты на нескольких наборах данных показывают преимущество SafeRoute перед базовыми методами."}, 'en': {'title': 'Smart Routing for Safer AI: Balancing Efficiency and Accuracy', 'desc': "This paper introduces SafeRoute, a binary router designed to improve the efficiency of safety guard models used with large language models (LLMs). The router identifies which inputs are 'hard' and require the computational power of a larger safety model, while 'easy' inputs can be handled by smaller, more efficient models. By selectively applying the larger model only to challenging cases, SafeRoute reduces overall computational costs without sacrificing safety performance. Experimental results show that this adaptive approach significantly enhances the balance between resource usage and accuracy compared to using only the larger model."}, 'zh': {'title': '智能路由，提升安全与效率！', 'desc': '在实际应用中部署大型语言模型（LLMs）需要强大的安全防护模型来检测和阻止有害的用户提示。虽然大型安全防护模型的性能很强，但其计算成本也很高。为了解决这个问题，研究者们使用了较小的蒸馏模型，但在处理“困难”示例时，它们的表现往往不如大型模型。我们提出了SafeRoute，一个二元路由器，可以区分困难示例和简单示例，从而提高效率，同时保持准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.09245', 'title': "You Do Not Fully Utilize Transformer's Representation Capacity", 'url': 'https://huggingface.co/papers/2502.09245', 'abstract': "In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that preserves the model's overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research.", 'score': 24, 'issue_id': 2291, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '8526a2bbb83d3754', 'authors': ['Gleb Gerasimov', 'Yaroslav Aksenov', 'Nikita Balagansky', 'Viacheslav Sinii', 'Daniil Gavrilov'], 'affiliations': ['HSE University', 'Moscow Institute of Physics and Technology', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.09245.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'LIMe: расширение памяти трансформеров для лучшей производительности', 'desc': 'Статья представляет новый подход к архитектуре трансформеров под названием Layer-Integrated Memory (LIMe). В отличие от стандартных трансформеров, которые используют только представления из предыдущего слоя, LIMe позволяет обращаться к скрытым состояниям из более ранних слоев. Это решение помогает избежать проблемы схлопывания представлений и улучшает производительность модели. Эксперименты показали, что LIMe последовательно улучшает результаты на широком спектре задач без увеличения общего объема памяти модели.'}, 'en': {'title': 'Unlocking Transformer Potential with Layer-Integrated Memory', 'desc': "This paper discusses the limitations of standard Transformers, which only utilize information from the most recent layer, leading to representation collapse and reduced performance. The authors propose a new method called Layer-Integrated Memory (LIMe) that allows access to hidden states from earlier layers, enhancing the model's representational capacity without increasing memory usage. Through various experiments, they show that LIMe consistently improves performance across different tasks and architectures. Additionally, the paper explores how LIMe integrates information across layers, suggesting new avenues for future research in model design."}, 'zh': {'title': '层集成记忆：提升变换器性能的新方法', 'desc': '与递归神经网络（RNN）不同，变换器（Transformers）可以直接关注所有之前的标记。然而，标准的变换器仅使用来自前一层的表示，这种设计选择导致了表示崩溃，从而影响了性能。为了解决这个问题，我们提出了一种名为层集成记忆（LIMe）的方法，它在保持模型整体内存占用的同时，扩展了表示能力，允许访问早期层的隐藏状态。通过在各种架构和不同查找机制上的广泛实验，我们展示了在多种任务上的一致性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.11433', 'title': 'FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading', 'url': 'https://huggingface.co/papers/2502.11433', 'abstract': 'Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.', 'score': 22, 'issue_id': 2286, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '0731051fe5131889', 'authors': ['Guojun Xiong', 'Zhiyang Deng', 'Keyi Wang', 'Yupeng Cao', 'Haohang Li', 'Yangyang Yu', 'Xueqing Peng', 'Mingquan Lin', 'Kaleb E Smith', 'Xiao-Yang Liu', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['Columbia University', 'Harvard University', 'NVIDIA', 'Rensselaer Polytechnic Institute', 'Stevens Institute of Technology', 'TheFinAI', 'University of Manchester', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2502.11433.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#architecture', '#training', '#reasoning', '#rl', '#multimodal'], 'emoji': '📈', 'ru': {'title': 'Усиление торговых стратегий с помощью языковых моделей и обучения с подкреплением', 'desc': 'В статье представлена архитектура FLAG-Trader, объединяющая языковую обработку с помощью больших языковых моделей (LLM) и обучение с подкреплением для оптимизации торговых стратегий. LLM используется в качестве политики, адаптируясь к финансовой области через эффективную дообучение. Метод улучшает принятие решений в интерактивных финансовых сценариях, таких как торговля. Эмпирические результаты подтверждают эффективность подхода не только в торговле, но и в других финансовых задачах.'}, 'en': {'title': 'Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning', 'desc': 'This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method.'}, 'zh': {'title': 'FLAG-Trader：提升金融决策的智能交易架构', 'desc': '本文提出了一种名为FLAG-Trader的统一架构，旨在提升大型语言模型（LLMs）在金融市场中的决策能力。该架构结合了语言处理和基于梯度的强化学习（RL）策略优化，使得部分微调的LLM可以作为策略网络，利用预训练知识并适应金融领域。通过交易奖励驱动的策略梯度优化，FLAG-Trader不仅提高了LLM在交易中的表现，还改善了其他金融领域任务的结果。我们提供了大量实证证据来验证这些改进。'}}}, {'id': 'https://huggingface.co/papers/2502.12513', 'title': 'RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm', 'url': 'https://huggingface.co/papers/2502.12513', 'abstract': 'After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.', 'score': 14, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '1ed14365f683281c', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Chaoyi Zhang', 'Yin Xie', 'Xiang An', 'Ziyong Feng', 'Dongnan Liu', 'Weidong Cai', 'Jiankang Deng'], 'affiliations': ['DeepGlint', 'Imperial College London', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2502.12513.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#data', '#synthetic', '#dataset', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'RealSyn: Улучшение мультимодального обучения с помощью реальных и синтетических данных', 'desc': 'Статья представляет новый подход к обучению мультимодальных моделей, использующих изображения и текст. Авторы разработали метод RealSyn, который извлекает высококачественные данные из непарных документов и создает синтетические тексты для улучшения визуальной информации. Они также применили стратегию семантически сбалансированной выборки для повышения разнообразия данных. Эксперименты показали, что модели, предобученные на датасете RealSyn, достигают наилучших результатов в различных задачах компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Unlocking Vision-Language Learning with RealSyn Dataset', 'desc': 'This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning.'}, 'zh': {'title': '利用未配对数据提升视觉-语言学习的创新方法', 'desc': '这篇论文介绍了一种新的数据集RealSyn，用于视觉-语言表示学习。研究者们通过提取高质量的图像和文本，利用未配对的数据来提升模型的性能。为了更好地关联图像和文本，设计了一种层次检索方法，并提出了图像语义增强生成模块来生成合成文本。实验结果表明，基于RealSyn预训练的模型在多个下游任务上达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.11271', 'title': 'OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning', 'url': 'https://huggingface.co/papers/2502.11271', 'abstract': "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.", 'score': 10, 'issue_id': 2291, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '1477460cf6641c67', 'authors': ['Pan Lu', 'Bowen Chen', 'Sheng Liu', 'Rahul Thapa', 'Joseph Boen', 'James Zou'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11271.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#open_source', '#training'], 'emoji': '🐙', 'ru': {'title': 'OctoTools: универсальный агент для сложных рассуждений с помощью ИИ', 'desc': 'Статья представляет OctoTools - универсальный фреймворк для решения сложных задач рассуждения в различных областях. OctoTools использует стандартизированные карточки инструментов, планировщик для высокоуровневого и низкоуровневого планирования, и исполнитель для применения инструментов. Фреймворк показывает значительное повышение точности на 9.3% по сравнению с GPT-4 на 16 разнообразных задачах. OctoTools также превосходит другие подходы, такие как AutoGen и LangChain, демонстрируя преимущества в планировании задач и эффективном использовании инструментов.'}, 'en': {'title': 'OctoTools: Empowering LLMs for Complex Reasoning Tasks', 'desc': 'This paper presents OctoTools, an innovative framework designed to enhance large language models (LLMs) in solving complex reasoning tasks without the need for additional training. OctoTools features standardized tool cards that define the capabilities of various tools, a planner for organizing tasks, and an executor to implement the planned actions. The framework has been tested across 16 different tasks, showing an impressive average accuracy improvement of 9.3% compared to GPT-4o. Additionally, OctoTools outperforms other existing methods like AutoGen and LangChain by up to 10.6%, highlighting its effectiveness in multi-step reasoning and tool utilization.'}, 'zh': {'title': 'OctoTools：跨领域复杂推理的新解决方案', 'desc': '本论文介绍了一种名为OctoTools的开源框架，旨在解决复杂推理任务。OctoTools不需要额外的训练数据，用户友好且易于扩展，能够在多个领域中应用。它通过标准化的工具卡片来封装工具功能，并提供高层次和低层次的规划器以及执行器来执行工具使用。实验结果表明，OctoTools在16个不同任务上相较于GPT-4o平均提高了9.3%的准确率，并在相同工具集下超越了其他方法。'}}}, {'id': 'https://huggingface.co/papers/2502.12859', 'title': 'PAFT: Prompt-Agnostic Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.12859', 'abstract': 'While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.', 'score': 10, 'issue_id': 2290, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'f5398572cb7bbb47', 'authors': ['Chenxing Wei', 'Yao Shu', 'Mingwen Ou', 'Ying Tiffany He', 'Fei Richard Yu'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University, China', 'Guangdong Lab of AI and Digital Economy (SZ), China', 'School of Information Technology, Carleton University, Canada', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12859.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#synthetic', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Устойчивость к промптам: новый подход к тонкой настройке языковых моделей', 'desc': 'Эта статья представляет новый метод тонкой настройки больших языковых моделей (LLM), называемый Prompt-Agnostic Fine-Tuning (PAFT). PAFT решает проблему снижения производительности модели при изменении промптов, используя динамическую корректировку промптов во время обучения. Метод включает создание разнообразного набора синтетических промптов и случайный выбор из них во время тонкой настройки. Эксперименты показывают, что PAFT повышает устойчивость модели к различным промптам, улучшая производительность и скорость вывода.'}, 'en': {'title': 'Enhancing Robustness in LLMs with Dynamic Prompting', 'desc': 'This paper introduces Prompt-Agnostic Fine-Tuning (PAFT), a method designed to improve the robustness of Large Language Models (LLMs) when adapting to various tasks. PAFT works by dynamically adjusting prompts during the fine-tuning process, which helps the model focus on the core principles of the tasks instead of memorizing specific prompt formats. The approach involves creating a diverse set of synthetic prompts and randomly sampling from them during training, leading to better generalization and performance on unseen prompts. Experimental results show that PAFT enhances both the robustness and inference speed of LLMs while keeping training efficient.'}, 'zh': {'title': '提升模型鲁棒性与泛化能力的提示无关微调', 'desc': '大型语言模型（LLMs）在微调后能够很好地适应下游任务，但这种适应性往往会影响提示的鲁棒性，因为即使是微小的提示变化也会显著降低性能。为了解决这个问题，我们提出了一种简单而有效的方法——提示无关微调（PAFT），该方法在微调过程中动态调整提示。PAFT的操作分为两个阶段：首先，构建一组多样化且有意义的合成候选提示；其次，在微调过程中，从这组提示中随机抽取，以创建动态训练输入。通过在多种数据集和LLMs上的广泛实验，证明了使用PAFT训练的模型在各种提示（包括未见过的提示）上表现出强大的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.12170', 'title': 'MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections', 'url': 'https://huggingface.co/papers/2502.12170', 'abstract': 'We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .', 'score': 10, 'issue_id': 2287, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'f06bcb1b611b7c39', 'authors': ['Da Xiao', 'Qingye Meng', 'Shengping Li', 'Xingyuan Yuan'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'ColorfulClouds Technology Co., Ltd., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12170.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🔀', 'ru': {'title': 'Динамические связи для эффективных трансформеров', 'desc': 'Исследователи предлагают новый метод MUDD (MUltiway Dynamic Dense) для улучшения связей между слоями в архитектуре Transformer. MUDD генерирует веса связей динамически в зависимости от скрытых состояний на каждой позиции последовательности и для каждого входного потока блока Transformer. Эксперименты показывают, что MUDDFormer значительно превосходит стандартные Transformer-модели в задачах языкового моделирования, достигая производительности моделей, обученных с использованием в 1.8-2.4 раза больше вычислительных ресурсов. Примечательно, что MUDDPythia-2.8B сопоставима по эффективности с Pythia-6.9B и даже конкурирует с Pythia-12B в некоторых задачах, добавляя всего 0.23% параметров и 0.4% вычислений.'}, 'en': {'title': 'Dynamic Connections for Enhanced Transformer Performance', 'desc': 'The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count.'}, 'zh': {'title': '动态连接，提升Transformer性能！', 'desc': '我们提出了一种名为多向动态稠密连接（MUDD）的简单有效方法，旨在解决残差连接的局限性，并增强Transformer中跨层信息流动。与现有的静态共享连接权重的稠密连接方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接可以无缝集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformer，表现出与训练时计算量为1.8X-2.4X的Transformer相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.12215', 'title': 'Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?', 'url': 'https://huggingface.co/papers/2502.12215', 'abstract': "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.", 'score': 9, 'issue_id': 2288, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'a02df3e32ba854de', 'authors': ['Zhiyuan Zeng', 'Qinyuan Cheng', 'Zhangyue Yin', 'Yunhua Zhou', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University, Shanghai, China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2502.12215.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Короче - лучше: новый взгляд на масштабирование языковых моделей', 'desc': 'Это исследование посвящено масштабированию во время вывода в больших языковых моделях (LLM). Авторы обнаружили, что более длинные цепочки рассуждений не всегда повышают точность, и часто правильные решения короче неправильных. Исследование показывает, что это связано со способностью моделей к самокоррекции, которая может ухудшать производительность. На основе этих наблюдений предложен метод Shortest Majority Vote, сочетающий параллельное масштабирование со свойствами длины цепочек рассуждений.'}, 'en': {'title': 'Enhancing LLM Performance with Shorter, Smarter Reasoning', 'desc': 'This paper investigates the concept of test-time scaling in large language models (LLMs), particularly focusing on the o1 series by OpenAI and its successors. It reveals that longer chains of thought (CoTs) do not always lead to better accuracy, as shorter CoTs can yield correct answers more frequently. The study highlights that the presence of self-revisions in longer CoTs can negatively impact performance. To enhance test-time scalability, the authors propose a new method called Shortest Majority Vote, which integrates parallel scaling strategies with CoT length characteristics, outperforming traditional majority voting methods.'}, 'zh': {'title': '提升模型推理能力的新方法', 'desc': '本文探讨了大型语言模型（LLMs）在推理时的测试时间缩放能力，特别是OpenAI的o1系列。研究发现，虽然一些后续模型如QwQ和Deepseek-R1模仿了这些进展，但它们的测试时间缩放能力仍未得到充分验证。更长的链式思维（CoT）并不总是提高准确性，反而正确答案往往比错误答案更短。我们提出了一种新的方法——最短多数投票，结合并行缩放策略和CoT长度特征，显著提升了模型的测试时间可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2502.13092', 'title': 'Text2World: Benchmarking Large Language Models for Symbolic World Model Generation', 'url': 'https://huggingface.co/papers/2502.13092', 'abstract': 'Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.', 'score': 8, 'issue_id': 2296, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'f5365c5f8afa57df', 'authors': ['Mengkang Hu', 'Tianxing Chen', 'Yude Zou', 'Yuheng Lei', 'Qiguang Chen', 'Ming Li', 'Hongyuan Zhang', 'Wenqi Shao', 'Ping Luo'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Shenzhen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.13092.jpg', 'data': {'categories': ['#rl', '#games', '#reasoning', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Text2World: новый бенчмарк для оценки LLM в моделировании мира', 'desc': 'Статья представляет новый бенчмарк Text2World для оценки способностей больших языковых моделей (LLM) генерировать символические модели мира из текстовых описаний. Бенчмарк основан на языке определения доменов планирования (PDDL) и включает сотни разнообразных доменов. Авторы оценивают современные LLM с помощью Text2World и обнаруживают, что модели рассуждений, обученные с помощью масштабного обучения с подкреплением, превосходят другие. Исследование также рассматривает стратегии улучшения возможностей моделирования мира для LLM.'}, 'en': {'title': 'Enhancing World Modeling with Text2World Benchmark', 'desc': 'This paper discusses the use of large language models (LLMs) to create symbolic representations of worlds from text descriptions. It identifies challenges in previous research, such as evaluation randomness and limited domain coverage. To overcome these issues, the authors introduce a new benchmark called Text2World, which uses planning domain definition language (PDDL) and offers a variety of evaluation metrics. The study finds that while LLMs trained with reinforcement learning show promise, they still struggle with world modeling, prompting the exploration of strategies to improve their capabilities.'}, 'zh': {'title': '利用LLMs提升世界建模能力的探索', 'desc': '最近，利用大型语言模型（LLMs）从文本描述生成符号世界模型的兴趣日益增长。尽管在世界建模方面对LLMs进行了广泛研究，但之前的研究面临评估随机性、依赖间接指标和领域范围有限等挑战。为了解决这些问题，我们引入了一个新的基准测试Text2World，基于规划领域定义语言（PDDL），涵盖数百个多样化的领域，并采用多标准、基于执行的指标进行更稳健的评估。我们使用Text2World对当前的LLMs进行基准测试，发现经过大规模强化学习训练的推理模型表现优于其他模型，但即使是表现最好的模型在世界建模能力上仍然有限。'}}}, {'id': 'https://huggingface.co/papers/2502.12996', 'title': 'Eager Updates For Overlapped Communication and Computation in DiLoCo', 'url': 'https://huggingface.co/papers/2502.12996', 'abstract': 'Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an inner optimization phase, where the workers independently execute multiple optimization steps on their own local data, and an outer optimization step, where the inner updates are synchronized. While such approaches require orders of magnitude less communication than standard data-parallel training, in settings where the workers are datacenters, even the limited communication requirements of these approaches can still cause significant slow downs due to the blocking necessary at each outer optimization step. In this paper, we investigate techniques to mitigate this issue by overlapping communication with computation in a manner that allows the outer optimization step to fully overlap with the inner optimization phase. We show that a particular variant, dubbed eager updates, provides competitive performance with standard DiLoCo in settings with low bandwidth between workers.', 'score': 7, 'issue_id': 2295, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '61c2807bd3fe3a67', 'authors': ['Satyen Kale', 'Arthur Douillard', 'Yanislav Donchev'], 'affiliations': ['Apple', 'Google DeepMind', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.12996.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение распределенного обучения с помощью перекрытия коммуникации и вычислений', 'desc': "В статье рассматриваются методы распределенной оптимизации для обучения очень больших моделей на нескольких распределенных узлах, таких как дата-центры. Описывается подход DiLoCo, который разделяет обновления на внутреннюю фазу оптимизации на локальных данных и внешний шаг синхронизации. Авторы предлагают технику под названием 'eager updates' для перекрытия коммуникации и вычислений, что позволяет полностью совместить внешний шаг оптимизации с внутренней фазой. Показано, что этот метод обеспечивает конкурентоспособную производительность по сравнению со стандартным DiLoCo в условиях низкой пропускной способности между узлами."}, 'en': {'title': 'Enhancing Distributed Training with Eager Updates', 'desc': "This paper explores distributed optimization methods, specifically DiLoCo, which are used to train large machine learning models across multiple workers in datacenters. The process involves an inner optimization phase where workers perform local updates and an outer phase for synchronizing these updates. However, the communication required during the outer phase can slow down training, even with reduced communication needs. The authors propose a solution that overlaps communication with computation, introducing 'eager updates' to enhance performance in low bandwidth scenarios while maintaining competitive results with traditional DiLoCo methods."}, 'zh': {'title': '重叠通信与计算，提升分布式优化效率', 'desc': '本文探讨了一种分布式优化方法，称为DiLoCo，适用于在多个分布式工作者（如数据中心）上训练大型模型。这种方法将更新分为两个部分：内部优化阶段和外部优化阶段，工作者在本地数据上独立执行多个优化步骤。尽管这种方法比标准的数据并行训练需要更少的通信，但在数据中心环境中，外部优化步骤的阻塞仍可能导致显著的延迟。我们提出了一种技术，通过重叠通信与计算，使外部优化步骤与内部优化阶段完全重叠，从而提高性能。'}}}, {'id': 'https://huggingface.co/papers/2502.09838', 'title': 'HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation', 'url': 'https://huggingface.co/papers/2502.09838', 'abstract': 'We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.', 'score': 7, 'issue_id': 2287, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': 'ce920c7d40d11dad', 'authors': ['Tianwei Lin', 'Wenqiao Zhang', 'Sijing Li', 'Yuqian Yuan', 'Binhe Yu', 'Haoyuan Li', 'Wanggui He', 'Hao Jiang', 'Mengze Li', 'Xiaohui Song', 'Siliang Tang', 'Jun Xiao', 'Hui Lin', 'Yueting Zhuang', 'Beng Chin Ooi'], 'affiliations': ['Alibaba', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09838.jpg', 'data': {'categories': ['#data', '#training', '#cv', '#dataset', '#healthcare'], 'emoji': '🏥', 'ru': {'title': 'HealthGPT: Единая модель для понимания и генерации медицинских изображений', 'desc': 'HealthGPT - это мощная мультимодальная модель для медицинской визуальной обработки и генерации. Она использует новую технику адаптации H-LoRA и иерархический подход к визуальному восприятию. Модель обучена на специально созданном наборе данных VL-Health. HealthGPT демонстрирует исключительную производительность в медицинских визуальных задачах.'}, 'en': {'title': 'Revolutionizing Medical AI with HealthGPT', 'desc': 'HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability.'}, 'zh': {'title': 'HealthGPT：医疗视觉语言模型的创新整合', 'desc': '我们提出了HealthGPT，这是一种强大的医疗大型视觉语言模型（Med-LVLM），它将医疗视觉理解和生成能力整合在一个统一的自回归框架中。我们的自举理念是逐步将异构的理解和生成知识适应于预训练的大型语言模型（LLMs）。这通过一种新颖的异构低秩适应（H-LoRA）技术实现，并辅以定制的分层视觉感知方法和三阶段学习策略。实验结果表明，HealthGPT在医疗视觉统一任务中表现出色，具有良好的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.', 'score': 6, 'issue_id': 2295, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'acefee7f3111548b', 'authors': ['Fengwei Teng', 'Zhaoyang Yu', 'Quan Shi', 'Jiayi Zhang', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12018.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Атомарное мышление: новый подход к рассуждениям языковых моделей', 'desc': "Статья представляет новый метод под названием 'Atom of Thoughts' (AoT) для улучшения рассуждений больших языковых моделей (LLM) во время вывода. AoT разбивает сложные вопросы на атомарные подвопросы, организованные в направленный ациклический граф, что позволяет эффективно использовать вычислительные ресурсы и избегать накопления избыточной информации. Метод может быть интегрирован в существующие подходы к масштабированию во время вывода, значительно улучшая их производительность. Эксперименты на шести бенчмарках показали эффективность AoT как самостоятельного фреймворка и как дополнения к другим методам."}, 'en': {'title': 'Enhancing Reasoning in LLMs with Atomic Question Decomposition', 'desc': 'This paper introduces Atom of Thoughts (AoT), a method designed to improve reasoning in Large Language Models (LLMs) during inference. AoT addresses the problem of accumulated historical information in existing test-time scaling methods, which can hinder effective reasoning and waste computational resources. By breaking down complex questions into independent subquestions, AoT allows for a more efficient reasoning process that resembles memoryless transitions in a Markov process. The proposed method not only enhances reasoning capabilities but also integrates well with existing frameworks, showing significant performance improvements in benchmark tests.'}, 'zh': {'title': '思维原子的力量：提升推理能力的创新方法', 'desc': '大型语言模型（LLMs）通过扩展训练规模和测试规模来提高性能。在推理过程中，现有的测试时间扩展方法由于历史信息的累积，导致计算资源浪费和推理效果干扰。为了解决这个问题，我们提出了“思维原子”（Atom of Thoughts，AoT），通过将当前问题分解为依赖关系的有向无环图，并收缩其子问题，形成新的原子问题状态。这种迭代的分解-收缩过程实现了问题状态之间的马尔可夫转移，并可以与现有的测试时间扩展方法无缝集成。'}}}, {'id': 'https://huggingface.co/papers/2502.12574', 'title': 'HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading', 'url': 'https://huggingface.co/papers/2502.12574', 'abstract': 'Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.', 'score': 6, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '6e5de7c584198857', 'authors': ['Cheng Luo', 'Zefan Cai', 'Hanshi Sun', 'Jinqi Xiao', 'Bo Yuan', 'Wen Xiao', 'Junjie Hu', 'Jiawei Zhao', 'Beidi Chen', 'Anima Anandkumar'], 'affiliations': ['California Institute of Technology', 'Carnegie Mellon University', 'Microsoft', 'Rutgers University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2502.12574.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'HEADINFER: эффективное использование памяти для LLM с длинным контекстом', 'desc': 'HEADINFER - это новый метод для оптимизации памяти при работе с большими языковыми моделями (LLM). Он позволяет выгружать кэш ключей-значений (KV cache) в оперативную память CPU, значительно снижая нагрузку на GPU. Используя стратегию выборочного хранения данных для отдельных голов внимания, HEADINFER сохраняет вычислительную эффективность при существенном уменьшении объема используемой памяти. Метод был успешно протестирован на модели Llama-3-8B с последовательностью в 1 миллион токенов, сократив использование памяти GPU на 92% по сравнению с базовым методом.'}, 'en': {'title': 'HEADINFER: Efficient Memory Management for Long Contexts in LLMs', 'desc': 'This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance.'}, 'zh': {'title': 'HEADINFER：优化大语言模型的内存使用', 'desc': '本文提出了一种名为HEADINFER的方法，旨在优化大语言模型（LLM）在长上下文生成中的内存使用。通过将关键值缓存（KV缓存）转移到CPU RAM，HEADINFER避免了在GPU上完全存储KV缓存的需求。该方法采用细粒度的头部级别卸载策略，仅在GPU上保留选择性的注意力头KV缓存，同时动态计算注意力输出。实验结果表明，HEADINFER在显著减少内存占用的同时，保持了计算效率，使得在单个消费级GPU上实现了对长达400万标记的推理。'}}}, {'id': 'https://huggingface.co/papers/2502.12501', 'title': 'Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge', 'url': 'https://huggingface.co/papers/2502.12501', 'abstract': "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.", 'score': 5, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '1c26233f72f504ee', 'authors': ['Qiyuan Zhang', 'Yufei Wang', 'Yuxin Jiang', 'Liangyou Li', 'Chuhan Wu', 'Yasheng Wang', 'Xin Jiang', 'Lifeng Shang', 'Ruiming Tang', 'Fuyuan Lyu', 'Chen Ma'], 'affiliations': ['City University of Hong Kong', 'Huawei Noahs Ark Lab', 'McGill University & MILA', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12501.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#inference', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение оценки ИИ через сравнение с мнением толпы', 'desc': "Статья представляет новый метод оценки моделей машинного обучения под названием 'Crowd-based Comparative Evaluation'. Этот подход улучшает традиционный метод LLM-as-a-Judge, добавляя сравнение с ответами толпы для более глубокого анализа. Эксперименты показали повышение точности оценки в среднем на 6.7% на пяти бенчмарках. Метод также демонстрирует улучшенное качество рассуждений (Chain-of-Thought) и эффективность в отборе данных для обучения с учителем."}, 'en': {'title': 'Enhancing LLM Evaluations with Crowd Insights', 'desc': 'This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning.'}, 'zh': {'title': '提升LLM评估可靠性的创新方法', 'desc': '本文提出了一种新的评估方法，称为基于人群的比较评估（Crowd-based Comparative Evaluation），旨在提高大型语言模型（LLM）作为评判者的可靠性。传统的链式推理（CoT）方法常常无法捕捉到全面和深入的细节，导致评估结果不完整。我们的方法通过引入额外的人群反馈，与候选响应进行比较，从而揭示候选响应中的更深层次和更全面的细节。实验结果表明，该方法在五个基准测试中平均提高了6.7%的评估准确性，并生成了更高质量的CoT，促进了监督微调的效率。'}}}, {'id': 'https://huggingface.co/papers/2502.12929', 'title': 'Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options', 'url': 'https://huggingface.co/papers/2502.12929', 'abstract': 'We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.', 'score': 4, 'issue_id': 2296, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '7cde53c7aa78c7ed', 'authors': ['Lakshmi Nair', 'Ian Trase', 'Mark Kim'], 'affiliations': ['Flagship Pioneering, Cambridge, MA 02142, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.12929.jpg', 'data': {'categories': ['#interpretability', '#training', '#rl', '#reasoning', '#cv', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Революция в автоматическом машинном обучении: FoO преодолевает предубеждения ИИ', 'desc': 'Представлен новый подход к рассуждениям под названием Flow-of-Options (FoO), предназначенный для устранения внутренних предубеждений в больших языковых моделях (LLM). FoO позволяет LLM систематически исследовать разнообразные возможности в своих рассуждениях, что демонстрируется агентной системой на основе FoO для автономного решения задач машинного обучения (AutoML). Этот фреймворк превосходит современные базовые модели, достигая улучшений на 38.2% - 69.2% в стандартных задачах анализа данных и на 37.4% - 47.9% в задачах терапевтической химии. Система применима к широкому спектру задач, включая обучение с подкреплением и генерацию изображений.'}, 'en': {'title': 'Unlocking Diverse Reasoning in Large Language Models with Flow-of-Options', 'desc': 'The paper introduces a new reasoning method called Flow-of-Options (FoO) that helps Large Language Models (LLMs) overcome their inherent biases. FoO allows these models to explore a wide variety of reasoning paths, which is particularly useful for automating machine learning tasks (AutoML). The proposed framework shows significant performance improvements over existing methods, achieving better results in both data science and therapeutic chemistry tasks while maintaining low operational costs. Additionally, FoO enhances the versatility of LLMs, enabling them to tackle not just classification and regression, but also reinforcement learning and image generation tasks.'}, 'zh': {'title': '选项流：提升大型语言模型推理的多样性与性能', 'desc': '我们提出了一种新的推理方法，称为选项流（Flow-of-Options，FoO），旨在解决大型语言模型（LLMs）中的内在偏见。FoO使得LLMs能够系统地探索多种推理可能性，特别是在自动化机器学习（AutoML）任务中表现出色。我们的框架在标准数据科学任务上提高了38.2%到69.2%的性能，在治疗化学任务上提高了37.4%到47.9%。此外，FoO还适用于强化学习和图像生成等更广泛的任务，展现了其在多样性和可解释性方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2502.10708', 'title': 'Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2502.10708', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to documenting research in the field of specialized LLM.', 'score': 3, 'issue_id': 2291, 'pub_date': '2025-02-15', 'pub_date_card': {'ru': '15 февраля', 'en': 'February 15', 'zh': '2月15日'}, 'hash': '01a93665c8e86d8d', 'authors': ['Zirui Song', 'Bin Yan', 'Yuhan Liu', 'Miao Fang', 'Mingzhe Li', 'Rui Yan', 'Xiuying Chen'], 'affiliations': ['ByteDance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10708.jpg', 'data': {'categories': ['#survey', '#dataset', '#healthcare', '#multilingual', '#benchmark', '#machine_translation', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Усиление LLM доменными знаниями: ключ к специализированным задачам', 'desc': 'Статья представляет обзор методов улучшения больших языковых моделей (LLM) для специализированных задач путем внедрения доменных знаний. Авторы выделяют четыре основных подхода: динамическое внедрение знаний, статическое встраивание знаний, модульные адаптеры и оптимизация промптов. В работе сравниваются преимущества и недостатки каждого метода, а также оценивается эффективность доменно-специфичных LLM по сравнению с общими моделями. Исследователи также обсуждают вызовы и возможности в этой развивающейся области и предоставляют информацию о наборах данных и бенчмарках для оценки моделей.'}, 'en': {'title': 'Enhancing LLMs with Domain-Specific Knowledge', 'desc': 'This paper surveys methods to enhance Large Language Models (LLMs) for specialized tasks in fields like healthcare and law. It categorizes these methods into four approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each method aims to integrate domain-specific knowledge into LLMs, improving their performance while considering trade-offs in flexibility and efficiency. The paper also evaluates the effectiveness of these specialized LLMs compared to general-purpose models and discusses the challenges and opportunities in this area.'}, 'zh': {'title': '增强大型语言模型的领域知识', 'desc': '大型语言模型（LLMs）在自然语言理解、文本摘要和机器翻译等任务中表现出色，但在需要专业知识的领域应用中效果有限。为了解决这个问题，研究人员探索了多种方法来增强LLMs，主要包括动态知识注入、静态知识嵌入、模块适配器和提示优化。每种方法都有其独特的机制，旨在为LLMs提供领域专业知识，同时在灵活性、可扩展性和效率之间进行权衡。本文综述了这些方法的优缺点，并讨论了领域特定LLMs与通用LLMs的比较，以及该领域面临的挑战和机遇。'}}}, {'id': 'https://huggingface.co/papers/2502.12669', 'title': 'Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research', 'url': 'https://huggingface.co/papers/2502.12669', 'abstract': 'The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research.', 'score': 2, 'issue_id': 2291, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'cb1e20aec1d7e12c', 'authors': ['Xiang Liu', 'Penglei Sun', 'Shuyan Chen', 'Longhan Zhang', 'Peijie Dong', 'Huajie You', 'Yongqi Zhang', 'Chang Yan', 'Xiaowen Chu', 'Tong-yi Zhang'], 'affiliations': ['Guangzhou Municipal Key Laboratory of Materials Informatics', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12669.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#science', '#architecture', '#multimodal', '#training', '#graphs'], 'emoji': '☀️', 'ru': {'title': 'Революция в исследованиях перовскитных солнечных элементов с помощью ИИ', 'desc': 'Статья представляет комплексную систему для исследований перовскитных солнечных элементов (PSC), включающую три ключевых компонента. Первый - это Perovskite-KG, предметно-ориентированный граф знаний, построенный на основе 1517 научных статей. Второй компонент - два набора данных: Perovskite-Chat с 55101 парой вопрос-ответ и Perovskite-Reasoning с 2217 задачами по материаловедению. Третий компонент - две специализированные языковые модели: Perovskite-Chat-LLM для помощи в предметной области и Perovskite-Reasoning-LLM для задач научного рассуждения.'}, 'en': {'title': 'Empowering PSC Research with Knowledge and Reasoning Systems', 'desc': 'This paper presents a knowledge-enhanced system specifically designed for perovskite solar cells (PSCs) research. It includes a knowledge graph built from over 1,500 research papers, which organizes entities and relationships relevant to PSCs. Additionally, the authors created two datasets: one for question-answer pairs and another for materials science problems, both aimed at improving knowledge retrieval and reasoning. The system also features specialized large language models that outperform existing tools, aiding researchers in literature review and experimental design.'}, 'zh': {'title': '钙钛矿太阳能电池的知识管理与推理系统', 'desc': '这篇论文介绍了一种针对钙钛矿太阳能电池的知识增强系统。该系统包括三个主要部分：首先，构建了一个包含1517篇研究论文的领域特定知识图谱，涵盖23789个实体和22272个关系。其次，创建了两个互补的数据集，分别用于问答和材料科学问题。最后，提出了两个专门的大型语言模型，显著提高了领域知识检索和科学推理的效果。'}}}, {'id': 'https://huggingface.co/papers/2502.13142', 'title': 'Pre-training Auto-regressive Robotic Models with 4D Representations', 'url': 'https://huggingface.co/papers/2502.13142', 'abstract': 'Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.', 'score': 2, 'issue_id': 2290, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '3c2f3b3e998c6c95', 'authors': ['Dantong Niu', 'Yuvan Sharma', 'Haoru Xue', 'Giscard Biamby', 'Junyi Zhang', 'Ziteng Ji', 'Trevor Darrell', 'Roei Herzig'], 'affiliations': ['BAIR, UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.13142.jpg', 'data': {'categories': ['#training', '#dataset', '#transfer_learning', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов на видео с людьми: революция в предобучении для робототехники', 'desc': 'Статья представляет ARM4R - авторегрессионную модель для робототехники, использующую 4D-представления, полученные из видеоданных с людьми. Модель применяет 3D-отслеживание точек из видео, поднимая 2D-представления в 3D-пространство с помощью монокулярной оценки глубины. Это позволяет эффективно переносить знания с видеоданных людей на низкоуровневое управление роботами. Эксперименты показывают, что ARM4R улучшает производительность в различных роботизированных средах и конфигурациях.'}, 'en': {'title': 'Revolutionizing Robotics with Human Video Insights', 'desc': 'This paper presents ARM4R, an Auto-regressive Robotic Model designed to enhance robotic learning by utilizing low-level 4D representations derived from human video data. By converting 2D video representations into 3D point tracking through monocular depth estimation, ARM4R captures the geometric relationships necessary for effective robotic control. The model enables efficient transfer learning, allowing robots to learn from human actions without the need for extensive annotations. Experimental results demonstrate that ARM4R significantly improves performance across diverse robotic tasks and environments.'}, 'zh': {'title': '利用视频数据提升机器人控制能力', 'desc': '本论文介绍了一种名为ARM4R的自回归机器人模型，它利用从人类视频数据中学习的低级4D表示来改进机器人的预训练模型。我们专注于利用从视频中提取的3D点跟踪表示，这些表示通过单目深度估计将2D表示提升到3D空间。4D表示在点和机器人状态表示之间保持共享的几何结构，从而实现从人类视频数据到低级机器人控制的高效迁移学习。实验结果表明，ARM4R能够有效地从人类视频数据迁移到机器人，并在各种机器人环境和配置中持续提高任务性能。'}}}, {'id': 'https://huggingface.co/papers/2502.10852', 'title': 'Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages', 'url': 'https://huggingface.co/papers/2502.10852', 'abstract': 'While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.', 'score': 2, 'issue_id': 2287, 'pub_date': '2025-02-15', 'pub_date_card': {'ru': '15 февраля', 'en': 'February 15', 'zh': '2月15日'}, 'hash': '11a782268b627ec1', 'authors': ['Zeli Su', 'Ziyin Zhang', 'Guixian Xu', 'Jianing Liu', 'XU Han', 'Ting Zhang', 'Yushuang Dong'], 'affiliations': ['Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE', 'Minzu University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10852.jpg', 'data': {'categories': ['#multilingual', '#low_resource'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языкового барьера: эффективная генерация текста для малоресурсных языков', 'desc': 'Статья представляет новый подход к адаптации многоязычных энкодеров для генерации текста на языках с крайне ограниченными ресурсами. Авторы предлагают метод повторного использования весов между энкодером и декодером, что позволяет модели эффективно использовать семантическое пространство энкодера. Применяя этот подход к четырем китайским языкам меньшинств, исследователи разработали модель XLM-SWCM. Эксперименты показали превосходные результаты XLM-SWCM на различных задачах даже в сравнении с гораздо более крупными моделями.'}, 'en': {'title': 'Empowering Low-Resource Languages with Efficient Multilingual Models', 'desc': 'This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks.'}, 'zh': {'title': '为极低资源语言赋能的多语言文本生成框架', 'desc': '多语言模型如XLM-R在自然语言处理中的多语言能力有所提升，但在极低资源语言上表现仍然较差。现代大型语言模型如LLaMA和Qwen支持的语言数量远少于XLM-R，导致许多语言缺乏文本生成模型。为了解决这个问题，我们提出了一种新框架，将多语言编码器适应于极低资源语言的文本生成。通过重用编码器和解码器之间的权重，我们的框架能够利用编码器学习到的语义空间，从而在低资源语言中实现高效学习和有效泛化。'}}}, {'id': 'https://huggingface.co/papers/2502.12524', 'title': 'YOLOv12: Attention-Centric Real-Time Object Detectors', 'url': 'https://huggingface.co/papers/2502.12524', 'abstract': 'Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms. YOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats RT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the computation and 45% of the parameters. More comparisons are shown in Figure 1.', 'score': 1, 'issue_id': 2302, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '6f915c875f0fb15d', 'authors': ['Yunjie Tian', 'Qixiang Ye', 'David Doermann'], 'affiliations': ['University at Buffalo', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.12524.jpg', 'data': {'categories': ['#cv', '#optimization', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'YOLOv12: Внимание со скоростью CNN', 'desc': 'YOLOv12 - это новая архитектура для обнаружения объектов, основанная на механизмах внимания. Она сочетает высокую точность моделей с вниманием и скорость CNN-моделей. YOLOv12 превосходит существующие детекторы реального времени по точности при сопоставимой скорости. Например, YOLOv12-N достигает 40.6% mAP с задержкой вывода 1.64 мс на GPU T4, превосходя YOLOv10-N и YOLOv11-N.'}, 'en': {'title': 'YOLOv12: Merging Speed and Accuracy with Attention Mechanisms', 'desc': 'This paper introduces YOLOv12, an enhanced version of the YOLO framework that integrates attention mechanisms to improve object detection accuracy while maintaining high speed. Unlike previous models that relied solely on CNN improvements, YOLOv12 achieves a mean Average Precision (mAP) of 40.6% with an inference latency of just 1.64 ms on a T4 GPU. It outperforms earlier YOLO versions and other real-time detectors like RT-DETR, demonstrating superior performance with reduced computational requirements. The results indicate that attention-based models can now compete effectively with traditional CNNs in real-time applications.'}, 'zh': {'title': 'YOLOv12：速度与准确性的完美结合', 'desc': '本论文提出了一种以注意力机制为中心的YOLO框架，称为YOLOv12。与传统的基于卷积神经网络（CNN）的模型相比，YOLOv12在保持相似速度的同时，利用了注意力机制的性能优势。实验结果表明，YOLOv12在准确性上超越了所有流行的实时目标检测器，并且在推理延迟方面表现出色。该模型在不同规模下均展现出优越的性能，尤其是在计算和参数使用上更为高效。'}}}, {'id': 'https://huggingface.co/papers/2502.08869', 'title': 'Harnessing Vision Models for Time Series Analysis: A Survey', 'url': 'https://huggingface.co/papers/2502.08869', 'abstract': 'Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.', 'score': 1, 'issue_id': 2299, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '8bfec40a634b6df7', 'authors': ['Jingchao Ni', 'Ziming Zhao', 'ChengAo Shen', 'Hanghang Tong', 'Dongjin Song', 'Wei Cheng', 'Dongsheng Luo', 'Haifeng Chen'], 'affiliations': ['Florida International University', 'NEC Laboratories America', 'University of Connecticut', 'University of Houston', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.08869.jpg', 'data': {'categories': ['#multimodal', '#cv', '#survey', '#data'], 'emoji': '📊', 'ru': {'title': 'Визуальные модели открывают новые горизонты в анализе временных рядов', 'desc': 'Данная статья представляет собой обзор методов анализа временных рядов с использованием моделей компьютерного зрения. Авторы рассматривают преимущества визуальных моделей по сравнению с языковыми моделями для анализа временных рядов. В работе представлена подробная таксономия методов кодирования временных рядов в изображения и их последующего моделирования. Также обсуждаются проблемы предобработки и постобработки данных в этом подходе и намечаются перспективные направления исследований.'}, 'en': {'title': 'Harnessing Vision Models for Enhanced Time Series Analysis', 'desc': 'This paper surveys the use of vision models, such as Large Vision Models (LVMs) and Vision Language Models (VLMs), in time series analysis, highlighting their advantages over traditional Large Language Models (LLMs). It discusses the challenges of converting continuous time series data into a format suitable for these models, particularly focusing on encoding time series as images. The paper also explores how to effectively model these imaged time series for various analytical tasks. Furthermore, it identifies key challenges in the pre- and post-processing stages and suggests future research directions to enhance the application of vision models in this field.'}, 'zh': {'title': '视觉模型助力时间序列分析的未来', 'desc': '时间序列分析经历了从传统自回归模型到深度学习模型，再到最近的变换器和大型语言模型（LLMs）的发展。尽管在时间序列分析中，视觉模型的应用也在增加，但由于对序列建模的研究占主导地位，这些努力并不显著。本文综述了视觉模型在时间序列分析中的优势，探讨了如何将时间序列编码为图像以及如何对图像化的时间序列进行建模。我们还讨论了该框架中预处理和后处理步骤的挑战，并提出了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2502.10990', 'title': 'FinMTEB: Finance Massive Text Embedding Benchmark', 'url': 'https://huggingface.co/papers/2502.10990', 'abstract': 'Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, FinPersona-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including FinPersona-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.', 'score': 1, 'issue_id': 2293, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '4cba4906d5d024ae', 'authors': ['Yixuan Tang', 'Yi Yang'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.10990.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#multilingual', '#transfer_learning', '#science', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'FinMTEB: Новый стандарт оценки эмбеддингов в финансовой сфере', 'desc': 'Статья представляет FinMTEB - специализированный бенчмарк для оценки моделей эмбеддингов в финансовой сфере. Он включает 64 набора данных по 7 задачам на английском и китайском языках. Авторы разработали модель FinPersona-E5, адаптированную для финансовой области. Результаты показывают, что модели, адаптированные для конкретной области, превосходят модели общего назначения, а простой подход Bag-of-Words превосходит сложные плотные эмбеддинги в задачах семантического сходства текстов.'}, 'en': {'title': 'FinMTEB: Elevating Financial NLP with Domain-Specific Embeddings', 'desc': 'This paper introduces the Finance Massive Text Embedding Benchmark (FinMTEB), which is designed specifically for evaluating embedding models in the financial domain. It includes 64 datasets across 7 tasks, focusing on various types of financial texts in both Chinese and English. The authors also present FinPersona-E5, a finance-adapted model that utilizes a persona-based data synthesis method for training on financial tasks. The study reveals that general-purpose benchmarks do not correlate well with financial tasks, domain-adapted models perform better, and a simple Bag-of-Words approach can outperform complex dense embeddings in certain financial tasks.'}, 'zh': {'title': '金融领域的嵌入模型评估新标准', 'desc': '嵌入模型在自然语言处理（NLP）应用中扮演着重要角色，尤其是在信息表示和检索方面。本文介绍了金融大规模文本嵌入基准（FinMTEB），这是一个专为金融领域设计的评估工具，包含64个金融特定的嵌入数据集，涵盖7个任务。我们还开发了一个适应金融领域的模型FinPersona-E5，利用基于角色的数据合成方法来训练多样化的金融嵌入任务。研究结果表明，通用基准的表现与金融领域任务的相关性有限，领域适应模型的表现优于通用模型，而简单的词袋模型在金融语义文本相似性任务中表现出乎意料地优于复杂的密集嵌入。'}}}, {'id': 'https://huggingface.co/papers/2502.06329', 'title': 'Expect the Unexpected: FailSafe Long Context QA for Finance', 'url': 'https://huggingface.co/papers/2502.06329', 'abstract': 'We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA', 'score': 102, 'issue_id': 2168, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '836d77158c8a414b', 'authors': ['Kiran Kamble', 'Melisa Russak', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Mateusz Russak', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc'], 'pdf_title_img': 'assets/pdf/title_img/2502.06329.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#optimization', '#long_context', '#hallucinations'], 'emoji': '💼', 'ru': {'title': 'FailSafeQA: Новый стандарт оценки надежности языковых моделей в финансах', 'desc': 'Исследователи представили новый эталонный тест FailSafeQA для оценки устойчивости и контекстной осведомленности языковых моделей (LLM) в финансовой сфере. Тест включает шесть вариаций взаимодействия человека с системой, основанной на LLM, и фокусируется на двух сценариях: отказ запроса и отказ контекста. Используя методологию LLM-as-a-Judge и модель Qwen2.5-72B-Instruct, авторы оценили 24 готовые модели по критериям устойчивости, контекстной обоснованности и соответствия. Результаты показали, что даже высокопроизводительные модели имеют значительный потенциал для улучшения, и подчеркнули важность FailSafeQA как инструмента для разработки более надежных LLM в финансовых приложениях.'}, 'en': {'title': 'Enhancing LLM Reliability in Finance with FailSafeQA', 'desc': 'The paper introduces FailSafeQA, a benchmark aimed at evaluating the robustness and context-awareness of large language models (LLMs) in financial query-answer systems. It focuses on two main failure scenarios: Query Failure, where the input query is altered in terms of expertise and clarity, and Context Failure, where irrelevant or degraded documents are introduced. The authors utilize the LLM-as-a-Judge methodology to assess various models based on their Robustness, Context Grounding, and Compliance scores. The findings reveal that while some models perform well under input variations, they struggle with maintaining accuracy, indicating a need for further enhancements in LLM reliability for financial contexts.'}, 'zh': {'title': 'FailSafeQA：提升金融领域LLM的鲁棒性与上下文意识', 'desc': '我们提出了一个新的长上下文金融基准，FailSafeQA，旨在测试大型语言模型（LLMs）在金融领域中对人机交互的鲁棒性和上下文意识。我们关注两个案例研究：查询失败和上下文失败。在查询失败场景中，我们通过改变领域专业性、完整性和语言准确性来扰动原始查询。在上下文失败案例中，我们模拟上传降级、无关和空文档的情况。'}}}, {'id': 'https://huggingface.co/papers/2502.06807', 'title': 'Competitive Programming with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2502.06807', 'abstract': 'We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.', 'score': 38, 'issue_id': 2164, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fd76cceb75f32321', 'authors': ['OpenAI', ':', 'Ahmed El-Kishky', 'Alexander Wei', 'Andre Saraiva', 'Borys Minaev', 'Daniel Selsam', 'David Dohan', 'Francis Song', 'Hunter Lightman', 'Ignasi Clavera', 'Jakub Pachocki', 'Jerry Tworek', 'Lorenz Kuhn', 'Lukasz Kaiser', 'Mark Chen', 'Max Schwarzer', 'Mostafa Rohaninejad', 'Nat McAleese', 'o3 contributors', 'Oleg Mürk', 'Rhythm Garg', 'Rui Shu', 'Szymon Sidor', 'Vineet Kosaraju', 'Wenda Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.06807.jpg', 'data': {'categories': ['#rlhf', '#rl', '#games', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением превосходит специализированные подходы в задачах рассуждения', 'desc': 'Исследование показывает, что применение обучения с подкреплением к большим языковым моделям (LLM) значительно улучшает их производительность в сложных задачах программирования и рассуждения. Авторы сравнивают модели общего назначения (OpenAI o1 и o3) со специализированной системой o1-ioi, разработанной для участия в Международной олимпиаде по информатике (IOI) 2024 года. Модель o3 достигла уровня золотой медали на IOI 2024 без использования специфических стратегий или послаблений правил. Результаты указывают на то, что масштабирование обучения с подкреплением общего назначения является более эффективным подходом к созданию ИИ для задач рассуждения, чем разработка узкоспециализированных техник.'}, 'en': {'title': 'Scaling General-Purpose Learning Outshines Specialized Strategies', 'desc': 'This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming.'}, 'zh': {'title': '强化学习助力通用模型超越特定领域系统', 'desc': '本论文展示了强化学习在大型语言模型（LLMs）中的应用，显著提升了复杂编码和推理任务的表现。我们比较了两种通用推理模型——OpenAI的o1和o3的早期检查点，以及一个特定领域的系统o1-ioi，该系统使用手工设计的推理策略。o1-ioi在2024年国际信息学奥林匹克竞赛中表现良好，获得了第49百分位的成绩，而在放宽竞争约束的情况下则获得了金牌。我们的研究表明，尽管专门的管道如o1-ioi能带来显著提升，但扩展的通用o3模型在没有依赖手工推理启发式的情况下，超越了这些结果。'}}}, {'id': 'https://huggingface.co/papers/2502.05878', 'title': 'Retrieval-augmented Large Language Models for Financial Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.05878', 'abstract': 'Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.', 'score': 24, 'issue_id': 2172, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'bb6c947ce857a2db', 'authors': ['Mengxi Xiao', 'Zihao Jiang', 'Lingfei Qian', 'Zhengyu Chen', 'Yueru He', 'Yijing Xu', 'Yuecheng Jiang', 'Dong Li', 'Ruey-Ling Weng', 'Min Peng', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['School of Computer Science, Wuhan University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05878.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#rag'], 'emoji': '📈', 'ru': {'title': 'FinSeer: Умный поиск для точного прогноза акций', 'desc': 'Статья представляет новую систему для прогнозирования движения акций на основе технологии извлечения и генерации (RAG). Ключевые инновации включают использование крупной языковой модели StockLLM, новый метод отбора кандидатов с обратной связью от LLM и специальную цель обучения для поиска значимых исторических последовательностей. Разработанная система FinSeer превосходит существующие методы извлечения информации, достигая на 8% более высокой точности на датасете BIGDATA22. Работа подчеркивает важность специализированных моделей извлечения данных в финансовом прогнозировании.'}, 'en': {'title': 'Revolutionizing Stock Prediction with RAG Framework', 'desc': 'This paper introduces a new approach to predicting stock movements by using a retrieval-augmented generation (RAG) framework specifically designed for financial time-series data. The framework employs a large language model called StockLLM, which has been fine-tuned to better understand financial contexts. It also features a unique candidate selection method that utilizes feedback from the language model to improve the relevance of retrieved data. The results show that this method significantly enhances prediction accuracy and uncovers important patterns in complex financial datasets, outperforming traditional retrieval techniques.'}, 'zh': {'title': '金融预测的新框架：检索增强生成', 'desc': '本文提出了一种用于金融时间序列预测的检索增强生成（RAG）框架，旨在从大量时间序列数据中识别和提取关键影响因素。我们使用了一个经过微调的1B参数大型语言模型（StockLLM）作为基础，并引入了一种新颖的候选选择方法，利用LLM反馈来优化检索过程。通过最大化查询与历史重要序列之间的相似性，我们的检索器FinSeer能够在复杂的金融数据中发现有意义的模式，同时减少噪声。实验结果表明，该RAG框架在准确性上优于传统方法，强调了定制检索模型在金融预测中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.07316', 'title': 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction', 'url': 'https://huggingface.co/papers/2502.07316', 'abstract': 'Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.', 'score': 21, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'fd4f34152d4de2c1', 'authors': ['Junlong Li', 'Daya Guo', 'Dejian Yang', 'Runxin Xu', 'Yu Wu', 'Junxian He'], 'affiliations': ['DeepSeek-AI', 'HKUST', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07316.jpg', 'data': {'categories': ['#dataset', '#data', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'CodeI/O: Раскрытие потенциала рассуждений в больших языковых моделях через код', 'desc': 'Статья представляет новый подход CodeI/O для улучшения способностей больших языковых моделей к рассуждениям. Метод преобразует разнообразные паттерны рассуждений, встроенные в код, в формат предсказания ввода-вывода кода на естественном языке. Это позволяет моделям изучать универсальные примитивы рассуждений, такие как планирование логического потока и модульная декомпозиция. Эксперименты показывают, что CodeI/O приводит к улучшениям в задачах символических, научных, логических и других типов рассуждений.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with CodeI/O', 'desc': 'This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++.'}, 'zh': {'title': 'CodeI/O：提升推理能力的新方法', 'desc': '这篇论文提出了一种新的方法，称为CodeI/O，旨在提高大型语言模型的推理能力。通过将原始代码转换为输入输出预测格式，CodeI/O系统地提炼了多样的推理模式。模型通过自然语言的链式思维（CoT）推理来预测代码的输入和输出，从而增强了逻辑流规划、状态空间搜索等推理原语的能力。实验结果表明，CodeI/O在多种推理任务上均表现出一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07701', 'title': 'Magic 1-For-1: Generating One Minute Video Clips within One Minute', 'url': 'https://huggingface.co/papers/2502.07701', 'abstract': 'In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.', 'score': 17, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '7212b752112bcd5a', 'authors': ['Hongwei Yi', 'Shitong Shao', 'Tian Ye', 'Jiantong Zhao', 'Qingyu Yin', 'Michael Lingelbach', 'Li Yuan', 'Yonghong Tian', 'Enze Xie', 'Daquan Zhou'], 'affiliations': ['Hedra Inc.', 'Nvidia', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07701.jpg', 'data': {'categories': ['#video', '#training', '#inference', '#multimodal', '#open_source', '#diffusion', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео: от текста к кадрам за секунды', 'desc': 'Magic 1-For-1 (Magic141) - это эффективная модель генерации видео с оптимизированным потреблением памяти и латентностью вывода. Ключевая идея заключается в разделении задачи генерации видео по тексту на две более простые задачи: генерацию изображения по тексту и генерацию видео по изображению. Авторы применяют ряд оптимизационных приемов, включая многомодальное введение условий, состязательную дистилляцию шагов и разреживание параметров. В результате модель способна генерировать 5-секундные видеоклипы менее чем за 3 секунды, а минутное видео - за одну минуту с улучшенным качеством и динамикой.'}, 'en': {'title': 'Efficient Video Generation: Simplifying with Magic141', 'desc': 'The paper introduces Magic 1-For-1 (Magic141), a video generation model designed to optimize memory usage and reduce inference time. It simplifies the text-to-video generation process by breaking it down into two tasks: generating images from text and then creating videos from those images. The authors demonstrate that the image-to-video task converges more easily than the direct text-to-video approach, allowing for faster training. They implement various optimization techniques to enhance model performance, achieving impressive video generation speeds while maintaining high visual quality.'}, 'zh': {'title': '高效视频生成，轻松实现！', 'desc': '本文介绍了一种高效的视频生成模型Magic 1-For-1（Magic141），该模型优化了内存消耗和推理延迟。其核心思想是将文本到视频生成任务分解为两个更简单的任务：文本到图像生成和图像到视频生成。研究表明，使用相同的优化算法，图像到视频任务的收敛速度确实优于文本到视频任务。通过多种优化技巧，模型能够在短时间内生成高质量的视频片段，显著降低计算成本。'}}}, {'id': 'https://huggingface.co/papers/2502.07374', 'title': 'LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!', 'url': 'https://huggingface.co/papers/2502.07374', 'abstract': "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.", 'score': 17, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '4df9e17df3250cb4', 'authors': ['Dacheng Li', 'Shiyi Cao', 'Tyler Griggs', 'Shu Liu', 'Xiangxi Mo', 'Shishir G. Patil', 'Matei Zaharia', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.07374.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#math', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение длинным цепочкам рассуждений в больших языковых моделях', 'desc': 'Исследование показывает, что большие языковые модели (LLM) могут эффективно обучаться длинным цепочкам рассуждений (Long CoT) с помощью контролируемой дообучки на небольшом наборе данных. Модель Qwen2.5-32B-Instruct, обученная на 17 тысячах примеров Long CoT, значительно улучшила результаты в задачах по математике и программированию. Структура Long CoT оказалась критически важной для обучения, в то время как содержание отдельных шагов рассуждения имело минимальное влияние. Эти выводы углубляют понимание того, как развивать способности к рассуждению в LLM.'}, 'en': {'title': 'Unlocking Reasoning: Structure Over Content in Large Models', 'desc': 'This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples.'}, 'zh': {'title': '长链思维：推理模型的关键结构', 'desc': '大型推理模型（LRMs）通过长链思维（Long CoT）解决复杂的推理问题，这种思维方式包括反思、回溯和自我验证。我们发现，大型语言模型（LLM）可以通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）有效学习长链思维。仅使用17,000个长链思维训练样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中取得了显著提升。研究表明，长链思维的结构对学习过程至关重要，而单个推理步骤的内容对性能影响较小。'}}}, {'id': 'https://huggingface.co/papers/2502.06857', 'title': 'Gemstones: A Model Suite for Multi-Faceted Scaling Laws', 'url': 'https://huggingface.co/papers/2502.06857', 'abstract': 'Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: https://github.com/mcleish7/gemstone-scaling-laws', 'score': 16, 'issue_id': 2172, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '8f1b246a774832da', 'authors': ['Sean McLeish', 'John Kirchenbauer', 'David Yu Miller', 'Siddharth Singh', 'Abhinav Bhatele', 'Micah Goldblum', 'Ashwinee Panda', 'Tom Goldstein'], 'affiliations': ['Columbia University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.06857.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#architecture', '#optimization'], 'emoji': '💎', 'ru': {'title': 'Переосмысление законов масштабирования в глубоком обучении', 'desc': 'Статья исследует законы масштабирования в машинном обучении, используя широкий спектр архитектурных и гиперпараметрических вариаций. Авторы представляют набор данных Gemstones, содержащий более 4000 контрольных точек трансформеров с различными параметрами обучения. Исследование показывает, что рекомендации, основанные на законах масштабирования, могут сильно зависеть от экспериментального дизайна и выбранных моделей. Работа предлагает более сложный подход к изучению масштабирования, включая закон, предсказывающий производительность языкового моделирования в зависимости от ширины и глубины модели.'}, 'en': {'title': 'Unlocking Scaling Laws with Gemstones Dataset', 'desc': 'This paper explores scaling laws in machine learning by analyzing a diverse set of models with various hyper-parameters. The authors introduce the Gemstones dataset, which includes over 4000 transformer model checkpoints, allowing for extensive experimentation on scaling effects. They demonstrate that the performance of language models can be predicted based on their architecture, specifically width and depth. The findings reveal that scaling law prescriptions are sensitive to the design of experiments and the specific models used, emphasizing the importance of comprehensive datasets in understanding scaling behavior.'}, 'zh': {'title': '探索缩放法则的多样性与敏感性', 'desc': '本文研究了缩放法则，使用了多种架构和超参数选择，强调了这些选择对结果的影响。我们发布了Gemstones数据集，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个来自变换器模型的检查点，参数量高达20亿。通过这些检查点，我们能够进行更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。研究发现，缩放法则的适用性对实验设计过程和使用的特定模型检查点非常敏感。'}}}, {'id': 'https://huggingface.co/papers/2502.03492', 'title': 'Teaching Language Models to Critique via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.03492', 'abstract': 'Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.', 'score': 14, 'issue_id': 2165, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a0c98706806837c6', 'authors': ['Zhihui Xie', 'Jie chen', 'Liyu Chen', 'Weichao Mao', 'Jingjing Xu', 'Lingpeng Kong'], 'affiliations': ['Bytedance, Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.03492.jpg', 'data': {'categories': ['#rlhf', '#training', '#benchmark', '#reasoning', '#rl', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствующиеся ИИ-критики для генерации кода', 'desc': 'Статья представляет CTRL - фреймворк для обучения моделей-критиков с помощью обучения с подкреплением. Цель - научить языковые модели (LLM) генерировать полезную обратную связь для улучшения качества генерируемого кода без участия человека. Результаты показывают, что обученные критики значительно повышают процент успешных решений и уменьшают накопление ошибок. Модели-критики также могут использоваться как генеративные модели вознаграждения и позволяют улучшать результаты во время тестирования через итеративную критику и исправления.'}, 'en': {'title': 'Empowering Code Generation with Self-Critiquing Models', 'desc': 'This paper introduces CTRL, a framework that trains large language models (LLMs) to act as critics for code generation tasks. By using reinforcement learning, CTRL enables these critic models to provide feedback that helps improve the performance of a fixed generator model without needing human input. The study shows that critics trained with CTRL can significantly increase the success rates of code generation and reduce errors. Additionally, these critics function as effective generative reward models, allowing for iterative improvements during testing, leading to substantial performance gains on difficult benchmarks.'}, 'zh': {'title': '通过批评训练提升代码生成性能', 'desc': '本文研究了大型语言模型（LLMs）在代码生成中的批评和改进能力。我们提出了CTRL框架，通过强化学习训练批评模型，生成反馈以提高固定生成模型的修正性能，而无需人工监督。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并减少了累积错误。此外，这些批评模型作为生成奖励模型，能够在测试时通过迭代批评-修订实现扩展，在具有挑战性的代码生成基准上实现了高达106.1%的相对改进。'}}}, {'id': 'https://huggingface.co/papers/2502.07617', 'title': 'Scaling Pre-training to One Hundred Billion Data for Vision Language Models', 'url': 'https://huggingface.co/papers/2502.07617', 'abstract': "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.", 'score': 13, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '503a9dac2cae323c', 'authors': ['Xiao Wang', 'Ibrahim Alabdulmohsin', 'Daniel Salz', 'Zhe Li', 'Keran Rong', 'Xiaohua Zhai'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.07617.jpg', 'data': {'categories': ['#cultural_diversity', '#multilingual', '#dataset', '#low_resource', '#data', '#multimodal', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Масштабное предобучение для инклюзивных мультимодальных систем', 'desc': 'Исследование посвящено предобучению мультимодальных моделей на беспрецедентно большом наборе данных в 100 миллиардов примеров. Авторы обнаружили, что производительность модели на многих западноцентричных бенчмарках насыщается при таком масштабе. Однако задачи, связанные с культурным разнообразием, показывают значительный прирост благодаря охвату редких концепций в больших данных. Кроме того, исследование выявило улучшение производительности для низкоресурсных языков и предостерегает от чрезмерной фильтрации данных, которая может снизить культурное разнообразие.'}, 'en': {'title': 'Unlocking Cultural Diversity with 100 Billion Examples', 'desc': 'This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems.'}, 'zh': {'title': '大规模预训练助力文化多样性', 'desc': '本文探讨了在前所未有的规模上（1000亿个示例）对视觉-语言模型进行预训练的潜力。研究发现，在许多常见的西方分类和检索基准上，模型性能在此规模下趋于饱和。然而，对于文化多样性的任务，1000亿规模的网络数据带来了更显著的提升，因为它涵盖了长尾概念。此外，研究还分析了模型的多语言能力，显示在低资源语言上也有提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07508', 'title': 'Enhance-A-Video: Better Generated Video for Free', 'url': 'https://huggingface.co/papers/2502.07508', 'abstract': 'DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.', 'score': 12, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'e02d3082d3b21016', 'authors': ['Yang Luo', 'Xuanlei Zhao', 'Mengzhao Chen', 'Kaipeng Zhang', 'Wenqi Shao', 'Kai Wang', 'Zhangyang Wang', 'Yang You'], 'affiliations': ['National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.07508.jpg', 'data': {'categories': ['#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'Enhance-A-Video: Повышение качества генерации видео без переобучения', 'desc': 'Данная статья представляет новый подход к улучшению качества видео, генерируемого моделями на основе DiT (Diffusion Transformer), без необходимости дополнительного обучения. Метод, названный Enhance-A-Video, фокусируется на усилении межкадровых корреляций с использованием недиагональных распределений временного внимания. Подход легко применим к большинству существующих фреймворков генерации видео на основе DiT без переобучения или дополнительной настройки. Результаты демонстрируют значительное улучшение как временной согласованности, так и визуального качества генерируемых видео.'}, 'en': {'title': 'Enhancing DiT Video Generation Without Retraining', 'desc': 'This paper presents a novel method called Enhance-A-Video, aimed at improving the coherence and quality of videos generated by DiT-based models. The approach focuses on enhancing cross-frame correlations using non-diagonal temporal attention distributions, which helps maintain consistency across frames. Importantly, Enhance-A-Video does not require any retraining or fine-tuning, making it easy to integrate into existing DiT frameworks. The results show significant improvements in both temporal consistency and visual quality, paving the way for further advancements in video generation techniques.'}, 'zh': {'title': '提升视频生成质量的新方法', 'desc': '本研究提出了一种名为 Enhance-A-Video 的方法，旨在提高基于 DiT 的视频生成模型的连贯性和质量。该方法通过增强跨帧相关性，利用非对角时间注意力分布来实现。由于其设计简单，该方法可以轻松应用于大多数基于 DiT 的视频生成框架，而无需重新训练或微调。我们的实验表明，该方法在时间一致性和视觉质量方面都取得了显著的改善。'}}}, {'id': 'https://huggingface.co/papers/2502.06589', 'title': 'Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training', 'url': 'https://huggingface.co/papers/2502.06589', 'abstract': 'Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.', 'score': 10, 'issue_id': 2164, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4273eabfdc59b328', 'authors': ['Yuchen Zhuang', 'Jingfeng Yang', 'Haoming Jiang', 'Xin Liu', 'Kewei Cheng', 'Sanket Lokegaonkar', 'Yifan Gao', 'Qing Ping', 'Tianyi Liu', 'Binxuan Huang', 'Zheng Li', 'Zhengyang Wang', 'Pei Chen', 'Ruijie Wang', 'Rongzhi Zhang', 'Nasser Zalmout', 'Priyanka Nigam', 'Bing Yin', 'Chao Zhang'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.06589.jpg', 'data': {'categories': ['#agents', '#transfer_learning', '#optimization', '#training', '#reasoning', '#dataset', '#benchmark'], 'emoji': '🛠️', 'ru': {'title': 'Кузница агентов: улучшение LLM через специализированное предобучение', 'desc': 'Исследователи представили Hephaestus-Forge - первый крупномасштабный корпус для предобучения, направленный на улучшение фундаментальных способностей агентов на основе больших языковых моделей (LLM). Корпус содержит 103 миллиарда специфичных для агентов данных, охватывающих 76,537 API, включая документацию и траектории вызовов функций. Применение Hephaestus-Forge в процессе дообучения позволило модели Hephaestus превзойти открытые LLM малого и среднего масштаба и конкурировать с коммерческими LLM в трех тестах для агентов. Это демонстрирует эффективность предложенного подхода в улучшении базовых агентных возможностей и обобщающей способности LLM для новых задач и сред.'}, 'en': {'title': 'Empowering LLM Agents with Hephaestus-Forge', 'desc': 'This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability.'}, 'zh': {'title': 'Hephaestus-Forge：提升LLM代理能力的创新预训练语料库', 'desc': '由于缺乏面向代理的预训练数据，基于大型语言模型（LLM）的自主代理通常依赖复杂的提示或广泛的微调，这往往无法在保持强泛化能力的同时引入新功能。我们提出了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强LLM代理在API功能调用、内在推理和规划以及适应环境反馈方面的基本能力。Hephaestus-Forge包含1030亿个特定于代理的数据，涵盖76,537个API，包括工具文档以介绍API功能的知识和功能调用轨迹以增强内在推理。通过在Hephaestus-Forge上持续预训练，Hephaestus在三个代理基准测试中超越了小到中型的开源LLM，并与商业LLM相媲美，证明了我们的预训练语料库在增强代理基本能力和LLM对新任务或环境的泛化能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04223', 'title': 'Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents', 'url': 'https://huggingface.co/papers/2502.04223', 'abstract': "Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce \\'Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, \\'Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. \\'Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate \\'Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.", 'score': 9, 'issue_id': 2170, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '98e15ce7f5732b9f', 'authors': ['Ilia Karmanov', 'Amala Sanjay Deshmukh', 'Lukas Voegtle', 'Philipp Fischer', 'Kateryna Chumachenko', 'Timo Roman', 'Jarno Seppänen', 'Jupinder Parmar', 'Joseph Jennings', 'Andrew Tao', 'Karan Sapra'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04223.jpg', 'data': {'categories': ['#benchmark', '#science', '#data', '#dataset', '#optimization', '#cv'], 'emoji': '📄', 'ru': {'title': 'Éclair: Передовой инструмент OCR для комплексного анализа структуры документов', 'desc': 'Статья представляет Éclair - инструмент для извлечения текста из изображений документов. Éclair не только извлекает текст, но и понимает структуру документа, включая форматирование, формулы, таблицы и порядок чтения. Инструмент также способен определять семантические элементы, такие как сноски и подписи к изображениям. Авторы представляют новый разнообразный бенчмарк для оценки OCR на уровне документов и семантической классификации, на котором Éclair показывает наилучшие результаты.'}, 'en': {'title': 'Eclair: Revolutionizing Document Understanding with Advanced OCR', 'desc': "This paper presents 'Eclair', an advanced Optical Character Recognition (OCR) tool designed to extract not just text, but also the structural and semantic elements of complex documents. It recognizes formatting, tables, and reading order, which are essential for understanding multi-page documents. 'Eclair' provides bounding boxes and semantic classes for extracted text, enhancing its utility for tasks like document retrieval and question answering. The tool demonstrates state-of-the-art performance on a newly introduced benchmark, showcasing its effectiveness compared to existing methods."}, 'zh': {'title': 'Eclair：全面理解文档的OCR工具', 'desc': "光学字符识别（OCR）技术广泛应用于从文档图像中提取文本，促进高效的数字化和数据检索。然而，仅仅提取文本对于处理复杂文档是不够的。全面理解这些文档需要了解其结构，包括格式、公式、表格以及跨多个页面的多个块和列的阅读顺序，以及检测脚注和图像标题等元素的语义信息。为了解决这个问题，我们提出了'Eclair'，这是一种通用的文本提取工具，专门设计用于处理各种文档类型，并在文档级OCR和语义分类的基准测试中实现了最先进的准确性。"}}}, {'id': 'https://huggingface.co/papers/2502.03997', 'title': 'CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing', 'url': 'https://huggingface.co/papers/2502.03997', 'abstract': 'Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.', 'score': 8, 'issue_id': 2165, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '69bfc4de9cf7106d', 'authors': ['Yu Yuan', 'Shizhao Sun', 'Qi Liu', 'Jiang Bian'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.03997.jpg', 'data': {'categories': ['#data', '#dataset', '#multimodal', '#architecture'], 'emoji': '🛠️', 'ru': {'title': 'CAD-Editor: Революция в текстовом редактировании CAD-моделей', 'desc': "CAD-Editor - это первая система для редактирования CAD-моделей на основе текстовых инструкций. Она использует автоматизированный конвейер для синтеза обучающих данных, сочетая модели вариаций дизайна и большие мультимодальные языковые модели. Система применяет подход 'locate-then-infill', разбивая задачу на локализацию областей для изменения и их заполнение соответствующими правками. CAD-Editor опирается на большие языковые модели для понимания естественного языка и знаний о CAD, демонстрируя превосходные результаты в экспериментах."}, 'en': {'title': 'Revolutionizing CAD Editing with Text Instructions', 'desc': 'This paper presents CAD-Editor, a novel framework for text-based editing of Computer Aided Design (CAD) models. It addresses the limitations of existing methods by integrating automated data synthesis and Large Vision-Language Models (LVLMs) to generate editing instructions from original and modified CAD models. The framework employs a locate-then-infill approach, breaking down the editing process into identifying areas for change and applying the necessary modifications. Experimental results demonstrate that CAD-Editor outperforms previous techniques in both quantitative metrics and qualitative assessments.'}, 'zh': {'title': '文本驱动的CAD编辑新纪元', 'desc': '计算机辅助设计（CAD）在各个行业中至关重要。基于文本的CAD编辑可以根据文本指令自动修改CAD模型，但这一领域尚未得到充分探索。现有方法主要集中在设计变体生成或基于文本的CAD生成，缺乏对文本控制的支持或忽视了现有CAD模型的约束。我们提出了CAD-Editor，这是第一个用于基于文本的CAD编辑的框架，利用大型语言模型（LLMs）和自动化数据合成管道，实现了高效的编辑指令生成和模型修改。'}}}, {'id': 'https://huggingface.co/papers/2502.07527', 'title': 'NatureLM: Deciphering the Language of Nature for Scientific Discovery', 'url': 'https://huggingface.co/papers/2502.07527', 'abstract': 'Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.', 'score': 8, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'a6e947f52bde9a9c', 'authors': ['Yingce Xia', 'Peiran Jin', 'Shufang Xie', 'Liang He', 'Chuan Cao', 'Renqian Luo', 'Guoqing Liu', 'Yue Wang', 'Zequn Liu', 'Yuan-Jyue Chen', 'Zekun Guo', 'Yeqi Bai', 'Pan Deng', 'Yaosen Min', 'Ziheng Lu', 'Hongxia Hao', 'Han Yang', 'Jielan Li', 'Chang Liu', 'Jia Zhang', 'Jianwei Zhu', 'Kehan Wu', 'Wei Zhang', 'Kaiyuan Gao', 'Qizhi Pei', 'Qian Wang', 'Xixian Liu', 'Yanting Li', 'Houtian Zhu', 'Yeqing Lu', 'Mingqian Ma', 'Zun Wang', 'Tian Xie', 'Krzysztof Maziarz', 'Marwin Segler', 'Zhao Yang', 'Zilong Chen', 'Yu Shi', 'Shuxin Zheng', 'Lijun Wu', 'Chen Hu', 'Peggy Dai', 'Tie-Yan Liu', 'Haiguang Liu', 'Tao Qin'], 'affiliations': ['Microsoft Research AI for Science'], 'pdf_title_img': 'assets/pdf/title_img/2502.07527.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#science', '#optimization', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'NatureLM: единая модель для научных открытий во множестве доменов', 'desc': 'Исследователи разработали NatureLM - языковую модель для научных открытий, объединяющую различные научные домены. Эта фундаментальная модель обучена на данных из нескольких областей, включая молекулы, материалы, белки, ДНК и РНК. NatureLM способна генерировать и оптимизировать объекты из разных доменов, а также выполнять кросс-доменные задачи. Модель демонстрирует высокую производительность в различных научных задачах и доступна в нескольких размерах, от 1 до 46,7 миллиардов параметров.'}, 'en': {'title': 'NatureLM: Unifying Science Through Language Models', 'desc': 'This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design.'}, 'zh': {'title': '自然语言模型：科学发现的新工具', 'desc': '基础模型在自然语言处理和人工智能领域带来了革命性的变化，显著提升了机器理解和生成自然语言的能力。受基础模型成功的启发，研究人员为各个科学领域开发了相应的基础模型，但这些模型通常是孤立训练的，缺乏跨领域整合的能力。我们提出了自然语言模型（NatureLM），这是一个基于序列的科学基础模型，旨在促进科学发现。NatureLM经过多领域数据的预训练，能够支持小分子、蛋白质、RNA和材料的生成与优化，并在多个科学任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.05364', 'title': 'Hypencoder: Hypernetworks for Information Retrieval', 'url': 'https://huggingface.co/papers/2502.05364', 'abstract': "The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms.", 'score': 5, 'issue_id': 2176, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c3495b093acff010', 'authors': ['Julian Killingback', 'Hansi Zeng', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.05364.jpg', 'data': {'categories': ['#benchmark', '#rag', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Hypencoder: революция в релевантном поиске с помощью динамических нейросетей', 'desc': 'В статье представлен новый подход к информационному поиску, называемый Hypencoder. Вместо создания векторного представления запроса, метод генерирует небольшую нейронную сеть, выступающую в роли функции релевантности. Эта сеть принимает представление документа и выдает скалярную оценку релевантности. Эксперименты показывают, что Hypencoder значительно превосходит существующие модели плотного поиска и ранжирования, особенно на сложных задачах поиска.'}, 'en': {'title': 'Revolutionizing Retrieval with Hypencoder: A Neural Network Approach', 'desc': 'This paper introduces a novel approach to document retrieval by replacing traditional vector representations with a small neural network that functions as a learned relevance function. Instead of using vector inner products to score relevance, the proposed Hypencoder generates a scalar relevance score based on document representations. The use of a hypernetwork allows for efficient weight generation for the Hypencoder, leading to superior performance on various retrieval tasks compared to existing dense retrieval and reranking models. Additionally, the Hypencoder demonstrates strong generalization capabilities and can efficiently search large document collections in a short time frame.'}, 'zh': {'title': 'Hypencoder：超越传统检索模型的新方法', 'desc': '大多数检索模型依赖于向量内积来生成查询和文档之间的相关性评分，这限制了相关性评分的表达能力。我们提出了一种新范式，使用小型神经网络作为学习的相关性函数，而不是生成向量来表示查询。这个小型神经网络接收文档的表示，并输出一个标量的相关性评分。我们的实验表明，Hypencoder在领域内检索任务中显著优于强大的密集检索模型，并且在一些困难的检索任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.06428', 'title': 'CoS: Chain-of-Shot Prompting for Long Video Understanding', 'url': 'https://huggingface.co/papers/2502.06428', 'abstract': 'Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.', 'score': 5, 'issue_id': 2173, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '8302e2d276dc5929', 'authors': ['Jian Hu', 'Zixu Cheng', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2502.06428.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умный выбор кадров для понимания длинных видео искусственным интеллектом', 'desc': 'Статья представляет новый метод Chain-of-Shot prompting (CoS) для улучшения понимания длинных видео мультимодальными большими языковыми моделями (MLLM). CoS решает проблему выбора релевантных кадров путем оптимизации визуальных промптов во время тестирования, адаптивно выбирая кадры в соответствии с задачей понимания видео. Метод включает механизм бинарного обобщения видео и модуль совместных рассуждений, которые помогают идентифицировать и сопоставлять релевантные и нерелевантные кадры. Эксперименты на пяти датасетах показали эффективность и адаптивность предложенного подхода.'}, 'en': {'title': 'Optimizing Video Understanding with Chain-of-Shot Prompting', 'desc': "This paper addresses the challenge that Multi-modal Large Language Models (MLLMs) face when processing long videos, which often contain too many visual tokens. These tokens can overwhelm the model with irrelevant information, making it difficult to understand the video's content. The authors propose a method called Chain-of-Shot prompting (CoS) that optimizes shot selection based on the specific task at hand, improving the alignment between selected shots and the semantic understanding required. CoS includes a binary video summary mechanism and a video co-reasoning module to enhance the model's focus on relevant shots, leading to better video comprehension."}, 'zh': {'title': '优化镜头选择，提升视频理解', 'desc': '多模态大型语言模型（MLLMs）在处理长视频时面临挑战，因为需要过多的视觉标记。这些标记超出了MLLMs的上下文长度，导致填充了大量与任务无关的镜头。如何选择镜头是一个未解决的关键问题：稀疏采样可能会错过关键细节，而全面采样则会使模型被无关内容淹没，从而导致视频理解错误。为了解决这个问题，我们提出了镜头链提示（CoS），通过优化镜头与任务的对齐来选择适合视频理解的镜头。'}}}, {'id': 'https://huggingface.co/papers/2502.07490', 'title': 'Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More', 'url': 'https://huggingface.co/papers/2502.07490', 'abstract': "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.", 'score': 5, 'issue_id': 2172, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '2d86c4124095af30', 'authors': ['Xialie Zhuang', 'Zhikai Jia', 'Jianjin Li', 'Zhenyu Zhang', 'Li Shen', 'Zheng Cao', 'Shiwei Liu'], 'affiliations': ['SCITIX (SGP) TECH PTE. LTD., Singapore', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China', 'South China Normal University, China', 'Sun YatSen University, China', 'University of Oxford, UK', 'University of Texas at Austin, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07490.jpg', 'data': {'categories': ['#training', '#reasoning', '#long_context', '#architecture', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'MEAP: Улучшение извлечения информации в больших языковых моделях', 'desc': 'Исследователи предложили новый метод обучения больших языковых моделей под названием MEAP (Mask-Enhanced Autoregressive Prediction). MEAP объединяет маскированное языковое моделирование (MLM) с предсказанием следующего токена (NTP) для улучшения способности модели извлекать ключевую информацию из контекста. Эксперименты показали, что MEAP значительно превосходит NTP в задачах извлечения ключевой информации и рассуждений на основе длинного контекста. Метод также демонстрирует преимущества при дообучении на размеченных данных, особенно в сценариях с потерей информации в середине последовательности.'}, 'en': {'title': 'Enhancing Information Retrieval in LLMs with MEAP', 'desc': 'This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a new training method for Large Language Models (LLMs) that improves their ability to retrieve important information. MEAP combines Masked Language Modeling (MLM) with Next-Token Prediction (NTP) by masking some input tokens and then predicting the next token using a decoder-only Transformer. This approach avoids the complexity of bidirectional attention and encoder-decoder structures, making it computationally efficient during training and inference. Experimental results show that MEAP significantly enhances performance in information retrieval and reasoning tasks, especially in scenarios where context is crucial, while also benefiting supervised fine-tuning.'}, 'zh': {'title': '掩码增强自回归预测：提升语言模型的信息检索能力', 'desc': '大型语言模型（LLMs）在准确检索关键信息方面存在问题。为了解决这个问题，我们提出了一种名为掩码增强自回归预测（MEAP）的训练范式，它将掩码语言建模（MLM）与下一个标记预测（NTP）无缝结合，以增强后者的上下文检索能力。MEAP通过随机掩盖输入标记的一小部分，然后使用仅解码器的Transformer直接进行标准的下一个标记预测。实验表明，MEAP在关键信息检索和长上下文推理任务上显著优于NTP，同时在常识推理任务上表现相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.07531', 'title': 'VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.07531', 'abstract': 'Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.', 'score': 5, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'dea5fd89dd98f3b1', 'authors': ['Sixiao Zheng', 'Zimian Peng', 'Yanpeng Zhou', 'Yi Zhu', 'Hang Xu', 'Xiangru Huang', 'Yanwei Fu'], 'affiliations': ['Fudan University, China', 'Huawei Noahs Ark Lab, China', 'Westlake University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07531.jpg', 'data': {'categories': ['#video', '#training', '#open_source', '#dataset', '#benchmark', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль над ключевыми элементами при генерации видео из изображений', 'desc': 'VidCRAFT3 - это новая система генерации видео из изображений, позволяющая одновременно контролировать движение камеры, движение объектов и направление освещения. В основе системы лежит Spatial Triple-Attention Transformer, который интегрирует информацию об освещении, тексте и изображении. Для обучения был создан синтетический датасет VideoLightingDirection с аннотациями направления освещения. Предложенная трехэтапная стратегия обучения позволяет обойтись без данных с одновременными аннотациями всех визуальных элементов.'}, 'en': {'title': 'VidCRAFT3: Mastering Multi-Element Control in Image-to-Video Generation', 'desc': 'This paper presents VidCRAFT3, a new framework for generating videos from images with enhanced control over multiple visual elements, including camera motion, object motion, and lighting direction. The authors introduce the Spatial Triple-Attention Transformer, which allows for better separation and management of these elements during the generation process. To support this framework, they created the VideoLightingDirection (VLD) dataset, which includes detailed lighting annotations to improve the realism of generated videos. The proposed three-stage training strategy enables effective learning without requiring simultaneous annotations for all visual elements, leading to superior video quality and coherence compared to existing methods.'}, 'zh': {'title': 'VidCRAFT3：多元素控制的图像到视频生成新框架', 'desc': '本文介绍了一种新的图像到视频生成框架VidCRAFT3，能够同时控制相机运动、物体运动和光照方向。为了更好地解耦每个视觉元素的控制，提出了空间三重注意力变换器，能够对光照方向、文本和图像进行对称整合。由于大多数真实世界视频数据集缺乏光照注释，研究者构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），该数据集包含光照方向注释和多样化外观的物体。通过广泛的实验，VidCRAFT3在生成高质量视频内容方面表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2502.07445', 'title': 'Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon', 'url': 'https://huggingface.co/papers/2502.07445', 'abstract': "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.", 'score': 4, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '3e6282dd3913750a', 'authors': ['Nurit Cohen-Inger', 'Yehonatan Elisha', 'Bracha Shapira', 'Lior Rokach', 'Seffi Cohen'], 'affiliations': ['Ben Gurion University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07445.jpg', 'data': {'categories': ['#interpretability', '#training', '#dataset', '#hallucinations', '#benchmark', '#optimization'], 'emoji': '🦎', 'ru': {'title': 'Разоблачение иллюзии понимания: как языковые модели маскируют переобучение', 'desc': 'Статья представляет новый метод оценки языковых моделей под названием C-BOD. Этот метод выявляет переобучение моделей путем систематического искажения входных данных с сохранением их семантического содержания. Исследование показало, что многие модели, включая крупные ЯМ, демонстрируют значительное снижение производительности при небольших изменениях формулировок. Авторы призывают сообщество уделять больше внимания устойчивости и обобщающей способности моделей, а не только показателям в рейтингах.'}, 'en': {'title': 'Beyond Scores: Evaluating True Language Understanding in LLMs', 'desc': "This paper discusses the limitations of large language models (LLMs) in truly understanding language, as they often rely on specific patterns in datasets rather than genuine comprehension. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a tool that modifies benchmark prompts to test if LLMs are overfitting to memorized cues. By analyzing the performance of 26 leading LLMs on the MMLU benchmark, they find that many models show a decline in performance when faced with slight changes in input, indicating a reliance on fixed patterns. The study emphasizes the need for better evaluation methods that focus on a model's ability to generalize and understand language, rather than just achieving high scores on benchmarks."}, 'zh': {'title': '超越分数，关注模型的鲁棒性与泛化能力', 'desc': '大型语言模型（LLMs）在公共基准测试中表现优异，但这些高分可能掩盖了模型对特定数据集表面特征的过度依赖，而非真正的语言理解。我们提出了变色龙基准过拟合检测器（C-BOD），这是一个通过参数变换系统性扭曲基准提示的元评估框架，用于检测LLMs的过拟合。C-BOD通过重新表述输入，同时保持其语义内容和标签，揭示模型性能是否受到记忆模式的驱动。我们的研究表明，经过适度扰动后，26个领先的LLM在MMLU基准上的平均性能下降了2.15%，这表明模型在评估时需要关注更强的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.06755', 'title': 'Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models', 'url': 'https://huggingface.co/papers/2502.06755', 'abstract': 'To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/SAE-V.', 'score': 3, 'issue_id': 2175, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': 'cf5a833e93ca6f88', 'authors': ['Samuel Stevens', 'Wei-Lun Chao', 'Tanya Berger-Wolf', 'Yu Su'], 'affiliations': ['The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06755.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#open_source', '#interpretability'], 'emoji': '🔬', 'ru': {'title': 'Разреженные автоэнкодеры: ключ к пониманию и контролю моделей компьютерного зрения', 'desc': 'Статья представляет унифицированную систему на основе разреженных автоэнкодеров (SAE) для интерпретации и контроля визуальных моделей машинного обучения. Метод позволяет обнаруживать интерпретируемые человеком визуальные признаки и точно манипулировать ими для проверки гипотез о поведении модели. Исследователи применили свой подход к современным моделям компьютерного зрения и выявили ключевые различия в семантических абстракциях, изученных моделями с разными целями предварительного обучения. Авторы демонстрируют практическое применение своей системы через контролируемые вмешательства в различных задачах компьютерного зрения.'}, 'en': {'title': 'Bridging Interpretation and Control in Vision Models with Sparse Autoencoders', 'desc': 'This paper introduces a new framework that uses sparse autoencoders (SAEs) to interpret and manipulate visual features in vision models. It addresses the challenge of validating the causal influence of these features through controlled experiments. By applying this framework, the authors uncover significant differences in the semantic abstractions learned by various models based on their pre-training objectives. The framework allows for reliable identification and manipulation of interpretable features without needing to retrain the models, enhancing our understanding of their behavior.'}, 'zh': {'title': '用稀疏自编码器理解和操控视觉模型', 'desc': '本文提出了一种统一框架，利用稀疏自编码器（SAE）来理解视觉模型的特征。该框架不仅可以发现人类可解释的视觉特征，还能精确操控这些特征，以测试模型行为的假设。通过对先进视觉模型的应用，我们揭示了不同预训练目标模型所学习的语义抽象之间的关键差异。我们的研究表明，SAE能够在不重新训练模型的情况下，可靠地识别和操控可解释的视觉特征，为理解和控制视觉模型行为提供了强大的工具。'}}}, {'id': 'https://huggingface.co/papers/2502.07785', 'title': 'Pippo: High-Resolution Multi-View Humans from a Single Image', 'url': 'https://huggingface.co/papers/2502.07785', 'abstract': 'We present Pippo, a generative model capable of producing 1K resolution dense turnaround videos of a person from a single casually clicked photo. Pippo is a multi-view diffusion transformer and does not require any additional inputs - e.g., a fitted parametric model or camera parameters of the input image. We pre-train Pippo on 3B human images without captions, and conduct multi-view mid-training and post-training on studio captured humans. During mid-training, to quickly absorb the studio dataset, we denoise several (up to 48) views at low-resolution, and encode target cameras coarsely using a shallow MLP. During post-training, we denoise fewer views at high-resolution and use pixel-aligned controls (e.g., Spatial anchor and Plucker rays) to enable 3D consistent generations. At inference, we propose an attention biasing technique that allows Pippo to simultaneously generate greater than 5 times as many views as seen during training. Finally, we also introduce an improved metric to evaluate 3D consistency of multi-view generations, and show that Pippo outperforms existing works on multi-view human generation from a single image.', 'score': 2, 'issue_id': 2179, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'a7eee475138619ae', 'authors': ['Yash Kant', 'Ethan Weber', 'Jin Kyu Kim', 'Rawal Khirodkar', 'Su Zhaoen', 'Julieta Martinez', 'Igor Gilitschenski', 'Shunsuke Saito', 'Timur Bagautdinov'], 'affiliations': ['Meta Reality Labs', 'UC Berkeley', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.07785.jpg', 'data': {'categories': ['#multimodal', '#inference', '#3d', '#diffusion', '#video', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Генерация 3D-видео человека из одного фото', 'desc': 'Представлена модель Pippo, способная генерировать видео с поворотом человека на 360 градусов в разрешении 1K из одной фотографии. Это мультиракурсный диффузионный трансформер, предварительно обученный на 3 миллиардах изображений людей. Модель использует многоракурсное обучение на студийных данных и пиксельно-выровненное управление для обеспечения 3D-согласованности. На этапе вывода применяется техника смещения внимания, позволяющая генерировать в 5 раз больше ракурсов, чем при обучении.'}, 'en': {'title': 'Pippo: Transforming Single Photos into Stunning 3D Videos!', 'desc': 'Pippo is a generative model designed to create high-resolution videos of a person using just one casual photo. It employs a multi-view diffusion transformer, eliminating the need for extra inputs like camera settings or parametric models. The model is pre-trained on a vast dataset of human images and fine-tuned with studio-captured data to enhance its performance. Pippo introduces innovative techniques for generating multiple views and evaluating the consistency of 3D outputs, outperforming previous methods in generating videos from single images.'}, 'zh': {'title': 'Pippo：从单张照片生成高质量视频的创新模型', 'desc': 'Pippo是一种生成模型，可以从一张随意拍摄的照片生成1K分辨率的密集视频。它使用多视角扩散变换器，不需要额外的输入，如参数模型或相机参数。Pippo在30亿张无标签的人类图像上进行预训练，并在后续的训练中使用低分辨率和高分辨率的多视角数据进行优化。最终，Pippo在生成多视角人类视频时表现优于现有的方法，且引入了一种新的评估指标来衡量3D一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.07640', 'title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2502.07640', 'abstract': 'We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.', 'score': 2, 'issue_id': 2175, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'f0efbd784e8053e1', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Jiayun Wu', 'Hongzhou Lin', 'Kaiyu Yang', 'Jia Li', 'Mengzhou Xia', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'Numina', 'Princeton Language and Intelligence, Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07640.jpg', 'data': {'categories': ['#training', '#math', '#data', '#benchmark', '#reasoning', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматизированном доказательстве теорем с помощью LLM', 'desc': 'Представлен Goedel-Prover - модель большого языка (LLM) с открытым исходным кодом для автоматизированной генерации формальных математических доказательств. Основная проблема в этой области - нехватка формализованных математических утверждений и доказательств, которую авторы решают путем создания датасета из 1,64 миллиона формальных утверждений. Используется итеративный подход для построения большого набора формальных доказательств, обучая серию доказывающих моделей. Goedel-Prover превосходит все существующие модели с открытым исходным кодом в генерации полных доказательств, достигая 57,6% успеха на бенчмарке miniF2F.'}, 'en': {'title': 'Revolutionizing Formal Proof Generation with Goedel-Prover', 'desc': 'Goedel-Prover is an advanced open-source large language model designed for generating formal proofs in mathematics. It addresses the challenge of limited formalized math statements by creating a dataset of 1.64 million formal statements from natural language problems. The model employs iterative training of provers, where each new prover builds on the successes of its predecessors, leading to improved proof generation capabilities. As a result, Goedel-Prover achieves state-of-the-art performance, surpassing previous models in both proof generation success rates and the volume of formal proofs produced.'}, 'zh': {'title': 'Goedel-Prover：数学证明生成的新突破', 'desc': 'Goedel-Prover 是一个开源的大型语言模型，专注于自动化形式证明生成，特别是在数学问题上表现出色。我们通过训练语句形式化器，将自然语言数学问题转换为正式语言（Lean 4），创建了一个包含164万条正式语句的数据集。使用大型语言模型来验证这些正式语句是否准确保留了原始自然语言问题的内容。最终的证明者在整个证明生成方面超越了所有现有的开源模型，成功率显著提高。'}}}, {'id': 'https://huggingface.co/papers/2502.05932', 'title': 'Skill Expansion and Composition in Parameter Space', 'url': 'https://huggingface.co/papers/2502.05932', 'abstract': "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.", 'score': 2, 'issue_id': 2170, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'd4dd3a87e6ed9712', 'authors': ['Tenglong Liu', 'Jianxiong Li', 'Yinan Zheng', 'Haoyi Niu', 'Yixing Lan', 'Xin Xu', 'Xianyuan Zhan'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'National University of Defense Technology', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05932.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#optimization', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эволюция навыков ИИ-агентов через параметрическое расширение и композицию', 'desc': 'Статья представляет новый фреймворк под названием Parametric Skill Expansion and Composition (PSEC) для итеративного развития возможностей автономных агентов. PSEC использует управляемую библиотеку навыков, интегрируя примитивы навыков как модули Low-Rank Adaptation (LoRA) для эффективной донастройки параметров. Фреймворк позволяет напрямую комбинировать навыки в пространстве параметров путем объединения модулей LoRA, кодирующих различные навыки. PSEC демонстрирует превосходную способность использовать предыдущие знания для эффективного решения новых задач и расширения библиотеки навыков.'}, 'en': {'title': 'Empowering Autonomous Agents with Efficient Skill Evolution', 'desc': "This paper introduces Parametric Skill Expansion and Composition (PSEC), a framework that enhances the ability of autonomous agents to learn new skills by building on existing knowledge. PSEC maintains a skill library that allows for efficient integration of skill primitives using Low-Rank Adaptation (LoRA) modules, which support parameter-efficient finetuning. The framework also enables the merging of these modules to create new skills by leveraging shared information, promoting flexibility in skill development. Experimental results demonstrate that PSEC significantly improves the agents' performance in adapting to new challenges while expanding their skill sets effectively."}, 'zh': {'title': '智能体技能的高效扩展与组合', 'desc': '本文提出了一种新的框架，称为参数化技能扩展与组合（PSEC），旨在提高自主智能体在面对新挑战时的能力。该框架通过维护一个可管理的技能库，逐步整合技能原语，支持高效的参数微调，从而实现灵活的技能扩展。PSEC还允许在参数空间中直接组合技能，通过合并不同技能的LoRA模块，利用共享信息有效编程新技能。实验结果表明，PSEC在利用先前知识应对新挑战方面表现出色，并能够扩展其技能库以进化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04465', 'title': 'FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks', 'url': 'https://huggingface.co/papers/2502.04465', 'abstract': 'Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.', 'score': 2, 'issue_id': 2167, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a6ebe3d69cd8bcc2', 'authors': ['Luca Della Libera', 'Francesco Paissan', 'Cem Subakan', 'Mirco Ravanelli'], 'affiliations': ['Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2502.04465.jpg', 'data': {'categories': ['#multilingual', '#audio', '#architecture'], 'emoji': '🎙️', 'ru': {'title': 'FocalCodec: Эффективное сжатие речи с сохранением семантики и акустики', 'desc': 'Исследователи представили FocalCodec - эффективный аудиокодек с низким битрейтом, основанный на фокальной модуляции. Он использует единый бинарный кодбук для сжатия речи до 0.16-0.65 кбит/с, что ниже, чем у современных аналогов. FocalCodec показывает конкурентоспособные результаты в ресинтезе речи и преобразовании голоса, сохраняя при этом семантическую и акустическую информацию. Кодек хорошо справляется с многоязычной речью и шумными условиями, а также подходит для генеративного моделирования.'}, 'en': {'title': 'FocalCodec: Efficient Speech Compression with Single Binary Codebook', 'desc': 'This paper presents FocalCodec, a novel low-bitrate codec designed for speech processing, which addresses the limitations of existing methods that often require complex multi-codebook architectures. By utilizing focal modulation and a single binary codebook, FocalCodec achieves efficient compression of speech at bitrates between 0.16 and 0.65 kbps. The model demonstrates strong performance in tasks like speech resynthesis and voice conversion, while maintaining the integrity of both semantic and acoustic information. Additionally, FocalCodec is effective in handling multilingual speech and noisy environments, making it a promising tool for generative modeling in natural language processing.'}, 'zh': {'title': 'FocalCodec：高效低比特率语音编解码器', 'desc': '大型语言模型通过在海量数据集上进行自监督预训练，彻底改变了自然语言处理。受到这一成功的启发，研究人员尝试将这些方法应用于语音处理，通过神经音频编解码器将连续音频离散化为标记。然而，现有方法面临高比特率、语义或声学信息丢失以及多代码本设计的复杂性等限制。为了解决这些问题，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，能够在0.16到0.65 kbps之间压缩语音，同时在语音重合成和语音转换中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.07776', 'title': 'Auditing Prompt Caching in Language Model APIs', 'url': 'https://huggingface.co/papers/2502.07776', 'abstract': "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.", 'score': 2, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '48f7472ef1c86b27', 'authors': ['Chenchen Gu', 'Xiang Lisa Li', 'Rohith Kuditipudi', 'Percy Liang', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07776.jpg', 'data': {'categories': ['#healthcare', '#leakage', '#inference', '#ethics', '#security', '#data'], 'emoji': '🕵️', 'ru': {'title': 'Кэширование промптов в LLM: скрытая угроза приватности', 'desc': 'Статья исследует проблему кэширования промптов в больших языковых моделях (LLM) и связанные с этим риски утечки данных. Авторы разработали методы статистического аудита для обнаружения кэширования промптов у реальных API-провайдеров LLM. Они обнаружили глобальное разделение кэша между пользователями у семи провайдеров, включая OpenAI, что может привести к утечке информации о промптах пользователей. Кроме того, временные различия из-за кэширования могут раскрывать информацию об архитектуре модели, например, авторы нашли доказательства того, что модель встраивания OpenAI является декодер-ориентированным трансформером.'}, 'en': {'title': 'Timing Variations: A Privacy Risk in Prompt Caching for LLMs', 'desc': "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."}, 'zh': {'title': '提示缓存的隐私风险与透明性', 'desc': '在大型语言模型（LLMs）中，提示缓存会导致数据依赖的时间变化：缓存的提示处理速度比非缓存的提示快。这些时间差异可能引发侧信道攻击的风险，例如，如果缓存被多个用户共享，攻击者可以通过快速的API响应时间识别出缓存的提示，从而获取其他用户提示的信息。由于提示缓存可能导致隐私泄露，因此API提供商的缓存政策透明度非常重要。我们开发并进行统计审计，以检测现实世界中LLM API提供商的提示缓存情况，发现七个API提供商（包括OpenAI）之间存在全球缓存共享，可能导致用户提示的隐私泄露。'}}}, {'id': 'https://huggingface.co/papers/2502.06394', 'title': 'SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators', 'url': 'https://huggingface.co/papers/2502.06394', 'abstract': 'Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.', 'score': 69, 'issue_id': 2145, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '86b7da795fcf943b', 'authors': ['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], 'affiliations': ['AIRI', 'ISP RAS Research Center for Trusted AI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2502.06394.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#dataset', '#data', '#synthetic', '#open_source'], 'emoji': '🧼', 'ru': {'title': 'Синтетические данные улучшают многоязычную детоксификацию текста', 'desc': 'Статья представляет новый подход к многоязычной детоксификации текста. Авторы разработали конвейер для генерации параллельных многоязычных данных для детоксификации и создали датасет SynthDetoxM, содержащий 16,000 высококачественных пар предложений на немецком, французском, испанском и русском языках. Эксперименты показали, что модели, обученные на синтетических данных, превосходят модели, обученные на аннотированном людьми датасете MultiParaDetox. Авторы опубликовали свой датасет и код для дальнейших исследований в области многоязычной детоксификации текста.'}, 'en': {'title': 'Enhancing Multilingual Detoxification with SynthDetoxM', 'desc': 'This paper addresses the challenge of multilingual text detoxification, which is limited by the lack of parallel datasets in multiple languages. The authors present a new pipeline for generating such datasets, introducing SynthDetoxM, a collection of 16,000 detoxified sentence pairs in German, French, Spanish, and Russian. These pairs were created by rewriting existing toxicity evaluation data using modern open-source large language models (LLMs) in a few-shot learning context. The results show that models trained on this synthetic dataset outperform those trained on existing human-annotated datasets, demonstrating the effectiveness of the proposed approach in enhancing multilingual detoxification efforts.'}, 'zh': {'title': '多语言文本去毒化的新突破', 'desc': '本研究提出了一种生成多语言平行去毒化数据的流程，以解决现有多语言文本去毒化方法中平行多语言数据集稀缺的问题。我们介绍了SynthDetoxM，这是一个手动收集和合成生成的多语言平行文本去毒化数据集，包含来自德语、法语、西班牙语和俄语的16,000对高质量去毒化句子。数据来源于不同的毒性评估数据集，并通过九种现代开源大语言模型在少量样本设置下进行重写。实验结果表明，基于合成数据集训练的模型在数据有限的情况下表现优于基于人工标注的MultiParaDetox数据集训练的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.06703', 'title': 'Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.06703', 'abstract': 'Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.', 'score': 60, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2129c5ac1750f3cc', 'authors': ['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06703.jpg', 'data': {'categories': ['#inference', '#reasoning', '#small_models', '#training', '#optimization', '#math'], 'emoji': '🧮', 'ru': {'title': 'Маленькие модели побеждают гигантов: сила масштабирования во время теста', 'desc': 'Это исследование анализирует влияние метода Test-Time Scaling (TTS) на производительность больших языковых моделей (LLM) при решении сложных математических задач. Авторы изучают, как выбор политики модели, модели вознаграждения процесса (PRM) и сложность задачи влияют на оптимальную стратегию TTS. Результаты показывают, что даже небольшие модели могут превзойти более крупные при использовании оптимальной стратегии TTS. Исследование демонстрирует потенциал TTS для улучшения способностей LLM к рассуждению и решению сложных задач.'}, 'en': {'title': 'Unlocking LLM Potential: Small Models, Big Gains with TTS!', 'desc': 'This paper investigates Test-Time Scaling (TTS), a technique that enhances the performance of Large Language Models (LLMs) by adjusting computation during inference. It addresses how different policy models, Process Reward Models (PRMs), and the difficulty of problems affect the effectiveness of TTS. The authors conduct experiments on MATH-500 and AIME24 tasks, revealing that smaller models can outperform larger ones when using an optimal TTS strategy. The results emphasize the importance of tailoring TTS methods to specific models and tasks to improve reasoning capabilities in LLMs.'}, 'zh': {'title': '优化测试时间扩展，提升小型模型性能！', 'desc': '测试时间扩展（TTS）是一种通过在推理阶段增加计算量来提高大型语言模型（LLMs）性能的方法。本文系统分析了策略模型、过程奖励模型（PRMs）和问题难度如何影响TTS的效果。研究表明，计算最优的TTS策略依赖于所选的策略模型、PRM和问题难度，且小型模型在某些情况下可以超越大型模型。通过在MATH-500和AIME24任务上的实验，我们发现适应特定任务和模型的TTS策略对于提升LLMs的推理能力至关重要。'}}}, {'id': 'https://huggingface.co/papers/2502.06781', 'title': 'Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2502.06781', 'abstract': 'Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future researchhttps://github.com/InternLM/OREAL.', 'score': 36, 'issue_id': 2142, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '9cd2694b7c865b94', 'authors': ['Chengqi Lyu', 'Songyang Gao', 'Yuzhe Gu', 'Wenwei Zhang', 'Jianfei Gao', 'Kuikun Liu', 'Ziyi Wang', 'Shuaibin Li', 'Qian Zhao', 'Haian Huang', 'Weihan Cao', 'Jiangning Liu', 'Hongwei Liu', 'Junnan Liu', 'Songyang Zhang', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['HKGAI under InnoHK', 'MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06781.jpg', 'data': {'categories': ['#training', '#open_source', '#rl', '#reasoning', '#optimization', '#math'], 'emoji': '🧮', 'ru': {'title': 'OREAL: Прорыв в обучении с подкреплением для математических рассуждений', 'desc': 'Статья представляет новый фреймворк обучения с подкреплением под названием OREAL для решения математических задач. Авторы теоретически доказывают, что клонирование поведения на положительных траекториях из выборки best-of-N достаточно для обучения оптимальной политики в средах с бинарной обратной связью. Для преодоления проблемы разреженных наград применяется модель вознаграждения на уровне токенов. С помощью OREAL модель размером 7B достигает точности 94.0% pass@1 на датасете MATH-500, что сопоставимо с результатами 32B моделей.'}, 'en': {'title': 'OREAL: Advancing AI Reasoning with Outcome-Based Reinforcement Learning', 'desc': 'This paper introduces a new reinforcement learning framework called OREAL, designed to enhance mathematical reasoning capabilities in AI models. OREAL focuses on using binary outcome rewards to improve learning efficiency, particularly in environments where feedback is sparse. The authors demonstrate that behavior cloning from positive examples can effectively learn optimal policies, while also reshaping negative rewards to maintain gradient consistency. The results show that OREAL achieves high accuracy on mathematical tasks, outperforming larger models and highlighting the significance of initial policy models in the training process.'}, 'zh': {'title': 'OREAL：数学推理的新突破', 'desc': '这篇论文提出了一种新的强化学习框架，称为OREAL，旨在提高数学推理任务的性能。OREAL使用基于结果的奖励机制，专注于二元结果奖励，以解决强化学习中的稀疏奖励问题。研究表明，通过对最佳样本进行行为克隆，可以有效学习最优策略，并且需要对负样本的奖励进行重塑以保持梯度一致性。实验结果显示，使用OREAL的7B模型在MATH-500上达到了94.0的准确率，表现与32B模型相当，且OREAL-32B在同一任务上超越了之前的32B模型。'}}}, {'id': 'https://huggingface.co/papers/2502.06060', 'title': 'Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.06060', 'abstract': "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/", 'score': 20, 'issue_id': 2146, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'd235746154e72f16', 'authors': ['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], 'affiliations': ['Stanford University, Stanford, United States of America'], 'pdf_title_img': 'assets/pdf/title_img/2502.06060.jpg', 'data': {'categories': ['#alignment', '#games', '#rlhf', '#agents', '#open_source', '#rl'], 'emoji': '🕵️', 'ru': {'title': 'Естественный язык как инструмент координации ИИ-агентов', 'desc': 'В статье представлен метод обучения языковых моделей вести продуктивные дискуссии в естественной среде без использования демонстраций от людей. Авторы разделяют проблему коммуникации на навыки слушания и говорения, используя цель агента для предсказания полезной информации об окружающей среде в качестве сигнала награды. Метод применяется к социальной игре на дедукцию, основанной на Among Us, где ключевой вопрос - определение личности противника. Результаты показывают, что техника позволяет вести эффективные обсуждения, удваивая показатели выигрыша по сравнению со стандартным обучением с подкреплением.'}, 'en': {'title': 'Empowering Agents with Natural Language Communication for Enhanced Coordination', 'desc': "This paper explores how language models can be trained to communicate effectively in multi-agent environments without relying on human demonstrations. The authors break down communication into two parts: listening and speaking, using the agents' goals as a reward signal to enhance their communication skills. By applying multi-agent reinforcement learning, they improve how agents generate and interpret messages, leading to more productive discussions. The study demonstrates that these enhanced communication strategies significantly increase success rates in a social deduction game, showcasing the importance of effective communication in complex scenarios."}, 'zh': {'title': '自然语言沟通提升多智能体协作', 'desc': '本研究探讨了在多智能体环境中，如何通过自然语言进行有效沟通。我们提出了一种方法，训练语言模型在没有人类示范的情况下，进行关于环境的讨论。通过将沟通问题分解为倾听和发言，我们利用智能体的目标来预测有用的信息，从而引导沟通。实验表明，这种方法在复杂社交场景中显著提高了智能体的胜率，促进了更强的讨论能力。'}}}, {'id': 'https://huggingface.co/papers/2502.05664', 'title': 'CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging', 'url': 'https://huggingface.co/papers/2502.05664', 'abstract': "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).", 'score': 15, 'issue_id': 2152, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '6a6a71a03d5f0d9c', 'authors': ['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], 'affiliations': ['Bangladesh University of Engineering and Technology (BUET)', 'Qatar Computing Research Institute (QCRI)'], 'pdf_title_img': 'assets/pdf/title_img/2502.05664.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#games', '#open_source', '#training'], 'emoji': '🤖', 'ru': {'title': 'CodeSim: Генерация кода на человеческом уровне', 'desc': 'CodeSim - это новая мультиагентная система для генерации кода, использующая подход, имитирующий человеческое восприятие. Она охватывает все этапы синтеза программ: планирование, кодирование и отладку, применяя уникальный метод верификации плана и внутренней отладки через пошаговую симуляцию ввода/вывода. Эксперименты на семи сложных бенчмарках по решению задач и синтезу программ показали выдающиеся результаты CodeSim в генерации кода. Система достигла новых показателей state-of-the-art (pass@1) на нескольких датасетах, включая HumanEval и MBPP.'}, 'en': {'title': 'CodeSim: Revolutionizing Code Generation with Human-like Perception', 'desc': 'This paper presents CodeSim, a new framework for code generation that improves the process of program synthesis by integrating planning, coding, and debugging stages. Unlike traditional methods that rely on external tools for debugging, CodeSim uses a human-like perception approach, allowing for step-by-step simulation of input and output to verify plans and debug internally. The framework has been tested on various competitive benchmarks, achieving state-of-the-art results in code generation tasks. Additionally, CodeSim shows promise for further improvements when combined with existing external debugging tools.'}, 'zh': {'title': 'CodeSim：类人感知的代码生成新框架', 'desc': '大型语言模型（LLMs）在代码生成和问题解决方面取得了显著进展。当前的方法依赖于外部工具的迭代调试器，这些调试器利用编译器或其他工具的运行时反馈来改进粗略的程序生成。然而，这些方法的有效性在很大程度上依赖于初始代码生成的质量，这仍然是一个开放的挑战。本文介绍了CodeSim，一个新颖的多智能体代码生成框架，通过类人感知的方法全面解决程序合成的各个阶段，包括规划、编码和调试。'}}}, {'id': 'https://huggingface.co/papers/2502.06049', 'title': 'LM2: Large Memory Models', 'url': 'https://huggingface.co/papers/2502.06049', 'abstract': 'This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.', 'score': 13, 'issue_id': 2140, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '5f62d7e814a6918f', 'authors': ['Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Alex J. Chan', 'Fraser Greenlee', 'George Thomas', 'Marvin Purtorab', 'Andy Toulis'], 'affiliations': ['Convergence Labs Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2502.06049.jpg', 'data': {'categories': ['#dataset', '#architecture', '#benchmark', '#interpretability', '#long_context', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'LM2: Трансформер с памятью для улучшенных рассуждений', 'desc': 'В статье представлена модель Large Memory Model (LM2), архитектура декодер-трансформер с дополнительным модулем памяти. LM2 решает проблемы стандартных трансформеров в многошаговых рассуждениях и обработке длинных контекстов. Модель показала значительное улучшение производительности на бенчмарке BABILong по сравнению с базовыми моделями. LM2 также продемонстрировала улучшенные возможности в многоходовых выводах, числовых вычислениях и ответах на вопросы с большим контекстом.'}, 'en': {'title': 'Enhancing Transformers with Memory for Superior Reasoning', 'desc': 'The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures.'}, 'zh': {'title': '大型记忆模型：提升Transformer推理能力的关键', 'desc': '本文介绍了一种名为大型记忆模型（LM2）的解码器仅Transformer架构，旨在解决标准Transformer在多步推理、关系论证和长上下文信息综合方面的局限性。LM2引入了一个辅助记忆模块，作为上下文表示的存储库，通过交叉注意力与输入标记交互，并通过门控机制进行更新。实验结果表明，LM2在BABILong基准测试中，平均性能比记忆增强的RMT模型提高了37.1%，比基线Llama-3.2模型提高了86.3%。LM2在多跳推理、数值推理和大上下文问答方面表现出色，证明了显式记忆在增强Transformer架构中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.05415', 'title': 'Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05415', 'abstract': 'There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at https://github.com/zhijie-group/Show-o-Turbo.', 'score': 12, 'issue_id': 2144, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '521adeebda96668f', 'authors': ['Chenkai Xu', 'Xu Wang', 'Zhenyi Liao', 'Yishun Li', 'Tianqi Hou', 'Zhijie Deng'], 'affiliations': ['Huawei', 'Shanghai Jiao Tong University', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05415.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Show-o Turbo: Ускоренная мультимодальная генерация без потери качества', 'desc': 'Статья представляет Show-o Turbo - улучшенную версию мультимодальной модели Show-o для генерации изображений по тексту и текста по изображениям. Авторы предлагают унифицированный подход к шумоподавлению для обоих типов генерации, основанный на параллельном декодировании текстовых токенов. Они применяют метод consistency distillation для ускорения процесса шумоподавления, а также вводят стратегию сегментации траектории и процедуру курикулярного обучения. Эксперименты показывают, что Show-o Turbo превосходит оригинальную модель по скорости и качеству генерации.'}, 'en': {'title': 'Show-o Turbo: Accelerating Multimodal Generation with Unified Denoising', 'desc': 'This paper presents Show-o Turbo, an advanced model for multimodal understanding and generation that improves upon the original Show-o framework. It addresses inefficiencies in the generation process by introducing a unified denoising approach that allows for parallel decoding of text tokens. The authors enhance the training process using consistency distillation and a new trajectory segmentation strategy, which leads to faster convergence. Empirical results show that Show-o Turbo achieves better performance in both text-to-image and image-to-text tasks, significantly reducing the number of sampling steps required for generation.'}, 'zh': {'title': '提升多模态生成效率的Show-o Turbo', 'desc': '本论文介绍了一种新的多模态生成模型Show-o Turbo，旨在提高文本到图像和图像到文本生成的效率。通过并行解码文本标记，Show-o Turbo采用统一的去噪视角，缩短了去噪过程。我们还引入了一种轨迹分割策略和课程学习程序，以改善训练收敛性。实验结果表明，Show-o Turbo在生成图像时的效率显著提高，同时在生成文本时也实现了1.5倍的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2502.05609', 'title': 'Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding', 'url': 'https://huggingface.co/papers/2502.05609', 'abstract': 'Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.', 'score': 12, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '6f559083c224138c', 'authors': ['Sukmin Cho', 'Sangjin Choi', 'Taeho Hwang', 'Jeongyeon Seo', 'Soyeong Jeong', 'Huije Lee', 'Hoyun Song', 'Jong C. Park', 'Youngjin Kwon'], 'affiliations': ['School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.05609.jpg', 'data': {'categories': ['#training', '#architecture', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Иерархическая черновая генерация: новый подход к ускорению вывода в LLM', 'desc': 'Статья представляет новый метод ускорения вывода в больших языковых моделях (LLM) под названием Hierarchy Drafting (HD). HD организует различные источники токенов в иерархические базы данных, основываясь на временной локальности. Метод последовательно обращается к базам данных для получения черновых токенов, обеспечивая стабильное ускорение на различных задачах. Эксперименты показали, что HD превосходит существующие методы черновой генерации, демонстрируя надежное ускорение вывода для моделей разного размера, задач и температур.'}, 'en': {'title': 'Boosting Inference Speed with Hierarchy Drafting in LLMs', 'desc': 'This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks.'}, 'zh': {'title': '层次草拟：加速大型语言模型推理的新方法', 'desc': '加速大型语言模型（LLMs）的推理对于实时交互至关重要。本文提出了一种新的无损草拟方法，称为层次草拟（HD），它通过基于时间局部性的层次框架组织多种令牌源。HD在草拟步骤中依次访问多个数据库，从最高到最低的局部性获取草拟令牌，从而确保在不同任务中一致的加速效果。实验结果表明，HD在推理速度上优于现有的数据库草拟方法，适用于不同规模的模型和任务。'}}}, {'id': 'https://huggingface.co/papers/2502.06772', 'title': 'ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates', 'url': 'https://huggingface.co/papers/2502.06772', 'abstract': 'We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux', 'score': 11, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1ac59597c9610fb2', 'authors': ['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang'], 'affiliations': ['Peking University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06772.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning', '#math', '#benchmark', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Революция в математическом мышлении ИИ: иерархические рассуждения на новом уровне', 'desc': 'Представлена модель ReasonFlux-32B, использующая иерархическое рассуждение с масштабированием шаблонов мышления для оптимизации пространства поиска решений. Модель превосходит математические способности мощных языковых моделей, таких как OpenAI o1-preview и DeepSeek V3. ReasonFlux-32B использует структурированную библиотеку шаблонов мышления и иерархическое обучение с подкреплением для планирования оптимальной траектории шаблонов. На бенчмарке MATH модель достигает точности 91.2%, превосходя o1-preview на 6.7%.'}, 'en': {'title': 'Revolutionizing Math Reasoning with Hierarchical Thought Templates', 'desc': 'This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3.'}, 'zh': {'title': '层次化推理，数学能力新突破', 'desc': '本文提出通过扩展思维模板的层次化大语言模型（LLM）推理，可以有效优化推理搜索空间，并超越强大的LLM如OpenAI o1-preview和DeepSeek V3的数学推理能力。我们训练的ReasonFlux-32B模型仅使用8个GPU，并引入了三项创新：一是构建了一个包含约500个高层次思维模板的结构化通用模板库，能够推广到类似的推理问题；二是对思维模板序列进行层次化强化学习，而不是长链的思维（CoTs），优化基础LLM以规划出处理复杂问题的最佳模板轨迹；三是全新的推理扩展系统，通过在推理时自适应扩展思维模板，实现层次化LLM推理。通过包含顺序思维模板的模板轨迹，ReasonFlux-32B在数学推理能力上显著提升，达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2502.06786', 'title': 'Matryoshka Quantization', 'url': 'https://huggingface.co/papers/2502.06786', 'abstract': 'Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.', 'score': 9, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1126a5fe83c7422d', 'authors': ['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.06786.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🪆', 'ru': {'title': 'MatQuant: Одна модель - множество уровней точности', 'desc': 'MatQuant - это новый метод многомасштабной квантизации для моделей машинного обучения. Он позволяет обучать и обслуживать одну модель, которую затем можно использовать с разными уровнями точности. Благодаря совместному обучению и ко-дистилляции, модели int2, полученные с помощью MatQuant, могут быть до 10% точнее, чем при стандартной квантизации int2. Этот метод значительно улучшает квантизацию моделей, что демонстрируется тем, что модель Gemma-2 9B с квантизацией FFN до int2 оказывается точнее, чем модель Gemma-2 2B с квантизацией FFN до int8.'}, 'en': {'title': 'One Model, Multiple Precision: Revolutionizing Quantization with MatQuant', 'desc': 'This paper introduces Matryoshka Quantization (MatQuant), a new technique for quantizing model weights that allows for multiple precision levels without sacrificing model quality. Traditional low-precision quantization, especially to int2, often leads to significant degradation in performance, forcing practitioners to manage several models. MatQuant leverages the nested structure of integer data types to enable a single model to be trained and served at various precision levels. The method also enhances the accuracy of int2 models by up to 10% compared to standard quantization techniques, showcasing its effectiveness in reducing the need for multiple quantized models.'}, 'zh': {'title': 'Matryoshka量化：单模型多精度服务的创新', 'desc': '量化模型权重对于减少大型模型的通信和推理成本至关重要。然而，将模型量化到低精度（如int4或int2）时，模型质量会受到影响，尤其是int2会显著降低模型性能。为了解决这个问题，本文提出了一种新的多尺度量化技术——Matryoshka量化（MatQuant），它允许只训练和维护一个模型，并在不同精度级别下进行服务。通过MatQuant的共同训练和共同蒸馏正则化，提取的int2精度模型的准确性比标准的int2量化高出10%。'}}}, {'id': 'https://huggingface.co/papers/2502.06788', 'title': 'EVEv2: Improved Baselines for Encoder-Free Vision-Language Models', 'url': 'https://huggingface.co/papers/2502.06788', 'abstract': 'Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.', 'score': 9, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4374edb93ca102c6', 'authors': ['Haiwen Diao', 'Xiaotong Li', 'Yufeng Cui', 'Yueze Wang', 'Haoge Deng', 'Ting Pan', 'Wenxuan Wang', 'Huchuan Lu', 'Xinlong Wang'], 'affiliations': ['BAAI', 'BUPT', 'CASIA', 'DLUT', 'PKU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2502.06788.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#agi', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Революция в мультимодальном обучении: EVEv2.0 - эффективность без энкодеров', 'desc': 'Статья представляет новое семейство моделей машинного обучения EVEv2.0, работающих с изображениями и текстом без использования энкодеров. Авторы систематически исследуют разрыв в производительности между моделями с энкодерами и без них, разрабатывая эффективные стратегии для последних. Они демонстрируют, что правильное разложение и иерархическая ассоциация зрения и языка в единой модели снижает интерференцию между модальностями. EVEv2.0 показывает превосходную эффективность использования данных и сильные способности к визуальному рассуждению.'}, 'en': {'title': 'EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders', 'desc': 'This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities.'}, 'zh': {'title': '无编码器VLM的潜力与创新', 'desc': '本论文探讨了无编码器的视觉-语言模型（VLMs）在性能上与基于编码器的模型之间的差距。我们系统性地分析了使用预训练视觉编码器和简约视觉层的无编码器VLMs的特性。通过开发高效的策略，我们推出了EVEv2.0，一个改进的无编码器VLM系列，展示了其在数据效率和视觉推理能力上的优势。我们的研究表明，合理分解和层次关联视觉与语言可以减少模态之间的干扰，并通过良好的训练策略实现有效优化。'}}}, {'id': 'https://huggingface.co/papers/2502.03628', 'title': 'The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering', 'url': 'https://huggingface.co/papers/2502.03628', 'abstract': 'Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.', 'score': 9, 'issue_id': 2140, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '04b182d80abf9219', 'authors': ['Zhuowei Li', 'Haizhou Shi', 'Yunhe Gao', 'Di Liu', 'Zhenting Wang', 'Yuxiao Chen', 'Ting Liu', 'Long Zhao', 'Hao Wang', 'Dimitris N. Metaxas'], 'affiliations': ['Google DeepMind', 'Rutgers University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03628.jpg', 'data': {'categories': ['#cv', '#training', '#hallucinations', '#benchmark', '#inference', '#interpretability', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в визуально-языковых моделях: метод VISTA', 'desc': 'Эта статья исследует проблему галлюцинаций в крупных визуально-языковых моделях (LVLM) при обработке текстовых и визуальных входных данных. Авторы анализируют внутренние механизмы возникновения галлюцинаций, изучая ранжирование логитов токенов в процессе генерации. На основе выявленных закономерностей предлагается метод VISTA для уменьшения галлюцинаций и усиления достоверной информации во время вывода. Эксперименты показывают, что VISTA в среднем снижает уровень галлюцинаций на 40% в задачах открытой генерации и превосходит существующие методы на четырех бенчмарках.'}, 'en': {'title': 'VISTA: Reducing Hallucination in Vision-Language Models', 'desc': 'This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures.'}, 'zh': {'title': '减少幻觉，提升真实信息的VISTA框架', 'desc': '大型视觉语言模型（LVLMs）能够有效地处理文本和视觉输入，但它们往往会产生语法上连贯但视觉上不真实的内容。本文研究了幻觉的内部动态，发现LVLMs在生成过程中处理信息的三种关键模式：逐渐丧失视觉信息、早期激活和隐藏的真实信息。基于这些发现，我们提出了VISTA（视觉信息引导与标记逻辑增强），这是一种无需训练的推理时干预框架，旨在减少幻觉并促进真实信息的生成。实验表明，VISTA在开放式生成任务中平均减少了约40%的幻觉，并在四个基准测试中在三种解码策略下始终优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06782', 'title': 'Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT', 'url': 'https://huggingface.co/papers/2502.06782', 'abstract': "Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.", 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '3b903654a6ff6710', 'authors': ['Dongyang Liu', 'Shicheng Li', 'Yutong Liu', 'Zhen Li', 'Kai Wang', 'Xinyue Li', 'Qi Qin', 'Yufei Liu', 'Yi Xin', 'Zhongyu Li', 'Bin Fu', 'Chenyang Si', 'Yuewen Cao', 'Conghui He', 'Ziwei Liu', 'Yu Qiao', 'Qibin Hou', 'Hongsheng Li', 'Peng Gao'], 'affiliations': ['Nankai University', 'Shanghai Correspondence AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.06782.jpg', 'data': {'categories': ['#video', '#architecture', '#synthetic', '#diffusion', '#audio', '#training'], 'emoji': '🎬', 'ru': {'title': 'Lumina-Video: новый уровень генерации видео с помощью диффузионных трансформеров', 'desc': 'Статья представляет Lumina-Video - новую архитектуру для генерации видео, основанную на Diffusion Transformers (DiT). Авторы предлагают мультимасштабную архитектуру Next-DiT, которая обучается на нескольких уровнях детализации одновременно. Модель использует условие движения для контроля динамики генерируемого видео. Благодаря прогрессивной схеме обучения и использованию смешанных данных, Lumina-Video достигает высокого качества и плавности движения при эффективном обучении и инференсе.'}, 'en': {'title': 'Revolutionizing Video Generation with Lumina-Video', 'desc': 'This paper introduces Lumina-Video, a new framework for generating videos using the strengths of Diffusion Transformers, specifically the Next-DiT model. It addresses the challenges of capturing the complex movements and changes in video data by using a Multi-scale Next-DiT architecture that learns from different segments of the video. The framework also incorporates a motion score to allow for better control over the dynamics of the generated videos. Additionally, Lumina-Video employs a progressive training approach to improve the quality and smoothness of the output while maintaining efficiency in both training and inference.'}, 'zh': {'title': 'Lumina-Video：高效生成视频的新框架', 'desc': '最近的研究表明，扩散变换器（DiTs）在生成建模中表现出色。基于这一成功，Lumina-Next在生成逼真图像方面取得了卓越的性能，但在视频生成方面仍面临挑战。为了解决这一问题，我们提出了Lumina-Video框架，它结合了Next-DiT的优势，并针对视频合成引入了定制化的解决方案。通过多尺度Next-DiT架构和运动评分的引入，Lumina-Video实现了高效、灵活的视频生成，并在训练和推理效率上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.06764', 'title': 'History-Guided Video Diffusion', 'url': 'https://huggingface.co/papers/2502.06764', 'abstract': 'Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance', 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '66644a3e757a5d21', 'authors': ['Kiwhan Song', 'Boyuan Chen', 'Max Simchowitz', 'Yilun Du', 'Russ Tedrake', 'Vincent Sitzmann'], 'affiliations': ['MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.06764.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Улучшение генерации видео с помощью гибкого обусловливания историей', 'desc': 'Статья представляет новый подход к улучшению генерации видео с помощью диффузионных моделей. Авторы предлагают архитектуру Diffusion Forcing Transformer (DFoT), которая позволяет использовать переменное количество кадров истории для обусловливания генерации. Они также вводят концепцию History Guidance - семейство методов, которые улучшают качество и временную согласованность генерируемого видео. Эксперименты показывают, что предложенный подход значительно улучшает динамику движения и позволяет генерировать очень длинные видео.'}, 'en': {'title': 'Enhancing Video Diffusion with Flexible History Guidance', 'desc': 'This paper introduces the Diffusion Forcing Transformer (DFoT), a novel architecture designed for video diffusion that allows for flexible conditioning on a variable number of context frames. The authors address challenges related to fixed-size conditioning architectures and the inefficacy of traditional classifier-free guidance (CFG) methods when applied to variable-length history. They propose History Guidance, a set of techniques that leverage DFoT to improve video generation quality and temporal consistency. The results demonstrate that these methods enhance motion dynamics and enable the generation of longer videos with better compositional generalization.'}, 'zh': {'title': '灵活历史引导，提升视频生成质量', 'desc': '本论文提出了一种新的视频扩散模型架构，称为Diffusion Forcing Transformer（DFoT），旨在解决在可变长度历史帧条件下进行视频生成的挑战。我们发现，传统的分类器无关引导（CFG）方法在处理可变长度历史时效果不佳，因此我们设计了新的引导方法，称为历史引导。DFoT允许灵活地使用历史帧进行条件生成，从而显著提高视频生成的质量和时间一致性。通过引入更高级的历史引导方法，我们进一步增强了运动动态，并实现了对超出分布历史的组合泛化。'}}}, {'id': 'https://huggingface.co/papers/2502.05957', 'title': 'MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents', 'url': 'https://huggingface.co/papers/2502.05957', 'abstract': "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChain's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", 'score': 7, 'issue_id': 2144, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'f3a18de353dcfad8', 'authors': ['Jiabin Tang', 'Tianyu Fan', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05957.jpg', 'data': {'categories': ['#rag', '#games', '#agents', '#benchmark', '#optimization', '#agi'], 'emoji': '🤖', 'ru': {'title': 'MetaChain: ИИ-агенты для всех без кода', 'desc': 'MetaChain - это полностью автоматизированная и самоуправляемая система для создания агентов на основе больших языковых моделей (LLM) без необходимости программирования. Она состоит из четырех ключевых компонентов: агентных системных утилит, движка действий на основе LLM, самоуправляемой файловой системы и модуля самонастройки агентов. MetaChain позволяет эффективно создавать и модифицировать инструменты, агенты и рабочие процессы без кодирования. Система показала превосходные результаты в задачах многоагентного взаимодействия и генерации с извлечением информации (RAG).'}, 'en': {'title': 'Empowering Everyone to Build LLM Agents with MetaChain', 'desc': 'This paper introduces MetaChain, a framework designed to allow users to create and deploy Large Language Model (LLM) agents using only natural language, eliminating the need for programming skills. MetaChain operates as an autonomous Agent Operating System, featuring components like an Actionable Engine and a Self-Managing File System to facilitate dynamic agent development. The framework addresses the accessibility gap in LLM agent creation, enabling a broader audience to leverage AI technology. Evaluations on the GAIA benchmark indicate that MetaChain outperforms existing methods in multi-agent tasks and demonstrates superior capabilities in Retrieval-Augmented Generation (RAG).'}, 'zh': {'title': '让每个人都能用自然语言构建智能代理', 'desc': '大型语言模型（LLM）代理在任务自动化和智能决策方面表现出色，但现有的开发框架主要面向技术背景深厚的开发者，限制了普通用户的使用。为了解决这一问题，我们提出了MetaChain，一个完全自动化且高度自我发展的框架，允许用户仅通过自然语言创建和部署LLM代理。MetaChain作为一个自主代理操作系统，包含四个关键组件，能够高效动态地创建和修改工具、代理和工作流程，而无需编写代码。经过GAIA基准的全面评估，MetaChain在通用多代理任务中表现优于现有的最先进方法，展现了其强大的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.06023', 'title': 'Dual Caption Preference Optimization for Diffusion Models', 'url': 'https://huggingface.co/papers/2502.06023', 'abstract': "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.", 'score': 6, 'issue_id': 2141, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '07782353b3b8b697', 'authors': ['Amir Saeidi', 'Yiran Luo', 'Agneet Chatterjee', 'Shamanthak Hegde', 'Bimsara Pathiraja', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06023.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#dataset', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Двойные подписи для улучшения генерации изображений по текстовым описаниям', 'desc': 'Эта статья описывает новый метод под названием Dual Caption Preference Optimization (DCPO) для улучшения моделей диффузии текст-в-изображение. DCPO использует два отдельных описания для решения проблемы нерелевантных промптов и конфликтующих распределений в наборах данных предпочтений. Авторы также представляют новый датасет Pick-Double Caption с отдельными подписями для предпочтительных и менее предпочтительных изображений. Эксперименты показывают, что DCPO значительно улучшает качество изображений и их соответствие промптам по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Image Generation with Dual Captions!', 'desc': 'This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics.'}, 'zh': {'title': '双重标题优化，提升图像质量！', 'desc': '最近在大型语言模型（LLMs）中发展的人类偏好优化技术，显示出在改进文本到图像扩散模型方面的巨大潜力。这些方法旨在学习偏好样本的分布，并将其与不太偏好的样本区分开来。然而，现有的偏好数据集通常存在分布重叠的问题，导致冲突分布。此外，我们发现输入提示中包含与不太偏好的图像无关的信息，这限制了去噪网络在偏好优化方法中的准确预测能力。为了解决这些挑战，我们提出了双重标题偏好优化（DCPO），利用两个不同的标题来减轻无关提示的问题。'}}}, {'id': 'https://huggingface.co/papers/2502.06155', 'title': 'Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile', 'url': 'https://huggingface.co/papers/2502.06155', 'abstract': 'Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.', 'score': 6, 'issue_id': 2140, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2f8d5e54db328d39', 'authors': ['Hangliang Ding', 'Dacheng Li', 'Runlong Su', 'Peiyuan Zhang', 'Zhijie Deng', 'Ion Stoica', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.06155.jpg', 'data': {'categories': ['#training', '#diffusion', '#optimization', '#inference', '#video'], 'emoji': '🎬', 'ru': {'title': 'Ускорение генерации видео: эффективность без потери качества', 'desc': 'Статья представляет новый подход к ускорению генерации видео с помощью Диффузионных Трансформеров (DiTs). Авторы предлагают прореживание 3D-внимания на основе избыточности видеоданных и сокращение процесса сэмплирования с помощью многошаговой дистилляции согласованности. Разработан трехэтапный процесс обучения для объединения внимания с низкой сложностью и возможностей генерации за несколько шагов. Результаты показывают ускорение в 7.4-7.8 раз для генерации видео 720p с 29 и 93 кадрами при использовании всего 0.1% данных предобучения.'}, 'en': {'title': 'Speeding Up Video Generation with Efficient Attention Mechanisms', 'desc': 'This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs.'}, 'zh': {'title': '高效视频生成的新方法', 'desc': '本论文提出了一种改进的Diffusion Transformers（DiTs）模型，以解决生成高保真视频时的效率问题。我们通过识别视频数据中的冗余，提出了一种稀疏的3D注意力机制，使其在视频帧数量上具有线性复杂度。其次，我们采用多步一致性蒸馏技术，缩短了采样过程，从而实现了更快速的视频生成。最终，我们的模型在使用极少的预训练数据时，生成速度提高了7.4到7.8倍，同时保持了良好的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.05795', 'title': 'The Curse of Depth in Large Language Models', 'url': 'https://huggingface.co/papers/2502.05795', 'abstract': 'In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.', 'score': 5, 'issue_id': 2147, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '3b1a3626926ac2f4', 'authors': ['Wenfang Sun', 'Xinyuan Song', 'Pengxiang Li', 'Lu Yin', 'Yefeng Zheng', 'Shiwei Liu'], 'affiliations': ['Dalian University of Technology, China', 'Emory University, USA', 'Medical Artificial Intelligence Laboratory, Westlake University, China', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.05795.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': "Преодоление 'Проклятия глубины' в больших языковых моделях", 'desc': "В статье представлена концепция 'Проклятия глубины', объясняющая низкую эффективность почти половины слоев в современных больших языковых моделях (LLM). Авторы подтверждают широкое распространение этого явления среди популярных семейств LLM, таких как Llama, Mistral, DeepSeek и Qwen. Анализ показывает, что причиной неэффективности глубоких слоев является использование предварительной нормализации слоев (Pre-LN). Для решения этой проблемы предлагается метод масштабирования LayerNorm, который улучшает вклад глубоких слоев в обучение модели."}, 'en': {'title': 'Unlocking the Power of Deep Layers in LLMs', 'desc': "This paper introduces the 'Curse of Depth', which describes a problem in Large Language Models (LLMs) where many layers do not perform as well as expected. The authors find that this issue is common in popular LLMs like Llama and Mistral, and it stems from the use of Pre-Layer Normalization (Pre-LN). Pre-LN helps stabilize training but leads to increased output variance in deeper layers, making them less effective. To address this, the authors propose LayerNorm Scaling, which reduces the output variance of deeper layers, resulting in improved training performance and better contributions from these layers."}, 'zh': {'title': '解决深度模型的训练困境', 'desc': '本文介绍了深度诅咒的概念，强调了现代大型语言模型（LLMs）中近一半层的效果低于预期的现象。我们确认了这一现象在流行的LLM家族中普遍存在，如Llama、Mistral、DeepSeek和Qwen。分析表明，深层无效的根本原因是广泛使用的预层归一化（Pre-LN），它导致输出方差随着模型深度的增加而指数增长。为了解决这个问题，我们提出了层归一化缩放（LayerNorm Scaling），通过对层归一化的输出方差进行缩放，显著提高了深层的贡献。'}}}, {'id': 'https://huggingface.co/papers/2502.06527', 'title': 'CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.06527', 'abstract': 'Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.', 'score': 5, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '44b3a6931980556a', 'authors': ['D. She', 'Mushui Liu', 'Jingxuan Pang', 'Jin Wang', 'Zhen Yang', 'Wanggui He', 'Guanghao Zhang', 'Yi Wang', 'Qihan Huang', 'Haobin Tang', 'Yunlong Yu', 'Siming Fu'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China', 'Zhejiang Univerisity'], 'pdf_title_img': 'assets/pdf/title_img/2502.06527.jpg', 'data': {'categories': ['#diffusion', '#video', '#benchmark', '#3d'], 'emoji': '🎬', 'ru': {'title': 'CustomVideoX: Персонализированная генерация видео нового уровня', 'desc': 'CustomVideoX - это инновационная система для персонализированной генерации видео на основе референсного изображения, использующая видео-диффузионный трансформер. Система применяет предобученные видеосети и обучает только параметры LoRA для извлечения признаков из референса, что обеспечивает эффективность и адаптивность. Предложенное 3D Reference Attention позволяет взаимодействовать признакам референсного изображения со всеми кадрами видео в пространственном и временном измерениях. Для улучшения качества генерации используются стратегии Time-Aware Reference Attention Bias и Entity Region-Aware Enhancement.'}, 'en': {'title': 'Revolutionizing Personalized Video Generation with CustomVideoX', 'desc': 'This paper presents CustomVideoX, a new framework designed for generating personalized videos from a reference image. It utilizes a video diffusion transformer and focuses on training LoRA parameters to efficiently extract features from the reference image. The framework introduces 3D Reference Attention to enhance the interaction between the reference image and video frames, addressing temporal inconsistencies. Additionally, it employs the Time-Aware Reference Attention Bias and the Entity Region-Aware Enhancement modules to improve video quality and consistency, validated by a new benchmark called VideoBench.'}, 'zh': {'title': '个性化视频生成的新突破', 'desc': '个性化视频生成在图像合成领域取得了显著进展，但由于时间不一致性和质量下降，仍然面临挑战。本文提出了CustomVideoX，一个创新框架，利用视频扩散变换器从参考图像生成个性化视频。CustomVideoX通过专门训练LoRA参数来提取参考特征，确保了效率和适应性。我们还提出了3D参考注意力机制，以便在空间和时间维度上直接和同时地将参考图像特征与所有视频帧进行交互。'}}}, {'id': 'https://huggingface.co/papers/2502.05431', 'title': 'APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding', 'url': 'https://huggingface.co/papers/2502.05431', 'abstract': "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.", 'score': 5, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '7bc5b7aeb3716893', 'authors': ['Xinyu Yang', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2502.05431.jpg', 'data': {'categories': ['#rag', '#optimization', '#inference', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение генерации с контекстом: адаптивное параллельное кодирование', 'desc': 'Статья представляет новый метод адаптивного параллельного кодирования (APE) для эффективной обработки множественных контекстов в задачах генерации с использованием контекста. APE позволяет предварительно вычислять и кэшировать KV-состояния каждого контекста независимо, что значительно ускоряет процесс обработки запросов. Метод решает проблему несоответствия распределения внимания при параллельном кодировании, используя общий префикс, температуру внимания и масштабирующий фактор. Эксперименты показывают, что APE сохраняет до 98% производительности последовательного кодирования, превосходя обычное параллельное кодирование на 3.6-7.9% в задачах RAG и ICL.'}, 'en': {'title': 'Boosting Efficiency in Context-Augmented Generation with APE', 'desc': 'This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently.'}, 'zh': {'title': '自适应并行编码：提升上下文生成效率的关键', 'desc': '本文探讨了上下文增强生成（CAG）技术中的并行编码方法，以提高生成用户查询响应的效率。传统方法在每次请求时都需要重新编码多个上下文，导致计算负担过重。我们提出了自适应并行编码（APE），通过共享前缀、注意力温度和缩放因子来调整并行编码与顺序编码的注意力分布，从而提高性能。实验结果表明，APE在保持高性能的同时，能够显著加快处理速度，适用于处理大量上下文。'}}}, {'id': 'https://huggingface.co/papers/2502.04370', 'title': 'DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04370', 'abstract': 'Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.', 'score': 4, 'issue_id': 2145, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a475f5281f318a1e', 'authors': ['Zhenglin Zhou', 'Xiaobo Xia', 'Fan Ma', 'Hehe Fan', 'Yi Yang', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04370.jpg', 'data': {'categories': ['#training', '#optimization', '#3d', '#alignment', '#open_source', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Создание 3D-контента с учетом человеческих предпочтений', 'desc': 'DreamDPO - это новый подход к генерации 3D-контента на основе текстовых описаний, который учитывает предпочтения человека. Метод использует попарное сравнение сгенерированных образцов для оценки их соответствия предпочтениям с помощью моделей вознаграждения или мультимодальных языковых моделей. DreamDPO оптимизирует 3D-представление с использованием функции потерь, ориентированной на предпочтения. Эксперименты показывают, что DreamDPO обеспечивает более качественный и контролируемый 3D-контент по сравнению с существующими методами.'}, 'en': {'title': 'DreamDPO: Aligning 3D Generation with Human Preferences', 'desc': 'This paper introduces DreamDPO, a new framework for generating 3D content from text that incorporates human preferences. It uses an optimization approach that focuses on pairwise comparisons to better align the generated 3D models with what people actually want. By employing a preference-driven loss function, DreamDPO enhances the quality and control of the generated content without needing exact quality scores. The results show that DreamDPO outperforms existing methods, making it a significant advancement in text-to-3D generation.'}, 'zh': {'title': 'DreamDPO：将人类偏好融入3D生成的创新框架', 'desc': '文本到3D生成技术可以根据文本描述自动创建3D内容，具有广泛的应用潜力。然而，现有方法在生成内容与人类偏好之间的对齐上存在困难，限制了其适用性和灵活性。为了解决这些问题，本文提出了DreamDPO，一个基于优化的框架，通过直接偏好优化将人类偏好融入3D生成过程中。实验表明，DreamDPO在生成高质量和可控的3D内容方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06635', 'title': 'Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM', 'url': 'https://huggingface.co/papers/2502.06635', 'abstract': "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.", 'score': 4, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2deb5075264d7660', 'authors': ['Qingshui Gu', 'Shu Li', 'Tianyu Zheng', 'Zhaoxiang Zhang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute of Automation, Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06635.jpg', 'data': {'categories': ['#small_models', '#multilingual', '#training', '#data', '#benchmark', '#open_source', '#dataset', '#low_resource'], 'emoji': '🇨🇳', 'ru': {'title': 'Создание эффективной китайскоязычной LLM с открытым исходным кодом', 'desc': 'Steel-LLM - это языковая модель, ориентированная на китайский язык, разработанная с нуля при ограниченных вычислительных ресурсах. Модель с 1 миллиардом параметров была обучена на крупномасштабном наборе данных, в основном на китайском языке. Steel-LLM показала конкурентоспособную производительность на бенчмарках CEVAL и CMMLU, превзойдя ранние модели от более крупных институтов. Статья предоставляет подробный отчет о процессе разработки, включая сбор данных, дизайн модели и методологии обучения.'}, 'en': {'title': 'Empowering Chinese NLP with Steel-LLM: Open-Source Innovation', 'desc': "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."}, 'zh': {'title': '打造中文优质开源语言模型的探索', 'desc': 'Steel-LLM是一个以中文为中心的语言模型，旨在在有限的计算资源下开发出高质量的开源模型。该项目于2024年3月启动，训练了一个拥有10亿参数的大规模模型，重点关注透明度和实用见解的分享。训练过程中主要使用中文数据，并适量包含英文数据，填补了现有开源大语言模型的空白。Steel-LLM在CEVAL和CMMLU等基准测试中表现出色，超越了大型机构的早期模型，为研究人员和实践者提供了宝贵的资源。'}}}, {'id': 'https://huggingface.co/papers/2502.06776', 'title': 'Towards Internet-Scale Training For Agents', 'url': 'https://huggingface.co/papers/2502.06776', 'abstract': 'The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: data-for-agents.github.io.', 'score': 1, 'issue_id': 2157, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '5ca8acf0bf4a0a58', 'authors': ['Brandon Trabucco', 'Gunnar Sigurdsson', 'Robinson Piramuthu', 'Ruslan Salakhutdinov'], 'affiliations': ['Amazon', 'Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06776.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#data', '#open_source', '#training'], 'emoji': '🌐', 'ru': {'title': 'Автоматизация обучения веб-агентов: от человеческих аннотаций к языковым моделям', 'desc': 'Статья представляет новый подход к обучению агентов веб-навигации без использования человеческих аннотаций. Авторы разработали конвейер, в котором языковая модель (ЛЯМ) генерирует задачи для 150 тысяч сайтов, агенты на основе ЛЯМ выполняют эти задачи, а затем другая ЛЯМ оценивает успешность выполнения. Результаты показывают, что языковые модели конкурентоспособны с человеческими аннотаторами в генерации задач и оценке траекторий. Обучение на данных, сгенерированных этим конвейером, сопоставимо с обучением на человеческих демонстрациях и значительно улучшает обобщающую способность агентов.'}, 'en': {'title': 'Automating Web Navigation Agent Training with LLMs', 'desc': "This paper presents a new method for training web navigation agents that reduces reliance on human-generated data. It introduces a pipeline where a large language model (LLM) creates tasks for a vast number of websites, allowing agents to learn from these automatically generated tasks. The LLM also evaluates the agents' performance, achieving high accuracy in detecting harmful content and judging task success. The results show that training with this pipeline can significantly enhance the agents' ability to generalize across diverse websites compared to traditional human data methods."}, 'zh': {'title': '无须人工标注，智能代理自我训练！', 'desc': '这篇论文提出了一种新的方法来训练网络导航代理，避免了繁琐的人类标注。首先，使用大型语言模型（LLM）为150,000个不同的网站生成任务。接着，LLM代理完成这些任务并生成轨迹，最后再由LLM对轨迹进行评估。实验结果表明，基于我们的方法训练的代理在多样化网站上表现优异，且在数据有限的情况下，能够显著提高任务的准确性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.18676', 'title': 'Embodied Red Teaming for Auditing Robotic Foundation Models', 'url': 'https://huggingface.co/papers/2411.18676', 'abstract': 'Language-conditioned robot models have the potential to enable robots to perform a wide range of tasks based on natural language instructions. However, assessing their safety and effectiveness remains challenging because it is difficult to test all the different ways a single task can be phrased. Current benchmarks have two key limitations: they rely on a limited set of human-generated instructions, missing many challenging cases, and focus only on task performance without assessing safety, such as avoiding damage. To address these gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method that generates diverse and challenging instructions to test these models. ERT uses automated red teaming techniques with Vision Language Models (VLMs) to create contextually grounded, difficult instructions. Experimental results show that state-of-the-art language-conditioned robot models fail or behave unsafely on ERT-generated instructions, underscoring the shortcomings of current benchmarks in evaluating real-world performance and safety. Code and videos are available at: https://s-karnik.github.io/embodied-red-team-project-page.', 'score': 0, 'issue_id': 2154, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '8f3c4c8885d5b2d0', 'authors': ['Sathwik Karnik', 'Zhang-Wei Hong', 'Nishant Abhangi', 'Yen-Chen Lin', 'Tsun-Hsuan Wang', 'Christophe Dupuy', 'Rahul Gupta', 'Pulkit Agrawal'], 'affiliations': ['Amazon', 'Improbable AI Lab', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2411.18676.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#alignment', '#security', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Новый метод тестирования безопасности языковых моделей для роботов', 'desc': 'Статья представляет новый метод оценки языковых моделей для роботов, называемый Embodied Red Teaming (ERT). ERT использует автоматизированные техники red teaming вместе с Vision Language Models для создания сложных и разнообразных инструкций. Эксперименты показали, что современные языковые модели для роботов часто не справляются с инструкциями, сгенерированными ERT. Это подчеркивает ограниченность существующих методов оценки в отношении реальной производительности и безопасности таких моделей.'}, 'en': {'title': 'Enhancing Robot Safety with Embodied Red Teaming', 'desc': 'This paper introduces a new evaluation method called Embodied Red Teaming (ERT) for assessing language-conditioned robot models. ERT generates diverse and challenging instructions using automated red teaming techniques combined with Vision Language Models (VLMs). The study reveals that existing benchmarks are inadequate, as they do not cover a wide range of task phrasings and fail to evaluate safety measures. Experimental results indicate that current state-of-the-art models often fail or act unsafely when faced with ERT-generated instructions, highlighting the need for improved evaluation methods.'}, 'zh': {'title': '具身红队评估：提升机器人安全性与有效性的新方法', 'desc': '本文介绍了一种新的评估方法，称为具身红队评估（ERT），旨在测试语言条件机器人模型的安全性和有效性。当前的评估基准存在局限性，主要依赖于有限的人类生成指令，且未能考虑安全性问题。ERT通过自动化红队技术与视觉语言模型（VLMs）结合，生成多样且具有挑战性的指令，以更全面地评估机器人模型。实验结果表明，现有的先进语言条件机器人模型在ERT生成的指令上表现不佳或不安全，突显了当前评估基准在真实世界性能和安全性评估中的不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 79, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Прямое выравнивание: простой путь к улучшению языковых моделей', 'desc': 'В статье рассматриваются алгоритмы прямого выравнивания (DAA) как альтернатива обучению с подкреплением в контексте выравнивания языковых моделей. Авторы классифицируют DAA по типу функции потерь, используемым наградам и необходимости предварительного обучения. Исследование показывает, что двухэтапные методы превосходят одноэтапные, а ключевым фактором эффективности является использование попарных, а не поточечных целевых функций. Результаты подчеркивают важность тщательной оценки при сравнении алгоритмов выравнивания языковых моделей.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': '优化对齐算法，提升模型性能！', 'desc': '直接对齐算法（DAAs）通过直接优化策略来简化语言模型的对齐，取代了人类反馈强化学习中的强化学习和奖励建模。DAAs可以根据其排名损失（成对与点对）和使用的奖励类型进行分类。研究表明，一阶段方法的表现不如两阶段方法，因此我们引入了显式的监督微调阶段，并在单阶段的ORPO和ASFT中加入了控制偏好优化强度的beta参数。这些改进使得它们在Alpaca Eval 2中的表现得到了显著提升，表明选择成对或点对目标是关键因素，而不是具体的隐式奖励或损失函数。'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 74, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'OmniHuman: универсальная модель для генерации реалистичных видео с людьми', 'desc': 'OmniHuman - это новая модель на основе Diffusion Transformer для генерации реалистичных видео с людьми. Она использует смешанные условия движения при обучении, что позволяет масштабировать данные и улучшить качество генерации. Модель поддерживает различные типы портретов, взаимодействие с объектами и сложные позы тела. OmniHuman может генерировать видео на основе аудио, видео или комбинированных сигналов управления.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHuman：灵活真实的人类动画生成', 'desc': '本文提出了一种名为OmniHuman的框架，旨在提升人类动画生成的质量和灵活性。该框架基于扩散变换器，通过在训练阶段混合与运动相关的条件来扩展数据规模。OmniHuman支持多种人像内容和不同的驱动模式，如音频驱动和视频驱动，能够生成高度真实的人类视频。与现有方法相比，OmniHuman不仅生成更真实的视频，还提供了更大的输入灵活性。'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 41, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'PRIME: Эффективное обучение ИИ-моделей с неявными процессными наградами', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей, называемый PRIME. Он использует неявные процессные награды, что позволяет обновлять модели вознаграждения процесса в режиме онлайн, используя только развертывания политики и метки результатов. PRIME решает проблемы обучения с плотными наградами, такие как уязвимость к взлому наград и высокая стоимость сбора качественных меток процесса. Метод показал значительное улучшение производительности на задачах рассуждения по сравнению с базовыми моделями, используя при этом меньше данных для обучения.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIME：提升大语言模型推理效率的新方法', 'desc': '本文提出了一种新的方法PRIME（通过隐式奖励进行过程强化学习），旨在解决大语言模型（LLMs）在复杂多步骤推理任务中的训练效率问题。传统的稀疏结果奖励在训练过程中存在效率低下和信用分配等问题，而PRIME通过仅使用策略回滚和结果标签来实现在线过程奖励模型（PRM）的更新。该方法避免了现有方法中需要的专门奖励模型训练阶段，从而显著降低了开发成本。实验结果表明，PRIME在数学和编码竞赛任务中表现出色，相较于传统模型有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 24, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 января', 'en': 'January 28', 'zh': '1月28日'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'SafeRAG: оценка уязвимостей генерации с дополнением на основе извлечения', 'desc': 'В статье представлен бенчмарк SafeRAG для оценки безопасности генерации с дополнением на основе извлечения (RAG). Авторы классифицируют атаки на RAG и создают набор данных для их моделирования. Эксперименты показывают, что RAG уязвим ко всем типам атак, даже самым очевидным. Исследование демонстрирует, что существующие компоненты RAG не способны эффективно противостоять атакам, что приводит к снижению качества работы системы.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': '提升RAG安全性，抵御知识攻击！', 'desc': '本文介绍了一种名为SafeRAG的基准，用于评估检索增强生成（RAG）模型的安全性。我们将攻击任务分为银噪声、上下文冲突、软广告和拒绝服务等类型，并为每种任务手动构建了安全评估数据集。通过使用SafeRAG数据集，我们模拟了RAG可能遇到的各种攻击场景。实验结果表明，RAG对所有攻击任务表现出显著的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器和过滤器，导致RAG服务质量下降。'}}}, {'id': 'https://huggingface.co/papers/2502.01341', 'title': 'AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2502.01341', 'abstract': 'Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.', 'score': 23, 'issue_id': 2030, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '88ea73fcb0da69ba', 'authors': ['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'Ecole de Technologie Superieure', 'McGill University', 'Mila', 'Polytechnique Montreal', 'ServiceNow', 'Universite de Montreal', 'University of British Columbia', 'University of Waterloo', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01341.jpg', 'data': {'categories': ['#alignment', '#cv', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Улучшение выравнивания модальностей в мультимодальных моделях', 'desc': 'AlignVLM - это новый метод выравнивания визуальных и текстовых признаков в мультимодальных моделях. Он отображает визуальные признаки в виде взвешенного среднего текстовых эмбеддингов языковой модели. Этот подход использует лингвистические приоры, закодированные в языковой модели, чтобы обеспечить эффективную интерпретацию визуальных признаков. AlignVLM особенно эффективен для задач понимания документов и демонстрирует улучшенное выравнивание признаков и устойчивость к шуму.'}, 'en': {'title': 'Aligning Vision and Language for Better Understanding', 'desc': 'This paper addresses the challenge of aligning visual features with language embeddings in vision-language models (VLMs). The authors introduce AlignVLM, a new method that connects visual features to a weighted average of language model (LLM) text embeddings, enhancing semantic similarity. By utilizing the linguistic knowledge embedded in the LLM, AlignVLM ensures that visual features are accurately represented in a way that the LLM can understand. The results show that AlignVLM outperforms existing methods, particularly in tasks involving document understanding, while also demonstrating improved robustness to noisy inputs.'}, 'zh': {'title': '视觉与语言的完美对齐', 'desc': '在视觉语言模型（VLMs）中，将视觉特征与语言嵌入对齐是一个关键挑战。现有的连接器，如多层感知器（MLP），常常会产生分布外或噪声输入，导致模态之间的不对齐。我们提出了一种新颖的视觉文本对齐方法AlignVLM，它将视觉特征映射到LLM文本嵌入的加权平均值。AlignVLM在文档理解任务中表现尤为出色，能够有效提高视觉特征与文本内容的对齐和抗噪声能力。'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 22, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'Осторожно: LLM-судьи могут быть предвзяты!', 'desc': "Исследование выявляет проблему 'утечки предпочтений' при использовании больших языковых моделей (LLM) в качестве судей для оценки других моделей. Эта проблема возникает из-за связанности между генераторами синтетических данных и LLM-оценщиками. Эксперименты подтверждают предвзятость судей к связанным с ними моделям-ученикам на различных базовых моделях и эталонных тестах. Результаты указывают на то, что утечка предпочтений является распространенной и трудно обнаруживаемой проблемой в области использования LLM в качестве судей."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': '偏好泄漏：LLM评判中的隐患', 'desc': '本文探讨了大型语言模型（LLM）作为评判者和基于LLM的数据合成在模型开发中的应用。我们揭示了偏好泄漏这一问题，它是由合成数据生成器与LLM评估者之间的相关性引起的。通过定义三种常见的相关性，我们进行了广泛的实验，证实了评判者对其相关学生模型的偏见。研究表明，偏好泄漏是一个普遍存在且难以检测的问题，影响了LLM作为评判者的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 15, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': '🎚️', 'ru': {'title': 'SliderSpace: Раскрытие скрытых возможностей диффузионных моделей', 'desc': 'SliderSpace - это фреймворк для автоматической декомпозиции визуальных возможностей диффузионных моделей на управляемые и понятные человеку направления. Он обнаруживает множество интерпретируемых и разнообразных направлений одновременно из одного текстового запроса. Каждое направление обучается как адаптер низкого ранга, что позволяет осуществлять композиционный контроль. Эксперименты показывают эффективность SliderSpace в различных приложениях, включая декомпозицию концепций и исследование художественных стилей.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpace：可控的视觉能力分解', 'desc': 'SliderSpace是一个框架，用于自动分解扩散模型的视觉能力，使其可控且易于理解。与现有方法不同，SliderSpace可以从单个文本提示中同时发现多个可解释和多样化的方向，而无需用户逐个指定属性。每个方向被训练为低秩适配器，从而实现组合控制，并在模型的潜在空间中发现意想不到的可能性。通过对最先进的扩散模型进行广泛实验，我们证明了SliderSpace在概念分解、艺术风格探索和多样性增强等三个应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 13, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальный ИИ проваливает тест на интеллект', 'desc': 'Статья представляет новый фреймворк MM-IQ для оценки когнитивных способностей мультимодальных систем искусственного интеллекта. Фреймворк включает 2,710 тестовых заданий по 8 типам рассуждений, аналогично тестам IQ для людей. Результаты показали, что даже передовые мультимодальные модели демонстрируют точность лишь немного выше случайного угадывания (27.49% против 25%). Это подчеркивает существенный разрыв между текущими возможностями ИИ и базовыми когнитивными способностями человека.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQ：评估多模态系统的认知能力新标准', 'desc': '本论文提出了一种名为MM-IQ的综合评估框架，用于量化多模态系统中的认知能力。该框架包含2710个精心策划的测试项目，涵盖8种不同的推理范式。通过对领先的多模态模型进行系统评估，结果显示即使是最先进的架构，其表现也仅略高于随机猜测。这个显著的性能差距表明当前多模态系统在接近人类基本推理能力方面的不足，强调了需要进行范式转变的进步。'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 12, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'FastKV: Ускорение обработки длинных последовательностей в LLM', 'desc': 'Статья представляет FastKV - новый метод сжатия кэша ключ-значение (KV) для больших языковых моделей (LLM), направленный на улучшение латентности при обработке длинных последовательностей. FastKV использует подход Token-Selective Propagation (TSP), который сохраняет полную контекстную информацию в начальных слоях LLM и выборочно распространяет только часть этой информации в более глубоких слоях. Метод также включает сжатие кэша KV с учетом grouped-query attention (GQA) для повышения эффективности памяти и вычислений. Эксперименты показывают, что FastKV достигает значительного улучшения времени до первого токена и пропускной способности по сравнению с современными методами, сохраняя при этом точность на уровне базовых показателей.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKV：提升长上下文处理速度的创新方法', 'desc': '本文介绍了一种名为FastKV的KV缓存压缩方法，旨在提高长上下文序列的处理速度。FastKV采用了一种新颖的选择性传播方法（TSP），在LLM的初始层保留完整的上下文信息，而在更深层次中仅选择性传播部分信息。该方法还结合了分组查询注意力（GQA）来优化内存和计算效率。实验结果表明，FastKV在首次令牌时间和吞吐量方面分别比现有的HeadKV方法提高了2.00倍和1.40倍，同时在长上下文基准测试中保持了与基线相当的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 12, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'Linköping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'AIN: Прорыв в арабоязычном мультимодальном ИИ', 'desc': 'Представлена модель AIN - двуязычная мультимодальная языковая модель для арабского и английского языков. Модель обучена на 3,6 миллионах высококачественных мультимодальных арабско-английских образцов данных. AIN демонстрирует передовые результаты в арабском языке, сохраняя при этом сильные визуальные возможности для английского. На бенчмарке CAMEL-Bench, охватывающем 38 поддоменов, 7B-версия AIN превосходит GPT-4o на 3,4% в среднем по восьми доменам.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': '推动阿拉伯语多模态AI的进步', 'desc': '随着大型语言模型（LLMs）和多模态模型（LMMs）的快速发展，阿拉伯语的研究仍然相对滞后。我们提出了AIN模型，这是一个旨在提升阿拉伯语和英语的多模态模型，利用了360万高质量的阿拉伯语-英语多模态数据样本。AIN在多个领域表现出色，尤其是在复杂的视觉理解和多图像理解方面，超越了现有的GPT-4o模型。该模型的优越性能为阿拉伯语使用者提供了先进的多模态生成AI工具，推动了相关应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 9, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DeepRAG: умное сочетание поиска и рассуждений для ИИ', 'desc': 'DeepRAG - это новый фреймворк, моделирующий рассуждения с поиском информации как марковский процесс принятия решений. Он итеративно декомпозирует запросы, динамически определяя необходимость внешнего поиска или параметрического рассуждения на каждом шаге. Эксперименты показывают, что DeepRAG повышает эффективность поиска и точность ответов на 21.99%. Фреймворк решает проблемы больших языковых моделей, связанные с фактическими галлюцинациями и неэффективной декомпозицией задач при интеграции рассуждений с поиском.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAG：优化检索增强推理的新框架', 'desc': '大型语言模型（LLMs）在推理方面展现了显著的潜力，但仍然面临严重的事实幻觉问题，主要由于参数知识的时效性、准确性和覆盖率不足。将推理与检索增强生成（RAG）结合起来仍然具有挑战性，主要是因为任务分解不有效和冗余检索可能引入噪声，降低响应质量。本文提出了DeepRAG框架，将检索增强推理建模为马尔可夫决策过程（MDP），实现了战略性和自适应的检索。实验表明，DeepRAG在提高检索效率的同时，答案准确性提高了21.99%，证明了其在优化检索增强推理方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без увеличения вычислительных затрат', 'desc': 'SCONE - это новый метод расширения слоёв входных эмбеддингов для улучшения производительности языковых моделей при увеличении размера слоя. Он вводит эмбеддинги для частых n-грамм, обеспечивая контекстуализированное представление для каждого входного токена. Эти эмбеддинги обучаются отдельной моделью и предварительно вычисляются для использования во время инференса. SCONE позволяет масштабировать количество кэшированных n-граммных эмбеддингов и модель для их обучения, сохраняя фиксированное количество FLOPS при инференсе.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONE：提升语言模型性能的新方法', 'desc': '我们提出了一种方法SCONE（可扩展的上下文化的离线N-gram嵌入），旨在通过扩展输入嵌入层来提升语言模型的性能。SCONE在保持原有词汇的同时，引入了一组常见n-gram的嵌入，以提供每个输入标记的上下文化表示。这些嵌入在训练过程中由一个单独的模型学习，并在推理时预先计算并存储在离线加速器内存中，几乎不影响推理速度。通过增加缓存的n-gram嵌入数量和扩展学习它们的模型，SCONE在多种语料库上超越了1.9B参数的基线，同时仅使用一半的推理时间FLOPS。'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 8, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Проклятие сложности в логическом мышлении LLM', 'desc': 'В статье исследуются возможности логического рассуждения больших языковых моделей (LLM) и их масштабируемость в сложных задачах немонотонного вывода. Для этого представлена ZebraLogic, комплексная система оценки производительности LLM на логических головоломках, основанных на задачах удовлетворения ограничений (CSP). ZebraLogic позволяет генерировать головоломки с контролируемой сложностью, что помогает систематически изучать пределы масштабируемости моделей, таких как Llama и DeepSeek-R1. Результаты показывают значительное снижение точности по мере увеличения сложности задач, что авторы называют "проклятием сложности".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': '揭示大型语言模型推理能力的复杂性挑战', 'desc': '本文研究了大型语言模型（LLMs）的逻辑推理能力及其在复杂非单调推理中的可扩展性。我们引入了ZebraLogic，一个全面的评估框架，用于评估LLM在基于约束满足问题（CSPs）的逻辑网格谜题上的推理表现。研究结果显示，随着问题复杂性的增加，模型的准确性显著下降，这一现象被称为复杂性诅咒。我们还探讨了增强逻辑推理的策略，包括最佳采样、回溯机制和自我验证提示。'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эволюция рассуждений: от символов к мультимодальности', 'desc': 'Статья рассматривает эволюцию возможностей рассуждения в мультимодальных задачах у языковых моделей серий GPT и OpenAI. Авторы отмечают значительный прогресс модели o3 в решении символических паттернов, но подчеркивают необходимость исследования мультимодальных сценариев. Проводится анализ производительности моделей на сложных визуально-лингвистических головоломках, требующих абстрактного и алгоритмического мышления. Результаты показывают общую тенденцию улучшения способностей к рассуждению, но выявляют сохраняющиеся трудности даже у передовых моделей в некоторых типах задач.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': '多模态推理能力的探索与挑战', 'desc': '本文探讨了OpenAI的o1和o3模型在大型语言模型中的先进推理能力。o3在抽象和推理语料库（ARC-AGI）中超越了人类，表现出色，但该基准仅限于符号模式。人类通常在多模态场景中进行推理，因此需要研究多模态任务中的高级推理能力。尽管o1在推理能力上有所提升，但在简单的多模态难题和算法难题上仍然存在不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Новый рубеж в model-based RL: превосходя человека в Craftax-classic', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе модели, достигающий наилучших результатов на бенчмарке Craftax-classic. Авторы разработали алгоритм, превосходящий как предыдущий SOTA-метод DreamerV3, так и человеческий уровень производительности. Ключевые улучшения включают комбинированную архитектуру политики с CNN и RNN, обучение на реальных и воображаемых данных, токенизацию изображений методом ближайших соседей и блочное teacher forcing для трансформерной модели мира. Эти инновации позволили значительно повысить эффективность использования данных в сложной среде, требующей широкого спектра навыков.'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': '基于模型的强化学习新突破！', 'desc': '本文提出了一种基于模型的强化学习方法，在Craftax-classic基准测试中取得了新的最佳表现。这是一款开放世界的2D生存游戏，要求智能体展现出强大的泛化能力、深度探索能力和长期推理能力。我们的MBRL算法在仅1M环境步骤后获得了67.4%的奖励，显著超越了DreamerV3的53.2%，并首次超过了人类表现的65.0%。该方法通过构建一个最先进的无模型基线，并结合CNN和RNN的新型策略架构，进一步提升了样本效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Безопасная генерация ответов LLM без переобучения', 'desc': 'Статья представляет новый подход к выравниванию больших языковых моделей (LLM) во время вывода, обеспечивающий генерацию безопасных ответов с вероятностью, близкой к единице. Авторы формулируют задачу как ограниченный марковский процесс принятия решений в латентном пространстве модели. Они вводят состояние безопасности, которое отслеживает эволюцию ограничений безопасности и позволяет доказать формальные гарантии безопасности. На основе этого подхода разработан метод InferenceGuard, который эффективно балансирует безопасность и производительность задачи, превосходя существующие методы выравнивания во время вывода.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': '推理时安全对齐的新方法', 'desc': '本文提出了一种新的推理时对齐方法，旨在确保大型语言模型（LLM）生成安全的响应。我们将安全生成推理时响应的问题框架化为一个约束的马尔可夫决策过程（MDP），并在LLM的潜在空间中进行处理。通过引入一个安全状态来跟踪安全约束的演变，我们能够在解决MDP时提供正式的安全保证。我们提出的InferenceGuard实现了在不修改模型权重的情况下安全对齐LLM，并在生成安全和对齐响应方面优于现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 6, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': '🧠', 'ru': {'title': 'Преодоление выбросов в латентном пространстве для улучшения консистентных моделей', 'desc': 'Эта статья представляет новый подход к обучению консистентных моделей в латентном пространстве для генеративных задач. Авторы обнаружили, что латентные данные часто содержат импульсивные выбросы, которые ухудшают производительность iCT. Для решения этой проблемы они предложили использовать функцию потерь Коши вместо Псевдо-Хубера, а также ввели диффузионные потери на ранних временных шагах и применили оптимальный транспорт. Эти стратегии позволили успешно обучить латентные консистентные модели, способные к высококачественному сэмплированию за один-два шага.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': '提升一致性模型性能的创新方法', 'desc': '一致性模型是一种新型生成模型，能够在单步或多步中生成高质量样本。最近，这些模型在像素空间中表现出色，达到了与扩散模型相当的效果。然而，在大规模数据集上进行一致性训练的成功，尤其是在文本到图像和视频生成任务中，取决于潜在空间的表现。为了解决潜在数据中的异常值对性能的影响，本文提出了使用Cauchy损失替代伪Huber损失，并引入扩散损失和最优传输方法，以提高模型的鲁棒性和性能。'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 5, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Новый бенчмарк раскрывает скрытые возможности и недостатки языковых моделей', 'desc': 'Авторы представляют новый бенчмарк для оценки языковых моделей, основанный на головоломках NPR Sunday Puzzle Challenge. В отличие от существующих тестов, требующих специализированных знаний, этот бенчмарк опирается на общие знания и легко проверяем. Исследование выявило значительное превосходство модели OpenAI o1 над другими моделями рассуждений. Анализ также обнаружил новые типы ошибок у моделей, такие как преждевременная капитуляция и неуверенность в ответах.'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': '挑战性与可验证性的全新基准测试', 'desc': '现有的前沿模型基准测试通常考察专业的、难以理解的知识。我们提出了一种基于NPR周日谜题挑战的基准，要求仅具备一般知识。该基准对人类和模型都具有挑战性，但正确答案易于验证，模型的错误也容易发现。我们的研究揭示了现有基准中未显现的能力差距，OpenAI o1在推理模型中表现优异，且分析推理输出揭示了新的失败类型。'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование редактирования знаний в нейросетях', 'desc': 'Статья посвящена улучшению методов последовательного редактирования знаний в больших языковых моделях. Авторы выявили проблемы переобучения и непропорционального роста нормы при использовании существующих методов редактирования. Они предложили новый метод ENCORE, который контролирует переобучение и рост нормы, позволяя выполнять до 10 000 последовательных правок без потери производительности модели. ENCORE также показывает значительное ускорение по сравнению с другими методами редактирования знаний.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCORE：高效的知识编辑解决方案', 'desc': '本论文研究了在知识编辑中进行大规模顺序编辑时模型性能下降的原因。我们发现，定位后编辑的方法容易导致对编辑事实的过拟合，并且连续的知识编辑会导致编辑矩阵的范数不成比例地增长。为了解决这些问题，我们提出了ENCORE方法，通过早停和范数约束来控制过拟合和范数增长，从而实现长时间的顺序编辑。ENCORE能够在不损失下游性能的情况下，进行多达10,000次的顺序编辑，并且比现有方法更快。'}}}, {'id': 'https://huggingface.co/papers/2502.01619', 'title': 'Learning to Generate Unit Tests for Automated Debugging', 'url': 'https://huggingface.co/papers/2502.01619', 'abstract': "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGen into UTDebug, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDebug (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGen outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines.", 'score': 2, 'issue_id': 2036, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '0806c010a6d2569d', 'authors': ['Archiki Prasad', 'Elias Stengel-Eskin', 'Justin Chih-Yao Chen', 'Zaid Khan', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.01619.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#plp'], 'emoji': '🧪', 'ru': {'title': 'Автоматическая генерация юнит-тестов для улучшения отладки кода языковыми моделями', 'desc': 'Статья представляет UTGen - метод обучения языковых моделей генерации юнит-тестов, выявляющих ошибки в коде и предсказывающих корректные выходные данные. UTGen интегрирован в конвейер отладки UTDebug, который использует сгенерированные тесты для эффективной отладки кода языковыми моделями. UTDebug масштабирует UTGen во время выполнения для улучшения предсказания выходных данных тестов и проверяет изменения на основе нескольких сгенерированных юнит-тестов. Результаты показывают, что UTGen превосходит базовые методы генерации юнит-тестов, а UTDebug улучшает точность отладки кода языковыми моделями.'}, 'en': {'title': 'Enhancing Debugging with Smart Unit Test Generation', 'desc': 'This paper introduces UTGen, a method that helps large language models (LLMs) generate unit test inputs that can effectively reveal errors in faulty code while also predicting the correct outputs. The authors highlight a challenge where generating tests that expose errors can lead to incorrect output predictions without having the correct solutions available. To overcome this, UTGen is integrated into a debugging pipeline called UTDebug, which enhances the debugging process by validating and refining the generated tests. The results show that UTGen significantly improves the accuracy of LLMs in debugging tasks, outperforming existing methods in generating effective unit tests.'}, 'zh': {'title': '自动化单元测试生成与调试的创新方案', 'desc': '本文提出了一种名为UTGen的自动化单元测试生成方法，旨在帮助大型语言模型（LLM）生成能够揭示错误的单元测试输入及其正确的预期输出。研究发现，生成的单元测试输入在揭示错误和正确预测输出之间存在权衡。UTGen被集成到UTDebug调试管道中，以提高LLM的调试效果，并通过多次生成的测试来验证和回溯编辑，避免过拟合。实验结果表明，UTGen在生成有效单元测试方面优于现有基线，并显著提高了LLM在调试任务中的准确性。'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 1, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Путь к надежным фундаментальным моделям в патологии: преодоление влияния медицинских центров', 'desc': 'Статья рассматривает проблему надежности фундаментальных моделей (ФМ) в патологии для использования в клинической практике. Авторы вводят новую метрику - Индекс надежности, который отражает степень доминирования биологических признаков над мешающими факторами, связанными с особенностями медицинских центров. Оценка десяти публично доступных ФМ показала, что все они сильно зависят от специфики медицинских центров, и только одна модель имеет индекс надежности больше единицы. Исследование демонстрирует, что ошибки классификации типов рака связаны с конфаундерами из того же медицинского центра, а пространства вложений ФМ организованы больше по медицинским центрам, чем по биологическим факторам.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': '确保病理模型的鲁棒性，助力临床应用', 'desc': '病理基础模型（FMs）在医疗保健中具有很大潜力，但在临床应用之前，必须确保它们对不同医疗中心的变化具有鲁棒性。我们提出了鲁棒性指数，这是一种新颖的鲁棒性度量，反映生物特征在多大程度上主导了混淆特征。研究发现，当前的病理基础模型在很大程度上代表了医疗中心，只有一个模型的鲁棒性指数大于一，表明生物特征略微主导混淆特征。通过定量方法分析医疗中心差异对模型预测性能的影响，发现癌症类型分类错误与同中心混淆因素密切相关。'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 1, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная сегментация опухолей с помощью усовершенствованных нейросетей', 'desc': 'Статья посвящена автоматической сегментации опухолей забрюшинного пространства с использованием различных архитектур глубокого обучения. Авторы сравнивают эффективность U-Net и его модификаций, включая CNN, ViT, Mamba SSM и xLSTM, на новом наборе данных КТ. Предложенная модель ViLU-Net интегрирует Vi-блоки для улучшения сегментации. Результаты показывают, что xLSTM обеспечивает наиболее эффективную работу в рамках архитектуры U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': '高效肿瘤分割：ViLU-Net的创新应用', 'desc': '本研究探讨了在后腹膜肿瘤的自动分割中使用U-Net及其变体的有效性。这些肿瘤形状不规则，手动分割耗时且困难，因此需要更高效的自动化方法。研究中引入了Mamba状态空间模型和扩展长短期记忆（xLSTM）等架构，以降低计算资源消耗并处理长距离依赖。最终提出的ViLU-Net模型通过集成Vi-blocks，显著提高了分割效果，xLSTM在U-Net框架中的效率表现尤为突出。'}}}, {'id': 'https://huggingface.co/papers/2502.01126', 'title': 'Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences', 'url': 'https://huggingface.co/papers/2502.01126', 'abstract': 'Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model\'s preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model\'s confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets.', 'score': 0, 'issue_id': 2035, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'ed2eef6c3e310379', 'authors': ['Vaishnavi Shrivastava', 'Ananya Kumar', 'Percy Liang'], 'affiliations': ['cs.stanford.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.01126.jpg', 'data': {'categories': ['#interpretability', '#rlhf', '#reasoning', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности языковых моделей через относительную оценку уверенности', 'desc': 'Исследование предлагает метод относительной оценки уверенности языковых моделей, где модель сравнивает свою уверенность в ответах на разные вопросы. Этот подход использует методы ранжирования, такие как рейтинг Эло и модель Брэдли-Терри, для перевода предпочтений модели в оценки уверенности. Результаты показывают, что относительная оценка уверенности превосходит абсолютную оценку и методы самосогласованности на 3.5% и 1.7% соответственно по метрике AUC выборочной классификации. Исследование проводилось на пяти современных языковых моделях и 14 сложных задачах по ответам на вопросы в областях STEM, социальных наук и здравого смысла.'}, 'en': {'title': 'Boosting Confidence: Relative Estimation Outshines Absolute Scores in Language Models', 'desc': 'This paper discusses the importance of reliable confidence estimates from language models (LMs) to help users identify potential errors in their outputs. It highlights the challenges LMs face in providing absolute confidence assessments and proposes a new method called relative confidence estimation. This method involves comparing questions against each other to determine which the model is more confident in answering correctly, using techniques like Elo rating and Bradley-Terry for ranking. The study shows that relative confidence estimation outperforms traditional absolute confidence methods, leading to improved reliability in confidence scores across various question answering tasks.'}, 'zh': {'title': '相对置信度估计：提升语言模型的可靠性', 'desc': '本文探讨了语言模型（LM）如何提供可靠的置信度评估，以帮助用户识别输出中的错误。我们提出了一种相对置信度估计的方法，通过比较问题之间的相对置信度来评估模型的信心。与绝对置信度评估相比，相对置信度估计在多个先进的语言模型上表现出更高的可靠性，尤其是在选择性分类任务中。实验结果显示，相对置信度估计在准确性上平均提高了3.5%。'}}}, {'id': 'https://huggingface.co/papers/2501.19393', 'title': 's1: Simple test-time scaling', 'url': 'https://huggingface.co/papers/2501.19393', 'abstract': 'Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\'s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\'s thinking process or lengthening it by appending "Wait" multiple times to the model\'s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.', 'score': 48, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8fcf84a9effc288f', 'authors': ['Niklas Muennighoff', 'Zitong Yang', 'Weijia Shi', 'Xiang Lisa Li', 'Li Fei-Fei', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Percy Liang', 'Emmanuel Candès', 'Tatsunori Hashimoto'], 'affiliations': ['Allen Institute for AI', 'Contextual AI', 'Stanford University', 'University of Washington, Seattle'], 'pdf_title_img': 'assets/pdf/title_img/2501.19393.jpg', 'data': {'categories': ['#dataset', '#open_source', '#reasoning', '#training', '#math', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Простое масштабирование для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет новый подход к языковому моделированию, называемый тестовым масштабированием. Авторы разработали модель s1, основанную на Qwen2.5-32B-Instruct, и метод бюджетного форсирования для контроля вычислений во время тестирования. Модель обучена на специально отобранном наборе данных s1K из 1000 вопросов с рассуждениями. Результаты показывают, что s1 превосходит модель o1-preview от OpenAI на математических задачах, демонстрируя эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing Language Models with Test-Time Scaling', 'desc': "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."}, 'zh': {'title': '测试时间扩展：提升语言模型性能的新方法', 'desc': '本文介绍了一种新的语言建模方法——测试时间扩展，旨在通过增加测试时间计算来提高模型性能。研究者们创建了一个包含1000个问题及其推理过程的小数据集s1K，并通过难度、多样性和质量三个标准进行验证。为了控制测试时间计算，提出了预算强制的方法，通过强制终止模型的思考过程或在模型生成时多次添加“等待”来延长思考时间，从而促使模型检查答案。经过监督微调后，模型s1在数学竞赛问题上超越了OpenAI的o1模型，表现提升达27%。'}}}, {'id': 'https://huggingface.co/papers/2501.19324', 'title': 'Reward-Guided Speculative Decoding for Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2501.19324', 'abstract': '', 'score': 26, 'issue_id': 1995, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'ce2d414eedfb7a1e', 'authors': ['Baohao Liao', 'Yuhui Xu', 'Hanze Dong', 'Junnan Li', 'Christof Monz', 'Silvio Savarese', 'Doyen Sahoo', 'Caiming Xiong'], 'affiliations': ['Language Technology Lab, University of Amsterdam', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.19324.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Новый шаг в обучении больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению LLM, который позволяет моделям лучше понимать контекст и улучшать качество генерации текста. Исследователи предложили метод, который использует более сложные архитектуры нейронных сетей для обработки больших объемов данных. Эксперименты показали, что предложенный метод значительно повышает точность и скорость работы моделей. Это открывает новые возможности для применения AI в различных областях, таких как обработка естественного языка и генерация контента.'}, 'en': {'title': 'Hybrid Networks: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '优化数据处理，提升机器学习性能', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的准确性和效率。作者提出了一种创新的方法，通过优化数据预处理和特征选择来增强学习效果。实验结果表明，该算法在多个数据集上表现优于现有技术。最终，研究表明，改进的数据处理流程对机器学习模型的性能至关重要。'}}}, {'id': 'https://huggingface.co/papers/2501.18119', 'title': 'Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models', 'url': 'https://huggingface.co/papers/2501.18119', 'abstract': 'Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.', 'score': 12, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': 'd751c8a690173842', 'authors': ['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], 'affiliations': ['National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2501.18119.jpg', 'data': {'categories': ['#inference', '#graphs', '#transfer_learning', '#training', '#multimodal', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективная интеграция графов знаний и языковых моделей через квантованные коды', 'desc': 'Статья представляет двухэтапный подход к интеграции графов знаний с большими языковыми моделями. Авторы предлагают метод самоконтролируемого квантованного представления (SSQR) для сжатия структурных и семантических знаний графа в дискретные коды. Эти коды затем используются для создания инструкций для обучения языковых моделей. Эксперименты показывают превосходство SSQR над существующими методами и улучшение производительности моделей LLaMA2 и LLaMA3.1 на задачах, связанных с графами знаний.'}, 'en': {'title': 'Seamless Integration of Knowledge Graphs and Language Models', 'desc': 'This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens.'}, 'zh': {'title': '无缝整合知识图谱与大型语言模型', 'desc': '本论文探讨了知识图谱（KG）结构与自然语言之间的差距，提出了一种两阶段框架，以实现KG与大型语言模型（LLM）的有效整合。首先，提出了一种自监督量化表示（SSQR）方法，将KG的结构和语义知识压缩为离散代码（即令牌），使其与语言句子的格式对齐。接着，设计了KG指令跟随数据，将这些学习到的代码视为特征，直接输入到LLM中，从而实现无缝整合。实验结果表明，SSQR在无监督量化方法中表现优越，生成的代码更具可区分性，且经过微调的LLaMA2和LLaMA3.1在KG链接预测和三元组分类任务中表现出色，仅使用每个实体16个令牌，而不是传统方法中的数千个。'}}}, {'id': 'https://huggingface.co/papers/2501.19339', 'title': 'PixelWorld: Towards Perceiving Everything as Pixels', 'url': 'https://huggingface.co/papers/2501.19339', 'abstract': 'Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models\' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models\' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.', 'score': 9, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '3e10b792328f7a4b', 'authors': ['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], 'affiliations': ['Department of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2501.19339.jpg', 'data': {'categories': ['#agi', '#benchmark', '#optimization', '#dataset', '#open_source', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Единый пиксельный взгляд на мир: новая парадигма для ИИ', 'desc': 'В статье предлагается новый подход к обработке различных модальностей данных (текст, изображения, код и т.д.) в виде пикселей, названный PEAP (Perceive Everything as Pixels). Авторы представляют набор данных PixelWorld для оценки эффективности моделей в этом unified подходе. Результаты показывают, что PEAP превосходит базовые модели на мультимодальных данных, но выявляет снижение производительности в задачах рассуждения и кодирования. Исследование демонстрирует потенциал и ограничения восприятия пиксельных данных современными языковыми моделями.'}, 'en': {'title': 'Unifying Perception: Everything as Pixels', 'desc': "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."}, 'zh': {'title': '统一感知：将一切视为像素', 'desc': '本论文提出了一种新的统一感知框架，称为“将一切视为像素”（PEAP），旨在将文本、表格、代码、图表和图像等多种输入形式统一为像素输入。我们引入了PixelWorld评估套件，以在像素空间中评估现有模型的性能。研究发现，PEAP在多模态数据集上优于基于标记的输入，显示出统一输入在消歧义方面的优势。同时，处理像素输入时，所有模型的推理和编码能力显著下降，表明需要增强基础模型的感知能力。'}}}, {'id': 'https://huggingface.co/papers/2501.14677', 'title': 'MatAnyone: Stable Video Matting with Consistent Memory Propagation', 'url': 'https://huggingface.co/papers/2501.14677', 'abstract': 'Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.', 'score': 6, 'issue_id': 2010, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 января', 'en': 'January 24', 'zh': '1月24日'}, 'hash': 'a9968478421ddc33', 'authors': ['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2501.14677.jpg', 'data': {'categories': ['#training', '#dataset', '#video'], 'emoji': '✂️', 'ru': {'title': 'MatAnyone: Точное выделение объектов на видео с помощью памяти и адаптивного обучения', 'desc': 'MatAnyone - это новый подход к выделению объектов на видео без вспомогательных данных. Он использует модуль памяти для адаптивного объединения информации из предыдущих кадров, обеспечивая стабильность основных областей и сохраняя детали на границах объектов. Авторы также создали новый большой набор данных для обучения и разработали стратегию, использующую данные сегментации для повышения стабильности. MatAnyone превосходит существующие методы в различных сценариях реального мира.'}, 'en': {'title': 'MatAnyone: Robust Video Matting with Memory Propagation', 'desc': 'The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods.'}, 'zh': {'title': 'MatAnyone：视频抠图的新突破', 'desc': '本论文提出了一种名为MatAnyone的视频抠图框架，旨在解决复杂背景下的抠图问题。该方法基于记忆传播模块，通过区域自适应记忆融合，动态整合前一帧的记忆信息，从而确保核心区域的语义稳定性。为了提高训练的鲁棒性，研究团队构建了一个更大、更高质量且多样化的视频抠图数据集，并采用了一种新颖的训练策略，充分利用大规模分割数据。最终，MatAnyone在多种真实场景中展现出优越的抠图效果，超越了现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2501.19399', 'title': 'Scalable-Softmax Is Superior for Attention', 'url': 'https://huggingface.co/papers/2501.19399', 'abstract': "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", 'score': 6, 'issue_id': 2009, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '12ed1cad789702aa', 'authors': ['Ken M. Nakanishi'], 'affiliations': ['Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2501.19399.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'SSMax: улучшение внимания трансформеров для длинных текстов', 'desc': 'Статья представляет новую функцию Scalable-Softmax (SSMax) для улучшения работы трансформеров с длинными контекстами. SSMax решает проблему уплощения распределения внимания при увеличении размера входного вектора. Эксперименты показывают, что модели с SSMax быстрее обучаются и лучше работают с длинными текстами. SSMax позволяет модели фокусироваться на ключевой информации даже в длинных контекстах.'}, 'en': {'title': 'Enhancing Attention with Scalable-Softmax for Better Context Handling', 'desc': "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."}, 'zh': {'title': '可扩展Softmax：提升Transformer模型的注意力能力', 'desc': '本文提出了一种新的方法，称为可扩展Softmax（SSMax），旨在解决Transformer模型在处理长上下文时的注意力分布扁平化问题。传统的Softmax函数在输入向量增大时，最大元素趋近于零，导致模型无法有效地优先考虑关键信息。SSMax可以无缝集成到现有的Transformer架构中，实验结果表明，使用SSMax的模型在语言建模中不仅在预训练期间实现了更快的损失减少，还显著提高了在长上下文中的性能。通过分析注意力分数，SSMax使模型能够在长上下文中更好地关注关键信息，提升了模型的长度泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.04983', 'title': 'DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning', 'url': 'https://huggingface.co/papers/2411.04983', 'abstract': 'The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.', 'score': 6, 'issue_id': 1999, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'e72081596b626524', 'authors': ['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], 'affiliations': ['Courant Institute, New York University', 'Meta-FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2411.04983.jpg', 'data': {'categories': ['#cv', '#agents', '#reasoning', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DINO-WM: универсальная модель мира для планирования поведения без реконструкции', 'desc': 'Статья представляет новый метод моделирования визуальной динамики без реконструкции визуального мира - DINO World Model (DINO-WM). DINO-WM использует пространственные признаки патчей, предварительно обученные с помощью DINOv2, что позволяет ему учиться на офлайн-траекториях поведения, предсказывая будущие признаки патчей. Этот подход позволяет DINO-WM достигать наблюдаемых целей путем оптимизации последовательности действий, облегчая планирование поведения, независимое от задачи. Эксперименты показывают, что DINO-WM может генерировать поведенческие решения с нуля во время тестирования, демонстрируя сильные способности к обобщению по сравнению с предыдущими современными методами.'}, 'en': {'title': 'DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning', 'desc': 'This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities.'}, 'zh': {'title': 'DINO-WM：无任务依赖的世界模型', 'desc': '本文提出了一种新的世界模型DINO-WM，旨在通过被动数据进行推理和规划。DINO-WM具有三个关键特性：可以在离线收集的轨迹上进行训练，支持测试时行为优化，并促进任务无关的推理。该模型利用DINOv2预训练的空间补丁特征，通过预测未来的补丁特征来学习，从而实现观察目标的行为规划。实验结果表明，DINO-WM在多种任务中表现出色，能够在没有专家示范和奖励建模的情况下生成零-shot行为解决方案。'}}}, {'id': 'https://huggingface.co/papers/2501.18837', 'title': 'Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming', 'url': 'https://huggingface.co/papers/2501.18837', 'abstract': 'Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.', 'score': 4, 'issue_id': 1996, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '62d14973b1140e58', 'authors': ['Mrinank Sharma', 'Meg Tong', 'Jesse Mu', 'Jerry Wei', 'Jorrit Kruthoff', 'Scott Goodfriend', 'Euan Ong', 'Alwin Peng', 'Raj Agarwal', 'Cem Anil', 'Amanda Askell', 'Nathan Bailey', 'Joe Benton', 'Emma Bluemke', 'Samuel R. Bowman', 'Eric Christiansen', 'Hoagy Cunningham', 'Andy Dau', 'Anjali Gopal', 'Rob Gilson', 'Logan Graham', 'Logan Howard', 'Nimit Kalra', 'Taesung Lee', 'Kevin Lin', 'Peter Lofgren', 'Francesco Mosconi', "Clare O'Hara", 'Catherine Olsson', 'Linda Petrini', 'Samir Rajani', 'Nikhil Saxena', 'Alex Silverstein', 'Tanya Singh', 'Theodore Sumers', 'Leonard Tang', 'Kevin K. Troy', 'Constantin Weisser', 'Ruiqi Zhong', 'Giulio Zhou', 'Jan Leike', 'Jared Kaplan', 'Ethan Perez'], 'affiliations': ['Safeguards Research Team, Anthropic'], 'pdf_title_img': 'assets/pdf/title_img/2501.18837.jpg', 'data': {'categories': ['#synthetic', '#training', '#architecture', '#dataset', '#security', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Конституционные Классификаторы: надежная защита языковых моделей', 'desc': 'Исследование представляет концепцию Конституционных Классификаторов - защитных механизмов для больших языковых моделей (LLM), обученных на синтетических данных с использованием правил, определяющих допустимый контент. Эти классификаторы эффективно противостоят универсальным методам обхода защиты, не позволяя извлекать вредоносную информацию из защищенных моделей. Эксперименты показали устойчивость классификаторов к различным атакам и их практическую применимость с минимальным влиянием на производительность. Исследование демонстрирует возможность эффективной защиты LLM от универсальных методов обхода при сохранении практической применимости.'}, 'en': {'title': 'Defending LLMs with Constitutional Classifiers', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications.'}, 'zh': {'title': '宪法分类器：保护大型语言模型的安全', 'desc': '大型语言模型（LLMs）容易受到普遍越狱攻击，这种攻击可以绕过模型的安全措施，允许用户进行有害操作。为此，我们提出了宪法分类器，这是一种基于合成数据训练的安全措施，合成数据是通过自然语言规则（即宪法）提示LLMs生成的，规定了允许和限制的内容。在超过3000小时的红队测试中，没有红队成员找到能够从早期分类器保护的LLM中提取信息的普遍越狱方法。我们的研究表明，在保持实际部署可行性的同时，防御普遍越狱攻击是可行的。'}}}, {'id': 'https://huggingface.co/papers/2501.18841', 'title': 'Trading Inference-Time Compute for Adversarial Robustness', 'url': 'https://huggingface.co/papers/2501.18841', 'abstract': 'We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.', 'score': 3, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'f1e75e6b24f3e044', 'authors': ['Wojciech Zaremba', 'Evgenia Nitishinskaya', 'Boaz Barak', 'Stephanie Lin', 'Sam Toyer', 'Yaodong Yu', 'Rachel Dias', 'Eric Wallace', 'Kai Xiao', 'Johannes Heidecke', 'Amelia Glaese'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.18841.jpg', 'data': {'categories': ['#security', '#reasoning', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Больше вычислений - выше защита: повышение устойчивости ИИ к атакам', 'desc': 'Исследование посвящено влиянию увеличения вычислительных ресурсов во время вывода на устойчивость моделей рассуждений к состязательным атакам. Эксперименты показали, что увеличение вычислений при выводе улучшает робастность моделей к различным атакам. В большинстве случаев доля успешных атак стремится к нулю при росте вычислительных ресурсов. Результаты указывают на потенциал увеличения вычислений при выводе для повышения устойчивости больших языковых моделей к состязательным атакам.'}, 'en': {'title': 'Boosting Robustness: More Compute, Less Vulnerability', 'desc': "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."}, 'zh': {'title': '增加推理计算，提升模型鲁棒性', 'desc': '本研究探讨了在推理模型中增加推理时间计算对其抵御对抗攻击的影响。我们发现，随着推理时间计算的增加，模型的鲁棒性得到了提升。大多数情况下，攻击成功的模型样本比例随着测试时间计算的增加而趋近于零。我们的结果表明，推理时间计算有潜力提高大型语言模型的对抗鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2501.18052', 'title': 'SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2501.18052', 'abstract': "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.", 'score': 2, 'issue_id': 2011, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': 'd94056a77d806ada', 'authors': ['Bartosz Cywiński', 'Kamil Deja'], 'affiliations': ['IDEAS NCBR', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2501.18052.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#security', '#architecture', '#ethics', '#training', '#benchmark'], 'emoji': '🧹', 'ru': {'title': 'Чистка нейросетей: SAeUron удаляет нежелательные концепции из диффузионных моделей', 'desc': 'SAeUron - это новый метод удаления нежелательных концепций в диффузионных моделях для генерации изображений по тексту. Он использует разреженные автоэнкодеры (SAE) для выделения интерпретируемых признаков, соответствующих конкретным концепциям. Метод позволяет точно вмешиваться в активации модели для блокировки целевого контента при сохранении общей производительности. SAeUron показывает лучшие результаты по сравнению с другими методами и может удалять несколько концепций одновременно.'}, 'en': {'title': 'SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders', 'desc': "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."}, 'zh': {'title': 'SAeUron：去除不良内容的新方法', 'desc': '扩散模型虽然强大，但可能会生成有害或不良内容，带来伦理和安全问题。最近的机器遗忘方法提供了潜在的解决方案，但通常缺乏透明性，难以理解对基础模型的更改。我们提出了一种新方法SAeUron，利用稀疏自编码器（SAE）学习的特征来去除文本到图像扩散模型中的不必要概念。通过在多个去噪时间步的激活上无监督训练SAE，我们捕捉到与特定概念对应的稀疏和可解释特征，从而实现精确干预。'}}}, {'id': 'https://huggingface.co/papers/2501.18804', 'title': 'Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion', 'url': 'https://huggingface.co/papers/2501.18804', 'abstract': 'Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.', 'score': 2, 'issue_id': 2010, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '32db517ad974401b', 'authors': ['Vitor Guizilini', 'Muhammad Zubair Irshad', 'Dian Chen', 'Greg Shakhnarovich', 'Rares Ambrus'], 'affiliations': ['Toyota Research Institute (TRI)', 'Toyota Technological Institute at Chicago (TTIC)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18804.jpg', 'data': {'categories': ['#training', '#3d', '#diffusion', '#architecture', '#benchmark', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'Генерация 3D сцен из разных ракурсов с помощью диффузионной модели', 'desc': 'Статья представляет MVGD - архитектуру на основе диффузии для генерации изображений и карт глубины с новых ракурсов. Метод использует райкарты для обогащения визуальных признаков пространственной информацией и направления генерации. Ключевой аспект - многозадачная генерация изображений и карт глубины с использованием обучаемых эмбеддингов задач. Модель обучена на более чем 60 миллионах мультиракурсных примеров и показывает современные результаты в задачах синтеза новых ракурсов и оценки глубины.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation', 'desc': 'This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks.'}, 'zh': {'title': 'MVGD：从多视角直接生成图像与深度图的创新方法', 'desc': '本文提出了一种名为MVGD的扩散基础架构，能够直接从多个视角生成图像和深度图。该方法通过光线图条件化，增强了视觉特征并引导生成过程。我们采用多任务生成技术，同时生成图像和深度图，并使用可学习的任务嵌入来优化扩散过程。经过在超过6000万多视角样本上的训练，我们在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2501.18965', 'title': 'The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training', 'url': 'https://huggingface.co/papers/2501.18965', 'abstract': 'We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.', 'score': 2, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a136293a2241150e', 'authors': ['Fabian Schaipp', 'Alexander Hägele', 'Adrien Taylor', 'Umut Simsekli', 'Francis Bach'], 'affiliations': ['EPFL, Lausanne, Switzerland', 'Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2501.18965.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Оптимизация графиков обучения для больших языковых моделей', 'desc': 'В статье исследуются графики изменения скорости обучения для больших моделей машинного обучения. Авторы обнаружили неожиданное сходство этих графиков с теоретическими границами из теории невыпуклой оптимизации. Они предлагают новый метод настройки скорости обучения, основанный на этом наблюдении. Применение метода позволило улучшить результаты обучения языковых моделей типа Llama размером 124M и 210M параметров.'}, 'en': {'title': 'Optimizing Learning Rates: Bridging Theory and Practice', 'desc': 'This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models.'}, 'zh': {'title': '优化学习率调度，提升大模型训练效果', 'desc': '本文探讨了大模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间的相似性。我们提供了一个线性冷却的常数调度的界限，特别是冷却的实际好处在于没有对数项的影响。进一步地，我们展示了优化理论与实践之间的紧密联系可以用于学习率调优：通过延长调度以继续训练并使用最佳学习率，我们在训练124M和210M的Llama类型模型时取得了显著的改进。最后，我们还展示了在不同调度之间转移最佳学习率的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.18753', 'title': 'INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation', 'url': 'https://huggingface.co/papers/2501.18753', 'abstract': 'Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.', 'score': 2, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '000663cf445862a1', 'authors': ['Jian Hu', 'Zixu Cheng', 'Shaogang Gong'], 'affiliations': ['Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2501.18753.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Адаптивная сегментация изображений с помощью интеллектуального отбора промптов', 'desc': 'Статья представляет новый метод сегментации изображений под названием INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). Этот подход направлен на улучшение генерации промптов, специфичных для конкретных экземпляров изображений, путем адаптивного уменьшения влияния нерелевантных знаний и усиления наиболее вероятных. INT состоит из двух компонентов: генерации промптов, специфичных для экземпляров, и генерации семантической маски. Метод был проверен на шести наборах данных, включая камуфлированные объекты и медицинские изображения, демонстрируя эффективность, надежность и масштабируемость.'}, 'en': {'title': 'Enhancing Image Segmentation with Smart Prompting', 'desc': 'This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results.'}, 'zh': {'title': '实例特定负采样优化图像分割', 'desc': '这篇论文提出了一种新的方法，称为实例特定负采样（INT），用于任务通用的可提示图像分割。INT的核心思想是自适应地减少无关的先验知识的影响，同时增加最有可能的先验知识的使用，以优化实例特定提示的生成。该方法包括两个主要部分：实例特定提示生成和语义掩码生成，确保每个图像实例的分割与提示的语义相匹配。通过在六个数据集上的验证，INT展示了其有效性、鲁棒性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2501.18128', 'title': 'Unraveling the Capabilities of Language Models in News Summarization', 'url': 'https://huggingface.co/papers/2501.18128', 'abstract': "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", 'score': 2, 'issue_id': 2000, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '1c3f3a16953a5a59', 'authors': ['Abdurrahman Odabaşı', 'Göksel Biricik'], 'affiliations': ['Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye', 'Department of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye'], 'pdf_title_img': 'assets/pdf/title_img/2501.18128.jpg', 'data': {'categories': ['#survey', '#multilingual', '#small_models', '#benchmark', '#transfer_learning'], 'emoji': '📰', 'ru': {'title': 'Маленькие модели бросают вызов гигантам в суммаризации новостей', 'desc': 'В этой работе проводится комплексное сравнение 20 современных языковых моделей, с акцентом на меньшие модели, для задачи суммаризации новостей. Исследование охватывает обучение в режимах zero-shot и few-shot на трех различных наборах данных, используя автоматические метрики, человеческую оценку и LLM в качестве судьи. Интересно, что включение демонстрационных примеров в режиме few-shot не улучшило производительность моделей, а в некоторых случаях даже ухудшило качество генерируемых сводок. Результаты показали превосходство GPT-3.5-Turbo и GPT-4, но также выявили перспективные открытые модели, такие как Qwen1.5-7B и SOLAR-10.7B-Instruct-v1.0.'}, 'en': {'title': 'Benchmarking News Summarization: Small Models Can Compete!', 'desc': 'This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks.'}, 'zh': {'title': '小模型在新闻摘要中的潜力与挑战', 'desc': '本研究对20种最新的语言模型进行了全面的基准测试，重点关注较小的模型在新闻摘要任务中的表现。我们系统地测试了这些模型在不同风格的新闻文章摘要中的能力和有效性，使用了三种不同的数据集。研究发现，在少量示例学习的设置中，提供示例并未提升模型的性能，反而在某些情况下导致生成摘要的质量下降。这主要是由于参考摘要的质量较差，影响了模型的表现，同时我们的研究结果显示，GPT-3.5-Turbo和GPT-4在性能上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2404.07097', 'title': 'Fast Encoder-Based 3D from Casual Videos via Point Track Processing', 'url': 'https://huggingface.co/papers/2404.07097', 'abstract': 'This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.', 'score': 1, 'issue_id': 1999, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'a526ae197fe3a8c7', 'authors': ['Yoni Kasten', 'Wuyue Lu', 'Haggai Maron'], 'affiliations': ['NVIDIA Research', 'Simon Fraser University', 'Technion'], 'pdf_title_img': 'assets/pdf/title_img/2404.07097.jpg', 'data': {'categories': ['#3d', '#training', '#cv', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная 3D реконструкция из видео с помощью глубокого обучения', 'desc': 'Статья представляет TracksTo4D - новый подход к реконструкции 3D структур из видео с динамическим содержанием. Метод использует нейронную сеть, обученную без учителя на 2D треках точек, извлеченных из обычных видео. TracksTo4D позволяет восстанавливать облако точек и положение камеры за один проход сети, значительно сокращая время вычислений по сравнению с существующими методами. Архитектура сети учитывает симметрии входных данных и предполагает низкоранговое представление паттернов движения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction from Casual Videos', 'desc': 'This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time.'}, 'zh': {'title': '高效重建3D结构，TracksTo4D引领新潮流', 'desc': '本文解决了从动态内容视频重建3D结构的长期挑战。现有方法无法处理普通相机录制的随意视频，或需要较长的优化时间。我们提出了一种基于学习的方法TracksTo4D，通过单次高效的前馈传递，从随意视频中推断3D结构和相机位置。该方法直接处理2D点轨迹，并设计了专门的架构，能够在无监督的情况下进行训练，显著提高了重建效率。'}}}, {'id': 'https://huggingface.co/papers/2502.11089', 'title': 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention', 'url': 'https://huggingface.co/papers/2502.11089', 'abstract': 'Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.', 'score': 55, 'issue_id': 2271, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'aa26756957a07b01', 'authors': ['Jingyang Yuan', 'Huazuo Gao', 'Damai Dai', 'Junyu Luo', 'Liang Zhao', 'Zhengyan Zhang', 'Zhenda Xie', 'Y. X. Wei', 'Lean Wang', 'Zhiping Xiao', 'Yuqing Wang', 'Chong Ruan', 'Ming Zhang', 'Wenfeng Liang', 'Wangding Zeng'], 'affiliations': ['DeepSeek-AI', 'Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.11089.jpg', 'data': {'categories': ['#long_context', '#training', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Эффективное внимание для длинного контекста', 'desc': 'Статья представляет NSA - новый механизм нативно обучаемого разреженного внимания для эффективного моделирования длинного контекста в языковых моделях. NSA использует динамическую иерархическую стратегию разреженности, сочетающую сжатие токенов на грубом уровне с точным выбором токенов на детальном уровне. Авторы достигают значительного ускорения за счет оптимизации алгоритма и реализации для современного оборудования. Эксперименты показывают, что модель с NSA сохраняет или превосходит модели с полным вниманием на различных задачах, при этом обеспечивая существенное ускорение на последовательностях длиной 64 тысячи токенов.'}, 'en': {'title': 'Efficient Long-Context Modeling with Natively Trainable Sparse Attention', 'desc': 'This paper introduces NSA, a new Sparse Attention mechanism designed for efficient long-context modeling in language models. NSA combines coarse-grained token compression with fine-grained token selection to enhance both global context and local detail while reducing computational costs. The method features innovations that optimize performance on modern hardware and allow for end-to-end training, which cuts down on pretraining time without losing effectiveness. Experimental results demonstrate that NSA not only matches but often surpasses traditional Full Attention models in various tasks, proving its efficiency and capability in handling long sequences.'}, 'zh': {'title': '高效长上下文建模的新突破', 'desc': '长上下文建模对下一代语言模型至关重要，但标准注意力机制的高计算成本带来了显著的挑战。稀疏注意力提供了一种有前景的方向，可以在保持模型能力的同时提高效率。我们提出了NSA，一种原生可训练的稀疏注意力机制，结合了算法创新和硬件优化，以实现高效的长上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的令牌压缩和细粒度的令牌选择，既保留了全局上下文意识，又保持了局部精度。'}}}, {'id': 'https://huggingface.co/papers/2502.12152', 'title': 'Learning Getting-Up Policies for Real-World Humanoid Robots', 'url': 'https://huggingface.co/papers/2502.12152', 'abstract': 'Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, which necessitates accurately modeling the collision geometry and sparser rewards. We address these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). To the best of our knowledge, this is the first successful demonstration of learned getting-up policies for human-sized humanoid robots in the real world. Project page: https://humanoid-getup.github.io/', 'score': 30, 'issue_id': 2266, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'c87382b187d7b745', 'authors': ['Xialin He', 'Runpei Dong', 'Zixuan Chen', 'Saurabh Gupta'], 'affiliations': ['Simon Fraser University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.12152.jpg', 'data': {'categories': ['#training', '#games', '#robotics', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Роботы учатся вставать: прорыв в адаптивном управлении гуманоидами', 'desc': 'Эта статья описывает разработку системы машинного обучения для создания контроллеров, позволяющих гуманоидным роботам вставать из различных положений на разных поверхностях. Авторы применяют двухэтапный подход с использованием куррикулума: сначала находится оптимальная траектория подъема, а затем она оптимизируется для плавности и устойчивости. Система успешно протестирована на реальном гуманоидном роботе G1 в различных условиях, включая скользкие и наклонные поверхности. Это первая успешная демонстрация обученных стратегий подъема для гуманоидных роботов человеческого размера в реальном мире.'}, 'en': {'title': 'Learning to Get Up: Humanoid Robots Rise to the Challenge!', 'desc': 'This paper presents a novel learning framework for enabling humanoid robots to recover from falls by getting up from various positions and terrains. The approach consists of a two-phase curriculum where the first phase focuses on discovering effective getting-up trajectories with minimal constraints, while the second phase refines these motions to ensure they are smooth and robust. The framework addresses the complexities of contact patterns and collision geometry, which are critical for the getting-up task. The results demonstrate successful real-world applications, allowing a humanoid robot to rise from both face-up and face-down positions on challenging surfaces.'}, 'zh': {'title': '人形机器人自主起身的创新学习框架', 'desc': '本文提出了一种学习框架，使人形机器人能够从不同的姿势和地形中自行站起。由于人形机器人在跌倒后可能处于多种复杂的姿势，设计控制器非常困难。我们采用了两阶段的方法，第一阶段发现适合的起身轨迹，第二阶段则将这些轨迹优化为平滑且稳健的动作。实验表明，该方法使得人形机器人能够在多种真实环境中成功站起。'}}}, {'id': 'https://huggingface.co/papers/2502.12115', 'title': 'SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?', 'url': 'https://huggingface.co/papers/2502.12115', 'abstract': 'We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.', 'score': 23, 'issue_id': 2266, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '0b2638455d4393b0', 'authors': ['Samuel Miserendino', 'Michele Wang', 'Tejal Patwardhan', 'Johannes Heidecke'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.12115.jpg', 'data': {'categories': ['#dataset', '#science', '#benchmark', '#open_source'], 'emoji': '💻', 'ru': {'title': 'SWE-Lancer: Измеряем возможности ИИ в реальных задачах разработки ПО', 'desc': 'SWE-Lancer - это новый бенчмарк для оценки систем искусственного интеллекта в области разработки программного обеспечения. Он включает более 1400 реальных задач с платформы Upwork общей стоимостью 1 миллион долларов. Задачи разделены на инженерные (от исправления ошибок до разработки крупных функций) и управленческие. Оценка производится с помощью автоматических тестов и сравнения с решениями опытных разработчиков.'}, 'en': {'title': "Unlocking AI's Potential in Freelance Software Engineering", 'desc': "SWE-Lancer is a new benchmark that includes over 1,400 freelance software engineering tasks sourced from Upwork, with a total value of $1 million. It features both independent tasks, like bug fixes and large feature implementations, and managerial tasks that require decision-making between different technical proposals. The tasks are rigorously evaluated, with independent tasks tested by experienced engineers and managerial decisions compared to those made by original engineering managers. Despite the evaluation, current advanced models struggle to complete most tasks, highlighting the need for further research in AI's economic implications."}, 'zh': {'title': 'SWE-Lancer：推动AI模型经济影响研究的基准', 'desc': '我们介绍了SWE-Lancer，这是一个包含1400多个来自Upwork的自由软件工程任务的基准，任务总价值达到100万美元。SWE-Lancer包括独立的工程任务，如50个bug修复和价值32000美元的功能实现，以及管理任务，模型需要在技术实施提案中进行选择。独立任务通过经验丰富的软件工程师进行三重验证的端到端测试来评分，而管理决策则与原雇佣的工程经理的选择进行比较。我们评估了模型的表现，发现前沿模型仍然无法解决大多数任务，SWE-Lancer的开源将促进未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2502.11190', 'title': 'ReLearn: Unlearning via Learning for Large Language Models', 'url': 'https://huggingface.co/papers/2502.11190', 'abstract': 'Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.', 'score': 17, 'issue_id': 2266, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '83a57c7c971f5060', 'authors': ['Haoming Xu', 'Ningyuan Zhao', 'Liming Yang', 'Sendong Zhao', 'Shumin Deng', 'Mengru Wang', 'Bryan Hooi', 'Nay Oo', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Harbin Institute of Technology', 'National University of Singapore', 'Tsinghua University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11190.jpg', 'data': {'categories': ['#data', '#training', '#hallucinations', '#open_source', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'ReLearn: эффективное разобучение без потери качества', 'desc': 'В статье представлен новый метод ReLearn для эффективного разобучения больших языковых моделей. В отличие от существующих подходов, ReLearn использует аугментацию данных и дообучение, что позволяет избежать нарушения когерентности текста. Авторы также предлагают новые метрики оценки: KFR, KRR и LS. Эксперименты показывают, что ReLearn успешно удаляет целевые знания, сохраняя при этом высокое качество генерации текста.'}, 'en': {'title': 'ReLearn: Effective Unlearning Without Sacrificing Coherence', 'desc': "This paper introduces ReLearn, a novel approach for unlearning in large language models that avoids the pitfalls of reverse optimization, which can harm the model's ability to predict subsequent tokens. ReLearn employs a data augmentation and fine-tuning pipeline that effectively targets specific knowledge for removal while maintaining the fluency and relevance of generated text. The authors propose new evaluation metrics, including Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR), to assess the balance between forgetting and retaining knowledge, alongside a Linguistic Score (LS) for output quality. Experimental results demonstrate that ReLearn achieves its unlearning goals without sacrificing the coherence of the model's text generation capabilities."}, 'zh': {'title': 'ReLearn：高效去学习与语言生成的完美结合', 'desc': '当前的大型语言模型的去学习方法通常依赖于反向优化来降低目标词的概率。然而，这种方法会干扰后续词的预测，导致模型性能和语言连贯性下降。此外，现有的评估指标过于强调上下文遗忘，而对响应的流畅性和相关性评估不足。为了解决这些问题，我们提出了ReLearn，一个有效的去学习的数据增强和微调流程，并引入了全面的评估框架。'}}}, {'id': 'https://huggingface.co/papers/2502.09061', 'title': 'CRANE: Reasoning with constrained LLM generation', 'url': 'https://huggingface.co/papers/2502.09061', 'abstract': 'Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.', 'score': 16, 'issue_id': 2264, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '4a44947deeb14cf4', 'authors': ['Debangshu Banerjee', 'Tarun Suresh', 'Shubham Ugare', 'Sasa Misailovic', 'Gagandeep Singh'], 'affiliations': ['Department of Computer Science, University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.09061.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#reasoning', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Баланс между ограничениями и рассуждением в языковых моделях', 'desc': 'Эта статья исследует проблему генерации синтаксически и семантически корректных выходных данных языковыми моделями (LLM) для задач, требующих формального рассуждения. Авторы предлагают теоретическое объяснение, почему строгое ограничение выходных данных LLM может снижать их способности к рассуждению. Они представляют алгоритм CRANE, который балансирует между корректностью ограниченной генерации и гибкостью неограниченной генерации. Эксперименты показывают, что CRANE значительно превосходит существующие методы ограниченного и неограниченного декодирования на сложных задачах символьного рассуждения.'}, 'en': {'title': 'Balancing Correctness and Reasoning in LLM Outputs with CRANE', 'desc': "This paper discusses the challenges of generating outputs from large language models (LLMs) that are both correct in form and meaning, especially in tasks like code generation and symbolic reasoning. It explains that overly strict constraints on the grammar can hinder the model's reasoning abilities. The authors propose a new approach called CRANE, which enhances the output grammar with additional rules to maintain reasoning capabilities while ensuring syntactic and semantic correctness. Their experiments show that CRANE outperforms existing methods, achieving significant accuracy improvements on difficult reasoning tasks."}, 'zh': {'title': '平衡推理能力与生成正确性的创新解码算法', 'desc': '本研究探讨了如何在生成代码和符号数学推理等任务中，确保大型语言模型（LLM）输出的语法和语义正确性。我们发现，过于严格的语法约束会降低模型的推理能力。为了解决这个问题，我们提出了一种新的解码算法CRANE，通过增加精心设计的额外规则，既能保持输出的正确性，又能增强推理能力。实验结果表明，CRANE在多个基准测试中显著优于现有的解码策略，提升了符号推理任务的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.12148', 'title': 'HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.12148', 'abstract': 'The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: https://github.com/Gen-Verse/HermesFlow', 'score': 14, 'issue_id': 2265, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '09e80125d90fd3df', 'authors': ['Ling Yang', 'Xinchen Zhang', 'Ye Tian', 'Chenming Shang', 'Minghao Xu', 'Wentao Zhang', 'Bin Cui'], 'affiliations': ['Mila - Quebec AI Institute', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12148.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#alignment'], 'emoji': '🌉', 'ru': {'title': 'HermesFlow: мост между пониманием и генерацией в мультимодальных ИИ', 'desc': 'Статья представляет новый фреймворк HermesFlow для мультимодальных больших языковых моделей (MLLM). Авторы обнаружили, что способности MLLM к пониманию обычно превосходят их генеративные возможности. HermesFlow призван устранить этот разрыв, используя гомологичные данные предпочтений для обучения. Фреймворк применяет методы Pair-DPO и итеративной оптимизации для эффективного выравнивания мультимодального понимания и генерации. Эксперименты показывают превосходство HermesFlow над существующими методами в сокращении разрыва между пониманием и генерацией в MLLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing Understanding and Generation in MLLMs with HermesFlow', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) and identifies a key issue: these models often understand information better than they can generate it. The authors introduce HermesFlow, a new framework that aims to improve the balance between understanding and generation in MLLMs. By using homologous data to create preference data for both tasks, HermesFlow employs Pair-DPO and self-play optimization to align these capabilities more effectively. Experimental results show that HermesFlow significantly reduces the performance gap between understanding and generation, suggesting its potential as a foundational model for future multimodal applications.'}, 'zh': {'title': 'HermesFlow：缩小理解与生成的差距', 'desc': '这篇论文探讨了自回归范式在多模态大型语言模型（MLLMs）中的成功，特别是像Show-o、Transfusion和Emu3这样的模型在图像理解和生成方面的进展。研究发现，MLLMs的理解能力通常强于生成能力，两者之间存在显著差距。为了解决这个问题，论文提出了HermesFlow框架，通过使用同源数据来优化理解和生成之间的对齐。实验结果表明，HermesFlow在缩小多模态理解与生成之间的差距方面优于之前的方法，展示了其作为下一代多模态基础模型对齐框架的潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.08745', 'title': 'IHEval: Evaluating Language Models on Following the Instruction Hierarchy', 'url': 'https://huggingface.co/papers/2502.08745', 'abstract': "The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.", 'score': 13, 'issue_id': 2279, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'b61f4ac9281295ed', 'authors': ['Zhihan Zhang', 'Shiyang Li', 'Zixuan Zhang', 'Xin Liu', 'Haoming Jiang', 'Xianfeng Tang', 'Yifan Gao', 'Zheng Li', 'Haodong Wang', 'Zhaoxuan Tan', 'Yichuan Li', 'Qingyu Yin', 'Bing Yin', 'Meng Jiang'], 'affiliations': ['Amazon', 'University of Notre Dame', 'Worcester Polytechnic Institute'], 'pdf_title_img': 'assets/pdf/title_img/2502.08745.jpg', 'data': {'categories': ['#optimization', '#open_source', '#multimodal', '#alignment', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'Иерархия инструкций: ахиллесова пята языковых моделей', 'desc': 'Статья представляет новый бенчмарк IHEval для оценки способности языковых моделей следовать иерархии инструкций. Бенчмарк содержит 3,538 примеров в девяти задачах, охватывающих случаи согласованных и конфликтующих инструкций разных приоритетов. Оценка популярных ЯМ показала их трудности в распознавании приоритетов инструкций и резкое снижение производительности при конфликтующих инструкциях. Лучшая модель с открытым исходным кодом достигла лишь 48% точности в разрешении таких конфликтов.'}, 'en': {'title': 'Enhancing Language Models: Prioritizing Instructions for Better Performance', 'desc': 'This paper discusses the importance of instruction hierarchy in language models (LMs), which helps prioritize system messages, user messages, and conversation history. The authors introduce IHEval, a new benchmark with 3,538 examples across nine tasks to evaluate how well LMs follow these instruction priorities. Their findings reveal that popular LMs struggle significantly when faced with conflicting instructions, showing a notable drop in performance. The results indicate a critical need for improvements in LMs to better handle instruction hierarchies and conflicts in the future.'}, 'zh': {'title': '提升语言模型的指令理解能力', 'desc': '本文探讨了指令层级的重要性，指令层级从系统消息到用户消息、对话历史和工具输出建立了优先顺序。这一主题在语言模型（LMs）中受到的关注有限，缺乏全面的基准来评估模型遵循指令层级的能力。为此，我们引入了IHEval，这是一个新颖的基准，包含3,538个示例，涵盖了不同优先级指令一致或冲突的九个任务。我们的评估显示，流行的语言模型在识别指令优先级方面存在困难，尤其在面对冲突指令时，性能显著下降。'}}}, {'id': 'https://huggingface.co/papers/2502.11196', 'title': 'How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training', 'url': 'https://huggingface.co/papers/2502.11196', 'abstract': 'Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.', 'score': 12, 'issue_id': 2266, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'd97eafe0888b9da4', 'authors': ['Yixin Ou', 'Yunzhi Yao', 'Ningyu Zhang', 'Hui Jin', 'Jiacheng Sun', 'Shumin Deng', 'Zhenguo Li', 'Huajun Chen'], 'affiliations': ['Huawei Noahs Ark Lab', 'National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11196.jpg', 'data': {'categories': ['#data', '#training', '#architecture', '#transfer_learning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны обучения нейросетей: эволюция цепей знаний в LLM', 'desc': "Это исследование посвящено изучению механизмов усвоения новых знаний в больших языковых моделях (LLM). Авторы анализируют эволюцию 'цепей знаний' - вычислительных подграфов, ответственных за хранение и обработку информации. Они обнаружили, что усвоение новых знаний зависит от их связи с уже имеющимися, а эволюция цепей знаний проходит фазы формирования и оптимизации. Результаты могут помочь улучшить стратегии непрерывного предобучения моделей."}, 'en': {'title': 'Understanding Knowledge Integration in Large Language Models', 'desc': 'This paper explores how Large Language Models (LLMs) learn and store new knowledge within their neural networks. It introduces the concept of knowledge circuit evolution, which refers to the computational pathways that help LLMs process and retain information. The study finds that new knowledge is better integrated when it relates to what the model already knows, and that the process of knowledge circuit evolution shifts from forming new connections to optimizing existing ones. Additionally, the research reveals that this evolution tends to move from deeper to shallower layers of the model, offering insights that could improve continual pre-training methods for LLMs.'}, 'zh': {'title': '理解大型语言模型的知识内化', 'desc': '大型语言模型（LLMs）在知识密集型任务中表现出色，但它们在理解如何内化新知识方面存在重要缺口。本文通过知识电路演化的视角，识别出促进知识存储和处理的计算子图。我们的系统分析表明，新知识的获取受已有知识相关性的影响，知识电路的演化经历从形成到优化的明显阶段转变，并且演化模式呈现由深到浅的特征。这些发现不仅推动了我们对LLMs中新知识获取机制的理论理解，还有助于改进持续预训练策略，从而提升模型性能。'}}}, {'id': 'https://huggingface.co/papers/2502.10458', 'title': 'I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models', 'url': 'https://huggingface.co/papers/2502.10458', 'abstract': 'This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the LLM decoder shares the same input feature space with diffusion decoders that use the corresponding LLM encoder for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff.', 'score': 11, 'issue_id': 2270, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'bd036baca649b696', 'authors': ['Zhenxing Mi', 'Kuan-Chieh Wang', 'Guocheng Qian', 'Hanrong Ye', 'Runtao Liu', 'Sergey Tulyakov', 'Kfir Aberman', 'Dan Xu'], 'affiliations': ['Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST)', 'Snap Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2502.10458.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#diffusion', '#benchmark', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'ThinkDiff: мультимодальное рассуждение для диффузионных моделей', 'desc': 'ThinkDiff - это новая парадигма выравнивания, которая наделяет диффузионные модели текст-изображение способностями мультимодального понимания и рассуждения в контексте, интегрируя сильные стороны моделей зрения-языка (VLM). Метод использует обучение зрению-языку как прокси-задачу, выравнивая VLM с декодером энкодер-декодерной большой языковой модели (LLM) вместо диффузионного декодера. ThinkDiff эффективно раскрывает возможности понимания, рассуждения и композиции в диффузионных моделях без сложного обучения и наборов данных. Эксперименты показывают значительное улучшение точности с 19.2% до 46.3% на сложном бенчмарке CoBSAT для генерации мультимодальных рассуждений в контексте.'}, 'en': {'title': 'Empowering Diffusion Models with Multimodal Reasoning', 'desc': 'ThinkDiff is a new approach that enhances text-to-image diffusion models by integrating vision-language models (VLMs) for better understanding and reasoning. Unlike previous methods that focused on pixel-level accuracy, ThinkDiff emphasizes in-context reasoning, overcoming challenges related to complex datasets. It aligns VLMs with the decoder of a large language model (LLM), simplifying the training process and improving performance. Experiments show that ThinkDiff significantly boosts accuracy on multimodal reasoning tasks and excels in creating coherent images from multiple inputs.'}, 'zh': {'title': 'ThinkDiff：提升文本到图像的推理能力', 'desc': '本文提出了ThinkDiff，这是一种新颖的对齐范式，旨在通过整合视觉-语言模型（VLMs）的优势，增强文本到图像扩散模型的多模态上下文理解和推理能力。现有的多模态扩散微调方法主要关注像素级重建，而忽视了上下文推理，并受到推理基础数据集复杂性和有限性的限制。ThinkDiff通过将视觉-语言训练作为代理任务，解决了这些挑战，将VLM与编码器-解码器大型语言模型（LLM）的解码器对齐，而不是扩散解码器。实验表明，ThinkDiff在多模态上下文推理生成的CoBSAT基准测试中，准确率从19.2%显著提高到46.3%，仅需在4个A100 GPU上训练5小时。'}}}, {'id': 'https://huggingface.co/papers/2502.11167', 'title': 'SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors', 'url': 'https://huggingface.co/papers/2502.11167', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE.', 'score': 11, 'issue_id': 2266, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '99e183fd31de3be0', 'authors': ['Bohan Lyu', 'Siqiao Huang', 'Zichen Liang'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua', 'Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua'], 'pdf_title_img': 'assets/pdf/title_img/2502.11167.jpg', 'data': {'categories': ['#training', '#plp', '#dataset', '#agi', '#open_source', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'LLM как виртуальные исполнители кода: возможности и ограничения', 'desc': 'Исследователи представили SURGE - комплексный бенчмарк для оценки способности больших языковых моделей (LLM) предсказывать результаты выполнения кода без его фактического запуска. Бенчмарк охватывает восемь ключевых аспектов, включая задачи на разных языках программирования, анализ репозиториев и научные вычисления. Оценка различных LLM на SURGE показала, что модели могут предсказывать результаты выполнения кода в некоторых случаях, но имеют ограничения в качестве универсальных суррогатных исполнителей. Исследование дает эмпирическое представление о возможности использования LLM в качестве суррогатных исполнителей кода.'}, 'en': {'title': 'Exploring LLMs as Surrogate Code Executors with SURGE', 'desc': 'This paper explores the potential of large language models (LLMs) to act as surrogate code executors, which means predicting the output of code without actually running it. The authors introduce a benchmark called SURGE, which tests LLMs on various programming tasks, including multi-language support and analysis of complex algorithms. They evaluate different LLMs to see how well they can predict code execution results and identify common errors in their predictions. The results show that while LLMs can succeed in some scenarios, they still have significant limitations in general-purpose code execution prediction.'}, 'zh': {'title': '探索大型语言模型作为代码执行器的潜力', 'desc': '大型语言模型（LLMs）在代码理解和生成方面表现出色，但它们能否作为通用的替代代码执行器来预测程序的输出和行为仍然是一个重要问题。为此，我们提出了SURGE，一个涵盖八个关键方面的综合基准，包括多语言编程任务和高成本科学计算等。我们对多种开源和专有的LLMs进行了评估，并研究了模型规模和训练数据规模对替代执行准确性的影响。研究结果表明，尽管LLMs在某些情况下能够预测代码执行结果，但在通用替代执行方面仍存在局限性。'}}}, {'id': 'https://huggingface.co/papers/2502.12146', 'title': 'Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening', 'url': 'https://huggingface.co/papers/2502.12146', 'abstract': 'We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening', 'score': 9, 'issue_id': 2265, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '25a69f8cf847067e', 'authors': ['Ye Tian', 'Ling Yang', 'Xinchen Zhang', 'Yunhai Tong', 'Mengdi Wang', 'Bin Cui'], 'affiliations': ['Peking University', 'Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12146.jpg', 'data': {'categories': ['#training', '#rlhf', '#optimization', '#diffusion', '#alignment', '#rl'], 'emoji': '🎯', 'ru': {'title': 'Оптимизация траекторий для повышения точности диффузионных моделей', 'desc': 'Авторы предлагают метод Diffusion-Sharpening для улучшения точности генеративных диффузионных моделей. Этот подход оптимизирует траектории сэмплирования во время обучения, используя интегралы по путям и обратную связь по вознаграждению. Diffusion-Sharpening демонстрирует превосходную эффективность обучения и вывода по сравнению с существующими методами. Эксперименты показывают, что предложенный метод превосходит другие подходы по различным метрикам, включая согласованность текста и предпочтения людей.'}, 'en': {'title': 'Optimize Sampling Paths for Better Model Alignment!', 'desc': 'The paper introduces Diffusion-Sharpening, a novel fine-tuning method that improves the alignment of machine learning models by optimizing the paths taken during sampling. Unlike traditional reinforcement learning (RL) methods that focus on individual training steps, this approach considers the entire trajectory, which enhances overall performance. By employing a path integral framework, Diffusion-Sharpening efficiently selects the best trajectories while minimizing inference costs. Experimental results show that this method not only converges faster but also achieves better performance than existing RL-based and trajectory optimization techniques across various evaluation metrics.'}, 'zh': {'title': '扩散锐化：高效的微调新方法', 'desc': '我们提出了一种名为扩散锐化（Diffusion-Sharpening）的微调方法，通过优化采样轨迹来增强下游对齐。现有的基于强化学习的微调方法主要关注单个训练时间步，忽视了轨迹级别的对齐，而最近的采样轨迹优化方法则带来了显著的推理成本。扩散锐化通过使用路径积分框架在训练过程中选择最佳轨迹，利用奖励反馈并摊销推理成本，从而克服了这些问题。我们的实验表明，扩散锐化在训练效率和推理效率上均优于现有的微调方法，提供了一种可扩展且高效的未来扩散模型微调解决方案。'}}}, {'id': 'https://huggingface.co/papers/2502.11831', 'title': 'Intuitive physics understanding emerges from self-supervised pretraining on natural videos', 'url': 'https://huggingface.co/papers/2502.11831', 'abstract': 'We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.', 'score': 6, 'issue_id': 2270, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '269bf0ee605c4537', 'authors': ['Quentin Garrido', 'Nicolas Ballas', 'Mahmoud Assran', 'Adrien Bardes', 'Laurent Najman', 'Michael Rabbat', 'Emmanuel Dupoux', 'Yann LeCun'], 'affiliations': ['EHESS', 'FAIR at Meta', 'Univ Gustave Eiffel'], 'pdf_title_img': 'assets/pdf/title_img/2502.11831.jpg', 'data': {'categories': ['#reasoning', '#video', '#architecture', '#multimodal', '#agi'], 'emoji': '🧠', 'ru': {'title': 'Интуитивная физика возникает в нейросетях без предварительного программирования', 'desc': 'Исследователи изучали возникновение интуитивного понимания физики в нейронных сетях, обученных предсказывать скрытые области в видео. Используя метод нарушения ожиданий, они обнаружили, что модели, предсказывающие результаты в выученном пространстве представлений, демонстрируют понимание базовых физических свойств. Напротив, модели, работающие с пиксельным пространством, и мультимодальные языковые модели показали результаты близкие к случайным. Исследование показывает, что для приобретения интуитивного понимания физики достаточно совместного обучения абстрактному пространству представлений и предсказания недостающих частей сенсорного ввода.'}, 'en': {'title': 'Learning Intuitive Physics Through Video Prediction', 'desc': 'This paper explores how deep neural networks can learn intuitive physics by predicting missing parts of videos. The researchers found that models trained in a learned representation space can grasp concepts like object permanence and shape consistency. In contrast, models that operate directly on pixel data or rely on text reasoning perform poorly. The study suggests that understanding intuitive physics can emerge from training on sensory input without needing pre-existing core knowledge.'}, 'zh': {'title': '通过预测学习直观物理理解', 'desc': '本研究探讨了通用深度神经网络模型在预测自然视频中被遮挡区域时，如何产生直观物理理解。我们发现，经过训练的模型在学习的表示空间中预测结果时，能够理解物体的持久性和形状一致性等直观物理特性。相比之下，在像素空间中进行视频预测的模型和通过文本推理的多模态大型语言模型，其表现接近随机水平。我们的研究表明，联合学习抽象表示空间并预测感官输入的缺失部分，足以获得直观物理的理解，甚至在仅用一周独特视频训练的模型也能表现出超出随机的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.11438', 'title': 'SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL', 'url': 'https://huggingface.co/papers/2502.11438', 'abstract': 'Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.', 'score': 6, 'issue_id': 2264, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '93b545df707b1538', 'authors': ['Jimin Lee', 'Ingeol Baek', 'Byeongjeong Kim', 'Hwanhee Lee'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.11438.jpg', 'data': {'categories': ['#data', '#dataset', '#transfer_learning', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Самоусиление ИИ в преобразовании текста в SQL', 'desc': 'SAFE-SQL - это новый подход к преобразованию естественного языка в SQL-запросы. Он использует большие языковые модели для генерации и фильтрации релевантных примеров, улучшая процесс обучения в контексте. SAFE-SQL превосходит предыдущие методы в задачах zero-shot и few-shot, достигая более высокой точности выполнения запросов. Особенно эффективен в сложных и ранее не встречавшихся сценариях, где традиционные методы часто не справляются.'}, 'en': {'title': 'Transforming Language to SQL with Self-Augmented Learning', 'desc': 'This paper introduces SAFE-SQL, a new framework for converting natural language questions into SQL queries. It addresses the limitations of previous methods that rely on existing training examples, which may not be available in real-world situations. SAFE-SQL enhances SQL generation by creating and filtering self-generated examples using a large language model (LLM). The framework demonstrates improved execution accuracy, especially in challenging and unseen scenarios, outperforming traditional zero-shot and few-shot approaches.'}, 'zh': {'title': '自我增强，提升Text-to-SQL的准确性', 'desc': '本文提出了一种新的框架SAFE-SQL，用于将自然语言问题转换为可执行的SQL查询。该框架通过自我增强的上下文学习和细粒度示例选择来提高SQL生成的质量。SAFE-SQL首先生成多个与测试输入相关的Text-to-SQL示例，然后通过三种相关性评估对这些示例进行过滤，从而构建高质量的学习示例。与传统的零样本和少样本方法相比，SAFE-SQL在执行准确性上取得了显著提升，尤其在困难和未见过的场景中表现更佳。'}}}, {'id': 'https://huggingface.co/papers/2502.11357', 'title': 'Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents', 'url': 'https://huggingface.co/papers/2502.11357', 'abstract': 'Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.', 'score': 5, 'issue_id': 2277, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '26d055a62173bda2', 'authors': ['Vardaan Pahuja', 'Yadong Lu', 'Corby Rosset', 'Boyu Gou', 'Arindam Mitra', 'Spencer Whitehead', 'Yu Su', 'Ahmed Awadallah'], 'affiliations': ['Microsoft Research, Redmond', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11357.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#synthetic', '#dataset', '#agents', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'Масштабные данные - ключ к совершенствованию веб-агентов', 'desc': 'Статья представляет новый подход к созданию крупномасштабного набора данных для обучения мультимодальных веб-агентов. Авторы разработали масштабируемый метод синтеза более 94 тысяч успешных мультимодальных веб-траекторий, охватывающих 49 тысяч уникальных URL-адресов. На основе этого набора данных был обучен веб-агент Explorer, показавший высокую производительность на различных бенчмарках. Исследование подчеркивает важность увеличения объема данных для улучшения возможностей веб-агентов.'}, 'en': {'title': 'Unlocking Web Tasks with Scalable Multimodal Datasets', 'desc': 'This paper presents a solution to the challenge of training large multimodal models (LMMs) for web tasks by creating a vast and diverse dataset of web trajectories. The dataset includes over 94,000 successful multimodal web interactions, which were synthesized through extensive web exploration, making it cost-effective to produce. The authors introduce Explorer, a multimodal web agent trained on this dataset, which shows improved performance on various benchmarks compared to previous models. The study emphasizes the importance of data scaling in enhancing the capabilities of web agents, aiming to make advanced LMM research more accessible to the community.'}, 'zh': {'title': '合成多样化数据集，提升多模态代理能力', 'desc': '本文介绍了一种新方法，用于合成大型多模态模型（LMM）所需的多样化轨迹级数据集。我们创建了一个包含超过94,000个成功的多模态网络轨迹的数据集，涵盖49,000个独特的URL和720,000个截图。通过广泛的网络探索和任务意图的细化，我们能够以低成本收集多样化的数据。我们的实验表明，数据规模的扩大是提升网络代理能力的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2502.12135', 'title': 'MagicArticulate: Make Your 3D Models Articulation-Ready', 'url': 'https://huggingface.co/papers/2502.12135', 'abstract': 'With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate.', 'score': 5, 'issue_id': 2270, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'ff42a91250856a78', 'authors': ['Chaoyue Song', 'Jianfeng Zhang', 'Xiu Li', 'Fan Yang', 'Yiwen Chen', 'Zhongcong Xu', 'Jun Hao Liew', 'Xiaoyang Guo', 'Fayao Liu', 'Jiashi Feng', 'Guosheng Lin'], 'affiliations': ['ByteDance Seed', 'Institute for Inforcomm Research, A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12135.jpg', 'data': {'categories': ['#3d', '#benchmark', '#dataset', '#architecture'], 'emoji': '🦾', 'ru': {'title': 'Магия оживления 3D: от статики к реалистичной анимации', 'desc': 'MagicArticulate - это фреймворк для автоматического преобразования статичных 3D-моделей в готовые к анимации версии. Авторы представили Articulation-XL - крупномасштабный набор данных с более чем 33 тысячами аннотированных 3D-моделей. Они предложили новый метод генерации скелета, использующий авторегрессионный трансформер для обработки различного количества костей и суставов. Для предсказания весов скиннинга применяется функциональный процесс диффузии с учетом объемных геодезических расстояний между вершинами и суставами.'}, 'en': {'title': 'Transforming 3D Models for Realistic Animation with MagicArticulate', 'desc': 'This paper introduces MagicArticulate, a framework designed to convert static 3D models into articulation-ready versions for realistic animation. It addresses the challenges of manual annotation by providing Articulation-XL, a large-scale benchmark with over 33,000 3D models and high-quality articulation annotations. The framework employs a novel skeleton generation method using an auto-regressive transformer to manage varying bone structures and their dependencies. Additionally, it utilizes a functional diffusion process to predict skinning weights, resulting in superior performance in generating articulated models compared to existing methods.'}, 'zh': {'title': '自动化3D模型关节化的革命性框架', 'desc': '随着3D内容创作的快速增长，自动将静态3D模型转换为可进行真实动画的关节模型的需求日益增加。传统方法依赖于手动标注，既耗时又费力，且缺乏大规模基准测试限制了基于学习的解决方案的发展。我们提出了MagicArticulate框架，能够自动将静态3D模型转化为适合关节动画的资产。我们的主要贡献包括建立了Articulation-XL基准、提出了一种新颖的骨架生成方法，并使用功能扩散过程预测蒙皮权重，显著提升了动画质量。'}}}, {'id': 'https://huggingface.co/papers/2502.11275', 'title': "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest", 'url': 'https://huggingface.co/papers/2502.11275', 'abstract': "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.", 'score': 5, 'issue_id': 2263, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '6444052efad6f8be', 'authors': ['Letian Peng', 'Zilong Wang', 'Feng Yao', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.11275.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#data', '#transfer_learning'], 'emoji': '🐣', 'ru': {'title': 'Извлечение информации на плечах гигантов: как IE модели могут использовать ресурсы LLM', 'desc': "Исследователи представили новый подход к извлечению информации (IE) с использованием ресурсов больших языковых моделей (LLM). Метод под названием 'извлечение следующих токенов' (NTE) позволяет переформулировать задачу предсказания следующего токена в задачу извлечения уже присутствующих в контексте токенов. Модель Cuckoo, обученная на 102,6 млн примеров извлекательных данных, показывает лучшие результаты в условиях малого количества обучающих примеров по сравнению с существующими предобученными IE моделями. Этот подход позволяет IE моделям автоматически развиваться вместе с улучшениями в подготовке данных для LLM без дополнительных ручных усилий."}, 'en': {'title': 'Leveraging LLMs for Enhanced Information Extraction', 'desc': "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."}, 'zh': {'title': '利用LLM提升信息提取模型的性能', 'desc': '本文探讨了如何利用大型语言模型（LLM）来提升信息提取（IE）模型的性能。我们提出了一种新的提取方法，称为下一标记提取（NTE），通过将下一个标记预测转化为对上下文中已存在标记的提取，从而使IE模型能够利用LLM的资源。我们开发的Cuckoo模型在少量样本的情况下，能够有效适应传统和复杂的指令跟随IE任务，并且表现优于现有的预训练IE模型。Cuckoo作为一个“搭便车者”，能够随着LLM数据准备的进步而自然演变，无需额外的人工努力。'}}}, {'id': 'https://huggingface.co/papers/2502.11157', 'title': 'Dyve: Thinking Fast and Slow for Dynamic Process Verification', 'url': 'https://huggingface.co/papers/2502.11157', 'abstract': "We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive analysis System 2 for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation with LLM based evaluation, Dyve curates high-quality supervision signals from noisy data. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.", 'score': 4, 'issue_id': 2272, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'df0803abac7073f4', 'authors': ['Jianyuan Zhong', 'Zeju Li', 'Zhijian Xu', 'Xiangyu Wen', 'Qiang Xu'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.11157.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#optimization', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Dyve: умное обнаружение ошибок в ИИ-рассуждениях', 'desc': 'Представлен Dyve - динамический верификатор процессов, улучшающий обнаружение ошибок рассуждения в больших языковых моделях. Он сочетает быстрое и медленное мышление, вдохновленное теорией Канемана о двух системах. Dyve использует новую технику пошаговой фильтрации консенсуса для создания качественных обучающих сигналов из зашумленных данных. Эксперименты на наборах данных ProcessBench и MATH показывают, что Dyve значительно превосходит существующие верификаторы на основе процессов.'}, 'en': {'title': 'Dyve: Enhancing Language Model Reasoning with Dual Thinking Strategies', 'desc': 'Dyve is a dynamic process verifier designed to improve error detection in large language models by utilizing two types of reasoning: fast (System 1) and slow (System 2) thinking. It applies quick, token-level checks for simple tasks while employing in-depth analysis for more complex processes. The system uses a unique method of step-wise consensus filtering, which combines Monte Carlo estimation with evaluations from large language models to generate reliable supervision signals from noisy data. Experiments show that Dyve outperforms current process verifiers and enhances performance in competitive settings.'}, 'zh': {'title': 'Dyve：提升语言模型推理的动态验证器', 'desc': 'Dyve是一种动态过程验证器，旨在提高大型语言模型中的推理错误检测能力。它结合了快速思维和慢速思维，灵感来源于卡尼曼的系统理论。Dyve根据任务的复杂性，灵活地应用即时的token级确认（系统1）和全面分析（系统2）。通过一种新颖的逐步共识过滤过程监督技术，Dyve从嘈杂数据中提取高质量的监督信号，实验结果表明其在多个数据集上显著优于现有的过程验证器。'}}}, {'id': 'https://huggingface.co/papers/2502.11775', 'title': 'video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model', 'url': 'https://huggingface.co/papers/2502.11775', 'abstract': 'While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.', 'score': 4, 'issue_id': 2265, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'ff6f52de37532ca7', 'authors': ['Guangzhi Sun', 'Yudong Yang', 'Jimin Zhuang', 'Changli Tang', 'Yixuan Li', 'Wei Li', 'Zejun MA', 'Chao Zhang'], 'affiliations': ['ByteDance', 'Tsinghua university', 'Univeristy of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.11775.jpg', 'data': {'categories': ['#reasoning', '#video', '#training', '#open_source', '#optimization', '#benchmark', '#multimodal', '#dataset'], 'emoji': '🎥', 'ru': {'title': 'Улучшение рассуждений в мультимодальных языковых моделях для понимания видео', 'desc': 'Статья представляет video-SALMONN-o1 - первую открытую аудиовизуальную языковую модель с улучшенными способностями рассуждения для общего понимания видео. Авторы разработали набор данных с интенсивными рассуждениями и метод оптимизации предпочтений процесса (pDPO) для эффективного обучения на многомодальных входных данных. Также был создан бенчмарк RivaBench для оценки способностей моделей к рассуждению при анализе видео. video-SALMONN-o1 демонстрирует значительные улучшения точности по сравнению с базовыми моделями на различных задачах понимания видео.'}, 'en': {'title': 'Revolutionizing Video Understanding with Enhanced Reasoning', 'desc': 'This paper introduces video-SALMONN-o1, an innovative open-source audio-visual large language model (LLM) aimed at improving general video understanding. It addresses the gap in reasoning capabilities for video content by creating a specialized dataset with complex audio-visual questions and detailed solutions. The authors also present process direct preference optimization (pDPO), a method that enhances reward modeling for multimodal inputs through contrastive step selection. The model demonstrates significant accuracy improvements over existing benchmarks, showcasing its effectiveness in tasks like synthetic video detection without prior training.'}, 'zh': {'title': '视频理解的新突破：video-SALMONN-o1', 'desc': '这篇论文提出了video-SALMONN-o1，这是第一个开源的增强推理音视频大语言模型，旨在解决一般视频理解任务。为了提升其推理能力，研究团队开发了一个包含具有挑战性的音视频问题和逐步解决方案的推理密集型数据集。论文还提出了过程直接偏好优化（pDPO），利用对比步骤选择实现针对多模态输入的高效步骤级奖励建模。此外，RivaBench作为第一个推理密集型视频理解基准，提供了超过4000个高质量的问题-答案对，涵盖了多种场景。'}}}, {'id': 'https://huggingface.co/papers/2502.11098', 'title': 'Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2502.11098', 'abstract': 'Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier.', 'score': 4, 'issue_id': 2265, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '9aaac2e7c7b495a4', 'authors': ['Zhao Wang', 'Sota Moriyama', 'Wei-Yao Wang', 'Briti Gangopadhyay', 'Shingo Takamatsu'], 'affiliations': ['Sony Group Corporation, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2502.11098.jpg', 'data': {'categories': ['#open_source', '#optimization', '#agents', '#multimodal', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Структурированное общение и иерархические действия для эффективного взаимодействия ИИ-агентов', 'desc': 'В статье представлена новая система TalkHier для улучшения взаимодействия между агентами на основе больших языковых моделей (LLM). TalkHier вводит структурированный протокол коммуникации и иерархическую систему уточнения для решения проблем неверных выводов и предвзятости. Система превосходит современные методы в различных задачах, включая ответы на вопросы и генерацию рекламных текстов. TalkHier демонстрирует потенциал для установления нового стандарта в многоагентных системах на основе LLM.'}, 'en': {'title': 'Enhancing Multi-Agent Collaboration with TalkHier Framework', 'desc': 'This paper introduces Talk Hierarchically, Act Structurally (TalkHier), a new framework designed to improve communication and collaboration among multi-agent systems using large language models (LLMs). It features a structured communication protocol that enhances context understanding and a hierarchical refinement system to correct errors and biases in agent outputs. The framework outperforms existing state-of-the-art models in various tasks, demonstrating its effectiveness in open-domain question answering and targeted text generation. Overall, TalkHier aims to establish a new benchmark for LLM-based multi-agent systems, promoting better teamwork and adaptability among agents.'}, 'zh': {'title': '结构化交流，分层行动的智能体协作新标准', 'desc': '本文提出了一种新的框架，称为Talk Structurally, Act Hierarchically（TalkHier），旨在改善大语言模型（LLM）多智能体系统中的通信和协作。该框架引入了一种结构化的通信协议，以便在复杂任务中进行丰富的上下文交流，并建立了一个分层的精炼系统，以解决错误输出、虚假信息和偏见等问题。实验结果表明，TalkHier在多个任务上超越了现有的最先进技术，包括开放领域问答和特定领域选择性提问等。该研究为LLM-MA系统设定了新的标准，推动了更有效、灵活和协作的多智能体框架的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.11901', 'title': 'Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity', 'url': 'https://huggingface.co/papers/2502.11901', 'abstract': "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.", 'score': 4, 'issue_id': 2263, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '9451c99877c67e4d', 'authors': ['Dylan Zhang', 'Justin Wang', 'Tianran Sun'], 'affiliations': ['Shanghai Jiaotong University', 'University of Chicago', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.11901.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#data', '#plp', '#transfer_learning', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Синтетические данные открывают новые горизонты в доказательном программировании', 'desc': 'Статья посвящена проблеме обучения языковых моделей программированию, ориентированному на доказательства. Авторы предлагают метод синтетического расширения данных для решения проблемы нехватки корпусов на языках доказательного программирования. Они создают модель PoPilot, которая превосходит GPT-4 на 64% в задачах проектного уровня. Метод также позволяет улучшить результаты GPT-4 на 54% путем исправления его выходных данных.'}, 'en': {'title': 'Enhancing Proof-Oriented Programming with Synthetic Data Augmentation', 'desc': 'This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks.'}, 'zh': {'title': '合成数据增强，提升证明编程能力！', 'desc': '现有的语言模型在面向证明的编程中面临数据稀缺的问题，主要体现在两个方面：缺乏足够的面向证明编程语言（如F*）的语料库，以及缺少大规模的项目级证明实现，无法教会模型复杂的推理过程。我们提出了一种基于合成数据增强的方法，专注于项目级的面向证明编程，既用于生成也用于修复。该方法通过合成基本的面向证明编程问题来解决数据稀缺问题，并结合多样化的编码数据以提高推理能力，同时在现有代码库中创建新的证明和修复数据。我们的14B参数模型PoPilot经过微调后，在项目级面向证明编程中超越了GPT-4o模型64%的性能，并通过修复其输出提高了54%的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.11748', 'title': 'ILIAS: Instance-Level Image retrieval At Scale', 'url': 'https://huggingface.co/papers/2502.11748', 'abstract': 'This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and a performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Large-scale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning a linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still a key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/', 'score': 3, 'issue_id': 2277, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '59967e50364f64f1', 'authors': ['Giorgos Kordopatis-Zilos', 'Vladan Stojnić', 'Anna Manko', 'Pavel Šuma', 'Nikolaos-Antonios Ypsilantis', 'Nikos Efthymiadis', 'Zakaria Laskar', 'Jiří Matas', 'Ondřej Chum', 'Giorgos Tolias'], 'affiliations': ['VRG, FEE, Czech Technical University in Prague'], 'pdf_title_img': 'assets/pdf/title_img/2502.11748.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ILIAS: Новый стандарт для оценки масштабного поиска изображений', 'desc': 'ILIAS - это новый набор данных для тестирования поиска изображений на уровне экземпляров в крупном масштабе. Он разработан для оценки способности современных и будущих моделей основания и методов поиска распознавать конкретные объекты. ILIAS включает запросы и положительные изображения для 1000 экземпляров объектов, собранных вручную для отражения сложных условий и разнообразных доменов. Тестирование проводится на фоне 100 миллионов отвлекающих изображений из YFCC100M, что позволяет оценить эффективность моделей в различных сценариях.'}, 'en': {'title': 'ILIAS: Advancing Instance-Level Image Retrieval with Scale and Diversity', 'desc': 'This paper presents ILIAS, a novel dataset aimed at enhancing instance-level image retrieval capabilities. It features a large-scale collection of 1,000 object instances with diverse domains and challenging conditions, evaluated against 100 million distractor images. The dataset allows for benchmarking of various models, revealing that domain-specific fine-tuning can lead to performance drops in broader contexts. Additionally, the study highlights the importance of local descriptors in retrieval tasks and notes that vision-language models perform comparably in text-to-image and image-to-image retrieval scenarios.'}, 'zh': {'title': 'ILIAS：大规模实例级图像检索的新标准', 'desc': '本研究介绍了ILIAS，这是一个用于大规模实例级图像检索的新测试数据集。它旨在评估当前和未来基础模型及检索技术识别特定对象的能力。ILIAS的优势在于其大规模、领域多样性、准确的真实标签，以及尚未饱和的性能表现。数据集中包含1000个对象实例的查询和正样本图像，手动收集以捕捉具有挑战性的条件和多样的领域。'}}}, {'id': 'https://huggingface.co/papers/2502.10550', 'title': 'Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.10550', 'abstract': "Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base - a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo - a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our contributions establish a unified framework for advancing memory RL research, driving the development of more reliable systems for real-world applications. The code is available at https://sites.google.com/view/memorybenchrobots/.", 'score': 3, 'issue_id': 2272, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '99224cee50b48e48', 'authors': ['Egor Cherepanov', 'Nikita Kachaev', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.10550.jpg', 'data': {'categories': ['#rl', '#optimization', '#benchmark', '#agents', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'MIKASA: универсальный бенчмарк для оценки памяти RL-агентов', 'desc': 'Статья представляет MIKASA - новый комплексный бенчмарк для оценки возможностей памяти агентов обучения с подкреплением. Авторы предлагают классификацию задач, требующих интенсивного использования памяти, и создают набор тестов MIKASA-Base для систематической оценки агентов с улучшенной памятью. Также разработан MIKASA-Robo - набор из 32 задач для оценки памяти в манипуляциях роботов. Этот бенчмарк призван способствовать развитию исследований в области обучения с подкреплением с использованием памяти.'}, 'en': {'title': 'MIKASA: Advancing Memory in Reinforcement Learning for Robotics', 'desc': 'This paper addresses the importance of memory in reinforcement learning (RL) for agents performing complex tasks that require understanding of time and space. It highlights the lack of standardized benchmarks to evaluate memory capabilities in RL, especially in tabletop robotic manipulation scenarios. To fill this gap, the authors introduce MIKASA, a benchmark suite designed to assess memory-intensive skills in agents. MIKASA includes a classification framework, a unified benchmark called MIKASA-Base, and a set of 32 specific tasks in MIKASA-Robo to systematically evaluate memory-enhanced agents.'}, 'zh': {'title': '提升智能体记忆能力的统一基准', 'desc': '本文探讨了记忆在强化学习（RL）中的重要性，尤其是在处理复杂任务时。我们提出了MIKASA，一个全面的基准测试套件，用于评估智能体的记忆能力。MIKASA包括一个分类框架和两个基准，分别用于系统评估记忆增强智能体和设计32个记忆密集型任务。我们的工作为记忆强化学习研究提供了统一的框架，推动了更可靠系统的开发。'}}}, {'id': 'https://huggingface.co/papers/2502.12054', 'title': 'PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning', 'url': 'https://huggingface.co/papers/2502.12054', 'abstract': 'Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.', 'score': 3, 'issue_id': 2269, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '4aaf92e2d2fd9766', 'authors': ['Xinyu Zhang', 'Yuxuan Dong', 'Yanrui Wu', 'Jiaxing Huang', 'Chengyou Jia', 'Basura Fernando', 'Mike Zheng Shou', 'Lingling Zhang', 'Jun Liu'], 'affiliations': ['Institute of High-Performance Computing, A*STAR', 'Show Lab, National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12054.jpg', 'data': {'categories': ['#math', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'PhysReason: испытание физикой для искусственного интеллекта', 'desc': 'Статья представляет PhysReason - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к физическому рассуждению. Бенчмарк состоит из 1200 задач разной сложности, требующих в среднем 8.1 шагов решения. Авторы предлагают систему оценки решений и выявляют основные проблемы моделей в применении физических теорем, понимании процессов, вычислениях и анализе условий. Даже лучшие модели показывают результат ниже 60% на сложных задачах.'}, 'en': {'title': 'PhysReason: A New Benchmark for Physics-Based Reasoning in AI', 'desc': 'This paper introduces PhysReason, a new benchmark designed to evaluate the physics-based reasoning abilities of large language models. It consists of 1,200 problems, with a mix of knowledge-based and reasoning-based tasks, categorized into three difficulty levels. The benchmark highlights the complexity of physics reasoning, requiring multiple solution steps, with hard problems demanding an average of 15.6 steps. The study also identifies key challenges in physics reasoning, such as theorem application and process understanding, providing insights into the limitations of current models.'}, 'zh': {'title': '物理推理能力的新基准', 'desc': '大型语言模型在数学和逻辑推理方面表现出色，但在物理推理方面的评估仍然不足。我们提出了PhysReason，这是一个包含1200个问题的基准测试，其中75%是推理类问题，分为简单、中等和困难三个难度级别。通过引入物理解题自动评分框架，我们能够有效评估模型在物理定理应用、过程理解、计算和条件分析等方面的能力。我们的研究表明，当前顶尖模型在物理推理任务上的表现仍有待提高，尤其是在困难问题上。'}}}, {'id': 'https://huggingface.co/papers/2502.11330', 'title': 'System Message Generation for User Preferences using Open-Source Models', 'url': 'https://huggingface.co/papers/2502.11330', 'abstract': 'System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.', 'score': 3, 'issue_id': 2267, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '36ca5a9ceb25e7fa', 'authors': ['Minbyul Jeong', 'Jungho Cho', 'Minsoo Khang', 'Dawoon Jung', 'Teakgyu Hong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.11330.jpg', 'data': {'categories': ['#open_source', '#alignment', '#training', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'SysGen: улучшение соответствия ответов LLM через генерацию системных сообщений', 'desc': 'Эта статья представляет SysGen - новый метод для генерации системных сообщений для больших языковых моделей (LLM). SysGen создает системные сообщения, которые лучше соответствуют ответам ассистента, используя наборы данных для обучения с учителем без исходных системных сообщений. Обучение на данных SysGen значительно улучшило соответствие ответов модели системным сообщениям и инструкциям пользователя, что было продемонстрировано на различных моделях с открытым исходным кодом. Качественный анализ подчеркивает важность разнообразных системных сообщений для лучшей адаптации к различным контекстам.'}, 'en': {'title': 'Enhancing LLM Responses with SysGen: Better System Messages for Better Alignment', 'desc': 'This paper presents SysGen, a new method for generating system messages that help large language models (LLMs) respond more accurately to user instructions. System messages are crucial for guiding LLMs in their interactions, but there is a lack of publicly available data that includes these messages. SysGen addresses this issue by creating a pipeline that generates system messages from existing supervised fine-tuning datasets, leading to improved alignment of model responses. The results show that training with SysGen data enhances the performance of various open-source models while keeping their effectiveness on other benchmarks largely unchanged.'}, 'zh': {'title': 'SysGen：提升语言模型响应对齐性的系统消息生成', 'desc': '本论文介绍了SysGen，一个用于生成系统消息的管道，旨在提高大型语言模型（LLMs）与用户指令的对齐度。系统消息在与LLMs的交互中起着重要作用，能够帮助用户指定角色和任务。通过在没有系统消息的监督微调数据集上进行训练，SysGen显著改善了模型响应的对齐性。我们的分析表明，多样化的系统消息对于在不同上下文中实现更好的适应性至关重要。'}}}, {'id': 'https://huggingface.co/papers/2502.10454', 'title': 'One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs', 'url': 'https://huggingface.co/papers/2502.10454', 'abstract': 'Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs\' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs\' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.', 'score': 3, 'issue_id': 2265, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '1821254437fc158d', 'authors': ['Yinghui Li', 'Jiayi Kuang', 'Haojing Huang', 'Zhikun Xu', 'Xinnian Liang', 'Yi Yu', 'Wenlian Lu', 'Yangning Li', 'Xiaoyu Tan', 'Chao Qu', 'Ying Shen', 'Hai-Tao Zheng', 'Philip S. Yu'], 'affiliations': ['ARC Lab, Arizona State University', 'Bytedance Inc.', 'INFLY TECH (Shanghai) Co., Ltd.', 'Peng Cheng Laboratory', 'School of Mathematical Science, Fudan University', 'Sun-Yat Sen University', 'Tsinghua University', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2502.10454.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#optimization', '#dataset'], 'emoji': '🧮', 'ru': {'title': 'Контрпримеры как ключ к улучшению математических способностей ИИ', 'desc': 'Статья исследует способность больших языковых моделей (LLM) генерировать математические доказательства. Авторы утверждают, что текущие LLM ограничены в глубоком понимании теорем и предлагают метод обучения на контрпримерах. Они создали бенчмарк CounterMATH для оценки способности LLM доказывать утверждения через контрпримеры. Эксперименты показывают, что улучшение навыков рассуждения на основе контрпримеров критически важно для повышения общих математических способностей LLM.'}, 'en': {'title': "Enhancing LLMs' Mathematical Proofs through Counterexamples", 'desc': 'This paper discusses the limitations of current Large Language Models (LLMs) in generating mathematical proofs, emphasizing their dependence on prior exposure to proof processes during training. The authors introduce a new benchmark called CounterMATH, which challenges LLMs to prove mathematical statements by providing counterexamples, thereby testing their understanding of mathematical concepts. They also present a data engineering framework to enhance the training data for LLMs, aiming to improve their reasoning capabilities. The findings suggest that enhancing counterexample-driven reasoning is essential for advancing the mathematical proficiency of LLMs.'}, 'zh': {'title': '通过反例提升数学推理能力', 'desc': '本论文探讨了利用大型语言模型（LLMs）进行数学证明生成的能力。我们认为，当前LLMs的证明能力主要依赖于其在训练过程中是否接触过相关的证明过程，这限制了它们对数学定理和相关概念的深入理解。我们提出了一种基于反例的证明方法，旨在通过反例增强LLMs的数学推理和证明能力。为此，我们手动创建了一个高质量的数学基准CounterMATH，以评估LLMs在提供反例时的数学概念掌握情况。'}}}, {'id': 'https://huggingface.co/papers/2502.11085', 'title': 'Towards Data-Efficient Pretraining for Atomic Property Prediction', 'url': 'https://huggingface.co/papers/2502.11085', 'abstract': "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\\'echet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.", 'score': 2, 'issue_id': 2270, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'de635e01a182309d', 'authors': ['Yasir Ghunaim', 'Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2502.11085.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#training', '#dataset', '#benchmark', '#data'], 'emoji': '🧪', 'ru': {'title': 'Качество важнее количества в предобучении моделей для предсказания атомных свойств', 'desc': 'Эта статья ставит под сомнение современную парадигму в предсказании атомных свойств, связывающую прогресс с увеличением размеров датасетов и вычислительных ресурсов. Авторы демонстрируют, что предобучение на тщательно отобранном, релевантном задаче датасете может сравниться или даже превзойти крупномасштабное предобучение, используя всего 1/24 вычислительных затрат. Они вводят новую метрику - Индекс Химического Сходства (CSI), вдохновленную расстоянием Фреше в компьютерном зрении, для молекулярных графов. Исследование показывает, что модели, предобученные на меньшем, но целевом датасете, стабильно превосходят модели, предобученные на огромных смешанных датасетах.'}, 'en': {'title': 'Quality Over Quantity in Atomic Property Prediction', 'desc': 'This paper argues that the success of predicting atomic properties does not solely depend on having large datasets and powerful computing resources. Instead, it demonstrates that pretraining on a well-chosen, relevant dataset can achieve better results while using significantly less computational power. The authors introduce the Chemical Similarity Index (CSI), a new metric that measures how well the pretraining dataset aligns with the specific task. Their research shows that using a focused dataset can lead to superior model performance compared to using larger, less relevant datasets, emphasizing that the quality of data is more important than its quantity.'}, 'zh': {'title': '质量胜于数量：原子属性预测的新视角', 'desc': '这篇论文挑战了原子属性预测领域的传统观念，认为进步与数据集规模和计算资源的增加有关。我们展示了在精心选择的、与任务相关的数据集上进行预训练，可以匹配甚至超越大规模预训练，同时计算成本仅为1/24。我们引入了化学相似性指数（CSI），这是一个新颖的指标，用于量化上游预训练数据集与下游任务之间的对齐程度。研究结果表明，选择最相关的数据集可以显著提高模型性能，强调了在原子属性预测中，质量往往优于数量。'}}}, {'id': 'https://huggingface.co/papers/2502.09509', 'title': 'EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling', 'url': 'https://huggingface.co/papers/2502.09509', 'abstract': 'Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.', 'score': 1, 'issue_id': 2280, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'd034cfa8688142a4', 'authors': ['Theodoros Kouzelis', 'Ioannis Kakogeorgiou', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ['Archimedes, Athena RC, Greece', 'IACM-Forth, Greece', 'National Technical University of Athens, Greece', 'University of Crete, Greece', 'valaio.ai, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.09509.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#training', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Эквивариантные автоэнкодеры для улучшения генеративных моделей изображений', 'desc': 'Статья представляет EQ-VAE - новый подход к регуляризации автоэнкодеров для генеративных моделей изображений. Метод обеспечивает эквивариантность латентного пространства к семантически-сохраняющим преобразованиям, таким как масштабирование и вращение. EQ-VAE улучшает производительность современных генеративных моделей, включая DiT, SiT, REPA и MaskGIT. Предложенный метод совместим как с непрерывными, так и с дискретными автоэнкодерами.'}, 'en': {'title': 'Enhancing Image Synthesis with EQ-VAE: Simplifying Latent Spaces for Better Generative Performance', 'desc': "This paper introduces EQ-VAE, a novel regularization technique designed to improve latent generative models used for image synthesis. The authors highlight that traditional autoencoders struggle with maintaining equivariance to transformations like scaling and rotation, which complicates the latent space and affects generative quality. By enforcing equivariance in the latent space, EQ-VAE simplifies the structure while preserving the quality of image reconstruction. The results show significant performance improvements in various state-of-the-art generative models, demonstrating EQ-VAE's effectiveness and versatility across different types of autoencoders."}, 'zh': {'title': 'EQ-VAE：提升潜在生成模型性能的关键', 'desc': '潜在生成模型已成为高质量图像合成的主要方法。这些模型使用自编码器将图像压缩到潜在空间，然后通过生成模型学习潜在分布。我们发现现有的自编码器在语义保持变换（如缩放和旋转）方面缺乏等变性，导致复杂的潜在空间，影响生成性能。为了解决这个问题，我们提出了EQ-VAE，这是一种简单的正则化方法，可以在潜在空间中强制等变性，从而降低复杂性而不降低重建质量。'}}}, {'id': 'https://huggingface.co/papers/2502.08826', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2502.08826', 'abstract': 'Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.', 'score': 1, 'issue_id': 2279, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'e299bbaebf315923', 'authors': ['Mohammad Mahdi Abootorabi', 'Amirhosein Zobeiri', 'Mahdi Dehghani', 'Mohammadali Mohammadkhani', 'Bardia Mohammadi', 'Omid Ghahroodi', 'Mahdieh Soleymani Baghshah', 'Ehsaneddin Asgari'], 'affiliations': ['College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran', 'Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran', 'Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2502.08826.jpg', 'data': {'categories': ['#optimization', '#training', '#rag', '#multimodal', '#survey', '#benchmark', '#hallucinations'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальный RAG: Новый фронтир в обогащении языковых моделей', 'desc': 'Эта статья представляет собой обзор систем мультимодального поиска и генерации (Multimodal RAG), которые интегрируют внешнюю динамическую информацию из различных модальностей для улучшения работы больших языковых моделей. Авторы анализируют методологии, наборы данных, метрики и инновации в области мультимодального RAG. Рассматриваются уникальные проблемы кросс-модального выравнивания и рассуждения, отличающие мультимодальный RAG от традиционного одномодального. Статья также обсуждает открытые вызовы и будущие направления исследований в этой развивающейся области.'}, 'en': {'title': 'Enhancing AI with Multimodal Retrieval-Augmented Generation', 'desc': 'This paper discusses the limitations of Large Language Models (LLMs) in handling hallucinations and outdated information due to their static training data. It introduces Retrieval-Augmented Generation (RAG) as a solution that incorporates external, dynamic information to improve the accuracy and relevance of generated content. The paper further explores Multimodal RAG, which combines various data types like text, images, and audio to enhance output quality, while addressing the challenges of cross-modal alignment and reasoning. It provides a comprehensive analysis of methodologies, evaluation metrics, and future research directions to advance the development of more reliable AI systems that utilize multimodal knowledge effectively.'}, 'zh': {'title': '多模态RAG：提升生成能力的未来之路', 'desc': '大型语言模型（LLMs）在处理幻觉和过时知识方面存在困难，因为它们依赖于静态训练数据。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，从而增强事实和更新的基础。最近的多模态学习进展导致了多模态RAG的发展，结合了文本、图像、音频和视频等多种模态，以增强生成的输出。然而，跨模态对齐和推理为多模态RAG带来了独特的挑战，使其与传统的单模态RAG有所不同。'}}}, {'id': 'https://huggingface.co/papers/2502.09969', 'title': 'Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.09969', 'abstract': 'Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.', 'score': 1, 'issue_id': 2278, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': '6530066ad48b1daf', 'authors': ['Ishika Agarwal', 'Dilek Hakkani-Tür'], 'affiliations': ['UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.09969.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#training', '#small_models'], 'emoji': '🚀', 'ru': {'title': 'Эффективная оценка влияния данных для языковых моделей с помощью малых нейросетей', 'desc': 'Статья представляет новый метод оценки влияния данных на языковые модели, называемый InfluenceNetwork. Этот подход использует небольшие нейронные сети для оценки значений влияния, что позволяет сократить вычислительные затраты до 99%. Авторы демонстрируют, что модели размером всего 0,0027% от полноразмерных языковых моделей могут эффективно оценивать влияние. Метод NN-CIFT применяется для выбора подмножества данных при дообучении инструкций, показывая производительность на уровне современных методов функций влияния при значительном ускорении.'}, 'en': {'title': 'Efficient Influence Estimation with InfluenceNetwork', 'desc': 'This paper introduces the InfluenceNetwork, a small neural network designed to efficiently estimate influence values in model training. Traditional methods are computationally expensive and struggle with large datasets, but the InfluenceNetwork achieves up to a 99% reduction in costs. The proposed method, NN-CIFT, allows for effective subset selection in instruction fine-tuning without sacrificing performance compared to existing influence functions. The authors also provide a detailed analysis of hyperparameters to optimize the performance of their approach.'}, 'zh': {'title': '小型神经网络实现高效影响估计', 'desc': '影响函数在模型训练中提供了重要的见解，但现有方法计算成本高且泛化能力有限。本文提出了一种小型神经网络，称为影响网络（InfluenceNetwork），用于估计影响值，成本降低高达99%。我们的方法（NN-CIFT）在选择子集进行指令微调的下游任务中表现良好，且与传统影响函数相比，性能没有妥协。通过对超参数的深入分析，我们证明了小模型也能有效估计影响值。'}}}, {'id': 'https://huggingface.co/papers/2502.08820', 'title': 'Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model', 'url': 'https://huggingface.co/papers/2502.08820', 'abstract': 'Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks.', 'score': 1, 'issue_id': 2274, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'f3f1c79c06903edb', 'authors': ['Emre Can Acikgoz', 'Jeremiah Greer', 'Akul Datta', 'Ze Yang', 'William Zeng', 'Oussama Elachqar', 'Emmanouil Koukoumidis', 'Dilek Hakkani-Tür', 'Gokhan Tur'], 'affiliations': ['Oumi', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.08820.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#benchmark', '#multimodal', '#dataset', '#agi', '#agents'], 'emoji': '🤖', 'ru': {'title': 'CALM: универсальный подход к созданию разговорных агентов нового поколения', 'desc': 'Эта статья представляет новый подход CALM (Conversational Agentic Language Model), объединяющий возможности диалоговых систем и языковых агентов. Авторы создали многозадачный датасет CALM-IT, сочетающий многоходовые рассуждения ReAct с использованием сложных API. На его основе обучены модели CALM 8B, 70B и 405B, превосходящие специализированные модели на трех бенчмарках: MultiWOZ 2.4, BFCL V3 и API-Bank. CALM решает проблему разрыва между системами задачно-ориентированного диалога (TOD) и языковыми агентами (LA), объединяя их сильные стороны.'}, 'en': {'title': 'CALM: Bridging the Gap in Conversational AI', 'desc': 'This paper discusses the development of CALM, a new model that combines the strengths of Language Agents (LA) and task-oriented dialogue (TOD) systems. Traditional TOD systems struggle with limited API training and maintaining user intent over multiple interactions, while LAs lack robust multi-turn management. The authors introduce CALM-IT, a multi-task dataset that integrates reasoning and API usage, allowing for better training of conversational agents. The results show that CALM models significantly outperform existing specialized models on key benchmarks, demonstrating the effectiveness of this unified approach.'}, 'zh': {'title': 'CALM：对话与代理能力的统一模型', 'desc': '本文介绍了一种新的对话代理模型CALM（Conversational Agentic Language Model），旨在解决传统任务导向对话系统（TOD）和语言代理（LA）在多轮对话管理和功能调用方面的不足。当前的系统通常在有限的目标API上训练，导致在与新服务交互时需要新的数据来维持质量，而语言代理则未能有效保持用户意图。我们通过创建CALM-IT数据集，将多轮推理与复杂API使用相结合，训练了三种不同规模的CALM模型，结果显示这些模型在多个基准测试中超越了现有的领域特定模型。该研究表明，CALM模型能够有效整合对话能力和代理能力，推动对话系统的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.08441', 'title': 'Better Embeddings with Coupled Adam', 'url': 'https://huggingface.co/papers/2502.08441', 'abstract': 'Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.', 'score': 1, 'issue_id': 2271, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '4357e7dc6b15b5b7', 'authors': ['Felix Stollenwerk', 'Tobias Stollenwerk'], 'affiliations': ['AI Sweden', 'Forschungszentrum Jülich'], 'pdf_title_img': 'assets/pdf/title_img/2502.08441.jpg', 'data': {'categories': ['#training', '#optimization', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Улучшение вложений слов с помощью Coupled Adam', 'desc': 'Статья исследует проблему анизотропии в представлениях слов, обучаемых большими языковыми моделями (LLM). Авторы утверждают, что второй момент в оптимизаторе Adam является причиной анизотропных вложений. Они предлагают модифицированный оптимизатор под названием Coupled Adam для смягчения этой проблемы. Эксперименты показывают, что Coupled Adam значительно улучшает качество вложений и приводит к лучшим результатам как на этапе обучения, так и при решении последующих задач на достаточно больших наборах данных.'}, 'en': {'title': 'Enhancing Embedding Quality with Coupled Adam', 'desc': 'This paper addresses the issue of anisotropic embeddings in large language models (LLMs), which can negatively impact their performance. The authors identify that the second moment in the Adam optimizer contributes to this anisotropy. To counteract this, they propose a new optimizer called Coupled Adam, which modifies the way embeddings are learned. Experimental results show that Coupled Adam not only enhances the quality of embeddings but also improves performance in both upstream and downstream tasks when applied to sufficiently large datasets.'}, 'zh': {'title': '改进优化器，提升嵌入质量', 'desc': '尽管大型语言模型（LLMs）具有出色的能力，但它们学习的词表示存在一种不理想且尚未充分理解的特征，即各向异性。本文认为，Adam优化器中的二阶矩是导致各向异性嵌入的原因，并提出了一种名为Coupled Adam的改进优化器来缓解这一问题。我们的实验表明，Coupled Adam显著提高了嵌入的质量，同时在足够大的数据集上也提升了上游和下游的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.09083', 'title': "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking", 'url': 'https://huggingface.co/papers/2502.09083', 'abstract': "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.", 'score': 1, 'issue_id': 2270, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'ecf7018a52bbede5', 'authors': ['Greta Warren', 'Irina Shklovski', 'Isabelle Augenstein'], 'affiliations': ['Linköping University, Linköping, Sweden', 'University of Copenhagen, Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2502.09083.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#ethics', '#healthcare', '#multimodal', '#data'], 'emoji': '🔍', 'ru': {'title': 'Объяснимая автоматизация для эффективной проверки фактов', 'desc': 'Статья посвящена актуальной проблеме автоматизированной проверки фактов в эпоху больших языковых моделей и генеративного ИИ. Авторы провели интервью с профессиональными фактчекерами, чтобы понять, как они оценивают доказательства и принимают решения. Исследование выявило потребность в объяснимых системах автоматизированной проверки фактов, которые могли бы интегрироваться в рабочие процессы фактчекеров. Результаты показывают необходимость в прозрачных объяснениях, отражающих ход рассуждений модели, ссылающихся на конкретные доказательства и указывающих на неопределенности и пробелы в информации.'}, 'en': {'title': 'Bridging the Gap: Enhancing Automated Fact-Checking with Human-Centric Explanations', 'desc': 'This paper addresses the growing challenge of misinformation in online media by exploring the role of automated fact-checking systems. It highlights the necessity for these systems to provide clear explanations that align with how human fact-checkers evaluate evidence and make decisions. Through interviews with fact-checking professionals, the study identifies specific requirements for explanations that can enhance the integration of automated tools into fact-checking workflows. The findings reveal critical gaps in current explanation practices and suggest criteria for developing more effective automated fact-checking solutions.'}, 'zh': {'title': '提升自动化事实核查的解释能力', 'desc': '这篇论文探讨了大型语言模型和生成性人工智能在在线媒体中的广泛应用，强调了自动化事实核查的必要性，以帮助核查员应对日益增加和复杂化的虚假信息。研究通过与事实核查专业人士的半结构化访谈，分析了核查员如何评估证据、做出决策以及解释他们的过程。论文还考察了核查员在实践中如何使用自动化工具，并识别了他们对自动化事实核查工具的解释需求。研究结果显示，核查员在解释方面存在未满足的需求，并确定了可复制的事实核查解释的重要标准，包括追踪模型的推理路径、引用具体证据以及突出不确定性和信息缺口。'}}}, {'id': 'https://huggingface.co/papers/2502.11574', 'title': 'Large Language Models and Mathematical Reasoning Failures', 'url': 'https://huggingface.co/papers/2502.11574', 'abstract': "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.", 'score': 1, 'issue_id': 2268, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '478a2c4575e67b28', 'authors': ['Johan Boye', 'Birger Moell'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2502.11574.jpg', 'data': {'categories': ['#training', '#dataset', '#reasoning', '#math'], 'emoji': '🧮', 'ru': {'title': 'Раскрывая ограничения математического мышления ИИ', 'desc': 'В статье исследуются способности крупных языковых моделей (LLM) к математическим рассуждениям на основе 50 новых задач уровня старшей школы. Авторы анализируют не только правильность ответов, но и ход решения, выявляя ошибки в рассуждениях. Оценка восьми современных моделей показала, что даже при улучшении точности ответов, все модели демонстрируют ошибки в пространственном мышлении, стратегическом планировании и арифметике. Результаты подчеркивают важность оценки процесса рассуждений, а не только ответов, и указывают на необходимость улучшения способностей LLM к структурированному мышлению и обработке ограничений.'}, 'en': {'title': 'Evaluating Reasoning, Not Just Answers in LLMs', 'desc': "This paper examines how well large language models (LLMs) can solve high-school-level math word problems by focusing on their reasoning abilities. It evaluates eight advanced models, revealing that while some newer models show better accuracy, they still struggle with spatial reasoning, strategic planning, and arithmetic. The analysis identifies common reasoning failures, such as making incorrect assumptions and having difficulty with multi-step deductions. The findings stress the importance of assessing the reasoning process in addition to the final answers, highlighting the need for improvements in LLMs' structured reasoning skills."}, 'zh': {'title': '评估推理过程，超越答案正确性', 'desc': '本论文研究了大型语言模型（LLMs）在数学推理方面的能力，使用了50个新构建的高中水平的文字问题。与以往只关注答案正确性的研究不同，我们严格分析了最终答案和解决步骤，以识别推理失败。评估了包括Mixtral、Llama、Gemini、GPT-4o和OpenAI的o1变体在内的八个最先进模型，发现尽管新模型（如o3-mini、deepseek-r1）在准确性上更高，但所有模型在空间推理、战略规划和算术方面都存在错误。我们的结果强调了评估推理过程的重要性，而不仅仅是答案，并警告不要高估LLMs的解决问题能力。'}}}, {'id': 'https://huggingface.co/papers/2502.11177', 'title': 'The Mirage of Model Editing: Revisiting Evaluation in the Wild', 'url': 'https://huggingface.co/papers/2502.11177', 'abstract': "Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.", 'score': 0, 'issue_id': 2273, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'd72ce28b4092ab0f', 'authors': ['Wanli Yang', 'Fei Sun', 'Jiajun Tan', 'Xinyu Ma', 'Qi Cao', 'Dawei Yin', 'Huawei Shen', 'Xueqi Cheng'], 'affiliations': ['Baidu Inc.', 'CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS', 'Huawei', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.11177.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#optimization', '#reasoning', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Переосмысление редактирования языковых моделей: от иллюзии к реальности', 'desc': 'Статья посвящена исследованию эффективности редактирования моделей в задачах вопросно-ответных систем. Авторы предлагают новый бенчмарк QAEdit и стандартизированную систему оценки для более точного измерения производительности методов редактирования. Результаты показывают, что существующие методы работают значительно хуже, чем сообщалось ранее, в основном из-за проблем в практиках оценки. Исследование выявляет ключевые проблемы и предлагает рекомендации для улучшения надежности и практичности методов редактирования моделей.'}, 'en': {'title': 'Rethinking Model Editing: Bridging Theory and Real-World Performance', 'desc': 'This paper investigates the effectiveness of model editing techniques in question answering (QA) systems, particularly focusing on large language models (LLMs). The authors introduce QAEdit, a new benchmark and evaluation framework to rigorously assess how well these editing methods correct errors in LLMs. Their findings reveal that current editing methods perform significantly worse in real-world scenarios than previously reported, highlighting flaws in past evaluation practices. By simulating real-world conditions, they show that existing approaches struggle with even a small number of edits, prompting a reevaluation of model editing methods and their assessment.'}, 'zh': {'title': '重新审视模型编辑的有效性与评估方法', 'desc': '本论文探讨了模型编辑在问答系统中的有效性，尤其是在真实应用中的表现。我们提出了QAEdit，一个新的基准测试，旨在评估现有编辑方法在纠正大型语言模型（LLM）错误时的效果。实验结果显示，当前的编辑方法在实际应用中的表现远低于之前的报告，只有38.5%的准确率。通过模块分析和控制实验，我们发现评估实践中的问题导致了这种性能下降，并提出了一个严格的评估框架，以推动模型编辑研究的可靠性和实用性。'}}}, {'id': 'https://huggingface.co/papers/2502.11578', 'title': 'Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance', 'url': 'https://huggingface.co/papers/2502.11578', 'abstract': "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", 'score': 0, 'issue_id': 2268, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '039b4ffb618ff3b1', 'authors': ['Birger Moell', 'Johan Boye'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2502.11578.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#interpretability', '#science'], 'emoji': '📊', 'ru': {'title': 'Измерение сложности языка как индикатор возможностей языковых моделей', 'desc': 'Статья исследует способность современных языковых моделей (LLM) выполнять задачи измерения сложности языка, в частности вычисление метрики читабельности LIX и среднего расстояния зависимостей (ADD). Эксперименты проводились на шведских эссе уровня старшей школы и университета. Результаты показывают, что модель ChatGPT-o1-mini демонстрирует наилучшую производительность в обеих задачах. Обнаружена сильная корреляция между точностью вычисления LIX и общей производительностью моделей на бенчмарке MMLU, что предполагает возможность использования задач измерения сложности языка как прокси для оценки общих возможностей LLM.'}, 'en': {'title': 'Evaluating Language Complexity as a Proxy for LLM Performance', 'desc': "This paper explores how well large language models (LLMs) can measure language complexity using specific metrics like the LIX readability score and Average Dependency Distance (ADD). The authors tested these models on Swedish essays from high school and university students to see how accurately they could compute LIX scores and perform dependency parsing. The results showed that while all models had some success, ChatGPT-o1-mini was the most reliable, achieving the best accuracy in both tasks. Furthermore, a strong correlation was found between the models' LIX computation accuracy and their performance on the MMLU benchmark, indicating that language complexity measurements can be useful for evaluating LLM capabilities without needing extensive datasets."}, 'zh': {'title': '语言复杂性测量：评估大型语言模型的新方法', 'desc': '本文研究了大型语言模型（LLMs）在语言复杂性测量任务中的表现，特别是计算LIX可读性指标和平均依赖距离（ADD）。我们使用瑞典高中和大学的论文来评估模型计算LIX分数和进行依赖解析的能力，并将结果与已建立的基准进行比较。研究发现，尽管所有模型在这些任务上都有一定能力，但ChatGPT-o1-mini在LIX计算和依赖解析中表现最为一致，准确率最高。此外，我们观察到模型在计算LIX时的准确性与其在大规模多任务语言理解（MMLU）基准上的整体表现之间存在显著的负相关关系。'}}}, {'id': 'https://huggingface.co/papers/2502.13923', 'title': 'Qwen2.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2502.13923', 'abstract': 'We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.', 'score': 70, 'issue_id': 2311, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'b0f349bddafadc4c', 'authors': ['Shuai Bai', 'Keqin Chen', 'Xuejing Liu', 'Jialin Wang', 'Wenbin Ge', 'Sibo Song', 'Kai Dang', 'Peng Wang', 'Shijie Wang', 'Jun Tang', 'Humen Zhong', 'Yuanzhi Zhu', 'Mingkun Yang', 'Zhaohai Li', 'Jianqiang Wan', 'Pengfei Wang', 'Wei Ding', 'Zheren Fu', 'Yiheng Xu', 'Jiabo Ye', 'Xi Zhang', 'Tianbao Xie', 'Zesen Cheng', 'Hang Zhang', 'Zhibo Yang', 'Haiyang Xu', 'Junyang Lin'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.13923.jpg', 'data': {'categories': ['#agi', '#architecture', '#cv', '#multimodal', '#long_context', '#agents', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Новый уровень понимания визуальной информации с Qwen2.5-VL', 'desc': 'Qwen2.5-VL - это новая модель машинного обучения для обработки визуальной и текстовой информации. Она демонстрирует значительные улучшения в распознавании объектов, анализе документов и понимании длинных видео. Модель использует динамическое разрешение и абсолютное временное кодирование для обработки изображений и видео различных размеров. Qwen2.5-VL доступна в трех размерах и сравнима по производительности с современными мультимодальными языковыми моделями.'}, 'en': {'title': 'Revolutionizing Vision-Language Interaction with Qwen2.5-VL', 'desc': 'Qwen2.5-VL is a cutting-edge vision-language model that enhances the understanding and interaction with visual data. It features advanced capabilities like precise object localization, effective document parsing, and the ability to comprehend long videos. The model utilizes dynamic resolution processing and absolute time encoding to handle complex inputs, allowing it to analyze images and videos of various sizes efficiently. With its robust performance in both visual and linguistic tasks, Qwen2.5-VL serves as an interactive agent for real-world applications, from edge AI to high-performance computing.'}, 'zh': {'title': 'Qwen2.5-VL：视觉与语言的完美结合', 'desc': 'Qwen2.5-VL是Qwen视觉语言系列的最新旗舰模型，展示了基础能力和创新功能的显著进步。它在视觉识别、物体定位、文档解析和长视频理解方面取得了重大突破。该模型能够准确地使用边界框或点来定位物体，并从发票、表单和表格中提取结构化数据。通过动态分辨率处理和绝对时间编码，Qwen2.5-VL能够处理不同大小的图像和长达数小时的视频，成为一个能够在现实场景中进行推理和任务执行的互动视觉代理。'}}}, {'id': 'https://huggingface.co/papers/2502.13144', 'title': 'RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.13144', 'abstract': 'Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD.', 'score': 27, 'issue_id': 2309, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '0db330614af75888', 'authors': ['Hao Gao', 'Shaoyu Chen', 'Bo Jiang', 'Bencheng Liao', 'Yiang Shi', 'Xiaoyang Guo', 'Yuechuan Pu', 'Haoran Yin', 'Xiangyu Li', 'Xinbang Zhang', 'Ying Zhang', 'Wenyu Liu', 'Qian Zhang', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.13144.jpg', 'data': {'categories': ['#rl', '#benchmark', '#alignment', '#3d', '#games', '#reasoning', '#agents'], 'emoji': '🚗', 'ru': {'title': 'Революция в автономном вождении: обучение с подкреплением в фотореалистичных 3D-мирах', 'desc': 'Статья представляет новый подход к автономному вождению, основанный на обучении с подкреплением (RL) с использованием фотореалистичных 3D-моделей окружающей среды. Авторы разработали систему RAD, которая позволяет политике автономного вождения исследовать различные сценарии и учиться справляться с нестандартными ситуациями через масштабные эксперименты. Для повышения безопасности были разработаны специальные функции вознаграждения, а для лучшего соответствия человеческому поведению при вождении было включено обучение по имитации (IL) в качестве регуляризации. Результаты показывают, что RAD превосходит методы, основанные только на IL, особенно в снижении частоты столкновений.'}, 'en': {'title': 'Revolutionizing Autonomous Driving with Closed-Loop Reinforcement Learning', 'desc': "This paper presents a new approach to autonomous driving using a closed-loop Reinforcement Learning (RL) paradigm, addressing limitations of traditional Imitation Learning (IL). By creating a realistic 3D digital environment, the model can explore various driving scenarios and learn from trial and error, improving its ability to handle unexpected situations. The authors introduce specialized rewards to enhance safety by teaching the model to react appropriately to critical events and understand causal relationships in driving. Additionally, they incorporate IL as a regularization term to better align the model's behavior with human driving patterns, resulting in significantly improved performance metrics, including a lower collision rate."}, 'zh': {'title': '闭环强化学习提升自主驾驶安全性与性能', 'desc': '现有的端到端自主驾驶算法通常采用模仿学习（IL）方法，但面临因果混淆和开放环路差距等挑战。本文提出了一种基于3DGS的闭环强化学习（RL）训练范式，通过构建真实物理世界的逼真数字复制品，使自主驾驶策略能够广泛探索状态空间，并通过大规模试错学习处理分布外场景。为了提高安全性，我们设计了专门的奖励机制，引导策略有效应对安全关键事件，并理解现实世界的因果关系。同时，为了更好地与人类驾驶行为对齐，我们将模仿学习作为正则化项融入到强化学习训练中。'}}}, {'id': 'https://huggingface.co/papers/2502.13128', 'title': 'SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation', 'url': 'https://huggingface.co/papers/2502.13128', 'abstract': 'Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, resulting in cumbersome training and inference pipelines. In this paper, we propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline. The generated samples are showcased on our project page at https://liuzh-19.github.io/SongGen/ , and the code will be available at https://github.com/LiuZH-19/SongGen .', 'score': 24, 'issue_id': 2312, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '30ac46f3e428e6cf', 'authors': ['Zihan Liu', 'Shuangrui Ding', 'Zhixiong Zhang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Beihang University, Beijing, China', 'Shanghai AI Laboratory, Shanghai, China', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.13128.jpg', 'data': {'categories': ['#story_generation', '#data', '#audio', '#training', '#open_source', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'SongGen: Революция в генерации песен с помощью ИИ', 'desc': 'SongGen - это новая модель машинного обучения для генерации песен на основе текстового ввода. Она использует архитектуру трансформера и позволяет контролировать различные музыкальные атрибуты, включая текст песни, инструментовку, жанр и настроение. Модель может работать в двух режимах: смешанном (генерация вокала и аккомпанемента вместе) и двухдорожечном (раздельная генерация). Авторы также разработали автоматизированный конвейер предобработки данных и планируют открыть исходный код и веса модели для дальнейших исследований.'}, 'en': {'title': 'SongGen: Simplifying Text-to-Song Generation with a Unified Model', 'desc': 'This paper introduces SongGen, a novel model for generating songs from text inputs using a single-stage auto-regressive transformer. It addresses the challenges of text-to-song generation by allowing fine control over various musical elements such as lyrics, genre, and mood. The model offers two output modes: mixed mode for simultaneous vocal and accompaniment generation, and dual-track mode for separate synthesis, enhancing flexibility for users. Additionally, the authors provide an automated data preprocessing pipeline and commit to releasing their resources to support further research in this area.'}, 'zh': {'title': 'SongGen：可控的歌曲生成新方法', 'desc': '本文介绍了一种名为SongGen的文本到歌曲生成模型，旨在从文本输入中生成歌词和伴奏。该模型采用单阶段自回归变换器，能够对多种音乐属性进行精细控制，如歌词、乐器描述、风格、情绪和音色。SongGen支持两种输出模式：混合模式和双轨模式，提供了更大的灵活性以满足不同应用需求。此外，研究团队还设计了自动化的数据预处理管道，以确保生成样本的质量，并计划开放模型权重和训练代码以促进社区参与。'}}}, {'id': 'https://huggingface.co/papers/2502.13685', 'title': 'MoM: Linear Sequence Modeling with Mixture-of-Memories', 'url': 'https://huggingface.co/papers/2502.13685', 'abstract': 'Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain\'s ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 19, 'issue_id': 2314, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'a9ffa1bb1a5e0137', 'authors': ['Jusen Du', 'Weigao Sun', 'Disen Lan', 'Jiaxi Hu', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.13685.jpg', 'data': {'categories': ['#long_context', '#architecture', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Множественная память для эффективного линейного моделирования последовательностей', 'desc': 'Статья представляет новую архитектуру под названием Mixture-of-Memories (MoM) для линейного моделирования последовательностей. MoM использует несколько независимых состояний памяти и сеть-маршрутизатор для направления входных токенов в конкретные состояния памяти. Это значительно увеличивает общую емкость памяти, минимизируя при этом интерференцию памяти. MoM показывает исключительные результаты на задачах, требующих интенсивного запоминания, превосходя существующие методы линейного моделирования последовательностей и даже достигая производительности, сравнимой с моделями Transformer.'}, 'en': {'title': 'Unlocking Memory Potential with Mixture-of-Memories', 'desc': 'This paper presents a new architecture called Mixture-of-Memories (MoM) that improves linear sequence modeling methods by using multiple independent memory states. Unlike traditional models that compress input sequences into a single memory state, MoM employs a router network to direct tokens to specific memory states, enhancing memory capacity and reducing interference. This design allows MoM to excel in recall-intensive tasks while maintaining linear complexity during training and constant complexity during inference. Experimental results demonstrate that MoM outperforms existing linear models and achieves performance levels comparable to Transformer models on various language tasks.'}, 'zh': {'title': '混合记忆：提升记忆能力的线性序列建模新方法', 'desc': '本文介绍了一种新的模型架构，称为混合记忆（Mixture-of-Memories, MoM），旨在提高线性序列建模方法在记忆密集型任务中的表现。MoM通过使用多个独立的记忆状态，并通过路由网络将输入令牌分配到特定的记忆状态，从而增强了整体记忆容量并减少了记忆干扰。尽管引入了多个记忆状态，MoM在训练时仍保持线性复杂度，在推理时则保持常数复杂度。实验结果表明，MoM在下游语言任务，尤其是记忆密集型任务上，显著超越了现有的线性序列模型，甚至达到了与Transformer模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.13347', 'title': 'Craw4LLM: Efficient Web Crawling for LLM Pretraining', 'url': 'https://huggingface.co/papers/2502.13347', 'abstract': "Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Crawl4LLM.", 'score': 18, 'issue_id': 2310, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'dd055f606e1ddfe2', 'authors': ['Shi Yu', 'Zhiyuan Liu', 'Chenyan Xiong'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'School of Computer Science, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13347.jpg', 'data': {'categories': ['#dataset', '#graphs', '#open_source', '#data'], 'emoji': '🕷️', 'ru': {'title': 'Умный веб-краулинг для эффективного обучения языковых моделей', 'desc': 'Статья представляет Crawl4LLM - эффективный метод веб-краулинга для предобучения больших языковых моделей (LLM). Метод использует влияние веб-страницы на предобучение LLM в качестве приоритета для планировщика краулера, заменяя стандартный подход на основе связности графа. Эксперименты на графе из 900 миллионов страниц показали эффективность Crawl4LLM в получении качественных данных для предобучения. При использовании всего 21% URL модели достигают тех же результатов, что и при предыдущих подходах к краулингу.'}, 'en': {'title': 'Crawl Smart: Boosting LLMs with Efficient Web Crawling', 'desc': 'This paper introduces Crawl4LLM, a novel web crawling technique designed to enhance the quality of pretraining data for large language models (LLMs). Instead of relying on traditional methods that prioritize web page connectivity, Crawl4LLM uses a priority score based on the potential influence of a webpage on LLM performance. The method was tested on a vast web graph with 900 million pages, showing that it can achieve comparable downstream performance with only 21% of the URLs crawled. This approach not only improves data quality but also minimizes the environmental impact of web crawling by reducing unnecessary data collection.'}, 'zh': {'title': '高效爬虫，提升LLM预训练数据质量', 'desc': '本论文提出了一种名为Crawl4LLM的高效网络爬虫方法，旨在提高大语言模型（LLM）预训练数据的质量。该方法通过网页在LLM预训练中的影响力作为优先级评分，优化了爬虫调度器的工作，而不是依赖于传统的图连接性优先级。实验结果表明，Crawl4LLM在仅爬取21%的网址的情况下，能够获得与之前爬取相同的下游性能，显著减少了爬取浪费。此方法不仅提高了数据质量，还减轻了对网站的负担。'}}}, {'id': 'https://huggingface.co/papers/2502.13922', 'title': 'LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization', 'url': 'https://huggingface.co/papers/2502.13922', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \\ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales.', 'score': 18, 'issue_id': 2309, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '93cf8365ba7edb80', 'authors': ['Guanzheng Chen', 'Xin Li', 'Michael Qizhe Shieh', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab, 310023, Hangzhou, China', 'National University of Singapore', 'Shanda AI Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2502.13922.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#benchmark', '#long_context', '#architecture', '#rlhf'], 'emoji': '📏', 'ru': {'title': 'LongPO: Самоэволюция языковых моделей для работы с длинным контекстом', 'desc': 'Статья представляет новый метод LongPO для улучшения работы языковых моделей с длинным контекстом. LongPO позволяет моделям, обученным на коротких контекстах, самостоятельно адаптироваться к длинным контекстам, используя самогенерируемые данные о предпочтениях. Метод включает ограничение KL для сохранения производительности на коротких контекстах. Эксперименты показывают, что LongPO превосходит наивные методы обучения на длинных и коротких контекстах.'}, 'en': {'title': 'Empowering Short-Context LLMs for Long-Context Mastery', 'desc': 'This paper presents LongPO, a method designed to enhance the performance of short-context Large Language Models (LLMs) on long-context tasks. The challenge arises from the difficulty of aligning LLMs for long contexts due to the lack of human-annotated data and the need to balance performance across different context lengths. LongPO allows LLMs to learn from their own generated data, creating a preference model that helps them adapt their short-context skills to long-context scenarios. The results show that models trained with LongPO maintain their short-context performance while significantly improving their long-context capabilities, even rivaling more advanced models like GPT-4.'}, 'zh': {'title': 'LongPO：短上下文模型的长上下文自我演化', 'desc': '大型语言模型（LLMs）在预训练和对齐方面表现出色，但在长上下文场景中可能表现不佳。为了解决这个问题，本文提出了LongPO方法，使短上下文的LLMs能够自我演化，以在长上下文任务中表现出色。LongPO通过生成短到长的偏好数据，帮助模型学习如何在长上下文中保持短上下文的能力。实验结果表明，使用LongPO的模型在长上下文基准测试中表现优异，甚至可以与更强大的LLMs相媲美。'}}}, {'id': 'https://huggingface.co/papers/2502.12143', 'title': 'Small Models Struggle to Learn from Strong Reasoners', 'url': 'https://huggingface.co/papers/2502.12143', 'abstract': 'Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (leq3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.', 'score': 15, 'issue_id': 2309, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '5abea4fb025815c2', 'authors': ['Yuetai Li', 'Xiang Yue', 'Zhangchen Xu', 'Fengqing Jiang', 'Luyao Niu', 'Bill Yuchen Lin', 'Bhaskar Ramasubramanian', 'Radha Poovendran'], 'affiliations': ['Carnegie Mellon University', 'University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12143.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#optimization', '#reasoning', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Адаптация сложности рассуждений для эффективного обучения малых языковых моделей', 'desc': 'Исследование показывает, что маленькие языковые модели (до 3 млрд параметров) не всегда успешно обучаются на длинных цепочках рассуждений или при дистилляции от больших моделей. Авторы обнаружили, что такие модели лучше обучаются на более коротких и простых цепочках рассуждений. Для решения этой проблемы предложен метод Mix Distillation, сочетающий длинные и короткие примеры рассуждений. Эксперименты показывают, что этот подход значительно улучшает способность маленьких моделей к рассуждениям.'}, 'en': {'title': 'Bridging the Learnability Gap for Small Models with Mix Distillation', 'desc': 'This paper explores the challenges faced by small language models (with 3 billion parameters or fewer) in learning from complex reasoning tasks. It identifies a phenomenon called the Small Model Learnability Gap, where these smaller models do not gain advantages from long chain-of-thought (CoT) reasoning or direct distillation from larger models. Instead, they perform better when trained on shorter, simpler reasoning chains that match their learning abilities. To improve their performance, the authors propose a method called Mix Distillation, which combines both long and short reasoning examples, leading to better reasoning outcomes for small models.'}, 'zh': {'title': '混合蒸馏：提升小模型推理能力的有效策略', 'desc': '大型语言模型在复杂推理任务中表现出色，但我们发现小模型（参数小于等于3亿）在长链推理或从大模型蒸馏中并不总是受益。相反，它们在短小简单的推理链上微调时表现更好，这与它们的学习能力更为契合。为了解决这个问题，我们提出了混合蒸馏（Mix Distillation）策略，通过结合长短推理示例或来自大模型和小模型的推理，平衡推理复杂性。实验表明，混合蒸馏显著提高了小模型的推理性能，强调了直接强模型蒸馏的局限性，并突出了适应推理复杂性的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.13965', 'title': 'Autellix: An Efficient Serving Engine for LLM Agents as General Programs', 'url': 'https://huggingface.co/papers/2502.13965', 'abstract': "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", 'score': 12, 'issue_id': 2310, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'f64be1cc6aba8b4c', 'authors': ['Michael Luo', 'Xiaoxiang Shi', 'Colin Cai', 'Tianjun Zhang', 'Justin Wong', 'Yichuan Wang', 'Chi Wang', 'Yanping Huang', 'Zhifeng Chen', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Google DeepMind', 'Shanghai Jiao Tong University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.13965.jpg', 'data': {'categories': ['#optimization', '#inference', '#agents', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Autellix: Революция в обслуживании LLM для агентных программ', 'desc': 'Статья представляет Autellix - систему обслуживания больших языковых моделей (LLM), оптимизирующую выполнение агентных программ. Autellix рассматривает программы как объекты первого класса, перехватывая вызовы LLM и обогащая планировщики контекстом на уровне программ. Предложены два алгоритма планирования для однопоточных и распределенных программ, которые приоритизируют вызовы LLM на основе ранее выполненных вызовов. Эксперименты показывают, что Autellix улучшает пропускную способность программ в 4-15 раз при той же задержке по сравнению с современными системами.'}, 'en': {'title': 'Autellix: Optimizing LLM Serving for Enhanced Throughput', 'desc': 'This paper discusses the evolution of large language models (LLMs) from simple chatbots to more complex, agentic programs that can perform a variety of tasks. It identifies a significant issue in current LLM serving systems, which overlook the dependencies between different programs and their calls, leading to inefficiencies and long wait times. The authors introduce Autellix, a new LLM serving system that optimizes these processes by treating programs as first-class entities and enhancing scheduling with program-level context. Their proposed scheduling algorithms significantly improve the throughput of LLM programs, achieving 4-15 times better performance compared to existing systems while maintaining similar latency.'}, 'zh': {'title': '优化LLM调用，提升程序性能的Autellix', 'desc': '这篇论文介绍了一种新的大型语言模型（LLM）服务系统，名为Autellix。Autellix通过将程序视为第一类公民，优化了LLM调用的调度，从而减少了整体延迟。研究表明，现有的LLM服务系统忽视了程序与调用之间的依赖关系，导致了长时间的等待。通过引入新的调度算法，Autellix在多种LLM和任务负载下，提升了程序的吞吐量，效果显著。'}}}, {'id': 'https://huggingface.co/papers/2502.11995', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'url': 'https://huggingface.co/papers/2502.11995', 'abstract': 'Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.', 'score': 9, 'issue_id': 2313, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '8bcbe61536105828', 'authors': ['Siddhesh Pawar', 'Arnav Arora', 'Lucie-Aimée Kaffee', 'Isabelle Augenstein'], 'affiliations': ['Hugging Face', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2502.11995.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#alignment', '#healthcare'], 'emoji': '👤', 'ru': {'title': 'Имена в ИИ: преодоление культурных стереотипов', 'desc': 'Статья исследует влияние имен на взаимодействие с языковыми моделями (LLM). Авторы изучают, как LLM делают предположения о культурной идентичности пользователей на основе их имен. Результаты показывают, что LLM демонстрируют сильные культурные предубеждения, связанные с именами. Исследование подчеркивает необходимость разработки более нюансированных систем персонализации, избегающих стереотипов.'}, 'en': {'title': 'Rethinking Personalization: Beyond Names and Stereotypes in LLMs', 'desc': "This paper explores how names influence identity and personalization in interactions with large language models (LLMs). It highlights that while names can provide valuable information for tailoring responses, they can also lead to oversimplified assumptions about a user's cultural background. The authors analyze biases in LLM outputs when names are used in suggestion-seeking queries, revealing strong cultural stereotypes linked to names. The findings suggest the need for more sophisticated personalization systems that respect individual identities without perpetuating stereotypes."}, 'zh': {'title': '名字与身份：个性化系统中的文化偏见', 'desc': '这篇论文探讨了名字与人类身份之间的深刻联系。名字不仅是个体性、文化遗产和个人历史的标志，还可能导致对复杂身份的过度简化。研究表明，在与大型语言模型（LLMs）互动时，名字会影响个性化的响应，可能引发文化偏见。我们的分析显示，LLMs在生成响应时对名字的文化身份有强烈的假设，这对设计更细致的个性化系统具有重要意义。'}}}, {'id': 'https://huggingface.co/papers/2502.13946', 'title': "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", 'url': 'https://huggingface.co/papers/2502.13946', 'abstract': "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.", 'score': 8, 'issue_id': 2311, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '72612658ea7eefab', 'authors': ['Chak Tou Leong', 'Qingyu Yin', 'Jian Wang', 'Wenjie Li'], 'affiliations': ['Department of Computing, The Hong Kong Polytechnic University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13946.jpg', 'data': {'categories': ['#alignment', '#security', '#training', '#inference', '#rlhf'], 'emoji': '🛡️', 'ru': {'title': 'Преодоление уязвимостей в безопасности языковых моделей', 'desc': "Это исследование посвящено проблеме безопасности больших языковых моделей (LLM) и их уязвимости к атакам типа 'jailbreak'. Авторы выдвигают гипотезу, что ключевым фактором уязвимости является чрезмерная зависимость механизмов безопасности LLM от шаблонного региона между инструкцией и начальным выводом модели. Эксперименты подтверждают, что эта проблема, названная 'привязкой выравнивания безопасности к шаблону', широко распространена среди различных LLM. Исследователи предлагают отделить механизмы безопасности от шаблонного региона для повышения устойчивости к атакам."}, 'en': {'title': 'Strengthening LLM Safety by Breaking Template Ties', 'desc': "This paper investigates the safety alignment of large language models (LLMs) and identifies a vulnerability linked to the use of fixed templates in their design. The authors propose that these templates anchor the models' safety decision-making, making them susceptible to simple jailbreak attacks. Through experiments, they confirm that this 'template-anchored safety alignment' is a common issue across various LLMs. The study suggests that improving safety mechanisms by detaching them from the template region could enhance the models' resilience against such attacks."}, 'zh': {'title': '模板锚定的安全对齐问题', 'desc': '大型语言模型（LLMs）的安全对齐仍然存在脆弱性，因为它们的初始行为容易受到简单攻击的影响。我们假设输入指令和初始模型输出之间的固定模板是导致这些脆弱性的关键因素，因为LLMs的安全决策过于依赖模板区域的信息。我们称这种问题为模板锚定的安全对齐。通过实验，我们发现这种现象在多种对齐的LLMs中普遍存在，并且分离安全机制与模板区域有助于减轻对攻击的脆弱性。'}}}, {'id': 'https://huggingface.co/papers/2502.13233', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?', 'url': 'https://huggingface.co/papers/2502.13233', 'abstract': "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.", 'score': 8, 'issue_id': 2310, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '58503159cb9ed740', 'authors': ['Yucheng Shi', 'Tianze Yang', 'Canyu Chen', 'Quanzheng Li', 'Tianming Liu', 'Xiang Li', 'Ninghao Liu'], 'affiliations': ['Illinois Institute of Technology', 'Massachusetts General Hospital and Harvard Medical School', 'University of Georgia'], 'pdf_title_img': 'assets/pdf/title_img/2502.13233.jpg', 'data': {'categories': ['#rag', '#synthetic', '#healthcare', '#science'], 'emoji': '🩺', 'ru': {'title': 'SearchRAG: Точные медицинские ответы с помощью актуального поиска', 'desc': 'Эта статья представляет SearchRAG - новый метод для улучшения ответов больших языковых моделей на медицинские вопросы. В отличие от традиционных подходов извлечения информации, SearchRAG использует поисковые системы в реальном времени для получения актуальных данных. Метод включает генерацию синтетических запросов и отбор релевантной информации на основе оценки неопределенности. Эксперименты показывают, что SearchRAG значительно повышает точность ответов на сложные медицинские вопросы, требующие детальных и современных знаний.'}, 'en': {'title': 'Enhancing Medical Q&A with Real-Time Search and Smart Querying', 'desc': 'This paper introduces SearchRAG, a new framework designed to enhance the performance of Large Language Models (LLMs) in medical question answering. Unlike traditional Retrieval-Augmented Generation (RAG) methods that rely on static knowledge bases, SearchRAG utilizes real-time search engines to access current and detailed medical information. The framework employs synthetic query generation to transform complex medical inquiries into queries suitable for search engines, and it uses uncertainty-based knowledge selection to ensure that only the most relevant information is included. Experimental results indicate that SearchRAG significantly boosts the accuracy of LLM responses, especially for intricate medical questions that demand precise and updated knowledge.'}, 'zh': {'title': '实时搜索提升医疗问答准确性', 'desc': '大型语言模型在一般领域表现出色，但在需要专业知识的任务中常常遇到困难。传统的检索增强生成（RAG）技术通常从静态知识库中检索外部信息，这些信息可能过时或不完整，缺乏准确医疗问答所需的细节。我们提出了一种新框架SearchRAG，通过利用实时搜索引擎来克服这些限制。实验结果表明，我们的方法在医疗问答任务中显著提高了响应准确性，尤其是对于需要详细和最新知识的复杂问题。'}}}, {'id': 'https://huggingface.co/papers/2502.13173', 'title': 'Thinking Preference Optimization', 'url': 'https://huggingface.co/papers/2502.13173', 'abstract': "Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (ThinkPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses. Instead, ThinkPO utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that ThinkPO further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%. Notably, ThinkPO is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.", 'score': 7, 'issue_id': 2311, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '6096d2396d4c584e', 'authors': ['Wang Yang', 'Hongye Jin', 'Jingfeng Yang', 'Vipin Chaudhary', 'Xiaotian Han'], 'affiliations': ['case.edu', 'gmail.com', 'tamu.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.13173.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#long_context', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация предпочтений мышления: новый шаг в улучшении рассуждений ИИ', 'desc': 'Этот научный труд представляет новый метод под названием Thinking Preference Optimization (ThinkPO) для улучшения способностей моделей машинного обучения к длинным цепочкам рассуждений. ThinkPO использует короткие ответы с рассуждениями как отвергнутые и длинные ответы как предпочтительные для одного и того же вопроса. Метод применяет прямую оптимизацию предпочтений, чтобы поощрять модель выдавать более длинные рассуждения. Эксперименты показывают, что ThinkPO улучшает точность математических рассуждений моделей на 8.6% и увеличивает длину выходных данных на 25.9%.'}, 'en': {'title': 'Boosting Reasoning with Preference Optimization', 'desc': "This paper introduces Thinking Preference Optimization (ThinkPO), a method designed to enhance long chain-of-thought (CoT) reasoning in supervised fine-tuning (SFT) of language models. Instead of needing new long CoT data, ThinkPO leverages existing short CoT responses as negative examples and long CoT responses as positive examples to optimize the model's preferences. The approach leads to significant improvements in reasoning accuracy and output length, demonstrating its effectiveness in refining model performance. Experiments show that ThinkPO can consistently boost the capabilities of SFT-ed models, particularly in mathematical reasoning tasks."}, 'zh': {'title': '思维偏好优化：提升长链推理的有效方法', 'desc': '监督微调（SFT）是一种有效的方法，用于提升小型大语言模型（LLM）在长链推理（CoT）方面的能力。为了持续提高推理能力，通常需要收集新的高质量长CoT数据，但这成本高且有限。本文提出了一种名为思维偏好优化（ThinkPO）的后SFT方法，它利用短CoT推理作为拒绝答案，长CoT推理作为选择答案，通过直接偏好优化来增强模型对长推理输出的偏好。实验表明，ThinkPO显著提高了经过SFT训练模型的推理性能，尤其在数学推理准确率上提升了8.6%。'}}}, {'id': 'https://huggingface.co/papers/2502.13962', 'title': 'Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering', 'url': 'https://huggingface.co/papers/2502.13962', 'abstract': 'Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.', 'score': 6, 'issue_id': 2311, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '06bee7f6d596d006', 'authors': ['William Jurayj', 'Jeffrey Cheng', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13962.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Уверенность языковых моделей: больше вычислений - меньше рисков', 'desc': 'Статья исследует влияние увеличения вычислительных ресурсов на работу больших языковых моделей при тестировании. Авторы вводят оценку уверенности модели в своих ответах и рассматривают сценарии с ненулевым уровнем риска ответа. Обнаружено, что увеличение вычислительных ресурсов не только повышает точность ответов, но и увеличивает уверенность модели в правильных ответах. Предлагается новый подход к оценке языковых моделей с учетом этих факторов.'}, 'en': {'title': 'Boosting Confidence in AI Responses through Compute Scaling', 'desc': "This paper discusses how increasing the computational resources available to large language models during inference can improve their performance on reasoning tasks. It highlights the importance of not just providing answers, but also assessing the model's confidence in those answers. By extracting confidence scores, the authors propose a method to filter responses based on their reliability. The study also introduces a new evaluation framework that accounts for the risk associated with model responses, moving beyond the traditional zero-risk assumption."}, 'zh': {'title': '提升模型信心，优化推理回答', 'desc': '这篇论文探讨了在推理基准测试中，大型语言模型在测试时计算能力的扩展所带来的显著性能提升。现有的评估方法假设推理系统必须对每个问题都给出答案，但这忽视了模型对答案的信心和是否总是提供回答的适当性。为了解决这些问题，研究者在推理过程中提取了置信度分数，以便对模型的回答进行阈值处理。结果表明，增加推理时的计算预算不仅提高了模型正确回答问题的能力，还增强了对正确回答的信心。'}}}, {'id': 'https://huggingface.co/papers/2502.13943', 'title': 'AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence', 'url': 'https://huggingface.co/papers/2502.13943', 'abstract': "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.", 'score': 5, 'issue_id': 2310, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '41ab630e56147df2', 'authors': ['Yuliang Liu', 'Junjie Lu', 'Zhaoling Chen', 'Chaofeng Qu', 'Jason Klein Liu', 'Chonghan Liu', 'Zefan Cai', 'Yunhui Xia', 'Li Zhao', 'Jiang Bian', 'Chuheng Zhang', 'Wei Shen', 'Zhouhan Lin'], 'affiliations': ['MSRA', 'Nanjing University', 'Shanghai Jiaotong University', 'UW-Madison', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2502.13943.jpg', 'data': {'categories': ['#reasoning', '#plp', '#training', '#open_source', '#math', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'AdaptiveStep: умное разбиение на шаги для эффективного обучения PRM', 'desc': 'Статья представляет новый метод AdaptiveStep для обучения моделей вознаграждения процессов (PRM). Вместо разбиения ответов на шаги фиксированной длины, AdaptiveStep использует уверенность модели в предсказании следующего слова для определения границ шагов рассуждения. Этот подход улучшает качество информации на каждом шаге и не требует ручной разметки. Эксперименты показали, что PRM, обученные с помощью AdaptiveStep, достигают лучших результатов в задачах математических рассуждений и генерации кода, превосходя существующие методы.'}, 'en': {'title': 'AdaptiveStep: Smarter Reasoning for Better Reward Models', 'desc': "This paper introduces AdaptiveStep, a novel method for training Process Reward Models (PRMs) that improves the way reasoning steps are defined. Instead of relying on fixed-length steps or predefined tokens, AdaptiveStep adjusts the reasoning process based on the model's confidence in predicting the next word. This approach enhances the decision-making information available at each step, leading to better performance in tasks like reward model learning. Experimental results show that PRMs trained with AdaptiveStep outperform traditional methods in mathematical reasoning and code generation, while also being more cost-effective."}, 'zh': {'title': '自适应步骤：提升奖励模型的决策能力', 'desc': '本文提出了一种新的训练过程奖励模型（PRM）的方法，称为AdaptiveStep。该方法通过根据模型对下一个单词预测的信心来划分推理步骤，从而提供更丰富的决策信息。与传统的基于规则的方法不同，AdaptiveStep不需要手动标注，且在数学推理和代码生成任务中表现出色。实验结果表明，使用AdaptiveStep训练的PRM在性能上超过了现有的开源PRM，并且构建成本降低了30%以上。'}}}, {'id': 'https://huggingface.co/papers/2502.13622', 'title': 'REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models', 'url': 'https://huggingface.co/papers/2502.13622', 'abstract': 'Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.', 'score': 3, 'issue_id': 2319, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '7c55d6b9ce07af47', 'authors': ['DongGeon Lee', 'Hwanjo Yu'], 'affiliations': ['Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.13622.jpg', 'data': {'categories': ['#hallucinations', '#multilingual', '#low_resource', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Точное обнаружение галлюцинаций в LLM с помощью извлечения информации', 'desc': 'Статья представляет новый метод REFIND для обнаружения галлюцинаций в выводах больших языковых моделей (LLM). REFIND использует извлеченные документы и новую метрику Context Sensitivity Ratio для количественной оценки чувствительности выводов LLM к доказательствам. Метод показал высокую эффективность в выявлении галлюцинаций на девяти языках, включая низкоресурсные. REFIND значительно превзошел базовые модели по показателю IoU при идентификации галлюцинированных участков текста.'}, 'en': {'title': 'REFIND: Enhancing LLM Reliability by Detecting Hallucinations', 'desc': 'This paper presents REFIND, a new framework designed to identify hallucinations in outputs from large language models (LLMs) by using information from retrieved documents. It introduces the Context Sensitivity Ratio (CSR), a metric that measures how much LLM outputs depend on the retrieved evidence. REFIND shows improved performance in detecting hallucinations compared to existing methods, achieving better intersection over union (IoU) scores across multiple languages, including those with fewer resources. The findings emphasize the importance of context sensitivity in enhancing the reliability of LLM applications.'}, 'zh': {'title': '提升大型语言模型的可靠性', 'desc': '本文介绍了一种新的框架REFIND，用于检测大型语言模型（LLM）输出中的幻觉现象。REFIND通过直接利用检索到的文档，识别LLM输出中的虚假信息。我们提出了一种新的度量标准——上下文敏感性比率（CSR），用于量化LLM输出对检索证据的敏感程度。REFIND在九种语言的评估中表现出色，显著优于基线模型，展示了在多语言环境中提高LLM应用可靠性的潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.12638', 'title': 'NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation', 'url': 'https://huggingface.co/papers/2502.12638', 'abstract': "3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol.", 'score': 3, 'issue_id': 2311, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'eb0d7b097262b590', 'authors': ['Zhiyuan Liu', 'Yanchen Luo', 'Han Huang', 'Enzhi Zhang', 'Sihang Li', 'Junfeng Fang', 'Yaorui Shi', 'Xiang Wang', 'Kenji Kawaguchi', 'Tat-Seng Chua'], 'affiliations': ['Chinese University of Hong Kong', 'Hokkaido University', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12638.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#architecture', '#diffusion', '#dataset', '#3d'], 'emoji': '🧪', 'ru': {'title': 'NExT-Mol: объединение языкового и диффузионного моделирования для генерации 3D-молекул', 'desc': 'NExT-Mol - это модель для генерации трехмерных молекул, сочетающая преимущества одномерных языковых моделей и трехмерных диффузионных моделей. Языковая модель используется для генерации валидных молекулярных структур в формате SELFIES, а диффузионная модель предсказывает их 3D-конформации. Авторы улучшили производительность, увеличив размер языковой модели, оптимизировав архитектуру диффузионной сети и применив трансферное обучение. NExT-Mol показала значительное улучшение результатов по сравнению с базовыми моделями в задачах de novo генерации и условной генерации 3D-молекул.'}, 'en': {'title': 'NExT-Mol: Bridging 1D Language Models and 3D Diffusion for Molecule Generation', 'desc': 'This paper presents NExT-Mol, a novel foundation model that integrates 1D SELFIES-based Language Models (LMs) with 3D diffusion models for generating 3D molecular structures. By leveraging the strengths of both approaches, NExT-Mol first generates valid 1D molecular representations and then predicts their corresponding 3D conformers. The model is enhanced through scaling the LM, refining the diffusion architecture, and employing transfer learning from 1D to 3D. The results show significant improvements in both 3D generation and conformer prediction, outperforming existing methods in terms of distributional similarity and validity.'}, 'zh': {'title': 'NExT-Mol：结合1D语言模型与3D扩散模型的分子生成新方法', 'desc': '3D分子生成对药物发现和材料设计至关重要。以往的研究主要集中在3D扩散模型上，但忽视了基于1D SELFIES的语言模型的优势，这些模型能够生成100%有效的分子并利用大规模的1D分子数据集。为此，我们提出了基础模型NExT-Mol，它结合了1D语言建模和3D扩散模型的优点，能够有效生成3D分子。通过扩大语言模型的规模、优化扩散神经网络架构以及应用1D到3D的迁移学习，NExT-Mol在3D生成和条件生成上都取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.13533', 'title': 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models', 'url': 'https://huggingface.co/papers/2502.13533', 'abstract': 'Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81times (16.95times), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).', 'score': 2, 'issue_id': 2318, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '968a6ad064935831', 'authors': ['Jun Zhang', 'Jue Wang', 'Huan Li', 'Lidan Shou', 'Ke Chen', 'Yang You', 'Guiming Xie', 'Xuejian Gong', 'Kunlong Zhou'], 'affiliations': ['AI Center, Guangdong OPPO Mobile Telecommunications Corp., Ltd.', 'College of Computer Science and Technology, Zhejiang University', 'Department of Computer Science, National University of Singapore', 'Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security', 'The State Key Laboratory of Blockchain and Data Security, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13533.jpg', 'data': {'categories': ['#small_models', '#transfer_learning', '#architecture', '#inference', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'LoRAM: Революция в обучении гигантских языковых моделей с минимальными ресурсами', 'desc': 'Статья представляет LoRAM - новый метод эффективного обучения больших языковых моделей (LLM). LoRAM обучает урезанную версию модели, значительно снижая требования к памяти, но сохраняя производительность полной модели при выводе. Метод использует прунинг, низкоранговую адаптацию (LoRA) и предварительное дообучение для преодоления разрыва в знаниях между урезанной и полной моделями. Эксперименты показывают, что LoRAM позволяет обучать 70-миллиардную модель на одном GPU с 20 ГБ памяти, демонстрируя значительное улучшение эффективности по сравнению с полным файнтюнингом и стандартным LoRA.'}, 'en': {'title': 'LoRAM: Efficient Fine-Tuning for Large Language Models', 'desc': 'This paper introduces LoRAM, a novel training scheme for Low-Rank Adaptation (LoRA) that enhances memory efficiency when fine-tuning large language models (LLMs). By focusing on a pruned version of the model, LoRAM identifies and trains only the essential low-rank matrices, significantly reducing the memory requirements during training. The approach leverages minimal-cost continual pre-training to bridge the knowledge gap between the pruned and original models, ensuring effective performance. Experimental results show that LoRAM can drastically cut down memory usage while maintaining or improving performance across various tasks compared to traditional LoRA methods.'}, 'zh': {'title': 'LoRAM：高效的低秩适应训练方案', 'desc': '大型语言模型（LLMs）在自然语言处理领域取得了显著进展，具备出色的任务泛化能力。低秩适应（LoRA）提供了一种经济高效的微调方案，通过冻结原始模型参数，仅训练轻量级的低秩适配器矩阵。然而，LoRA的内存占用主要由原始模型参数决定。为了解决这个问题，我们提出了LoRAM，一种内存高效的LoRA训练方案，通过在剪枝后的小模型上训练，获得剪枝的低秩矩阵，并在推理时与原始大模型结合使用。'}}}, {'id': 'https://huggingface.co/papers/2502.13766', 'title': 'GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking', 'url': 'https://huggingface.co/papers/2502.13766', 'abstract': 'Large Vision-Language Models (LVLMs) have recently gained attention due to their distinctive performance and broad applicability. While it has been previously shown that their efficacy in usage scenarios involving non-Western contexts falls short, existing studies are limited in scope, covering just a narrow range of cultures, focusing exclusively on a small number of cultural aspects, or evaluating a limited selection of models on a single task only. Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive multimodal benchmark designed to assess a broad spectrum of cultural knowledge across 144 countries representing six global macro-regions. GIMMICK comprises six tasks built upon three new datasets that span 728 unique cultural events or facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary and 26 open-weight models of all sizes. We systematically examine (1) regional cultural biases, (2) the influence of model size, (3) input modalities, and (4) external cues. Our analyses reveal strong biases toward Western cultures across models and tasks and highlight strong correlations between model size and performance, as well as the effectiveness of multimodal input and external geographic cues. We further find that models have more knowledge of tangible than intangible aspects (e.g., food vs. rituals) and that they excel in recognizing broad cultural origins but struggle with a more nuanced understanding.', 'score': 2, 'issue_id': 2317, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'e8f5148a9ef41989', 'authors': ['Florian Schneider', 'Carolin Holtermann', 'Chris Biemann', 'Anne Lauscher'], 'affiliations': ['Data Science Group, University of Hamburg', 'Language Technology Group, University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2502.13766.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#multimodal', '#ethics', '#dataset'], 'emoji': '🌍', 'ru': {'title': 'GIMMICK: глобальная оценка культурных знаний AI-моделей', 'desc': 'Статья представляет GIMMICK - новый мультимодальный бенчмарк для оценки культурных знаний крупных визуально-языковых моделей (LVLM) по 144 странам. Исследование охватывает 20 LVLM и 11 LLM моделей, оценивая их на 6 задачах, построенных на 3 новых наборах данных. Результаты показывают сильное смещение моделей в сторону западных культур и корреляцию между размером модели и производительностью. Также отмечается, что модели лучше распознают материальные аспекты культуры, чем нематериальные, и хорошо определяют общее культурное происхождение, но хуже справляются с более тонким пониманием.'}, 'en': {'title': 'GIMMICK: Bridging Cultural Gaps in Vision-Language Models', 'desc': 'This paper introduces GIMMICK, a comprehensive benchmark for evaluating Large Vision-Language Models (LVLMs) on cultural knowledge across 144 countries. It addresses the limitations of previous studies that focused mainly on Western contexts and a narrow range of cultural aspects. The benchmark includes six tasks and three new datasets, assessing 20 LVLMs and 11 LLMs on their understanding of diverse cultural events. The findings reveal significant biases towards Western cultures, a correlation between model size and performance, and a disparity in knowledge of tangible versus intangible cultural elements.'}, 'zh': {'title': '全球文化知识的全面评估', 'desc': '大型视觉语言模型（LVLMs）在性能和应用范围上引起了广泛关注。然而，现有研究在非西方文化场景中的有效性不足，且研究范围有限。为了解决这一问题，我们提出了GIMMICK，这是一个广泛的多模态基准，旨在评估144个国家的文化知识。我们的分析显示，模型对西方文化存在明显偏见，并且模型的大小、输入模态和外部线索对性能有显著影响。'}}}, {'id': 'https://huggingface.co/papers/2502.13581', 'title': 'ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation', 'url': 'https://huggingface.co/papers/2502.13581', 'abstract': 'Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@10 by 6.00% to 12.82%.', 'score': 2, 'issue_id': 2315, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '341531ed7311973d', 'authors': ['Yupeng Hou', 'Jianmo Ni', 'Zhankui He', 'Noveen Sachdeva', 'Wang-Cheng Kang', 'Ed H. Chi', 'Julian McAuley', 'Derek Zhiyuan Cheng'], 'affiliations': ['Google DeepMind', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.13581.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#optimization', '#data'], 'emoji': '🧩', 'ru': {'title': 'Контекстно-зависимая токенизация действий для улучшения генеративных рекомендаций', 'desc': 'Статья представляет новый подход к токенизации действий пользователей в генеративных рекомендательных системах под названием ActionPiece. В отличие от существующих методов, ActionPiece учитывает контекст при токенизации последовательностей действий, представляя каждое действие как набор признаков элементов. Словарь токенов формируется путем объединения часто встречающихся паттернов признаков. Метод также вводит регуляризацию перестановок наборов для учета их неупорядоченной природы.'}, 'en': {'title': 'Context Matters: Enhancing Generative Recommendations with ActionPiece', 'desc': 'This paper introduces ActionPiece, a novel approach to generative recommendation that enhances the tokenization of user actions by incorporating contextual information. Unlike traditional methods that treat each action independently, ActionPiece uses item features to create a more meaningful representation of actions based on their context. The model constructs a vocabulary of tokens by merging feature patterns that frequently occur together, allowing for a richer understanding of user behavior. Experimental results show that ActionPiece significantly improves recommendation performance, as evidenced by higher NDCG@10 scores compared to existing methods.'}, 'zh': {'title': '上下文驱动的生成推荐新方法', 'desc': '生成推荐（GR）是一种新兴的推荐方法，它将用户行为转化为离散的模式进行预测。现有的GR模型独立地对每个行为进行标记，未考虑上下文关系，导致相同的行为在不同序列中被赋予相同的固定标记。为了提高推荐效果，我们提出了ActionPiece，通过在标记行为序列时显式地融入上下文信息来解决这一问题。实验结果表明，ActionPiece在多个公共数据集上优于现有的行为标记方法，显著提高了推荐的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.11573', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'url': 'https://huggingface.co/papers/2502.11573', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \\InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.', 'score': 1, 'issue_id': 2317, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '6dbc067f1a808c3a', 'authors': ['Congkai Xie', 'Shuo Cai', 'Wenjun Wang', 'Pengxiang Li', 'Zhijie Sang', 'Kejing Yang', 'Yiming Zhang', 'Zhen Li', 'Guanghao Zhu', 'Zeyu Liu', 'Yang Yu', 'Yuhang Liu', 'Su Lu', 'Baoyi He', 'Qi Zhou', 'Xiaotian Han', 'Jianbo Yuan', 'Shengyu Zhang', 'Fei Wu', 'Hongxia Yang'], 'affiliations': ['Amazon', 'Beijing University of Posts and Telecommunications', 'Dalian University of Technology', 'Harbin Institute of Technology', 'Reallm Labs', 'South China University of Technology', 'The Hong Kong Polytechnic University', 'TikTok', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11573.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Малые модели с большими возможностями: революция в эффективном ИИ', 'desc': 'Статья посвящена разработке эффективных малых языковых моделей (SLM) и мультимодальных малых языковых моделей (MSLM), сохраняющих способности к рассуждению. Авторы представляют новый конвейер обучения, который улучшает возможности рассуждения и облегчает развертывание на периферийных устройствах. Модели достигают высокой производительности при минимизации затрат на разработку. Целью является продвижение систем искусственного интеллекта путем улучшения рассуждений, снижения барьеров для внедрения и решения проблем конфиденциальности за счет уменьшения размеров моделей.'}, 'en': {'title': 'Efficient AI: Small Models, Big Reasoning!', 'desc': 'This paper discusses the development of Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that maintain strong reasoning abilities while being more efficient. It addresses the challenges of high computational costs and privacy issues associated with larger models. The authors propose a new training pipeline that enhances the reasoning capabilities of these smaller models, making them suitable for deployment on edge devices. The goal is to improve AI systems by making them more accessible and secure through reduced model sizes.'}, 'zh': {'title': '小型模型，大智慧！', 'desc': '大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在推理能力上取得了显著进展，但仍面临高计算需求和隐私问题的挑战。本文专注于开发高效的小型语言模型（SLMs）和多模态小型语言模型（MSLMs），以保持竞争力的推理能力。我们提出了一种新颖的训练流程，增强了推理能力，并便于在边缘设备上部署，同时在降低开发成本的同时实现了最先进的性能。通过减小模型规模，\nInfR~旨在推动人工智能系统的发展，改善推理能力，降低采用门槛，并解决隐私问题。'}}}, {'id': 'https://huggingface.co/papers/2502.13573', 'title': 'Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective', 'url': 'https://huggingface.co/papers/2502.13573', 'abstract': 'Semi-supervised heterogeneous domain adaptation (SHDA) addresses learning across domains with distinct feature representations and distributions, where source samples are labeled while most target samples are unlabeled, with only a small fraction labeled. Moreover, there is no one-to-one correspondence between source and target samples. Although various SHDA methods have been developed to tackle this problem, the nature of the knowledge transferred across heterogeneous domains remains unclear. This paper delves into this question from an empirical perspective. We conduct extensive experiments on about 330 SHDA tasks, employing two supervised learning methods and seven representative SHDA methods. Surprisingly, our observations indicate that both the category and feature information of source samples do not significantly impact the performance of the target domain. Additionally, noise drawn from simple distributions, when used as source samples, may contain transferable knowledge. Based on this insight, we perform a series of experiments to uncover the underlying principles of transferable knowledge in SHDA. Specifically, we design a unified Knowledge Transfer Framework (KTF) for SHDA. Based on the KTF, we find that the transferable knowledge in SHDA primarily stems from the transferability and discriminability of the source domain. Consequently, ensuring those properties in source samples, regardless of their origin (e.g., image, text, noise), can enhance the effectiveness of knowledge transfer in SHDA tasks. The codes and datasets are available at https://github.com/yyyaoyuan/SHDA.', 'score': 0, 'issue_id': 2317, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': '68464eca5ca15556', 'authors': ['Yuan Yao', 'Xiaopu Zhang', 'Yu Zhang', 'Jian Jin', 'Qiang Yang'], 'affiliations': ['Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing 100095, China', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and also with WeBank, Shenzhen 518052, China', 'Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Research and Development, Inspur Computer Technology Co., Ltd., Beijing 100095, China', 'Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing 100095, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.13573.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн передачи знаний в гетерогенной адаптации доменов', 'desc': 'Статья исследует природу передаваемых знаний в задаче полуконтролируемой гетерогенной адаптации доменов (SHDA). Авторы провели масштабные эксперименты на 330 задачах SHDA, используя различные методы. Результаты показали, что категория и признаки исходных образцов не оказывают значительного влияния на производительность в целевом домене. Исследование выявило, что ключевыми факторами для эффективной передачи знаний в SHDA являются переносимость и различимость исходного домена.'}, 'en': {'title': 'Unlocking Knowledge Transfer in Heterogeneous Domains', 'desc': 'This paper explores Semi-supervised Heterogeneous Domain Adaptation (SHDA), which involves transferring knowledge from labeled source samples to unlabeled target samples that have different feature representations. The authors conducted extensive experiments on 330 SHDA tasks to understand the nature of knowledge transfer across these domains. Surprisingly, they found that the specific category and feature information of source samples do not significantly influence the performance in the target domain. Instead, they propose a Knowledge Transfer Framework (KTF) that emphasizes the importance of transferability and discriminability of source samples to improve knowledge transfer effectiveness in SHDA tasks.'}, 'zh': {'title': '揭示半监督异构领域适应中的可转移知识', 'desc': '半监督异构领域适应（SHDA）研究在特征表示和分布不同的领域之间进行学习的问题。在这种情况下，源样本是有标签的，而大多数目标样本是无标签的，只有少量样本是有标签的。本文通过大量实验探讨了在异构领域中可转移知识的本质，发现源样本的类别和特征信息对目标领域的性能影响不大。我们提出了一个统一的知识转移框架（KTF），并发现可转移知识主要来自源领域的可转移性和可区分性。'}}}, {'id': 'https://huggingface.co/papers/2502.05173', 'title': 'VideoRoPE: What Makes for Good Video Rotary Position Embedding?', 'url': 'https://huggingface.co/papers/2502.05173', 'abstract': 'While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.', 'score': 48, 'issue_id': 2118, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ba284ed1a62b3c2c', 'authors': ['Xilin Wei', 'Xiaoran Liu', 'Yuhang Zang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Jian Tong', 'Haodong Duan', 'Qipeng Guo', 'Jiaqi Wang', 'Xipeng Qiu', 'Dahua Lin'], 'affiliations': ['Fudan University, Shanghai, China', 'Shanghai AI Laboratory, Shanghai, China', 'Shanghai Innovation Institute, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05173.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#architecture', '#video', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'VideoRoPE: Эффективное позиционное кодирование для глубокого обучения на видео', 'desc': 'Статья представляет VideoRoPE - новый метод позиционного кодирования для видео, основанный на Rotary Position Embedding. Авторы провели анализ и выявили 4 ключевые характеристики для эффективной адаптации RoPE к видео. Они предложили сложную задачу V-NIAH-D для демонстрации недостатков существующих вариантов RoPE. VideoRoPE имеет 3D-структуру, сохраняющую пространственно-временные отношения, и превосходит предыдущие варианты RoPE в различных задачах обработки видео.'}, 'en': {'title': 'Enhancing Video Understanding with VideoRoPE', 'desc': 'This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data.'}, 'zh': {'title': 'VideoRoPE：视频中的旋转位置嵌入新突破', 'desc': '本文探讨了如何将旋转位置嵌入（RoPE）有效地扩展到视频数据中。研究分析了四个关键特性，这些特性对于RoPE在视频中的适应性至关重要。我们提出了一个新的任务V-NIAH-D，展示了现有RoPE变体在处理视频时容易受到干扰的缺陷。基于这些分析，我们提出了VideoRoPE，它通过3D结构来保持时空关系，并在多个下游任务中表现优于之前的RoPE变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04507', 'title': 'Fast Video Generation with Sliding Tile Attention', 'url': 'https://huggingface.co/papers/2502.04507', 'abstract': 'Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.', 'score': 38, 'issue_id': 2120, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'dcbf1070dac1b391', 'authors': ['Peiyuan Zhang', 'Yongqi Chen', 'Runlong Su', 'Hangliang Ding', 'Ion Stoica', 'Zhenghong Liu', 'Hao Zhang'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2502.04507.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью скользящего плиточного внимания', 'desc': 'Статья представляет метод скользящего плиточного внимания (STA) для ускорения генерации видео с помощью диффузионных трансформеров. STA использует наблюдение, что оценки внимания в предобученных моделях диффузии видео в основном концентрируются в локализованных 3D-окнах. Этот подход устраняет избыточность полного внимания, сохраняя выразительность и эффективность на аппаратном уровне. STA ускоряет внимание в 2.8-17 раз по сравнению с FlashAttention-2 и в 1.6-10 раз по сравнению с FlashAttention-3, значительно сокращая время генерации видео.'}, 'en': {'title': 'Efficient Video Generation with Sliding Tile Attention', 'desc': 'This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks.'}, 'zh': {'title': '滑动瓦片注意力：高效视频生成的新突破', 'desc': '本论文介绍了一种新的滑动瓦片注意力机制（STA），旨在提高视频生成的效率。传统的扩散变换器在生成视频时计算成本高，而STA通过关注局部的时空区域来减少冗余计算。与传统的滑动窗口注意力不同，STA采用了硬件友好的设计，逐块处理，保持了表达能力的同时提高了计算效率。经过优化，STA在视频生成任务中显著加速了注意力计算，降低了延迟，同时不影响生成质量。'}}}, {'id': 'https://huggingface.co/papers/2502.04896', 'title': 'Goku: Flow Based Video Generative Foundation Models', 'url': 'https://huggingface.co/papers/2502.04896', 'abstract': 'This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.', 'score': 30, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ad6ef6eed233cc90', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Yuqi Zhang', 'Yida Zhang', 'Fengda Zhu', 'Hao Yang', 'Hongxiang Hao', 'Hui Wu', 'Zhichao Lai', 'Yifei Hu', 'Ting-Che Lin', 'Shilong Zhang', 'Fu Li', 'Chuan Li', 'Xing Wang', 'Yanghua Peng', 'Peize Sun', 'Ping Luo', 'Yi Jiang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['Bytedance Inc', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04896.jpg', 'data': {'categories': ['#cv', '#training', '#video', '#architecture', '#data', '#benchmark', '#dataset'], 'emoji': '🐉', 'ru': {'title': 'Goku: Новый уровень генерации изображений и видео', 'desc': 'Статья представляет семейство моделей Goku для совместной генерации изображений и видео. Модели используют трансформеры с выпрямленным потоком для достижения передовых результатов. Авторы описывают ключевые элементы, включая подготовку данных, архитектуру модели и инфраструктуру для эффективного обучения. Goku демонстрирует превосходную производительность в качественных и количественных оценках, устанавливая новые стандарты в основных задачах генерации изображений и видео.'}, 'en': {'title': 'Goku: Revolutionizing Image and Video Generation with Transformers', 'desc': 'This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models.'}, 'zh': {'title': 'Goku：图像与视频生成的新标杆', 'desc': '本文介绍了Goku，这是一种先进的联合图像和视频生成模型，利用了修正流Transformer以实现行业领先的性能。我们详细阐述了高质量视觉生成的基础要素，包括数据整理流程、模型架构设计、流的公式化以及高效稳健的大规模训练基础设施。Goku模型在定性和定量评估中表现优越，为主要任务设定了新的基准。具体而言，Goku在文本到图像生成中达到了0.76的GenEval和83.65的DPG-Bench，在文本到视频任务中达到了84.85的VBench。'}}}, {'id': 'https://huggingface.co/papers/2502.05003', 'title': 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations', 'url': 'https://huggingface.co/papers/2502.05003', 'abstract': 'One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.', 'score': 28, 'issue_id': 2122, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c011c3548ad7a5dd', 'authors': ['Andrei Panferov', 'Jiale Chen', 'Soroush Tabesh', 'Roberto L. Castro', 'Mahdi Nikdan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05003.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное квантование для обучения языковых моделей', 'desc': 'Статья представляет новый метод QuEST для обучения больших языковых моделей с использованием квантования. QuEST позволяет обучать модели с весами и активациями в 4 бита или меньше, сохраняя конкурентоспособную точность по сравнению с FP16. Метод улучшает квантование распределений весов и активаций, а также вводит новый оценщик градиента доверия. Эксперименты показывают, что QuEST обеспечивает стабильные законы масштабирования для различных уровней точности.'}, 'en': {'title': 'QuEST: Revolutionizing Quantization for Efficient Language Models', 'desc': 'This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support.'}, 'zh': {'title': 'QuEST：低精度高效训练大语言模型的创新方法', 'desc': '本文提出了一种名为QuEST的新方法，旨在通过量化感知训练（QAT）来提高大语言模型的训练效率。QuEST能够在4位或更低的精度下训练模型，同时保持与FP16精度相当的准确性。该方法通过改进权重和激活的量化过程，以及引入新的信任梯度估计器，来实现更稳定的训练。实验结果表明，QuEST在各种硬件支持的精度范围内都能实现稳定的扩展性，并且可以有效执行。'}}}, {'id': 'https://huggingface.co/papers/2502.05171', 'title': 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach', 'url': 'https://huggingface.co/papers/2502.05171', 'abstract': 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.', 'score': 24, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '4386159312d9856b', 'authors': ['Jonas Geiping', 'Sean McLeish', 'Neel Jain', 'John Kirchenbauer', 'Siddharth Singh', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Tom Goldstein'], 'affiliations': ['ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center', 'Lawrence Livermore National Laboratory', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.05171.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Глубокие рассуждения в латентном пространстве: новый подход к языковым моделям', 'desc': 'В статье представлена новая архитектура языковой модели, способная масштабировать вычисления во время тестирования путем неявных рассуждений в латентном пространстве. Модель работает путем итерации рекуррентного блока, разворачиваясь до произвольной глубины во время тестирования. В отличие от подходов, основанных на цепочке рассуждений, этот метод не требует специализированных обучающих данных и может работать с небольшими контекстными окнами. Авторы масштабировали экспериментальную модель до 3,5 миллиардов параметров и 800 миллиардов токенов, показав значительное улучшение производительности на тестах рассуждений.'}, 'en': {'title': 'Scaling Reasoning with Latent Space Computation', 'desc': 'This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts.'}, 'zh': {'title': '隐式推理，提升语言模型的计算能力', 'desc': '我们研究了一种新颖的语言模型架构，该架构能够通过在潜在空间中隐式推理来扩展测试时的计算能力。我们的模型通过迭代递归块工作，从而在测试时可以展开到任意深度。这与主流推理模型不同，后者通过生成更多的标记来增加计算量。我们的模型不需要特殊的训练数据，能够处理小的上下文窗口，并且能够捕捉不易用语言表示的推理类型。'}}}, {'id': 'https://huggingface.co/papers/2502.05176', 'title': 'AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting', 'url': 'https://huggingface.co/papers/2502.05176', 'abstract': 'Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.', 'score': 22, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '9b52f2788f53c3c0', 'authors': ['Chung-Ho Wu', 'Yang-Jung Chen', 'Ying-Huan Chen', 'Jie-Ying Lee', 'Bo-Hsu Ke', 'Chun-Wei Tuan Mu', 'Yi-Chuan Huang', 'Chin-Yang Lin', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05176.jpg', 'data': {'categories': ['#dataset', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Революция в 3D-реконструкции: AuraFusion360 для безупречного восстановления сцен', 'desc': 'AuraFusion360 - это новый метод восстановления трехмерных сцен на основе Gaussian Splatting. Он использует генерацию масок невидимых областей с учетом глубины, адаптивную диффузию глубины и улучшение деталей на основе SDEdit для создания высококачественных результатов. Метод превосходит существующие подходы по качеству восприятия и геометрической точности при изменении точки обзора. Авторы также представили первый набор данных 360-USID для оценки методов восстановления сцен с охватом 360 градусов.'}, 'en': {'title': 'Revolutionizing 3D Scene Inpainting with AuraFusion360', 'desc': 'AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy.'}, 'zh': {'title': 'AuraFusion360：三维场景修复的新突破', 'desc': '三维场景修复在虚拟现实和建筑可视化等应用中非常重要，但现有方法在360度无界场景中面临视图一致性和几何精度的挑战。我们提出了AuraFusion360，这是一种新颖的基于参考的方法，能够在高质量的3D场景中进行物体移除和孔填充。该方法引入了深度感知的未见掩码生成、适应性引导深度扩散和基于SDEdit的细节增强，确保多视图的一致性。通过大量实验，AuraFusion360在感知质量和几何精度方面显著优于现有方法，能够在剧烈视角变化中保持高质量的修复效果。'}}}, {'id': 'https://huggingface.co/papers/2502.05163', 'title': 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails', 'url': 'https://huggingface.co/papers/2502.05163', 'abstract': 'The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.', 'score': 17, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ae863d89ab71ab51', 'authors': ['Yihe Deng', 'Yu Yang', 'Junkai Zhang', 'Wei Wang', 'Bo Li'], 'affiliations': ['University of California, Los Angeles', 'University of Illinois at Urbana-Champaign', 'VirtueAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05163.jpg', 'data': {'categories': ['#low_resource', '#inference', '#synthetic', '#dataset', '#open_source', '#multilingual', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Улучшение многоязычной безопасности LLM через совместное обучение генератора и ограничителя', 'desc': 'Статья представляет новый подход к созданию многоязычных моделей-ограничителей для обеспечения безопасности больших языковых моделей (LLM). Авторы предлагают framework с двумя игроками на основе обучения с подкреплением, где генератор и модель-ограничитель развиваются совместно для создания синтетических данных. Теоретически это взаимодействие формализовано как игра двух игроков с доказанной сходимостью к равновесию Нэша. Эмпирические оценки показывают, что предложенная модель превосходит современные аналоги, особенно для языков с меньшими ресурсами.'}, 'en': {'title': 'Enhancing Multilingual Safety in LLMs with Synthetic Data Generation', 'desc': "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."}, 'zh': {'title': '多语言护栏模型的创新进展', 'desc': '随着大型语言模型（LLMs）的快速发展，确保其负责任使用的护栏模型需求增加，尤其是在检测不安全和非法内容方面。虽然英语的安全数据相对丰富，但由于其他语言开放源代码安全数据的稀缺，多语言护栏建模仍然未被充分探索。为了解决这一问题，我们提出了一种新颖的双玩家强化学习框架，其中生成器和护栏模型对抗性地共同进化，以生成高质量的合成数据用于多语言护栏训练。我们的模型在多语言安全任务中取得了显著进展，特别是在处理低资源语言的不平衡问题上。'}}}, {'id': 'https://huggingface.co/papers/2502.04403', 'title': 'Agency Is Frame-Dependent', 'url': 'https://huggingface.co/papers/2502.04403', 'abstract': "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.", 'score': 13, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '32ceb8df4d77794a', 'authors': ['David Abel', 'André Barreto', 'Michael Bowling', 'Will Dabney', 'Shi Dong', 'Steven Hansen', 'Anna Harutyunyan', 'Khimya Khetarpal', 'Clare Lyle', 'Razvan Pascanu', 'Georgios Piliouras', 'Doina Precup', 'Jonathan Richens', 'Mark Rowland', 'Tom Schaul', 'Satinder Singh'], 'affiliations': ['Amii, University of Alberta', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.04403.jpg', 'data': {'categories': ['#rl', '#agi', '#reasoning', '#math'], 'emoji': '🤖', 'ru': {'title': 'Агентность: все зависит от точки зрения', 'desc': 'Статья рассматривает концепцию агентности в контексте обучения с подкреплением. Авторы утверждают, что агентность фундаментально зависит от системы отсчета. Они поддерживают этот тезис, анализируя ключевые свойства агентности, предложенные в предыдущих исследованиях. Статья подчеркивает необходимость учета зависимости от системы отсчета в изучении агентности и обсуждает последствия для обучения с подкреплением.'}, 'en': {'title': 'Agency in Reinforcement Learning: A Frame-Dependent Perspective', 'desc': 'This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning.'}, 'zh': {'title': '能动性：依赖于框架的系统能力', 'desc': '本文探讨了系统的能动性，特别是在强化学习的背景下。能动性是指系统朝着目标引导结果的能力，但判断一个系统是否具备能动性是一个复杂的问题。我们认为，能动性是依赖于参考框架的，任何对系统能动性的测量都必须相对于某个参考框架进行。通过哲学论证，我们支持这一观点，并讨论了这一结论对强化学习的影响。'}}}, {'id': 'https://huggingface.co/papers/2502.04728', 'title': 'Generating Symbolic World Models via Test-time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2502.04728', 'abstract': 'Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.', 'score': 12, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'cd97668cd0eee0ee', 'authors': ['Zhouliang Yu', 'Yuhuan Yuan', 'Tim Z. Xiao', 'Fuxiang Frank Xia', 'Jie Fu', 'Ge Zhang', 'Ge Lin', 'Weiyang Liu'], 'affiliations': ['Environmental Systems Research Institute, Inc.', 'Max Planck Institute for Intelligent Systems, Tübingen', 'SEED, Bytedance', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04728.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#data', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности планирования с помощью PDDL и LLM', 'desc': 'Статья представляет новый подход к решению сложных задач планирования с использованием больших языковых моделей (LLM). Авторы предлагают использовать язык определения планирования (PDDL) для создания точной символической модели мира. Метод включает алгоритм Best-of-N для улучшения качества начального решения и последующее уточнение с помощью вербализованного машинного обучения. Результаты показывают значительное превосходство над существующими методами в генерации доменов PDDL и решении задач планирования высокого уровня.'}, 'en': {'title': 'Enhancing LLMs for Optimal Planning with PDDL', 'desc': "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."}, 'zh': {'title': '利用PDDL提升规划问题解决能力', 'desc': '本论文探讨了如何利用大型语言模型（LLMs）解决复杂的规划问题。为了避免规则违反和确保最优性，研究者们引入了规划领域定义语言（PDDL），作为一种精确的状态描述工具。通过PDDL，可以生成符号世界模型，并应用经典搜索算法（如A*）来寻找最优计划。本文提出了一种简单有效的算法，通过Best-of-N采样和细致的机器学习优化，显著提高了PDDL领域的生成质量，成功率超过50%。'}}}, {'id': 'https://huggingface.co/papers/2502.05179', 'title': 'FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation', 'url': 'https://huggingface.co/papers/2502.05179', 'abstract': 'DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .', 'score': 9, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c3147244e03af4a6', 'authors': ['Shilong Zhang', 'Wenbo Li', 'Shoufa Chen', 'Chongjian Ge', 'Peize Sun', 'Yida Zhang', 'Yi Jiang', 'Zehuan Yuan', 'Binyue Peng', 'Ping Luo'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05179.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео высокого разрешения с предпросмотром', 'desc': 'Статья представляет новый двухэтапный подход к генерации видео на основе текста под названием FlashVideo. На первом этапе модель фокусируется на соответствии промпту, генерируя видео низкого разрешения. Второй этап использует сопоставление потоков для эффективного создания деталей высокого разрешения. Этот метод позволяет достичь высокого качества генерации видео при меньших вычислительных затратах по сравнению с существующими подходами. Кроме того, пользователи могут предварительно просмотреть результат перед полной генерацией, что повышает коммерческую привлекательность технологии.'}, 'en': {'title': 'FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework', 'desc': 'The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use.'}, 'zh': {'title': 'FlashVideo：高效生成高分辨率视频的创新框架', 'desc': 'DiT扩散模型在文本到视频生成方面取得了显著成功，但高内容和运动保真度通常需要大量模型参数和函数评估。为了解决这些计算需求，我们提出了一种新的两阶段框架FlashVideo，旨在平衡生成的保真度和质量。在第一阶段，通过低分辨率生成过程优先考虑提示保真度，利用大参数和足够的函数评估提高计算效率。第二阶段则在低分辨率和高分辨率之间建立流匹配，有效生成细节，且所需的函数评估最小化。'}}}, {'id': 'https://huggingface.co/papers/2502.04959', 'title': 'No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces', 'url': 'https://huggingface.co/papers/2502.04959', 'abstract': 'Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .', 'score': 8, 'issue_id': 2127, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a73679e79ff14b7c', 'authors': ['Daniel Marczak', 'Simone Magistri', 'Sebastian Cygert', 'Bartłomiej Twardowski', 'Andrew D. Bagdanov', 'Joost van de Weijer'], 'affiliations': ['Computer Vision Center, Barcelona, Spain', 'Department of Computer Science, Universitat Autonoma de Barcelona, Spain', 'Department of Information Engineering, University of Florence, Italy', 'Gdansk University of Technology, Poland', 'IDEAS NCBR, Warsaw, Poland', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2502.04959.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Эффективное объединение моделей машинного обучения без дополнительного обучения', 'desc': 'Статья посвящена объединению весов нескольких моделей, обученных на конкретных задачах, в одну многозадачную модель. Авторы исследуют ключевые характеристики матриц задач, которые позволяют эффективно объединять модели. Они предлагают изотропный фреймворк для объединения, который выравнивает спектр сингулярных значений матриц задач и улучшает их выравнивание. Предложенный подход достигает наилучших результатов в различных сценариях, включая разные наборы задач и масштабы моделей.'}, 'en': {'title': 'Bridging the Performance Gap in Model Merging', 'desc': 'This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios.'}, 'zh': {'title': '有效的模型合并方法提升多任务性能', 'desc': '模型合并是将多个特定任务模型的权重整合为一个多任务模型的过程。尽管这一问题受到越来越多的关注，但合并模型与单任务模型之间仍存在显著的性能差距。本文研究了任务矩阵的关键特性，这些矩阵是应用于预训练模型的权重更新矩阵，发现任务特定矩阵与合并矩阵之间的对齐程度与性能提升密切相关。我们提出了一种各向同性合并框架，通过平坦化任务矩阵的奇异值谱，增强对齐性，从而缩小性能差距，并在多个场景中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04520', 'title': "Linear Correlation in LM's Compositional Generalization and Hallucination", 'url': 'https://huggingface.co/papers/2502.04520', 'abstract': 'The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., "X lives in the city of" rightarrow "X lives in the country of" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM\'s generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.', 'score': 8, 'issue_id': 2119, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '41ef9027d1533f06', 'authors': ['Letian Peng', 'Chenyang An', 'Shibo Hao', 'Chengyu Dong', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.04520.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#agi', '#data', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Линейность как ключ к обобщению языковых моделей', 'desc': 'Статья исследует феномен линейных корреляций в языковых моделях при композиции знаний. Авторы обнаружили, что существует линейное преобразование между связанными знаниями, которое отображает логиты предсказания следующего токена от одного промпта к другому. Это явление устойчиво к масштабному дообучению и может служить потенциальным идентификатором обобщения языковой модели. Исследование показывает, что такие линейные корреляции могут быть изучены с помощью одной полносвязной нейронной сети и предобученных представлений словаря.'}, 'en': {'title': 'Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition', 'desc': 'This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge.'}, 'zh': {'title': '语言模型的线性相关性与知识组合', 'desc': '这篇论文探讨了语言模型（LM）在知识组合中的线性相关现象。研究发现，某些相关知识之间存在线性变换，这种变换可以将一个提示的下一个标记预测从一个映射到另一个。比如，从"X 住在城市"可以转变为"X 住在国家"。结果表明，线性变换在大规模微调中具有韧性，并且当与现实世界关系一致时能够推广更新的知识，但当偏离时则会导致幻觉。'}}}, {'id': 'https://huggingface.co/papers/2502.04416', 'title': 'CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference', 'url': 'https://huggingface.co/papers/2502.04416', 'abstract': 'Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.', 'score': 7, 'issue_id': 2125, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0b4520b0860c835c', 'authors': ['Zehua Pei', 'Lancheng Zou', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04416.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное превращение плотных LLM в разреженные MoE модели', 'desc': 'Статья представляет новый метод CMoE (Carved MoE) для эффективного создания моделей Mixture-of-Experts из плотных моделей больших языковых моделей (LLM). CMoE группирует нейроны в общие и маршрутизируемые экспертные группы на основе уровней активации. Метод включает механизм маршрутизации с дифференцируемым процессом и балансировкой нагрузки. CMoE позволяет быстро создать эффективную MoE модель из плотной модели размером 7 миллиардов параметров, используя небольшой объем данных.'}, 'en': {'title': 'Efficiently Carving Mixture-of-Experts for Large Language Models', 'desc': 'This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning.'}, 'zh': {'title': '高效雕刻混合专家模型的创新框架', 'desc': '大型语言模型（LLMs）通过增加模型参数实现了出色的性能，但这也带来了显著的推理开销。前馈网络（FFNs）在LLM参数中占主导地位，隐藏神经元的激活稀疏性很高。为此，研究人员提出了混合专家（MoE）架构，仅激活一部分参数。然而，现有方法通常需要大量的训练数据和资源，限制了其实用性。我们提出了CMoE（Carved MoE），一个新颖的框架，可以高效地从稠密模型中雕刻出MoE模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04404', 'title': 'Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models', 'url': 'https://huggingface.co/papers/2502.04404', 'abstract': "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.", 'score': 7, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a7bd201755c7ea1d', 'authors': ['Xiao-Wen Yang', 'Xuan-Yi Zhu', 'Wen-Da Wei', 'Ding-Chu Zhang', 'Jie-Jing Shao', 'Zhi Zhou', 'Lan-Zhe Guo', 'Yu-Feng Li'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University, China', 'School of Artificial Intelligence, Nanjing University, China', 'School of Intelligence Science and Technology, Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04404.jpg', 'data': {'categories': ['#agi', '#training', '#inference', '#agents', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самостоятельный возврат: путь к более разумным ИИ', 'desc': 'Статья представляет новый механизм самостоятельного возврата (self-backtracking) для больших языковых моделей (LLM). Этот механизм позволяет LLM автономно определять, когда и где нужно вернуться назад в процессе рассуждений. Авторы утверждают, что это улучшает способности LLM к рассуждению и повышает эффективность, превращая медленное мышление в быстрое через самосовершенствование. Эмпирические оценки показывают значительное улучшение возможностей рассуждения LLM с использованием этого подхода.'}, 'en': {'title': 'Empowering LLMs with Self-Backtracking for Enhanced Reasoning', 'desc': 'This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods.'}, 'zh': {'title': '自我回溯机制：提升语言模型推理能力的关键', 'desc': '将慢思考机制整合到大型语言模型（LLMs）中，为实现二级AGI推理器提供了有希望的途径。当前的挑战包括低效的过度思考和对辅助奖励模型的过度依赖。我们指出，这些限制源于LLMs无法内化搜索过程，而搜索过程是有效推理的关键组成部分。我们提出了一种自我回溯机制，使LLMs能够在训练和推理过程中自主决定何时以及如何回溯，从而显著提升推理能力和效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03738', 'title': 'Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More', 'url': 'https://huggingface.co/papers/2502.03738', 'abstract': 'Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.', 'score': 6, 'issue_id': 2122, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'aa76478090a36c04', 'authors': ['Feng Wang', 'Yaodong Yu', 'Guoyizhe Wei', 'Wei Shao', 'Yuyin Zhou', 'Alan Yuille', 'Cihang Xie'], 'affiliations': ['Johns Hopkins University', 'UC Berkeley', 'UC Santa Cruz', 'University of Florida'], 'pdf_title_img': 'assets/pdf/title_img/2502.03738.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Пиксельная токенизация превосходит патчи в vision-моделях', 'desc': 'Исследование посвящено анализу влияния размера патчей в Vision Transformer (ViT) на качество распознавания изображений. Авторы обнаружили, что уменьшение размера патчей до 1x1 пикселя (пиксельная токенизация) улучшает предсказательную способность модели. Этот эффект наблюдается для различных задач компьютерного зрения, масштабов входных данных и архитектур, включая ViT и Mamba. Эксперименты показали, что модель базового размера с длиной визуальной последовательности 50 176 токенов достигает точности 84,6% на наборе данных ImageNet-1k.'}, 'en': {'title': 'Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers', 'desc': 'This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models.'}, 'zh': {'title': '小块更优，视觉理解更强！', 'desc': '本文研究了视觉变换器（ViT）中图像分块（patchification）对信息损失的影响。通过缩小图像的空间大小，分块方法可以有效减少令牌序列的长度，从而降低计算成本。我们发现，随着分块大小的减小，模型的预测性能持续提高，直到达到最小的1x1像素分块。该研究结果适用于多种视觉任务和不同的模型架构，为未来构建非压缩视觉模型提供了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2502.05178', 'title': 'QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05178', 'abstract': 'We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.', 'score': 6, 'issue_id': 2121, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'bbc1f1a0b7f5423f', 'authors': ['Yue Zhao', 'Fuzhao Xue', 'Scott Reed', 'Linxi Fan', 'Yuke Zhu', 'Jan Kautz', 'Zhiding Yu', 'Philipp Krähenbühl', 'De-An Huang'], 'affiliations': ['NVIDIA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.05178.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'QLIP: Единый подход к пониманию и генерации изображений', 'desc': 'QLIP - это метод визуальной токенизации, объединяющий высококачественную реконструкцию изображений с пониманием изображений без предварительного обучения. Он использует автоэнкодер с бинарно-сферическим квантованием и оптимизирует одновременно реконструкцию и выравнивание языка и изображений. Авторы предлагают двухэтапный процесс обучения для эффективного сочетания требований к большим батчам и ограничений по памяти. QLIP может заменить визуальный энкодер в мультимодальных моделях и токенизатор изображений в генеративных моделях, показывая сопоставимую или лучшую производительность.'}, 'en': {'title': 'QLIP: Bridging Visual and Language Understanding with Efficient Tokenization', 'desc': 'The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation.'}, 'zh': {'title': '量化语言-图像预训练：多模态理解的新突破', 'desc': '我们介绍了一种量化语言-图像预训练方法（QLIP），这是一种视觉标记化方法，结合了最先进的重建质量和零-shot图像理解能力。QLIP使用基于二元球面量化的自编码器进行训练，同时优化重建和语言-图像对齐目标。我们首次证明这两个目标可以动态平衡，而不是相互对立。QLIP在多模态理解和文本条件图像生成方面表现出色，可以作为LLaVA的视觉编码器和LlamaGen的图像标记器的替代方案，性能相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.04363', 'title': 'On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices', 'url': 'https://huggingface.co/papers/2502.04363', 'abstract': 'We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.', 'score': 6, 'issue_id': 2119, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '339b45dee733174c', 'authors': ['Bosung Kim', 'Kyuhwan Lee', 'Isu Jeong', 'Jungmin Cheon', 'Yeojin Lee', 'Seulki Lee'], 'affiliations': ['Ulsan National Institute of Science and Technology South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.04363.jpg', 'data': {'categories': ['#inference', '#video', '#open_source', '#diffusion', '#architecture', '#low_resource'], 'emoji': '📱', 'ru': {'title': 'Генерация видео по тексту прямо на вашем смартфоне', 'desc': 'On-device Sora представляет собой инновационное решение для генерации видео на основе текста с использованием диффузионных моделей, работающее на смартфонах. Система применяет три новые техники: Linear Proportional Leap (LPL) для сокращения шагов денойзинга, Temporal Dimension Token Merging (TDTM) для оптимизации вычислений в слоях внимания, и Concurrent Inference with Dynamic Loading (CI-DL) для эффективного использования ограниченной памяти устройства. Эксперименты на iPhone 15 Pro показали, что On-device Sora способна генерировать видео высокого качества, сравнимые с результатами Open-Sora на мощных GPU.'}, 'en': {'title': 'Empowering Video Creation on Your Smartphone!', 'desc': 'On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services.'}, 'zh': {'title': '移动设备上的高效视频生成新突破', 'desc': '我们提出了On-device Sora，这是首个基于扩散模型的移动设备文本到视频生成解决方案，能够高效地在智能手机上运行。该系统采用了三种新技术来解决移动设备在计算和内存方面的限制。首先，线性比例跳跃（LPL）通过高效的跳跃方法减少了视频扩散中所需的去噪步骤。其次，时间维度令牌合并（TDTM）通过沿时间维度合并连续令牌，降低了注意力层中密集的令牌处理计算。'}}}, {'id': 'https://huggingface.co/papers/2502.05092', 'title': 'Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2502.05092', 'abstract': "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.", 'score': 5, 'issue_id': 2128, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ea9f14d34d4cbb60', 'authors': ['Rohit Saxena', 'Aryo Pradipta Gema', 'Pasquale Minervini'], 'affiliations': ['ILCC, School of Informatics, University of Edinburgh', 'Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05092.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#cv', '#interpretability'], 'emoji': '⏰', 'ru': {'title': 'Время бросает вызов искусственному интеллекту', 'desc': 'Статья исследует способности мультимодальных больших языковых моделей (MLLM) в интерпретации времени и дат через аналоговые часы и календари. Авторы создали структурированный набор данных, включающий ClockQA с различными стилями часов и CalendarQA с календарными изображениями и вопросами. Цель исследования - проанализировать, как MLLM выполняют визуальное распознавание, числовые рассуждения и временные выводы. Результаты показывают, что надежное понимание времени остается значительной проблемой для MLLM, несмотря на недавние достижения.'}, 'en': {'title': 'Unlocking Time: Challenges for Multimodal Language Models', 'desc': "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."}, 'zh': {'title': '理解时间的挑战：多模态语言模型的局限性', 'desc': '本研究探讨了多模态大型语言模型（MLLMs）在理解时间和日期方面的能力，特别是通过模拟时钟和年度日历的视觉表示。我们创建了一个结构化的数据集，包括两部分：ClockQA，包含不同风格的时钟及相关时间问题；CalendarQA，包含年度日历图像及常见日期问题。我们的目标是分析MLLMs在处理与时间相关的视觉数据时的视觉识别、数值推理和时间推断能力。尽管最近取得了一些进展，但MLLMs在可靠理解时间方面仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2502.03512', 'title': 'YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment', 'url': 'https://huggingface.co/papers/2502.03512', 'abstract': 'Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.', 'score': 4, 'issue_id': 2122, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ebf6482c46f8fe2f', 'authors': ['Amitava Das', 'Yaswanth Narsupalli', 'Gurpreet Singh', 'Vinija Jain', 'Vasu Sharma', 'Suranjana Trivedy', 'Aman Chadha', 'Amit Sheth'], 'affiliations': ['Amazon AI, USA', 'Artificial Intelligence Institute, University of South Carolina, USA', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.03512.jpg', 'data': {'categories': ['#benchmark', '#rag', '#rlhf', '#ethics', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Баланс противоречий в генерации изображений: YinYangAlign на страже точности', 'desc': 'Статья представляет YinYangAlign - передовую систему оценки для измерения точности выравнивания текстово-изобразительных (T2I) моделей. Она фокусируется на шести фундаментальных и противоречивых целях дизайна, отражающих ключевые напряжения в генерации изображений. YinYangAlign включает наборы данных с человеческими запросами, выровненными и невыровненными ответами ИИ, а также объяснениями противоречий. Система направлена на улучшение надежности и точности генерации изображений, применяя методы выравнивания, успешно используемые в больших языковых моделях.'}, 'en': {'title': 'Enhancing Image Generation Fidelity with YinYangAlign', 'desc': 'This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation.'}, 'zh': {'title': '提升文本到图像系统的对齐精度', 'desc': '本文探讨了文本到图像（T2I）系统中精确对齐的重要性，以确保生成的图像既能准确反映用户意图，又符合伦理和美学标准。研究表明，像Google Gemini这样的事件凸显了强大对齐机制的必要性。与此相比，大型语言模型（LLMs）在对齐方面取得了显著成功，研究人员希望将类似的对齐技术应用于T2I系统，以提高图像生成的准确性和可靠性。我们提出了YinYangAlign，这是一个先进的基准框架，系统地量化T2I系统的对齐保真度，解决了六个基本且内在矛盾的设计目标。'}}}, {'id': 'https://huggingface.co/papers/2502.04350', 'title': 'CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance', 'url': 'https://huggingface.co/papers/2502.04350', 'abstract': 'Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.', 'score': 4, 'issue_id': 2118, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'ad7829c09c28de41', 'authors': ['Yongchao Chen', 'Yilun Hao', 'Yueying Liu', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard University, Boston, MA, USA', 'MIT-IBM Watson AI Lab, Boston, MA, USA', 'Massachusetts Institute of Technology, Boston, MA, USA', 'University of Illinois Urbana-Champaign, Urbana, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04350.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#rlhf', '#open_source', '#optimization', '#reasoning'], 'emoji': '🧭', 'ru': {'title': 'CodeSteer: Умное управление для раскрытия потенциала LLM в символьных вычислениях', 'desc': 'CodeSteer - это новый метод для эффективного управления генерацией кода и текста в больших языковых моделях (LLM). Исследователи создали комплексный бенчмарк SymBench и синтезировали наборы данных для обучения модели. Они дообучили модель Llama-3-8B с использованием многораундового обучения с учителем и прямой оптимизации предпочтений. Результирующая модель CodeSteerLLM значительно улучшает производительность других LLM в задачах символьных вычислений, превосходя даже лучшие существующие модели.'}, 'en': {'title': 'CodeSteer: Guiding LLMs to Master Code and Reasoning!', 'desc': 'This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks.'}, 'zh': {'title': 'CodeSteer：引导LLM实现符号计算的突破', 'desc': '现有的方法无法有效引导大型语言模型（LLMs）在文本推理和代码生成之间切换，导致符号计算能力未得到充分利用。我们提出了一种名为CodeSteer的方法，能够有效指导LLM的代码和文本生成。我们构建了一个全面的基准SymBench，包含37个具有可调复杂度的符号任务，并合成了包含1.2万多轮指导/生成轨迹和5500对指导比较的数据集。通过对Llama-3-8B模型进行多轮监督微调（SFT）和直接偏好优化（DPO），我们得到的CodeSteerLLM模型能够有效引导更大模型的代码/文本生成。'}}}, {'id': 'https://huggingface.co/papers/2502.04327', 'title': 'Value-Based Deep RL Scales Predictably', 'url': 'https://huggingface.co/papers/2502.04327', 'abstract': 'Scaling data and compute is critical to the success of machine learning. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.', 'score': 3, 'issue_id': 2133, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '4b04abb62254054b', 'authors': ['Oleh Rybkin', 'Michal Nauman', 'Preston Fu', 'Charlie Snell', 'Pieter Abbeel', 'Sergey Levine', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'University of California, Berkeley', 'University of Warsaw'], 'pdf_title_img': 'assets/pdf/title_img/2502.04327.jpg', 'data': {'categories': ['#rl', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Предсказуемое масштабирование в обучении с подкреплением', 'desc': 'Статья посвящена предсказуемости масштабирования методов обучения с подкреплением вне политики, основанных на значениях. Авторы демонстрируют, что требования к данным и вычислительным ресурсам для достижения определенного уровня производительности лежат на границе Парето, контролируемой соотношением обновлений к данным (UTD). Исследователи предлагают метод оценки этой границы для прогнозирования требований к ресурсам при масштабировании. Подход валидируется на трех алгоритмах (SAC, BRO, PQL) в различных средах обучения с подкреплением.'}, 'en': {'title': 'Predictable Scaling in Reinforcement Learning', 'desc': 'This paper discusses how to effectively scale data and compute resources in machine learning, particularly in reinforcement learning (RL). It demonstrates that value-based off-policy RL methods can be predictable, contrary to common beliefs about their erratic behavior. The authors introduce a Pareto frontier that helps estimate the data and compute requirements for achieving desired performance levels. They also provide a method for optimizing resource allocation and hyperparameters to enhance performance while managing overfitting and plasticity loss in RL.'}, 'zh': {'title': '可预测的强化学习扩展方法', 'desc': '在机器学习中，扩展数据和计算能力是成功的关键。然而，扩展需要可预测性：我们希望方法不仅在更多计算或数据下表现良好，而且其性能可以从小规模实验中预测。本文展示了基于价值的离线策略强化学习方法在可预测性方面的表现，尽管社区普遍认为其行为不稳定。我们通过估计帕累托前沿，预测在给定计算或数据时所需的资源，并确定在特定预算下的最佳资源分配，以最大化性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04376', 'title': 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf', 'url': 'https://huggingface.co/papers/2502.04376', 'abstract': 'In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.', 'score': 3, 'issue_id': 2121, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '049ab6200a9d2eae', 'authors': ['Lingxiang Hu', 'Shurun Yuan', 'Xiaoting Qin', 'Jue Zhang', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'Northeastern University, China', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04376.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#agi', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LLM как виртуальные делегаты: будущее эффективных совещаний', 'desc': 'Исследователи разработали прототип системы делегирования участия в совещаниях на основе больших языковых моделей (LLM). Они создали комплексный бенчмарк, используя реальные стенограммы совещаний, для оценки эффективности различных LLM в роли делегатов. Результаты показали, что около 60% ответов моделей затрагивают как минимум один ключевой момент из эталонных данных, но требуется улучшение в снижении нерелевантного контента и повышении устойчивости к ошибкам транскрипции. Исследование подчеркивает потенциал и проблемы использования LLM в качестве делегатов на совещаниях, предлагая ценные выводы для их практического применения.'}, 'en': {'title': 'Empowering Meetings with LLMs: Balancing Engagement and Efficiency', 'desc': 'This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications.'}, 'zh': {'title': '利用大型语言模型优化会议参与', 'desc': '在现代工作场所，会议是交流思想和确保团队一致性的重要环节，但常常面临时间消耗、日程冲突和参与效率低下等挑战。本文探讨了大型语言模型（LLMs）在会议中有效分配参与者的能力，开发了一个基于LLM的会议代理系统原型，并使用真实会议记录创建了全面的基准测试。评估结果显示，GPT-4/4o在积极和谨慎的参与策略之间保持了平衡，而Gemini 1.5 Pro则更倾向于谨慎，Gemini 1.5 Flash和Llama3-8B/70B则表现出更积极的倾向。尽管约60%的回应涵盖了至少一个关键点，但仍需改进以减少无关或重复内容，并提高对真实场景中转录错误的容忍度。'}}}, {'id': 'https://huggingface.co/papers/2502.04689', 'title': 'ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning', 'url': 'https://huggingface.co/papers/2502.04689', 'abstract': 'Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.', 'score': 2, 'issue_id': 2123, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a052e3be1fe147cd', 'authors': ['Yuwei Yin', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04689.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ARR: Новый шаг в улучшении рассуждений языковых моделей', 'desc': 'Статья представляет новый метод промптинга для больших языковых моделей под названием ARR. Этот метод включает три ключевых шага: анализ намерения вопроса, извлечение релевантной информации и пошаговое рассуждение. ARR показывает лучшие результаты по сравнению с базовым подходом и методом Chain-of-Thought на различных задачах вопросно-ответного типа. Эксперименты подтверждают эффективность и обобщаемость ARR для разных размеров и типов языковых моделей.'}, 'en': {'title': 'ARR: A Structured Approach to Boost LLM Reasoning', 'desc': "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."}, 'zh': {'title': 'ARR：提升问答推理的新方法', 'desc': '本文介绍了一种新的零-shot提示方法ARR，旨在提高大型语言模型（LLMs）在多项选择问答任务中的推理能力。ARR明确包含三个关键步骤：分析问题意图、检索相关信息和逐步推理。通过在多种复杂问答任务上的实验，ARR consistently outperform了传统的基线方法和Chain-of-Thought（CoT）提示。研究表明，意图分析在ARR中起着至关重要的作用，进一步验证了每个组成部分的积极贡献。'}}}, {'id': 'https://huggingface.co/papers/2501.12387', 'title': 'Continuous 3D Perception Model with Persistent State', 'url': 'https://huggingface.co/papers/2501.12387', 'abstract': 'We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: https://cut3r.github.io/', 'score': 1, 'issue_id': 2134, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '2269a12b01fff7a4', 'authors': ['Qianqian Wang', 'Yifei Zhang', 'Aleksander Holynski', 'Alexei A. Efros', 'Angjoo Kanazawa'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2501.12387.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🏗️', 'ru': {'title': 'Универсальная 3D-реконструкция в реальном времени', 'desc': 'Авторы представляют универсальную модель CUT3R для решения широкого спектра задач 3D-реконструкции. Эта рекуррентная модель непрерывно обновляет свое внутреннее представление с каждым новым наблюдением, генерируя метрические поточечные карты для каждого входного изображения. CUT3R способна накапливать согласованную плотную реконструкцию сцены и даже предсказывать ненаблюдаемые области. Модель демонстрирует конкурентоспособную или лучшую производительность в различных задачах 3D/4D реконструкции.'}, 'en': {'title': 'CUT3R: Real-Time 3D Reconstruction with Continuous Updates', 'desc': 'This paper introduces CUT3R, a novel framework for 3D reconstruction that utilizes a stateful recurrent model to continuously update its state with new image observations. The model generates metric-scale pointmaps in real-time, allowing for the accumulation of a dense scene reconstruction as more images are processed. CUT3R is capable of inferring unseen areas of a scene by simulating views that have not been directly observed. The approach is versatile, handling both video streams and unordered photo collections, and shows strong performance across various 3D tasks.'}, 'zh': {'title': 'CUT3R：持续更新的三维重建模型', 'desc': '我们提出了一个统一的框架，能够解决广泛的三维任务。该方法采用状态递归模型，能够随着每个新观察不断更新其状态表示。通过图像流，这种不断演变的状态可以在线生成度量尺度的点图，为每个新输入生成每像素的三维点。我们的模型CUT3R不仅可以从图像观察中预测准确的点图，还可以通过探测虚拟的未观察视角推断场景中未见的区域。'}}}, {'id': 'https://huggingface.co/papers/2502.03032', 'title': 'Analyze Feature Flow to Enhance Interpretation and Steering in Language Models', 'url': 'https://huggingface.co/papers/2502.03032', 'abstract': 'We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.', 'score': 49, 'issue_id': 2090, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '030f362f419e9eb4', 'authors': ['Daniil Laptev', 'Nikita Balagansky', 'Yaroslav Aksenov', 'Daniil Gavrilov'], 'affiliations': ['1T-Tech', 'Moscow Institute of Physics and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.03032.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': '🧠', 'ru': {'title': 'Прозрачное управление языковыми моделями через межслойный анализ признаков', 'desc': 'Представлен новый подход к систематическому отображению признаков, обнаруженных разреженным автоэнкодером, между последовательными слоями больших языковых моделей. Метод использует технику косинусного сходства без данных для отслеживания эволюции признаков на каждом этапе. Это позволяет создавать подробные графы потоков развития признаков, обеспечивая детальную интерпретируемость и механистическое понимание вычислений модели. Исследование демонстрирует, как межслойные карты признаков могут использоваться для прямого управления поведением модели путем усиления или подавления выбранных признаков.'}, 'en': {'title': 'Mapping and Manipulating Features in Language Models', 'desc': 'This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.'}, 'zh': {'title': '特征映射：引导大型语言模型的新方法', 'desc': '本文提出了一种新方法，系统地映射稀疏自编码器在大型语言模型中各层发现的特征。我们使用无数据的余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。这种方法生成了特征演变的细粒度流图，增强了模型计算的可解释性和机制洞察力。我们还展示了如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成中的主题控制。'}}}, {'id': 'https://huggingface.co/papers/2502.03544', 'title': 'Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2', 'url': 'https://huggingface.co/papers/2502.03544', 'abstract': 'We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.', 'score': 28, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '93cdc6bc9f5f22d7', 'authors': ['Yuri Chervonyi', 'Trieu H. Trinh', 'Miroslav Olšák', 'Xiaomeng Yang', 'Hoang Nguyen', 'Marcelo Menegali', 'Junehyuk Jung', 'Vikas Verma', 'Quoc V. Le', 'Thang Luong'], 'affiliations': ['Brown University', 'Georgia Institute of Technology', 'Google DeepMind', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.03544.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#training', '#architecture', '#optimization', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'ИИ превосходит человека в олимпиадной геометрии', 'desc': 'AlphaGeometry2 - это улучшенная версия системы для решения олимпиадных задач по геометрии, превзошедшая средний уровень золотого медалиста. Разработчики расширили язык AlphaGeometry для решения более сложных задач, включая движение объектов и линейные уравнения с углами, отношениями и расстояниями. Система использует архитектуру Gemini для улучшенного языкового моделирования и новый механизм обмена знаниями, комбинирующий несколько деревьев поиска. AlphaGeometry2 достигла 84% успешности решения геометрических задач за последние 25 лет Международных математических олимпиад.'}, 'en': {'title': 'AlphaGeometry2: Mastering Olympiad Geometry with Advanced AI', 'desc': "AlphaGeometry2 is an advanced version of the original AlphaGeometry, designed to solve complex Olympiad geometry problems more effectively. It enhances the language capabilities to handle intricate scenarios involving object movements and linear equations related to angles and distances. The model's performance has improved significantly, increasing its problem coverage from 66% to 88% for International Math Olympiad geometry problems. With a new search process utilizing Gemini architecture and a knowledge-sharing mechanism, AlphaGeometry2 achieves an impressive solving rate of 84%, marking a substantial leap from its predecessor."}, 'zh': {'title': 'AlphaGeometry2：几何问题解决的新突破', 'desc': 'AlphaGeometry2是对AlphaGeometry的显著改进版本，能够超越平均金牌得主在解决奥林匹克几何问题上的表现。它扩展了原有的语言，能够处理更复杂的对象运动和包含角度、比例及距离的线性方程问题。通过使用Gemini架构和新颖的知识共享机制，AlphaGeometry2的搜索过程得到了显著提升，几何问题的解决率达到了84%。该系统还朝着从自然语言输入直接解决几何问题的全自动化系统迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2502.03621', 'title': 'DynVFX: Augmenting Real Videos with Dynamic Content', 'url': 'https://huggingface.co/papers/2502.03621', 'abstract': 'We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.', 'score': 23, 'issue_id': 2089, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '8c22f8cda7e633d5', 'authors': ['Danah Yatim', 'Rafail Fridman', 'Omer Bar-Tal', 'Tali Dekel'], 'affiliations': ['Pika Labs, USA', 'Weizmann Institute of Science, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.03621.jpg', 'data': {'categories': ['#inference', '#video', '#multimodal', '#synthetic', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического контента в видео по текстовым инструкциям', 'desc': 'Авторы представляют метод для дополнения реальных видео новым динамическим контентом на основе текстовых инструкций пользователя. Метод использует предобученные модели text-to-video и Vision Language Model для синтеза и интеграции нового контента в исходное видео. Предложен новый подход на основе манипуляции признаками в механизме внимания для точной локализации и бесшовной интеграции нового контента. Метод полностью автоматизирован и требует только простой инструкции от пользователя, демонстрируя эффективность на широком спектре редактирований реальных видео.'}, 'en': {'title': 'Seamless Video Augmentation with Dynamic Content', 'desc': 'This paper introduces a novel approach for enhancing real-world videos by adding dynamic content based on user instructions. The method utilizes a pre-trained text-to-video diffusion transformer to generate new objects and effects that interact naturally with the existing scene. It employs a zero-shot, training-free framework that ensures seamless integration of the new content while considering factors like camera motion and occlusions. The approach is fully automated, allowing users to simply provide text instructions to achieve realistic video augmentation.'}, 'zh': {'title': '自动化视频增强，轻松生成动态内容！', 'desc': '我们提出了一种增强现实视频的新方法，可以生成动态内容。用户只需提供简单的文本指令，我们的方法就能合成与原始场景自然互动的动态对象或复杂场景效果。新内容的位置、外观和运动与原始视频无缝结合，同时考虑了相机运动、遮挡和其他动态对象的互动。该方法采用零-shot、无训练的框架，利用预训练的文本到视频扩散变换器和视觉语言模型，实现了自动化的内容合成。'}}}, {'id': 'https://huggingface.co/papers/2502.04320', 'title': 'ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features', 'url': 'https://huggingface.co/papers/2502.04320', 'abstract': 'Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.', 'score': 19, 'issue_id': 2101, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '043b9ff9f5eaf227', 'authors': ['Alec Helbling', 'Tuna Han Salih Meral', 'Ben Hoover', 'Pinar Yanardag', 'Duen Horng Chau'], 'affiliations': ['Georgia Tech', 'IBM Research', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.04320.jpg', 'data': {'categories': ['#interpretability', '#transfer_learning', '#dataset', '#multimodal', '#cv', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ConceptAttention: новый взгляд на интерпретируемость мультимодальных моделей', 'desc': 'Статья представляет новый метод ConceptAttention, который использует возможности слоев внимания мультимодальных диффузионных трансформеров (DiT) для создания высококачественных карт важности. Этот метод позволяет точно локализовать текстовые концепции на изображениях без дополнительного обучения. ConceptAttention превосходит существующие методы в задаче сегментации изображений с нулевым обучением на наборах данных ImageNet-Segmentation и PascalVOC. Исследование показывает, что представления моделей DiT, таких как Flux, хорошо переносятся на задачи компьютерного зрения, превосходя даже мультимодальные базовые модели вроде CLIP.'}, 'en': {'title': 'Enhancing Interpretability with ConceptAttention in Multi-Modal Transformers', 'desc': 'This paper explores the unique properties of multi-modal diffusion transformers (DiTs) and their interpretability through a new method called ConceptAttention. ConceptAttention utilizes the attention layers of DiTs to create detailed saliency maps that accurately identify textual concepts in images without needing extra training. The study reveals that linear projections in the output space of DiT attention layers lead to sharper saliency maps compared to traditional cross-attention methods. Additionally, ConceptAttention demonstrates superior performance in zero-shot image segmentation tasks, surpassing other interpretability techniques and showing the strong transferability of DiT representations to vision applications.'}, 'zh': {'title': '多模态扩散变换器的可解释性新突破', 'desc': '本文探讨了多模态扩散变换器（DiTs）在可解释性方面的独特性质。我们提出了一种新方法ConceptAttention，利用DiT注意力层的表达能力生成高质量的显著性图，准确定位图像中的文本概念。通过对DiT注意力层输出空间进行线性投影，ConceptAttention生成的显著性图比常用的交叉注意力机制更清晰。我们的研究首次证明了多模态DiT模型的表示在视觉任务（如分割）中具有高度可转移性，甚至超越了像CLIP这样的多模态基础模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04153', 'title': 'UltraIF: Advancing Instruction Following from the Wild', 'url': 'https://huggingface.co/papers/2502.04153', 'abstract': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.', 'score': 19, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c7c902f1effa8a3', 'authors': ['Kaikai An', 'Li Sheng', 'Ganqu Cui', 'Shuzheng Si', 'Ning Ding', 'Yu Cheng', 'Baobao Chang'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04153.jpg', 'data': {'categories': ['#open_source', '#training', '#benchmark', '#alignment', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'UltraIF: простой метод для обучения LLM следовать сложным инструкциям', 'desc': 'Исследователи представили подход UltraIF для улучшения способности больших языковых моделей (LLM) следовать сложным инструкциям. Метод разбивает пользовательские запросы на более простые компоненты и использует специальную модель UltraComposer для составления сложных инструкций с вопросами для оценки. Эксперименты показали, что UltraIF позволяет значительно улучшить следование инструкциям у базовой модели LLaMA-3.1-8B без использования специальных данных. Подход также продемонстрировал возможность дальнейшего улучшения версии модели, уже обученной следовать инструкциям.'}, 'en': {'title': 'UltraIF: Simplifying Complex Instructions for LLMs', 'desc': 'This paper introduces UltraIF, a method designed to enhance large language models (LLMs) in following complex instructions using open-source data. The approach involves breaking down user prompts into simpler components, such as queries and constraints, and then training a model called UltraComposer to generate these structured prompts. By synthesizing complicated instructions and evaluating responses, UltraIF successfully aligns the LLaMA-3.1-8B-Base model with its instruct version on multiple benchmarks without prior benchmark data. Additionally, the method shows potential for improving existing instruct models through self-alignment, expanding its applicability in various scenarios.'}, 'zh': {'title': 'UltraIF：让大型语言模型更聪明的秘密武器', 'desc': '本文提出了一种名为UltraIF的方法，用于提高大型语言模型（LLMs）对复杂指令的理解能力。该方法通过将用户的真实请求分解为更简单的查询、约束和相应的评估问题来实现。接着，训练一个名为UltraComposer的模型，能够生成与约束相关的提示，并结合评估问题来过滤响应。实验表明，UltraIF成功地使LLaMA-3.1-8B-Base在多个指令跟随基准上与其指令版本对齐，展示了该方法的有效性和广泛应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.04313', 'title': 'Great Models Think Alike and this Undermines AI Oversight', 'url': 'https://huggingface.co/papers/2502.04313', 'abstract': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.', 'score': 18, 'issue_id': 2091, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a44111741eb33c43', 'authors': ['Shashwat Goel', 'Joschka Struber', 'Ilze Amanda Auzina', 'Karuna K Chandra', 'Ponnurangam Kumaraguru', 'Douwe Kiela', 'Ameya Prabhu', 'Matthias Bethge', 'Jonas Geiping'], 'affiliations': ['Contextual AI', 'ELLIS Institute Tubingen', 'IIIT Hyderabad', 'Max Planck Institute for Intelligent Systems', 'Stanford University', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.04313.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Сходство языковых моделей: проблемы и риски AI-надзора', 'desc': "В статье рассматривается проблема оценки и контроля языковых моделей (ЯМ) с помощью других ЯМ, что авторы называют 'AI Oversight'. Исследуется влияние сходства моделей на эффективность такого надзора с использованием вероятностной метрики, основанной на пересечении ошибок. Обнаружено, что оценки ЯМ-судей смещены в пользу похожих моделей, а при обучении на аннотациях ЯМ важную роль играет дополняющее знание. Авторы отмечают тревожную тенденцию: с ростом возможностей ЯМ их ошибки становятся более схожими, что указывает на риски коррелированных сбоев."}, 'en': {'title': 'Navigating AI Oversight: Understanding Model Similarity and Its Risks', 'desc': 'This paper explores the challenges of evaluating and supervising advanced language models (LMs) as their capabilities grow. It introduces a probabilistic metric to measure LM similarity based on the overlap in their mistakes, which aids in understanding AI oversight. The study reveals that when using one LM to evaluate another, models that are similar tend to score each other favorably, indicating a potential bias. Additionally, it highlights the risks of correlated failures among models as they become more capable, emphasizing the need for careful reporting and correction of model similarities in AI oversight practices.'}, 'zh': {'title': 'AI监督：模型相似性的重要性', 'desc': '随着语言模型（LM）能力的提升，人类对其进行评估和监督变得越来越困难。我们提出了一种基于模型错误重叠的概率度量来研究模型相似性对AI监督的影响。研究表明，作为评判者的语言模型更倾向于偏好与其相似的模型，这与最近的自我偏好结果一致。此外，弱监督者与强学生模型之间的互补知识在“弱到强的泛化”中起着关键作用。'}}}, {'id': 'https://huggingface.co/papers/2502.04328', 'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment', 'url': 'https://huggingface.co/papers/2502.04328', 'abstract': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.', 'score': 18, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0e25e42e93e9e560', 'authors': ['Zuyan Liu', 'Yuhao Dong', 'Jiahui Wang', 'Ziwei Liu', 'Winston Hu', 'Jiwen Lu', 'Yongming Rao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.04328.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#training', '#multimodal'], 'emoji': '🦉', 'ru': {'title': 'Ola: Прогрессивное обучение для создания мощной омнимодальной модели', 'desc': 'В статье представлена Ola - омнимодальная языковая модель, способная понимать изображения, видео и аудио на уровне специализированных моделей. Ключевая особенность Ola - стратегия прогрессивного выравнивания модальностей, начинающаяся с изображений и текста, затем расширяющаяся на речь и видео. Модель использует небольшой объем кросс-модальных данных для обучения, что делает ее разработку менее затратной. Эксперименты показывают, что Ola превосходит существующие открытые омнимодальные модели по всем модальностям.'}, 'en': {'title': 'Ola: Bridging Modalities for Superior Understanding', 'desc': "This paper introduces Ola, an omni-modal language model designed to understand and process multiple types of data, including images, videos, and audio. Ola employs a progressive modality alignment strategy, starting with image and text, and gradually incorporating speech and video data to enhance its capabilities. The model's training pipeline is efficient, allowing it to maintain a smaller dataset for cross-modal alignment, making it easier and more cost-effective to develop. Experimental results show that Ola outperforms existing open omni-modal models and competes well with specialized models, aiming to contribute to future research in omni-modal understanding."}, 'zh': {'title': 'Ola：全模态理解的新突破', 'desc': '本文介绍了一种名为Ola的全模态语言模型，能够在图像、视频和音频理解方面与专门的单模态模型竞争。Ola的核心设计是逐步模态对齐策略，首先从图像和文本开始训练，然后逐步引入语音和视频数据。这样的训练流程使得跨模态对齐数据的规模相对较小，降低了开发全模态模型的成本。通过广泛的实验，Ola在所有模态上超越了现有的开放全模态语言模型，并在与同类专门模型的竞争中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.00473', 'title': 'Weak-to-Strong Diffusion with Reflection', 'url': 'https://huggingface.co/papers/2502.00473', 'abstract': 'The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.', 'score': 17, 'issue_id': 2095, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '82d22d57d66f1a7a', 'authors': ['Lichen Bai', 'Masashi Sugiyama', 'Zeke Xie'], 'affiliations': ['RIKEN AIP', 'The University of Tokyo', 'xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.00473.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#optimization', '#dataset', '#diffusion', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'W2SD: Преодоление разрыва между генеративными моделями и реальными данными', 'desc': 'Статья представляет новый фреймворк под названием Weak-to-Strong Diffusion (W2SD) для улучшения диффузионных генеративных моделей. W2SD использует разницу между слабыми и сильными моделями для аппроксимации разрыва между идеальной и сильной моделью. Метод применяет рефлексивную операцию, чередующую денойзинг и инверсию с учетом разницы weak-to-strong, что теоретически направляет латентные переменные к реальному распределению данных. Эксперименты показывают, что W2SD значительно улучшает предпочтения пользователей, эстетическое качество и соответствие промпту, достигая SOTA результатов в различных модальностях и архитектурах.'}, 'en': {'title': 'Bridging the Gap: Weak-to-Strong Diffusion for Enhanced Generative Models', 'desc': "This paper introduces Weak-to-Strong Diffusion (W2SD), a new framework designed to enhance diffusion generative models by addressing the gap between generated outputs and real data. W2SD leverages the differences between weak and strong models to better approximate the ideal model's performance. By alternating between denoising and inversion processes, it guides latent variables towards areas that closely resemble the real data distribution. The framework shows significant improvements in various applications, achieving state-of-the-art results while maintaining efficiency in computational resources."}, 'zh': {'title': '弱到强扩散：缩小生成与真实数据的差距', 'desc': '扩散生成模型的目标是通过梯度评分匹配将学习到的分布与真实数据分布对齐。然而，训练数据质量、建模策略和架构设计的固有限制导致生成输出与真实数据之间存在不可避免的差距。为了解决这个问题，我们提出了弱到强扩散（W2SD）框架，利用现有弱模型和强模型之间的差异来近似理想模型与强模型之间的差距。W2SD通过反射操作在去噪和反演之间交替，理论上引导潜在变量沿着采样轨迹朝向真实数据分布的区域，从而显著提高生成结果的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.04235', 'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion', 'url': 'https://huggingface.co/papers/2502.04235', 'abstract': "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive Genre-Audience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.", 'score': 16, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b1c2fec586443af8', 'authors': ['Xintong Hao', 'Ke Shen', 'Chenggang Li'], 'affiliations': ['Seed-LLM, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.04235.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#training', '#synthetic'], 'emoji': '🚀', 'ru': {'title': 'MAGA: преодоление ограничений данных для масштабирования языковых моделей', 'desc': 'Статья представляет метод MAGA для синтеза разнообразных и контекстуально богатых данных для предобучения языковых моделей. Авторы создали корпус MAGACorpus объемом 770 миллиардов токенов и продемонстрировали улучшение результатов для моделей различных размеров. Исследование также анализирует влияние инженерии промптов на синтетический коллапс обучения. Работа показывает, что MAGA может значительно расширить наборы данных для обучения, сохраняя их качество.'}, 'en': {'title': 'Expanding Language Models with MAGA: A Path Beyond Data Limitations', 'desc': 'This paper addresses the challenge of limited high-quality pretraining data for large language models. It introduces the MAssive Genre-Audience (MAGA) reformulation method, which creates diverse and contextually-rich pretraining data from existing sources. The authors present a new dataset called MAGACorpus, containing 770 billion tokens, and demonstrate its effectiveness across various model sizes. Additionally, they analyze the effects of prompt engineering on training stability and highlight the shortcomings of traditional metrics for detecting training collapse.'}, 'zh': {'title': 'MAGA：突破数据限制的预训练新方法', 'desc': '尽管大型语言模型在各种任务中表现出色，但它们在扩展时面临高质量预训练数据稀缺的挑战。为了解决这个瓶颈，我们提出了MAssive Genre-Audience（MAGA）重构方法，该方法系统地从现有语料库中合成多样化且富有上下文的预训练数据。我们的研究贡献包括提出了一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含7700亿个标记的MAGACorpus。通过对不同数据预算扩展策略的评估，我们证明了在各种模型规模下（134M-13B）的一致性改进，展示了下一代大规模合成预训练语言模型的必要性。'}}}, {'id': 'https://huggingface.co/papers/2502.02358', 'title': 'MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm', 'url': 'https://huggingface.co/papers/2502.02358', 'abstract': 'Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.', 'score': 14, 'issue_id': 2088, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '967ac00db9aae918', 'authors': ['Ziyan Guo', 'Zeyu Hu', 'Na Zhao', 'De Wen Soh'], 'affiliations': ['LightSpeed Studios, Singapore', 'Singapore University of Technology and Design, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.02358.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Универсальный подход к генерации и редактированию движений человека', 'desc': 'Статья представляет новый подход к генерации и редактированию движений человека в компьютерной графике и компьютерном зрении. Авторы предлагают парадигму Motion-Condition-Motion и унифицированный фреймворк MotionLab, использующий исправленные потоки для отображения исходного движения в целевое. MotionLab включает в себя MotionFlow Transformer, выровненное позиционное кодирование вращения, модуляцию инструкций для конкретных задач и учебную программу движения для эффективного обучения. Фреймворк демонстрирует многообещающие возможности обобщения и эффективность вывода на нескольких эталонных тестах для движений человека.'}, 'en': {'title': 'Unifying Human Motion Generation and Editing with MotionLab', 'desc': 'This paper presents a new approach to human motion generation and editing called MotionLab, which aims to unify various motion-related tasks into a single framework. The proposed paradigm, Motion-Condition-Motion, allows for the integration of source motion, conditions, and target motion, facilitating better control and editing capabilities. MotionLab employs advanced techniques like the MotionFlow Transformer and Aligned Rotational Position Encoding to enhance the generation process and ensure synchronization between motions. Overall, this framework shows improved efficiency and generalization across different benchmarks, making it a versatile tool for real-world applications in computer graphics and vision.'}, 'zh': {'title': '统一人类运动生成与编辑的创新框架', 'desc': '人类运动生成和编辑是计算机图形学和视觉的重要组成部分。现有的方法往往针对特定任务提供孤立的解决方案，效率低下且不适用于实际应用。为了解决这些问题，我们提出了一种新的范式：运动条件运动（Motion-Condition-Motion），它通过源运动、条件和目标运动三个概念统一处理多种任务。基于这一范式，我们开发了MotionLab框架，能够有效地进行人类运动的生成和编辑，并在多个基准测试中展示了良好的泛化能力和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03860', 'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation', 'url': 'https://huggingface.co/papers/2502.03860', 'abstract': "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.", 'score': 14, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b861ba86ae27e974', 'authors': ['Bo Pang', 'Hanze Dong', 'Jiacheng Xu', 'Silvio Savarese', 'Yingbo Zhou', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.03860.jpg', 'data': {'categories': ['#training', '#benchmark', '#math', '#long_context', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ сложным рассуждениям без подсказок', 'desc': 'Эта статья представляет новый подход к обучению больших языковых моделей (LLM) способности генерировать длинные цепочки рассуждений (LongCoT) без использования дистилляции знаний от существующих моделей. Метод BOLT включает три этапа: начальную генерацию данных LongCoT с помощью обучения в контексте, супервизорную донастройку и онлайн-обучение для дальнейшего улучшения способностей. Авторы применили свой метод к моделям различных масштабов и достигли впечатляющих результатов на ряде бенчмарков, оценивающих разнообразные способности решения задач и рассуждений. Этот подход позволяет развивать способности LLM к сложным рассуждениям без необходимости в дорогостоящих аннотациях или данных от существующих продвинутых моделей.'}, 'en': {'title': 'Empowering LLMs with Efficient Long Chain-of-Thought Bootstrapping', 'desc': "This paper presents a new method called Bootstrapping Long Chain-of-Thought (BOLT) to enhance the reasoning abilities of large language models (LLMs) without relying on knowledge distillation from existing models. BOLT consists of three stages: bootstrapping LongCoT data using in-context learning, supervised fine-tuning for LongCoT, and online training for further refinement. The approach requires only a few examples to start the bootstrapping process, making it efficient and scalable across different model sizes. Experimental results show that BOLT significantly improves performance on various reasoning and problem-solving benchmarks, demonstrating its effectiveness in developing LLMs' LongCoT capabilities."}, 'zh': {'title': '引导长链思维，提升推理能力！', 'desc': '本文介绍了一种新方法，旨在使大型语言模型（LLM）具备长链思维（LongCoT）能力，而无需依赖于类似o1模型的知识蒸馏或昂贵的人类标注。该方法称为BOLT，分为三个阶段：首先通过上下文学习从标准指令模型引导LongCoT数据，其次进行LongCoT的监督微调，最后进行在线训练以进一步提升LongCoT能力。实验中，我们仅构建了10个示例，证明了该方法的可行性。我们在多个基准测试上取得了显著的性能，展示了该方法在解决复杂问题和推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04306', 'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04306', 'abstract': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow', 'score': 13, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '51ace85d35c202d5', 'authors': ['Yinjie Wang', 'Ling Yang', 'Guohao Li', 'Mengdi Wang', 'Bryon Aragam'], 'affiliations': ['Princeton University', 'University of Chicago', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2502.04306.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#agents', '#rlhf', '#small_models'], 'emoji': '🚀', 'ru': {'title': 'ScoreFlow: эффективная оптимизация мультиагентных систем на основе ИИ', 'desc': 'В статье представлен новый фреймворк ScoreFlow для оптимизации рабочих процессов мультиагентных систем на основе больших языковых моделей. ScoreFlow использует градиентную оптимизацию в непрерывном пространстве, что позволяет преодолеть ограничения существующих методов. Ключевым компонентом является Score-DPO - новый вариант метода прямой оптимизации предпочтений, учитывающий количественную обратную связь. На шести тестовых задачах ScoreFlow показал улучшение результатов на 8.2% по сравнению с базовыми методами.'}, 'en': {'title': 'ScoreFlow: Optimizing Multi-Agent Systems with Continuous Gradient Techniques', 'desc': 'This paper introduces ScoreFlow, a new framework designed to enhance the performance of multi-agent systems in solving complex problems. It addresses the limitations of existing optimization methods by utilizing gradient-based optimization in a continuous space, which allows for greater flexibility and scalability. ScoreFlow features Score-DPO, a novel approach that incorporates quantitative feedback into the optimization process. The results show that ScoreFlow not only improves performance by 8.2% over previous methods but also enables smaller models to achieve better results than larger models at a lower cost.'}, 'zh': {'title': 'ScoreFlow：高效的多智能体优化框架', 'desc': '最近的研究利用大型语言模型的多智能体系统来解决复杂问题，同时努力减少构建这些系统所需的手动工作。现有方法由于表示限制、缺乏适应性和依赖离散优化技术而导致灵活性不足。我们提出了ScoreFlow，这是一个简单但高性能的框架，利用基于梯度的优化在连续空间中进行优化。ScoreFlow在六个基准测试中表现出色，超越了现有基线，并使较小的模型以更低的推理成本超越较大的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04128', 'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.04128', 'abstract': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.', 'score': 12, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '196e7cb6a29a44ea', 'authors': ['Zhen Ye', 'Xinfa Zhu', 'Chi-Min Chan', 'Xinsheng Wang', 'Xu Tan', 'Jiahe Lei', 'Yi Peng', 'Haohe Liu', 'Yizhu Jin', 'Zheqi DAI', 'Hongzhan Lin', 'Jianyi Chen', 'Xingjian Du', 'Liumeng Xue', 'Yunlin Chen', 'Zhifei Li', 'Lei Xie', 'Qiuqiang Kong', 'Yike Guo', 'Wei Xue'], 'affiliations': ['ASLP Lab, Northwestern Polytechnical University', 'Chinese University of Hong Kong', 'Hong Kong Baptist University', 'Independent Researcher', 'Shanghai Mobvoi Information Technology Co., Ltd.', 'The Hong Kong University of Science and Technology', 'University of Rochester', 'University of Science and Technology Beijing', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2502.04128.jpg', 'data': {'categories': ['#open_source', '#dataset', '#audio', '#training', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Llasa: масштабируемый синтез речи на основе единой языковой модели', 'desc': 'В статье представлена новая система синтеза речи Llasa, использующая одноуровневый векторный квантователь и единую архитектуру трансформера, аналогичную стандартным языковым моделям. Исследование показывает, что увеличение вычислительных ресурсов при обучении улучшает естественность синтезированной речи и позволяет генерировать более сложные просодические паттерны. Масштабирование вычислений во время вывода с использованием моделей понимания речи в качестве верификаторов улучшает эмоциональную выразительность, согласованность тембра и точность содержания. Авторы опубликовали контрольные точки и код обучения для своей модели синтеза речи.'}, 'en': {'title': 'Simplifying Speech Synthesis with Scalable LLMs', 'desc': 'This paper discusses advancements in text-to-speech (TTS) systems that utilize large language models (LLMs) like GPT and o1. It introduces Llasa, a new framework that simplifies speech synthesis by using a single-layer vector quantizer and a Transformer architecture, aligning it with standard LLMs. The research shows that increasing training compute enhances the naturalness and complexity of the generated speech. Additionally, it highlights how scaling inference compute can improve emotional expressiveness and accuracy by using speech understanding models as verifiers during the synthesis process.'}, 'zh': {'title': '简化语音合成，提升自然性与情感表达', 'desc': '本文探讨了文本基础的大型语言模型（LLMs）在语音合成中的应用，特别是GPT系列和o1模型的最新进展。我们提出了一种名为Llasa的简单框架，使用单层向量量化（VQ）编解码器和单一Transformer架构，旨在简化多阶段的语音合成过程。实验结果表明，增加训练时间的计算资源可以显著提高合成语音的自然性，并生成更复杂的韵律模式。此外，我们还发现，在推理时间增加计算资源可以改善情感表现、音色一致性和内容准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.04299', 'title': 'MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.04299', 'abstract': 'This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.', 'score': 10, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '91b39568cf3793e2', 'authors': ['Jinbo Xing', 'Long Mai', 'Cusuh Ham', 'Jiahui Huang', 'Aniruddha Mahapatra', 'Chi-Wing Fu', 'Tien-Tsin Wong', 'Feng Liu'], 'affiliations': ['Adobe Research', 'Monash University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04299.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion', '#3d', '#games'], 'emoji': '🎬', 'ru': {'title': 'MotionCanvas: Интуитивное проектирование видеокадров с контролем движения', 'desc': 'Эта статья представляет метод MotionCanvas, позволяющий пользователям проектировать кинематографические видеокадры в контексте генерации видео из изображений. Метод интегрирует пользовательское управление в модели I2V, позволяя контролировать движения объектов и камеры с учетом сцены. MotionCanvas соединяет классическую компьютерную графику и современные методы генерации видео, обеспечивая 3D-осведомленное управление движением без необходимости в дорогостоящих 3D-данных для обучения. Метод позволяет интуитивно описывать намерения движения в пространстве сцены и преобразовывать их в пространственно-временные сигналы для обусловливания движения в диффузионных моделях видео.'}, 'en': {'title': 'Empowering Cinematic Creativity with MotionCanvas', 'desc': 'This paper introduces MotionCanvas, a novel method for designing cinematic video shots in image-to-video generation. It addresses two main challenges: capturing user intentions for both camera and object motions, and effectively representing this motion information for video diffusion models. By integrating user-driven controls, MotionCanvas allows for intuitive scene-aware motion design without the need for expensive 3D training data. The method enhances creative workflows in digital content creation, making it easier for users to specify and visualize their motion intentions in video synthesis.'}, 'zh': {'title': '直观设计电影镜头，提升视频生成体验', 'desc': '本文提出了一种方法，使用户能够在图像到视频生成的背景下设计电影镜头。镜头设计是电影制作中的关键环节，涉及到对相机运动和场景中物体运动的精心规划。我们介绍了MotionCanvas，这是一种将用户驱动的控制集成到图像到视频生成模型中的方法，允许用户以场景感知的方式控制物体和相机的运动。通过将经典计算机图形学的见解与现代视频生成技术相结合，我们展示了在不需要昂贵的3D训练数据的情况下，实现I2V合成中的3D感知运动控制的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04270', 'title': 'PILAF: Optimal Human Preference Sampling for Reward Modeling', 'url': 'https://huggingface.co/papers/2502.04270', 'abstract': 'As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.', 'score': 7, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c0dd6afed760a2b', 'authors': ['Yunzhen Feng', 'Ariel Kwiatkowski', 'Kunhao Zheng', 'Julia Kempe', 'Yaqi Duan'], 'affiliations': ['Meta FAIR', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2502.04270.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'PILAF: точное согласование ИИ с человеческими ценностями', 'desc': 'Статья представляет новый метод обучения языковых моделей с учетом человеческих ценностей - PILAF (Policy-Interpolated Learning for Aligned Feedback). Этот подход улучшает существующую технику обучения с подкреплением на основе обратной связи от человека (RLHF). PILAF предлагает стратегию выборки ответов для маркировки предпочтений, которая явно согласует обучение предпочтениям с максимизацией базовой награды. Метод демонстрирует сильную производительность в итеративных и онлайн-настройках RLHF, где критически важна курация обратной связи.'}, 'en': {'title': 'Aligning AI with Human Values through PILAF', 'desc': 'This paper introduces Policy-Interpolated Learning for Aligned Feedback (PILAF), a new method for improving Reinforcement Learning from Human Feedback (RLHF). PILAF focuses on aligning preference learning with the true human values by using a response sampling strategy that enhances reward model accuracy. The approach is theoretically sound, showing optimal performance in both optimization and statistical contexts. It is easy to implement and performs well in scenarios where feedback curation is essential, making it a valuable tool for aligning large language models with human values.'}, 'zh': {'title': '政策插值学习：对齐人类反馈的新策略', 'desc': '随着大型语言模型在实际应用中的广泛使用，使其与人类价值观保持一致变得至关重要。强化学习与人类反馈（RLHF）成为了一种关键技术，它将偏好数据转化为奖励模型，尤其是在无法获取人类价值观的情况下。我们提出了一种新的响应采样策略，称为政策插值学习（PILAF），它明确将偏好学习与最大化基础奖励对齐。PILAF在理论上是有依据的，从优化和统计的角度都展示了其最优性，并且在反馈策划至关重要的迭代和在线RLHF环境中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.04295', 'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'url': 'https://huggingface.co/papers/2502.04295', 'abstract': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at https://github.com/HenryLau7/CFPO.', 'score': 7, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'f72f46ad2c1b9853', 'authors': ['Yuanye Liu', 'Jiahang Xu', 'Li Lyna Zhang', 'Qi Chen', 'Xuan Feng', 'Yang Chen', 'Zhongxin Guo', 'Yuqing Yang', 'Cheng Peng'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04295.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Интегрированная оптимизация формы и содержания промптов повышает эффективность LLM', 'desc': 'Статья представляет новую методологию оптимизации промптов для больших языковых моделей (LLM), называемую CFPO. Этот подход оптимизирует как содержание, так и форматирование промптов через итеративный процесс уточнения. CFPO использует мутации естественного языка для исследования вариаций содержания и применяет динамическую стратегию исследования форматов. Результаты показывают значительное улучшение производительности по сравнению с методами оптимизации, ориентированными только на содержание.'}, 'en': {'title': 'Enhancing LLMs with Integrated Content and Format Optimization', 'desc': 'This paper presents a new method called Content-Format Integrated Prompt Optimization (CFPO) that improves the performance of Large Language Models (LLMs) by optimizing both the content and formatting of prompts. The authors argue that while prompt content has been the focus of recent research, the formatting aspect is equally important and has not been thoroughly explored. CFPO uses natural language mutations to create variations in content and a dynamic strategy to test different formatting options. The results show that this integrated approach leads to better performance in various tasks compared to methods that only optimize content.'}, 'zh': {'title': '内容与格式的完美结合，提升LLM性能！', 'desc': '大型语言模型（LLMs）在各种任务中表现出色，其实际效果往往依赖于提示设计。尽管最近的研究主要集中在优化提示内容上，但提示格式的作用却被忽视，缺乏系统性的研究。本文提出了一种新的方法——内容格式集成提示优化（CFPO），通过迭代优化过程同时优化提示内容和格式。我们的评估表明，CFPO在多个任务和开源LLMs上相较于仅优化内容的方法，显示出显著的性能提升，强调了内容与格式集成优化的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.00989', 'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution', 'url': 'https://huggingface.co/papers/2502.00989', 'abstract': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.', 'score': 6, 'issue_id': 2092, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '4d26f9419f20aadb', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['ACM, New York, NY, USA', 'IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00989.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#agents', '#cv', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'Точные ответы по графикам с доказательствами от ИИ', 'desc': 'ChartCitor - это мультиагентная система для ответов на вопросы по графикам с использованием больших языковых моделей (LLM). Она решает проблему галлюцинаций и необоснованных ответов путем предоставления точных ссылок на области изображения графика. Система включает в себя извлечение данных из графика в таблицу, переформулировку вопроса, дополнение таблицы, поиск доказательств и сопоставление таблицы с графиком. ChartCitor превосходит существующие методы и повышает доверие пользователей к генеративному ИИ.'}, 'en': {'title': 'ChartCitor: Enhancing Trust in AI with Accurate Chart Question-Answering', 'desc': "This paper introduces ChartCitor, a multi-agent framework designed to improve the accuracy of question-answering tasks involving charts by addressing the issue of hallucinated responses from Large Language Models (LLMs). It enhances answer attribution by providing precise bounding box citations that link responses to specific parts of chart images, overcoming challenges related to visual-semantic context and complex layouts. The framework includes processes for chart-to-table extraction, answer reformulation, and evidence retrieval, which collectively improve the reliability of the generated answers. User studies indicate that ChartCitor not only boosts the performance of LLMs in chart QA tasks but also increases user trust and productivity by offering clearer explanations of the AI's reasoning."}, 'zh': {'title': 'ChartCitor：提升图表问答的可信度与效率', 'desc': '大型语言模型（LLMs）可以执行图表问答任务，但常常生成未经验证的虚假回答。现有的答案归属方法由于视觉语义上下文有限、复杂的视觉文本对齐要求以及在复杂布局中进行边界框预测的困难，难以将回答与源图表关联。我们提出了ChartCitor，一个多代理框架，通过识别图表图像中的支持证据来提供细粒度的边界框引用。ChartCitor在不同图表类型上超越了现有基准，增强了用户对生成式人工智能的信任，并提高了专业人士的工作效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03639', 'title': 'Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach', 'url': 'https://huggingface.co/papers/2502.03639', 'abstract': 'We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.', 'score': 6, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '7875c341693f4d1e', 'authors': ['Yunuo Chen', 'Junli Cao', 'Anil Kag', 'Vidit Goel', 'Sergei Korolev', 'Chenfanfu Jiang', 'Sergey Tulyakov', 'Jian Ren'], 'affiliations': ['Snap Inc.', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2502.03639.jpg', 'data': {'categories': ['#diffusion', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': '3D-осведомленная генерация видео с улучшенной геометрией и динамикой', 'desc': 'Представлена новая система генерации видео, объединяющая трехмерную геометрию и динамическое восприятие. Двумерные видео дополняются трехмерными траекториями точек и выравниваются в пиксельном пространстве. Полученный набор данных PointVid используется для дообучения модели латентной диффузии, что позволяет отслеживать 2D объекты с помощью 3D координат. Регуляризация формы и движения объектов улучшает качество генерируемых RGB видео и устраняет нежелательные артефакты.'}, 'en': {'title': 'Enhancing Video Generation with 3D Awareness', 'desc': 'This paper introduces a new framework for generating videos that combines 3D geometry with an understanding of dynamic movements. It creates a dataset called PointVid by enhancing 2D videos with 3D point trajectories, which helps the model learn to track objects in three dimensions. The framework uses a latent diffusion model that is fine-tuned to ensure that the shapes and motions of objects are realistic, reducing issues like nonphysical deformation. By focusing on 3D consistency, the model improves the quality of generated videos, especially in scenarios where objects interact closely, such as in task-oriented videos.'}, 'zh': {'title': '三维感知，提升视频生成质量', 'desc': '我们提出了一种新的视频生成框架，结合了三维几何和动态感知。通过在像素空间中对齐三维点轨迹，我们增强了二维视频，创建了一个三维感知的视频数据集PointVid。利用这个数据集，我们对潜在扩散模型进行微调，使其能够跟踪具有三维坐标的二维物体。我们的模型通过正则化物体的形状和运动，消除了不必要的伪影，提高了生成RGB视频的质量，特别是在处理复杂的接触场景时。'}}}, {'id': 'https://huggingface.co/papers/2502.00988', 'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback', 'url': 'https://huggingface.co/papers/2502.00988', 'abstract': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.', 'score': 5, 'issue_id': 2094, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9548b306478edf6d', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00988.jpg', 'data': {'categories': ['#cv', '#agents', '#science', '#dataset', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'PlotGen: ИИ-помощник для создания точных научных визуализаций', 'desc': 'Статья представляет PlotGen - новую мультиагентную систему для автоматического создания научных визуализаций данных. Система использует несколько агентов на основе больших языковых моделей (LLM) для планирования, генерации кода и итеративного улучшения графиков. PlotGen включает агентов для разбиения сложных запросов, генерации Python-кода и многомодальной обратной связи для уточнения точности данных, текстовых меток и визуальной корректности. Эксперименты показывают, что PlotGen превосходит базовые методы на 4-6% на датасете MatPlotBench, повышая доверие пользователей к визуализациям, созданным с помощью LLM.'}, 'en': {'title': 'Automating Scientific Visualization with PlotGen', 'desc': 'This paper introduces PlotGen, a multi-agent framework designed to automate the creation of accurate scientific visualizations. It utilizes several Large Language Model (LLM)-based agents to streamline the process, including a Query Planning Agent for breaking down user requests and a Code Generation Agent for converting pseudocode into Python code. Additionally, it employs retrieval feedback agents that enhance the quality of visualizations by refining data accuracy and visual elements through iterative self-reflection. Experimental results demonstrate that PlotGen significantly improves visualization accuracy and reduces debugging time, thereby increasing user trust and productivity for novice users.'}, 'zh': {'title': '自动化科学可视化的未来', 'desc': '科学数据可视化对于将原始数据转化为易于理解的视觉表示至关重要，能够帮助识别模式和预测结果。新手用户在选择合适工具和掌握可视化技术时常常面临困难。本文提出了一种名为PlotGen的新型多代理框架，旨在自动化创建精确的科学可视化。通过多个基于大语言模型的代理，PlotGen能够有效地分解用户请求并生成高质量的可视化图表。'}}}, {'id': 'https://huggingface.co/papers/2501.19085', 'title': 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet', 'url': 'https://huggingface.co/papers/2501.19085', 'abstract': "The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.", 'score': 4, 'issue_id': 2094, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8c0ab784750b1038', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['Software Institute USI Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2501.19085.jpg', 'data': {'categories': ['#training', '#low_resource', '#plp', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Повышение эффективности языковых моделей для малоресурсных языков программирования', 'desc': 'Исследование посвящено улучшению генерации кода языковыми моделями (ЯМ) для малоресурсных языков программирования. Авторы сравнивают эффективность различных подходов, включая дообучение, обучение в контексте и предобучение на задаче перевода. Результаты показывают, что для небольших ЯМ лучше всего работает дообучение, а для крупных - обучение в контексте. Очень большие ЯМ могут ухудшать производительность при дообучении из-за недостатка данных.'}, 'en': {'title': 'Boosting Code Generation for Low-Resource Languages with LLMs', 'desc': 'This paper explores how Large Language Models (LLMs) can be improved for generating code in low-resource programming languages, which have limited training data. It examines various methods such as fine-tuning, in-context learning, and a pre-training objective that translates between high- and low-resource languages. The study finds that smaller LLMs benefit most from fine-tuning, while larger models perform better with in-context learning due to their architecture. However, fine-tuning large LLMs can lead to worse performance on low-resource languages because they require more data to adjust their parameters effectively.'}, 'zh': {'title': '提升低资源语言代码生成的有效策略', 'desc': '大型语言模型（LLMs）的出现显著推动了自动代码生成领域的发展。这些模型依赖于大量多样化的数据集来学习编程语言的语法、语义和使用模式。然而，对于低资源语言（即训练数据稀缺的小众编程语言），数据的有限性限制了模型的泛化能力，导致代码生成性能较差。因此，本文研究了几种提升LLMs在低资源语言上表现的有效方法，包括经典的微调和几种上下文学习变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04296', 'title': 'Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression', 'url': 'https://huggingface.co/papers/2502.04296', 'abstract': 'We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \\ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.', 'score': 4, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'da9d11d5ea5d9d9e', 'authors': ['Lirui Wang', 'Kevin Zhao', 'Chaoqi Liu', 'Xinlei Chen'], 'affiliations': ['MIT', 'Meta, FAIR', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.04296.jpg', 'data': {'categories': ['#games', '#robotics', '#dataset', '#video', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'HMA: Быстрое и качественное моделирование видео для обучения роботов', 'desc': 'Статья представляет новый метод Heterogeneous Masked Autoregression (HMA) для моделирования динамики видео действий в робототехнике. HMA использует гетерогенное предобучение на наблюдениях и последовательностях действий из различных роботизированных воплощений, доменов и задач. Метод применяет маскированную авторегрессию для генерации квантованных или мягких токенов для предсказания видео. HMA достигает лучшего визуального качества и управляемости по сравнению с предыдущими моделями генерации видео для роботов, работая в 15 раз быстрее в реальном мире.'}, 'en': {'title': 'Revolutionizing Robot Learning with Heterogeneous Video Modeling', 'desc': 'The paper introduces Heterogeneous Masked Autoregression (HMA), a novel approach for modeling the dynamics of action videos to enhance robot learning. HMA addresses the challenges of diverse environments and the need for real-time computational efficiency by leveraging heterogeneous pre-training from various robotic tasks and domains. By employing masked autoregression, HMA generates high-quality video predictions using quantized or soft tokens. The results show that HMA significantly improves visual fidelity and controllability while operating 15 times faster than previous models, making it a valuable tool for simulating video from low-level actions and evaluating robotic policies.'}, 'zh': {'title': '异构掩蔽自回归：提升机器人学习的视频生成', 'desc': '我们提出了异构掩蔽自回归（HMA）模型，用于建模动作视频的动态，以生成高质量的数据并评估机器人学习的扩展性。构建交互式视频世界模型和机器人策略面临挑战，因为需要处理多样化的环境，同时保持实时运行的计算效率。HMA通过对不同机器人形态、领域和任务的观察和动作序列进行异构预训练，来提高模型的性能。经过后期训练，该模型可以作为视频模拟器，从低级动作输入中评估策略并生成合成数据。'}}}, {'id': 'https://huggingface.co/papers/2502.04322', 'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions', 'url': 'https://huggingface.co/papers/2502.04322', 'abstract': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.', 'score': 3, 'issue_id': 2090, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '1ad4a9febd48be28', 'authors': ['Yik Siu Chan', 'Narutatsu Ri', 'Yuxin Xiao', 'Marzyeh Ghassemi'], 'affiliations': ['Brown University', 'Columbia University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.04322.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multilingual', '#benchmark', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Простое взаимодействие - скрытая угроза безопасности языковых моделей', 'desc': 'Статья посвящена проблеме уязвимостей больших языковых моделей (LLM) к атакам, направленным на обход систем безопасности. Авторы предлагают новую метрику HarmScore для оценки эффективности вредоносных ответов LLM и разрабатывают фреймворк атаки Speak Easy, основанный на многошаговом многоязычном взаимодействии. Исследование показывает, что простые паттерны взаимодействия могут быть легко использованы злоумышленниками для вредоносных целей. Результаты демонстрируют значительное увеличение успешности атак и показателя HarmScore при применении Speak Easy к различным LLM.'}, 'en': {'title': 'Uncovering Hidden Vulnerabilities in Language Models', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.'}, 'zh': {'title': '揭示大型语言模型的安全漏洞', 'desc': '尽管已经进行了广泛的安全对齐工作，大型语言模型（LLMs）仍然容易受到越狱攻击，这会引发有害行为。本文探讨了两个关键问题：越狱响应是否真正帮助普通用户实施有害行为，以及在更常见的人类与LLM的简单互动中是否存在安全漏洞。我们提出了HarmScore，这是一种评估LLM响应如何有效促进有害行为的指标，并介绍了Speak Easy，一个简单的多步骤、多语言攻击框架。研究表明，恶意用户可以轻松利用常见的互动模式来实现有害意图。'}}}, {'id': 'https://huggingface.co/papers/2502.02492', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models', 'url': 'https://huggingface.co/papers/2502.02492', 'abstract': "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/", 'score': 23, 'issue_id': 2042, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '33581479f8c6ed9f', 'authors': ['Hila Chefer', 'Uriel Singer', 'Amit Zohar', 'Yuval Kirstain', 'Adam Polyak', 'Yaniv Taigman', 'Lior Wolf', 'Shelly Sheynin'], 'affiliations': ['GenAI, Meta', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02492.jpg', 'data': {'categories': ['#video'], 'emoji': '🎬', 'ru': {'title': 'VideoJAM: Реалистичное движение в генеративных видеомоделях', 'desc': 'Статья представляет VideoJAM - новый подход к генерации видео, решающий проблему недостаточной реалистичности движения в существующих моделях. Авторы предлагают обучать модель совместному представлению внешнего вида и движения, а также вводят механизм Inner-Guidance для улучшения когерентности движения при генерации. VideoJAM может быть применен к любой видеомодели без существенных изменений и показывает лучшие результаты по сравнению с существующими решениями. Исследование демонстрирует, что интеграция внешнего вида и движения улучшает как визуальное качество, так и согласованность генерируемого видео.'}, 'en': {'title': 'Enhancing Video Generation with Motion Coherence', 'desc': 'This paper addresses the challenges faced by generative video models in accurately capturing real-world motion and dynamics. The authors identify that traditional pixel reconstruction methods prioritize visual appearance over motion coherence, leading to less realistic video outputs. To overcome this, they propose VideoJAM, a framework that integrates a motion prior into video generation by learning a combined representation of appearance and motion. By extending the training objective and introducing a dynamic guidance mechanism during inference, VideoJAM significantly improves motion coherence and visual quality, outperforming existing models without requiring changes to training data or model architecture.'}, 'zh': {'title': 'VideoJAM：提升视频生成的运动一致性与视觉质量', 'desc': '尽管生成视频模型在最近取得了巨大进展，但仍然难以捕捉真实世界的运动和动态。本文提出了VideoJAM框架，通过引入有效的运动先验，帮助视频生成器学习联合的外观-运动表示。该框架在训练过程中扩展了目标，预测生成的像素及其对应的运动，并在推理阶段引入了内部引导机制，以实现一致的运动生成。VideoJAM在运动一致性方面达到了最先进的性能，同时提升了生成视频的视觉质量，表明外观和运动可以互为补充，合理整合后能增强视频生成的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.01362', 'title': 'Inverse Bridge Matching Distillation', 'url': 'https://huggingface.co/papers/2502.01362', 'abstract': 'Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.', 'score': 22, 'issue_id': 2045, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '061049a23278b0f6', 'authors': ['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], 'affiliations': ['Skolkovo Institute of Science and Technology', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01362.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных мостов: искусство эффективной дистилляции', 'desc': 'Статья представляет новый метод дистилляции для моделей диффузионного моста (DBM). Этот подход позволяет ускорить вывод DBM от 4 до 100 раз, сохраняя или даже улучшая качество генерации. Метод основан на формулировке обратного сопоставления моста и может применяться как к условным, так и к безусловным DBM. Техника была успешно протестирована на различных задачах, включая суперразрешение и восстановление JPEG.'}, 'en': {'title': 'Accelerating Diffusion Bridge Models with Innovative Distillation Techniques', 'desc': 'This paper introduces a new method to improve the speed and practicality of diffusion bridge models (DBMs), which are used for tasks like image translation. The authors present a distillation technique that leverages inverse bridge matching to create a more efficient training process. This method allows for the distillation of both conditional and unconditional DBMs, using only corrupted images, and enables one-step generation. The results show that their approach can significantly speed up inference times by up to 100 times while maintaining or even enhancing the quality of generated images compared to the original models.'}, 'zh': {'title': '加速扩散桥模型，提升生成质量！', 'desc': '扩散桥模型（DBMs）是一种有前景的扩展，适用于图像到图像的转换。然而，DBMs在推理时速度较慢，这是现代扩散和流模型普遍面临的问题。为了解决这个问题，我们提出了一种基于逆桥匹配的蒸馏技术，并推导出可行的目标来实际解决它。我们的蒸馏方法能够同时处理条件和无条件的DBMs，并且只使用损坏的图像进行训练，从而显著加快推理速度。'}}}, {'id': 'https://huggingface.co/papers/2502.01718', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'url': 'https://huggingface.co/papers/2502.01718', 'abstract': 'Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.', 'score': 15, 'issue_id': 2041, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b5b43fe7221df9d8', 'authors': ['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent Researcher', 'Netmind.AI', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2502.01718.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении моделей кода: мощь RL и автоматизированных тест-кейсов', 'desc': 'В этой статье представлен новый подход к обучению моделей для генерации кода с использованием обучения с подкреплением (RL). Авторы разработали пайплайн для создания пар (вопрос, тест-кейсы) из существующих кодовых данных, которые затем используются для обучения моделей вознаграждения. Применение этого метода привело к значительным улучшениям производительности моделей на различных бенчмарках, включая HumanEval и MBPP. Результаты исследования показывают большой потенциал обучения с подкреплением в области моделей для генерации кода.'}, 'en': {'title': 'Unlocking the Power of Reinforcement Learning in Coder Models', 'desc': 'This paper explores the use of reinforcement learning (RL) to improve coder models, which have primarily relied on supervised fine-tuning (SFT). The authors introduce a method for generating large-scale test-case pairs from existing code, which helps create reliable reward signals for training. By employing a preference-based reward model using the Bradley-Terry loss, they achieve significant performance gains in various coding benchmarks. The results demonstrate that RL can substantially enhance the capabilities of coder models, showcasing its untapped potential in this domain.'}, 'zh': {'title': '强化学习提升代码模型的潜力', 'desc': '本文探讨了在代码模型训练中使用强化学习（RL）的潜力，尤其是在缺乏可靠奖励数据的情况下。我们设计了一种自动化的大规模测试用例合成管道，以生成大量（问题，测试用例）对，从而增强代码模型的训练。通过使用这些测试用例，我们构建了基于通过率的偏好对，并利用Bradley-Terry损失训练奖励模型。实验结果表明，使用强化学习后，模型在多个基准测试上均有显著提升，展示了强化学习在代码模型中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.02584', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'url': 'https://huggingface.co/papers/2502.02584', 'abstract': 'Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.', 'score': 11, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'f2d3938d4ad71761', 'authors': ['Zongyu Lin', 'Yao Tang', 'Xingcheng Yao', 'Da Yin', 'Ziniu Hu', 'Yizhou Sun', 'Kai-Wei Chang'], 'affiliations': ['Shanghai Jiaotong University, Shanghai, China', 'University of California, Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.02584.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#inference', '#reasoning', '#agents', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'QLASS: Пошаговое обучение языковых агентов для повышения эффективности', 'desc': 'Статья представляет новый метод QLASS для обучения языковых агентов. QLASS использует пошаговую оценку Q-значений для генерации аннотаций и улучшения промежуточного обучения. Метод вводит дерево рассуждений и моделирование вознаграждений процесса для эффективного пошагового руководства. QLASS позволяет языковым агентам лучше адаптироваться к долгосрочным целям, значительно улучшая производительность при решении сложных интерактивных задач.'}, 'en': {'title': 'Enhancing Language Agents with Stepwise Q-Guidance', 'desc': 'This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.'}, 'zh': {'title': 'QLASS：提升语言代理的决策能力', 'desc': '本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。'}}}, {'id': 'https://huggingface.co/papers/2502.02508', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'url': 'https://huggingface.co/papers/2502.02508', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.", 'score': 9, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '80bd687783bd609b', 'authors': ['Maohao Shen', 'Guangtao Zeng', 'Zhenting Qi', 'Zhang-Wei Hong', 'Zhenfang Chen', 'Wei Lu', 'Gregory Wornell', 'Subhro Das', 'David Cox', 'Chuang Gan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab, IBM Research', 'Singapore University of Technology and Design', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.02508.jpg', 'data': {'categories': ['#small_models', '#open_source', '#training', '#reasoning', '#math', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Satori: LLM с внутренним поиском для улучшенного рассуждения', 'desc': 'Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассуждению. Они разработали метод Chain-of-Action-Thought (COAT), который позволяет модели проводить самоанализ и исследовать новые стратегии решения задач. Процесс обучения включает два этапа: настройку формата на небольшом масштабе и масштабное самосовершенствование с использованием обучения с подкреплением. В результате была создана 7-миллиардная модель Satori, показавшая отличные результаты в задачах математического рассуждения и обобщения на новые области.'}, 'en': {'title': 'Empowering LLMs with Internalized Reasoning through COAT', 'desc': 'This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.'}, 'zh': {'title': '内化搜索能力，提升推理能力！', 'desc': '大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。'}}}, {'id': 'https://huggingface.co/papers/2502.01941', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'url': 'https://huggingface.co/papers/2502.01941', 'abstract': "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.", 'score': 7, 'issue_id': 2041, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '1352d78ee18eadfa', 'authors': ['Xiang Liu', 'Zhenheng Tang', 'Hong Chen', 'Peijie Dong', 'Zeyu Li', 'Xiuze Zhou', 'Bo Li', 'Xuming Hu', 'Xiaowen Chu'], 'affiliations': ['The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China', 'The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.01941.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Сжатие KV-кэша в LLM: баланс между эффективностью и производительностью', 'desc': 'Статья исследует влияние методов сжатия KV-кэша на фундаментальные возможности больших языковых моделей (LLM). Авторы провели комплексный эмпирический анализ различных методов сжатия на разнообразных задачах, включая общие знания, здравый смысл, арифметические рассуждения и генерацию кода. Результаты показали, что сжатие KV-кэша может значительно ухудшать производительность модели, особенно в задачах арифметических рассуждений. На основе анализа авторы предложили новый метод ShotKV, который по-разному обрабатывает фазы предзаполнения и декодирования, сохраняя семантическую целостность.'}, 'en': {'title': 'Optimizing KV Cache Compression for Enhanced LLM Performance', 'desc': "This paper explores how compressing the KV cache in large language models (LLMs) affects their performance on various tasks. While compression can reduce memory usage, it may also lead to a decline in the model's ability to perform tasks like arithmetic reasoning and code generation. The study finds that different compression methods impact tasks differently, with some methods causing significant performance drops, especially in arithmetic reasoning. To address these issues, the authors introduce ShotKV, a new compression technique that improves performance on long-context tasks while preserving important semantic information."}, 'zh': {'title': 'KV缓存压缩对大型语言模型能力的影响研究', 'desc': '本文研究了大型语言模型（LLMs）中一个未被充分探讨的挑战：KV缓存压缩方法对LLMs基本能力的影响。虽然现有方法在长上下文基准测试中取得了令人印象深刻的压缩比，但它们对核心模型能力的影响仍然缺乏研究。我们的实证研究评估了多种KV缓存压缩方法在不同任务上的表现，包括世界知识、常识推理、算术推理、代码生成、安全性以及长上下文理解和生成。分析结果显示，KV缓存压缩方法在特定任务上表现出性能下降，尤其是算术推理任务对激进压缩特别敏感，性能下降幅度达到17.4%-43.3%。'}}}, {'id': 'https://huggingface.co/papers/2502.02589', 'title': 'COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.02589', 'abstract': 'This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning.', 'score': 6, 'issue_id': 2056, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '28c4625deea8ac72', 'authors': ['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2502.02589.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Детальные аннотации для лучшего понимания изображений', 'desc': 'Статья представляет набор данных COCONut-PanCap для улучшения панорамной сегментации и привязки подписей к изображениям. Этот датасет расширяет COCO, добавляя детальные панорамные маски и подробные описания сцен, привязанные к регионам изображения. COCONut-PanCap направлен на преодоление ограничений существующих наборов данных изображение-текст, которые часто не содержат всесторонних описаний сцен. Экспериментальные результаты показывают, что использование COCONut-PanCap значительно улучшает производительность моделей в задачах понимания и генерации изображений.'}, 'en': {'title': 'Enhancing Image Understanding with COCONut-PanCap Dataset', 'desc': 'The COCONut-PanCap dataset is designed to improve panoptic segmentation and grounded image captioning by providing detailed scene descriptions. It builds on the existing COCO dataset by adding advanced panoptic masks and fine-grained, region-level captions. This dataset enhances the training of vision-language models (VLMs) by offering high-quality, human-edited annotations that ensure consistency and detail in generated captions. Experimental results show that COCONut-PanCap significantly enhances performance in both understanding and generation tasks, establishing a new standard for evaluating models in multi-modal learning.'}, 'zh': {'title': 'COCONut-PanCap：提升全景分割与图像描述生成的新数据集', 'desc': '本文介绍了COCONut-PanCap数据集，旨在增强全景分割和基于图像的描述生成。该数据集在COCO数据集的基础上，结合了先进的COCONut全景掩码，克服了现有图像-文本数据集中缺乏详细场景描述的局限性。COCONut-PanCap数据集包含基于全景分割掩码的细粒度区域级描述，确保了一致性并提高了生成描述的细节。实验结果表明，COCONut-PanCap在理解和生成任务中显著提升了性能，为多模态学习中的高质量图像-文本注释需求提供了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2502.00674', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?', 'url': 'https://huggingface.co/papers/2502.00674', 'abstract': 'Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of 3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.', 'score': 5, 'issue_id': 2050, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '34ba4144afc562aa', 'authors': ['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.00674.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Один лучше многих: новый подход к ансамблированию языковых моделей', 'desc': 'Исследователи предложили новый метод ансамблирования под названием Self-MoA, который агрегирует выходные данные только одной наиболее эффективной языковой модели. Эксперименты показали, что Self-MoA превосходит стандартный метод Mixture-of-Agents (MoA), который объединяет разные языковые модели, на многих бенчмарках. Авторы изучили компромисс между разнообразием и качеством выходных данных в различных конфигурациях MoA. Также была представлена последовательная версия Self-MoA, способная агрегировать большое количество выходных данных языковой модели в несколько этапов.'}, 'en': {'title': 'Self-MoA: Elevating Performance with a Single Top LLM', 'desc': 'This paper investigates the effectiveness of an ensemble method called Self-MoA, which aggregates outputs from a single top-performing Large Language Model (LLM) instead of mixing multiple LLMs. The authors find that Self-MoA significantly outperforms the traditional Mixture-of-Agents (MoA) method, achieving notable improvements on various benchmarks. Their experiments reveal that the quality of outputs is crucial, as mixing different LLMs can reduce overall performance due to lower average quality. Additionally, the paper introduces a sequential version of Self-MoA that efficiently aggregates outputs over multiple rounds, maintaining high effectiveness.'}, 'zh': {'title': '单一模型集成，超越多样性', 'desc': '本论文探讨了混合不同大型语言模型（LLMs）输出的有效性，提出了一种新的集成方法Self-MoA。Self-MoA仅聚合单一表现最佳的LLM的输出，实验结果显示其在多个基准测试中表现优于传统的Mixture-of-Agents（MoA）方法。具体而言，Self-MoA在AlpacaEval 2.0基准上提高了6.6%的性能，并在多个基准上平均提高了3.8%。此外，论文还分析了输出多样性与质量之间的权衡，确认混合不同LLMs可能会降低模型的平均质量。'}}}, {'id': 'https://huggingface.co/papers/2501.19066', 'title': 'Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations', 'url': 'https://huggingface.co/papers/2501.19066', 'abstract': 'Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (e.g., nudity) or to introduce a new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of 20.01% in unsafe concept removal, is effective in style manipulation, and is sim5x faster than current state-of-the-art.', 'score': 4, 'issue_id': 2050, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a8acff84a873ecb8', 'authors': ['Dahye Kim', 'Deepti Ghadiyaram'], 'affiliations': ['Department of Computer Science, Boston U'], 'pdf_title_img': 'assets/pdf/title_img/2501.19066.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#interpretability', '#security', '#cv', '#training'], 'emoji': '🎨', 'ru': {'title': 'Безопасное и эффективное управление концепциями в генерации изображений', 'desc': 'Статья представляет новый подход к манипулированию концепциями в генеративных моделях изображений с использованием разреженных автоэнкодеров (k-SAE). Авторы предлагают метод для эффективного удаления нежелательного контента и добавления новых стилей без необходимости переобучения базовой модели. Эксперименты показывают, что предложенный подход превосходит существующие методы по скорости и эффективности удаления небезопасных концепций на 20.01%. Метод также демонстрирует устойчивость к состязательным атакам и сохраняет качество генерируемых изображений.'}, 'en': {'title': 'Efficient Concept Control in Text-to-Image Generation', 'desc': 'This paper presents a new method for improving text-to-image generative models by using k-sparse autoencoders (k-SAEs) to manipulate concepts in a more efficient and interpretable way. Instead of fine-tuning models, which can be slow and reduce quality, this approach allows for precise control over the generation of specific concepts, such as removing unsafe content or adding new styles. The authors demonstrate that their method does not require retraining the base model and is significantly faster than existing techniques, achieving a 20.01% improvement in unsafe concept removal. Overall, this framework enhances the safety and versatility of generative models while maintaining high-quality outputs.'}, 'zh': {'title': '高效操控生成模型中的概念', 'desc': '本文提出了一种新颖的框架，利用k稀疏自编码器（k-SAEs）来实现扩散模型中的概念高效且可解释的操控。我们首先在文本嵌入的潜在空间中识别可解释的单义概念，并利用这些概念精确地引导生成内容，避免或引入特定概念。通过大量实验，我们证明了该方法简单易用，无需重新训练基础模型或使用LoRA适配器，且不影响生成质量。我们的技术在去除不安全概念方面提高了20.01%，在风格操控上也表现出色，速度比当前最先进的方法快5倍。'}}}, {'id': 'https://huggingface.co/papers/2502.01720', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'url': 'https://huggingface.co/papers/2502.01720', 'abstract': 'Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.', 'score': 2, 'issue_id': 2043, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd249f21cea90b464', 'authors': ['Nupur Kumari', 'Xi Yin', 'Jun-Yan Zhu', 'Ishan Misra', 'Samaneh Azadi'], 'affiliations': ['Carnegie Mellon University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2502.01720.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#architecture', '#cv', '#inference'], 'emoji': '🎨', 'ru': {'title': 'Улучшение кастомизации моделей text-to-image с помощью синтетических данных и новой архитектуры', 'desc': 'Статья представляет новый подход к кастомизации моделей text-to-image. Авторы создали синтетический набор данных SynCD с множественными изображениями объектов в разных условиях. Они предложили новую архитектуру энкодера, основанную на механизмах разделяемого внимания, для лучшего учета визуальных деталей. Также был разработан метод вывода, нормализующий векторы текстового и изображенческого руководства для устранения проблем переэкспозиции.'}, 'en': {'title': 'Enhancing Customization in Text-to-Image Models with Synthetic Datasets', 'desc': 'This paper presents a novel approach to customize text-to-image models, allowing users to generate images of custom concepts in various settings. The authors create a Synthetic Customization Dataset (SynCD) using 3D datasets, which includes multiple images of the same object under different conditions. They introduce a new encoder architecture that utilizes shared attention mechanisms to capture detailed visual information effectively. Additionally, a new inference technique is proposed to address overexposure issues, resulting in improved image quality compared to existing methods.'}, 'zh': {'title': '高质量定制化：突破文本到图像生成的限制', 'desc': '本文提出了一种文本到图像模型的定制化方法，允许用户在未见过的环境中生成自定义概念。现有方法通常依赖于昂贵的测试时优化或在单图像数据集上训练编码器，导致图像质量较差。我们的方法利用现有的文本到图像模型和3D数据集，创建了一个高质量的合成定制数据集（SynCD），包含同一对象在不同光照、背景和姿势下的多张图像。通过新的编码器架构和推理技术，我们的模型在标准定制基准测试中表现优于现有的无调优方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01839', 'title': 'Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification', 'url': 'https://huggingface.co/papers/2502.01839', 'abstract': "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.", 'score': 1, 'issue_id': 2055, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd5c87eae437c7bec', 'authors': ['Eric Zhao', 'Pranjal Awasthi', 'Sreenivas Gollapudi'], 'affiliations': ['Google Research', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.01839.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Масштабирование поиска: простота ведет к силе', 'desc': 'Статья исследует масштабирование поиска на основе выборки в моделях машинного обучения. Авторы обнаружили, что простое увеличение количества случайных выборок и прямая самопроверка могут значительно улучшить производительность моделей, например Gemini v1.5 Pro. Выявлено явление неявного масштабирования, когда увеличение пула ответов повышает точность верификации. Исследование также подчеркивает важность сравнения ответов и адаптации стилей вывода модели для улучшения самопроверки.'}, 'en': {'title': 'Boosting Model Performance with Smart Sampling and Verification', 'desc': 'This paper explores how sampling-based search can enhance the performance of machine learning models during testing. By generating multiple candidate responses and verifying them, the authors show that scaling up this approach leads to better reasoning capabilities in models like Gemini v1.5 Pro. They discover that larger pools of sampled responses improve the accuracy of verification, a concept they call implicit scaling. Additionally, the paper highlights two principles for enhancing self-verification: comparing responses to identify errors and using different output styles for various tasks.'}, 'zh': {'title': '基于采样的搜索：提升模型推理能力的关键', 'desc': '本文研究了基于采样的搜索方法，这是一种在测试时利用计算资源的简单策略。我们发现，通过扩展一个仅使用随机采样和自我验证的简化实现，可以显著提高模型的推理能力。我们还提出了两个原则来改善自我验证能力：比较不同响应可以帮助识别错误位置，而不同的模型输出风格在不同上下文中有不同的效果。尽管可以实现准确的验证，但当前的前沿模型在自我验证能力上仍然表现较弱。'}}}, {'id': 'https://huggingface.co/papers/2501.19389', 'title': 'Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2501.19389', 'abstract': "Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.", 'score': 0, 'issue_id': 2058, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'c106990f1f2dc4d8', 'authors': ['Wenzhi Fang', 'Dong-Jun Han', 'Liangqi Yuan', 'Seyyedali Hosseinalipour', 'Christopher G. Brinton'], 'affiliations': ['Department of Computer Science and Engineering, Yonsei University', 'Department of Electrical Engineering, University at Buffalo-SUNY', 'Department of Electrical and Computer Engineering, Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2501.19389.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#transfer_learning'], 'emoji': '📱', 'ru': {'title': 'FSLoRA: эффективная федеративная настройка больших языковых моделей на устройствах', 'desc': 'Данная статья представляет новый метод под названием FSLoRA (Federated Sketching LoRA) для федеративной настройки больших языковых моделей на устройствах с ограниченными ресурсами. FSLoRA использует механизм скетчинга, позволяющий устройствам выборочно обновлять подматрицы глобальных LoRA-модулей на сервере. Метод адаптируется к вычислительным ограничениям конкретных устройств путем настройки коэффициентов скетчинга. Авторы проводят строгий анализ сходимости FSLoRA и демонстрируют превосходную производительность метода в экспериментах на различных наборах данных и моделях.'}, 'en': {'title': 'Adaptive Fine-Tuning for Diverse Devices with FSLoRA', 'desc': "This paper introduces federated sketching LoRA (FSLoRA), a method for fine-tuning large language models (LLMs) on devices with varying computational resources. FSLoRA uses a sketching mechanism that allows devices to update only specific parts of the global LoRA modules, making it adaptable to different device capabilities. The method addresses the challenges of data scarcity and model size by adjusting sketching ratios, which control the ranks of the updates based on device constraints. The authors provide a detailed analysis of FSLoRA's convergence properties and demonstrate its effectiveness through experiments, showing it outperforms existing methods."}, 'zh': {'title': '灵活适应设备限制的联邦草图LoRA', 'desc': '本文提出了一种新的方法，称为联邦草图LoRA（FSLoRA），旨在解决在设备上微调大型语言模型（LLMs）时的计算资源异质性问题。FSLoRA利用草图机制，使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比例，FSLoRA能够灵活适应设备特定的通信和计算限制。实验结果表明，FSLoRA在多个数据集和LLM模型上表现优于现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2502.08910', 'title': 'InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU', 'url': 'https://huggingface.co/papers/2502.08910', 'abstract': 'In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.', 'score': 124, 'issue_id': 2210, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'eed812d17aeec57e', 'authors': ['Heejun Lee', 'Geon Park', 'Jaduk Suh', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai, Seoul, Korea', 'Graduate School of AI, KAIST, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.08910.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#long_context', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Преодоление барьера длинного контекста в LLM', 'desc': 'InfiniteHiP - это новая система для обработки больших языковых моделей (LLM) с длинным контекстом. Она ускоряет обработку, удаляя неважные токены контекста с помощью иерархического алгоритма. Система позволяет обобщать на более длинные последовательности, применяя методы настройки RoPE. InfiniteHiP достигает 18.95-кратного ускорения в декодировании внимания для контекста в 1 миллион токенов без дополнительного обучения.'}, 'en': {'title': 'InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently', 'desc': "This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model's ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training."}, 'zh': {'title': 'InfiniteHiP：高效处理超长上下文的LLM框架', 'desc': '在现代大型语言模型（LLMs）中，处理非常长的上下文长度面临显著挑战，导致推理速度变慢和内存成本增加。现有的大多数预训练LLMs无法超出其原始训练序列长度进行泛化。为了解决这一问题，我们提出了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过模块化的分层令牌修剪算法动态消除无关的上下文令牌，从而加速处理。我们的框架能够在不需要额外训练的情况下，实现对长达300万令牌的处理，并在1百万令牌上下文中实现18.95倍的注意力解码加速。'}}}, {'id': 'https://huggingface.co/papers/2502.08946', 'title': "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding", 'url': 'https://huggingface.co/papers/2502.08946', 'abstract': 'In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.', 'score': 104, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'daecc7f38306f7b8', 'authors': ['Mo Yu', 'Lemao Liu', 'Junjie Wu', 'Tsz Ting Chung', 'Shunchi Zhang', 'Jiangnan Li', 'Dit-Yan Yeung', 'Jie Zhou'], 'affiliations': ['HKUST', 'JHU', 'WeChat AI, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2502.08946.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#training', '#reasoning', '#interpretability'], 'emoji': '🦜', 'ru': {'title': 'Языковые модели: понимание или имитация?', 'desc': "Исследователи изучают вопрос о действительном понимании языковыми моделями (LLM) того, что они говорят. Они разработали задачу PhysiCo для оценки понимания физических концепций, используя абстрактные сетки вместо естественного языка. Результаты показывают, что современные LLM, включая GPT-4 и Gemini 2.0, отстают от людей примерно на 40% в этой задаче. Исследование подтверждает феномен 'стохастического попугая', так как модели не справляются с сеточной задачей, но хорошо описывают те же концепции на естественном языке."}, 'en': {'title': 'Unveiling the Limits of LLM Understanding', 'desc': "This paper investigates whether large language models (LLMs) truly understand the content they generate, addressing the concept of the 'Stochastic Parrot'. The authors introduce a new assessment task called PhysiCo, which uses grid-format inputs to evaluate understanding of physical concepts. Their findings reveal that top-performing LLMs, like GPT-4o and Gemini 2.0, significantly underperform compared to humans, indicating a gap in comprehension. Additionally, the study shows that LLMs struggle with the task due to inherent challenges in understanding rather than just the format of the input data."}, 'zh': {'title': '探究大型语言模型的理解能力', 'desc': '本研究系统性地探讨了一个常见问题：大型语言模型（LLMs）是否真正理解它们所说的内容。我们提出了一种名为PhysiCo的评估任务，旨在通过网格格式的输入来减轻记忆问题，这些输入抽象地描述了物理现象。研究表明，当前最先进的LLMs在理解能力上落后于人类约40%，并且在网格任务中表现不佳，显示出随机鹦鹉现象的存在。我们的任务挑战了LLMs，主要是由于内在的困难，而非网格格式的不熟悉。'}}}, {'id': 'https://huggingface.co/papers/2502.08690', 'title': 'Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2502.08690', 'abstract': 'Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.', 'score': 31, 'issue_id': 2210, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'c7734d31994dcb35', 'authors': ['Hoigi Seo', 'Wongi Jeong', 'Jae-sun Seo', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea', 'INMC & IPAI, Seoul National University, Republic of Korea', 'School of Electrical and Computer Engineering, Cornell Tech, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.08690.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#inference', '#diffusion'], 'emoji': '✂️', 'ru': {'title': 'Эффективное сжатие текстовых энкодеров для генерации изображений без потери качества', 'desc': 'Статья представляет новый метод оптимизации текстовых энкодеров в моделях диффузии текст-в-изображение (T2I). Авторы предлагают стратегию прунинга под названием Skip and Re-use layers (Skrr), которая позволяет значительно сократить потребление памяти без ущерба для качества генерируемых изображений. Метод Skrr избирательно пропускает или повторно использует определенные слои в трансформерных блоках, учитывая специфику задач T2I. Эксперименты показывают, что Skrr превосходит существующие методы поблочного прунинга и достигает наилучших показателей эффективности использования памяти при сохранении производительности по различным метрикам оценки.'}, 'en': {'title': 'Efficient Memory Use in Text-to-Image Models with Skrr', 'desc': 'This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks.'}, 'zh': {'title': '提升文本编码器的内存效率', 'desc': '本文提出了一种名为Skip and Re-use layers（Skrr）的新策略，旨在提高文本到图像扩散模型中文本编码器的内存效率。尽管文本编码器在推理时间和浮点运算方面的贡献较小，但它们的内存需求却高达去噪模块的八倍。Skrr通过选择性跳过或重用某些变换器层，利用了变换器块中的冗余性，从而在不影响性能的情况下减少内存消耗。实验结果表明，Skrr在高稀疏度下仍能保持与原始模型相当的图像质量，并在多个评估指标上实现了最先进的内存效率。'}}}, {'id': 'https://huggingface.co/papers/2502.09619', 'title': 'Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights', 'url': 'https://huggingface.co/papers/2502.09619', 'abstract': 'With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as "Dog", without access to model metadata or training data. Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes). Our method supports both logit-based retrieval ("find more logits like this") and zero-shot, text-based retrieval ("find all logits corresponding to dogs"). As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x. We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories.', 'score': 27, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '2222d8b83a19a957', 'authors': ['Jonathan Kahana', 'Or Nathan', 'Eliahu Horwitz', 'Yedid Hoshen'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.09619.jpg', 'data': {'categories': ['#rag', '#dataset', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ProbeLog: Умный поиск нужных моделей машинного обучения', 'desc': 'ProbeLog - это метод поиска классификационных моделей без доступа к метаданным или обучающим данным. Он вычисляет дескриптор для каждого выходного измерения модели, наблюдая ее ответы на фиксированном наборе входных данных. ProbeLog поддерживает как поиск на основе логитов, так и текстовый поиск с нулевым обучением. Метод использует совместную фильтрацию для снижения вычислительных затрат и демонстрирует высокую точность поиска в реальных задачах.'}, 'en': {'title': 'Efficient Model Retrieval with ProbeLog', 'desc': "This paper introduces ProbeLog, a novel method for retrieving classification models that can identify specific concepts without needing detailed model information. It generates a descriptor for each model's output dimension by analyzing its responses to a set of predefined inputs, known as probes. ProbeLog allows users to perform both logit-based and zero-shot retrieval, making it easier to find models relevant to their needs. Additionally, it employs collaborative filtering to significantly reduce the computational cost of processing large model repositories, achieving high accuracy in model retrieval tasks."}, 'zh': {'title': '高效模型检索，轻松找到所需！', 'desc': '随着公开模型数量的增加，用户所需的任务几乎都有预训练的在线模型。然而，目前的模型搜索方法相对简单，主要依赖文档中的文本搜索，导致用户无法找到相关模型。本文提出了ProbeLog，一种检索分类模型的方法，可以识别目标概念，如“狗”，而无需访问模型元数据或训练数据。与之前的探测方法不同，ProbeLog通过观察模型在固定输入集上的响应，为每个模型的每个输出维度（logit）计算描述符，从而实现高效的模型检索。'}}}, {'id': 'https://huggingface.co/papers/2502.09056', 'title': 'An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging', 'url': 'https://huggingface.co/papers/2502.09056', 'abstract': 'This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.', 'score': 26, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '36c3c29072ae279d', 'authors': ['Kunat Pipatanakul', 'Pittawat Taveekitworachai', 'Potsawee Manakul', 'Kasima Tharnpipitchai'], 'affiliations': ['SCB 10X R&D SCBX Group Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2502.09056.jpg', 'data': {'categories': ['#data', '#dataset', '#low_resource', '#multilingual', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Усиление логики локальных языковых моделей', 'desc': 'Данное исследование посвящено методам улучшения способностей к рассуждению у языково-специфичных больших языковых моделей (LLM), в частности для тайского языка. Авторы предлагают подходы к отбору данных и слиянию моделей, чтобы перенести продвинутые навыки рассуждения из модели DeepSeek R1 в локальные LLM. Цель состоит в том, чтобы усилить логические возможности языково-специфичных моделей, сохраняя при этом их способности в целевом языке. Исследователи показывают, что даже с ограниченным бюджетом и общедоступными данными можно значительно улучшить рассуждения локальных LLM до уровня DeepSeek R1.'}, 'en': {'title': 'Empowering Thai LLMs with Enhanced Reasoning Capabilities', 'desc': "This paper explores methods for selecting data and merging models to improve reasoning abilities in language-specific large language models (LLMs), particularly for the Thai language. The authors aim to enhance these models' reasoning skills while ensuring they remain effective in their target languages. They highlight the challenges faced by low-resource languages, which often lack the extensive training data available for high-resource languages like English. The study demonstrates that it is feasible to boost the reasoning capabilities of these LLMs using only publicly available datasets and a modest budget, achieving results comparable to advanced models like DeepSeek R1."}, 'zh': {'title': '提升低资源语言LLM的推理能力', 'desc': '本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入特定语言的大型语言模型（LLMs），特别关注泰语LLM。我们的目标是增强特定语言LLMs的推理能力，同时保持其目标语言的能力。DeepSeek R1在推理方面表现出色，但主要受益于高资源语言，如英语和中文，而低资源语言则受到忽视。我们展示了仅使用公开数据集和120美元的计算预算，就可以提升特定语言LLMs的推理能力，使其达到DeepSeek R1的水平，而不影响其在目标语言任务上的表现。'}}}, {'id': 'https://huggingface.co/papers/2502.09604', 'title': 'SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models', 'url': 'https://huggingface.co/papers/2502.09604', 'abstract': 'We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.', 'score': 26, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '7aa5ce3731848736', 'authors': ['Yung-Sung Chuang', 'Benjamin Cohen-Wang', 'Shannon Zejiang Shen', 'Zhaofeng Wu', 'Hu Xu', 'Xi Victoria Lin', 'James Glass', 'Shang-Wen Li', 'Wen-tau Yih'], 'affiliations': ['Massachusetts Institute of Technology', 'Meta FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2502.09604.jpg', 'data': {'categories': ['#optimization', '#long_context', '#training', '#alignment', '#rlhf', '#benchmark'], 'emoji': '📚', 'ru': {'title': 'SelfCite: Самообучение ИИ искусству цитирования', 'desc': 'SelfCite - это новый подход к самообучению больших языковых моделей (LLM) для генерации качественных цитат на уровне предложений. Метод использует сигнал награды, предоставляемый самой моделью через абляцию контекста, что позволяет избежать дорогостоящей ручной разметки. SelfCite применяет стратегию выборки best-of-N во время вывода и оптимизацию предпочтений для точной настройки моделей. Эффективность подхода подтверждается увеличением F1-меры цитирования до 5.3 пунктов на бенчмарке LongBench-Cite.'}, 'en': {'title': 'Enhancing Citation Quality with SelfCite', 'desc': 'SelfCite is a self-supervised method designed to enhance the citation quality in responses generated by large language models (LLMs). It uses a unique reward signal derived from the LLM itself, which assesses the necessity of citations by analyzing the impact of removing or retaining cited text. This approach not only improves the citation generation process during inference but also allows for fine-tuning the models to produce better citations through preference optimization. The results show a significant increase in citation accuracy, as evidenced by a 5.3 point improvement in citation F1 scores on the LongBench-Cite benchmark.'}, 'zh': {'title': '自监督引用生成的创新方法', 'desc': 'SelfCite是一种新颖的自监督方法，旨在使大型语言模型（LLM）生成高质量、细粒度的句子级引用。该方法通过上下文消融提供的奖励信号，减少对昂贵和劳动密集型注释的依赖。具体来说，如果引用是必要的，移除被引用文本应防止相同的响应；如果足够，保留被引用文本应保持相同的响应。SelfCite在LongBench-Cite基准测试中显示出有效性，使引用的F1分数提高了5.3个百分点。'}}}, {'id': 'https://huggingface.co/papers/2502.09560', 'title': 'EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents', 'url': 'https://huggingface.co/papers/2502.09560', 'abstract': 'Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.', 'score': 25, 'issue_id': 2211, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '019b4d19788a85cc', 'authors': ['Rui Yang', 'Hanyang Chen', 'Junyu Zhang', 'Mark Zhao', 'Cheng Qian', 'Kangrui Wang', 'Qineng Wang', 'Teja Venkat Koripella', 'Marziyeh Movahedi', 'Manling Li', 'Heng Ji', 'Huan Zhang', 'Tong Zhang'], 'affiliations': ['Northwestern University', 'Toyota Technological Institute at Chicago', 'University of Illinois Urbana-Champaign', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.09560.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#games', '#reasoning', '#agents'], 'emoji': '🤖', 'ru': {'title': 'EmbodiedBench: новый стандарт оценки воплощенного ИИ', 'desc': 'В статье представлен EmbodiedBench - комплексный инструмент для оценки мультимодальных языковых моделей (MLLM) в задачах воплощенного искусственного интеллекта. Бенчмарк включает 1128 тестовых заданий в четырех средах, охватывающих как высокоуровневые семантические задачи, так и низкоуровневые действия. Эксперименты с 13 ведущими MLLM показали, что модели успешно справляются с высокоуровневыми задачами, но испытывают трудности с низкоуровневыми манипуляциями. EmbodiedBench предоставляет стандартизированную платформу для оценки и развития воплощенных агентов на основе MLLM.'}, 'en': {'title': 'Empowering Embodied Agents with Comprehensive Evaluation', 'desc': 'This paper discusses the development of EmbodiedBench, a benchmark for evaluating multi-modal large language models (MLLMs) in the context of embodied agents. It highlights the gap in existing evaluation frameworks for MLLM-based agents, which are crucial for performing real-world tasks. The benchmark includes a wide range of tasks that assess various capabilities such as commonsense reasoning and spatial awareness. The results indicate that while MLLMs perform well on high-level tasks, they face significant challenges with low-level manipulation tasks.'}, 'zh': {'title': '多模态大型语言模型助力具身智能体的评估与发展', 'desc': '本论文探讨了多模态大型语言模型（MLLMs）在创建具身智能体方面的应用，旨在解决现实世界任务。我们提出了EmbodiedBench，这是一个全面的基准测试框架，用于评估视觉驱动的具身智能体。EmbodiedBench包含1288个测试任务，涵盖高层语义任务和低层原子动作任务，并评估智能体的常识推理、复杂指令理解、空间意识、视觉感知和长期规划等能力。实验结果表明，尽管MLLMs在高层任务中表现优异，但在低层操作任务上仍然存在挑战，最好的模型GPT-4o的平均得分仅为28.9%。'}}}, {'id': 'https://huggingface.co/papers/2502.09620', 'title': 'Exploring the Potential of Encoder-free Architectures in 3D LMMs', 'url': 'https://huggingface.co/papers/2502.09620', 'abstract': 'Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL', 'score': 22, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '2e519b64f13f6506', 'authors': ['Yiwen Tang', 'Zoey Guo', 'Zhuhao Wang', 'Ray Zhang', 'Qizhi Chen', 'Junli Liu', 'Delin Qu', 'Zhigang Wang', 'Dong Wang', 'Xuelong Li', 'Bin Zhao'], 'affiliations': ['Northwestern Polytechnical University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09620.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#optimization', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Революция в 3D-понимании: архитектуры без энкодера покоряют новые вершины', 'desc': 'Статья представляет первое всестороннее исследование потенциала архитектур без энкодера для 3D-понимания. Авторы предлагают стратегию LLM-embedded Semantic Encoding для предобучения и Hierarchical Geometry Aggregation для инструктивной настройки. Разработанная модель ENEL с 7 миллиардами параметров показывает результаты, сопоставимые с современными моделями, имеющими 13 миллиардов параметров. Исследование демонстрирует перспективность архитектур без энкодера в области 3D-понимания.'}, 'en': {'title': 'Revolutionizing 3D Understanding with Encoder-Free Architectures', 'desc': 'This paper explores the use of encoder-free architectures for 3D understanding, addressing limitations of traditional encoder-based models. It introduces the LLM-embedded Semantic Encoding strategy during pre-training to enhance point cloud representation and proposes a Hybrid Semantic Loss for better semantic extraction. Additionally, the Hierarchical Geometry Aggregation strategy is introduced in the instruction tuning phase to improve local detail focus in point clouds. The proposed model, ENEL, demonstrates competitive performance against existing models, indicating the potential of encoder-free approaches in 3D Large Multimodal Models.'}, 'zh': {'title': '无编码器架构：3D理解的新突破', 'desc': '本论文探讨了无编码器架构在3D理解中的应用潜力，首次对其进行全面研究。我们提出了一种新的策略，使大型语言模型（LLM）能够替代传统的3D编码器，解决了点云分辨率变化和特征语义需求不匹配的问题。通过引入嵌入式语义编码和分层几何聚合策略，我们的模型在分类、描述和视觉问答任务上表现出色。最终，我们的7B模型ENEL在性能上与当前最先进的模型ShapeLLM-13B相媲美，显示出无编码器架构在3D理解领域的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.06608', 'title': 'TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models', 'url': 'https://huggingface.co/papers/2502.06608', 'abstract': 'Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.', 'score': 22, 'issue_id': 2210, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '0602cc4a46e4c69c', 'authors': ['Yangguang Li', 'Zi-Xin Zou', 'Zexiang Liu', 'Dehu Wang', 'Yuan Liang', 'Zhipeng Yu', 'Xingchao Liu', 'Yuan-Chen Guo', 'Ding Liang', 'Wanli Ouyang', 'Yan-Pei Cao'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'The University of Texas at Austin', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2502.06608.jpg', 'data': {'categories': ['#3d', '#open_source', '#training', '#dataset', '#diffusion'], 'emoji': '🧊', 'ru': {'title': 'TripoSG: Революция в генерации трехмерных форм', 'desc': 'TripoSG - это новая парадигма генерации трехмерных форм, способная создавать высококачественные 3D-модели с точным соответствием входным изображениям. Она использует масштабный трансформер с выпрямленным потоком для генерации 3D-форм, обученный на обширных высококачественных данных. TripoSG применяет гибридную стратегию обучения, сочетающую потери SDF, нормалей и эйконала для 3D VAE. Благодаря интеграции этих компонентов, TripoSG достигает передового уровня в генерации 3D-форм с улучшенной детализацией и верностью входным изображениям.'}, 'en': {'title': 'TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques', 'desc': 'This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models.'}, 'zh': {'title': 'TripoSG：高保真3D形状生成的新范式', 'desc': '本论文介绍了一种新的3D形状生成方法TripoSG，旨在提高3D网格的生成质量。我们提出了一种大规模的流动变换器，能够在大量高质量数据上进行训练，从而实现高保真度的3D形状生成。此外，我们采用了一种混合监督训练策略，结合了SDF、法线和Eikonal损失，以提高3D重建性能。通过全面的实验验证，我们的框架在3D形状生成方面达到了最先进的性能，展现了对输入图像的高保真度和多样化的生成能力。'}}}, {'id': 'https://huggingface.co/papers/2502.09082', 'title': 'CoSER: Coordinating LLM-Based Persona Simulation of Established Roles', 'url': 'https://huggingface.co/papers/2502.09082', 'abstract': 'Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.', 'score': 20, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '3d13dc492344cf84', 'authors': ['Xintao Wang', 'Heng Wang', 'Yifei Zhang', 'Xinfeng Yuan', 'Rui Xu', 'Jen-tse Huang', 'Siyu Yuan', 'Haoran Guo', 'Jiangjie Chen', 'Wei Wang', 'Yanghua Xiao', 'Shuchang Zhou'], 'affiliations': ['Fudan University', 'Johns Hopkins University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2502.09082.jpg', 'data': {'categories': ['#dataset', '#story_generation', '#benchmark', '#agents', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'CoSER: революция в ролевом воспроизведении персонажей с помощью ИИ', 'desc': "Статья представляет CoSER - набор данных, открытые модели и протокол оценки для эффективного ролевого воспроизведения персонажей с помощью больших языковых моделей (LLM). Набор данных CoSER содержит 17 966 персонажей из 771 известной книги, включая аутентичные диалоги и разнообразные типы данных. Авторы вводят понятие 'актерской игры в заданных обстоятельствах' для обучения и оценки ролевых LLM. Разработанные модели CoSER 8B и CoSER 70B демонстрируют высокую эффективность, причем CoSER 70B превосходит или соответствует GPT-4 по нескольким критериям оценки."}, 'en': {'title': 'Empowering Role-Playing Agents with CoSER: A New Dataset and Methodology', 'desc': 'This paper introduces CoSER, a comprehensive dataset designed to enhance role-playing language agents (RPLAs) using large language models (LLMs). It includes 17,966 characters from 771 well-known books, providing authentic dialogues and various data types that reflect real-world interactions. The authors propose a novel training and evaluation method based on acting techniques, allowing LLMs to portray multiple characters in specific scenes. The results show that the CoSER 70B model outperforms existing models like GPT-4o in several benchmarks, demonstrating the effectiveness of the CoSER dataset for training and evaluating RPLAs.'}, 'zh': {'title': '提升角色扮演语言代理的有效性', 'desc': '角色扮演语言代理（RPLA）是大型语言模型（LLM）的新兴应用，但模拟已建立角色的任务具有挑战性，因为缺乏真实的角色数据集和细致的评估方法。本文介绍了CoSER，一个高质量的数据集、开放模型和评估协议，旨在有效支持已建立角色的RPLA。CoSER数据集涵盖了771本著名书籍中的17,966个角色，提供了真实对话和多样化的数据类型。通过引入给定情境表演的方法，我们训练和评估角色扮演的LLM，实验结果表明CoSER数据集在RPLA训练、评估和检索中的重要价值。'}}}, {'id': 'https://huggingface.co/papers/2502.09621', 'title': 'MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency', 'url': 'https://huggingface.co/papers/2502.09621', 'abstract': 'Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/', 'score': 19, 'issue_id': 2213, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'ac755f4f5584a58c', 'authors': ['Dongzhi Jiang', 'Renrui Zhang', 'Ziyu Guo', 'Yanwei Li', 'Yu Qi', 'Xinyan Chen', 'Liuhui Wang', 'Jianhan Jin', 'Claire Guo', 'Shen Yan', 'Bo Zhang', 'Chaoyou Fu', 'Peng Gao', 'Hongsheng Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.09621.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Оценка мультимодальных рассуждений: новый взгляд на Chain-of-Thought в LMM', 'desc': 'Статья представляет MME-CoT - специализированный бенчмарк для оценки производительности цепочки рассуждений (Chain-of-Thought) в мультимодальных языковых моделях (LMM). Исследование охватывает шесть областей и предлагает три новых метрики для оценки качества, надежности и эффективности рассуждений. Анализ показывает, что модели с механизмом рефлексии демонстрируют превосходное качество CoT, при этом Kimi k1.5 превосходит GPT-4o. Однако выявлено, что CoT может ухудшать производительность LMM в задачах, требующих восприятия, а модели с рефлексией демонстрируют значительную неэффективность.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MME-CoT', 'desc': 'This paper presents MME-CoT, a benchmark designed to evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Multimodal Models (LMMs) across various domains. It introduces three new metrics to assess reasoning quality, robustness, and efficiency in a detailed manner. The study reveals that models utilizing a reflection mechanism, like Kimi k1.5, outperform others such as GPT-4o in CoT quality, but may struggle with perception-heavy tasks due to overthinking. Overall, MME-CoT aims to enhance the understanding and development of multimodal reasoning in LMMs.'}, 'zh': {'title': 'MME-CoT：提升多模态模型推理能力的基准', 'desc': '本文介绍了MME-CoT，这是一个专门评估大型多模态模型（LMMs）在链式思维（CoT）推理性能的基准，涵盖数学、科学、光学字符识别、逻辑、时空和一般场景六个领域。我们提出了一套全面的评估工具，包含三种新颖的指标，细致评估推理质量、鲁棒性和效率。研究发现，具有反思机制的模型在CoT质量上表现优越，而在感知密集型任务中，CoT提示可能会降低LMM的性能。尽管CoT质量较高，但具有反思的LMM在正常响应和自我修正阶段表现出显著的低效率。'}}}, {'id': 'https://huggingface.co/papers/2502.09100', 'title': 'Logical Reasoning in Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2502.09100', 'abstract': 'With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.', 'score': 17, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '72b32d40c559c7e4', 'authors': ['Hanmeng Liu', 'Zhizhang Fu', 'Mengru Ding', 'Ruoxi Ning', 'Chaoli Zhang', 'Xiaozhang Liu', 'Yue Zhang'], 'affiliations': ['Hainan University', 'Westlake University', 'Zhejiang Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09100.jpg', 'data': {'categories': ['#training', '#survey', '#reasoning', '#rl', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Логическое мышление в больших языковых моделях: прогресс и перспективы', 'desc': 'Этот обзор посвящен последним достижениям в области логического рассуждения в больших языковых моделях (LLM). В нем рассматриваются теоретические основы и методы оценки способностей LLM к логическому мышлению. Авторы анализируют существующие возможности в различных парадигмах рассуждений, включая дедуктивное, индуктивное, абдуктивное и аналогическое. Также обсуждаются стратегии улучшения производительности рассуждений, такие как настройка данных, обучение с подкреплением и нейросимволические подходы.'}, 'en': {'title': 'Enhancing Logical Reasoning in Large Language Models', 'desc': 'This paper reviews the progress made in enhancing logical reasoning capabilities of large language models (LLMs) like OpenAI o3 and DeepSeek-R1. It discusses various reasoning paradigms such as deductive, inductive, abductive, and analogical reasoning, highlighting their theoretical foundations and evaluation benchmarks. The authors analyze strategies to improve reasoning performance, including data-centric tuning and neuro-symbolic approaches. The paper concludes by suggesting future research directions to further develop logical reasoning in AI systems.'}, 'zh': {'title': '提升AI系统逻辑推理能力的探索', 'desc': '随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展现了出色的推理能力。然而，它们在进行严格逻辑推理方面的能力仍然是一个未解之谜。本文综述了LLMs中逻辑推理的最新进展，探讨了逻辑推理的范围、理论基础以及评估推理能力的基准。我们分析了不同推理范式（如演绎、归纳、溯因和类比）的现有能力，并评估了增强推理性能的策略，包括数据中心调优、强化学习、解码策略和神经符号方法。'}}}, {'id': 'https://huggingface.co/papers/2502.09042', 'title': 'Typhoon T1: An Open Thai Reasoning Model', 'url': 'https://huggingface.co/papers/2502.09042', 'abstract': 'This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.', 'score': 15, 'issue_id': 2213, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '5cb078b546437366', 'authors': ['Pittawat Taveekitworachai', 'Potsawee Manakul', 'Kasima Tharnpipitchai', 'Kunat Pipatanakul'], 'affiliations': ['SCB 10X R&D SCBX Group Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2502.09042.jpg', 'data': {'categories': ['#reasoning', '#training', '#multilingual', '#dataset', '#open_source', '#low_resource', '#data', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Открытая модель рассуждений для тайского языка', 'desc': 'Статья представляет Typhoon T1 - открытую модель рассуждений на тайском языке. Модель рассуждений - это новый тип генеративной модели, построенной на основе больших языковых моделей (LLM). Она генерирует длинную цепочку мыслей перед выдачей окончательного ответа, что улучшает производительность на сложных задачах. Авторы делятся деталями разработки такой модели с использованием обучения с учителем на открытых датасетах вместо обучения с подкреплением.'}, 'en': {'title': 'Typhoon T1: Advancing Thai Reasoning Models for Complex Tasks', 'desc': 'This paper presents Typhoon T1, an initiative to create an open reasoning model specifically for the Thai language. Reasoning models are advanced generative models that produce a sequence of thoughts leading to a conclusion, enhancing performance on intricate tasks. The authors focus on developing this model using supervised fine-tuning with open datasets, rather than relying on reinforcement learning, making it more accessible and cost-effective. The paper details the synthetic data generation process, training methods, and shares the model weights, aiming to support future research in low-resource language reasoning models.'}, 'zh': {'title': '开放泰语推理模型的创新之路', 'desc': '本文介绍了Typhoon T1，这是一个开发开放泰语推理模型的项目。推理模型是一种新型的生成模型，基于大型语言模型（LLMs）构建，能够在得出最终答案之前生成长链思维，从而提高复杂任务的表现。Typhoon T1通过利用监督微调和开放数据集，以更具成本效益的方式深入探讨推理模型的开发，避免了强化学习的复杂性。我们希望这个开放项目为该领域的进一步研究奠定基础。'}}}, {'id': 'https://huggingface.co/papers/2502.09390', 'title': 'SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models', 'url': 'https://huggingface.co/papers/2502.09390', 'abstract': "In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.", 'score': 12, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '4caf9cec56011350', 'authors': ['Daniel Fleischer', 'Moshe Berchansky', 'Gad Markovits', 'Moshe Wasserblat'], 'affiliations': ['IntelLabs'], 'pdf_title_img': 'assets/pdf/title_img/2502.09390.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'SQuARE: самоопрос для улучшения рассуждений ИИ', 'desc': 'В статье представлен новый метод промптинга под названием SQuARE, который улучшает способности больших языковых моделей к рассуждению. SQuARE побуждает модели генерировать и решать вспомогательные вопросы перед ответом на основной запрос, что способствует более глубокому исследованию темы. Эксперименты с моделями Llama 3 и GPT-4 показали, что SQuARE значительно превосходит традиционные методы цепочки размышлений и существующие методы перефразирования. Предложенный подход систематически декомпозирует запросы, повышая эффективность языковых моделей в задачах, требующих рассуждений.'}, 'en': {'title': 'Unlocking Deeper Reasoning with SQuARE', 'desc': "This paper presents SQuARE, a new prompting technique aimed at enhancing the reasoning abilities of Large Language Models (LLMs) in Natural Language Processing. Unlike traditional methods that may not fully utilize a model's reasoning potential, SQuARE employs a self-interrogation approach, encouraging models to generate and answer multiple auxiliary questions before addressing the main question. This method builds on existing chain-of-thought frameworks, allowing for a deeper exploration of topics. Evaluations with Llama 3 and GPT-4o show that SQuARE outperforms conventional prompting techniques, leading to improved reasoning in question-answering tasks."}, 'zh': {'title': 'SQuARE：提升推理能力的新方法', 'desc': '在自然语言处理领域，大型语言模型（LLMs）面临越来越复杂的推理挑战。传统的链式思维提示方法虽然有一定效果，但往往无法充分发挥模型的推理能力。本文提出了一种新颖的提示技术SQuARE（顺序问答推理引擎），通过自我质询的方式来改善推理过程。SQuARE在多个问答数据集上的评估结果显示，其在推理任务中的表现显著优于传统的链式思维提示和现有的重述-回应方法。'}}}, {'id': 'https://huggingface.co/papers/2502.09601', 'title': 'CoT-Valve: Length-Compressible Chain-of-Thought Tuning', 'url': 'https://huggingface.co/papers/2502.09601', 'abstract': "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.", 'score': 11, 'issue_id': 2212, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '4b0518724dea8b37', 'authors': ['Xinyin Ma', 'Guangnian Wan', 'Runpeng Yu', 'Gongfan Fang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.09601.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#optimization', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эластичное управление длиной цепочки рассуждений в моделях машинного обучения', 'desc': 'Статья представляет новую стратегию обучения и вывода под названием CoT-Valve, которая позволяет моделям генерировать цепочки рассуждений различной длины. Авторы предлагают идентифицировать направление в пространстве параметров, манипулируя которым, можно эффективно контролировать длину генерируемой цепочки рассуждений. Они разрабатывают методы точного обучения сжимаемых цепочек рассуждений и прогрессивного сжатия длины цепочки. Эксперименты показывают, что CoT-Valve успешно обеспечивает управляемость и сжимаемость цепочки, превосходя контроль на основе промптов.'}, 'en': {'title': 'Dynamic Reasoning Control with CoT-Valve', 'desc': 'This paper presents CoT-Valve, a new strategy for controlling the length of reasoning chains in machine learning models, particularly in Chain-of-Thought (CoT) reasoning. The authors observe that while longer reasoning paths improve performance, they also increase inference costs, especially on harder tasks. CoT-Valve allows a single model to dynamically adjust the length of its reasoning based on task difficulty, thus optimizing efficiency. The experiments demonstrate that this method not only compresses reasoning chains significantly but also maintains high performance compared to traditional prompt-based controls.'}, 'zh': {'title': '动态控制推理链长度的创新策略', 'desc': '本文提出了一种名为CoT-Valve的新策略，用于动态控制推理链的长度，以降低推理模型的计算开销。研究发现，推理路径在简单任务中容易压缩，但在困难任务中则较为困难，因此我们探索了如何在单一模型中实现这一目标。通过调整参数空间中的方向，CoT-Valve能够有效控制生成的推理链长度，并且在压缩推理链方面表现出色。实验结果表明，CoT-Valve在控制和压缩推理链方面优于基于提示的控制方法，且在性能上仅有轻微下降。'}}}, {'id': 'https://huggingface.co/papers/2502.08468', 'title': 'mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data', 'url': 'https://huggingface.co/papers/2502.08468', 'abstract': 'Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.', 'score': 10, 'issue_id': 2211, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'ac7814f15d1e9616', 'authors': ['Haonan Chen', 'Liang Wang', 'Nan Yang', 'Yutao Zhu', 'Ziliang Zhao', 'Furu Wei', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Microsoft Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2502.08468.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#synthetic', '#dataset', '#training', '#data', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Синтетические данные открывают новые горизонты для мультимодальных эмбеддингов', 'desc': 'В статье представлена новая модель мультимодальных эмбеддингов mmE5, обученная на синтетических данных высокого качества. Авторы определили три критерия для создания таких данных: широкий охват задач и модальностей, надежное кросс-модальное выравнивание и высокая достоверность. Используя эти принципы, они синтезировали наборы данных с помощью мультимодальной большой языковой модели. Эксперименты показали, что mmE5 достигает передовых результатов на бенчмарках MMEB и XTD.'}, 'en': {'title': 'Enhancing Multimodal Learning with High-Quality Synthetic Data', 'desc': 'This paper discusses the development of multimodal embedding models that integrate different types of data, like text and images, into a single representation. It highlights the challenge of limited labeled multimodal data and proposes a solution through high-quality synthetic data generation. The authors establish three key criteria for effective synthetic data: broad scope, robust cross-modal alignment, and high fidelity. By adhering to these principles, they create a multimodal multilingual E5 model, mmE5, which demonstrates exceptional performance on various benchmarks.'}, 'zh': {'title': '高质量合成数据助力多模态模型', 'desc': '多模态嵌入模型能够将文本和图像等不同模态的数据映射到统一的表示空间，但有限的标注多模态数据常常影响嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。通过这些标准，我们合成了覆盖多种任务和模态组合的数据集，并利用多模态大语言模型进行生成。最终，我们训练的多模态多语言E5模型mmE5在MMEB基准测试中表现出色，并在XTD基准测试中展现了卓越的多语言性能。'}}}, {'id': 'https://huggingface.co/papers/2502.08680', 'title': 'Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges', 'url': 'https://huggingface.co/papers/2502.08680', 'abstract': "Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates-up to 14 percentage points-as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs' mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models.", 'score': 9, 'issue_id': 2221, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '7d5f08a6e4748cfc', 'authors': ['Safal Shrestha', 'Minwu Kim', 'Keith Ross'], 'affiliations': ['New York University Abu Dhabi'], 'pdf_title_img': 'assets/pdf/title_img/2502.08680.jpg', 'data': {'categories': ['#benchmark', '#math', '#dataset', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Новый подход к оценке математических способностей языковых моделей', 'desc': 'Статья представляет новый метод оценки математических рассуждений в больших языковых моделях (LLM). Авторы предлагают генератор данных GSM-Ranges, который систематически изменяет числовые значения в математических задачах для оценки устойчивости моделей к различным числовым масштабам. Также вводится новая методология оценки, различающая логические и нелогические ошибки. Эксперименты показали значительное увеличение логических ошибок при повышении числовой сложности, выявляя общую слабость в рассуждениях с нестандартными числовыми значениями.'}, 'en': {'title': 'Enhancing Mathematical Reasoning Evaluation in LLMs with GSM-Ranges', 'desc': 'This paper addresses the limitations of evaluating mathematical reasoning in Large Language Models (LLMs) using benchmarks with narrow numerical ranges. It introduces GSM-Ranges, a dataset generator that modifies numerical values in math problems to test model robustness across different scales. The authors also present a new grading method that differentiates between logical and non-logical errors, enhancing the evaluation of reasoning processes. Experiments show that as numerical complexity increases, models struggle more with logical reasoning, especially in word problems, highlighting areas for improvement in numerical generalization.'}, 'zh': {'title': '提升语言模型的数学推理能力', 'desc': '这篇论文探讨了大型语言模型（LLMs）在数学推理方面的局限性，特别是在处理不同数值范围的问题时。作者提出了GSM-Ranges数据集生成器，通过系统性地改变数学问题中的数值，来评估模型在不同数值规模下的鲁棒性。此外，论文还提出了一种新的评分方法，可以区分逻辑错误和非逻辑错误，从而更准确地评估推理过程。实验结果显示，随着数值复杂性的增加，模型的逻辑错误率显著上升，表明在处理超出训练范围的数值时，模型存在普遍的推理弱点。'}}}, {'id': 'https://huggingface.co/papers/2502.09614', 'title': 'DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References', 'url': 'https://huggingface.co/papers/2502.09614', 'abstract': "We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.", 'score': 9, 'issue_id': 2216, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'f83cdb806eef0812', 'authors': ['Xueyi Liu', 'Jianibieke Adalibieke', 'Qianwei Han', 'Yuzhe Qin', 'Li Yi'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.09614.jpg', 'data': {'categories': ['#robotics', '#training', '#optimization', '#rl', '#games', '#agents'], 'emoji': '🦾', 'ru': {'title': 'Нейронный контроллер для ловких манипуляций роботизированной руки', 'desc': 'Статья описывает разработку нейронного контроллера для отслеживания движений при манипуляциях с объектами роботизированной рукой. Авторы предлагают подход, основанный на обучении на большом наборе демонстраций успешного отслеживания движений человека. Используется итеративное улучшение контроллера с помощью комбинации обучения с подкреплением и имитационного обучения. Метод включает оптимизацию отдельных траекторий с использованием гомотопической оптимизации для повышения разнообразия демонстраций.'}, 'en': {'title': 'Empowering Robots with Human-Like Dexterity through Neural Tracking!', 'desc': 'This paper presents a novel neural tracking controller designed for dexterous manipulation of objects by a robot hand, using human references as a guide. The authors tackle the complexities of contact dynamics and the need for the controller to be adaptable and robust across various tasks. They propose a method that combines reinforcement learning and imitation learning, leveraging a large dataset of successful robot tracking demonstrations to improve performance iteratively. The results show a significant improvement in success rates, demonstrating the effectiveness of their approach in both simulated and real-world environments.'}, 'zh': {'title': '通用神经控制器：提升灵巧操作的成功率', 'desc': '本文提出了一种通用的神经跟踪控制器，用于从人类参考中进行灵巧操作。该控制器旨在管理灵巧机器人手，以操控多种物体，适应不同的人机交互需求。我们的方法通过大规模成功的机器人跟踪示例来训练控制器，并结合强化学习和模仿学习来提升其在动态环境中的表现。最终，我们在仿真和现实世界中评估了该控制器，成功率比现有方法提高了10%以上。'}}}, {'id': 'https://huggingface.co/papers/2502.05761', 'title': '3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly', 'url': 'https://huggingface.co/papers/2502.05761', 'abstract': 'Industrial anomaly detection achieves progress thanks to datasets such as MVTec-AD and VisA. However, they suf- fer from limitations in terms of the number of defect sam- ples, types of defects, and availability of real-world scenes. These constraints inhibit researchers from further exploring the performance of industrial detection with higher accuracy. To this end, we propose a new large-scale anomaly detection dataset called 3CAD, which is derived from real 3C produc- tion lines. Specifically, the proposed 3CAD includes eight different types of manufactured parts, totaling 27,039 high- resolution images labeled with pixel-level anomalies. The key features of 3CAD are that it covers anomalous regions of different sizes, multiple anomaly types, and the possibility of multiple anomalous regions and multiple anomaly types per anomaly image. This is the largest and first anomaly de- tection dataset dedicated to 3C product quality control for community exploration and development. Meanwhile, we in- troduce a simple yet effective framework for unsupervised anomaly detection: a Coarse-to-Fine detection paradigm with Recovery Guidance (CFRG). To detect small defect anoma- lies, the proposed CFRG utilizes a coarse-to-fine detection paradigm. Specifically, we utilize a heterogeneous distilla- tion model for coarse localization and then fine localiza- tion through a segmentation model. In addition, to better capture normal patterns, we introduce recovery features as guidance. Finally, we report the results of our CFRG frame- work and popular anomaly detection methods on the 3CAD dataset, demonstrating strong competitiveness and providing a highly challenging benchmark to promote the development of the anomaly detection field. Data and code are available: https://github.com/EnquanYang2022/3CAD.', 'score': 5, 'issue_id': 2215, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'f7f244334d53bca7', 'authors': ['Enquan Yang', 'Peng Xing', 'Hanyang Sun', 'Wenbo Guo', 'Yuanwei Ma', 'Zechao Li', 'Dan Zeng'], 'affiliations': ['Changzhou Microintelligence Corporation', 'Nanjing University of Science and Technology', 'School of Communication and Information Engineering, Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05761.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset'], 'emoji': '🔍', 'ru': {'title': '3CAD: Крупномасштабный датасет для обнаружения промышленных аномалий', 'desc': 'Авторы представляют новый крупномасштабный набор данных для обнаружения аномалий под названием 3CAD, полученный с реальных производственных линий 3C. Этот датасет включает 27,039 изображений высокого разрешения с пиксельной разметкой аномалий для восьми различных типов производственных деталей. 3CAD отличается разнообразием размеров аномальных областей, множеством типов аномалий и возможностью наличия нескольких аномальных регионов и типов аномалий на одном изображении. Также авторы предлагают эффективный фреймворк для обнаружения аномалий без учителя, названный CFRG, который использует парадигму обнаружения от грубого к точному с руководством по восстановлению.'}, 'en': {'title': '3CAD: A New Benchmark for Anomaly Detection in 3C Products', 'desc': 'This paper introduces a new dataset called 3CAD for industrial anomaly detection, specifically focusing on 3C product quality control. The 3CAD dataset contains 27,039 high-resolution images with pixel-level annotations for eight types of manufactured parts, addressing the limitations of existing datasets like MVTec-AD and VisA. To enhance anomaly detection, the authors propose a Coarse-to-Fine detection framework with Recovery Guidance (CFRG), which improves the localization of small defects by first identifying coarse regions and then refining the detection through segmentation. The results show that the CFRG framework is competitive with existing methods, providing a valuable resource for advancing research in anomaly detection.'}, 'zh': {'title': '3CAD：推动3C产品异常检测的新数据集', 'desc': '本论文提出了一个新的大规模异常检测数据集，名为3CAD，专注于3C产品的质量控制。该数据集包含27,039张高分辨率图像，标注了不同类型和大小的像素级异常。为了提高异常检测的准确性，论文还介绍了一种简单有效的无监督异常检测框架，称为CFRG，采用粗到细的检测范式。通过该框架，研究人员可以更好地捕捉正常模式并定位小缺陷，推动异常检测领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.09613', 'title': 'Latent Radiance Fields with 3D-aware 2D Representations', 'url': 'https://huggingface.co/papers/2502.09613', 'abstract': 'Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering performance. To address this challenge, we propose a novel framework that integrates 3D awareness into the 2D latent space. The framework consists of three stages: (1) a correspondence-aware autoencoding method that enhances the 3D consistency of 2D latent representations, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field (VAE-RF) alignment strategy that improves image decoding from the rendered 2D representations. Extensive experiments demonstrate that our method outperforms the state-of-the-art latent 3D reconstruction approaches in terms of synthesis performance and cross-dataset generalizability across diverse indoor and outdoor scenes. To our knowledge, this is the first work showing the radiance field representations constructed from 2D latent representations can yield photorealistic 3D reconstruction performance.', 'score': 4, 'issue_id': 2231, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'edb945270af8ccb2', 'authors': ['Chaoyi Zhou', 'Xi Liu', 'Feng Luo', 'Siyu Huang'], 'affiliations': ['Visual Computing Division, School of Computing, Clemson University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09613.jpg', 'data': {'categories': ['#3d', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение 3D-реконструкции через интеграцию трехмерного понимания в латентное пространство', 'desc': 'Статья представляет новую систему для улучшения трехмерной реконструкции изображений. Авторы предлагают метод, интегрирующий трехмерное понимание в двумерное латентное пространство. Система состоит из трех этапов: автоэнкодинга с учетом соответствий, латентного поля излучения и стратегии выравнивания VAE-RF. Эксперименты показывают, что данный подход превосходит современные методы латентной 3D-реконструкции по качеству синтеза и обобщаемости на разных наборах данных.'}, 'en': {'title': 'Bridging 2D and 3D: A New Era in Latent Reconstruction', 'desc': 'This paper presents a new framework for improving 3D reconstruction from 2D images by addressing the gap between 2D features and 3D representations. The proposed method enhances 3D consistency in 2D latent representations through a correspondence-aware autoencoder. It then transforms these enhanced representations into 3D using a latent radiance field, followed by an alignment strategy to optimize image decoding. The results show that this approach significantly outperforms existing methods in generating realistic 3D models from 2D data across various environments.'}, 'zh': {'title': '将2D潜在表示转化为真实感3D重建的创新框架', 'desc': '潜在3D重建在3D语义理解和生成方面展现了巨大潜力，但现有方法在2D特征空间与3D表示之间存在领域差距，导致渲染性能下降。为了解决这个问题，我们提出了一种新颖的框架，将3D意识整合到2D潜在空间中。该框架包括三个阶段：首先是增强2D潜在表示的3D一致性的对应感知自编码方法；其次是将这些3D感知的2D表示提升到3D空间的潜在辐射场（LRF）；最后是改进从渲染的2D表示中解码图像的VAE-辐射场（VAE-RF）对齐策略。大量实验表明，我们的方法在合成性能和跨数据集的泛化能力方面优于现有的潜在3D重建方法。'}}}, {'id': 'https://huggingface.co/papers/2502.05979', 'title': 'VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer', 'url': 'https://huggingface.co/papers/2502.05979', 'abstract': 'Crafting magic and illusions is one of the most thrilling aspects of filmmaking, with visual effects (VFX) serving as the powerhouse behind unforgettable cinematic experiences. While recent advances in generative artificial intelligence have driven progress in generic image and video synthesis, the domain of controllable VFX generation remains relatively underexplored. In this work, we propose a novel paradigm for animated VFX generation as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images.   Our work makes two primary contributions: (i) Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with textual descriptions, instance segmentation masks for spatial conditioning, and start-end timestamps for temporal control. (ii) VFX Creator, a simple yet effective controllable VFX generation framework based on a Video Diffusion Transformer. The model incorporates a spatial and temporal controllable LoRA adapter, requiring minimal training videos. Specifically, a plug-and-play mask control module enables instance-level spatial manipulation, while tokenized start-end motion timestamps embedded in the diffusion process, alongside the text encoder, allow precise temporal control over effect timing and pace.   Extensive experiments on the Open-VFX test set demonstrate the superiority of the proposed system in generating realistic and dynamic effects, achieving state-of-the-art performance and generalization ability in both spatial and temporal controllability. Furthermore, we introduce a specialized metric to evaluate the precision of temporal control. By bridging traditional VFX techniques with generative approaches, VFX Creator unlocks new possibilities for efficient and high-quality video effect generation, making advanced VFX accessible to a broader audience.', 'score': 4, 'issue_id': 2220, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'c31da576f1519381', 'authors': ['Xinyu Liu', 'Ailing Zeng', 'Wei Xue', 'Harry Yang', 'Wenhan Luo', 'Qifeng Liu', 'Yike Guo'], 'affiliations': ['Hong Kong University of Science and Technology, China', 'Tencent AI Lab, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05979.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#benchmark', '#video', '#open_source', '#cv'], 'emoji': '🎬', 'ru': {'title': 'Революция в создании VFX: ИИ открывает новые горизонты кинопроизводства', 'desc': 'Статья представляет новый подход к генерации анимированных визуальных эффектов (VFX) с использованием генеративного искусственного интеллекта. Авторы предлагают Open-VFX - первый высококачественный набор данных VFX-видео, и VFX Creator - фреймворк для контролируемой генерации VFX на основе трансформера видеодиффузии. Система использует пространственно-временной контролируемый LoRA-адаптер и модуль управления масками для точного контроля эффектов. Эксперименты показывают превосходство предложенной системы в генерации реалистичных и динамичных эффектов с высокой степенью контроля.'}, 'en': {'title': 'Revolutionizing VFX Generation with AI: Control at Your Fingertips!', 'desc': 'This paper presents a new method for generating visual effects (VFX) in films using artificial intelligence. It introduces Open-VFX, a comprehensive dataset that includes various effect categories, textual descriptions, and detailed annotations for better control over the generated effects. The authors also propose VFX Creator, a framework that utilizes a Video Diffusion Transformer to create dynamic VFX from simple text prompts and static images, allowing for both spatial and temporal adjustments. The results show that this approach outperforms existing methods, making high-quality VFX generation more accessible and efficient.'}, 'zh': {'title': '开启可控视觉特效生成的新纪元', 'desc': '本文提出了一种新的动画视觉特效生成范式，称为图像动画，用户可以通过文本描述和静态参考图像生成动态特效。我们创建了Open-VFX数据集，这是第一个涵盖15种不同特效类别的高质量VFX视频数据集，包含文本描述和实例分割掩码。我们还开发了VFX Creator，一个基于视频扩散变换器的可控VFX生成框架，能够实现空间和时间的精确控制。通过将传统VFX技术与生成方法结合，VFX Creator为高效和高质量的视频特效生成开辟了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2502.08127', 'title': 'Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance', 'url': 'https://huggingface.co/papers/2502.08127', 'abstract': 'Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.', 'score': 37, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'fa3f08993ba529cd', 'authors': ['Lingfei Qian', 'Weipeng Zhou', 'Yan Wang', 'Xueqing Peng', 'Jimin Huang', 'Qianqian Xie'], 'affiliations': ['TheFinAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.08127.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#rl', '#open_source', '#training', '#long_context'], 'emoji': '💹', 'ru': {'title': 'Специализация языковых моделей - ключ к успеху в финансовом анализе', 'desc': 'В этом исследовании оценивается эффективность 16 мощных языковых моделей в решении сложных финансовых задач. Авторы обнаружили, что улучшение наборов данных и предварительное обучение повышают способности моделей к финансовым рассуждениям. Они разработали специализированную модель на основе Llama-3.1-8B-Instruct, которая превзошла даже более крупные модели в финансовых задачах. Исследование подчеркивает необходимость адаптации моделей к специфике финансовой области.'}, 'en': {'title': 'Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations', 'desc': 'This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking.'}, 'zh': {'title': '金融推理模型的创新与提升', 'desc': '本研究评估了16种强大的语言模型在金融推理任务中的表现。这些任务包括金融文本、表格数据和方程式，涉及数值推理、表格解读和金融术语理解等方面。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但通用的增强方法如链式推理微调并不总是有效。为了解决这些问题，我们开发了一种基于Llama-3.1-8B-Instruct的金融推理增强模型，经过微调和强化学习后，在多个任务上实现了10%的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07870', 'title': 'TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation', 'url': 'https://huggingface.co/papers/2502.07870', 'abstract': 'Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.', 'score': 32, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '5b552b320e2e69f0', 'authors': ['Alex Jinpeng Wang', 'Dongxing Mao', 'Jiawei Zhang', 'Weiming Han', 'Zhuobai Dong', 'Linjie Li', 'Yiqi Lin', 'Zhengyuan Yang', 'Libo Qin', 'Fuwei Zhang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft', 'National University of Singapore', 'North University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07870.jpg', 'data': {'categories': ['#dataset', '#open_source', '#long_context', '#benchmark', '#cv'], 'emoji': '📚', 'ru': {'title': 'TextAtlas5M: Новый горизонт в генерации изображений с длинным текстом', 'desc': 'Статья представляет новый датасет TextAtlas5M для оценки генерации изображений с длинным текстом. Датасет содержит 5 миллионов изображений с разнообразными типами данных, что позволяет всесторонне оценивать крупномасштабные генеративные модели. Авторы также создали тестовый набор TextAtlasEval из 3000 улучшенных человеком изображений. Оценка показала, что даже самые продвинутые проприетарные модели (например, GPT4o с DallE-3) сталкиваются со значительными трудностями при работе с этим набором данных.'}, 'en': {'title': 'Empowering Long-Text Image Generation with TextAtlas5M', 'desc': 'This paper introduces TextAtlas5M, a new dataset aimed at improving text-conditioned image generation, particularly for long-form text. Current models struggle with generating images that include dense and intricate text, as existing datasets primarily focus on shorter text. TextAtlas5M contains 5 million images with long text, allowing for better evaluation of generative models. The paper also presents TextAtlasEval, a curated test set that highlights the challenges faced by both proprietary and open-source models in this area.'}, 'zh': {'title': '长文本图像生成的新突破', 'desc': '本文介绍了一个新的数据集TextAtlas5M，旨在解决文本条件下图像生成中长文本渲染的挑战。现有的数据集通常只关注短文本，限制了生成模型的能力。TextAtlas5M包含500万张长文本生成的图像，覆盖多种数据类型，为大规模生成模型的评估提供了基础。通过建立3000个经过人工改进的测试集TextAtlasEval，本文为文本条件生成提供了一个广泛的基准，帮助未来的研究和模型训练。'}}}, {'id': 'https://huggingface.co/papers/2502.08590', 'title': 'Light-A-Video: Training-free Video Relighting via Progressive Light Fusion', 'url': 'https://huggingface.co/papers/2502.08590', 'abstract': "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.", 'score': 32, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'dcc03282320e88b1', 'authors': ['Yujie Zhou', 'Jiazi Bu', 'Pengyang Ling', 'Pan Zhang', 'Tong Wu', 'Qidong Huang', 'Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Anyi Rao', 'Jiaqi Wang', 'Li Niu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08590.jpg', 'data': {'categories': ['#video', '#diffusion', '#cv'], 'emoji': '💡', 'ru': {'title': 'Плавное переосвещение видео без обучения', 'desc': 'Эта статья представляет Light-A-Video - подход к переосвещению видео без дополнительного обучения. Авторы предлагают два ключевых метода для улучшения согласованности освещения: модуль Consistent Light Attention (CLA) для стабилизации генерации фонового источника света и стратегию Progressive Light Fusion (PLF) для плавных переходов освещения между кадрами. Light-A-Video адаптирует модели переосвещения изображений для видео, решая проблемы несогласованности источника света и мерцания. Эксперименты показывают, что метод улучшает временную согласованность переосвещенного видео, сохраняя качество изображения.'}, 'en': {'title': 'Achieving Smooth Video Relighting with Light-A-Video', 'desc': 'This paper presents Light-A-Video, a novel approach for video relighting that addresses the challenges of lighting consistency and flickering in generated videos. Unlike traditional methods that apply image relighting on a frame-by-frame basis, Light-A-Video utilizes a training-free strategy to enhance temporal smoothness. It introduces a Consistent Light Attention (CLA) module to improve interactions between frames and stabilize background lighting. Additionally, the Progressive Light Fusion (PLF) technique is employed to blend the original and relighted appearances, ensuring coherent lighting transitions across video frames.'}, 'zh': {'title': '实现视频重光的一致性与平滑性', 'desc': '最近，图像重光模型的进展得益于大规模数据集和预训练的扩散模型，使得一致的光照效果得以实现。然而，视频重光仍然滞后，主要是由于训练成本高和缺乏多样化的高质量视频重光数据集。简单地将图像重光模型逐帧应用会导致光源不一致和重光外观不一致，从而在生成的视频中产生闪烁现象。我们提出了Light-A-Video，这是一种无训练的方法，旨在实现时间上平滑的视频重光。'}}}, {'id': 'https://huggingface.co/papers/2502.07346', 'title': 'BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models', 'url': 'https://huggingface.co/papers/2502.07346', 'abstract': 'Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.', 'score': 31, 'issue_id': 2192, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '8a6e122fc0804618', 'authors': ['Xu Huang', 'Wenhao Zhu', 'Hanxu Hu', 'Conghui He', 'Lei Li', 'Shujian Huang', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2502.07346.jpg', 'data': {'categories': ['#long_context', '#low_resource', '#machine_translation', '#multilingual', '#dataset', '#open_source', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'BenchMAX: Многоязычная оценка продвинутых возможностей языковых моделей', 'desc': 'BenchMAX - это новый многоязычный бенчмарк для оценки продвинутых возможностей больших языковых моделей (LLM) в 17 языках. Он фокусируется на таких задачах, как следование инструкциям, рассуждение, понимание длинного контекста и генерация кода. Каждый пример аннотируется тремя носителями языка для обеспечения высокого качества. Эксперименты показали различную эффективность ключевых возможностей LLM в разных языках, что нельзя преодолеть простым увеличением размера модели.'}, 'en': {'title': 'BenchMAX: Bridging Multilingual Gaps in Advanced Language Model Evaluation', 'desc': 'This paper introduces BenchMAX, a new multilingual evaluation benchmark designed to assess advanced capabilities of large language models (LLMs) such as instruction following, reasoning, and code generation across multiple languages. Unlike previous benchmarks that focused on simpler tasks, BenchMAX allows for a fair comparison of these complex abilities by using machine-translated data and independent annotations from native speakers. The study reveals significant performance gaps in LLMs when evaluated on these advanced tasks, indicating that merely increasing model size is not sufficient to improve multilingual proficiency. BenchMAX aims to enhance the development of multilingual models by providing a robust platform for evaluation and research.'}, 'zh': {'title': 'BenchMAX：多语言能力评估的新基准', 'desc': '本文介绍了BenchMAX，这是一个多语言评估基准，旨在比较大型语言模型在指令遵循、推理、长文本理解和代码生成等高级能力上的表现。以往的多语言基准主要关注简单理解任务，而BenchMAX则填补了这一空白，允许对不同语言的能力进行公平比较。为了确保数据质量，三位母语评审员独立对每个样本进行标注，确保评估的准确性。实验结果显示，不同语言在核心能力上的表现差异，表明仅仅增加模型规模无法解决这些性能差距。'}}}, {'id': 'https://huggingface.co/papers/2502.08639', 'title': 'CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.08639', 'abstract': 'In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.', 'score': 28, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '3d3890b6b6bf7904', 'authors': ['Qinghe Wang', 'Yawen Luo', 'Xiaoyu Shi', 'Xu Jia', 'Huchuan Lu', 'Tianfan Xue', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai'], 'affiliations': ['Dalian University of Technology', 'Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.08639.jpg', 'data': {'categories': ['#dataset', '#video', '#diffusion', '#3d', '#games'], 'emoji': '🎬', 'ru': {'title': 'CineMaster: Режиссируйте свое видео в 3D', 'desc': 'CineMaster - это новая система для создания 3D-ориентированного и контролируемого видео на основе текста. Она позволяет пользователям точно размещать объекты в сцене, гибко манипулировать объектами и камерой в 3D-пространстве. Система работает в два этапа: сначала пользователь интерактивно создает 3D-сигналы управления, затем эти сигналы используются для управления диффузионной моделью генерации видео. Для обучения была разработана автоматизированная система аннотации видеоданных с извлечением 3D-ограничивающих рамок и траекторий камеры.'}, 'en': {'title': 'Empowering Video Creation with 3D Control', 'desc': 'CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions.'}, 'zh': {'title': 'CineMaster：让视频生成如导演般可控', 'desc': 'CineMaster是一个新颖的框架，用于生成具有3D感知和可控性的文本到视频。它使用户能够像专业电影导演一样精确控制场景中的物体位置、灵活操作3D空间中的物体和相机，并直观地布局渲染帧。该框架分为两个阶段：第一阶段通过交互式工作流程构建3D感知的条件信号，第二阶段利用这些信号指导文本到视频的扩散模型生成用户所需的视频内容。此外，CineMaster还建立了一个自动化数据注释管道，以解决缺乏3D物体运动和相机姿态标注的数据集问题。'}}}, {'id': 'https://huggingface.co/papers/2502.07864', 'title': 'TransMLA: Multi-head Latent Attention Is All You Need', 'url': 'https://huggingface.co/papers/2502.07864', 'abstract': 'Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.', 'score': 21, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'dbff84dafe8c2312', 'authors': ['Fanxu Meng', 'Zengwei Yao', 'Muhan Zhang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, Peking University', 'Xiaomi Corp., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07864.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'Революция в архитектуре внимания: MLA для эффективных языковых моделей', 'desc': 'Статья представляет новый метод под названием Multi-head Latent Attention (MLA), который решает проблему коммуникационных узких мест в больших языковых моделях. MLA использует матрицы низкого ранга в слоях ключ-значение, что позволяет сжимать и кэшировать латентные состояния KV. Авторы также предлагают метод TransMLA для преобразования предобученных моделей на основе Group Query Attention (GQA) в модели на основе MLA. Этот подход позволяет значительно уменьшить размер KV-кэша и ускорить вывод, сохраняя при этом выразительность модели.'}, 'en': {'title': 'Transforming Attention: From GQA to Efficient MLA', 'desc': 'This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size.'}, 'zh': {'title': '提升语言模型效率的关键：多头潜在注意力', 'desc': '现代大型语言模型（LLMs）在当前硬件上常常面临通信瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在键值（KV）层中使用低秩矩阵来解决这个问题，从而允许压缩的潜在KV状态被缓存。这种方法显著减少了KV缓存的大小，相比传统的多头注意力，推理速度更快。此外，MLA使用上投影矩阵来增加表达能力，以额外的计算换取减少的通信开销。'}}}, {'id': 'https://huggingface.co/papers/2502.08047', 'title': 'WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation', 'url': 'https://huggingface.co/papers/2502.08047', 'abstract': 'Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.', 'score': 20, 'issue_id': 2190, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '0f83dccb05181f21', 'authors': ['Henry Hengyuan Zhao', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.08047.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'Критическое мышление для улучшения автоматизации GUI', 'desc': 'Статья представляет WorldGUI - новый бенчмарк для оценки графических интерфейсов, который симулирует реальные взаимодействия пользователей с компьютером в различных начальных состояниях. Авторы также предлагают фреймворк GUI-Thinker, использующий механизм критического мышления для эффективного управления непредсказуемостью и сложностью взаимодействий с GUI. Эксперименты показывают, что GUI-Thinker значительно превосходит Claude-3.5 (Computer Use) по показателю успешности выполнения задач в WorldGUI. Это исследование подчеркивает эффективность подхода, основанного на критическом мышлении, для улучшения автоматизации GUI.'}, 'en': {'title': 'Enhancing GUI Automation with WorldGUI and GUI-Thinker', 'desc': 'This paper addresses the challenges faced by current GUI agents in planning tasks due to their sensitivity to the initial state of the environment. It introduces WorldGUI, a new benchmark that simulates real user interactions by incorporating various initial states across multiple software applications. To tackle the complexities of dynamic GUI automation, the authors propose GUI-Thinker, a framework that utilizes a critique mechanism to improve decision-making in unpredictable scenarios. Experimental results show that GUI-Thinker outperforms existing models, highlighting its effectiveness in enhancing the success rate of GUI automation tasks.'}, 'zh': {'title': '提升GUI自动化的关键思维框架', 'desc': '当前的图形用户界面（GUI）代理在元素定位方面表现出色，但在规划方面仍然面临挑战，尤其是对环境初始状态的敏感性。初始状态的微小差异，例如目标软件未打开或界面不在默认状态，常常导致规划错误。为了解决这个问题，本文提出了WorldGUI，一个新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。我们还提出了GUI-Thinker，一个全面的框架，通过批判机制有效管理GUI交互的不可预测性和复杂性，实验结果显示其在WorldGUI任务上的成功率比Claude-3.5高出14.9%。'}}}, {'id': 'https://huggingface.co/papers/2502.07563', 'title': 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid', 'url': 'https://huggingface.co/papers/2502.07563', 'abstract': 'Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 19, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'f5a4cfd0a0d018ae', 'authors': ['Weigao Sun', 'Disen Lan', 'Yiran Zhong', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.07563.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обучения трансформеров с линейным вниманием на сверхдлинных последовательностях', 'desc': 'Статья представляет новый метод параллелизма последовательностей LASP-2 для обучения моделей трансформеров с линейным вниманием на очень длинных входных последовательностях. LASP-2 оптимизирует коммуникацию и вычислительный параллелизм, требуя только одну операцию AllGather для промежуточных состояний памяти, размер которых не зависит от длины последовательности. Авторы также предлагают расширение LASP-2H для гибридных моделей, сочетающих линейное и стандартное внимание. Эксперименты на модели Linear-Llama3 показывают, что LASP-2 превосходит предыдущие методы по скорости обучения на длинных последовательностях при распределенных вычислениях.'}, 'en': {'title': 'Boosting Efficiency in Linear Attention with LASP-2', 'desc': 'This paper presents LASP-2, a new sequence parallelism (SP) method designed to improve the efficiency of training linear attention transformer models with very long input sequences. LASP-2 optimizes communication and computation parallelism by minimizing the communication requirements for linear attention layers, allowing for a single AllGather operation on intermediate memory states. This approach leads to significant enhancements in both communication and computation parallelism, enabling better scalability in distributed systems. Additionally, LASP-2 is extended to LASP-2H, which applies similar optimizations to standard attention modules, providing an efficient solution for hybrid models that utilize both linear and standard attention.'}, 'zh': {'title': 'LASP-2：提升线性注意力模型的并行性', 'desc': '本文介绍了一种新的序列并行方法LASP-2，旨在提高线性注意力变换器模型在处理非常长输入序列时的通信和计算并行性。与之前的LASP方法相比，LASP-2重新思考了线性注意力层的最小通信需求，并重新组织了通信和计算的工作流程。通过这种方式，LASP-2只需在中间内存状态上进行一次AllGather集体通信，显著提高了通信和计算的并行性及其重叠。我们的评估表明，LASP-2在训练速度上比LASP提高了15.2%，比环形注意力提高了36.6%。'}}}, {'id': 'https://huggingface.co/papers/2502.08606', 'title': 'Distillation Scaling Laws', 'url': 'https://huggingface.co/papers/2502.08606', 'abstract': 'We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.', 'score': 16, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '774eeded4c92c597', 'authors': ['Dan Busbridge', 'Amitis Shidani', 'Floris Weers', 'Jason Ramapuram', 'Etai Littwin', 'Russ Webb'], 'affiliations': ['Apple', 'University of Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.08606.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Масштабируемая дистилляция: оптимизация обучения моделей', 'desc': 'Эта статья представляет закон масштабирования дистилляции, который оценивает производительность дистиллированной модели на основе вычислительного бюджета и его распределения между студентом и учителем. Авторы предоставляют оптимальные рецепты дистилляции для различных сценариев, включая случаи с существующим учителем или необходимостью его обучения. Исследование показывает, что дистилляция превосходит обычное предобучение с учителем до определенного уровня вычислений, который предсказуемо растет с размером модели-студента. Работа также предоставляет ценные insights о процессе дистилляции, улучшающие понимание этого метода и информирующие дизайн экспериментов.'}, 'en': {'title': 'Maximizing Student Performance through Optimal Distillation Strategies', 'desc': 'This paper introduces a distillation scaling law that helps predict how well a distilled model will perform based on the available computing resources and how those resources are divided between the teacher and student models. The authors demonstrate that by optimizing the compute allocation, the performance of the student model can be significantly improved, especially when multiple students are distilled from a pre-trained teacher. They also provide guidelines for when to use distillation versus supervised learning, depending on whether a teacher model is available and whether it requires training. Overall, the study enhances our understanding of model distillation and offers practical strategies for effective implementation in machine learning tasks.'}, 'zh': {'title': '优化蒸馏模型性能的计算法则', 'desc': '本文提出了一种蒸馏缩放法则，用于根据计算预算和在学生与教师之间的分配来估计蒸馏模型的性能。研究结果降低了大规模使用蒸馏的风险，能够优化教师和学生模型的计算分配，以最大化学生的表现。我们提供了计算最优的蒸馏方案，适用于已有教师或需要训练教师的情况。通过大规模研究，我们增加了对蒸馏的理解，并为实验设计提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2502.06533', 'title': 'Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.06533', 'abstract': 'The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.', 'score': 10, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '178e4717b984d64d', 'authors': ['Jean Vassoyan', 'Nathanaël Beau', 'Roman Plaud'], 'affiliations': ['Institut Polytechnique de Paris', 'Université Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, France', 'Université de Paris, LLF, CNRS, France', 'onepoint, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.06533.jpg', 'data': {'categories': ['#rl', '#training', '#long_context', '#small_models', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное исследование для LLM через оптимизацию критических токенов', 'desc': "В статье рассматривается проблема достижения долгосрочных целей большими языковыми моделями (LLM). Авторы предлагают использовать обучение с подкреплением для настройки предобученных LLM, чтобы оптимизировать заданную цель. Они исследуют динамику исследования на простой арифметической задаче, показывая влияние предварительного обучения и важность 'критических токенов'. Предлагается модификация KL-штрафа для более эффективного исследования критических токенов при обучении с подкреплением."}, 'en': {'title': 'Enhancing Exploration in Language Models with Critical Tokens', 'desc': "This paper addresses the challenge of enabling large language models (LLMs) to achieve long-term goals through reinforcement learning (RL). It highlights the difficulty of exploration in LLMs, where a balance must be maintained between discovering new solutions and preserving the model's foundational capabilities. The authors investigate how pre-training affects exploration dynamics in a small language model tasked with simple arithmetic. They introduce a modified Kullback-Leibler (KL) penalty that enhances exploration of 'critical tokens', leading to more efficient RL fine-tuning."}, 'zh': {'title': '优化长期目标的探索策略', 'desc': '本文探讨了大型语言模型（LLMs）在实现长期目标时面临的挑战。我们提出通过强化学习（RL）对预训练的LLMs进行微调，以优化特定目标的解决方案。研究表明，预训练的程度对探索过程有显著影响，尤其是“关键标记”在最终结果中起着重要作用。我们还引入了一种对KL惩罚的简单修改，以促进对关键标记的探索，从而提高RL微调阶段的效率。'}}}, {'id': 'https://huggingface.co/papers/2502.08168', 'title': 'SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation', 'url': 'https://huggingface.co/papers/2502.08168', 'abstract': "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.", 'score': 10, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '1ca2b8d38e35b203', 'authors': ['Zhiming Ma', 'Xiayang Xiao', 'Sihao Dong', 'Peidong Wang', 'HaiPeng Wang', 'Qingyun Pan'], 'affiliations': ['China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China', 'China Mobile Internet Company Ltd., Guangzhou, China', 'School of Computer Science and Engineering, Northeastern University, Shenyang, China', 'The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China', 'The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08168.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source', '#synthetic'], 'emoji': '🛰️', 'ru': {'title': 'SARChat-2M: Революция в интерпретации изображений SAR с помощью мультимодальных диалоговых моделей', 'desc': 'Статья представляет первый крупномасштабный мультимодальный диалоговый датасет для изображений SAR, названный SARChat-2M. Он содержит около 2 миллионов пар изображение-текст высокого качества и охватывает различные сценарии с детальными аннотациями целей. Датасет поддерживает ключевые задачи, такие как визуальное понимание и обнаружение объектов, а также предоставляет основу для создания мультимодальных датасетов в различных областях дистанционного зондирования. Эффективность датасета была подтверждена экспериментами на 16 основных VLM, что позволило создать первый многозадачный диалоговый бенчмарк в области SAR.'}, 'en': {'title': 'Empowering SAR Image Interpretation with SARChat-2M!', 'desc': 'This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications.'}, 'zh': {'title': '推动SAR图像解读的多模态对话数据集', 'desc': '在合成孔径雷达（SAR）遥感图像解读领域，尽管视觉语言模型（VLMs）在自然语言处理和图像理解方面取得了显著进展，但由于缺乏专业领域的知识，其应用仍然有限。本文创新性地提出了第一个大规模的SAR图像多模态对话数据集SARChat-2M，包含约200万对高质量的图像-文本配对，涵盖了多种场景和详细的目标注释。该数据集不仅支持视觉理解和目标检测等关键任务，还开发了SAR领域的视觉语言数据集和基准，评估VLMs在SAR图像解读中的能力。通过对16个主流VLM的实验验证，该数据集的有效性得到了充分证明，并成功建立了SAR领域的第一个多任务对话基准。'}}}, {'id': 'https://huggingface.co/papers/2502.07599', 'title': 'DPO-Shift: Shifting the Distribution of Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.07599', 'abstract': 'Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.', 'score': 10, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '85d178e03a57421f', 'authors': ['Xiliang Yang', 'Feng Jiang', 'Qianen Zhang', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen', 'School of Mathematics, South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.07599.jpg', 'data': {'categories': ['#alignment', '#training', '#rlhf'], 'emoji': '🔀', 'ru': {'title': 'Контролируемое смещение вероятностей для улучшения обучения языковых моделей', 'desc': 'Статья представляет новый метод под названием DPO-Shift для улучшения обучения языковых моделей с учетом человеческих предпочтений. Авторы решают проблему смещения вероятности выбранных ответов, которая возникает при использовании метода Direct Preference Optimization (DPO). DPO-Shift позволяет контролируемо смещать распределение вероятности выбранных ответов. Экспериментальные результаты показывают превосходство DPO-Shift над DPO на ряде задач, включая MT-Bench.'}, 'en': {'title': 'Mitigating Likelihood Displacement in Language Model Training', 'desc': 'This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement.'}, 'zh': {'title': '解决选择概率下降的有效方法', 'desc': '本文介绍了一种新的方法\textit{method}，旨在解决直接偏好优化（DPO）中出现的选择概率下降问题。研究表明，在训练过程中，模型对选择响应的概率往往会降低，这被称为似然位移。我们的方法可以控制选择概率的分布，从而改善模型的表现。通过理论分析和实验验证，我们证明了\textit{method}在下游任务中优于传统的DPO方法。'}}}, {'id': 'https://huggingface.co/papers/2502.08524', 'title': 'LLM Pretraining with Continuous Concepts', 'url': 'https://huggingface.co/papers/2502.08524', 'abstract': "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.", 'score': 7, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '99ad370e7cd11e3c', 'authors': ['Jihoon Tack', 'Jack Lanchantin', 'Jane Yu', 'Andrew Cohen', 'Ilia Kulikov', 'Janice Lan', 'Shibo Hao', 'Yuandong Tian', 'Jason Weston', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'KAIST', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.08524.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#interpretability', '#training', '#architecture', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'CoCoMix: смешивание концепций для улучшения языковых моделей', 'desc': 'В этой статье представлен новый метод предобучения языковых моделей под названием Continuous Concept Mixing (CoCoMix). В отличие от стандартного подхода предсказания следующего токена, CoCoMix сочетает дискретное предсказание токенов с непрерывными концепциями, полученными из предобученного разреженного автоэнкодера. Эксперименты показывают, что CoCoMix превосходит стандартные методы по эффективности обучения и качеству на различных задачах. Кроме того, CoCoMix улучшает интерпретируемость и управляемость модели, позволяя напрямую анализировать и модифицировать предсказанные концепции.'}, 'en': {'title': 'Revolutionizing Language Models with Continuous Concept Mixing', 'desc': "This paper introduces Continuous Concept Mixing (CoCoMix), a new framework for pretraining large language models. Unlike traditional methods that focus solely on predicting the next token, CoCoMix integrates continuous concepts derived from a sparse autoencoder into the model's hidden states. This approach improves sample efficiency and enhances performance on various tasks, including language modeling and reasoning. Additionally, CoCoMix allows for better interpretability and control over the model's reasoning by enabling direct manipulation of the predicted concepts."}, 'zh': {'title': '连续概念混合：提升语言模型的效率与可解释性', 'desc': '本文提出了一种新的预训练框架，称为连续概念混合（CoCoMix），它结合了离散的下一个标记预测和连续概念。CoCoMix通过将从预训练稀疏自编码器中学习的连续概念与标记的隐藏表示交错混合，来优化模型的隐藏状态。实验结果表明，CoCoMix在样本效率上表现更佳，并且在语言建模和推理任务上均优于传统的下一个标记预测和知识蒸馏方法。该方法还增强了模型的可解释性和可引导性，使得用户可以直接检查和修改预测的概念，从而透明地引导模型的内部推理过程。'}}}, {'id': 'https://huggingface.co/papers/2502.05167', 'title': 'NoLiMa: Long-Context Evaluation Beyond Literal Matching', 'url': 'https://huggingface.co/papers/2502.05167', 'abstract': 'Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.', 'score': 5, 'issue_id': 2188, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '4ff0f34526efea9f', 'authors': ['Ali Modarressi', 'Hanieh Deilamsalehy', 'Franck Dernoncourt', 'Trung Bui', 'Ryan A. Rossi', 'Seunghyun Yoon', 'Hinrich Schütze'], 'affiliations': ['Adobe Research', 'Center for Information and Language Processing'], 'pdf_title_img': 'assets/pdf/title_img/2502.05167.jpg', 'data': {'categories': ['#benchmark', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'NoLiMa: Новый вызов для больших языковых моделей в работе с длинным контекстом', 'desc': 'Статья представляет новый бенчмарк NoLiMa для оценки способности больших языковых моделей (LLM) работать с длинным контекстом. В отличие от существующих тестов, NoLiMa требует от моделей выявления скрытых ассоциаций между вопросом и релевантной информацией в тексте. Исследование показало, что производительность 12 популярных LLM значительно снижается при увеличении длины контекста. Результаты указывают на трудности механизма внимания в обработке длинных последовательностей при отсутствии прямых лексических совпадений.'}, 'en': {'title': 'NoLiMa: Challenging LLMs Beyond Literal Matches', 'desc': 'This paper introduces NoLiMa, a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieving relevant information from extensive contexts. Unlike traditional methods that allow models to rely on literal matches, NoLiMa requires models to infer deeper associations between questions and answers with minimal lexical overlap. The study evaluates 12 popular LLMs, revealing that while they perform well in shorter contexts, their performance significantly declines as context length increases, particularly beyond 1K tokens. The findings indicate that the attention mechanism struggles to effectively retrieve relevant information in longer contexts when direct matches are not available.'}, 'zh': {'title': '长上下文中的信息检索挑战', 'desc': '最近的大型语言模型（LLMs）支持长达128K到1M的上下文。本文提出了一种新的基准测试NoLiMa，旨在评估模型在长上下文中检索相关信息的能力。与传统的针在干草堆（NIAH）测试不同，NoLiMa设计了最小词汇重叠的针集，要求模型推断潜在关联以找到针。我们的评估显示，尽管这些模型在短上下文中表现良好，但在长上下文中性能显著下降，尤其是在缺乏字面匹配的情况下。'}}}, {'id': 'https://huggingface.co/papers/2502.07737', 'title': 'Next Block Prediction: Video Generation via Semi-Autoregressive Modeling', 'url': 'https://huggingface.co/papers/2502.07737', 'abstract': 'Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.', 'score': 5, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '8038af0ecacc031f', 'authors': ['Shuhuai Ren', 'Shuming Ma', 'Xu Sun', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07737.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#training'], 'emoji': '🎬', 'ru': {'title': 'NBP: Быстрая и качественная генерация видео блоками', 'desc': 'Статья представляет новый подход к генерации видео под названием Next-Block Prediction (NBP). В отличие от традиционного метода Next-Token Prediction, NBP использует полуавтореrрессивную модель, разбивая видео на блоки и предсказывая их параллельно. Это позволяет значительно ускорить процесс генерации и улучшить качество результатов. Модель NBP превзошла базовые методы по метрике FVD на датасетах UCF101 и K600, демонстрируя масштабируемость и эффективность подхода.'}, 'en': {'title': 'Revolutionizing Video Generation with Next-Block Prediction', 'desc': 'This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model.'}, 'zh': {'title': '视频生成的新突破：下一块预测', 'desc': '本文提出了一种新的半自回归框架，称为下一块预测（NBP），用于视频生成。与传统的自回归方法不同，NBP通过将视频内容均匀分解为相等大小的块，使得每个块内的标记可以同时预测下一个块的对应标记，从而捕捉更强的空间依赖性。该方法通过并行预测多个标记，显著减少了生成步骤，提高了推理速度，达到了每秒生成8.89帧的效果。实验结果表明，NBP在多个数据集上表现优于传统模型，展示了其在生成质量和速度上的优势。'}}}, {'id': 'https://huggingface.co/papers/2502.06145', 'title': 'Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance', 'url': 'https://huggingface.co/papers/2502.06145', 'abstract': 'Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.', 'score': 4, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '66fa48cd36ed02b0', 'authors': ['Li Hu', 'Guangyuan Wang', 'Zhen Shen', 'Xin Gao', 'Dechao Meng', 'Lian Zhuo', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.06145.jpg', 'data': {'categories': ['#multimodal', '#cv', '#video', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Оживление персонажей с учетом окружающей среды', 'desc': 'Данная работа представляет Animate Anyone 2 - метод анимации персонажей с учетом окружающей среды. В отличие от предыдущих подходов, основанных на диффузионных моделях, здесь извлекаются не только сигналы движения, но и представления окружения в качестве условных входных данных. Предложена стратегия маски, не зависящая от формы, для лучшей характеристики взаимосвязи персонажа и среды. Для повышения точности взаимодействия с объектами используется объектный направляющий и пространственное смешивание признаков.'}, 'en': {'title': 'Animating Characters with Environmental Awareness', 'desc': 'This paper presents Animate Anyone 2, an advanced character animation method that improves upon previous diffusion models by integrating environmental context into the animation process. Unlike earlier methods, which struggled to connect characters with their surroundings, this approach captures environmental representations as conditional inputs, allowing for more coherent animations. The authors introduce a shape-agnostic mask strategy to better define the relationship between characters and their environments, enhancing the realism of interactions. Additionally, they implement an object guider and spatial blending techniques to refine object interactions and a pose modulation strategy to accommodate diverse motion patterns, resulting in superior animation quality.'}, 'zh': {'title': '角色与环境的完美结合', 'desc': '本文介绍了一种新的角色动画方法Animate Anyone 2，旨在解决现有基于扩散模型的动画方法在角色与环境之间的关联不足的问题。我们通过提取环境表示作为条件输入，使角色动画能够与环境的特征相一致。我们提出了一种形状无关的掩码策略，更有效地描述角色与环境之间的关系。此外，我们还引入了姿态调节策略，以处理更丰富的运动模式，实验结果表明该方法在性能上优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2502.06872', 'title': 'Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2502.06872', 'abstract': "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.", 'score': 4, 'issue_id': 2188, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': 'f454782ce3101c66', 'authors': ['Bo Ni', 'Zheyuan Liu', 'Leyao Wang', 'Yongjia Lei', 'Yuying Zhao', 'Xueqi Cheng', 'Qingkai Zeng', 'Luna Dong', 'Yinglong Xia', 'Krishnaram Kenthapadi', 'Ryan Rossi', 'Franck Dernoncourt', 'Md Mehrab Tanjim', 'Nesreen Ahmed', 'Xiaorui Liu', 'Wenqi Fan', 'Erik Blasch', 'Yu Wang', 'Meng Jiang', 'Tyler Derr'], 'affiliations': ['Adobe Research', 'Air Force Research Lab', 'Cisco AI Research', 'Meta', 'North Carolina State University', 'Oracle Health AI', 'The Hong Kong Polytechnic University', 'University of Notre Dame', 'University of Oregon', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06872.jpg', 'data': {'categories': ['#hallucinations', '#interpretability', '#security', '#survey', '#rag', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Путь к надёжному ИИ: Преодоление рисков в генерации с дополнением извлечённой информацией', 'desc': 'Статья посвящена методу генерации с дополнением извлечённой информацией (RAG), который улучшает генерацию контента искусственным интеллектом. Авторы рассматривают риски, связанные с RAG, включая проблемы надёжности, конфиденциальности и безопасности. Предлагается комплексная дорожная карта для разработки надёжных RAG-систем с акцентом на пять ключевых аспектов. Статья также освещает применение надёжных RAG-систем в различных прикладных областях.'}, 'en': {'title': 'Building Trust in Retrieval-Augmented Generation Systems', 'desc': 'This paper discusses Retrieval-Augmented Generation (RAG), a technique that enhances AI-generated content by incorporating external knowledge for better accuracy and relevance. While RAG improves content generation, it also brings new challenges such as robustness, privacy, and accountability issues that need to be addressed. The authors propose a comprehensive framework that focuses on five key areas: reliability, privacy, safety, fairness, and explainability, to guide future research and development of trustworthy RAG systems. By providing a structured approach, the paper aims to foster innovation and highlight the importance of trust in RAG applications.'}, 'zh': {'title': '构建可信赖的检索增强生成系统', 'desc': '检索增强生成（RAG）是一种先进的技术，旨在解决人工智能生成内容（AIGC）面临的挑战。通过将上下文检索与内容生成相结合，RAG 提供可靠且最新的外部知识，减少幻觉现象，并确保在各种任务中保持相关上下文。然而，尽管 RAG 取得了成功，但最近的研究表明，该范式也引入了新的风险，包括鲁棒性问题、隐私问题、对抗性攻击和问责问题。本文旨在提供一个全面的路线图，以开发可信赖的 RAG 系统，围绕可靠性、隐私、安全性、公平性、可解释性和问责性等五个关键视角进行讨论。'}}}, {'id': 'https://huggingface.co/papers/2502.04411', 'title': 'Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing', 'url': 'https://huggingface.co/papers/2502.04411', 'abstract': 'Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.', 'score': 3, 'issue_id': 2192, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '9370051a307713bf', 'authors': ['Kunfeng Lai', 'Zhenheng Tang', 'Xinglin Pan', 'Peijie Dong', 'Xiang Liu', 'Haolan Chen', 'Li Shen', 'Bo Li', 'Xiaowen Chu'], 'affiliations': ['Platform and Content Group, Tencent', 'Sun Yatsen University', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04411.jpg', 'data': {'categories': ['#model merging', '#training', '#architecture', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Умное слияние языковых моделей: эффективность без компромиссов', 'desc': 'Статья описывает новый метод объединения языковых моделей, обученных на разных задачах. Авторы предлагают усреднять слои с минимальными конфликтами параметров и использовать маршрутизацию экспертов для слоев со значительными конфликтами. Они также вводят разделение экспертов на плотный и разреженные для уменьшения затрат памяти. Эксперименты на моделях LLaMA и Qwen показывают улучшение производительности при меньших системных затратах по сравнению с существующими методами.'}, 'en': {'title': 'Merging Models Smartly: Harnessing Layer Insights for Better Performance', 'desc': 'This paper presents a method for merging Large Language Models (LLMs) that have been fine-tuned on different tasks, aiming to create a more powerful model. The authors identify that parameter conflicts between models can degrade performance when simply averaging them. To address this, they propose a strategy that averages layers with minimal conflicts and employs task-level expert routing for layers with significant conflicts. Their approach not only reduces storage costs by decoupling fine-tuned experts but also improves performance on real-world reasoning tasks while being more efficient than existing methods.'}, 'zh': {'title': '智能合并，提升模型性能！', 'desc': '本论文提出了一种模型合并的方法，旨在将不同任务上微调的大型语言模型（LLMs）聚合成一个更强的模型。我们发现不同层之间的参数冲突程度不同，因此我们对参数冲突较小的层进行平均，而对参数冲突较大的层采用新颖的任务级专家路由。为了进一步降低存储成本，我们将多个微调的专家解耦为一个稠密专家和几个稀疏专家，并根据输入数据的任务不确定性选择和合并合适的专家。实验结果表明，我们的方法在性能上显著优于现有方法，同时减少了系统成本。'}}}, {'id': 'https://huggingface.co/papers/2502.00963', 'title': 'PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs', 'url': 'https://huggingface.co/papers/2502.00963', 'abstract': 'While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs). Our approach enables LLMs to transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control. We build a holistic solution comprising datasets (both human-written cases and 2 million synthetic samples), math-reasoning models, and novel evaluation metrics, all of which require significant effort. Our PDE-Controller significantly outperforms prompting the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, achieving up to a 62% improvement in utility gain for PDE control. By bridging the gap between language generation and PDE systems, we demonstrate the potential of LLMs in addressing complex scientific and engineering challenges. We will release all data, model checkpoints, and code at https://pde-controller.github.io/.', 'score': 1, 'issue_id': 2200, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '44c533b68f27ca69', 'authors': ['Mauricio Soroco', 'Jialin Song', 'Mengzhou Xia', 'Kye Emond', 'Weiran Sun', 'Wuyang Chen'], 'affiliations': ['Department of Computer Science, Princeton University', 'School of Computing Science, Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2502.00963.jpg', 'data': {'categories': ['#open_source', '#training', '#dataset', '#synthetic', '#math', '#science', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Языковые модели покоряют дифференциальные уравнения', 'desc': 'Исследователи представили PDE-Controller - фреймворк, позволяющий большим языковым моделям (LLM) управлять системами, описываемыми дифференциальными уравнениями в частных производных (PDE). Этот подход позволяет LLM преобразовывать неформальные инструкции на естественном языке в формальные спецификации, а затем выполнять рассуждения и планирование для улучшения управления PDE. Авторы создали комплексное решение, включающее наборы данных, модели для математических рассуждений и новые метрики оценки. PDE-Controller значительно превосходит существующие модели в рассуждениях, автоформализации и синтезе программ, демонстрируя потенциал LLM в решении сложных научных и инженерных задач.'}, 'en': {'title': 'Empowering Language Models to Control Complex PDE Systems', 'desc': 'This paper introduces PDE-Controller, a framework that allows large language models (LLMs) to manage systems described by partial differential equations (PDEs). It transforms informal language instructions into formal specifications, enabling effective reasoning and planning for PDE control. The framework includes a comprehensive solution with datasets, math-reasoning models, and new evaluation metrics, showcasing significant improvements over existing models. By achieving up to a 62% increase in utility for PDE control, this work highlights the capability of LLMs to tackle complex problems in science and engineering.'}, 'zh': {'title': '利用LLMs提升偏微分方程控制的实用性', 'desc': '本论文介绍了PDE-Controller框架，旨在利用大型语言模型（LLMs）控制由偏微分方程（PDEs）所支配的系统。该方法能够将自然语言指令转化为正式规范，并执行推理和规划步骤，从而提高PDE控制的实用性。我们构建了一个全面的解决方案，包括数据集（人类编写的案例和200万个合成样本）、数学推理模型和新颖的评估指标。PDE-Controller在推理、自动形式化和程序合成方面显著优于最新的开源和GPT模型，PDE控制的实用性提升达62%。'}}}, {'id': 'https://huggingface.co/papers/2502.08213', 'title': 'LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention', 'url': 'https://huggingface.co/papers/2502.08213', 'abstract': 'In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method.', 'score': 1, 'issue_id': 2194, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '2623177431838d6c', 'authors': ['Konstantin Kolomeitsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.08213.jpg', 'data': {'categories': ['#small_models', '#transfer_learning', '#optimization', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная передача знаний между языковыми моделями разного размера', 'desc': 'В статье предлагается архитектура модулей LLM, позволяющая передавать знания от большой предобученной модели к меньшей с помощью улучшенного механизма кросс-внимания. Замороженная модель Qwen2-1.5B передает свои представления через специальные слои внимания модели GPT-Neo-125M, которая обучается на ограниченных вычислительных ресурсах. Эксперименты на наборе данных Bespoke-Stratos-17k показывают, что после 15 эпох обучения комбинированная модель генерирует ответы, сравнимые по качеству с полученными путем дистилляции. Авторы обсуждают преимущества модульного подхода и перспективы дальнейшего развития метода.'}, 'en': {'title': 'Empowering Smaller Models with Enhanced Knowledge Transfer', 'desc': 'This paper introduces a new architecture called LLM Modules that facilitates knowledge transfer from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. The Qwen2-1.5B model is kept unchanged, and its learned representations are utilized by the smaller GPT-Neo-125M model, which is optimized for limited computational resources. The results from experiments on the Bespoke-Stratos-17k dataset show that after 15 training epochs, the performance of the combined model is on par with traditional distillation methods. The authors highlight the benefits of this modular approach and provide examples and analyses to support their findings, while also discussing future enhancements.'}, 'zh': {'title': '知识转移的新方法：模块化架构与增强交叉注意力', 'desc': '本文提出了一种LLM模块架构，能够通过增强的交叉注意力机制将知识从大型预训练模型转移到较小的模型。我们将Qwen2-1.5B模型固定，并通过专门设计的注意力层将其表示传递给在有限计算资源上训练的GPT-Neo-125M模型。实验结果表明，在Bespoke-Stratos-17k数据集上经过15个训练周期后，组合模型生成的响应质量与蒸馏获得的结果相当。我们讨论了模块化方法的优势，提供了输入查询的示例和比较分析，并概述了该方法进一步扩展的前景。'}}}, {'id': 'https://huggingface.co/papers/2502.07985', 'title': 'MetaSC: Test-Time Safety Specification Optimization for Language Models', 'url': 'https://huggingface.co/papers/2502.07985', 'abstract': 'We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .', 'score': 1, 'issue_id': 2190, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '68244a5483bfc513', 'authors': ['Víctor Gallego'], 'affiliations': ['Komorebi AI, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2502.07985.jpg', 'data': {'categories': ['#optimization', '#training', '#security', '#alignment', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Динамическая оптимизация безопасности языковых моделей без переобучения', 'desc': 'Авторы предлагают новую динамическую систему безопасности для оптимизации рассуждений языковых моделей во время вывода без изменения весов модели. Подход основан на механизме мета-критики, который итеративно обновляет промпты безопасности для адаптивного управления процессом критики и пересмотра. Оптимизация во время тестирования улучшает защиту от попыток обхода ограничений и повышает безопасность в различных задачах. Эмпирические оценки показывают, что динамически оптимизированные промпты безопасности дают значительно более высокие показатели безопасности по сравнению с фиксированными системными промптами.'}, 'en': {'title': 'Dynamic Safety for Language Models: Adapting Prompts for Better Protection', 'desc': "This paper introduces a dynamic safety framework designed to enhance the safety of language models during inference without altering their underlying weights. It utilizes a meta-critique mechanism that adaptively updates safety prompts, known as specifications, to improve the model's ability to handle adversarial inputs and general safety tasks. The approach shows significant improvements in safety performance, particularly in avoiding moral harm and ensuring honest responses. Empirical results indicate that the dynamically optimized safety prompts outperform traditional fixed prompts and static defenses in various scenarios."}, 'zh': {'title': '动态优化，提升语言模型安全性！', 'desc': '我们提出了一种新颖的动态安全框架，旨在优化语言模型在推理时的安全性推理，而无需修改模型权重。该方法基于自我批评方法的最新进展，利用元批评机制迭代更新安全提示（称为规范），以自适应地推动批评和修订过程。此测试时优化不仅提高了对抗性越狱请求的性能，还在避免道德伤害和追求诚实回应等多种安全相关任务中表现出色。我们的实证评估显示，动态优化的安全提示相比于固定系统提示和静态自我批评防御，显著提高了安全评分。'}}}, {'id': 'https://huggingface.co/papers/2502.05282', 'title': 'Homeomorphism Prior for False Positive and Negative Problem in Medical Image Dense Contrastive Representation Learning', 'url': 'https://huggingface.co/papers/2502.05282', 'abstract': "Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: https://github.com/YutingHe-list/GEMINI.", 'score': 0, 'issue_id': 2203, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '966279f30488fb3d', 'authors': ['Yuting He', 'Boyu Wang', 'Rongjun Ge', 'Yang Chen', 'Guanyu Yang', 'Shuo Li'], 'affiliations': ['Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs)', 'Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA', 'Department of Computer Science, Western University, London, ON N6A 3K7, Canada', 'Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China', 'School of Instrument Science and Engineering, Southeast University, Nanjing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05282.jpg', 'data': {'categories': ['#dataset', '#training', '#cv', '#healthcare', '#open_source', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'GEMINI: Геометрическое обучение для надежного сопоставления медицинских изображений', 'desc': 'Статья представляет новый метод обучения плотным контрастным представлениям (DCRL) для задач плотного предсказания на медицинских изображениях. Авторы предлагают подход GEMINI, который встраивает гомеоморфное отображение в DCRL для надежного обнаружения соответствий. Метод включает обучение деформируемому гомеоморфизму (DHL) и геометрическое семантическое сходство (GSS) для улучшения эффективности. Эксперименты на семи наборах данных показывают превосходство предложенного метода над существующими подходами.'}, 'en': {'title': 'Enhancing Medical Image Learning with GEMINI: Reliable Correspondence Discovery', 'desc': 'This paper introduces GEMINI, a novel approach to improve dense contrastive representation learning (DCRL) for medical images. It addresses the challenge of unreliable correspondence discovery by incorporating a homeomorphism prior, which helps in accurately mapping pixel correspondences while preserving the topological structure of the images. The method includes deformable homeomorphism learning (DHL) to reduce false positive and negative pairs, enhancing the learning process. Additionally, geometric semantic similarity (GSS) is utilized to assess the alignment of features, leading to more effective and reliable dense contrast learning.'}, 'zh': {'title': '提升医疗图像学习效率的几何对比方法', 'desc': '密集对比表示学习（DCRL）在图像密集预测任务中显著提高了学习效率，尤其在医疗图像收集和密集标注方面具有巨大潜力。然而，医疗图像的特性导致了不可靠的对应关系发现，造成了大量的假阳性和假阴性对。本文提出了几何视觉密集相似性（GEMINI）学习，通过引入同胚性先验来增强DCRL的可靠性，从而有效地发现对应关系。我们还提出了可变形同胚学习（DHL）和几何语义相似性（GSS），这两者共同提高了对应学习的效率和性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (20)', '#agents (33)', '#agi (14)', '#alignment (27)', '#architecture (83)', '#audio (6)', '#benchmark (114)', '#cv (45)', '#data (42)', '#dataset (115)', '#diffusion (39)', '#ethics (12)', '#games (16)', '#graphs (3)', '#hallucinations (13)', '#healthcare (10)', '#inference (53)', '#interpretability (30)', '#leakage (3)', '#long_context (27)', '#low_resource (15)', '#machine_translation (2)', '#math (24)', '#multilingual (19)', '#multimodal (69)', '#open_source (66)', '#optimization (144)', '#plp (7)', '#rag (15)', '#reasoning (89)', '#rl (27)', '#rlhf (23)', '#robotics (8)', '#science (11)', '#security (16)', '#small_models (14)', '#story_generation (2)', '#survey (6)', '#synthetic (22)', '#training (189)', '#transfer_learning (30)', '#video (32)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-20 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-20 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-20 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    