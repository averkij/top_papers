
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 119 papers. February 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Февраль 2025</span> | <span id="title-articles-count">119 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-01.html">⬅️ <span id="prev-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-03.html">➡️ <span id="next-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Февраль 2025', 'en': 'February 2025', 'zh': '2月2025年'};
        let feedDateNext = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let feedDatePrev = {'ru': '01.2025', 'en': '01/2025', 'zh': '1月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.03628', 'title': 'The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering', 'url': 'https://huggingface.co/papers/2502.03628', 'abstract': 'Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.', 'score': 7, 'issue_id': 2140, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '04b182d80abf9219', 'authors': ['Zhuowei Li', 'Haizhou Shi', 'Yunhe Gao', 'Di Liu', 'Zhenting Wang', 'Yuxiao Chen', 'Ting Liu', 'Long Zhao', 'Hao Wang', 'Dimitris N. Metaxas'], 'affiliations': ['Google DeepMind', 'Rutgers University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03628.jpg', 'data': {'categories': ['#cv', '#training', '#hallucinations', '#benchmark', '#inference', '#interpretability', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в визуально-языковых моделях: метод VISTA', 'desc': 'Эта статья исследует проблему галлюцинаций в крупных визуально-языковых моделях (LVLM) при обработке текстовых и визуальных входных данных. Авторы анализируют внутренние механизмы возникновения галлюцинаций, изучая ранжирование логитов токенов в процессе генерации. На основе выявленных закономерностей предлагается метод VISTA для уменьшения галлюцинаций и усиления достоверной информации во время вывода. Эксперименты показывают, что VISTA в среднем снижает уровень галлюцинаций на 40% в задачах открытой генерации и превосходит существующие методы на четырех бенчмарках.'}, 'en': {'title': 'VISTA: Reducing Hallucination in Vision-Language Models', 'desc': 'This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures.'}, 'zh': {'title': '减少幻觉，提升真实信息的VISTA框架', 'desc': '大型视觉语言模型（LVLMs）能够有效地处理文本和视觉输入，但它们往往会产生语法上连贯但视觉上不真实的内容。本文研究了幻觉的内部动态，发现LVLMs在生成过程中处理信息的三种关键模式：逐渐丧失视觉信息、早期激活和隐藏的真实信息。基于这些发现，我们提出了VISTA（视觉信息引导与标记逻辑增强），这是一种无需训练的推理时干预框架，旨在减少幻觉并促进真实信息的生成。实验表明，VISTA在开放式生成任务中平均减少了约40%的幻觉，并在四个基准测试中在三种解码策略下始终优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.05609', 'title': 'Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding', 'url': 'https://huggingface.co/papers/2502.05609', 'abstract': 'Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.', 'score': 6, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '6f559083c224138c', 'authors': ['Sukmin Cho', 'Sangjin Choi', 'Taeho Hwang', 'Jeongyeon Seo', 'Soyeong Jeong', 'Huije Lee', 'Hoyun Song', 'Jong C. Park', 'Youngjin Kwon'], 'affiliations': ['School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.05609.jpg', 'data': {'categories': ['#training', '#architecture', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Иерархическая черновая генерация: новый подход к ускорению вывода в LLM', 'desc': 'Статья представляет новый метод ускорения вывода в больших языковых моделях (LLM) под названием Hierarchy Drafting (HD). HD организует различные источники токенов в иерархические базы данных, основываясь на временной локальности. Метод последовательно обращается к базам данных для получения черновых токенов, обеспечивая стабильное ускорение на различных задачах. Эксперименты показали, что HD превосходит существующие методы черновой генерации, демонстрируя надежное ускорение вывода для моделей разного размера, задач и температур.'}, 'en': {'title': 'Boosting Inference Speed with Hierarchy Drafting in LLMs', 'desc': 'This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks.'}, 'zh': {'title': '层次草拟：加速大型语言模型推理的新方法', 'desc': '加速大型语言模型（LLMs）的推理对于实时交互至关重要。本文提出了一种新的无损草拟方法，称为层次草拟（HD），它通过基于时间局部性的层次框架组织多种令牌源。HD在草拟步骤中依次访问多个数据库，从最高到最低的局部性获取草拟令牌，从而确保在不同任务中一致的加速效果。实验结果表明，HD在推理速度上优于现有的数据库草拟方法，适用于不同规模的模型和任务。'}}}, {'id': 'https://huggingface.co/papers/2502.06772', 'title': 'ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates', 'url': 'https://huggingface.co/papers/2502.06772', 'abstract': 'We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux', 'score': 4, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1ac59597c9610fb2', 'authors': ['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang'], 'affiliations': ['Peking University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06772.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning', '#math', '#benchmark', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Революция в математическом мышлении ИИ: иерархические рассуждения на новом уровне', 'desc': 'Представлена модель ReasonFlux-32B, использующая иерархическое рассуждение с масштабированием шаблонов мышления для оптимизации пространства поиска решений. Модель превосходит математические способности мощных языковых моделей, таких как OpenAI o1-preview и DeepSeek V3. ReasonFlux-32B использует структурированную библиотеку шаблонов мышления и иерархическое обучение с подкреплением для планирования оптимальной траектории шаблонов. На бенчмарке MATH модель достигает точности 91.2%, превосходя o1-preview на 6.7%.'}, 'en': {'title': 'Revolutionizing Math Reasoning with Hierarchical Thought Templates', 'desc': 'This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3.'}, 'zh': {'title': '层次化推理，数学能力新突破', 'desc': '本文提出通过扩展思维模板的层次化大语言模型（LLM）推理，可以有效优化推理搜索空间，并超越强大的LLM如OpenAI o1-preview和DeepSeek V3的数学推理能力。我们训练的ReasonFlux-32B模型仅使用8个GPU，并引入了三项创新：一是构建了一个包含约500个高层次思维模板的结构化通用模板库，能够推广到类似的推理问题；二是对思维模板序列进行层次化强化学习，而不是长链的思维（CoTs），优化基础LLM以规划出处理复杂问题的最佳模板轨迹；三是全新的推理扩展系统，通过在推理时自适应扩展思维模板，实现层次化LLM推理。通过包含顺序思维模板的模板轨迹，ReasonFlux-32B在数学推理能力上显著提升，达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2502.06788', 'title': 'EVEv2: Improved Baselines for Encoder-Free Vision-Language Models', 'url': 'https://huggingface.co/papers/2502.06788', 'abstract': 'Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.', 'score': 2, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4374edb93ca102c6', 'authors': ['Haiwen Diao', 'Xiaotong Li', 'Yufeng Cui', 'Yueze Wang', 'Haoge Deng', 'Ting Pan', 'Wenxuan Wang', 'Huchuan Lu', 'Xinlong Wang'], 'affiliations': ['BAAI', 'BUPT', 'CASIA', 'DLUT', 'PKU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2502.06788.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#agi', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Революция в мультимодальном обучении: EVEv2.0 - эффективность без энкодеров', 'desc': 'Статья представляет новое семейство моделей машинного обучения EVEv2.0, работающих с изображениями и текстом без использования энкодеров. Авторы систематически исследуют разрыв в производительности между моделями с энкодерами и без них, разрабатывая эффективные стратегии для последних. Они демонстрируют, что правильное разложение и иерархическая ассоциация зрения и языка в единой модели снижает интерференцию между модальностями. EVEv2.0 показывает превосходную эффективность использования данных и сильные способности к визуальному рассуждению.'}, 'en': {'title': 'EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders', 'desc': 'This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities.'}, 'zh': {'title': '无编码器VLM的潜力与创新', 'desc': '本论文探讨了无编码器的视觉-语言模型（VLMs）在性能上与基于编码器的模型之间的差距。我们系统性地分析了使用预训练视觉编码器和简约视觉层的无编码器VLMs的特性。通过开发高效的策略，我们推出了EVEv2.0，一个改进的无编码器VLM系列，展示了其在数据效率和视觉推理能力上的优势。我们的研究表明，合理分解和层次关联视觉与语言可以减少模态之间的干扰，并通过良好的训练策略实现有效优化。'}}}, {'id': 'https://huggingface.co/papers/2502.06635', 'title': 'Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM', 'url': 'https://huggingface.co/papers/2502.06635', 'abstract': "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.", 'score': 2, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2deb5075264d7660', 'authors': ['Qingshui Gu', 'Shu Li', 'Tianyu Zheng', 'Zhaoxiang Zhang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute of Automation, Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06635.jpg', 'data': {'categories': ['#small_models', '#multilingual', '#training', '#data', '#benchmark', '#open_source', '#dataset', '#low_resource'], 'emoji': '🇨🇳', 'ru': {'title': 'Создание эффективной китайскоязычной LLM с открытым исходным кодом', 'desc': 'Steel-LLM - это языковая модель, ориентированная на китайский язык, разработанная с нуля при ограниченных вычислительных ресурсах. Модель с 1 миллиардом параметров была обучена на крупномасштабном наборе данных, в основном на китайском языке. Steel-LLM показала конкурентоспособную производительность на бенчмарках CEVAL и CMMLU, превзойдя ранние модели от более крупных институтов. Статья предоставляет подробный отчет о процессе разработки, включая сбор данных, дизайн модели и методологии обучения.'}, 'en': {'title': 'Empowering Chinese NLP with Steel-LLM: Open-Source Innovation', 'desc': "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."}, 'zh': {'title': '打造中文优质开源语言模型的探索', 'desc': 'Steel-LLM是一个以中文为中心的语言模型，旨在在有限的计算资源下开发出高质量的开源模型。该项目于2024年3月启动，训练了一个拥有10亿参数的大规模模型，重点关注透明度和实用见解的分享。训练过程中主要使用中文数据，并适量包含英文数据，填补了现有开源大语言模型的空白。Steel-LLM在CEVAL和CMMLU等基准测试中表现出色，超越了大型机构的早期模型，为研究人员和实践者提供了宝贵的资源。'}}}, {'id': 'https://huggingface.co/papers/2502.06049', 'title': 'LM2: Large Memory Models', 'url': 'https://huggingface.co/papers/2502.06049', 'abstract': 'This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.', 'score': 2, 'issue_id': 2140, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '5f62d7e814a6918f', 'authors': ['Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Alex J. Chan', 'Fraser Greenlee', 'George Thomas', 'Marvin Purtorab', 'Andy Toulis'], 'affiliations': ['Convergence Labs Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2502.06049.jpg', 'data': {'categories': ['#dataset', '#architecture', '#benchmark', '#interpretability', '#long_context', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'LM2: Трансформер с памятью для улучшенных рассуждений', 'desc': 'В статье представлена модель Large Memory Model (LM2), архитектура декодер-трансформер с дополнительным модулем памяти. LM2 решает проблемы стандартных трансформеров в многошаговых рассуждениях и обработке длинных контекстов. Модель показала значительное улучшение производительности на бенчмарке BABILong по сравнению с базовыми моделями. LM2 также продемонстрировала улучшенные возможности в многоходовых выводах, числовых вычислениях и ответах на вопросы с большим контекстом.'}, 'en': {'title': 'Enhancing Transformers with Memory for Superior Reasoning', 'desc': 'The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures.'}, 'zh': {'title': '大型记忆模型：提升Transformer推理能力的关键', 'desc': '本文介绍了一种名为大型记忆模型（LM2）的解码器仅Transformer架构，旨在解决标准Transformer在多步推理、关系论证和长上下文信息综合方面的局限性。LM2引入了一个辅助记忆模块，作为上下文表示的存储库，通过交叉注意力与输入标记交互，并通过门控机制进行更新。实验结果表明，LM2在BABILong基准测试中，平均性能比记忆增强的RMT模型提高了37.1%，比基线Llama-3.2模型提高了86.3%。LM2在多跳推理、数值推理和大上下文问答方面表现出色，证明了显式记忆在增强Transformer架构中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.06155', 'title': 'Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile', 'url': 'https://huggingface.co/papers/2502.06155', 'abstract': 'Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.', 'score': 2, 'issue_id': 2140, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2f8d5e54db328d39', 'authors': ['Hangliang Ding', 'Dacheng Li', 'Runlong Su', 'Peiyuan Zhang', 'Zhijie Deng', 'Ion Stoica', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.06155.jpg', 'data': {'categories': ['#training', '#diffusion', '#optimization', '#inference', '#video'], 'emoji': '🎬', 'ru': {'title': 'Ускорение генерации видео: эффективность без потери качества', 'desc': 'Статья представляет новый подход к ускорению генерации видео с помощью Диффузионных Трансформеров (DiTs). Авторы предлагают прореживание 3D-внимания на основе избыточности видеоданных и сокращение процесса сэмплирования с помощью многошаговой дистилляции согласованности. Разработан трехэтапный процесс обучения для объединения внимания с низкой сложностью и возможностей генерации за несколько шагов. Результаты показывают ускорение в 7.4-7.8 раз для генерации видео 720p с 29 и 93 кадрами при использовании всего 0.1% данных предобучения.'}, 'en': {'title': 'Speeding Up Video Generation with Efficient Attention Mechanisms', 'desc': 'This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs.'}, 'zh': {'title': '高效视频生成的新方法', 'desc': '本论文提出了一种改进的Diffusion Transformers（DiTs）模型，以解决生成高保真视频时的效率问题。我们通过识别视频数据中的冗余，提出了一种稀疏的3D注意力机制，使其在视频帧数量上具有线性复杂度。其次，我们采用多步一致性蒸馏技术，缩短了采样过程，从而实现了更快速的视频生成。最终，我们的模型在使用极少的预训练数据时，生成速度提高了7.4到7.8倍，同时保持了良好的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.06023', 'title': 'Dual Caption Preference Optimization for Diffusion Models', 'url': 'https://huggingface.co/papers/2502.06023', 'abstract': "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.", 'score': 1, 'issue_id': 2141, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '07782353b3b8b697', 'authors': ['Amir Saeidi', 'Yiran Luo', 'Agneet Chatterjee', 'Shamanthak Hegde', 'Bimsara Pathiraja', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06023.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#dataset', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Двойные подписи для улучшения генерации изображений по текстовым описаниям', 'desc': 'Эта статья описывает новый метод под названием Dual Caption Preference Optimization (DCPO) для улучшения моделей диффузии текст-в-изображение. DCPO использует два отдельных описания для решения проблемы нерелевантных промптов и конфликтующих распределений в наборах данных предпочтений. Авторы также представляют новый датасет Pick-Double Caption с отдельными подписями для предпочтительных и менее предпочтительных изображений. Эксперименты показывают, что DCPO значительно улучшает качество изображений и их соответствие промптам по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Image Generation with Dual Captions!', 'desc': 'This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics.'}, 'zh': {'title': '双重标题优化，提升图像质量！', 'desc': '最近在大型语言模型（LLMs）中发展的人类偏好优化技术，显示出在改进文本到图像扩散模型方面的巨大潜力。这些方法旨在学习偏好样本的分布，并将其与不太偏好的样本区分开来。然而，现有的偏好数据集通常存在分布重叠的问题，导致冲突分布。此外，我们发现输入提示中包含与不太偏好的图像无关的信息，这限制了去噪网络在偏好优化方法中的准确预测能力。为了解决这些挑战，我们提出了双重标题偏好优化（DCPO），利用两个不同的标题来减轻无关提示的问题。'}}}, {'id': 'https://huggingface.co/papers/2502.05431', 'title': 'APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding', 'url': 'https://huggingface.co/papers/2502.05431', 'abstract': "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.", 'score': 0, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '7bc5b7aeb3716893', 'authors': ['Xinyu Yang', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2502.05431.jpg', 'data': {'categories': ['#rag', '#optimization', '#inference', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение генерации с контекстом: адаптивное параллельное кодирование', 'desc': 'Статья представляет новый метод адаптивного параллельного кодирования (APE) для эффективной обработки множественных контекстов в задачах генерации с использованием контекста. APE позволяет предварительно вычислять и кэшировать KV-состояния каждого контекста независимо, что значительно ускоряет процесс обработки запросов. Метод решает проблему несоответствия распределения внимания при параллельном кодировании, используя общий префикс, температуру внимания и масштабирующий фактор. Эксперименты показывают, что APE сохраняет до 98% производительности последовательного кодирования, превосходя обычное параллельное кодирование на 3.6-7.9% в задачах RAG и ICL.'}, 'en': {'title': 'Boosting Efficiency in Context-Augmented Generation with APE', 'desc': 'This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently.'}, 'zh': {'title': '自适应并行编码：提升上下文生成效率的关键', 'desc': '本文探讨了上下文增强生成（CAG）技术中的并行编码方法，以提高生成用户查询响应的效率。传统方法在每次请求时都需要重新编码多个上下文，导致计算负担过重。我们提出了自适应并行编码（APE），通过共享前缀、注意力温度和缩放因子来调整并行编码与顺序编码的注意力分布，从而提高性能。实验结果表明，APE在保持高性能的同时，能够显著加快处理速度，适用于处理大量上下文。'}}}, {'id': 'https://huggingface.co/papers/2501.19393', 'title': 's1: Simple test-time scaling', 'url': 'https://huggingface.co/papers/2501.19393', 'abstract': 'Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\'s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\'s thinking process or lengthening it by appending "Wait" multiple times to the model\'s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.', 'score': 48, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8fcf84a9effc288f', 'authors': ['Niklas Muennighoff', 'Zitong Yang', 'Weijia Shi', 'Xiang Lisa Li', 'Li Fei-Fei', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Percy Liang', 'Emmanuel Candès', 'Tatsunori Hashimoto'], 'affiliations': ['Allen Institute for AI', 'Contextual AI', 'Stanford University', 'University of Washington, Seattle'], 'pdf_title_img': 'assets/pdf/title_img/2501.19393.jpg', 'data': {'categories': ['#dataset', '#open_source', '#reasoning', '#training', '#math', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Простое масштабирование для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет новый подход к языковому моделированию, называемый тестовым масштабированием. Авторы разработали модель s1, основанную на Qwen2.5-32B-Instruct, и метод бюджетного форсирования для контроля вычислений во время тестирования. Модель обучена на специально отобранном наборе данных s1K из 1000 вопросов с рассуждениями. Результаты показывают, что s1 превосходит модель o1-preview от OpenAI на математических задачах, демонстрируя эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing Language Models with Test-Time Scaling', 'desc': "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."}, 'zh': {'title': '测试时间扩展：提升语言模型性能的新方法', 'desc': '本文介绍了一种新的语言建模方法——测试时间扩展，旨在通过增加测试时间计算来提高模型性能。研究者们创建了一个包含1000个问题及其推理过程的小数据集s1K，并通过难度、多样性和质量三个标准进行验证。为了控制测试时间计算，提出了预算强制的方法，通过强制终止模型的思考过程或在模型生成时多次添加“等待”来延长思考时间，从而促使模型检查答案。经过监督微调后，模型s1在数学竞赛问题上超越了OpenAI的o1模型，表现提升达27%。'}}}, {'id': 'https://huggingface.co/papers/2501.19324', 'title': 'Reward-Guided Speculative Decoding for Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2501.19324', 'abstract': '', 'score': 26, 'issue_id': 1995, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'ce2d414eedfb7a1e', 'authors': ['Baohao Liao', 'Yuhui Xu', 'Hanze Dong', 'Junnan Li', 'Christof Monz', 'Silvio Savarese', 'Doyen Sahoo', 'Caiming Xiong'], 'affiliations': ['Language Technology Lab, University of Amsterdam', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.19324.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Новый шаг в обучении больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению LLM, который позволяет моделям лучше понимать контекст и улучшать качество генерации текста. Исследователи предложили метод, который использует более сложные архитектуры нейронных сетей для обработки больших объемов данных. Эксперименты показали, что предложенный метод значительно повышает точность и скорость работы моделей. Это открывает новые возможности для применения AI в различных областях, таких как обработка естественного языка и генерация контента.'}, 'en': {'title': 'Hybrid Networks: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '优化数据处理，提升机器学习性能', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的准确性和效率。作者提出了一种创新的方法，通过优化数据预处理和特征选择来增强学习效果。实验结果表明，该算法在多个数据集上表现优于现有技术。最终，研究表明，改进的数据处理流程对机器学习模型的性能至关重要。'}}}, {'id': 'https://huggingface.co/papers/2501.18119', 'title': 'Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models', 'url': 'https://huggingface.co/papers/2501.18119', 'abstract': 'Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.', 'score': 12, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': 'd751c8a690173842', 'authors': ['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], 'affiliations': ['National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2501.18119.jpg', 'data': {'categories': ['#inference', '#graphs', '#transfer_learning', '#training', '#multimodal', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективная интеграция графов знаний и языковых моделей через квантованные коды', 'desc': 'Статья представляет двухэтапный подход к интеграции графов знаний с большими языковыми моделями. Авторы предлагают метод самоконтролируемого квантованного представления (SSQR) для сжатия структурных и семантических знаний графа в дискретные коды. Эти коды затем используются для создания инструкций для обучения языковых моделей. Эксперименты показывают превосходство SSQR над существующими методами и улучшение производительности моделей LLaMA2 и LLaMA3.1 на задачах, связанных с графами знаний.'}, 'en': {'title': 'Seamless Integration of Knowledge Graphs and Language Models', 'desc': 'This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens.'}, 'zh': {'title': '无缝整合知识图谱与大型语言模型', 'desc': '本论文探讨了知识图谱（KG）结构与自然语言之间的差距，提出了一种两阶段框架，以实现KG与大型语言模型（LLM）的有效整合。首先，提出了一种自监督量化表示（SSQR）方法，将KG的结构和语义知识压缩为离散代码（即令牌），使其与语言句子的格式对齐。接着，设计了KG指令跟随数据，将这些学习到的代码视为特征，直接输入到LLM中，从而实现无缝整合。实验结果表明，SSQR在无监督量化方法中表现优越，生成的代码更具可区分性，且经过微调的LLaMA2和LLaMA3.1在KG链接预测和三元组分类任务中表现出色，仅使用每个实体16个令牌，而不是传统方法中的数千个。'}}}, {'id': 'https://huggingface.co/papers/2501.19339', 'title': 'PixelWorld: Towards Perceiving Everything as Pixels', 'url': 'https://huggingface.co/papers/2501.19339', 'abstract': 'Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models\' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models\' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.', 'score': 9, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '3e10b792328f7a4b', 'authors': ['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], 'affiliations': ['Department of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2501.19339.jpg', 'data': {'categories': ['#agi', '#benchmark', '#optimization', '#dataset', '#open_source', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Единый пиксельный взгляд на мир: новая парадигма для ИИ', 'desc': 'В статье предлагается новый подход к обработке различных модальностей данных (текст, изображения, код и т.д.) в виде пикселей, названный PEAP (Perceive Everything as Pixels). Авторы представляют набор данных PixelWorld для оценки эффективности моделей в этом unified подходе. Результаты показывают, что PEAP превосходит базовые модели на мультимодальных данных, но выявляет снижение производительности в задачах рассуждения и кодирования. Исследование демонстрирует потенциал и ограничения восприятия пиксельных данных современными языковыми моделями.'}, 'en': {'title': 'Unifying Perception: Everything as Pixels', 'desc': "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."}, 'zh': {'title': '统一感知：将一切视为像素', 'desc': '本论文提出了一种新的统一感知框架，称为“将一切视为像素”（PEAP），旨在将文本、表格、代码、图表和图像等多种输入形式统一为像素输入。我们引入了PixelWorld评估套件，以在像素空间中评估现有模型的性能。研究发现，PEAP在多模态数据集上优于基于标记的输入，显示出统一输入在消歧义方面的优势。同时，处理像素输入时，所有模型的推理和编码能力显著下降，表明需要增强基础模型的感知能力。'}}}, {'id': 'https://huggingface.co/papers/2501.14677', 'title': 'MatAnyone: Stable Video Matting with Consistent Memory Propagation', 'url': 'https://huggingface.co/papers/2501.14677', 'abstract': 'Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.', 'score': 6, 'issue_id': 2010, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 января', 'en': 'January 24', 'zh': '1月24日'}, 'hash': 'a9968478421ddc33', 'authors': ['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2501.14677.jpg', 'data': {'categories': ['#training', '#dataset', '#video'], 'emoji': '✂️', 'ru': {'title': 'MatAnyone: Точное выделение объектов на видео с помощью памяти и адаптивного обучения', 'desc': 'MatAnyone - это новый подход к выделению объектов на видео без вспомогательных данных. Он использует модуль памяти для адаптивного объединения информации из предыдущих кадров, обеспечивая стабильность основных областей и сохраняя детали на границах объектов. Авторы также создали новый большой набор данных для обучения и разработали стратегию, использующую данные сегментации для повышения стабильности. MatAnyone превосходит существующие методы в различных сценариях реального мира.'}, 'en': {'title': 'MatAnyone: Robust Video Matting with Memory Propagation', 'desc': 'The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods.'}, 'zh': {'title': 'MatAnyone：视频抠图的新突破', 'desc': '本论文提出了一种名为MatAnyone的视频抠图框架，旨在解决复杂背景下的抠图问题。该方法基于记忆传播模块，通过区域自适应记忆融合，动态整合前一帧的记忆信息，从而确保核心区域的语义稳定性。为了提高训练的鲁棒性，研究团队构建了一个更大、更高质量且多样化的视频抠图数据集，并采用了一种新颖的训练策略，充分利用大规模分割数据。最终，MatAnyone在多种真实场景中展现出优越的抠图效果，超越了现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2501.19399', 'title': 'Scalable-Softmax Is Superior for Attention', 'url': 'https://huggingface.co/papers/2501.19399', 'abstract': "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", 'score': 6, 'issue_id': 2009, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '12ed1cad789702aa', 'authors': ['Ken M. Nakanishi'], 'affiliations': ['Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2501.19399.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'SSMax: улучшение внимания трансформеров для длинных текстов', 'desc': 'Статья представляет новую функцию Scalable-Softmax (SSMax) для улучшения работы трансформеров с длинными контекстами. SSMax решает проблему уплощения распределения внимания при увеличении размера входного вектора. Эксперименты показывают, что модели с SSMax быстрее обучаются и лучше работают с длинными текстами. SSMax позволяет модели фокусироваться на ключевой информации даже в длинных контекстах.'}, 'en': {'title': 'Enhancing Attention with Scalable-Softmax for Better Context Handling', 'desc': "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."}, 'zh': {'title': '可扩展Softmax：提升Transformer模型的注意力能力', 'desc': '本文提出了一种新的方法，称为可扩展Softmax（SSMax），旨在解决Transformer模型在处理长上下文时的注意力分布扁平化问题。传统的Softmax函数在输入向量增大时，最大元素趋近于零，导致模型无法有效地优先考虑关键信息。SSMax可以无缝集成到现有的Transformer架构中，实验结果表明，使用SSMax的模型在语言建模中不仅在预训练期间实现了更快的损失减少，还显著提高了在长上下文中的性能。通过分析注意力分数，SSMax使模型能够在长上下文中更好地关注关键信息，提升了模型的长度泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.04983', 'title': 'DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning', 'url': 'https://huggingface.co/papers/2411.04983', 'abstract': 'The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.', 'score': 6, 'issue_id': 1999, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'e72081596b626524', 'authors': ['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], 'affiliations': ['Courant Institute, New York University', 'Meta-FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2411.04983.jpg', 'data': {'categories': ['#cv', '#agents', '#reasoning', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DINO-WM: универсальная модель мира для планирования поведения без реконструкции', 'desc': 'Статья представляет новый метод моделирования визуальной динамики без реконструкции визуального мира - DINO World Model (DINO-WM). DINO-WM использует пространственные признаки патчей, предварительно обученные с помощью DINOv2, что позволяет ему учиться на офлайн-траекториях поведения, предсказывая будущие признаки патчей. Этот подход позволяет DINO-WM достигать наблюдаемых целей путем оптимизации последовательности действий, облегчая планирование поведения, независимое от задачи. Эксперименты показывают, что DINO-WM может генерировать поведенческие решения с нуля во время тестирования, демонстрируя сильные способности к обобщению по сравнению с предыдущими современными методами.'}, 'en': {'title': 'DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning', 'desc': 'This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities.'}, 'zh': {'title': 'DINO-WM：无任务依赖的世界模型', 'desc': '本文提出了一种新的世界模型DINO-WM，旨在通过被动数据进行推理和规划。DINO-WM具有三个关键特性：可以在离线收集的轨迹上进行训练，支持测试时行为优化，并促进任务无关的推理。该模型利用DINOv2预训练的空间补丁特征，通过预测未来的补丁特征来学习，从而实现观察目标的行为规划。实验结果表明，DINO-WM在多种任务中表现出色，能够在没有专家示范和奖励建模的情况下生成零-shot行为解决方案。'}}}, {'id': 'https://huggingface.co/papers/2501.18837', 'title': 'Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming', 'url': 'https://huggingface.co/papers/2501.18837', 'abstract': 'Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.', 'score': 4, 'issue_id': 1996, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '62d14973b1140e58', 'authors': ['Mrinank Sharma', 'Meg Tong', 'Jesse Mu', 'Jerry Wei', 'Jorrit Kruthoff', 'Scott Goodfriend', 'Euan Ong', 'Alwin Peng', 'Raj Agarwal', 'Cem Anil', 'Amanda Askell', 'Nathan Bailey', 'Joe Benton', 'Emma Bluemke', 'Samuel R. Bowman', 'Eric Christiansen', 'Hoagy Cunningham', 'Andy Dau', 'Anjali Gopal', 'Rob Gilson', 'Logan Graham', 'Logan Howard', 'Nimit Kalra', 'Taesung Lee', 'Kevin Lin', 'Peter Lofgren', 'Francesco Mosconi', "Clare O'Hara", 'Catherine Olsson', 'Linda Petrini', 'Samir Rajani', 'Nikhil Saxena', 'Alex Silverstein', 'Tanya Singh', 'Theodore Sumers', 'Leonard Tang', 'Kevin K. Troy', 'Constantin Weisser', 'Ruiqi Zhong', 'Giulio Zhou', 'Jan Leike', 'Jared Kaplan', 'Ethan Perez'], 'affiliations': ['Safeguards Research Team, Anthropic'], 'pdf_title_img': 'assets/pdf/title_img/2501.18837.jpg', 'data': {'categories': ['#synthetic', '#training', '#architecture', '#dataset', '#security', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Конституционные Классификаторы: надежная защита языковых моделей', 'desc': 'Исследование представляет концепцию Конституционных Классификаторов - защитных механизмов для больших языковых моделей (LLM), обученных на синтетических данных с использованием правил, определяющих допустимый контент. Эти классификаторы эффективно противостоят универсальным методам обхода защиты, не позволяя извлекать вредоносную информацию из защищенных моделей. Эксперименты показали устойчивость классификаторов к различным атакам и их практическую применимость с минимальным влиянием на производительность. Исследование демонстрирует возможность эффективной защиты LLM от универсальных методов обхода при сохранении практической применимости.'}, 'en': {'title': 'Defending LLMs with Constitutional Classifiers', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications.'}, 'zh': {'title': '宪法分类器：保护大型语言模型的安全', 'desc': '大型语言模型（LLMs）容易受到普遍越狱攻击，这种攻击可以绕过模型的安全措施，允许用户进行有害操作。为此，我们提出了宪法分类器，这是一种基于合成数据训练的安全措施，合成数据是通过自然语言规则（即宪法）提示LLMs生成的，规定了允许和限制的内容。在超过3000小时的红队测试中，没有红队成员找到能够从早期分类器保护的LLM中提取信息的普遍越狱方法。我们的研究表明，在保持实际部署可行性的同时，防御普遍越狱攻击是可行的。'}}}, {'id': 'https://huggingface.co/papers/2501.18841', 'title': 'Trading Inference-Time Compute for Adversarial Robustness', 'url': 'https://huggingface.co/papers/2501.18841', 'abstract': 'We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.', 'score': 3, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'f1e75e6b24f3e044', 'authors': ['Wojciech Zaremba', 'Evgenia Nitishinskaya', 'Boaz Barak', 'Stephanie Lin', 'Sam Toyer', 'Yaodong Yu', 'Rachel Dias', 'Eric Wallace', 'Kai Xiao', 'Johannes Heidecke', 'Amelia Glaese'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.18841.jpg', 'data': {'categories': ['#security', '#reasoning', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Больше вычислений - выше защита: повышение устойчивости ИИ к атакам', 'desc': 'Исследование посвящено влиянию увеличения вычислительных ресурсов во время вывода на устойчивость моделей рассуждений к состязательным атакам. Эксперименты показали, что увеличение вычислений при выводе улучшает робастность моделей к различным атакам. В большинстве случаев доля успешных атак стремится к нулю при росте вычислительных ресурсов. Результаты указывают на потенциал увеличения вычислений при выводе для повышения устойчивости больших языковых моделей к состязательным атакам.'}, 'en': {'title': 'Boosting Robustness: More Compute, Less Vulnerability', 'desc': "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."}, 'zh': {'title': '增加推理计算，提升模型鲁棒性', 'desc': '本研究探讨了在推理模型中增加推理时间计算对其抵御对抗攻击的影响。我们发现，随着推理时间计算的增加，模型的鲁棒性得到了提升。大多数情况下，攻击成功的模型样本比例随着测试时间计算的增加而趋近于零。我们的结果表明，推理时间计算有潜力提高大型语言模型的对抗鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2501.18052', 'title': 'SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2501.18052', 'abstract': "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.", 'score': 2, 'issue_id': 2011, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': 'd94056a77d806ada', 'authors': ['Bartosz Cywiński', 'Kamil Deja'], 'affiliations': ['IDEAS NCBR', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2501.18052.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#security', '#architecture', '#ethics', '#training', '#benchmark'], 'emoji': '🧹', 'ru': {'title': 'Чистка нейросетей: SAeUron удаляет нежелательные концепции из диффузионных моделей', 'desc': 'SAeUron - это новый метод удаления нежелательных концепций в диффузионных моделях для генерации изображений по тексту. Он использует разреженные автоэнкодеры (SAE) для выделения интерпретируемых признаков, соответствующих конкретным концепциям. Метод позволяет точно вмешиваться в активации модели для блокировки целевого контента при сохранении общей производительности. SAeUron показывает лучшие результаты по сравнению с другими методами и может удалять несколько концепций одновременно.'}, 'en': {'title': 'SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders', 'desc': "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."}, 'zh': {'title': 'SAeUron：去除不良内容的新方法', 'desc': '扩散模型虽然强大，但可能会生成有害或不良内容，带来伦理和安全问题。最近的机器遗忘方法提供了潜在的解决方案，但通常缺乏透明性，难以理解对基础模型的更改。我们提出了一种新方法SAeUron，利用稀疏自编码器（SAE）学习的特征来去除文本到图像扩散模型中的不必要概念。通过在多个去噪时间步的激活上无监督训练SAE，我们捕捉到与特定概念对应的稀疏和可解释特征，从而实现精确干预。'}}}, {'id': 'https://huggingface.co/papers/2501.18804', 'title': 'Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion', 'url': 'https://huggingface.co/papers/2501.18804', 'abstract': 'Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.', 'score': 2, 'issue_id': 2010, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '32db517ad974401b', 'authors': ['Vitor Guizilini', 'Muhammad Zubair Irshad', 'Dian Chen', 'Greg Shakhnarovich', 'Rares Ambrus'], 'affiliations': ['Toyota Research Institute (TRI)', 'Toyota Technological Institute at Chicago (TTIC)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18804.jpg', 'data': {'categories': ['#training', '#3d', '#diffusion', '#architecture', '#benchmark', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'Генерация 3D сцен из разных ракурсов с помощью диффузионной модели', 'desc': 'Статья представляет MVGD - архитектуру на основе диффузии для генерации изображений и карт глубины с новых ракурсов. Метод использует райкарты для обогащения визуальных признаков пространственной информацией и направления генерации. Ключевой аспект - многозадачная генерация изображений и карт глубины с использованием обучаемых эмбеддингов задач. Модель обучена на более чем 60 миллионах мультиракурсных примеров и показывает современные результаты в задачах синтеза новых ракурсов и оценки глубины.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation', 'desc': 'This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks.'}, 'zh': {'title': 'MVGD：从多视角直接生成图像与深度图的创新方法', 'desc': '本文提出了一种名为MVGD的扩散基础架构，能够直接从多个视角生成图像和深度图。该方法通过光线图条件化，增强了视觉特征并引导生成过程。我们采用多任务生成技术，同时生成图像和深度图，并使用可学习的任务嵌入来优化扩散过程。经过在超过6000万多视角样本上的训练，我们在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2501.18965', 'title': 'The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training', 'url': 'https://huggingface.co/papers/2501.18965', 'abstract': 'We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.', 'score': 2, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a136293a2241150e', 'authors': ['Fabian Schaipp', 'Alexander Hägele', 'Adrien Taylor', 'Umut Simsekli', 'Francis Bach'], 'affiliations': ['EPFL, Lausanne, Switzerland', 'Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2501.18965.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Оптимизация графиков обучения для больших языковых моделей', 'desc': 'В статье исследуются графики изменения скорости обучения для больших моделей машинного обучения. Авторы обнаружили неожиданное сходство этих графиков с теоретическими границами из теории невыпуклой оптимизации. Они предлагают новый метод настройки скорости обучения, основанный на этом наблюдении. Применение метода позволило улучшить результаты обучения языковых моделей типа Llama размером 124M и 210M параметров.'}, 'en': {'title': 'Optimizing Learning Rates: Bridging Theory and Practice', 'desc': 'This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models.'}, 'zh': {'title': '优化学习率调度，提升大模型训练效果', 'desc': '本文探讨了大模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间的相似性。我们提供了一个线性冷却的常数调度的界限，特别是冷却的实际好处在于没有对数项的影响。进一步地，我们展示了优化理论与实践之间的紧密联系可以用于学习率调优：通过延长调度以继续训练并使用最佳学习率，我们在训练124M和210M的Llama类型模型时取得了显著的改进。最后，我们还展示了在不同调度之间转移最佳学习率的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.18753', 'title': 'INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation', 'url': 'https://huggingface.co/papers/2501.18753', 'abstract': 'Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.', 'score': 2, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '000663cf445862a1', 'authors': ['Jian Hu', 'Zixu Cheng', 'Shaogang Gong'], 'affiliations': ['Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2501.18753.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Адаптивная сегментация изображений с помощью интеллектуального отбора промптов', 'desc': 'Статья представляет новый метод сегментации изображений под названием INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). Этот подход направлен на улучшение генерации промптов, специфичных для конкретных экземпляров изображений, путем адаптивного уменьшения влияния нерелевантных знаний и усиления наиболее вероятных. INT состоит из двух компонентов: генерации промптов, специфичных для экземпляров, и генерации семантической маски. Метод был проверен на шести наборах данных, включая камуфлированные объекты и медицинские изображения, демонстрируя эффективность, надежность и масштабируемость.'}, 'en': {'title': 'Enhancing Image Segmentation with Smart Prompting', 'desc': 'This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results.'}, 'zh': {'title': '实例特定负采样优化图像分割', 'desc': '这篇论文提出了一种新的方法，称为实例特定负采样（INT），用于任务通用的可提示图像分割。INT的核心思想是自适应地减少无关的先验知识的影响，同时增加最有可能的先验知识的使用，以优化实例特定提示的生成。该方法包括两个主要部分：实例特定提示生成和语义掩码生成，确保每个图像实例的分割与提示的语义相匹配。通过在六个数据集上的验证，INT展示了其有效性、鲁棒性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2501.18128', 'title': 'Unraveling the Capabilities of Language Models in News Summarization', 'url': 'https://huggingface.co/papers/2501.18128', 'abstract': "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", 'score': 2, 'issue_id': 2000, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '1c3f3a16953a5a59', 'authors': ['Abdurrahman Odabaşı', 'Göksel Biricik'], 'affiliations': ['Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye', 'Department of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye'], 'pdf_title_img': 'assets/pdf/title_img/2501.18128.jpg', 'data': {'categories': ['#survey', '#multilingual', '#small_models', '#benchmark', '#transfer_learning'], 'emoji': '📰', 'ru': {'title': 'Маленькие модели бросают вызов гигантам в суммаризации новостей', 'desc': 'В этой работе проводится комплексное сравнение 20 современных языковых моделей, с акцентом на меньшие модели, для задачи суммаризации новостей. Исследование охватывает обучение в режимах zero-shot и few-shot на трех различных наборах данных, используя автоматические метрики, человеческую оценку и LLM в качестве судьи. Интересно, что включение демонстрационных примеров в режиме few-shot не улучшило производительность моделей, а в некоторых случаях даже ухудшило качество генерируемых сводок. Результаты показали превосходство GPT-3.5-Turbo и GPT-4, но также выявили перспективные открытые модели, такие как Qwen1.5-7B и SOLAR-10.7B-Instruct-v1.0.'}, 'en': {'title': 'Benchmarking News Summarization: Small Models Can Compete!', 'desc': 'This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks.'}, 'zh': {'title': '小模型在新闻摘要中的潜力与挑战', 'desc': '本研究对20种最新的语言模型进行了全面的基准测试，重点关注较小的模型在新闻摘要任务中的表现。我们系统地测试了这些模型在不同风格的新闻文章摘要中的能力和有效性，使用了三种不同的数据集。研究发现，在少量示例学习的设置中，提供示例并未提升模型的性能，反而在某些情况下导致生成摘要的质量下降。这主要是由于参考摘要的质量较差，影响了模型的表现，同时我们的研究结果显示，GPT-3.5-Turbo和GPT-4在性能上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2404.07097', 'title': 'Fast Encoder-Based 3D from Casual Videos via Point Track Processing', 'url': 'https://huggingface.co/papers/2404.07097', 'abstract': 'This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.', 'score': 1, 'issue_id': 1999, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'a526ae197fe3a8c7', 'authors': ['Yoni Kasten', 'Wuyue Lu', 'Haggai Maron'], 'affiliations': ['NVIDIA Research', 'Simon Fraser University', 'Technion'], 'pdf_title_img': 'assets/pdf/title_img/2404.07097.jpg', 'data': {'categories': ['#3d', '#training', '#cv', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная 3D реконструкция из видео с помощью глубокого обучения', 'desc': 'Статья представляет TracksTo4D - новый подход к реконструкции 3D структур из видео с динамическим содержанием. Метод использует нейронную сеть, обученную без учителя на 2D треках точек, извлеченных из обычных видео. TracksTo4D позволяет восстанавливать облако точек и положение камеры за один проход сети, значительно сокращая время вычислений по сравнению с существующими методами. Архитектура сети учитывает симметрии входных данных и предполагает низкоранговое представление паттернов движения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction from Casual Videos', 'desc': 'This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time.'}, 'zh': {'title': '高效重建3D结构，TracksTo4D引领新潮流', 'desc': '本文解决了从动态内容视频重建3D结构的长期挑战。现有方法无法处理普通相机录制的随意视频，或需要较长的优化时间。我们提出了一种基于学习的方法TracksTo4D，通过单次高效的前馈传递，从随意视频中推断3D结构和相机位置。该方法直接处理2D点轨迹，并设计了专门的架构，能够在无监督的情况下进行训练，显著提高了重建效率。'}}}, {'id': 'https://huggingface.co/papers/2502.02492', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models', 'url': 'https://huggingface.co/papers/2502.02492', 'abstract': "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/", 'score': 23, 'issue_id': 2042, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '33581479f8c6ed9f', 'authors': ['Hila Chefer', 'Uriel Singer', 'Amit Zohar', 'Yuval Kirstain', 'Adam Polyak', 'Yaniv Taigman', 'Lior Wolf', 'Shelly Sheynin'], 'affiliations': ['GenAI, Meta', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02492.jpg', 'data': {'categories': ['#video'], 'emoji': '🎬', 'ru': {'title': 'VideoJAM: Реалистичное движение в генеративных видеомоделях', 'desc': 'Статья представляет VideoJAM - новый подход к генерации видео, решающий проблему недостаточной реалистичности движения в существующих моделях. Авторы предлагают обучать модель совместному представлению внешнего вида и движения, а также вводят механизм Inner-Guidance для улучшения когерентности движения при генерации. VideoJAM может быть применен к любой видеомодели без существенных изменений и показывает лучшие результаты по сравнению с существующими решениями. Исследование демонстрирует, что интеграция внешнего вида и движения улучшает как визуальное качество, так и согласованность генерируемого видео.'}, 'en': {'title': 'Enhancing Video Generation with Motion Coherence', 'desc': 'This paper addresses the challenges faced by generative video models in accurately capturing real-world motion and dynamics. The authors identify that traditional pixel reconstruction methods prioritize visual appearance over motion coherence, leading to less realistic video outputs. To overcome this, they propose VideoJAM, a framework that integrates a motion prior into video generation by learning a combined representation of appearance and motion. By extending the training objective and introducing a dynamic guidance mechanism during inference, VideoJAM significantly improves motion coherence and visual quality, outperforming existing models without requiring changes to training data or model architecture.'}, 'zh': {'title': 'VideoJAM：提升视频生成的运动一致性与视觉质量', 'desc': '尽管生成视频模型在最近取得了巨大进展，但仍然难以捕捉真实世界的运动和动态。本文提出了VideoJAM框架，通过引入有效的运动先验，帮助视频生成器学习联合的外观-运动表示。该框架在训练过程中扩展了目标，预测生成的像素及其对应的运动，并在推理阶段引入了内部引导机制，以实现一致的运动生成。VideoJAM在运动一致性方面达到了最先进的性能，同时提升了生成视频的视觉质量，表明外观和运动可以互为补充，合理整合后能增强视频生成的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.01362', 'title': 'Inverse Bridge Matching Distillation', 'url': 'https://huggingface.co/papers/2502.01362', 'abstract': 'Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.', 'score': 22, 'issue_id': 2045, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '061049a23278b0f6', 'authors': ['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], 'affiliations': ['Skolkovo Institute of Science and Technology', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01362.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных мостов: искусство эффективной дистилляции', 'desc': 'Статья представляет новый метод дистилляции для моделей диффузионного моста (DBM). Этот подход позволяет ускорить вывод DBM от 4 до 100 раз, сохраняя или даже улучшая качество генерации. Метод основан на формулировке обратного сопоставления моста и может применяться как к условным, так и к безусловным DBM. Техника была успешно протестирована на различных задачах, включая суперразрешение и восстановление JPEG.'}, 'en': {'title': 'Accelerating Diffusion Bridge Models with Innovative Distillation Techniques', 'desc': 'This paper introduces a new method to improve the speed and practicality of diffusion bridge models (DBMs), which are used for tasks like image translation. The authors present a distillation technique that leverages inverse bridge matching to create a more efficient training process. This method allows for the distillation of both conditional and unconditional DBMs, using only corrupted images, and enables one-step generation. The results show that their approach can significantly speed up inference times by up to 100 times while maintaining or even enhancing the quality of generated images compared to the original models.'}, 'zh': {'title': '加速扩散桥模型，提升生成质量！', 'desc': '扩散桥模型（DBMs）是一种有前景的扩展，适用于图像到图像的转换。然而，DBMs在推理时速度较慢，这是现代扩散和流模型普遍面临的问题。为了解决这个问题，我们提出了一种基于逆桥匹配的蒸馏技术，并推导出可行的目标来实际解决它。我们的蒸馏方法能够同时处理条件和无条件的DBMs，并且只使用损坏的图像进行训练，从而显著加快推理速度。'}}}, {'id': 'https://huggingface.co/papers/2502.01718', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'url': 'https://huggingface.co/papers/2502.01718', 'abstract': 'Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.', 'score': 15, 'issue_id': 2041, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b5b43fe7221df9d8', 'authors': ['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent Researcher', 'Netmind.AI', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2502.01718.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении моделей кода: мощь RL и автоматизированных тест-кейсов', 'desc': 'В этой статье представлен новый подход к обучению моделей для генерации кода с использованием обучения с подкреплением (RL). Авторы разработали пайплайн для создания пар (вопрос, тест-кейсы) из существующих кодовых данных, которые затем используются для обучения моделей вознаграждения. Применение этого метода привело к значительным улучшениям производительности моделей на различных бенчмарках, включая HumanEval и MBPP. Результаты исследования показывают большой потенциал обучения с подкреплением в области моделей для генерации кода.'}, 'en': {'title': 'Unlocking the Power of Reinforcement Learning in Coder Models', 'desc': 'This paper explores the use of reinforcement learning (RL) to improve coder models, which have primarily relied on supervised fine-tuning (SFT). The authors introduce a method for generating large-scale test-case pairs from existing code, which helps create reliable reward signals for training. By employing a preference-based reward model using the Bradley-Terry loss, they achieve significant performance gains in various coding benchmarks. The results demonstrate that RL can substantially enhance the capabilities of coder models, showcasing its untapped potential in this domain.'}, 'zh': {'title': '强化学习提升代码模型的潜力', 'desc': '本文探讨了在代码模型训练中使用强化学习（RL）的潜力，尤其是在缺乏可靠奖励数据的情况下。我们设计了一种自动化的大规模测试用例合成管道，以生成大量（问题，测试用例）对，从而增强代码模型的训练。通过使用这些测试用例，我们构建了基于通过率的偏好对，并利用Bradley-Terry损失训练奖励模型。实验结果表明，使用强化学习后，模型在多个基准测试上均有显著提升，展示了强化学习在代码模型中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.02584', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'url': 'https://huggingface.co/papers/2502.02584', 'abstract': 'Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.', 'score': 11, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'f2d3938d4ad71761', 'authors': ['Zongyu Lin', 'Yao Tang', 'Xingcheng Yao', 'Da Yin', 'Ziniu Hu', 'Yizhou Sun', 'Kai-Wei Chang'], 'affiliations': ['Shanghai Jiaotong University, Shanghai, China', 'University of California, Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.02584.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#inference', '#reasoning', '#agents', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'QLASS: Пошаговое обучение языковых агентов для повышения эффективности', 'desc': 'Статья представляет новый метод QLASS для обучения языковых агентов. QLASS использует пошаговую оценку Q-значений для генерации аннотаций и улучшения промежуточного обучения. Метод вводит дерево рассуждений и моделирование вознаграждений процесса для эффективного пошагового руководства. QLASS позволяет языковым агентам лучше адаптироваться к долгосрочным целям, значительно улучшая производительность при решении сложных интерактивных задач.'}, 'en': {'title': 'Enhancing Language Agents with Stepwise Q-Guidance', 'desc': 'This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.'}, 'zh': {'title': 'QLASS：提升语言代理的决策能力', 'desc': '本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。'}}}, {'id': 'https://huggingface.co/papers/2502.02508', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'url': 'https://huggingface.co/papers/2502.02508', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.", 'score': 9, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '80bd687783bd609b', 'authors': ['Maohao Shen', 'Guangtao Zeng', 'Zhenting Qi', 'Zhang-Wei Hong', 'Zhenfang Chen', 'Wei Lu', 'Gregory Wornell', 'Subhro Das', 'David Cox', 'Chuang Gan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab, IBM Research', 'Singapore University of Technology and Design', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.02508.jpg', 'data': {'categories': ['#small_models', '#open_source', '#training', '#reasoning', '#math', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Satori: LLM с внутренним поиском для улучшенного рассуждения', 'desc': 'Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассуждению. Они разработали метод Chain-of-Action-Thought (COAT), который позволяет модели проводить самоанализ и исследовать новые стратегии решения задач. Процесс обучения включает два этапа: настройку формата на небольшом масштабе и масштабное самосовершенствование с использованием обучения с подкреплением. В результате была создана 7-миллиардная модель Satori, показавшая отличные результаты в задачах математического рассуждения и обобщения на новые области.'}, 'en': {'title': 'Empowering LLMs with Internalized Reasoning through COAT', 'desc': 'This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.'}, 'zh': {'title': '内化搜索能力，提升推理能力！', 'desc': '大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。'}}}, {'id': 'https://huggingface.co/papers/2502.01941', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'url': 'https://huggingface.co/papers/2502.01941', 'abstract': "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.", 'score': 7, 'issue_id': 2041, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '1352d78ee18eadfa', 'authors': ['Xiang Liu', 'Zhenheng Tang', 'Hong Chen', 'Peijie Dong', 'Zeyu Li', 'Xiuze Zhou', 'Bo Li', 'Xuming Hu', 'Xiaowen Chu'], 'affiliations': ['The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China', 'The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.01941.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Сжатие KV-кэша в LLM: баланс между эффективностью и производительностью', 'desc': 'Статья исследует влияние методов сжатия KV-кэша на фундаментальные возможности больших языковых моделей (LLM). Авторы провели комплексный эмпирический анализ различных методов сжатия на разнообразных задачах, включая общие знания, здравый смысл, арифметические рассуждения и генерацию кода. Результаты показали, что сжатие KV-кэша может значительно ухудшать производительность модели, особенно в задачах арифметических рассуждений. На основе анализа авторы предложили новый метод ShotKV, который по-разному обрабатывает фазы предзаполнения и декодирования, сохраняя семантическую целостность.'}, 'en': {'title': 'Optimizing KV Cache Compression for Enhanced LLM Performance', 'desc': "This paper explores how compressing the KV cache in large language models (LLMs) affects their performance on various tasks. While compression can reduce memory usage, it may also lead to a decline in the model's ability to perform tasks like arithmetic reasoning and code generation. The study finds that different compression methods impact tasks differently, with some methods causing significant performance drops, especially in arithmetic reasoning. To address these issues, the authors introduce ShotKV, a new compression technique that improves performance on long-context tasks while preserving important semantic information."}, 'zh': {'title': 'KV缓存压缩对大型语言模型能力的影响研究', 'desc': '本文研究了大型语言模型（LLMs）中一个未被充分探讨的挑战：KV缓存压缩方法对LLMs基本能力的影响。虽然现有方法在长上下文基准测试中取得了令人印象深刻的压缩比，但它们对核心模型能力的影响仍然缺乏研究。我们的实证研究评估了多种KV缓存压缩方法在不同任务上的表现，包括世界知识、常识推理、算术推理、代码生成、安全性以及长上下文理解和生成。分析结果显示，KV缓存压缩方法在特定任务上表现出性能下降，尤其是算术推理任务对激进压缩特别敏感，性能下降幅度达到17.4%-43.3%。'}}}, {'id': 'https://huggingface.co/papers/2502.02589', 'title': 'COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.02589', 'abstract': 'This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning.', 'score': 6, 'issue_id': 2056, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '28c4625deea8ac72', 'authors': ['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2502.02589.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Детальные аннотации для лучшего понимания изображений', 'desc': 'Статья представляет набор данных COCONut-PanCap для улучшения панорамной сегментации и привязки подписей к изображениям. Этот датасет расширяет COCO, добавляя детальные панорамные маски и подробные описания сцен, привязанные к регионам изображения. COCONut-PanCap направлен на преодоление ограничений существующих наборов данных изображение-текст, которые часто не содержат всесторонних описаний сцен. Экспериментальные результаты показывают, что использование COCONut-PanCap значительно улучшает производительность моделей в задачах понимания и генерации изображений.'}, 'en': {'title': 'Enhancing Image Understanding with COCONut-PanCap Dataset', 'desc': 'The COCONut-PanCap dataset is designed to improve panoptic segmentation and grounded image captioning by providing detailed scene descriptions. It builds on the existing COCO dataset by adding advanced panoptic masks and fine-grained, region-level captions. This dataset enhances the training of vision-language models (VLMs) by offering high-quality, human-edited annotations that ensure consistency and detail in generated captions. Experimental results show that COCONut-PanCap significantly enhances performance in both understanding and generation tasks, establishing a new standard for evaluating models in multi-modal learning.'}, 'zh': {'title': 'COCONut-PanCap：提升全景分割与图像描述生成的新数据集', 'desc': '本文介绍了COCONut-PanCap数据集，旨在增强全景分割和基于图像的描述生成。该数据集在COCO数据集的基础上，结合了先进的COCONut全景掩码，克服了现有图像-文本数据集中缺乏详细场景描述的局限性。COCONut-PanCap数据集包含基于全景分割掩码的细粒度区域级描述，确保了一致性并提高了生成描述的细节。实验结果表明，COCONut-PanCap在理解和生成任务中显著提升了性能，为多模态学习中的高质量图像-文本注释需求提供了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2502.00674', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?', 'url': 'https://huggingface.co/papers/2502.00674', 'abstract': 'Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of 3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.', 'score': 5, 'issue_id': 2050, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '34ba4144afc562aa', 'authors': ['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.00674.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Один лучше многих: новый подход к ансамблированию языковых моделей', 'desc': 'Исследователи предложили новый метод ансамблирования под названием Self-MoA, который агрегирует выходные данные только одной наиболее эффективной языковой модели. Эксперименты показали, что Self-MoA превосходит стандартный метод Mixture-of-Agents (MoA), который объединяет разные языковые модели, на многих бенчмарках. Авторы изучили компромисс между разнообразием и качеством выходных данных в различных конфигурациях MoA. Также была представлена последовательная версия Self-MoA, способная агрегировать большое количество выходных данных языковой модели в несколько этапов.'}, 'en': {'title': 'Self-MoA: Elevating Performance with a Single Top LLM', 'desc': 'This paper investigates the effectiveness of an ensemble method called Self-MoA, which aggregates outputs from a single top-performing Large Language Model (LLM) instead of mixing multiple LLMs. The authors find that Self-MoA significantly outperforms the traditional Mixture-of-Agents (MoA) method, achieving notable improvements on various benchmarks. Their experiments reveal that the quality of outputs is crucial, as mixing different LLMs can reduce overall performance due to lower average quality. Additionally, the paper introduces a sequential version of Self-MoA that efficiently aggregates outputs over multiple rounds, maintaining high effectiveness.'}, 'zh': {'title': '单一模型集成，超越多样性', 'desc': '本论文探讨了混合不同大型语言模型（LLMs）输出的有效性，提出了一种新的集成方法Self-MoA。Self-MoA仅聚合单一表现最佳的LLM的输出，实验结果显示其在多个基准测试中表现优于传统的Mixture-of-Agents（MoA）方法。具体而言，Self-MoA在AlpacaEval 2.0基准上提高了6.6%的性能，并在多个基准上平均提高了3.8%。此外，论文还分析了输出多样性与质量之间的权衡，确认混合不同LLMs可能会降低模型的平均质量。'}}}, {'id': 'https://huggingface.co/papers/2501.19066', 'title': 'Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations', 'url': 'https://huggingface.co/papers/2501.19066', 'abstract': 'Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (e.g., nudity) or to introduce a new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of 20.01% in unsafe concept removal, is effective in style manipulation, and is sim5x faster than current state-of-the-art.', 'score': 4, 'issue_id': 2050, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a8acff84a873ecb8', 'authors': ['Dahye Kim', 'Deepti Ghadiyaram'], 'affiliations': ['Department of Computer Science, Boston U'], 'pdf_title_img': 'assets/pdf/title_img/2501.19066.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#interpretability', '#security', '#cv', '#training'], 'emoji': '🎨', 'ru': {'title': 'Безопасное и эффективное управление концепциями в генерации изображений', 'desc': 'Статья представляет новый подход к манипулированию концепциями в генеративных моделях изображений с использованием разреженных автоэнкодеров (k-SAE). Авторы предлагают метод для эффективного удаления нежелательного контента и добавления новых стилей без необходимости переобучения базовой модели. Эксперименты показывают, что предложенный подход превосходит существующие методы по скорости и эффективности удаления небезопасных концепций на 20.01%. Метод также демонстрирует устойчивость к состязательным атакам и сохраняет качество генерируемых изображений.'}, 'en': {'title': 'Efficient Concept Control in Text-to-Image Generation', 'desc': 'This paper presents a new method for improving text-to-image generative models by using k-sparse autoencoders (k-SAEs) to manipulate concepts in a more efficient and interpretable way. Instead of fine-tuning models, which can be slow and reduce quality, this approach allows for precise control over the generation of specific concepts, such as removing unsafe content or adding new styles. The authors demonstrate that their method does not require retraining the base model and is significantly faster than existing techniques, achieving a 20.01% improvement in unsafe concept removal. Overall, this framework enhances the safety and versatility of generative models while maintaining high-quality outputs.'}, 'zh': {'title': '高效操控生成模型中的概念', 'desc': '本文提出了一种新颖的框架，利用k稀疏自编码器（k-SAEs）来实现扩散模型中的概念高效且可解释的操控。我们首先在文本嵌入的潜在空间中识别可解释的单义概念，并利用这些概念精确地引导生成内容，避免或引入特定概念。通过大量实验，我们证明了该方法简单易用，无需重新训练基础模型或使用LoRA适配器，且不影响生成质量。我们的技术在去除不安全概念方面提高了20.01%，在风格操控上也表现出色，速度比当前最先进的方法快5倍。'}}}, {'id': 'https://huggingface.co/papers/2502.01720', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'url': 'https://huggingface.co/papers/2502.01720', 'abstract': 'Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.', 'score': 2, 'issue_id': 2043, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd249f21cea90b464', 'authors': ['Nupur Kumari', 'Xi Yin', 'Jun-Yan Zhu', 'Ishan Misra', 'Samaneh Azadi'], 'affiliations': ['Carnegie Mellon University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2502.01720.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#architecture', '#cv', '#inference'], 'emoji': '🎨', 'ru': {'title': 'Улучшение кастомизации моделей text-to-image с помощью синтетических данных и новой архитектуры', 'desc': 'Статья представляет новый подход к кастомизации моделей text-to-image. Авторы создали синтетический набор данных SynCD с множественными изображениями объектов в разных условиях. Они предложили новую архитектуру энкодера, основанную на механизмах разделяемого внимания, для лучшего учета визуальных деталей. Также был разработан метод вывода, нормализующий векторы текстового и изображенческого руководства для устранения проблем переэкспозиции.'}, 'en': {'title': 'Enhancing Customization in Text-to-Image Models with Synthetic Datasets', 'desc': 'This paper presents a novel approach to customize text-to-image models, allowing users to generate images of custom concepts in various settings. The authors create a Synthetic Customization Dataset (SynCD) using 3D datasets, which includes multiple images of the same object under different conditions. They introduce a new encoder architecture that utilizes shared attention mechanisms to capture detailed visual information effectively. Additionally, a new inference technique is proposed to address overexposure issues, resulting in improved image quality compared to existing methods.'}, 'zh': {'title': '高质量定制化：突破文本到图像生成的限制', 'desc': '本文提出了一种文本到图像模型的定制化方法，允许用户在未见过的环境中生成自定义概念。现有方法通常依赖于昂贵的测试时优化或在单图像数据集上训练编码器，导致图像质量较差。我们的方法利用现有的文本到图像模型和3D数据集，创建了一个高质量的合成定制数据集（SynCD），包含同一对象在不同光照、背景和姿势下的多张图像。通过新的编码器架构和推理技术，我们的模型在标准定制基准测试中表现优于现有的无调优方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01839', 'title': 'Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification', 'url': 'https://huggingface.co/papers/2502.01839', 'abstract': "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.", 'score': 1, 'issue_id': 2055, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd5c87eae437c7bec', 'authors': ['Eric Zhao', 'Pranjal Awasthi', 'Sreenivas Gollapudi'], 'affiliations': ['Google Research', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.01839.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Масштабирование поиска: простота ведет к силе', 'desc': 'Статья исследует масштабирование поиска на основе выборки в моделях машинного обучения. Авторы обнаружили, что простое увеличение количества случайных выборок и прямая самопроверка могут значительно улучшить производительность моделей, например Gemini v1.5 Pro. Выявлено явление неявного масштабирования, когда увеличение пула ответов повышает точность верификации. Исследование также подчеркивает важность сравнения ответов и адаптации стилей вывода модели для улучшения самопроверки.'}, 'en': {'title': 'Boosting Model Performance with Smart Sampling and Verification', 'desc': 'This paper explores how sampling-based search can enhance the performance of machine learning models during testing. By generating multiple candidate responses and verifying them, the authors show that scaling up this approach leads to better reasoning capabilities in models like Gemini v1.5 Pro. They discover that larger pools of sampled responses improve the accuracy of verification, a concept they call implicit scaling. Additionally, the paper highlights two principles for enhancing self-verification: comparing responses to identify errors and using different output styles for various tasks.'}, 'zh': {'title': '基于采样的搜索：提升模型推理能力的关键', 'desc': '本文研究了基于采样的搜索方法，这是一种在测试时利用计算资源的简单策略。我们发现，通过扩展一个仅使用随机采样和自我验证的简化实现，可以显著提高模型的推理能力。我们还提出了两个原则来改善自我验证能力：比较不同响应可以帮助识别错误位置，而不同的模型输出风格在不同上下文中有不同的效果。尽管可以实现准确的验证，但当前的前沿模型在自我验证能力上仍然表现较弱。'}}}, {'id': 'https://huggingface.co/papers/2501.19389', 'title': 'Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2501.19389', 'abstract': "Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.", 'score': 0, 'issue_id': 2058, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'c106990f1f2dc4d8', 'authors': ['Wenzhi Fang', 'Dong-Jun Han', 'Liangqi Yuan', 'Seyyedali Hosseinalipour', 'Christopher G. Brinton'], 'affiliations': ['Department of Computer Science and Engineering, Yonsei University', 'Department of Electrical Engineering, University at Buffalo-SUNY', 'Department of Electrical and Computer Engineering, Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2501.19389.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#transfer_learning'], 'emoji': '📱', 'ru': {'title': 'FSLoRA: эффективная федеративная настройка больших языковых моделей на устройствах', 'desc': 'Данная статья представляет новый метод под названием FSLoRA (Federated Sketching LoRA) для федеративной настройки больших языковых моделей на устройствах с ограниченными ресурсами. FSLoRA использует механизм скетчинга, позволяющий устройствам выборочно обновлять подматрицы глобальных LoRA-модулей на сервере. Метод адаптируется к вычислительным ограничениям конкретных устройств путем настройки коэффициентов скетчинга. Авторы проводят строгий анализ сходимости FSLoRA и демонстрируют превосходную производительность метода в экспериментах на различных наборах данных и моделях.'}, 'en': {'title': 'Adaptive Fine-Tuning for Diverse Devices with FSLoRA', 'desc': "This paper introduces federated sketching LoRA (FSLoRA), a method for fine-tuning large language models (LLMs) on devices with varying computational resources. FSLoRA uses a sketching mechanism that allows devices to update only specific parts of the global LoRA modules, making it adaptable to different device capabilities. The method addresses the challenges of data scarcity and model size by adjusting sketching ratios, which control the ranks of the updates based on device constraints. The authors provide a detailed analysis of FSLoRA's convergence properties and demonstrate its effectiveness through experiments, showing it outperforms existing methods."}, 'zh': {'title': '灵活适应设备限制的联邦草图LoRA', 'desc': '本文提出了一种新的方法，称为联邦草图LoRA（FSLoRA），旨在解决在设备上微调大型语言模型（LLMs）时的计算资源异质性问题。FSLoRA利用草图机制，使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比例，FSLoRA能够灵活适应设备特定的通信和计算限制。实验结果表明，FSLoRA在多个数据集和LLM模型上表现优于现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2502.03032', 'title': 'Analyze Feature Flow to Enhance Interpretation and Steering in Language Models', 'url': 'https://huggingface.co/papers/2502.03032', 'abstract': 'We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.', 'score': 49, 'issue_id': 2090, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '030f362f419e9eb4', 'authors': ['Daniil Laptev', 'Nikita Balagansky', 'Yaroslav Aksenov', 'Daniil Gavrilov'], 'affiliations': ['1T-Tech', 'Moscow Institute of Physics and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.03032.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': '🧠', 'ru': {'title': 'Прозрачное управление языковыми моделями через межслойный анализ признаков', 'desc': 'Представлен новый подход к систематическому отображению признаков, обнаруженных разреженным автоэнкодером, между последовательными слоями больших языковых моделей. Метод использует технику косинусного сходства без данных для отслеживания эволюции признаков на каждом этапе. Это позволяет создавать подробные графы потоков развития признаков, обеспечивая детальную интерпретируемость и механистическое понимание вычислений модели. Исследование демонстрирует, как межслойные карты признаков могут использоваться для прямого управления поведением модели путем усиления или подавления выбранных признаков.'}, 'en': {'title': 'Mapping and Manipulating Features in Language Models', 'desc': 'This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.'}, 'zh': {'title': '特征映射：引导大型语言模型的新方法', 'desc': '本文提出了一种新方法，系统地映射稀疏自编码器在大型语言模型中各层发现的特征。我们使用无数据的余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。这种方法生成了特征演变的细粒度流图，增强了模型计算的可解释性和机制洞察力。我们还展示了如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成中的主题控制。'}}}, {'id': 'https://huggingface.co/papers/2502.03544', 'title': 'Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2', 'url': 'https://huggingface.co/papers/2502.03544', 'abstract': 'We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.', 'score': 28, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '93cdc6bc9f5f22d7', 'authors': ['Yuri Chervonyi', 'Trieu H. Trinh', 'Miroslav Olšák', 'Xiaomeng Yang', 'Hoang Nguyen', 'Marcelo Menegali', 'Junehyuk Jung', 'Vikas Verma', 'Quoc V. Le', 'Thang Luong'], 'affiliations': ['Brown University', 'Georgia Institute of Technology', 'Google DeepMind', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.03544.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#training', '#architecture', '#optimization', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'ИИ превосходит человека в олимпиадной геометрии', 'desc': 'AlphaGeometry2 - это улучшенная версия системы для решения олимпиадных задач по геометрии, превзошедшая средний уровень золотого медалиста. Разработчики расширили язык AlphaGeometry для решения более сложных задач, включая движение объектов и линейные уравнения с углами, отношениями и расстояниями. Система использует архитектуру Gemini для улучшенного языкового моделирования и новый механизм обмена знаниями, комбинирующий несколько деревьев поиска. AlphaGeometry2 достигла 84% успешности решения геометрических задач за последние 25 лет Международных математических олимпиад.'}, 'en': {'title': 'AlphaGeometry2: Mastering Olympiad Geometry with Advanced AI', 'desc': "AlphaGeometry2 is an advanced version of the original AlphaGeometry, designed to solve complex Olympiad geometry problems more effectively. It enhances the language capabilities to handle intricate scenarios involving object movements and linear equations related to angles and distances. The model's performance has improved significantly, increasing its problem coverage from 66% to 88% for International Math Olympiad geometry problems. With a new search process utilizing Gemini architecture and a knowledge-sharing mechanism, AlphaGeometry2 achieves an impressive solving rate of 84%, marking a substantial leap from its predecessor."}, 'zh': {'title': 'AlphaGeometry2：几何问题解决的新突破', 'desc': 'AlphaGeometry2是对AlphaGeometry的显著改进版本，能够超越平均金牌得主在解决奥林匹克几何问题上的表现。它扩展了原有的语言，能够处理更复杂的对象运动和包含角度、比例及距离的线性方程问题。通过使用Gemini架构和新颖的知识共享机制，AlphaGeometry2的搜索过程得到了显著提升，几何问题的解决率达到了84%。该系统还朝着从自然语言输入直接解决几何问题的全自动化系统迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2502.03621', 'title': 'DynVFX: Augmenting Real Videos with Dynamic Content', 'url': 'https://huggingface.co/papers/2502.03621', 'abstract': 'We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.', 'score': 23, 'issue_id': 2089, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '8c22f8cda7e633d5', 'authors': ['Danah Yatim', 'Rafail Fridman', 'Omer Bar-Tal', 'Tali Dekel'], 'affiliations': ['Pika Labs, USA', 'Weizmann Institute of Science, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.03621.jpg', 'data': {'categories': ['#inference', '#video', '#multimodal', '#synthetic', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического контента в видео по текстовым инструкциям', 'desc': 'Авторы представляют метод для дополнения реальных видео новым динамическим контентом на основе текстовых инструкций пользователя. Метод использует предобученные модели text-to-video и Vision Language Model для синтеза и интеграции нового контента в исходное видео. Предложен новый подход на основе манипуляции признаками в механизме внимания для точной локализации и бесшовной интеграции нового контента. Метод полностью автоматизирован и требует только простой инструкции от пользователя, демонстрируя эффективность на широком спектре редактирований реальных видео.'}, 'en': {'title': 'Seamless Video Augmentation with Dynamic Content', 'desc': 'This paper introduces a novel approach for enhancing real-world videos by adding dynamic content based on user instructions. The method utilizes a pre-trained text-to-video diffusion transformer to generate new objects and effects that interact naturally with the existing scene. It employs a zero-shot, training-free framework that ensures seamless integration of the new content while considering factors like camera motion and occlusions. The approach is fully automated, allowing users to simply provide text instructions to achieve realistic video augmentation.'}, 'zh': {'title': '自动化视频增强，轻松生成动态内容！', 'desc': '我们提出了一种增强现实视频的新方法，可以生成动态内容。用户只需提供简单的文本指令，我们的方法就能合成与原始场景自然互动的动态对象或复杂场景效果。新内容的位置、外观和运动与原始视频无缝结合，同时考虑了相机运动、遮挡和其他动态对象的互动。该方法采用零-shot、无训练的框架，利用预训练的文本到视频扩散变换器和视觉语言模型，实现了自动化的内容合成。'}}}, {'id': 'https://huggingface.co/papers/2502.04320', 'title': 'ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features', 'url': 'https://huggingface.co/papers/2502.04320', 'abstract': 'Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.', 'score': 19, 'issue_id': 2101, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '043b9ff9f5eaf227', 'authors': ['Alec Helbling', 'Tuna Han Salih Meral', 'Ben Hoover', 'Pinar Yanardag', 'Duen Horng Chau'], 'affiliations': ['Georgia Tech', 'IBM Research', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.04320.jpg', 'data': {'categories': ['#interpretability', '#transfer_learning', '#dataset', '#multimodal', '#cv', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ConceptAttention: новый взгляд на интерпретируемость мультимодальных моделей', 'desc': 'Статья представляет новый метод ConceptAttention, который использует возможности слоев внимания мультимодальных диффузионных трансформеров (DiT) для создания высококачественных карт важности. Этот метод позволяет точно локализовать текстовые концепции на изображениях без дополнительного обучения. ConceptAttention превосходит существующие методы в задаче сегментации изображений с нулевым обучением на наборах данных ImageNet-Segmentation и PascalVOC. Исследование показывает, что представления моделей DiT, таких как Flux, хорошо переносятся на задачи компьютерного зрения, превосходя даже мультимодальные базовые модели вроде CLIP.'}, 'en': {'title': 'Enhancing Interpretability with ConceptAttention in Multi-Modal Transformers', 'desc': 'This paper explores the unique properties of multi-modal diffusion transformers (DiTs) and their interpretability through a new method called ConceptAttention. ConceptAttention utilizes the attention layers of DiTs to create detailed saliency maps that accurately identify textual concepts in images without needing extra training. The study reveals that linear projections in the output space of DiT attention layers lead to sharper saliency maps compared to traditional cross-attention methods. Additionally, ConceptAttention demonstrates superior performance in zero-shot image segmentation tasks, surpassing other interpretability techniques and showing the strong transferability of DiT representations to vision applications.'}, 'zh': {'title': '多模态扩散变换器的可解释性新突破', 'desc': '本文探讨了多模态扩散变换器（DiTs）在可解释性方面的独特性质。我们提出了一种新方法ConceptAttention，利用DiT注意力层的表达能力生成高质量的显著性图，准确定位图像中的文本概念。通过对DiT注意力层输出空间进行线性投影，ConceptAttention生成的显著性图比常用的交叉注意力机制更清晰。我们的研究首次证明了多模态DiT模型的表示在视觉任务（如分割）中具有高度可转移性，甚至超越了像CLIP这样的多模态基础模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04153', 'title': 'UltraIF: Advancing Instruction Following from the Wild', 'url': 'https://huggingface.co/papers/2502.04153', 'abstract': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.', 'score': 19, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c7c902f1effa8a3', 'authors': ['Kaikai An', 'Li Sheng', 'Ganqu Cui', 'Shuzheng Si', 'Ning Ding', 'Yu Cheng', 'Baobao Chang'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04153.jpg', 'data': {'categories': ['#open_source', '#training', '#benchmark', '#alignment', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'UltraIF: простой метод для обучения LLM следовать сложным инструкциям', 'desc': 'Исследователи представили подход UltraIF для улучшения способности больших языковых моделей (LLM) следовать сложным инструкциям. Метод разбивает пользовательские запросы на более простые компоненты и использует специальную модель UltraComposer для составления сложных инструкций с вопросами для оценки. Эксперименты показали, что UltraIF позволяет значительно улучшить следование инструкциям у базовой модели LLaMA-3.1-8B без использования специальных данных. Подход также продемонстрировал возможность дальнейшего улучшения версии модели, уже обученной следовать инструкциям.'}, 'en': {'title': 'UltraIF: Simplifying Complex Instructions for LLMs', 'desc': 'This paper introduces UltraIF, a method designed to enhance large language models (LLMs) in following complex instructions using open-source data. The approach involves breaking down user prompts into simpler components, such as queries and constraints, and then training a model called UltraComposer to generate these structured prompts. By synthesizing complicated instructions and evaluating responses, UltraIF successfully aligns the LLaMA-3.1-8B-Base model with its instruct version on multiple benchmarks without prior benchmark data. Additionally, the method shows potential for improving existing instruct models through self-alignment, expanding its applicability in various scenarios.'}, 'zh': {'title': 'UltraIF：让大型语言模型更聪明的秘密武器', 'desc': '本文提出了一种名为UltraIF的方法，用于提高大型语言模型（LLMs）对复杂指令的理解能力。该方法通过将用户的真实请求分解为更简单的查询、约束和相应的评估问题来实现。接着，训练一个名为UltraComposer的模型，能够生成与约束相关的提示，并结合评估问题来过滤响应。实验表明，UltraIF成功地使LLaMA-3.1-8B-Base在多个指令跟随基准上与其指令版本对齐，展示了该方法的有效性和广泛应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.04313', 'title': 'Great Models Think Alike and this Undermines AI Oversight', 'url': 'https://huggingface.co/papers/2502.04313', 'abstract': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.', 'score': 18, 'issue_id': 2091, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a44111741eb33c43', 'authors': ['Shashwat Goel', 'Joschka Struber', 'Ilze Amanda Auzina', 'Karuna K Chandra', 'Ponnurangam Kumaraguru', 'Douwe Kiela', 'Ameya Prabhu', 'Matthias Bethge', 'Jonas Geiping'], 'affiliations': ['Contextual AI', 'ELLIS Institute Tubingen', 'IIIT Hyderabad', 'Max Planck Institute for Intelligent Systems', 'Stanford University', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.04313.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Сходство языковых моделей: проблемы и риски AI-надзора', 'desc': "В статье рассматривается проблема оценки и контроля языковых моделей (ЯМ) с помощью других ЯМ, что авторы называют 'AI Oversight'. Исследуется влияние сходства моделей на эффективность такого надзора с использованием вероятностной метрики, основанной на пересечении ошибок. Обнаружено, что оценки ЯМ-судей смещены в пользу похожих моделей, а при обучении на аннотациях ЯМ важную роль играет дополняющее знание. Авторы отмечают тревожную тенденцию: с ростом возможностей ЯМ их ошибки становятся более схожими, что указывает на риски коррелированных сбоев."}, 'en': {'title': 'Navigating AI Oversight: Understanding Model Similarity and Its Risks', 'desc': 'This paper explores the challenges of evaluating and supervising advanced language models (LMs) as their capabilities grow. It introduces a probabilistic metric to measure LM similarity based on the overlap in their mistakes, which aids in understanding AI oversight. The study reveals that when using one LM to evaluate another, models that are similar tend to score each other favorably, indicating a potential bias. Additionally, it highlights the risks of correlated failures among models as they become more capable, emphasizing the need for careful reporting and correction of model similarities in AI oversight practices.'}, 'zh': {'title': 'AI监督：模型相似性的重要性', 'desc': '随着语言模型（LM）能力的提升，人类对其进行评估和监督变得越来越困难。我们提出了一种基于模型错误重叠的概率度量来研究模型相似性对AI监督的影响。研究表明，作为评判者的语言模型更倾向于偏好与其相似的模型，这与最近的自我偏好结果一致。此外，弱监督者与强学生模型之间的互补知识在“弱到强的泛化”中起着关键作用。'}}}, {'id': 'https://huggingface.co/papers/2502.04328', 'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment', 'url': 'https://huggingface.co/papers/2502.04328', 'abstract': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.', 'score': 18, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0e25e42e93e9e560', 'authors': ['Zuyan Liu', 'Yuhao Dong', 'Jiahui Wang', 'Ziwei Liu', 'Winston Hu', 'Jiwen Lu', 'Yongming Rao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.04328.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#training', '#multimodal'], 'emoji': '🦉', 'ru': {'title': 'Ola: Прогрессивное обучение для создания мощной омнимодальной модели', 'desc': 'В статье представлена Ola - омнимодальная языковая модель, способная понимать изображения, видео и аудио на уровне специализированных моделей. Ключевая особенность Ola - стратегия прогрессивного выравнивания модальностей, начинающаяся с изображений и текста, затем расширяющаяся на речь и видео. Модель использует небольшой объем кросс-модальных данных для обучения, что делает ее разработку менее затратной. Эксперименты показывают, что Ola превосходит существующие открытые омнимодальные модели по всем модальностям.'}, 'en': {'title': 'Ola: Bridging Modalities for Superior Understanding', 'desc': "This paper introduces Ola, an omni-modal language model designed to understand and process multiple types of data, including images, videos, and audio. Ola employs a progressive modality alignment strategy, starting with image and text, and gradually incorporating speech and video data to enhance its capabilities. The model's training pipeline is efficient, allowing it to maintain a smaller dataset for cross-modal alignment, making it easier and more cost-effective to develop. Experimental results show that Ola outperforms existing open omni-modal models and competes well with specialized models, aiming to contribute to future research in omni-modal understanding."}, 'zh': {'title': 'Ola：全模态理解的新突破', 'desc': '本文介绍了一种名为Ola的全模态语言模型，能够在图像、视频和音频理解方面与专门的单模态模型竞争。Ola的核心设计是逐步模态对齐策略，首先从图像和文本开始训练，然后逐步引入语音和视频数据。这样的训练流程使得跨模态对齐数据的规模相对较小，降低了开发全模态模型的成本。通过广泛的实验，Ola在所有模态上超越了现有的开放全模态语言模型，并在与同类专门模型的竞争中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.00473', 'title': 'Weak-to-Strong Diffusion with Reflection', 'url': 'https://huggingface.co/papers/2502.00473', 'abstract': 'The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.', 'score': 17, 'issue_id': 2095, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '82d22d57d66f1a7a', 'authors': ['Lichen Bai', 'Masashi Sugiyama', 'Zeke Xie'], 'affiliations': ['RIKEN AIP', 'The University of Tokyo', 'xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.00473.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#optimization', '#dataset', '#diffusion', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'W2SD: Преодоление разрыва между генеративными моделями и реальными данными', 'desc': 'Статья представляет новый фреймворк под названием Weak-to-Strong Diffusion (W2SD) для улучшения диффузионных генеративных моделей. W2SD использует разницу между слабыми и сильными моделями для аппроксимации разрыва между идеальной и сильной моделью. Метод применяет рефлексивную операцию, чередующую денойзинг и инверсию с учетом разницы weak-to-strong, что теоретически направляет латентные переменные к реальному распределению данных. Эксперименты показывают, что W2SD значительно улучшает предпочтения пользователей, эстетическое качество и соответствие промпту, достигая SOTA результатов в различных модальностях и архитектурах.'}, 'en': {'title': 'Bridging the Gap: Weak-to-Strong Diffusion for Enhanced Generative Models', 'desc': "This paper introduces Weak-to-Strong Diffusion (W2SD), a new framework designed to enhance diffusion generative models by addressing the gap between generated outputs and real data. W2SD leverages the differences between weak and strong models to better approximate the ideal model's performance. By alternating between denoising and inversion processes, it guides latent variables towards areas that closely resemble the real data distribution. The framework shows significant improvements in various applications, achieving state-of-the-art results while maintaining efficiency in computational resources."}, 'zh': {'title': '弱到强扩散：缩小生成与真实数据的差距', 'desc': '扩散生成模型的目标是通过梯度评分匹配将学习到的分布与真实数据分布对齐。然而，训练数据质量、建模策略和架构设计的固有限制导致生成输出与真实数据之间存在不可避免的差距。为了解决这个问题，我们提出了弱到强扩散（W2SD）框架，利用现有弱模型和强模型之间的差异来近似理想模型与强模型之间的差距。W2SD通过反射操作在去噪和反演之间交替，理论上引导潜在变量沿着采样轨迹朝向真实数据分布的区域，从而显著提高生成结果的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.04235', 'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion', 'url': 'https://huggingface.co/papers/2502.04235', 'abstract': "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive Genre-Audience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.", 'score': 16, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b1c2fec586443af8', 'authors': ['Xintong Hao', 'Ke Shen', 'Chenggang Li'], 'affiliations': ['Seed-LLM, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.04235.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#training', '#synthetic'], 'emoji': '🚀', 'ru': {'title': 'MAGA: преодоление ограничений данных для масштабирования языковых моделей', 'desc': 'Статья представляет метод MAGA для синтеза разнообразных и контекстуально богатых данных для предобучения языковых моделей. Авторы создали корпус MAGACorpus объемом 770 миллиардов токенов и продемонстрировали улучшение результатов для моделей различных размеров. Исследование также анализирует влияние инженерии промптов на синтетический коллапс обучения. Работа показывает, что MAGA может значительно расширить наборы данных для обучения, сохраняя их качество.'}, 'en': {'title': 'Expanding Language Models with MAGA: A Path Beyond Data Limitations', 'desc': 'This paper addresses the challenge of limited high-quality pretraining data for large language models. It introduces the MAssive Genre-Audience (MAGA) reformulation method, which creates diverse and contextually-rich pretraining data from existing sources. The authors present a new dataset called MAGACorpus, containing 770 billion tokens, and demonstrate its effectiveness across various model sizes. Additionally, they analyze the effects of prompt engineering on training stability and highlight the shortcomings of traditional metrics for detecting training collapse.'}, 'zh': {'title': 'MAGA：突破数据限制的预训练新方法', 'desc': '尽管大型语言模型在各种任务中表现出色，但它们在扩展时面临高质量预训练数据稀缺的挑战。为了解决这个瓶颈，我们提出了MAssive Genre-Audience（MAGA）重构方法，该方法系统地从现有语料库中合成多样化且富有上下文的预训练数据。我们的研究贡献包括提出了一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含7700亿个标记的MAGACorpus。通过对不同数据预算扩展策略的评估，我们证明了在各种模型规模下（134M-13B）的一致性改进，展示了下一代大规模合成预训练语言模型的必要性。'}}}, {'id': 'https://huggingface.co/papers/2502.02358', 'title': 'MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm', 'url': 'https://huggingface.co/papers/2502.02358', 'abstract': 'Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.', 'score': 14, 'issue_id': 2088, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '967ac00db9aae918', 'authors': ['Ziyan Guo', 'Zeyu Hu', 'Na Zhao', 'De Wen Soh'], 'affiliations': ['LightSpeed Studios, Singapore', 'Singapore University of Technology and Design, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.02358.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Универсальный подход к генерации и редактированию движений человека', 'desc': 'Статья представляет новый подход к генерации и редактированию движений человека в компьютерной графике и компьютерном зрении. Авторы предлагают парадигму Motion-Condition-Motion и унифицированный фреймворк MotionLab, использующий исправленные потоки для отображения исходного движения в целевое. MotionLab включает в себя MotionFlow Transformer, выровненное позиционное кодирование вращения, модуляцию инструкций для конкретных задач и учебную программу движения для эффективного обучения. Фреймворк демонстрирует многообещающие возможности обобщения и эффективность вывода на нескольких эталонных тестах для движений человека.'}, 'en': {'title': 'Unifying Human Motion Generation and Editing with MotionLab', 'desc': 'This paper presents a new approach to human motion generation and editing called MotionLab, which aims to unify various motion-related tasks into a single framework. The proposed paradigm, Motion-Condition-Motion, allows for the integration of source motion, conditions, and target motion, facilitating better control and editing capabilities. MotionLab employs advanced techniques like the MotionFlow Transformer and Aligned Rotational Position Encoding to enhance the generation process and ensure synchronization between motions. Overall, this framework shows improved efficiency and generalization across different benchmarks, making it a versatile tool for real-world applications in computer graphics and vision.'}, 'zh': {'title': '统一人类运动生成与编辑的创新框架', 'desc': '人类运动生成和编辑是计算机图形学和视觉的重要组成部分。现有的方法往往针对特定任务提供孤立的解决方案，效率低下且不适用于实际应用。为了解决这些问题，我们提出了一种新的范式：运动条件运动（Motion-Condition-Motion），它通过源运动、条件和目标运动三个概念统一处理多种任务。基于这一范式，我们开发了MotionLab框架，能够有效地进行人类运动的生成和编辑，并在多个基准测试中展示了良好的泛化能力和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03860', 'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation', 'url': 'https://huggingface.co/papers/2502.03860', 'abstract': "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.", 'score': 14, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b861ba86ae27e974', 'authors': ['Bo Pang', 'Hanze Dong', 'Jiacheng Xu', 'Silvio Savarese', 'Yingbo Zhou', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.03860.jpg', 'data': {'categories': ['#training', '#benchmark', '#math', '#long_context', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ сложным рассуждениям без подсказок', 'desc': 'Эта статья представляет новый подход к обучению больших языковых моделей (LLM) способности генерировать длинные цепочки рассуждений (LongCoT) без использования дистилляции знаний от существующих моделей. Метод BOLT включает три этапа: начальную генерацию данных LongCoT с помощью обучения в контексте, супервизорную донастройку и онлайн-обучение для дальнейшего улучшения способностей. Авторы применили свой метод к моделям различных масштабов и достигли впечатляющих результатов на ряде бенчмарков, оценивающих разнообразные способности решения задач и рассуждений. Этот подход позволяет развивать способности LLM к сложным рассуждениям без необходимости в дорогостоящих аннотациях или данных от существующих продвинутых моделей.'}, 'en': {'title': 'Empowering LLMs with Efficient Long Chain-of-Thought Bootstrapping', 'desc': "This paper presents a new method called Bootstrapping Long Chain-of-Thought (BOLT) to enhance the reasoning abilities of large language models (LLMs) without relying on knowledge distillation from existing models. BOLT consists of three stages: bootstrapping LongCoT data using in-context learning, supervised fine-tuning for LongCoT, and online training for further refinement. The approach requires only a few examples to start the bootstrapping process, making it efficient and scalable across different model sizes. Experimental results show that BOLT significantly improves performance on various reasoning and problem-solving benchmarks, demonstrating its effectiveness in developing LLMs' LongCoT capabilities."}, 'zh': {'title': '引导长链思维，提升推理能力！', 'desc': '本文介绍了一种新方法，旨在使大型语言模型（LLM）具备长链思维（LongCoT）能力，而无需依赖于类似o1模型的知识蒸馏或昂贵的人类标注。该方法称为BOLT，分为三个阶段：首先通过上下文学习从标准指令模型引导LongCoT数据，其次进行LongCoT的监督微调，最后进行在线训练以进一步提升LongCoT能力。实验中，我们仅构建了10个示例，证明了该方法的可行性。我们在多个基准测试上取得了显著的性能，展示了该方法在解决复杂问题和推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04306', 'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04306', 'abstract': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow', 'score': 13, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '51ace85d35c202d5', 'authors': ['Yinjie Wang', 'Ling Yang', 'Guohao Li', 'Mengdi Wang', 'Bryon Aragam'], 'affiliations': ['Princeton University', 'University of Chicago', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2502.04306.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#agents', '#rlhf', '#small_models'], 'emoji': '🚀', 'ru': {'title': 'ScoreFlow: эффективная оптимизация мультиагентных систем на основе ИИ', 'desc': 'В статье представлен новый фреймворк ScoreFlow для оптимизации рабочих процессов мультиагентных систем на основе больших языковых моделей. ScoreFlow использует градиентную оптимизацию в непрерывном пространстве, что позволяет преодолеть ограничения существующих методов. Ключевым компонентом является Score-DPO - новый вариант метода прямой оптимизации предпочтений, учитывающий количественную обратную связь. На шести тестовых задачах ScoreFlow показал улучшение результатов на 8.2% по сравнению с базовыми методами.'}, 'en': {'title': 'ScoreFlow: Optimizing Multi-Agent Systems with Continuous Gradient Techniques', 'desc': 'This paper introduces ScoreFlow, a new framework designed to enhance the performance of multi-agent systems in solving complex problems. It addresses the limitations of existing optimization methods by utilizing gradient-based optimization in a continuous space, which allows for greater flexibility and scalability. ScoreFlow features Score-DPO, a novel approach that incorporates quantitative feedback into the optimization process. The results show that ScoreFlow not only improves performance by 8.2% over previous methods but also enables smaller models to achieve better results than larger models at a lower cost.'}, 'zh': {'title': 'ScoreFlow：高效的多智能体优化框架', 'desc': '最近的研究利用大型语言模型的多智能体系统来解决复杂问题，同时努力减少构建这些系统所需的手动工作。现有方法由于表示限制、缺乏适应性和依赖离散优化技术而导致灵活性不足。我们提出了ScoreFlow，这是一个简单但高性能的框架，利用基于梯度的优化在连续空间中进行优化。ScoreFlow在六个基准测试中表现出色，超越了现有基线，并使较小的模型以更低的推理成本超越较大的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04128', 'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.04128', 'abstract': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.', 'score': 12, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '196e7cb6a29a44ea', 'authors': ['Zhen Ye', 'Xinfa Zhu', 'Chi-Min Chan', 'Xinsheng Wang', 'Xu Tan', 'Jiahe Lei', 'Yi Peng', 'Haohe Liu', 'Yizhu Jin', 'Zheqi DAI', 'Hongzhan Lin', 'Jianyi Chen', 'Xingjian Du', 'Liumeng Xue', 'Yunlin Chen', 'Zhifei Li', 'Lei Xie', 'Qiuqiang Kong', 'Yike Guo', 'Wei Xue'], 'affiliations': ['ASLP Lab, Northwestern Polytechnical University', 'Chinese University of Hong Kong', 'Hong Kong Baptist University', 'Independent Researcher', 'Shanghai Mobvoi Information Technology Co., Ltd.', 'The Hong Kong University of Science and Technology', 'University of Rochester', 'University of Science and Technology Beijing', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2502.04128.jpg', 'data': {'categories': ['#open_source', '#dataset', '#audio', '#training', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Llasa: масштабируемый синтез речи на основе единой языковой модели', 'desc': 'В статье представлена новая система синтеза речи Llasa, использующая одноуровневый векторный квантователь и единую архитектуру трансформера, аналогичную стандартным языковым моделям. Исследование показывает, что увеличение вычислительных ресурсов при обучении улучшает естественность синтезированной речи и позволяет генерировать более сложные просодические паттерны. Масштабирование вычислений во время вывода с использованием моделей понимания речи в качестве верификаторов улучшает эмоциональную выразительность, согласованность тембра и точность содержания. Авторы опубликовали контрольные точки и код обучения для своей модели синтеза речи.'}, 'en': {'title': 'Simplifying Speech Synthesis with Scalable LLMs', 'desc': 'This paper discusses advancements in text-to-speech (TTS) systems that utilize large language models (LLMs) like GPT and o1. It introduces Llasa, a new framework that simplifies speech synthesis by using a single-layer vector quantizer and a Transformer architecture, aligning it with standard LLMs. The research shows that increasing training compute enhances the naturalness and complexity of the generated speech. Additionally, it highlights how scaling inference compute can improve emotional expressiveness and accuracy by using speech understanding models as verifiers during the synthesis process.'}, 'zh': {'title': '简化语音合成，提升自然性与情感表达', 'desc': '本文探讨了文本基础的大型语言模型（LLMs）在语音合成中的应用，特别是GPT系列和o1模型的最新进展。我们提出了一种名为Llasa的简单框架，使用单层向量量化（VQ）编解码器和单一Transformer架构，旨在简化多阶段的语音合成过程。实验结果表明，增加训练时间的计算资源可以显著提高合成语音的自然性，并生成更复杂的韵律模式。此外，我们还发现，在推理时间增加计算资源可以改善情感表现、音色一致性和内容准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.04299', 'title': 'MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.04299', 'abstract': 'This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.', 'score': 10, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '91b39568cf3793e2', 'authors': ['Jinbo Xing', 'Long Mai', 'Cusuh Ham', 'Jiahui Huang', 'Aniruddha Mahapatra', 'Chi-Wing Fu', 'Tien-Tsin Wong', 'Feng Liu'], 'affiliations': ['Adobe Research', 'Monash University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04299.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion', '#3d', '#games'], 'emoji': '🎬', 'ru': {'title': 'MotionCanvas: Интуитивное проектирование видеокадров с контролем движения', 'desc': 'Эта статья представляет метод MotionCanvas, позволяющий пользователям проектировать кинематографические видеокадры в контексте генерации видео из изображений. Метод интегрирует пользовательское управление в модели I2V, позволяя контролировать движения объектов и камеры с учетом сцены. MotionCanvas соединяет классическую компьютерную графику и современные методы генерации видео, обеспечивая 3D-осведомленное управление движением без необходимости в дорогостоящих 3D-данных для обучения. Метод позволяет интуитивно описывать намерения движения в пространстве сцены и преобразовывать их в пространственно-временные сигналы для обусловливания движения в диффузионных моделях видео.'}, 'en': {'title': 'Empowering Cinematic Creativity with MotionCanvas', 'desc': 'This paper introduces MotionCanvas, a novel method for designing cinematic video shots in image-to-video generation. It addresses two main challenges: capturing user intentions for both camera and object motions, and effectively representing this motion information for video diffusion models. By integrating user-driven controls, MotionCanvas allows for intuitive scene-aware motion design without the need for expensive 3D training data. The method enhances creative workflows in digital content creation, making it easier for users to specify and visualize their motion intentions in video synthesis.'}, 'zh': {'title': '直观设计电影镜头，提升视频生成体验', 'desc': '本文提出了一种方法，使用户能够在图像到视频生成的背景下设计电影镜头。镜头设计是电影制作中的关键环节，涉及到对相机运动和场景中物体运动的精心规划。我们介绍了MotionCanvas，这是一种将用户驱动的控制集成到图像到视频生成模型中的方法，允许用户以场景感知的方式控制物体和相机的运动。通过将经典计算机图形学的见解与现代视频生成技术相结合，我们展示了在不需要昂贵的3D训练数据的情况下，实现I2V合成中的3D感知运动控制的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04270', 'title': 'PILAF: Optimal Human Preference Sampling for Reward Modeling', 'url': 'https://huggingface.co/papers/2502.04270', 'abstract': 'As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.', 'score': 7, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c0dd6afed760a2b', 'authors': ['Yunzhen Feng', 'Ariel Kwiatkowski', 'Kunhao Zheng', 'Julia Kempe', 'Yaqi Duan'], 'affiliations': ['Meta FAIR', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2502.04270.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'PILAF: точное согласование ИИ с человеческими ценностями', 'desc': 'Статья представляет новый метод обучения языковых моделей с учетом человеческих ценностей - PILAF (Policy-Interpolated Learning for Aligned Feedback). Этот подход улучшает существующую технику обучения с подкреплением на основе обратной связи от человека (RLHF). PILAF предлагает стратегию выборки ответов для маркировки предпочтений, которая явно согласует обучение предпочтениям с максимизацией базовой награды. Метод демонстрирует сильную производительность в итеративных и онлайн-настройках RLHF, где критически важна курация обратной связи.'}, 'en': {'title': 'Aligning AI with Human Values through PILAF', 'desc': 'This paper introduces Policy-Interpolated Learning for Aligned Feedback (PILAF), a new method for improving Reinforcement Learning from Human Feedback (RLHF). PILAF focuses on aligning preference learning with the true human values by using a response sampling strategy that enhances reward model accuracy. The approach is theoretically sound, showing optimal performance in both optimization and statistical contexts. It is easy to implement and performs well in scenarios where feedback curation is essential, making it a valuable tool for aligning large language models with human values.'}, 'zh': {'title': '政策插值学习：对齐人类反馈的新策略', 'desc': '随着大型语言模型在实际应用中的广泛使用，使其与人类价值观保持一致变得至关重要。强化学习与人类反馈（RLHF）成为了一种关键技术，它将偏好数据转化为奖励模型，尤其是在无法获取人类价值观的情况下。我们提出了一种新的响应采样策略，称为政策插值学习（PILAF），它明确将偏好学习与最大化基础奖励对齐。PILAF在理论上是有依据的，从优化和统计的角度都展示了其最优性，并且在反馈策划至关重要的迭代和在线RLHF环境中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.04295', 'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'url': 'https://huggingface.co/papers/2502.04295', 'abstract': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at https://github.com/HenryLau7/CFPO.', 'score': 7, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'f72f46ad2c1b9853', 'authors': ['Yuanye Liu', 'Jiahang Xu', 'Li Lyna Zhang', 'Qi Chen', 'Xuan Feng', 'Yang Chen', 'Zhongxin Guo', 'Yuqing Yang', 'Cheng Peng'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04295.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Интегрированная оптимизация формы и содержания промптов повышает эффективность LLM', 'desc': 'Статья представляет новую методологию оптимизации промптов для больших языковых моделей (LLM), называемую CFPO. Этот подход оптимизирует как содержание, так и форматирование промптов через итеративный процесс уточнения. CFPO использует мутации естественного языка для исследования вариаций содержания и применяет динамическую стратегию исследования форматов. Результаты показывают значительное улучшение производительности по сравнению с методами оптимизации, ориентированными только на содержание.'}, 'en': {'title': 'Enhancing LLMs with Integrated Content and Format Optimization', 'desc': 'This paper presents a new method called Content-Format Integrated Prompt Optimization (CFPO) that improves the performance of Large Language Models (LLMs) by optimizing both the content and formatting of prompts. The authors argue that while prompt content has been the focus of recent research, the formatting aspect is equally important and has not been thoroughly explored. CFPO uses natural language mutations to create variations in content and a dynamic strategy to test different formatting options. The results show that this integrated approach leads to better performance in various tasks compared to methods that only optimize content.'}, 'zh': {'title': '内容与格式的完美结合，提升LLM性能！', 'desc': '大型语言模型（LLMs）在各种任务中表现出色，其实际效果往往依赖于提示设计。尽管最近的研究主要集中在优化提示内容上，但提示格式的作用却被忽视，缺乏系统性的研究。本文提出了一种新的方法——内容格式集成提示优化（CFPO），通过迭代优化过程同时优化提示内容和格式。我们的评估表明，CFPO在多个任务和开源LLMs上相较于仅优化内容的方法，显示出显著的性能提升，强调了内容与格式集成优化的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.00989', 'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution', 'url': 'https://huggingface.co/papers/2502.00989', 'abstract': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.', 'score': 6, 'issue_id': 2092, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '4d26f9419f20aadb', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['ACM, New York, NY, USA', 'IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00989.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#agents', '#cv', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'Точные ответы по графикам с доказательствами от ИИ', 'desc': 'ChartCitor - это мультиагентная система для ответов на вопросы по графикам с использованием больших языковых моделей (LLM). Она решает проблему галлюцинаций и необоснованных ответов путем предоставления точных ссылок на области изображения графика. Система включает в себя извлечение данных из графика в таблицу, переформулировку вопроса, дополнение таблицы, поиск доказательств и сопоставление таблицы с графиком. ChartCitor превосходит существующие методы и повышает доверие пользователей к генеративному ИИ.'}, 'en': {'title': 'ChartCitor: Enhancing Trust in AI with Accurate Chart Question-Answering', 'desc': "This paper introduces ChartCitor, a multi-agent framework designed to improve the accuracy of question-answering tasks involving charts by addressing the issue of hallucinated responses from Large Language Models (LLMs). It enhances answer attribution by providing precise bounding box citations that link responses to specific parts of chart images, overcoming challenges related to visual-semantic context and complex layouts. The framework includes processes for chart-to-table extraction, answer reformulation, and evidence retrieval, which collectively improve the reliability of the generated answers. User studies indicate that ChartCitor not only boosts the performance of LLMs in chart QA tasks but also increases user trust and productivity by offering clearer explanations of the AI's reasoning."}, 'zh': {'title': 'ChartCitor：提升图表问答的可信度与效率', 'desc': '大型语言模型（LLMs）可以执行图表问答任务，但常常生成未经验证的虚假回答。现有的答案归属方法由于视觉语义上下文有限、复杂的视觉文本对齐要求以及在复杂布局中进行边界框预测的困难，难以将回答与源图表关联。我们提出了ChartCitor，一个多代理框架，通过识别图表图像中的支持证据来提供细粒度的边界框引用。ChartCitor在不同图表类型上超越了现有基准，增强了用户对生成式人工智能的信任，并提高了专业人士的工作效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03639', 'title': 'Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach', 'url': 'https://huggingface.co/papers/2502.03639', 'abstract': 'We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.', 'score': 6, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '7875c341693f4d1e', 'authors': ['Yunuo Chen', 'Junli Cao', 'Anil Kag', 'Vidit Goel', 'Sergei Korolev', 'Chenfanfu Jiang', 'Sergey Tulyakov', 'Jian Ren'], 'affiliations': ['Snap Inc.', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2502.03639.jpg', 'data': {'categories': ['#diffusion', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': '3D-осведомленная генерация видео с улучшенной геометрией и динамикой', 'desc': 'Представлена новая система генерации видео, объединяющая трехмерную геометрию и динамическое восприятие. Двумерные видео дополняются трехмерными траекториями точек и выравниваются в пиксельном пространстве. Полученный набор данных PointVid используется для дообучения модели латентной диффузии, что позволяет отслеживать 2D объекты с помощью 3D координат. Регуляризация формы и движения объектов улучшает качество генерируемых RGB видео и устраняет нежелательные артефакты.'}, 'en': {'title': 'Enhancing Video Generation with 3D Awareness', 'desc': 'This paper introduces a new framework for generating videos that combines 3D geometry with an understanding of dynamic movements. It creates a dataset called PointVid by enhancing 2D videos with 3D point trajectories, which helps the model learn to track objects in three dimensions. The framework uses a latent diffusion model that is fine-tuned to ensure that the shapes and motions of objects are realistic, reducing issues like nonphysical deformation. By focusing on 3D consistency, the model improves the quality of generated videos, especially in scenarios where objects interact closely, such as in task-oriented videos.'}, 'zh': {'title': '三维感知，提升视频生成质量', 'desc': '我们提出了一种新的视频生成框架，结合了三维几何和动态感知。通过在像素空间中对齐三维点轨迹，我们增强了二维视频，创建了一个三维感知的视频数据集PointVid。利用这个数据集，我们对潜在扩散模型进行微调，使其能够跟踪具有三维坐标的二维物体。我们的模型通过正则化物体的形状和运动，消除了不必要的伪影，提高了生成RGB视频的质量，特别是在处理复杂的接触场景时。'}}}, {'id': 'https://huggingface.co/papers/2502.00988', 'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback', 'url': 'https://huggingface.co/papers/2502.00988', 'abstract': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.', 'score': 5, 'issue_id': 2094, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9548b306478edf6d', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00988.jpg', 'data': {'categories': ['#cv', '#agents', '#science', '#dataset', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'PlotGen: ИИ-помощник для создания точных научных визуализаций', 'desc': 'Статья представляет PlotGen - новую мультиагентную систему для автоматического создания научных визуализаций данных. Система использует несколько агентов на основе больших языковых моделей (LLM) для планирования, генерации кода и итеративного улучшения графиков. PlotGen включает агентов для разбиения сложных запросов, генерации Python-кода и многомодальной обратной связи для уточнения точности данных, текстовых меток и визуальной корректности. Эксперименты показывают, что PlotGen превосходит базовые методы на 4-6% на датасете MatPlotBench, повышая доверие пользователей к визуализациям, созданным с помощью LLM.'}, 'en': {'title': 'Automating Scientific Visualization with PlotGen', 'desc': 'This paper introduces PlotGen, a multi-agent framework designed to automate the creation of accurate scientific visualizations. It utilizes several Large Language Model (LLM)-based agents to streamline the process, including a Query Planning Agent for breaking down user requests and a Code Generation Agent for converting pseudocode into Python code. Additionally, it employs retrieval feedback agents that enhance the quality of visualizations by refining data accuracy and visual elements through iterative self-reflection. Experimental results demonstrate that PlotGen significantly improves visualization accuracy and reduces debugging time, thereby increasing user trust and productivity for novice users.'}, 'zh': {'title': '自动化科学可视化的未来', 'desc': '科学数据可视化对于将原始数据转化为易于理解的视觉表示至关重要，能够帮助识别模式和预测结果。新手用户在选择合适工具和掌握可视化技术时常常面临困难。本文提出了一种名为PlotGen的新型多代理框架，旨在自动化创建精确的科学可视化。通过多个基于大语言模型的代理，PlotGen能够有效地分解用户请求并生成高质量的可视化图表。'}}}, {'id': 'https://huggingface.co/papers/2501.19085', 'title': 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet', 'url': 'https://huggingface.co/papers/2501.19085', 'abstract': "The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.", 'score': 4, 'issue_id': 2094, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8c0ab784750b1038', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['Software Institute USI Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2501.19085.jpg', 'data': {'categories': ['#training', '#low_resource', '#plp', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Повышение эффективности языковых моделей для малоресурсных языков программирования', 'desc': 'Исследование посвящено улучшению генерации кода языковыми моделями (ЯМ) для малоресурсных языков программирования. Авторы сравнивают эффективность различных подходов, включая дообучение, обучение в контексте и предобучение на задаче перевода. Результаты показывают, что для небольших ЯМ лучше всего работает дообучение, а для крупных - обучение в контексте. Очень большие ЯМ могут ухудшать производительность при дообучении из-за недостатка данных.'}, 'en': {'title': 'Boosting Code Generation for Low-Resource Languages with LLMs', 'desc': 'This paper explores how Large Language Models (LLMs) can be improved for generating code in low-resource programming languages, which have limited training data. It examines various methods such as fine-tuning, in-context learning, and a pre-training objective that translates between high- and low-resource languages. The study finds that smaller LLMs benefit most from fine-tuning, while larger models perform better with in-context learning due to their architecture. However, fine-tuning large LLMs can lead to worse performance on low-resource languages because they require more data to adjust their parameters effectively.'}, 'zh': {'title': '提升低资源语言代码生成的有效策略', 'desc': '大型语言模型（LLMs）的出现显著推动了自动代码生成领域的发展。这些模型依赖于大量多样化的数据集来学习编程语言的语法、语义和使用模式。然而，对于低资源语言（即训练数据稀缺的小众编程语言），数据的有限性限制了模型的泛化能力，导致代码生成性能较差。因此，本文研究了几种提升LLMs在低资源语言上表现的有效方法，包括经典的微调和几种上下文学习变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04296', 'title': 'Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression', 'url': 'https://huggingface.co/papers/2502.04296', 'abstract': 'We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \\ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.', 'score': 4, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'da9d11d5ea5d9d9e', 'authors': ['Lirui Wang', 'Kevin Zhao', 'Chaoqi Liu', 'Xinlei Chen'], 'affiliations': ['MIT', 'Meta, FAIR', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.04296.jpg', 'data': {'categories': ['#games', '#robotics', '#dataset', '#video', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'HMA: Быстрое и качественное моделирование видео для обучения роботов', 'desc': 'Статья представляет новый метод Heterogeneous Masked Autoregression (HMA) для моделирования динамики видео действий в робототехнике. HMA использует гетерогенное предобучение на наблюдениях и последовательностях действий из различных роботизированных воплощений, доменов и задач. Метод применяет маскированную авторегрессию для генерации квантованных или мягких токенов для предсказания видео. HMA достигает лучшего визуального качества и управляемости по сравнению с предыдущими моделями генерации видео для роботов, работая в 15 раз быстрее в реальном мире.'}, 'en': {'title': 'Revolutionizing Robot Learning with Heterogeneous Video Modeling', 'desc': 'The paper introduces Heterogeneous Masked Autoregression (HMA), a novel approach for modeling the dynamics of action videos to enhance robot learning. HMA addresses the challenges of diverse environments and the need for real-time computational efficiency by leveraging heterogeneous pre-training from various robotic tasks and domains. By employing masked autoregression, HMA generates high-quality video predictions using quantized or soft tokens. The results show that HMA significantly improves visual fidelity and controllability while operating 15 times faster than previous models, making it a valuable tool for simulating video from low-level actions and evaluating robotic policies.'}, 'zh': {'title': '异构掩蔽自回归：提升机器人学习的视频生成', 'desc': '我们提出了异构掩蔽自回归（HMA）模型，用于建模动作视频的动态，以生成高质量的数据并评估机器人学习的扩展性。构建交互式视频世界模型和机器人策略面临挑战，因为需要处理多样化的环境，同时保持实时运行的计算效率。HMA通过对不同机器人形态、领域和任务的观察和动作序列进行异构预训练，来提高模型的性能。经过后期训练，该模型可以作为视频模拟器，从低级动作输入中评估策略并生成合成数据。'}}}, {'id': 'https://huggingface.co/papers/2502.04322', 'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions', 'url': 'https://huggingface.co/papers/2502.04322', 'abstract': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.', 'score': 3, 'issue_id': 2090, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '1ad4a9febd48be28', 'authors': ['Yik Siu Chan', 'Narutatsu Ri', 'Yuxin Xiao', 'Marzyeh Ghassemi'], 'affiliations': ['Brown University', 'Columbia University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.04322.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multilingual', '#benchmark', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Простое взаимодействие - скрытая угроза безопасности языковых моделей', 'desc': 'Статья посвящена проблеме уязвимостей больших языковых моделей (LLM) к атакам, направленным на обход систем безопасности. Авторы предлагают новую метрику HarmScore для оценки эффективности вредоносных ответов LLM и разрабатывают фреймворк атаки Speak Easy, основанный на многошаговом многоязычном взаимодействии. Исследование показывает, что простые паттерны взаимодействия могут быть легко использованы злоумышленниками для вредоносных целей. Результаты демонстрируют значительное увеличение успешности атак и показателя HarmScore при применении Speak Easy к различным LLM.'}, 'en': {'title': 'Uncovering Hidden Vulnerabilities in Language Models', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.'}, 'zh': {'title': '揭示大型语言模型的安全漏洞', 'desc': '尽管已经进行了广泛的安全对齐工作，大型语言模型（LLMs）仍然容易受到越狱攻击，这会引发有害行为。本文探讨了两个关键问题：越狱响应是否真正帮助普通用户实施有害行为，以及在更常见的人类与LLM的简单互动中是否存在安全漏洞。我们提出了HarmScore，这是一种评估LLM响应如何有效促进有害行为的指标，并介绍了Speak Easy，一个简单的多步骤、多语言攻击框架。研究表明，恶意用户可以轻松利用常见的互动模式来实现有害意图。'}}}, {'id': 'https://huggingface.co/papers/2502.02737', 'title': 'SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model', 'url': 'https://huggingface.co/papers/2502.02737', 'abstract': 'While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art "small" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.', 'score': 80, 'issue_id': 2066, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'c78fe4c39300443d', 'authors': ['Loubna Ben Allal', 'Anton Lozhkov', 'Elie Bakouch', 'Gabriel Martín Blázquez', 'Guilherme Penedo', 'Lewis Tunstall', 'Andrés Marafioti', 'Hynek Kydlíček', 'Agustín Piqueres Lajarín', 'Vaibhav Srivastav', 'Joshua Lochner', 'Caleb Fahlgren', 'Xuan-Son Nguyen', 'Clémentine Fourrier', 'Ben Burtenshaw', 'Hugo Larcher', 'Haojun Zhao', 'Cyril Zakka', 'Mathieu Morlon', 'Colin Raffel', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['HuggingFaceTB'], 'pdf_title_img': 'assets/pdf/title_img/2502.02737.jpg', 'data': {'categories': ['#open_source', '#dataset', '#low_resource', '#training', '#small_models'], 'emoji': '🤏', 'ru': {'title': 'Большие возможности в маленьком пакете: SmolLM2 - компактная языковая модель с впечатляющей производительностью', 'desc': "Статья описывает разработку SmolLM2 - современной 'маленькой' языковой модели с 1,7 миллиардами параметров. Модель обучалась на ~11 триллионах токенов данных с использованием многоэтапного процесса, сочетающего веб-тексты со специализированными данными по математике, коду и выполнению инструкций. Авторы также представили новые специализированные наборы данных и провели эксперименты для оптимизации процесса обучения. В результате SmolLM2 превзошла другие современные малые языковые модели, такие как Qwen2.5-1.5B и Llama3.2-1B."}, 'en': {'title': 'SmolLM2: Efficient Language Modeling for Resource-Constrained Environments', 'desc': 'This paper presents SmolLM2, a compact language model with 1.7 billion parameters designed to be efficient for deployment in resource-limited environments. The model is trained on an extensive dataset of approximately 11 trillion tokens, utilizing a multi-stage training approach that incorporates diverse data sources, including web text and specialized datasets for math and coding. The authors introduce new datasets to enhance training quality and perform systematic evaluations to optimize dataset mixing based on performance feedback. SmolLM2 demonstrates superior performance compared to other recent small language models, and the authors provide access to the model and datasets for further research.'}, 'zh': {'title': '小型语言模型的强大突破', 'desc': '本论文介绍了SmolLM2的开发，这是一个具有17亿参数的小型语言模型。为了实现强大的性能，我们在约11万亿个数据上进行了过度训练，采用了多阶段训练过程，结合了网络文本、数学、代码和指令跟随数据。我们还引入了新的专用数据集，以解决现有数据集规模小或质量低的问题。最终，我们证明SmolLM2在性能上超越了其他近期的小型语言模型，如Qwen2.5-1.5B和Llama3.2-1B。'}}}, {'id': 'https://huggingface.co/papers/2502.01506', 'title': 'TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets', 'url': 'https://huggingface.co/papers/2502.01506', 'abstract': 'The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.', 'score': 26, 'issue_id': 2063, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'f5ec0450054af574', 'authors': ['Yuzhe Yang', 'Yifei Zhang', 'Minghao Wu', 'Kaidi Zhang', 'Yunmiao Zhang', 'Honghai Yu', 'Yan Hu', 'Benyou Wang'], 'affiliations': ['Nanjing University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.01506.jpg', 'data': {'categories': ['#multimodal', '#agents'], 'emoji': '📊', 'ru': {'title': 'LLM-агенты раскрывают тайны социально-экономической динамики', 'desc': 'Статья представляет новый фреймворк TwinMarket для моделирования социально-экономических систем с использованием больших языковых моделей (LLM). Авторы применяют LLM-агентов для более реалистичного моделирования человеческого поведения, учитывая когнитивные искажения и эмоциональные факторы. В экспериментах на симулированном фондовом рынке демонстрируется, как индивидуальные действия приводят к групповому поведению и эмергентным явлениям. Этот подход позволяет лучше понять взаимосвязь между индивидуальным принятием решений и коллективными социально-экономическими паттернами.'}, 'en': {'title': 'Harnessing LLMs for Realistic Socio-Economic Simulations', 'desc': 'This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns.'}, 'zh': {'title': '利用大型语言模型模拟社会经济系统的涌现现象', 'desc': '本研究探讨了社会涌现现象，传统的基于规则的代理模型（ABM）难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。我们提出了一种新的多代理框架TwinMarket，利用大型语言模型（LLM）来模拟社会经济系统。通过模拟股票市场环境的实验，我们展示了个体行为如何通过互动和反馈机制引发集体动态，导致金融泡沫和经济衰退等涌现现象。该方法为个体决策与集体社会经济模式之间的复杂关系提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2502.03373', 'title': 'Demystifying Long Chain-of-Thought Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2502.03373', 'abstract': 'Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.', 'score': 20, 'issue_id': 2064, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a1d00a6c8452131a', 'authors': ['Edward Yeo', 'Yuxuan Tong', 'Morry Niu', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'IN.AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03373.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая секреты длинных цепочек рассуждений в ИИ', 'desc': 'Статья исследует механизмы длинных цепочек рассуждений (CoT) в больших языковых моделях (LLM). Авторы выявляют ключевые факторы, влияющие на способность моделей генерировать длинные CoT траектории через эксперименты с обучением с подкреплением (RL) и тонкой настройкой. Исследование показывает важность масштабирования вычислительных ресурсов, формирования наград и использования веб-данных для улучшения рассуждений. Результаты предоставляют практические рекомендации по оптимизации стратегий обучения для усиления длинных CoT рассуждений в LLM.'}, 'en': {'title': 'Unlocking Reasoning Power in Large Language Models', 'desc': 'This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs.'}, 'zh': {'title': '优化训练策略，提升长推理链能力', 'desc': '本研究探讨了大型语言模型（LLMs）中长推理链（CoTs）的生成机制，揭示了影响模型生成长推理链的关键因素。我们发现，虽然监督微调（SFT）不是绝对必要的，但它可以简化训练过程并提高效率。随着训练计算能力的增加，推理能力有可能出现，但其发展并不总是保证，因此奖励设计对于稳定推理链的长度增长至关重要。最后，我们的研究为优化训练策略以增强LLMs中的长推理链提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2502.03387', 'title': 'LIMO: Less is More for Reasoning', 'url': 'https://huggingface.co/papers/2502.03387', 'abstract': 'We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models\' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model\'s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.', 'score': 18, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ad1fa98bc3904527', 'authors': ['Yixin Ye', 'Zhen Huang', 'Yang Xiao', 'Ethan Chern', 'Shijie Xia', 'Pengfei Liu'], 'affiliations': ['SJTU, SII, GAIR'], 'pdf_title_img': 'assets/pdf/title_img/2502.03387.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Меньше значит больше: революция в обучении языковых моделей сложным рассуждениям', 'desc': 'Исследователи обнаружили, что сложные математические рассуждения в больших языковых моделях можно вызвать с помощью удивительно малого количества примеров. Их модель LIMO достигла впечатляющих результатов на математических тестах, используя всего 817 обучающих образцов, что значительно меньше, чем у предыдущих подходов. LIMO также продемонстрировала исключительную способность к обобщению вне распределения, превзойдя модели, обученные на гораздо большем объеме данных. На основе этих результатов авторы предлагают гипотезу LIMO, согласно которой сложные рассуждения могут возникать через минимальные, но точно организованные демонстрации когнитивных процессов в предварительно обученных моделях.'}, 'en': {'title': 'Less Data, More Reasoning: The LIMO Hypothesis', 'desc': 'This paper reveals a surprising finding about how large language models can perform complex reasoning tasks. It shows that instead of needing a lot of training data, a model called LIMO can achieve high accuracy in mathematical reasoning with only a small number of examples. LIMO outperforms previous models that used much more data, indicating that less can be more when it comes to training for reasoning tasks. The authors introduce the Less-Is-More Reasoning Hypothesis, suggesting that a well-prepared model can effectively learn complex reasoning from minimal, well-structured examples.'}, 'zh': {'title': '少即是多，推理能力的新发现', 'desc': '本文提出了一项重要发现，挑战了我们对大型语言模型中复杂推理能力产生机制的理解。传统观点认为，复杂推理任务需要大量的训练数据，但我们证明只需少量示例即可有效引发复杂的数学推理能力。我们的模型LIMO在数学推理方面表现出前所未有的性能，使用仅817个训练样本，分别在AIME和MATH上达到了57.1%和94.8%的准确率。我们提出的“少即是多推理假设”表明，在基础模型中，经过充分编码的领域知识可以通过精心设计的少量示例来激发复杂推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.02339', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'url': 'https://huggingface.co/papers/2502.02339', 'abstract': "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.", 'score': 9, 'issue_id': 2063, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '3f3413717efb32f6', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Ruihan Jin', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Beijing', 'Department of Automation, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.02339.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#multimodal', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'AStar: Эффективное структурированное мышление для мультимодальных ИИ', 'desc': 'Статья представляет новый подход к улучшению визуального рассуждения мультимодальных больших языковых моделей (MLLM). Авторы предлагают метод AStar, использующий автоматизированное структурированное мышление на основе поиска Монте-Карло по дереву (MCTS). AStar автоматически извлекает высокоуровневые паттерны рассуждений из ограниченных данных и интегрирует внутренние способности модели с внешними указаниями. Эксперименты показывают, что AStar достигает точности 54.0% на бенчмарке MathVerse, превосходя GPT-4o при высокой эффективности использования данных и вычислений.'}, 'en': {'title': 'AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking', 'desc': "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."}, 'zh': {'title': 'AStar：高效的多模态推理新范式', 'desc': '多模态大型语言模型（MLLMs）在复杂视觉推理方面表现出色，但仍面临挑战。尽管最近的研究尝试通过引入结构化思维和教师指导来增强推理能力，但在性能和效率之间的平衡仍然困难。本文提出了一种名为AStar的自动化结构化思维范式，利用蒙特卡洛树搜索（MCTS）从有限数据中自动推导高层次的认知推理模式。AStar通过统一的推理框架，结合模型的内部推理能力和外部推理指导，实现高效推理，显著提高了准确性和数据利用效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01105', 'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2502.01105', 'abstract': "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.", 'score': 6, 'issue_id': 2067, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b4eb829c549c6a2e', 'authors': ['Yiren Song', 'Danze Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.01105.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#diffusion', '#cv', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'LayerTracer: ИИ-дизайнер векторной графики', 'desc': 'LayerTracer - это новый подход к созданию многослойных SVG изображений, основанный на диффузионном трансформере. Он имитирует процесс работы дизайнера, генерируя пошаговые растровые чертежи, а затем векторизуя их по слоям. Метод использует условный диффузионный механизм для кодирования референсных изображений в латентные токены. Эксперименты показывают превосходство LayerTracer над существующими методами в качестве генерации и редактируемости векторной графики.'}, 'en': {'title': 'LayerTracer: Bridging the Gap in Layered SVG Generation', 'desc': 'This paper introduces LayerTracer, a new framework that improves the generation of layered SVGs by learning from how designers create them. It uses a two-phase process: first, it generates rasterized blueprints that mimic human design steps, and then it converts these into clean, editable SVGs while removing duplicate paths. The framework employs a conditional diffusion mechanism to ensure that the generated images maintain their structure and quality. Experiments show that LayerTracer outperforms existing methods in both the quality of the generated designs and their ease of editing, aligning better with professional design practices.'}, 'zh': {'title': 'LayerTracer：智能生成可编辑的分层SVG图形', 'desc': '本文提出了一种名为LayerTracer的框架，旨在生成认知对齐的分层SVG图形。该方法通过学习设计师的分层SVG创建过程，利用一个新颖的顺序设计操作数据集。LayerTracer分为两个阶段：首先，基于文本的扩散变换器生成多阶段的光栅化构建蓝图；其次，通过路径去重实现分层矢量化，生成干净且可编辑的SVG文件。实验结果表明，LayerTracer在生成质量和可编辑性方面优于基于优化和神经网络的基线方法，有效地将AI生成的矢量图与专业设计认知对齐。'}}}, {'id': 'https://huggingface.co/papers/2502.02671', 'title': 'On Teacher Hacking in Language Model Distillation', 'url': 'https://huggingface.co/papers/2502.02671', 'abstract': "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.", 'score': 5, 'issue_id': 2072, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'defca87e9bf06d0b', 'authors': ['Daniil Tiapkin', 'Daniele Calandriello', 'Johan Ferret', 'Sarah Perrin', 'Nino Vieillard', 'Alexandre Ramé', 'Mathieu Blondel'], 'affiliations': ['Ecole 1CMAP, France; Polytechnique', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.02671.jpg', 'data': {'categories': ['#alignment', '#optimization', '#rlhf', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': "Борьба с 'teacher hacking': ключ к robust языковым моделям", 'desc': "Статья исследует феномен 'teacher hacking' при дистилляции знаний в языковых моделях. Авторы предлагают экспериментальную установку с оракулом, учителем и учеником для изучения этого явления. Результаты показывают, что 'teacher hacking' возникает при использовании фиксированного офлайн-датасета, но может быть смягчен с помощью онлайн-генерации данных. Исследование подчеркивает важность разнообразия данных для предотвращения 'hacking' и построения надежных языковых моделей."}, 'en': {'title': 'Preventing Teacher Hacking: The Key Role of Data Diversity in Distillation', 'desc': "This paper explores the concept of 'teacher hacking' in the context of knowledge distillation for language models (LMs). Teacher hacking occurs when a student LM overly optimizes based on an imperfect teacher LM, leading to poor performance on the actual task. The authors conducted experiments using an oracle LM as the ground truth, a teacher LM distilled from it, and a student LM distilled from the teacher. They found that using diverse online data can prevent teacher hacking, highlighting the importance of data diversity in the distillation process."}, 'zh': {'title': '防止教师黑客，提升语言模型的蒸馏效果', 'desc': '本文探讨了语言模型（LM）在知识蒸馏阶段可能出现的“教师黑客”现象。教师黑客是指学生模型在模仿教师模型时，过度优化导致性能下降的情况。这种现象与古德哈特法则相符，可能源于教师模型对真实分布的不完美近似。我们的实验表明，使用固定的离线数据集进行蒸馏时，教师黑客现象会发生，而采用在线数据生成技术则能有效缓解这一问题，数据多样性是防止教师黑客的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2502.01618', 'title': 'A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods', 'url': 'https://huggingface.co/papers/2502.01618', 'abstract': 'Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at https://probabilistic-inference-scaling.github.io.', 'score': 5, 'issue_id': 2065, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'c9971916eb027101', 'authors': ['Isha Puri', 'Shivchander Sudalairaj', 'Guangxuan Xu', 'Kai Xu', 'Akash Srivastava'], 'affiliations': ['MIT CSAIL', 'Red Hat AI Innovation'], 'pdf_title_img': 'assets/pdf/title_img/2502.01618.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#inference'], 'emoji': '🎲', 'ru': {'title': 'Вероятностный подход к масштабированию вывода LLM', 'desc': 'Статья представляет новый подход к масштабированию вычислений во время вывода для больших языковых моделей (LLM). Вместо оптимизации с помощью моделей вознаграждения, авторы рассматривают задачу как вероятностный вывод, используя методы Монте-Карло на основе частиц. Эмпирическая оценка показывает, что предложенный метод имеет в 4-16 раз лучшую скорость масштабирования по сравнению с детерминированными аналогами на сложных задачах математических рассуждений. Исследование демонстрирует, как небольшие модели могут достичь точности крупных моделей при меньшем количестве прогонов.'}, 'en': {'title': 'Revolutionizing Inference: Probabilistic Scaling for LLMs', 'desc': 'This paper addresses the limitations of scaling large language models (LLMs) by focusing on improving inference time rather than just increasing model size or data. The authors propose a new approach that treats inference-time scaling as a probabilistic inference task, using sampling techniques to better explore the state distribution. By applying particle-based Monte Carlo methods, their method shows significant improvements in scaling rates compared to traditional deterministic search methods. The results indicate that their approach can achieve higher accuracy with fewer rollouts, demonstrating a promising direction for enhancing LLM performance.'}, 'zh': {'title': '推理时间扩展的新方法：概率推理与粒子采样结合', 'desc': '大型语言模型（LLMs）通过增加模型规模和数据量取得了显著的性能提升。然而，最近的研究表明，这种方法的收益递减，促使我们考虑在推理时增加计算量。现有的推理时间扩展方法通常将任务视为搜索问题，容易受到奖励模型的近似误差影响而导致奖励操控。本文提出了一种新的推理时间扩展方法，通过适应基于粒子的蒙特卡洛方法，将推理时间扩展视为概率推理任务，从而在各种数学推理任务中实现了更好的扩展率。'}}}, {'id': 'https://huggingface.co/papers/2502.01154', 'title': 'Jailbreaking with Universal Multi-Prompts', 'url': 'https://huggingface.co/papers/2502.01154', 'abstract': 'Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.', 'score': 3, 'issue_id': 2068, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'aa9860c81d83ac21', 'authors': ['Yu-Ling Hsu', 'Hsuan Su', 'Shang-Tse Chen'], 'affiliations': ['National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01154.jpg', 'data': {'categories': ['#security', '#rl', '#data', '#optimization', '#transfer_learning', '#training', '#ethics'], 'emoji': '🔓', 'ru': {'title': 'Универсальный взлом языковых моделей: новый метод JUMP', 'desc': 'Статья описывает новый метод под названием JUMP для универсального взлома (jailbreak) больших языковых моделей с помощью мульти-промптов. Авторы также представляют адаптированную версию этого метода для защиты, называемую DUMP. Экспериментальные результаты показывают, что предложенный подход превосходит существующие техники по оптимизации универсальных мульти-промптов. Исследование затрагивает важную тему этических проблем и новых типов атак на языковые модели.'}, 'en': {'title': 'JUMP: Universal Multi-Prompts for Jailbreaking LLMs', 'desc': 'This paper presents JUMP, a novel method for jailbreaking large language models (LLMs) using universal multi-prompts. Unlike traditional prompting techniques that focus on specific adversarial inputs, JUMP aims to create a universal attacker that can adapt to various unseen tasks, reducing computational costs. Additionally, the authors propose a defense mechanism called DUMP, which leverages the same principles to protect against such attacks. Experimental results indicate that JUMP significantly outperforms existing methods in optimizing these universal multi-prompts.'}, 'zh': {'title': '通用多提示：破解与防御的创新方法', 'desc': '大型语言模型（LLMs）近年来迅速发展，改变了许多应用，显著提高了便利性和生产力。然而，随着其强大能力的提升，伦理问题和新型攻击（如越狱攻击）也随之出现。大多数提示技术专注于优化单个案例的对抗输入，这在处理大数据集时会导致更高的计算成本。本文介绍了一种名为JUMP的方法，旨在使用通用多提示对LLMs进行越狱，同时我们还提出了防御方法DUMP，实验结果表明我们的方法在优化通用多提示方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2502.03275', 'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning', 'url': 'https://huggingface.co/papers/2502.03275', 'abstract': 'Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.', 'score': 3, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'f94d674e0f57dcf9', 'authors': ['DiJia Su', 'Hanlin Zhu', 'Yingchen Xu', 'Jiantao Jiao', 'Yuandong Tian', 'Qinqing Zheng'], 'affiliations': ['Meta AI', 'UC Berkeley', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2502.03275.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#optimization', '#training', '#benchmark', '#math'], 'emoji': '🧠', 'ru': {'title': 'Гибридное представление рассуждений: эффективность через абстракцию', 'desc': 'Данная статья предлагает гибридный подход к представлению процесса рассуждений в больших языковых моделях (LLM). Авторы используют латентные дискретные токены, генерируемые VQ-VAE, для частичной абстракции начальных шагов рассуждения, что значительно сокращает длину входных данных. Метод применяется как при обучении модели с нуля, так и при дообучении существующих LLM на гибридных данных с расширенным словарем. Предложенный подход превосходит базовые методы в различных тестах на логические и математические рассуждения.'}, 'en': {'title': 'Streamlining Reasoning with Hybrid Token Representations', 'desc': 'This paper presents a new method to improve reasoning in Large Language Models (LLMs) by using a hybrid representation of reasoning processes. Instead of relying solely on lengthy text inputs, the authors introduce latent discrete tokens generated by VQ-VAE to simplify the reasoning steps. This approach reduces the input length and computational resources needed while maintaining effective reasoning capabilities. The proposed method shows superior performance in training and fine-tuning scenarios for logical and mathematical reasoning tasks compared to traditional methods.'}, 'zh': {'title': '优化推理过程，提升模型效率', 'desc': '本文探讨了大型语言模型（LLMs）在推理和规划中的应用，特别是在链式思维（CoT）数据训练时的表现。我们提出了一种混合表示法，通过使用VQ-VAE生成的潜在离散标记，部分抽象化初始推理步骤，从而显著减少推理过程的长度。我们在两个场景中探索了潜在追踪抽象的使用：一是从头开始训练模型解决钥匙寻找迷宫问题，二是对LLMs进行微调以处理逻辑和数学推理问题。我们的训练方法通过随机混合潜在标记和文本标记，促进了对新潜在标记的快速适应，且在多个基准测试中表现优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2502.02928', 'title': 'Large Language Model Guided Self-Debugging Code Generation', 'url': 'https://huggingface.co/papers/2502.02928', 'abstract': 'Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.', 'score': 2, 'issue_id': 2075, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'dc7aaadeeee7e1e7', 'authors': ['Muntasir Adnan', 'Zhiwei Xu', 'Carlos C. N. Kuhn'], 'affiliations': ['Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2502.02928.jpg', 'data': {'categories': ['#agents', '#plp', '#training'], 'emoji': '🐍', 'ru': {'title': 'PyCapsule: Эффективная генерация Python-кода с самоотладкой', 'desc': 'PyCapsule - это новая система для автоматической генерации кода на Python, использующая двухагентный конвейер и модули самоотладки. Система включает в себя сложный вывод промптов, итеративную обработку ошибок и тестирование примеров, что обеспечивает высокую стабильность, безопасность и корректность генерации. PyCapsule демонстрирует значительное улучшение успешности на различных бенчмарках по сравнению с современными методами. Однако наблюдается снижение нормализованной успешности при увеличении попыток самоотладки, возможно, из-за ограниченной и зашумленной обратной связи об ошибках.'}, 'en': {'title': 'Revolutionizing Python Code Generation with PyCapsule', 'desc': 'This paper introduces PyCapsule, a new framework designed to enhance automated code generation, particularly for Python. It employs a two-agent pipeline that focuses on efficient self-debugging and robust error handling, addressing common issues in existing methods. The framework utilizes advanced prompt inference and iterative testing to improve the stability and correctness of generated code. Empirical results show that PyCapsule outperforms current state-of-the-art techniques in various benchmarks, highlighting its potential for more efficient AI-driven programming solutions.'}, 'zh': {'title': 'PyCapsule：高效的自动化代码生成框架', 'desc': '自动化代码生成在智能计算机编程和系统部署中变得越来越重要。然而，现有的方法在计算效率上常常面临挑战，并且缺乏强大的代码解析和错误修正机制。本文提出了一种新颖的框架PyCapsule，采用简单而有效的双代理管道和高效的自我调试模块来生成Python代码。PyCapsule通过复杂的提示推理、迭代错误处理和案例测试，确保了高生成稳定性、安全性和正确性。'}}}, {'id': 'https://huggingface.co/papers/2502.02421', 'title': 'Activation-Informed Merging of Large Language Models', 'url': 'https://huggingface.co/papers/2502.02421', 'abstract': 'Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.', 'score': 1, 'issue_id': 2079, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '90e80efaaef789ec', 'authors': ['Amin Heyrani Nobari', 'Kaveh Alimohammadi', 'Ali ArjomandBigdeli', 'Akash Srivastava', 'Faez Ahmed', 'Navid Azizan'], 'affiliations': ['Massachusetts Institute of Technology', 'RedHat AI Innovation & MIT-IBM Watson AI Lab', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02421.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'AIM: Умное слияние языковых моделей для повышения эффективности', 'desc': 'Статья представляет новый метод объединения языковых моделей под названием Activation-Informed Merging (AIM). AIM использует информацию из пространства активаций моделей для улучшения производительности и устойчивости объединенной модели. Метод применим к любому существующему способу слияния моделей и использует принципы непрерывного обучения и сжатия моделей. Эмпирические результаты показывают значительное улучшение производительности объединенных моделей на различных бенчмарках, с увеличением до 40%.'}, 'en': {'title': 'Boosting Model Performance with Activation-Informed Merging', 'desc': 'This paper presents Activation-Informed Merging (AIM), a novel technique for merging large language models (LLMs) that leverages activation space information to improve model performance. AIM enhances the merging process by selectively prioritizing essential weights from the base models, which helps maintain robustness and efficiency. The method is flexible and can be integrated with existing merging techniques, making it widely applicable. Empirical results show that AIM can lead to significant performance improvements, achieving up to a 40% increase in benchmark scores for merged models.'}, 'zh': {'title': '激活信息合并：提升模型合并性能的新方法', 'desc': '模型合并是一种将多个微调的大型语言模型（LLMs）的参数和嵌入结合起来的方法，能够在保持计算效率的同时提升模型在各种任务上的表现。本文提出了一种名为激活信息合并（AIM）的技术，它将LLMs的激活空间信息整合到合并过程中，以提高性能和鲁棒性。AIM旨在作为一种灵活的补充解决方案，适用于任何现有的合并方法，并通过持续学习和模型压缩的原则来保留基础模型中的关键权重。通过使用与任务无关的校准集，AIM在合并过程中优先考虑重要权重，实验证明AIM在多个基准测试中显著提升了合并模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.00306', 'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2502.00306', 'abstract': "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.", 'score': 1, 'issue_id': 2076, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '4987f380f5ddb7af', 'authors': ['Ali Naseh', 'Yuefeng Peng', 'Anshuman Suri', 'Harsh Chaudhari', 'Alina Oprea', 'Amir Houmansadr'], 'affiliations': ['Northeastern University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.00306.jpg', 'data': {'categories': ['#inference', '#rag', '#leakage', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Незаметная атака на RAG-системы: как выявить документы в базе знаний', 'desc': "Статья представляет новый метод атаки на системы генерации текста с извлечением информации (RAG). Авторы предлагают технику под названием 'Interrogation Attack', которая позволяет определить, содержится ли конкретный документ в базе знаний RAG-системы. Метод основан на создании естественных запросов, на которые можно ответить только при наличии целевого документа. Эксперименты показывают, что атака эффективнее существующих методов и трудно обнаружима стандартными детекторами."}, 'en': {'title': 'Stealthy Inference: Unveiling Membership in RAG Systems', 'desc': 'This paper introduces a new method called Interrogation Attack (IA) for membership inference in Retrieval-Augmented Generation (RAG) systems. RAG allows Large Language Models (LLMs) to generate responses using external knowledge without changing their internal parameters, but this can be exploited by adversaries. The IA technique uses natural-text queries that can only be answered if a specific document is present, making it harder to detect than previous methods. The authors demonstrate that their approach is more effective and stealthy, achieving better performance with fewer queries and lower costs compared to existing techniques.'}, 'zh': {'title': '隐蔽的会员推断攻击：RAG系统的新挑战', 'desc': '本论文介绍了一种新的会员推断技术，称为审问攻击（Interrogation Attack, IA），旨在针对RAG数据存储中的文档进行攻击。该方法通过构造自然语言查询，仅在目标文档存在时才能得到答案，从而实现有效的推断。与现有方法相比，我们的攻击在仅使用30个查询的情况下，成功率提高了2倍，同时保持隐蔽性。我们的研究表明，IA在多种RAG配置下的表现优于以往的推断攻击，且每个文档的推断成本低于0.02美元。'}}}, {'id': 'https://huggingface.co/papers/2502.00226', 'title': 'HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems', 'url': 'https://huggingface.co/papers/2502.00226', 'abstract': 'Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.', 'score': 0, 'issue_id': 2079, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'c9615b5d00a42037', 'authors': ['Jun Xing', 'Mayur Bhatia', 'Sahil Phulwani', 'Darshan Suresh', 'Rafik Matta'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00226.jpg', 'data': {'categories': ['#benchmark', '#science', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый бенчмарк раскрывает потенциал LLM в реальной разработке ПО', 'desc': 'Статья представляет новый бенчмарк для оценки применимости больших языковых моделей (LLM) в реальных задачах разработки программного обеспечения. HackerRank-ASTRA Benchmark включает проектные задачи кодирования, имитирующие реальные сценарии, и оценивает согласованность модели через 32 запуска. Исследование показало, что три ведущие модели достигли сравнимых средних оценок в 75%. Модель Claude-3.5-Sonnet-1022 продемонстрировала наивысшую согласованность и низкую вариативность результатов.'}, 'en': {'title': 'Benchmarking LLMs for Real-World Coding Consistency', 'desc': 'This paper evaluates the effectiveness of large language models (LLMs) in real-world software development tasks using the HackerRank-ASTRA Benchmark. Unlike previous benchmarks that focus on isolated coding problems, this benchmark introduces project-based scenarios that require multi-file handling and assesses model consistency through extensive testing. The study analyzes the performance of top models, revealing that while they achieved similar average scores, one model, Claude-3.5-Sonnet-1022, stood out for its high consistency and low variability in results. This research emphasizes the importance of rigorous evaluation methods to ensure LLMs are reliable for practical applications in coding.'}, 'zh': {'title': '评估大型语言模型在软件开发中的真实应用性', 'desc': '这篇论文评估了大型语言模型（LLMs）在实际软件开发任务中的适用性。现有的基准测试通常只关注单一的编码问题或特定库，忽视了多文件、基于项目的场景，并缺乏对一致性的严格评估。HackerRank-ASTRA基准引入了模拟真实场景的项目基础编码问题，并通过32次运行评估模型的一致性。初步评估显示，Claude-3.5-Sonnet-1022在问题一致性方面表现最佳，具有较低的变异性，突显了其在实际软件开发任务中的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2502.05173', 'title': 'VideoRoPE: What Makes for Good Video Rotary Position Embedding?', 'url': 'https://huggingface.co/papers/2502.05173', 'abstract': 'While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.', 'score': 48, 'issue_id': 2118, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ba284ed1a62b3c2c', 'authors': ['Xilin Wei', 'Xiaoran Liu', 'Yuhang Zang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Jian Tong', 'Haodong Duan', 'Qipeng Guo', 'Jiaqi Wang', 'Xipeng Qiu', 'Dahua Lin'], 'affiliations': ['Fudan University, Shanghai, China', 'Shanghai AI Laboratory, Shanghai, China', 'Shanghai Innovation Institute, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05173.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#architecture', '#video', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'VideoRoPE: Эффективное позиционное кодирование для глубокого обучения на видео', 'desc': 'Статья представляет VideoRoPE - новый метод позиционного кодирования для видео, основанный на Rotary Position Embedding. Авторы провели анализ и выявили 4 ключевые характеристики для эффективной адаптации RoPE к видео. Они предложили сложную задачу V-NIAH-D для демонстрации недостатков существующих вариантов RoPE. VideoRoPE имеет 3D-структуру, сохраняющую пространственно-временные отношения, и превосходит предыдущие варианты RoPE в различных задачах обработки видео.'}, 'en': {'title': 'Enhancing Video Understanding with VideoRoPE', 'desc': 'This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data.'}, 'zh': {'title': 'VideoRoPE：视频中的旋转位置嵌入新突破', 'desc': '本文探讨了如何将旋转位置嵌入（RoPE）有效地扩展到视频数据中。研究分析了四个关键特性，这些特性对于RoPE在视频中的适应性至关重要。我们提出了一个新的任务V-NIAH-D，展示了现有RoPE变体在处理视频时容易受到干扰的缺陷。基于这些分析，我们提出了VideoRoPE，它通过3D结构来保持时空关系，并在多个下游任务中表现优于之前的RoPE变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04507', 'title': 'Fast Video Generation with Sliding Tile Attention', 'url': 'https://huggingface.co/papers/2502.04507', 'abstract': 'Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.', 'score': 38, 'issue_id': 2120, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'dcbf1070dac1b391', 'authors': ['Peiyuan Zhang', 'Yongqi Chen', 'Runlong Su', 'Hangliang Ding', 'Ion Stoica', 'Zhenghong Liu', 'Hao Zhang'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2502.04507.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью скользящего плиточного внимания', 'desc': 'Статья представляет метод скользящего плиточного внимания (STA) для ускорения генерации видео с помощью диффузионных трансформеров. STA использует наблюдение, что оценки внимания в предобученных моделях диффузии видео в основном концентрируются в локализованных 3D-окнах. Этот подход устраняет избыточность полного внимания, сохраняя выразительность и эффективность на аппаратном уровне. STA ускоряет внимание в 2.8-17 раз по сравнению с FlashAttention-2 и в 1.6-10 раз по сравнению с FlashAttention-3, значительно сокращая время генерации видео.'}, 'en': {'title': 'Efficient Video Generation with Sliding Tile Attention', 'desc': 'This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks.'}, 'zh': {'title': '滑动瓦片注意力：高效视频生成的新突破', 'desc': '本论文介绍了一种新的滑动瓦片注意力机制（STA），旨在提高视频生成的效率。传统的扩散变换器在生成视频时计算成本高，而STA通过关注局部的时空区域来减少冗余计算。与传统的滑动窗口注意力不同，STA采用了硬件友好的设计，逐块处理，保持了表达能力的同时提高了计算效率。经过优化，STA在视频生成任务中显著加速了注意力计算，降低了延迟，同时不影响生成质量。'}}}, {'id': 'https://huggingface.co/papers/2502.04896', 'title': 'Goku: Flow Based Video Generative Foundation Models', 'url': 'https://huggingface.co/papers/2502.04896', 'abstract': 'This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.', 'score': 30, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ad6ef6eed233cc90', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Yuqi Zhang', 'Yida Zhang', 'Fengda Zhu', 'Hao Yang', 'Hongxiang Hao', 'Hui Wu', 'Zhichao Lai', 'Yifei Hu', 'Ting-Che Lin', 'Shilong Zhang', 'Fu Li', 'Chuan Li', 'Xing Wang', 'Yanghua Peng', 'Peize Sun', 'Ping Luo', 'Yi Jiang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['Bytedance Inc', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04896.jpg', 'data': {'categories': ['#cv', '#training', '#video', '#architecture', '#data', '#benchmark', '#dataset'], 'emoji': '🐉', 'ru': {'title': 'Goku: Новый уровень генерации изображений и видео', 'desc': 'Статья представляет семейство моделей Goku для совместной генерации изображений и видео. Модели используют трансформеры с выпрямленным потоком для достижения передовых результатов. Авторы описывают ключевые элементы, включая подготовку данных, архитектуру модели и инфраструктуру для эффективного обучения. Goku демонстрирует превосходную производительность в качественных и количественных оценках, устанавливая новые стандарты в основных задачах генерации изображений и видео.'}, 'en': {'title': 'Goku: Revolutionizing Image and Video Generation with Transformers', 'desc': 'This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models.'}, 'zh': {'title': 'Goku：图像与视频生成的新标杆', 'desc': '本文介绍了Goku，这是一种先进的联合图像和视频生成模型，利用了修正流Transformer以实现行业领先的性能。我们详细阐述了高质量视觉生成的基础要素，包括数据整理流程、模型架构设计、流的公式化以及高效稳健的大规模训练基础设施。Goku模型在定性和定量评估中表现优越，为主要任务设定了新的基准。具体而言，Goku在文本到图像生成中达到了0.76的GenEval和83.65的DPG-Bench，在文本到视频任务中达到了84.85的VBench。'}}}, {'id': 'https://huggingface.co/papers/2502.05003', 'title': 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations', 'url': 'https://huggingface.co/papers/2502.05003', 'abstract': 'One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.', 'score': 28, 'issue_id': 2122, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c011c3548ad7a5dd', 'authors': ['Andrei Panferov', 'Jiale Chen', 'Soroush Tabesh', 'Roberto L. Castro', 'Mahdi Nikdan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05003.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное квантование для обучения языковых моделей', 'desc': 'Статья представляет новый метод QuEST для обучения больших языковых моделей с использованием квантования. QuEST позволяет обучать модели с весами и активациями в 4 бита или меньше, сохраняя конкурентоспособную точность по сравнению с FP16. Метод улучшает квантование распределений весов и активаций, а также вводит новый оценщик градиента доверия. Эксперименты показывают, что QuEST обеспечивает стабильные законы масштабирования для различных уровней точности.'}, 'en': {'title': 'QuEST: Revolutionizing Quantization for Efficient Language Models', 'desc': 'This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support.'}, 'zh': {'title': 'QuEST：低精度高效训练大语言模型的创新方法', 'desc': '本文提出了一种名为QuEST的新方法，旨在通过量化感知训练（QAT）来提高大语言模型的训练效率。QuEST能够在4位或更低的精度下训练模型，同时保持与FP16精度相当的准确性。该方法通过改进权重和激活的量化过程，以及引入新的信任梯度估计器，来实现更稳定的训练。实验结果表明，QuEST在各种硬件支持的精度范围内都能实现稳定的扩展性，并且可以有效执行。'}}}, {'id': 'https://huggingface.co/papers/2502.05171', 'title': 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach', 'url': 'https://huggingface.co/papers/2502.05171', 'abstract': 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.', 'score': 24, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '4386159312d9856b', 'authors': ['Jonas Geiping', 'Sean McLeish', 'Neel Jain', 'John Kirchenbauer', 'Siddharth Singh', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Tom Goldstein'], 'affiliations': ['ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center', 'Lawrence Livermore National Laboratory', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.05171.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Глубокие рассуждения в латентном пространстве: новый подход к языковым моделям', 'desc': 'В статье представлена новая архитектура языковой модели, способная масштабировать вычисления во время тестирования путем неявных рассуждений в латентном пространстве. Модель работает путем итерации рекуррентного блока, разворачиваясь до произвольной глубины во время тестирования. В отличие от подходов, основанных на цепочке рассуждений, этот метод не требует специализированных обучающих данных и может работать с небольшими контекстными окнами. Авторы масштабировали экспериментальную модель до 3,5 миллиардов параметров и 800 миллиардов токенов, показав значительное улучшение производительности на тестах рассуждений.'}, 'en': {'title': 'Scaling Reasoning with Latent Space Computation', 'desc': 'This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts.'}, 'zh': {'title': '隐式推理，提升语言模型的计算能力', 'desc': '我们研究了一种新颖的语言模型架构，该架构能够通过在潜在空间中隐式推理来扩展测试时的计算能力。我们的模型通过迭代递归块工作，从而在测试时可以展开到任意深度。这与主流推理模型不同，后者通过生成更多的标记来增加计算量。我们的模型不需要特殊的训练数据，能够处理小的上下文窗口，并且能够捕捉不易用语言表示的推理类型。'}}}, {'id': 'https://huggingface.co/papers/2502.05176', 'title': 'AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting', 'url': 'https://huggingface.co/papers/2502.05176', 'abstract': 'Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.', 'score': 22, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '9b52f2788f53c3c0', 'authors': ['Chung-Ho Wu', 'Yang-Jung Chen', 'Ying-Huan Chen', 'Jie-Ying Lee', 'Bo-Hsu Ke', 'Chun-Wei Tuan Mu', 'Yi-Chuan Huang', 'Chin-Yang Lin', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05176.jpg', 'data': {'categories': ['#dataset', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Революция в 3D-реконструкции: AuraFusion360 для безупречного восстановления сцен', 'desc': 'AuraFusion360 - это новый метод восстановления трехмерных сцен на основе Gaussian Splatting. Он использует генерацию масок невидимых областей с учетом глубины, адаптивную диффузию глубины и улучшение деталей на основе SDEdit для создания высококачественных результатов. Метод превосходит существующие подходы по качеству восприятия и геометрической точности при изменении точки обзора. Авторы также представили первый набор данных 360-USID для оценки методов восстановления сцен с охватом 360 градусов.'}, 'en': {'title': 'Revolutionizing 3D Scene Inpainting with AuraFusion360', 'desc': 'AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy.'}, 'zh': {'title': 'AuraFusion360：三维场景修复的新突破', 'desc': '三维场景修复在虚拟现实和建筑可视化等应用中非常重要，但现有方法在360度无界场景中面临视图一致性和几何精度的挑战。我们提出了AuraFusion360，这是一种新颖的基于参考的方法，能够在高质量的3D场景中进行物体移除和孔填充。该方法引入了深度感知的未见掩码生成、适应性引导深度扩散和基于SDEdit的细节增强，确保多视图的一致性。通过大量实验，AuraFusion360在感知质量和几何精度方面显著优于现有方法，能够在剧烈视角变化中保持高质量的修复效果。'}}}, {'id': 'https://huggingface.co/papers/2502.05163', 'title': 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails', 'url': 'https://huggingface.co/papers/2502.05163', 'abstract': 'The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.', 'score': 17, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ae863d89ab71ab51', 'authors': ['Yihe Deng', 'Yu Yang', 'Junkai Zhang', 'Wei Wang', 'Bo Li'], 'affiliations': ['University of California, Los Angeles', 'University of Illinois at Urbana-Champaign', 'VirtueAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05163.jpg', 'data': {'categories': ['#low_resource', '#inference', '#synthetic', '#dataset', '#open_source', '#multilingual', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Улучшение многоязычной безопасности LLM через совместное обучение генератора и ограничителя', 'desc': 'Статья представляет новый подход к созданию многоязычных моделей-ограничителей для обеспечения безопасности больших языковых моделей (LLM). Авторы предлагают framework с двумя игроками на основе обучения с подкреплением, где генератор и модель-ограничитель развиваются совместно для создания синтетических данных. Теоретически это взаимодействие формализовано как игра двух игроков с доказанной сходимостью к равновесию Нэша. Эмпирические оценки показывают, что предложенная модель превосходит современные аналоги, особенно для языков с меньшими ресурсами.'}, 'en': {'title': 'Enhancing Multilingual Safety in LLMs with Synthetic Data Generation', 'desc': "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."}, 'zh': {'title': '多语言护栏模型的创新进展', 'desc': '随着大型语言模型（LLMs）的快速发展，确保其负责任使用的护栏模型需求增加，尤其是在检测不安全和非法内容方面。虽然英语的安全数据相对丰富，但由于其他语言开放源代码安全数据的稀缺，多语言护栏建模仍然未被充分探索。为了解决这一问题，我们提出了一种新颖的双玩家强化学习框架，其中生成器和护栏模型对抗性地共同进化，以生成高质量的合成数据用于多语言护栏训练。我们的模型在多语言安全任务中取得了显著进展，特别是在处理低资源语言的不平衡问题上。'}}}, {'id': 'https://huggingface.co/papers/2502.04403', 'title': 'Agency Is Frame-Dependent', 'url': 'https://huggingface.co/papers/2502.04403', 'abstract': "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.", 'score': 13, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '32ceb8df4d77794a', 'authors': ['David Abel', 'André Barreto', 'Michael Bowling', 'Will Dabney', 'Shi Dong', 'Steven Hansen', 'Anna Harutyunyan', 'Khimya Khetarpal', 'Clare Lyle', 'Razvan Pascanu', 'Georgios Piliouras', 'Doina Precup', 'Jonathan Richens', 'Mark Rowland', 'Tom Schaul', 'Satinder Singh'], 'affiliations': ['Amii, University of Alberta', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.04403.jpg', 'data': {'categories': ['#rl', '#agi', '#reasoning', '#math'], 'emoji': '🤖', 'ru': {'title': 'Агентность: все зависит от точки зрения', 'desc': 'Статья рассматривает концепцию агентности в контексте обучения с подкреплением. Авторы утверждают, что агентность фундаментально зависит от системы отсчета. Они поддерживают этот тезис, анализируя ключевые свойства агентности, предложенные в предыдущих исследованиях. Статья подчеркивает необходимость учета зависимости от системы отсчета в изучении агентности и обсуждает последствия для обучения с подкреплением.'}, 'en': {'title': 'Agency in Reinforcement Learning: A Frame-Dependent Perspective', 'desc': 'This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning.'}, 'zh': {'title': '能动性：依赖于框架的系统能力', 'desc': '本文探讨了系统的能动性，特别是在强化学习的背景下。能动性是指系统朝着目标引导结果的能力，但判断一个系统是否具备能动性是一个复杂的问题。我们认为，能动性是依赖于参考框架的，任何对系统能动性的测量都必须相对于某个参考框架进行。通过哲学论证，我们支持这一观点，并讨论了这一结论对强化学习的影响。'}}}, {'id': 'https://huggingface.co/papers/2502.04728', 'title': 'Generating Symbolic World Models via Test-time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2502.04728', 'abstract': 'Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.', 'score': 12, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'cd97668cd0eee0ee', 'authors': ['Zhouliang Yu', 'Yuhuan Yuan', 'Tim Z. Xiao', 'Fuxiang Frank Xia', 'Jie Fu', 'Ge Zhang', 'Ge Lin', 'Weiyang Liu'], 'affiliations': ['Environmental Systems Research Institute, Inc.', 'Max Planck Institute for Intelligent Systems, Tübingen', 'SEED, Bytedance', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04728.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#data', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности планирования с помощью PDDL и LLM', 'desc': 'Статья представляет новый подход к решению сложных задач планирования с использованием больших языковых моделей (LLM). Авторы предлагают использовать язык определения планирования (PDDL) для создания точной символической модели мира. Метод включает алгоритм Best-of-N для улучшения качества начального решения и последующее уточнение с помощью вербализованного машинного обучения. Результаты показывают значительное превосходство над существующими методами в генерации доменов PDDL и решении задач планирования высокого уровня.'}, 'en': {'title': 'Enhancing LLMs for Optimal Planning with PDDL', 'desc': "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."}, 'zh': {'title': '利用PDDL提升规划问题解决能力', 'desc': '本论文探讨了如何利用大型语言模型（LLMs）解决复杂的规划问题。为了避免规则违反和确保最优性，研究者们引入了规划领域定义语言（PDDL），作为一种精确的状态描述工具。通过PDDL，可以生成符号世界模型，并应用经典搜索算法（如A*）来寻找最优计划。本文提出了一种简单有效的算法，通过Best-of-N采样和细致的机器学习优化，显著提高了PDDL领域的生成质量，成功率超过50%。'}}}, {'id': 'https://huggingface.co/papers/2502.05179', 'title': 'FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation', 'url': 'https://huggingface.co/papers/2502.05179', 'abstract': 'DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .', 'score': 9, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c3147244e03af4a6', 'authors': ['Shilong Zhang', 'Wenbo Li', 'Shoufa Chen', 'Chongjian Ge', 'Peize Sun', 'Yida Zhang', 'Yi Jiang', 'Zehuan Yuan', 'Binyue Peng', 'Ping Luo'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05179.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео высокого разрешения с предпросмотром', 'desc': 'Статья представляет новый двухэтапный подход к генерации видео на основе текста под названием FlashVideo. На первом этапе модель фокусируется на соответствии промпту, генерируя видео низкого разрешения. Второй этап использует сопоставление потоков для эффективного создания деталей высокого разрешения. Этот метод позволяет достичь высокого качества генерации видео при меньших вычислительных затратах по сравнению с существующими подходами. Кроме того, пользователи могут предварительно просмотреть результат перед полной генерацией, что повышает коммерческую привлекательность технологии.'}, 'en': {'title': 'FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework', 'desc': 'The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use.'}, 'zh': {'title': 'FlashVideo：高效生成高分辨率视频的创新框架', 'desc': 'DiT扩散模型在文本到视频生成方面取得了显著成功，但高内容和运动保真度通常需要大量模型参数和函数评估。为了解决这些计算需求，我们提出了一种新的两阶段框架FlashVideo，旨在平衡生成的保真度和质量。在第一阶段，通过低分辨率生成过程优先考虑提示保真度，利用大参数和足够的函数评估提高计算效率。第二阶段则在低分辨率和高分辨率之间建立流匹配，有效生成细节，且所需的函数评估最小化。'}}}, {'id': 'https://huggingface.co/papers/2502.04959', 'title': 'No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces', 'url': 'https://huggingface.co/papers/2502.04959', 'abstract': 'Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .', 'score': 8, 'issue_id': 2127, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a73679e79ff14b7c', 'authors': ['Daniel Marczak', 'Simone Magistri', 'Sebastian Cygert', 'Bartłomiej Twardowski', 'Andrew D. Bagdanov', 'Joost van de Weijer'], 'affiliations': ['Computer Vision Center, Barcelona, Spain', 'Department of Computer Science, Universitat Autonoma de Barcelona, Spain', 'Department of Information Engineering, University of Florence, Italy', 'Gdansk University of Technology, Poland', 'IDEAS NCBR, Warsaw, Poland', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2502.04959.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Эффективное объединение моделей машинного обучения без дополнительного обучения', 'desc': 'Статья посвящена объединению весов нескольких моделей, обученных на конкретных задачах, в одну многозадачную модель. Авторы исследуют ключевые характеристики матриц задач, которые позволяют эффективно объединять модели. Они предлагают изотропный фреймворк для объединения, который выравнивает спектр сингулярных значений матриц задач и улучшает их выравнивание. Предложенный подход достигает наилучших результатов в различных сценариях, включая разные наборы задач и масштабы моделей.'}, 'en': {'title': 'Bridging the Performance Gap in Model Merging', 'desc': 'This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios.'}, 'zh': {'title': '有效的模型合并方法提升多任务性能', 'desc': '模型合并是将多个特定任务模型的权重整合为一个多任务模型的过程。尽管这一问题受到越来越多的关注，但合并模型与单任务模型之间仍存在显著的性能差距。本文研究了任务矩阵的关键特性，这些矩阵是应用于预训练模型的权重更新矩阵，发现任务特定矩阵与合并矩阵之间的对齐程度与性能提升密切相关。我们提出了一种各向同性合并框架，通过平坦化任务矩阵的奇异值谱，增强对齐性，从而缩小性能差距，并在多个场景中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04520', 'title': "Linear Correlation in LM's Compositional Generalization and Hallucination", 'url': 'https://huggingface.co/papers/2502.04520', 'abstract': 'The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., "X lives in the city of" rightarrow "X lives in the country of" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM\'s generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.', 'score': 8, 'issue_id': 2119, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '41ef9027d1533f06', 'authors': ['Letian Peng', 'Chenyang An', 'Shibo Hao', 'Chengyu Dong', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.04520.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#agi', '#data', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Линейность как ключ к обобщению языковых моделей', 'desc': 'Статья исследует феномен линейных корреляций в языковых моделях при композиции знаний. Авторы обнаружили, что существует линейное преобразование между связанными знаниями, которое отображает логиты предсказания следующего токена от одного промпта к другому. Это явление устойчиво к масштабному дообучению и может служить потенциальным идентификатором обобщения языковой модели. Исследование показывает, что такие линейные корреляции могут быть изучены с помощью одной полносвязной нейронной сети и предобученных представлений словаря.'}, 'en': {'title': 'Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition', 'desc': 'This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge.'}, 'zh': {'title': '语言模型的线性相关性与知识组合', 'desc': '这篇论文探讨了语言模型（LM）在知识组合中的线性相关现象。研究发现，某些相关知识之间存在线性变换，这种变换可以将一个提示的下一个标记预测从一个映射到另一个。比如，从"X 住在城市"可以转变为"X 住在国家"。结果表明，线性变换在大规模微调中具有韧性，并且当与现实世界关系一致时能够推广更新的知识，但当偏离时则会导致幻觉。'}}}, {'id': 'https://huggingface.co/papers/2502.04416', 'title': 'CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference', 'url': 'https://huggingface.co/papers/2502.04416', 'abstract': 'Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.', 'score': 7, 'issue_id': 2125, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0b4520b0860c835c', 'authors': ['Zehua Pei', 'Lancheng Zou', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04416.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное превращение плотных LLM в разреженные MoE модели', 'desc': 'Статья представляет новый метод CMoE (Carved MoE) для эффективного создания моделей Mixture-of-Experts из плотных моделей больших языковых моделей (LLM). CMoE группирует нейроны в общие и маршрутизируемые экспертные группы на основе уровней активации. Метод включает механизм маршрутизации с дифференцируемым процессом и балансировкой нагрузки. CMoE позволяет быстро создать эффективную MoE модель из плотной модели размером 7 миллиардов параметров, используя небольшой объем данных.'}, 'en': {'title': 'Efficiently Carving Mixture-of-Experts for Large Language Models', 'desc': 'This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning.'}, 'zh': {'title': '高效雕刻混合专家模型的创新框架', 'desc': '大型语言模型（LLMs）通过增加模型参数实现了出色的性能，但这也带来了显著的推理开销。前馈网络（FFNs）在LLM参数中占主导地位，隐藏神经元的激活稀疏性很高。为此，研究人员提出了混合专家（MoE）架构，仅激活一部分参数。然而，现有方法通常需要大量的训练数据和资源，限制了其实用性。我们提出了CMoE（Carved MoE），一个新颖的框架，可以高效地从稠密模型中雕刻出MoE模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04404', 'title': 'Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models', 'url': 'https://huggingface.co/papers/2502.04404', 'abstract': "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.", 'score': 7, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a7bd201755c7ea1d', 'authors': ['Xiao-Wen Yang', 'Xuan-Yi Zhu', 'Wen-Da Wei', 'Ding-Chu Zhang', 'Jie-Jing Shao', 'Zhi Zhou', 'Lan-Zhe Guo', 'Yu-Feng Li'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University, China', 'School of Artificial Intelligence, Nanjing University, China', 'School of Intelligence Science and Technology, Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04404.jpg', 'data': {'categories': ['#agi', '#training', '#inference', '#agents', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самостоятельный возврат: путь к более разумным ИИ', 'desc': 'Статья представляет новый механизм самостоятельного возврата (self-backtracking) для больших языковых моделей (LLM). Этот механизм позволяет LLM автономно определять, когда и где нужно вернуться назад в процессе рассуждений. Авторы утверждают, что это улучшает способности LLM к рассуждению и повышает эффективность, превращая медленное мышление в быстрое через самосовершенствование. Эмпирические оценки показывают значительное улучшение возможностей рассуждения LLM с использованием этого подхода.'}, 'en': {'title': 'Empowering LLMs with Self-Backtracking for Enhanced Reasoning', 'desc': 'This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods.'}, 'zh': {'title': '自我回溯机制：提升语言模型推理能力的关键', 'desc': '将慢思考机制整合到大型语言模型（LLMs）中，为实现二级AGI推理器提供了有希望的途径。当前的挑战包括低效的过度思考和对辅助奖励模型的过度依赖。我们指出，这些限制源于LLMs无法内化搜索过程，而搜索过程是有效推理的关键组成部分。我们提出了一种自我回溯机制，使LLMs能够在训练和推理过程中自主决定何时以及如何回溯，从而显著提升推理能力和效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03738', 'title': 'Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More', 'url': 'https://huggingface.co/papers/2502.03738', 'abstract': 'Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.', 'score': 6, 'issue_id': 2122, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'aa76478090a36c04', 'authors': ['Feng Wang', 'Yaodong Yu', 'Guoyizhe Wei', 'Wei Shao', 'Yuyin Zhou', 'Alan Yuille', 'Cihang Xie'], 'affiliations': ['Johns Hopkins University', 'UC Berkeley', 'UC Santa Cruz', 'University of Florida'], 'pdf_title_img': 'assets/pdf/title_img/2502.03738.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Пиксельная токенизация превосходит патчи в vision-моделях', 'desc': 'Исследование посвящено анализу влияния размера патчей в Vision Transformer (ViT) на качество распознавания изображений. Авторы обнаружили, что уменьшение размера патчей до 1x1 пикселя (пиксельная токенизация) улучшает предсказательную способность модели. Этот эффект наблюдается для различных задач компьютерного зрения, масштабов входных данных и архитектур, включая ViT и Mamba. Эксперименты показали, что модель базового размера с длиной визуальной последовательности 50 176 токенов достигает точности 84,6% на наборе данных ImageNet-1k.'}, 'en': {'title': 'Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers', 'desc': 'This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models.'}, 'zh': {'title': '小块更优，视觉理解更强！', 'desc': '本文研究了视觉变换器（ViT）中图像分块（patchification）对信息损失的影响。通过缩小图像的空间大小，分块方法可以有效减少令牌序列的长度，从而降低计算成本。我们发现，随着分块大小的减小，模型的预测性能持续提高，直到达到最小的1x1像素分块。该研究结果适用于多种视觉任务和不同的模型架构，为未来构建非压缩视觉模型提供了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2502.05178', 'title': 'QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05178', 'abstract': 'We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.', 'score': 6, 'issue_id': 2121, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'bbc1f1a0b7f5423f', 'authors': ['Yue Zhao', 'Fuzhao Xue', 'Scott Reed', 'Linxi Fan', 'Yuke Zhu', 'Jan Kautz', 'Zhiding Yu', 'Philipp Krähenbühl', 'De-An Huang'], 'affiliations': ['NVIDIA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.05178.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'QLIP: Единый подход к пониманию и генерации изображений', 'desc': 'QLIP - это метод визуальной токенизации, объединяющий высококачественную реконструкцию изображений с пониманием изображений без предварительного обучения. Он использует автоэнкодер с бинарно-сферическим квантованием и оптимизирует одновременно реконструкцию и выравнивание языка и изображений. Авторы предлагают двухэтапный процесс обучения для эффективного сочетания требований к большим батчам и ограничений по памяти. QLIP может заменить визуальный энкодер в мультимодальных моделях и токенизатор изображений в генеративных моделях, показывая сопоставимую или лучшую производительность.'}, 'en': {'title': 'QLIP: Bridging Visual and Language Understanding with Efficient Tokenization', 'desc': 'The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation.'}, 'zh': {'title': '量化语言-图像预训练：多模态理解的新突破', 'desc': '我们介绍了一种量化语言-图像预训练方法（QLIP），这是一种视觉标记化方法，结合了最先进的重建质量和零-shot图像理解能力。QLIP使用基于二元球面量化的自编码器进行训练，同时优化重建和语言-图像对齐目标。我们首次证明这两个目标可以动态平衡，而不是相互对立。QLIP在多模态理解和文本条件图像生成方面表现出色，可以作为LLaVA的视觉编码器和LlamaGen的图像标记器的替代方案，性能相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.04363', 'title': 'On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices', 'url': 'https://huggingface.co/papers/2502.04363', 'abstract': 'We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.', 'score': 6, 'issue_id': 2119, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '339b45dee733174c', 'authors': ['Bosung Kim', 'Kyuhwan Lee', 'Isu Jeong', 'Jungmin Cheon', 'Yeojin Lee', 'Seulki Lee'], 'affiliations': ['Ulsan National Institute of Science and Technology South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.04363.jpg', 'data': {'categories': ['#inference', '#video', '#open_source', '#diffusion', '#architecture', '#low_resource'], 'emoji': '📱', 'ru': {'title': 'Генерация видео по тексту прямо на вашем смартфоне', 'desc': 'On-device Sora представляет собой инновационное решение для генерации видео на основе текста с использованием диффузионных моделей, работающее на смартфонах. Система применяет три новые техники: Linear Proportional Leap (LPL) для сокращения шагов денойзинга, Temporal Dimension Token Merging (TDTM) для оптимизации вычислений в слоях внимания, и Concurrent Inference with Dynamic Loading (CI-DL) для эффективного использования ограниченной памяти устройства. Эксперименты на iPhone 15 Pro показали, что On-device Sora способна генерировать видео высокого качества, сравнимые с результатами Open-Sora на мощных GPU.'}, 'en': {'title': 'Empowering Video Creation on Your Smartphone!', 'desc': 'On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services.'}, 'zh': {'title': '移动设备上的高效视频生成新突破', 'desc': '我们提出了On-device Sora，这是首个基于扩散模型的移动设备文本到视频生成解决方案，能够高效地在智能手机上运行。该系统采用了三种新技术来解决移动设备在计算和内存方面的限制。首先，线性比例跳跃（LPL）通过高效的跳跃方法减少了视频扩散中所需的去噪步骤。其次，时间维度令牌合并（TDTM）通过沿时间维度合并连续令牌，降低了注意力层中密集的令牌处理计算。'}}}, {'id': 'https://huggingface.co/papers/2502.05092', 'title': 'Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2502.05092', 'abstract': "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.", 'score': 5, 'issue_id': 2128, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ea9f14d34d4cbb60', 'authors': ['Rohit Saxena', 'Aryo Pradipta Gema', 'Pasquale Minervini'], 'affiliations': ['ILCC, School of Informatics, University of Edinburgh', 'Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05092.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#cv', '#interpretability'], 'emoji': '⏰', 'ru': {'title': 'Время бросает вызов искусственному интеллекту', 'desc': 'Статья исследует способности мультимодальных больших языковых моделей (MLLM) в интерпретации времени и дат через аналоговые часы и календари. Авторы создали структурированный набор данных, включающий ClockQA с различными стилями часов и CalendarQA с календарными изображениями и вопросами. Цель исследования - проанализировать, как MLLM выполняют визуальное распознавание, числовые рассуждения и временные выводы. Результаты показывают, что надежное понимание времени остается значительной проблемой для MLLM, несмотря на недавние достижения.'}, 'en': {'title': 'Unlocking Time: Challenges for Multimodal Language Models', 'desc': "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."}, 'zh': {'title': '理解时间的挑战：多模态语言模型的局限性', 'desc': '本研究探讨了多模态大型语言模型（MLLMs）在理解时间和日期方面的能力，特别是通过模拟时钟和年度日历的视觉表示。我们创建了一个结构化的数据集，包括两部分：ClockQA，包含不同风格的时钟及相关时间问题；CalendarQA，包含年度日历图像及常见日期问题。我们的目标是分析MLLMs在处理与时间相关的视觉数据时的视觉识别、数值推理和时间推断能力。尽管最近取得了一些进展，但MLLMs在可靠理解时间方面仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2502.03512', 'title': 'YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment', 'url': 'https://huggingface.co/papers/2502.03512', 'abstract': 'Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.', 'score': 4, 'issue_id': 2122, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ebf6482c46f8fe2f', 'authors': ['Amitava Das', 'Yaswanth Narsupalli', 'Gurpreet Singh', 'Vinija Jain', 'Vasu Sharma', 'Suranjana Trivedy', 'Aman Chadha', 'Amit Sheth'], 'affiliations': ['Amazon AI, USA', 'Artificial Intelligence Institute, University of South Carolina, USA', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.03512.jpg', 'data': {'categories': ['#benchmark', '#rag', '#rlhf', '#ethics', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Баланс противоречий в генерации изображений: YinYangAlign на страже точности', 'desc': 'Статья представляет YinYangAlign - передовую систему оценки для измерения точности выравнивания текстово-изобразительных (T2I) моделей. Она фокусируется на шести фундаментальных и противоречивых целях дизайна, отражающих ключевые напряжения в генерации изображений. YinYangAlign включает наборы данных с человеческими запросами, выровненными и невыровненными ответами ИИ, а также объяснениями противоречий. Система направлена на улучшение надежности и точности генерации изображений, применяя методы выравнивания, успешно используемые в больших языковых моделях.'}, 'en': {'title': 'Enhancing Image Generation Fidelity with YinYangAlign', 'desc': 'This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation.'}, 'zh': {'title': '提升文本到图像系统的对齐精度', 'desc': '本文探讨了文本到图像（T2I）系统中精确对齐的重要性，以确保生成的图像既能准确反映用户意图，又符合伦理和美学标准。研究表明，像Google Gemini这样的事件凸显了强大对齐机制的必要性。与此相比，大型语言模型（LLMs）在对齐方面取得了显著成功，研究人员希望将类似的对齐技术应用于T2I系统，以提高图像生成的准确性和可靠性。我们提出了YinYangAlign，这是一个先进的基准框架，系统地量化T2I系统的对齐保真度，解决了六个基本且内在矛盾的设计目标。'}}}, {'id': 'https://huggingface.co/papers/2502.04350', 'title': 'CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance', 'url': 'https://huggingface.co/papers/2502.04350', 'abstract': 'Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.', 'score': 4, 'issue_id': 2118, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'ad7829c09c28de41', 'authors': ['Yongchao Chen', 'Yilun Hao', 'Yueying Liu', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard University, Boston, MA, USA', 'MIT-IBM Watson AI Lab, Boston, MA, USA', 'Massachusetts Institute of Technology, Boston, MA, USA', 'University of Illinois Urbana-Champaign, Urbana, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04350.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#rlhf', '#open_source', '#optimization', '#reasoning'], 'emoji': '🧭', 'ru': {'title': 'CodeSteer: Умное управление для раскрытия потенциала LLM в символьных вычислениях', 'desc': 'CodeSteer - это новый метод для эффективного управления генерацией кода и текста в больших языковых моделях (LLM). Исследователи создали комплексный бенчмарк SymBench и синтезировали наборы данных для обучения модели. Они дообучили модель Llama-3-8B с использованием многораундового обучения с учителем и прямой оптимизации предпочтений. Результирующая модель CodeSteerLLM значительно улучшает производительность других LLM в задачах символьных вычислений, превосходя даже лучшие существующие модели.'}, 'en': {'title': 'CodeSteer: Guiding LLMs to Master Code and Reasoning!', 'desc': 'This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks.'}, 'zh': {'title': 'CodeSteer：引导LLM实现符号计算的突破', 'desc': '现有的方法无法有效引导大型语言模型（LLMs）在文本推理和代码生成之间切换，导致符号计算能力未得到充分利用。我们提出了一种名为CodeSteer的方法，能够有效指导LLM的代码和文本生成。我们构建了一个全面的基准SymBench，包含37个具有可调复杂度的符号任务，并合成了包含1.2万多轮指导/生成轨迹和5500对指导比较的数据集。通过对Llama-3-8B模型进行多轮监督微调（SFT）和直接偏好优化（DPO），我们得到的CodeSteerLLM模型能够有效引导更大模型的代码/文本生成。'}}}, {'id': 'https://huggingface.co/papers/2502.04327', 'title': 'Value-Based Deep RL Scales Predictably', 'url': 'https://huggingface.co/papers/2502.04327', 'abstract': 'Scaling data and compute is critical to the success of machine learning. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.', 'score': 3, 'issue_id': 2133, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '4b04abb62254054b', 'authors': ['Oleh Rybkin', 'Michal Nauman', 'Preston Fu', 'Charlie Snell', 'Pieter Abbeel', 'Sergey Levine', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'University of California, Berkeley', 'University of Warsaw'], 'pdf_title_img': 'assets/pdf/title_img/2502.04327.jpg', 'data': {'categories': ['#rl', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Предсказуемое масштабирование в обучении с подкреплением', 'desc': 'Статья посвящена предсказуемости масштабирования методов обучения с подкреплением вне политики, основанных на значениях. Авторы демонстрируют, что требования к данным и вычислительным ресурсам для достижения определенного уровня производительности лежат на границе Парето, контролируемой соотношением обновлений к данным (UTD). Исследователи предлагают метод оценки этой границы для прогнозирования требований к ресурсам при масштабировании. Подход валидируется на трех алгоритмах (SAC, BRO, PQL) в различных средах обучения с подкреплением.'}, 'en': {'title': 'Predictable Scaling in Reinforcement Learning', 'desc': 'This paper discusses how to effectively scale data and compute resources in machine learning, particularly in reinforcement learning (RL). It demonstrates that value-based off-policy RL methods can be predictable, contrary to common beliefs about their erratic behavior. The authors introduce a Pareto frontier that helps estimate the data and compute requirements for achieving desired performance levels. They also provide a method for optimizing resource allocation and hyperparameters to enhance performance while managing overfitting and plasticity loss in RL.'}, 'zh': {'title': '可预测的强化学习扩展方法', 'desc': '在机器学习中，扩展数据和计算能力是成功的关键。然而，扩展需要可预测性：我们希望方法不仅在更多计算或数据下表现良好，而且其性能可以从小规模实验中预测。本文展示了基于价值的离线策略强化学习方法在可预测性方面的表现，尽管社区普遍认为其行为不稳定。我们通过估计帕累托前沿，预测在给定计算或数据时所需的资源，并确定在特定预算下的最佳资源分配，以最大化性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04376', 'title': 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf', 'url': 'https://huggingface.co/papers/2502.04376', 'abstract': 'In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.', 'score': 3, 'issue_id': 2121, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '049ab6200a9d2eae', 'authors': ['Lingxiang Hu', 'Shurun Yuan', 'Xiaoting Qin', 'Jue Zhang', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'Northeastern University, China', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04376.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#agi', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LLM как виртуальные делегаты: будущее эффективных совещаний', 'desc': 'Исследователи разработали прототип системы делегирования участия в совещаниях на основе больших языковых моделей (LLM). Они создали комплексный бенчмарк, используя реальные стенограммы совещаний, для оценки эффективности различных LLM в роли делегатов. Результаты показали, что около 60% ответов моделей затрагивают как минимум один ключевой момент из эталонных данных, но требуется улучшение в снижении нерелевантного контента и повышении устойчивости к ошибкам транскрипции. Исследование подчеркивает потенциал и проблемы использования LLM в качестве делегатов на совещаниях, предлагая ценные выводы для их практического применения.'}, 'en': {'title': 'Empowering Meetings with LLMs: Balancing Engagement and Efficiency', 'desc': 'This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications.'}, 'zh': {'title': '利用大型语言模型优化会议参与', 'desc': '在现代工作场所，会议是交流思想和确保团队一致性的重要环节，但常常面临时间消耗、日程冲突和参与效率低下等挑战。本文探讨了大型语言模型（LLMs）在会议中有效分配参与者的能力，开发了一个基于LLM的会议代理系统原型，并使用真实会议记录创建了全面的基准测试。评估结果显示，GPT-4/4o在积极和谨慎的参与策略之间保持了平衡，而Gemini 1.5 Pro则更倾向于谨慎，Gemini 1.5 Flash和Llama3-8B/70B则表现出更积极的倾向。尽管约60%的回应涵盖了至少一个关键点，但仍需改进以减少无关或重复内容，并提高对真实场景中转录错误的容忍度。'}}}, {'id': 'https://huggingface.co/papers/2502.04689', 'title': 'ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning', 'url': 'https://huggingface.co/papers/2502.04689', 'abstract': 'Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.', 'score': 2, 'issue_id': 2123, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a052e3be1fe147cd', 'authors': ['Yuwei Yin', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04689.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ARR: Новый шаг в улучшении рассуждений языковых моделей', 'desc': 'Статья представляет новый метод промптинга для больших языковых моделей под названием ARR. Этот метод включает три ключевых шага: анализ намерения вопроса, извлечение релевантной информации и пошаговое рассуждение. ARR показывает лучшие результаты по сравнению с базовым подходом и методом Chain-of-Thought на различных задачах вопросно-ответного типа. Эксперименты подтверждают эффективность и обобщаемость ARR для разных размеров и типов языковых моделей.'}, 'en': {'title': 'ARR: A Structured Approach to Boost LLM Reasoning', 'desc': "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."}, 'zh': {'title': 'ARR：提升问答推理的新方法', 'desc': '本文介绍了一种新的零-shot提示方法ARR，旨在提高大型语言模型（LLMs）在多项选择问答任务中的推理能力。ARR明确包含三个关键步骤：分析问题意图、检索相关信息和逐步推理。通过在多种复杂问答任务上的实验，ARR consistently outperform了传统的基线方法和Chain-of-Thought（CoT）提示。研究表明，意图分析在ARR中起着至关重要的作用，进一步验证了每个组成部分的积极贡献。'}}}, {'id': 'https://huggingface.co/papers/2501.12387', 'title': 'Continuous 3D Perception Model with Persistent State', 'url': 'https://huggingface.co/papers/2501.12387', 'abstract': 'We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: https://cut3r.github.io/', 'score': 1, 'issue_id': 2134, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '2269a12b01fff7a4', 'authors': ['Qianqian Wang', 'Yifei Zhang', 'Aleksander Holynski', 'Alexei A. Efros', 'Angjoo Kanazawa'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2501.12387.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🏗️', 'ru': {'title': 'Универсальная 3D-реконструкция в реальном времени', 'desc': 'Авторы представляют универсальную модель CUT3R для решения широкого спектра задач 3D-реконструкции. Эта рекуррентная модель непрерывно обновляет свое внутреннее представление с каждым новым наблюдением, генерируя метрические поточечные карты для каждого входного изображения. CUT3R способна накапливать согласованную плотную реконструкцию сцены и даже предсказывать ненаблюдаемые области. Модель демонстрирует конкурентоспособную или лучшую производительность в различных задачах 3D/4D реконструкции.'}, 'en': {'title': 'CUT3R: Real-Time 3D Reconstruction with Continuous Updates', 'desc': 'This paper introduces CUT3R, a novel framework for 3D reconstruction that utilizes a stateful recurrent model to continuously update its state with new image observations. The model generates metric-scale pointmaps in real-time, allowing for the accumulation of a dense scene reconstruction as more images are processed. CUT3R is capable of inferring unseen areas of a scene by simulating views that have not been directly observed. The approach is versatile, handling both video streams and unordered photo collections, and shows strong performance across various 3D tasks.'}, 'zh': {'title': 'CUT3R：持续更新的三维重建模型', 'desc': '我们提出了一个统一的框架，能够解决广泛的三维任务。该方法采用状态递归模型，能够随着每个新观察不断更新其状态表示。通过图像流，这种不断演变的状态可以在线生成度量尺度的点图，为每个新输入生成每像素的三维点。我们的模型CUT3R不仅可以从图像观察中预测准确的点图，还可以通过探测虚拟的未观察视角推断场景中未见的区域。'}}}, {'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 79, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Прямое выравнивание: простой путь к улучшению языковых моделей', 'desc': 'В статье рассматриваются алгоритмы прямого выравнивания (DAA) как альтернатива обучению с подкреплением в контексте выравнивания языковых моделей. Авторы классифицируют DAA по типу функции потерь, используемым наградам и необходимости предварительного обучения. Исследование показывает, что двухэтапные методы превосходят одноэтапные, а ключевым фактором эффективности является использование попарных, а не поточечных целевых функций. Результаты подчеркивают важность тщательной оценки при сравнении алгоритмов выравнивания языковых моделей.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': '优化对齐算法，提升模型性能！', 'desc': '直接对齐算法（DAAs）通过直接优化策略来简化语言模型的对齐，取代了人类反馈强化学习中的强化学习和奖励建模。DAAs可以根据其排名损失（成对与点对）和使用的奖励类型进行分类。研究表明，一阶段方法的表现不如两阶段方法，因此我们引入了显式的监督微调阶段，并在单阶段的ORPO和ASFT中加入了控制偏好优化强度的beta参数。这些改进使得它们在Alpaca Eval 2中的表现得到了显著提升，表明选择成对或点对目标是关键因素，而不是具体的隐式奖励或损失函数。'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 74, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'OmniHuman: универсальная модель для генерации реалистичных видео с людьми', 'desc': 'OmniHuman - это новая модель на основе Diffusion Transformer для генерации реалистичных видео с людьми. Она использует смешанные условия движения при обучении, что позволяет масштабировать данные и улучшить качество генерации. Модель поддерживает различные типы портретов, взаимодействие с объектами и сложные позы тела. OmniHuman может генерировать видео на основе аудио, видео или комбинированных сигналов управления.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHuman：灵活真实的人类动画生成', 'desc': '本文提出了一种名为OmniHuman的框架，旨在提升人类动画生成的质量和灵活性。该框架基于扩散变换器，通过在训练阶段混合与运动相关的条件来扩展数据规模。OmniHuman支持多种人像内容和不同的驱动模式，如音频驱动和视频驱动，能够生成高度真实的人类视频。与现有方法相比，OmniHuman不仅生成更真实的视频，还提供了更大的输入灵活性。'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 41, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'PRIME: Эффективное обучение ИИ-моделей с неявными процессными наградами', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей, называемый PRIME. Он использует неявные процессные награды, что позволяет обновлять модели вознаграждения процесса в режиме онлайн, используя только развертывания политики и метки результатов. PRIME решает проблемы обучения с плотными наградами, такие как уязвимость к взлому наград и высокая стоимость сбора качественных меток процесса. Метод показал значительное улучшение производительности на задачах рассуждения по сравнению с базовыми моделями, используя при этом меньше данных для обучения.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIME：提升大语言模型推理效率的新方法', 'desc': '本文提出了一种新的方法PRIME（通过隐式奖励进行过程强化学习），旨在解决大语言模型（LLMs）在复杂多步骤推理任务中的训练效率问题。传统的稀疏结果奖励在训练过程中存在效率低下和信用分配等问题，而PRIME通过仅使用策略回滚和结果标签来实现在线过程奖励模型（PRM）的更新。该方法避免了现有方法中需要的专门奖励模型训练阶段，从而显著降低了开发成本。实验结果表明，PRIME在数学和编码竞赛任务中表现出色，相较于传统模型有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 24, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 января', 'en': 'January 28', 'zh': '1月28日'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'SafeRAG: оценка уязвимостей генерации с дополнением на основе извлечения', 'desc': 'В статье представлен бенчмарк SafeRAG для оценки безопасности генерации с дополнением на основе извлечения (RAG). Авторы классифицируют атаки на RAG и создают набор данных для их моделирования. Эксперименты показывают, что RAG уязвим ко всем типам атак, даже самым очевидным. Исследование демонстрирует, что существующие компоненты RAG не способны эффективно противостоять атакам, что приводит к снижению качества работы системы.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': '提升RAG安全性，抵御知识攻击！', 'desc': '本文介绍了一种名为SafeRAG的基准，用于评估检索增强生成（RAG）模型的安全性。我们将攻击任务分为银噪声、上下文冲突、软广告和拒绝服务等类型，并为每种任务手动构建了安全评估数据集。通过使用SafeRAG数据集，我们模拟了RAG可能遇到的各种攻击场景。实验结果表明，RAG对所有攻击任务表现出显著的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器和过滤器，导致RAG服务质量下降。'}}}, {'id': 'https://huggingface.co/papers/2502.01341', 'title': 'AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2502.01341', 'abstract': 'Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.', 'score': 23, 'issue_id': 2030, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '88ea73fcb0da69ba', 'authors': ['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'Ecole de Technologie Superieure', 'McGill University', 'Mila', 'Polytechnique Montreal', 'ServiceNow', 'Universite de Montreal', 'University of British Columbia', 'University of Waterloo', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01341.jpg', 'data': {'categories': ['#alignment', '#cv', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Улучшение выравнивания модальностей в мультимодальных моделях', 'desc': 'AlignVLM - это новый метод выравнивания визуальных и текстовых признаков в мультимодальных моделях. Он отображает визуальные признаки в виде взвешенного среднего текстовых эмбеддингов языковой модели. Этот подход использует лингвистические приоры, закодированные в языковой модели, чтобы обеспечить эффективную интерпретацию визуальных признаков. AlignVLM особенно эффективен для задач понимания документов и демонстрирует улучшенное выравнивание признаков и устойчивость к шуму.'}, 'en': {'title': 'Aligning Vision and Language for Better Understanding', 'desc': 'This paper addresses the challenge of aligning visual features with language embeddings in vision-language models (VLMs). The authors introduce AlignVLM, a new method that connects visual features to a weighted average of language model (LLM) text embeddings, enhancing semantic similarity. By utilizing the linguistic knowledge embedded in the LLM, AlignVLM ensures that visual features are accurately represented in a way that the LLM can understand. The results show that AlignVLM outperforms existing methods, particularly in tasks involving document understanding, while also demonstrating improved robustness to noisy inputs.'}, 'zh': {'title': '视觉与语言的完美对齐', 'desc': '在视觉语言模型（VLMs）中，将视觉特征与语言嵌入对齐是一个关键挑战。现有的连接器，如多层感知器（MLP），常常会产生分布外或噪声输入，导致模态之间的不对齐。我们提出了一种新颖的视觉文本对齐方法AlignVLM，它将视觉特征映射到LLM文本嵌入的加权平均值。AlignVLM在文档理解任务中表现尤为出色，能够有效提高视觉特征与文本内容的对齐和抗噪声能力。'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 22, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'Осторожно: LLM-судьи могут быть предвзяты!', 'desc': "Исследование выявляет проблему 'утечки предпочтений' при использовании больших языковых моделей (LLM) в качестве судей для оценки других моделей. Эта проблема возникает из-за связанности между генераторами синтетических данных и LLM-оценщиками. Эксперименты подтверждают предвзятость судей к связанным с ними моделям-ученикам на различных базовых моделях и эталонных тестах. Результаты указывают на то, что утечка предпочтений является распространенной и трудно обнаруживаемой проблемой в области использования LLM в качестве судей."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': '偏好泄漏：LLM评判中的隐患', 'desc': '本文探讨了大型语言模型（LLM）作为评判者和基于LLM的数据合成在模型开发中的应用。我们揭示了偏好泄漏这一问题，它是由合成数据生成器与LLM评估者之间的相关性引起的。通过定义三种常见的相关性，我们进行了广泛的实验，证实了评判者对其相关学生模型的偏见。研究表明，偏好泄漏是一个普遍存在且难以检测的问题，影响了LLM作为评判者的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 15, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': '🎚️', 'ru': {'title': 'SliderSpace: Раскрытие скрытых возможностей диффузионных моделей', 'desc': 'SliderSpace - это фреймворк для автоматической декомпозиции визуальных возможностей диффузионных моделей на управляемые и понятные человеку направления. Он обнаруживает множество интерпретируемых и разнообразных направлений одновременно из одного текстового запроса. Каждое направление обучается как адаптер низкого ранга, что позволяет осуществлять композиционный контроль. Эксперименты показывают эффективность SliderSpace в различных приложениях, включая декомпозицию концепций и исследование художественных стилей.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpace：可控的视觉能力分解', 'desc': 'SliderSpace是一个框架，用于自动分解扩散模型的视觉能力，使其可控且易于理解。与现有方法不同，SliderSpace可以从单个文本提示中同时发现多个可解释和多样化的方向，而无需用户逐个指定属性。每个方向被训练为低秩适配器，从而实现组合控制，并在模型的潜在空间中发现意想不到的可能性。通过对最先进的扩散模型进行广泛实验，我们证明了SliderSpace在概念分解、艺术风格探索和多样性增强等三个应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 13, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальный ИИ проваливает тест на интеллект', 'desc': 'Статья представляет новый фреймворк MM-IQ для оценки когнитивных способностей мультимодальных систем искусственного интеллекта. Фреймворк включает 2,710 тестовых заданий по 8 типам рассуждений, аналогично тестам IQ для людей. Результаты показали, что даже передовые мультимодальные модели демонстрируют точность лишь немного выше случайного угадывания (27.49% против 25%). Это подчеркивает существенный разрыв между текущими возможностями ИИ и базовыми когнитивными способностями человека.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQ：评估多模态系统的认知能力新标准', 'desc': '本论文提出了一种名为MM-IQ的综合评估框架，用于量化多模态系统中的认知能力。该框架包含2710个精心策划的测试项目，涵盖8种不同的推理范式。通过对领先的多模态模型进行系统评估，结果显示即使是最先进的架构，其表现也仅略高于随机猜测。这个显著的性能差距表明当前多模态系统在接近人类基本推理能力方面的不足，强调了需要进行范式转变的进步。'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 12, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'FastKV: Ускорение обработки длинных последовательностей в LLM', 'desc': 'Статья представляет FastKV - новый метод сжатия кэша ключ-значение (KV) для больших языковых моделей (LLM), направленный на улучшение латентности при обработке длинных последовательностей. FastKV использует подход Token-Selective Propagation (TSP), который сохраняет полную контекстную информацию в начальных слоях LLM и выборочно распространяет только часть этой информации в более глубоких слоях. Метод также включает сжатие кэша KV с учетом grouped-query attention (GQA) для повышения эффективности памяти и вычислений. Эксперименты показывают, что FastKV достигает значительного улучшения времени до первого токена и пропускной способности по сравнению с современными методами, сохраняя при этом точность на уровне базовых показателей.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKV：提升长上下文处理速度的创新方法', 'desc': '本文介绍了一种名为FastKV的KV缓存压缩方法，旨在提高长上下文序列的处理速度。FastKV采用了一种新颖的选择性传播方法（TSP），在LLM的初始层保留完整的上下文信息，而在更深层次中仅选择性传播部分信息。该方法还结合了分组查询注意力（GQA）来优化内存和计算效率。实验结果表明，FastKV在首次令牌时间和吞吐量方面分别比现有的HeadKV方法提高了2.00倍和1.40倍，同时在长上下文基准测试中保持了与基线相当的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 12, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'Linköping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'AIN: Прорыв в арабоязычном мультимодальном ИИ', 'desc': 'Представлена модель AIN - двуязычная мультимодальная языковая модель для арабского и английского языков. Модель обучена на 3,6 миллионах высококачественных мультимодальных арабско-английских образцов данных. AIN демонстрирует передовые результаты в арабском языке, сохраняя при этом сильные визуальные возможности для английского. На бенчмарке CAMEL-Bench, охватывающем 38 поддоменов, 7B-версия AIN превосходит GPT-4o на 3,4% в среднем по восьми доменам.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': '推动阿拉伯语多模态AI的进步', 'desc': '随着大型语言模型（LLMs）和多模态模型（LMMs）的快速发展，阿拉伯语的研究仍然相对滞后。我们提出了AIN模型，这是一个旨在提升阿拉伯语和英语的多模态模型，利用了360万高质量的阿拉伯语-英语多模态数据样本。AIN在多个领域表现出色，尤其是在复杂的视觉理解和多图像理解方面，超越了现有的GPT-4o模型。该模型的优越性能为阿拉伯语使用者提供了先进的多模态生成AI工具，推动了相关应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 9, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DeepRAG: умное сочетание поиска и рассуждений для ИИ', 'desc': 'DeepRAG - это новый фреймворк, моделирующий рассуждения с поиском информации как марковский процесс принятия решений. Он итеративно декомпозирует запросы, динамически определяя необходимость внешнего поиска или параметрического рассуждения на каждом шаге. Эксперименты показывают, что DeepRAG повышает эффективность поиска и точность ответов на 21.99%. Фреймворк решает проблемы больших языковых моделей, связанные с фактическими галлюцинациями и неэффективной декомпозицией задач при интеграции рассуждений с поиском.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAG：优化检索增强推理的新框架', 'desc': '大型语言模型（LLMs）在推理方面展现了显著的潜力，但仍然面临严重的事实幻觉问题，主要由于参数知识的时效性、准确性和覆盖率不足。将推理与检索增强生成（RAG）结合起来仍然具有挑战性，主要是因为任务分解不有效和冗余检索可能引入噪声，降低响应质量。本文提出了DeepRAG框架，将检索增强推理建模为马尔可夫决策过程（MDP），实现了战略性和自适应的检索。实验表明，DeepRAG在提高检索效率的同时，答案准确性提高了21.99%，证明了其在优化检索增强推理方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без увеличения вычислительных затрат', 'desc': 'SCONE - это новый метод расширения слоёв входных эмбеддингов для улучшения производительности языковых моделей при увеличении размера слоя. Он вводит эмбеддинги для частых n-грамм, обеспечивая контекстуализированное представление для каждого входного токена. Эти эмбеддинги обучаются отдельной моделью и предварительно вычисляются для использования во время инференса. SCONE позволяет масштабировать количество кэшированных n-граммных эмбеддингов и модель для их обучения, сохраняя фиксированное количество FLOPS при инференсе.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONE：提升语言模型性能的新方法', 'desc': '我们提出了一种方法SCONE（可扩展的上下文化的离线N-gram嵌入），旨在通过扩展输入嵌入层来提升语言模型的性能。SCONE在保持原有词汇的同时，引入了一组常见n-gram的嵌入，以提供每个输入标记的上下文化表示。这些嵌入在训练过程中由一个单独的模型学习，并在推理时预先计算并存储在离线加速器内存中，几乎不影响推理速度。通过增加缓存的n-gram嵌入数量和扩展学习它们的模型，SCONE在多种语料库上超越了1.9B参数的基线，同时仅使用一半的推理时间FLOPS。'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 8, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Проклятие сложности в логическом мышлении LLM', 'desc': 'В статье исследуются возможности логического рассуждения больших языковых моделей (LLM) и их масштабируемость в сложных задачах немонотонного вывода. Для этого представлена ZebraLogic, комплексная система оценки производительности LLM на логических головоломках, основанных на задачах удовлетворения ограничений (CSP). ZebraLogic позволяет генерировать головоломки с контролируемой сложностью, что помогает систематически изучать пределы масштабируемости моделей, таких как Llama и DeepSeek-R1. Результаты показывают значительное снижение точности по мере увеличения сложности задач, что авторы называют "проклятием сложности".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': '揭示大型语言模型推理能力的复杂性挑战', 'desc': '本文研究了大型语言模型（LLMs）的逻辑推理能力及其在复杂非单调推理中的可扩展性。我们引入了ZebraLogic，一个全面的评估框架，用于评估LLM在基于约束满足问题（CSPs）的逻辑网格谜题上的推理表现。研究结果显示，随着问题复杂性的增加，模型的准确性显著下降，这一现象被称为复杂性诅咒。我们还探讨了增强逻辑推理的策略，包括最佳采样、回溯机制和自我验证提示。'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эволюция рассуждений: от символов к мультимодальности', 'desc': 'Статья рассматривает эволюцию возможностей рассуждения в мультимодальных задачах у языковых моделей серий GPT и OpenAI. Авторы отмечают значительный прогресс модели o3 в решении символических паттернов, но подчеркивают необходимость исследования мультимодальных сценариев. Проводится анализ производительности моделей на сложных визуально-лингвистических головоломках, требующих абстрактного и алгоритмического мышления. Результаты показывают общую тенденцию улучшения способностей к рассуждению, но выявляют сохраняющиеся трудности даже у передовых моделей в некоторых типах задач.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': '多模态推理能力的探索与挑战', 'desc': '本文探讨了OpenAI的o1和o3模型在大型语言模型中的先进推理能力。o3在抽象和推理语料库（ARC-AGI）中超越了人类，表现出色，但该基准仅限于符号模式。人类通常在多模态场景中进行推理，因此需要研究多模态任务中的高级推理能力。尽管o1在推理能力上有所提升，但在简单的多模态难题和算法难题上仍然存在不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Новый рубеж в model-based RL: превосходя человека в Craftax-classic', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе модели, достигающий наилучших результатов на бенчмарке Craftax-classic. Авторы разработали алгоритм, превосходящий как предыдущий SOTA-метод DreamerV3, так и человеческий уровень производительности. Ключевые улучшения включают комбинированную архитектуру политики с CNN и RNN, обучение на реальных и воображаемых данных, токенизацию изображений методом ближайших соседей и блочное teacher forcing для трансформерной модели мира. Эти инновации позволили значительно повысить эффективность использования данных в сложной среде, требующей широкого спектра навыков.'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': '基于模型的强化学习新突破！', 'desc': '本文提出了一种基于模型的强化学习方法，在Craftax-classic基准测试中取得了新的最佳表现。这是一款开放世界的2D生存游戏，要求智能体展现出强大的泛化能力、深度探索能力和长期推理能力。我们的MBRL算法在仅1M环境步骤后获得了67.4%的奖励，显著超越了DreamerV3的53.2%，并首次超过了人类表现的65.0%。该方法通过构建一个最先进的无模型基线，并结合CNN和RNN的新型策略架构，进一步提升了样本效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Безопасная генерация ответов LLM без переобучения', 'desc': 'Статья представляет новый подход к выравниванию больших языковых моделей (LLM) во время вывода, обеспечивающий генерацию безопасных ответов с вероятностью, близкой к единице. Авторы формулируют задачу как ограниченный марковский процесс принятия решений в латентном пространстве модели. Они вводят состояние безопасности, которое отслеживает эволюцию ограничений безопасности и позволяет доказать формальные гарантии безопасности. На основе этого подхода разработан метод InferenceGuard, который эффективно балансирует безопасность и производительность задачи, превосходя существующие методы выравнивания во время вывода.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': '推理时安全对齐的新方法', 'desc': '本文提出了一种新的推理时对齐方法，旨在确保大型语言模型（LLM）生成安全的响应。我们将安全生成推理时响应的问题框架化为一个约束的马尔可夫决策过程（MDP），并在LLM的潜在空间中进行处理。通过引入一个安全状态来跟踪安全约束的演变，我们能够在解决MDP时提供正式的安全保证。我们提出的InferenceGuard实现了在不修改模型权重的情况下安全对齐LLM，并在生成安全和对齐响应方面优于现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 6, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': '🧠', 'ru': {'title': 'Преодоление выбросов в латентном пространстве для улучшения консистентных моделей', 'desc': 'Эта статья представляет новый подход к обучению консистентных моделей в латентном пространстве для генеративных задач. Авторы обнаружили, что латентные данные часто содержат импульсивные выбросы, которые ухудшают производительность iCT. Для решения этой проблемы они предложили использовать функцию потерь Коши вместо Псевдо-Хубера, а также ввели диффузионные потери на ранних временных шагах и применили оптимальный транспорт. Эти стратегии позволили успешно обучить латентные консистентные модели, способные к высококачественному сэмплированию за один-два шага.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': '提升一致性模型性能的创新方法', 'desc': '一致性模型是一种新型生成模型，能够在单步或多步中生成高质量样本。最近，这些模型在像素空间中表现出色，达到了与扩散模型相当的效果。然而，在大规模数据集上进行一致性训练的成功，尤其是在文本到图像和视频生成任务中，取决于潜在空间的表现。为了解决潜在数据中的异常值对性能的影响，本文提出了使用Cauchy损失替代伪Huber损失，并引入扩散损失和最优传输方法，以提高模型的鲁棒性和性能。'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 5, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Новый бенчмарк раскрывает скрытые возможности и недостатки языковых моделей', 'desc': 'Авторы представляют новый бенчмарк для оценки языковых моделей, основанный на головоломках NPR Sunday Puzzle Challenge. В отличие от существующих тестов, требующих специализированных знаний, этот бенчмарк опирается на общие знания и легко проверяем. Исследование выявило значительное превосходство модели OpenAI o1 над другими моделями рассуждений. Анализ также обнаружил новые типы ошибок у моделей, такие как преждевременная капитуляция и неуверенность в ответах.'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': '挑战性与可验证性的全新基准测试', 'desc': '现有的前沿模型基准测试通常考察专业的、难以理解的知识。我们提出了一种基于NPR周日谜题挑战的基准，要求仅具备一般知识。该基准对人类和模型都具有挑战性，但正确答案易于验证，模型的错误也容易发现。我们的研究揭示了现有基准中未显现的能力差距，OpenAI o1在推理模型中表现优异，且分析推理输出揭示了新的失败类型。'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование редактирования знаний в нейросетях', 'desc': 'Статья посвящена улучшению методов последовательного редактирования знаний в больших языковых моделях. Авторы выявили проблемы переобучения и непропорционального роста нормы при использовании существующих методов редактирования. Они предложили новый метод ENCORE, который контролирует переобучение и рост нормы, позволяя выполнять до 10 000 последовательных правок без потери производительности модели. ENCORE также показывает значительное ускорение по сравнению с другими методами редактирования знаний.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCORE：高效的知识编辑解决方案', 'desc': '本论文研究了在知识编辑中进行大规模顺序编辑时模型性能下降的原因。我们发现，定位后编辑的方法容易导致对编辑事实的过拟合，并且连续的知识编辑会导致编辑矩阵的范数不成比例地增长。为了解决这些问题，我们提出了ENCORE方法，通过早停和范数约束来控制过拟合和范数增长，从而实现长时间的顺序编辑。ENCORE能够在不损失下游性能的情况下，进行多达10,000次的顺序编辑，并且比现有方法更快。'}}}, {'id': 'https://huggingface.co/papers/2502.01619', 'title': 'Learning to Generate Unit Tests for Automated Debugging', 'url': 'https://huggingface.co/papers/2502.01619', 'abstract': "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGen into UTDebug, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDebug (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGen outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines.", 'score': 2, 'issue_id': 2036, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '0806c010a6d2569d', 'authors': ['Archiki Prasad', 'Elias Stengel-Eskin', 'Justin Chih-Yao Chen', 'Zaid Khan', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.01619.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#plp'], 'emoji': '🧪', 'ru': {'title': 'Автоматическая генерация юнит-тестов для улучшения отладки кода языковыми моделями', 'desc': 'Статья представляет UTGen - метод обучения языковых моделей генерации юнит-тестов, выявляющих ошибки в коде и предсказывающих корректные выходные данные. UTGen интегрирован в конвейер отладки UTDebug, который использует сгенерированные тесты для эффективной отладки кода языковыми моделями. UTDebug масштабирует UTGen во время выполнения для улучшения предсказания выходных данных тестов и проверяет изменения на основе нескольких сгенерированных юнит-тестов. Результаты показывают, что UTGen превосходит базовые методы генерации юнит-тестов, а UTDebug улучшает точность отладки кода языковыми моделями.'}, 'en': {'title': 'Enhancing Debugging with Smart Unit Test Generation', 'desc': 'This paper introduces UTGen, a method that helps large language models (LLMs) generate unit test inputs that can effectively reveal errors in faulty code while also predicting the correct outputs. The authors highlight a challenge where generating tests that expose errors can lead to incorrect output predictions without having the correct solutions available. To overcome this, UTGen is integrated into a debugging pipeline called UTDebug, which enhances the debugging process by validating and refining the generated tests. The results show that UTGen significantly improves the accuracy of LLMs in debugging tasks, outperforming existing methods in generating effective unit tests.'}, 'zh': {'title': '自动化单元测试生成与调试的创新方案', 'desc': '本文提出了一种名为UTGen的自动化单元测试生成方法，旨在帮助大型语言模型（LLM）生成能够揭示错误的单元测试输入及其正确的预期输出。研究发现，生成的单元测试输入在揭示错误和正确预测输出之间存在权衡。UTGen被集成到UTDebug调试管道中，以提高LLM的调试效果，并通过多次生成的测试来验证和回溯编辑，避免过拟合。实验结果表明，UTGen在生成有效单元测试方面优于现有基线，并显著提高了LLM在调试任务中的准确性。'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 1, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Путь к надежным фундаментальным моделям в патологии: преодоление влияния медицинских центров', 'desc': 'Статья рассматривает проблему надежности фундаментальных моделей (ФМ) в патологии для использования в клинической практике. Авторы вводят новую метрику - Индекс надежности, который отражает степень доминирования биологических признаков над мешающими факторами, связанными с особенностями медицинских центров. Оценка десяти публично доступных ФМ показала, что все они сильно зависят от специфики медицинских центров, и только одна модель имеет индекс надежности больше единицы. Исследование демонстрирует, что ошибки классификации типов рака связаны с конфаундерами из того же медицинского центра, а пространства вложений ФМ организованы больше по медицинским центрам, чем по биологическим факторам.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': '确保病理模型的鲁棒性，助力临床应用', 'desc': '病理基础模型（FMs）在医疗保健中具有很大潜力，但在临床应用之前，必须确保它们对不同医疗中心的变化具有鲁棒性。我们提出了鲁棒性指数，这是一种新颖的鲁棒性度量，反映生物特征在多大程度上主导了混淆特征。研究发现，当前的病理基础模型在很大程度上代表了医疗中心，只有一个模型的鲁棒性指数大于一，表明生物特征略微主导混淆特征。通过定量方法分析医疗中心差异对模型预测性能的影响，发现癌症类型分类错误与同中心混淆因素密切相关。'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 1, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная сегментация опухолей с помощью усовершенствованных нейросетей', 'desc': 'Статья посвящена автоматической сегментации опухолей забрюшинного пространства с использованием различных архитектур глубокого обучения. Авторы сравнивают эффективность U-Net и его модификаций, включая CNN, ViT, Mamba SSM и xLSTM, на новом наборе данных КТ. Предложенная модель ViLU-Net интегрирует Vi-блоки для улучшения сегментации. Результаты показывают, что xLSTM обеспечивает наиболее эффективную работу в рамках архитектуры U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': '高效肿瘤分割：ViLU-Net的创新应用', 'desc': '本研究探讨了在后腹膜肿瘤的自动分割中使用U-Net及其变体的有效性。这些肿瘤形状不规则，手动分割耗时且困难，因此需要更高效的自动化方法。研究中引入了Mamba状态空间模型和扩展长短期记忆（xLSTM）等架构，以降低计算资源消耗并处理长距离依赖。最终提出的ViLU-Net模型通过集成Vi-blocks，显著提高了分割效果，xLSTM在U-Net框架中的效率表现尤为突出。'}}}, {'id': 'https://huggingface.co/papers/2502.01126', 'title': 'Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences', 'url': 'https://huggingface.co/papers/2502.01126', 'abstract': 'Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model\'s preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model\'s confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets.', 'score': 0, 'issue_id': 2035, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'ed2eef6c3e310379', 'authors': ['Vaishnavi Shrivastava', 'Ananya Kumar', 'Percy Liang'], 'affiliations': ['cs.stanford.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.01126.jpg', 'data': {'categories': ['#interpretability', '#rlhf', '#reasoning', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности языковых моделей через относительную оценку уверенности', 'desc': 'Исследование предлагает метод относительной оценки уверенности языковых моделей, где модель сравнивает свою уверенность в ответах на разные вопросы. Этот подход использует методы ранжирования, такие как рейтинг Эло и модель Брэдли-Терри, для перевода предпочтений модели в оценки уверенности. Результаты показывают, что относительная оценка уверенности превосходит абсолютную оценку и методы самосогласованности на 3.5% и 1.7% соответственно по метрике AUC выборочной классификации. Исследование проводилось на пяти современных языковых моделях и 14 сложных задачах по ответам на вопросы в областях STEM, социальных наук и здравого смысла.'}, 'en': {'title': 'Boosting Confidence: Relative Estimation Outshines Absolute Scores in Language Models', 'desc': 'This paper discusses the importance of reliable confidence estimates from language models (LMs) to help users identify potential errors in their outputs. It highlights the challenges LMs face in providing absolute confidence assessments and proposes a new method called relative confidence estimation. This method involves comparing questions against each other to determine which the model is more confident in answering correctly, using techniques like Elo rating and Bradley-Terry for ranking. The study shows that relative confidence estimation outperforms traditional absolute confidence methods, leading to improved reliability in confidence scores across various question answering tasks.'}, 'zh': {'title': '相对置信度估计：提升语言模型的可靠性', 'desc': '本文探讨了语言模型（LM）如何提供可靠的置信度评估，以帮助用户识别输出中的错误。我们提出了一种相对置信度估计的方法，通过比较问题之间的相对置信度来评估模型的信心。与绝对置信度评估相比，相对置信度估计在多个先进的语言模型上表现出更高的可靠性，尤其是在选择性分类任务中。实验结果显示，相对置信度估计在准确性上平均提高了3.5%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (7)', '#agents (8)', '#agi (7)', '#alignment (8)', '#architecture (34)', '#audio (2)', '#benchmark (41)', '#cv (22)', '#data (10)', '#dataset (39)', '#diffusion (16)', '#ethics (6)', '#games (3)', '#graphs (1)', '#hallucinations (6)', '#healthcare (2)', '#inference (27)', '#interpretability (15)', '#leakage (2)', '#long_context (10)', '#low_resource (6)', '#machine_translation', '#math (10)', '#multilingual (5)', '#multimodal (28)', '#open_source (20)', '#optimization (54)', '#plp (3)', '#rag (5)', '#reasoning (33)', '#rl (12)', '#rlhf (11)', '#robotics (1)', '#science (2)', '#security (9)', '#small_models (5)', '#story_generation', '#survey (1)', '#synthetic (7)', '#training (74)', '#transfer_learning (6)', '#video (15)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-11 05:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-11 05:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-11 05:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    