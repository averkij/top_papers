
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. May 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Май 2025</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-04.html">⬅️ <span id="prev-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-06.html">➡️ <span id="next-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Май 2025', 'en': 'May 2025', 'zh': '5月2025年'};
        let feedDateNext = {'ru': '06.2025', 'en': '06/2025', 'zh': '6月2025年'};
        let feedDatePrev = {'ru': '04.2025', 'en': '04/2025', 'zh': '4月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.21853', 'title': 'A Survey of Interactive Generative Video', 'url': 'https://huggingface.co/papers/2504.21853', 'abstract': 'Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.', 'score': 25, 'issue_id': 3550, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '4e975f915f638955', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Hao Chen', 'Xihui Liu'], 'affiliations': ['Kuaishou Technology, Shenzhen, China', 'The Hong Kong University of Science and Technology (HKUST), Hong Kong', 'The University of Hong Kong, Pok Fu Lam, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.21853.jpg', 'data': {'categories': ['#robotics', '#interpretability', '#video', '#optimization', '#survey', '#agents', '#games', '#multimodal'], 'emoji': '🎮', 'ru': {'title': 'IGV: Будущее интерактивного видеоконтента', 'desc': 'Статья представляет обзор технологии Интерактивного Генеративного Видео (IGV), которая сочетает генеративные возможности с интерактивными функциями. Авторы рассматривают применение IGV в трех основных областях: игры, воплощенный ИИ и автономное вождение. Предложена комплексная структура, разбивающая идеальную систему IGV на пять модулей: Генерация, Контроль, Память, Динамика и Интеллект. В работе анализируются технические проблемы и будущие направления развития каждого компонента IGV.'}, 'en': {'title': 'Empowering Interactive Experiences with Generative Video', 'desc': 'Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.'}, 'zh': {'title': '推动互动生成视频技术的未来发展', 'desc': '互动生成视频（IGV）是一种新兴技术，旨在满足对高质量互动视频内容的需求。本文将IGV定义为一种结合生成能力和互动特性的技术，能够通过控制信号和反馈实现用户参与。我们调查了IGV在游戏、具身人工智能和自动驾驶等三个主要领域的应用，并提出了一个全面的框架，将理想的IGV系统分解为生成、控制、记忆、动态和智能五个模块。通过系统分析技术挑战和未来方向，本文旨在推动IGV领域的研究与发展。'}}}, {'id': 'https://huggingface.co/papers/2505.00662', 'title': 'DeepCritic: Deliberate Critique with Large Language Models', 'url': 'https://huggingface.co/papers/2505.00662', 'abstract': 'As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.', 'score': 22, 'issue_id': 3549, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '259dddc97137d27a', 'authors': ['Wenkai Yang', 'Jingwen Chen', 'Yankai Lin', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00662.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#reasoning', '#rlhf', '#math', '#training'], 'emoji': '🧮', 'ru': {'title': 'Усовершенствование LLM для глубокой критики математических решений', 'desc': 'Эта статья представляет новый двухэтапный подход к улучшению способностей больших языковых моделей (LLM) критиковать математические решения. На первом этапе используется Qwen2.5-72B-Instruct для генерации подробных пошаговых критик, которые служат обучающими данными. Затем применяется обучение с подкреплением для дальнейшего улучшения критических способностей модели. Разработанная модель-критик на базе Qwen2.5-7B-Instruct превосходит существующие LLM-критики в выявлении ошибок и предоставлении детальной обратной связи.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions', 'desc': "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."}, 'zh': {'title': '提升大型语言模型的数学批评能力', 'desc': '随着大型语言模型（LLMs）的快速发展，提供准确的反馈和可扩展的监督变得尤为重要。本文提出了一种新颖的两阶段框架，旨在增强LLMs在数学批评方面的能力。通过生成详细的逐步批评，模型能够更准确地识别错误并提供深入的反馈，帮助生成模型纠正错误。我们的实验表明，改进后的批评模型在错误识别基准测试中显著优于现有的LLM批评者。'}}}, {'id': 'https://huggingface.co/papers/2505.00703', 'title': 'T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT', 'url': 'https://huggingface.co/papers/2505.00703', 'abstract': 'Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1', 'score': 16, 'issue_id': 3548, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'ca564761ff71d15e', 'authors': ['Dongzhi Jiang', 'Ziyu Guo', 'Renrui Zhang', 'Zhuofan Zong', 'Hao Li', 'Le Zhuo', 'Shilin Yan', 'Pheng-Ann Heng', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'CUHK MiuLar Lab', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.00703.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🎨', 'ru': {'title': 'Рассуждения на двух уровнях улучшают генерацию изображений по тексту', 'desc': 'Данная статья представляет T2I-R1 - новую модель генерации изображений по тексту, улучшенную с помощью рассуждений и обучения с подкреплением. Модель использует двухуровневый процесс рассуждений: семантический уровень для планирования промпта и токенный уровень для попиксельной генерации. Авторы вводят метод BiCoT-GRPO для оптимизации обоих уровней рассуждений. Применение этих стратегий к базовой модели Janus-Pro позволило значительно улучшить результаты на бенчмарках T2I-CompBench и WISE.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Enhanced Reasoning', 'desc': 'This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests.'}, 'zh': {'title': '双层思维链提升文本到图像生成', 'desc': '本文介绍了一种新颖的文本到图像生成模型T2I-R1，该模型结合了强化学习和双层思维链推理过程。我们提出了两种思维链：语义层次的思维链用于高层次的提示规划，令生成过程更具逻辑性；而令牌层次的思维链则用于在逐块生成过程中进行低层次的像素处理。通过引入BiCoT-GRPO，我们能够在同一训练步骤中优化这两种思维链，从而提升生成效果。实验结果表明，T2I-R1在多个基准测试中表现优异，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2505.00497', 'title': 'KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution', 'url': 'https://huggingface.co/papers/2505.00497', 'abstract': 'Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.', 'score': 3, 'issue_id': 3554, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'dc645b5c7e476cea', 'authors': ['Antoni Bigata', 'Rodrigo Mira', 'Stella Bounareli', 'Michał Stypułkowski', 'Konstantinos Vougioukas', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London', 'University of Wrocław'], 'pdf_title_img': 'assets/pdf/title_img/2505.00497.jpg', 'data': {'categories': ['#cv', '#leakage', '#video', '#open_source', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'KeySync: Революция в синхронизации губ для видео', 'desc': 'KeySync - это новая двухэтапная система для синхронизации губ в видео с новым аудио. Она решает проблемы временной согласованности, утечки выражений и окклюзий лица с помощью специальной стратегии маскирования. KeySync достигает лучших результатов в реконструкции губ и кросс-синхронизации, улучшая визуальное качество и уменьшая утечку выражений согласно новой метрике LipLeak. Система эффективно справляется с окклюзиями и превосходит существующие методы в реальных приложениях, таких как автоматический дубляж.'}, 'en': {'title': 'KeySync: Mastering Lip Synchronization with Precision', 'desc': 'This paper introduces KeySync, a two-stage framework designed to improve lip synchronization in videos by aligning lip movements with new audio inputs. It addresses common challenges in talking head generation, such as maintaining temporal consistency and managing expression leakage and facial occlusions. KeySync employs a unique masking strategy to effectively tackle these issues, resulting in enhanced visual quality and reduced leakage as measured by a new metric called LipLeak. The framework demonstrates state-of-the-art performance in lip reconstruction and cross-synchronization, validated through various ablation studies.'}, 'zh': {'title': 'KeySync：提升唇部同步的创新框架', 'desc': '本论文介绍了一种名为KeySync的双阶段框架，用于解决唇部同步中的时间一致性问题。该方法还通过精心设计的遮罩策略，解决了输入视频中的表情泄漏和面部遮挡等新挑战。实验结果表明，KeySync在唇部重建和交叉同步方面达到了最先进的效果，显著提高了视觉质量并减少了表情泄漏。我们还通过多项消融研究验证了新遮罩方法在处理遮挡方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.21659', 'title': 'AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization', 'url': 'https://huggingface.co/papers/2504.21659', 'abstract': 'Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1', 'score': 3, 'issue_id': 3549, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '6487e5a67faf07a5', 'authors': ['Haotian Luo', 'Haiying He', 'Yibo Wang', 'Jinluan Yang', 'Rui Liu', 'Naiqiang Tan', 'Xiaochun Cao', 'Dacheng Tao', 'Li Shen'], 'affiliations': ['China Agricultural University', 'Didichuxing Co. Ltd', 'Nanyang Technological University', 'Sun Yat-sen University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21659.jpg', 'data': {'categories': ['#reasoning', '#inference', '#math', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное рассуждение: эффективность без потери качества', 'desc': 'Статья представляет новый подход к адаптивному и эффективному рассуждению в языковых моделях. Авторы предлагают двухэтапную структуру, включающую гибридную модель рассуждений и би-уровневое обучение предпочтениям. Эксперименты показывают, что метод значительно сокращает вычислительные затраты при сохранении производительности. На пяти математических наборах данных средняя длина рассуждений сократилась более чем на 50%.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Inference in Complex Tasks', 'desc': 'This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets.'}, 'zh': {'title': '自适应推理，提升效率！', 'desc': '最近，长推理模型在复杂推理任务中表现出色，但常常导致推理开销大，因此效率成为一个关键问题。我们的实证分析显示，使用长链推理（Long-CoT）的好处在不同问题上差异很大：有些问题需要复杂推理，而其他问题则没有改善，甚至准确率下降。这促使我们提出自适应推理策略，根据输入调整推理深度。我们提出了一种新颖的两阶段框架，通过混合长短链推理模型和双层偏好训练，显著降低推理成本，同时保持性能。'}}}, {'id': 'https://huggingface.co/papers/2504.20605', 'title': 'TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models', 'url': 'https://huggingface.co/papers/2504.20605', 'abstract': 'Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.', 'score': 3, 'issue_id': 3554, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '9637fef0c8d474ed', 'authors': ['Mihai Nadas', 'Laura Diosan', 'Andrei Piscoran', 'Andreea Tomescu'], 'affiliations': ['Babes-Bolyai University', 'KlusAI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.20605.jpg', 'data': {'categories': ['#story_generation', '#ethics', '#multimodal', '#dataset', '#benchmark', '#alignment', '#open_source'], 'emoji': '📚', 'ru': {'title': 'Масштабное моральное повествование без гигантских моделей', 'desc': 'Исследователи создали TF1-EN-3M - первый открытый датасет из трех миллионов англоязычных басен, сгенерированных моделями до 8 миллиардов параметров. Каждая история следует шестиэлементной структуре, охватывая широкий тематический спектр. Оценка качества историй производится с помощью гибридного подхода, сочетающего критику на основе GPT и метрики разнообразия и читабельности. Датасет, код генерации и скрипты оценки выпущены под свободной лицензией, открывая возможности для исследований в области следования инструкциям, нарративного интеллекта и образовательного ИИ.'}, 'en': {'title': 'Empowering Moral Storytelling with TF1-EN-3M', 'desc': 'This paper introduces TF1-EN-3M, a novel dataset containing three million English fables designed to teach moral lessons, generated by smaller instruction-tuned models. The stories are structured using a six-slot framework that ensures each narrative includes essential elements like characters, traits, and morals. A unique evaluation system combines assessments from a GPT-based critic and various metrics to ensure quality and diversity in the generated fables. The dataset and associated tools are made publicly available, promoting further research in areas like narrative intelligence and ethical AI development.'}, 'zh': {'title': '道德故事生成的新纪元', 'desc': '这篇论文介绍了TF1-EN-3M，这是第一个包含三百万个英语寓言的开放数据集，专门由指令调优模型生成。每个故事遵循六个部分的结构，包括角色、特征、背景、冲突、解决方案和道德，确保了故事的连贯性和主题的多样性。论文还提出了一种混合评估方法，结合了基于GPT的评分和无参考的多样性与可读性指标。TF1-EN-3M为道德故事的生成和教育AI的研究提供了新的可能性，表明大规模的道德叙事不再需要专有的大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.19394', 'title': 'LLMs for Engineering: Teaching Models to Design High Powered Rockets', 'url': 'https://huggingface.co/papers/2504.19394', 'abstract': "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.", 'score': 1, 'issue_id': 3558, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': '5b65f8f309063f13', 'authors': ['Toby Simonds'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.19394.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#agi', '#benchmark', '#games', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'ИИ с обучением с подкреплением превосходит людей в проектировании ракет', 'desc': 'Это исследование оценивает возможности больших языковых моделей (LLM) в проектировании мощных ракет с помощью бенчмарка RocketBench. Модели тестируются на задачах оптимизации целевой высоты и точной посадки. Результаты показывают, что современные LLM демонстрируют хорошие базовые инженерные знания, но затрудняются с итерациями на основе результатов симуляций. Однако при усилении обучением с подкреплением (RL) модель с 7 миллиардами параметров превосходит как современные базовые модели, так и экспертов-людей.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Rocket Design Challenges', 'desc': 'This paper investigates the use of Large Language Models (LLMs) in the field of physical engineering, specifically in high-powered rocketry design. It introduces RocketBench, a benchmark that connects LLMs to advanced rocket simulations, and evaluates their performance on design tasks like optimizing target altitude and achieving precision landings. The results indicate that while LLMs possess a solid foundation in engineering knowledge, they struggle to improve their designs based on simulation feedback, often falling short of human capabilities. However, by incorporating reinforcement learning, a 7B parameter model surpasses both state-of-the-art models and human experts, highlighting the potential of RL-enhanced LLMs in complex engineering optimization tasks.'}, 'zh': {'title': '强化学习助力火箭设计，超越人类专家！', 'desc': '大型语言模型（LLMs）在软件工程中取得了显著进展，但在物理工程领域的应用仍然不够深入。本文通过RocketBench基准评估LLMs在高功率火箭设计中的能力，连接LLMs与高保真火箭模拟。我们测试了两个复杂的设计任务：目标高度优化和精确着陆挑战。研究发现，尽管最先进的LLMs展现出强大的工程知识，但在根据模拟结果迭代设计时表现不佳，最终未能超越人类专家的水平；然而，通过强化学习（RL）增强后，7B参数模型的表现超过了现有基础模型和人类专家。'}}}, {'id': 'https://huggingface.co/papers/2504.18983', 'title': 'MediAug: Exploring Visual Augmentation in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.18983', 'abstract': 'Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.', 'score': 1, 'issue_id': 3554, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': '13143b6fb9ae9216', 'authors': ['Xuyin Qi', 'Zeyu Zhang', 'Canxuan Gang', 'Hao Zhang', 'Lei Zhang', 'Zhiwei Zhang', 'Yang Zhao'], 'affiliations': ['AIML', 'ANU', 'La Trobe', 'PSU', 'UCAS', 'UNSW'], 'pdf_title_img': 'assets/pdf/title_img/2504.18983.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#healthcare', '#synthetic', '#training'], 'emoji': '🩺', 'ru': {'title': 'MediAug: Новый эталон аугментации данных для медицинской визуализации', 'desc': 'Эта статья представляет MediAug - комплексный фреймворк для оценки методов аугментации данных в медицинской визуализации. Авторы исследуют шесть методов аугментации на основе смешивания изображений, применяя их к задачам классификации опухолей мозга и глазных заболеваний. Эксперименты проводятся с использованием как сверточных нейронных сетей (ResNet-50), так и трансформеров (ViT-B). Результаты показывают, что разные методы аугментации дают наилучшие результаты для разных архитектур и задач, подчеркивая важность выбора подходящего метода для конкретной задачи.'}, 'en': {'title': 'Enhancing Medical Imaging with Advanced Data Augmentation', 'desc': 'This paper addresses the challenges of data augmentation in medical imaging, particularly the domain gap between natural images and medical scans. It introduces MediAug, a benchmark framework that evaluates six mix-based augmentation methods across different neural network architectures. The study finds that MixUp and SnapMix significantly enhance classification accuracy for brain tumors and eye diseases, respectively. By providing a systematic evaluation, the paper clarifies the effectiveness of various augmentation strategies in improving medical image analysis.'}, 'zh': {'title': '医学影像数据增强的新突破', 'desc': '数据增强在医学影像中对于提高分类准确性、病变检测和器官分割至关重要，尤其是在数据有限的情况下。然而，医学影像与自然照片之间的显著领域差距可能会扭曲关键的疾病特征。此外，现有的增强研究往往局限于单一任务或架构，导致先进的混合策略的优势不明确。为了解决这些问题，我们提出了一个统一的评估框架，整合了六种基于混合的数据增强方法，并在脑肿瘤MRI和眼病视网膜图像数据集上进行了评估。'}}}, {'id': 'https://huggingface.co/papers/2505.00534', 'title': 'A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic', 'url': 'https://huggingface.co/papers/2505.00534', 'abstract': 'Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.', 'score': 0, 'issue_id': 3560, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'd411bc0f9d34adf4', 'authors': ['Muhammad Imran Zaman', 'Usama Ijaz Bajwa', 'Gulshan Saleem', 'Rana Hammad Raza'], 'affiliations': ['Department of Computer Science, COMSATS University Islamabad, Lahore Campus, Lahore, Pakistan', 'Pakistan Navy Engineering College, National University of Sciences and Technology (NUST), Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2505.00534.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#optimization', '#video', '#transfer_learning'], 'emoji': '🚗', 'ru': {'title': 'Умное отслеживание транспорта с помощью глубокого обучения', 'desc': 'Статья представляет фреймворк для отслеживания транспортных средств с использованием нескольких камер в городских условиях. Авторы применяют глубокое обучение, включая Mask R-CNN для обнаружения объектов и ResNet-152 для извлечения признаков. Система решает проблемы окклюзии, изменения освещения и теней с помощью специальных функций потерь и метрик расстояния. Фреймворк показал высокую эффективность на наборе данных AI City Challenge, достигнув IDF1 0.8289.'}, 'en': {'title': 'Revolutionizing Traffic Monitoring with Deep Learning Tracking', 'desc': 'This paper presents a deep learning framework for Multi-Object Multi-Camera Tracking (MO-MCT) aimed at improving traffic monitoring in Intelligent Transportation Systems. The framework utilizes Mask R-CNN for detecting vehicles and applies Non-Maximum Suppression (NMS) to manage overlapping detections. It incorporates transfer learning for vehicle re-identification, allowing for the tracking of vehicles across different cameras despite challenges like occlusions and varying lighting conditions. The system is evaluated on a large dataset and shows strong performance metrics, indicating its potential for real-world traffic applications.'}, 'zh': {'title': '智能交通中的高效多摄像头跟踪解决方案', 'desc': '随着网络摄像头数量的增加，视觉传感器在智能交通系统中的重要性日益增强。本文提出了一种基于深度学习的多目标多摄像头跟踪框架，旨在解决城市交通场景中手动目标跟踪和匹配的挑战。该框架利用Mask R-CNN进行目标检测，并通过非极大值抑制（NMS）选择重叠检测中的目标。通过迁移学习实现车辆的重新识别，结合适当的损失函数和距离度量，最终在AI City Challenge数据集上取得了竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.21635', 'title': 'Sadeed: Advancing Arabic Diacritization Through Small Language Model', 'url': 'https://huggingface.co/papers/2504.21635', 'abstract': "Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.", 'score': 44, 'issue_id': 3530, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'af5a1b038b3ccab3', 'authors': ['Zeina Aldallal', 'Sara Chrouf', 'Khalil Hennara', 'Mohamed Motaism Hamed', 'Muhammad Hreden', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21635.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#multilingual', '#benchmark', '#data', '#machine_translation'], 'emoji': '🔠', 'ru': {'title': 'Sadeed: Прорыв в диакритизации арабского текста с помощью компактной языковой модели', 'desc': 'Sadeed - это новый подход к диакритизации арабского текста, основанный на модели Kuwain 1.5B. Модель была дообучена на тщательно отобранных и обработанных наборах данных с диакритическими знаками. Несмотря на ограниченные вычислительные ресурсы, Sadeed показывает конкурентоспособные результаты по сравнению с проприетарными большими языковыми моделями. Авторы также представили новый бенчмарк SadeedDiac-25 для более справедливой оценки моделей диакритизации арабского текста.'}, 'en': {'title': 'Sadeed: Advancing Arabic Diacritization with Efficiency and Precision', 'desc': 'This paper presents Sadeed, a new method for Arabic text diacritization using a fine-tuned decoder-only language model based on Kuwain 1.5B. Sadeed is specifically trained on high-quality diacritized datasets, which were created through a thorough data-cleaning process. The model demonstrates competitive performance against larger proprietary models while requiring less computational power. Additionally, the authors introduce SadeedDiac-25, a new benchmark for evaluating Arabic diacritization, aiming to improve assessment practices in the field.'}, 'zh': {'title': 'Sadeed：阿拉伯语标记化的新突破', 'desc': '本文介绍了一种新的阿拉伯语文本标记化方法，名为Sadeed。该方法基于经过微调的解码器语言模型，专门针对阿拉伯语的丰富形态特征进行优化。Sadeed在高质量的标记化数据集上进行微调，尽管计算资源有限，但其性能与大型语言模型相当，且优于传统模型。此外，本文还提出了SadeedDiac-25基准，以促进对阿拉伯语标记化的公平评估。'}}}, {'id': 'https://huggingface.co/papers/2504.21776', 'title': 'WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability', 'url': 'https://huggingface.co/papers/2504.21776', 'abstract': 'Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.', 'score': 26, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '61ce82abe42f584a', 'authors': ['Xiaoxi Li', 'Jiajie Jin', 'Guanting Dong', 'Hongjin Qian', 'Yutao Zhu', 'Yongkang Wu', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21776.jpg', 'data': {'categories': ['#rl', '#science', '#agents', '#rlhf', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🕸️', 'ru': {'title': 'WebThinker: Расширение возможностей ИИ для глубоких веб-исследований', 'desc': 'WebThinker - это глубокая исследовательская система, которая расширяет возможности больших языковых моделей (LLM) для автономного поиска в интернете и составления исследовательских отчетов. Система включает модуль Deep Web Explorer для динамического поиска и извлечения информации из веб-страниц, а также стратегию автономного мышления, поиска и составления черновиков. WebThinker использует обучение с подкреплением на основе прямой оптимизации предпочтений (DPO) для улучшения использования исследовательских инструментов. Эксперименты показывают, что WebThinker значительно превосходит существующие методы на сложных задачах рассуждения и генерации научных отчетов.'}, 'en': {'title': 'Empowering LRMs with Real-Time Web Research Capabilities', 'desc': "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."}, 'zh': {'title': 'WebThinker：让推理模型更智能的研究助手', 'desc': '大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在长时间推理方面表现出色，但它们依赖静态内部知识，限制了在复杂知识密集型任务中的表现。为了解决这个问题，我们提出了WebThinker，一个深度研究代理，能够让LRMs自主搜索网络、浏览网页并在推理过程中撰写研究报告。WebThinker集成了深网探索模块，使LRMs在遇到知识空白时能够动态搜索和提取信息。通过引入基于强化学习的训练策略，我们的实验表明WebThinker在复杂推理基准和科学报告生成任务中显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.21850', 'title': 'COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning', 'url': 'https://huggingface.co/papers/2504.21850', 'abstract': 'Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.', 'score': 20, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '3db97f245360deb4', 'authors': ['Xindi Wu', 'Hee Seung Hwang', 'Polina Kirichenko', 'Olga Russakovsky'], 'affiliations': ['Meta AI', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21850.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#data', '#optimization', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'COMPACT: эффективное обучение MLLM сложным визуально-языковым задачам', 'desc': 'Статья представляет новый метод обучения мультимодальных больших языковых моделей (MLLM) под названием COMPACT. Этот подход генерирует набор данных для обучения, контролируя композиционную сложность примеров. COMPACT позволяет MLLM эффективнее обучаться сложным задачам, комбинируя атомарные возможности. Метод достигает сопоставимых результатов с LLaVA-665k, используя менее 10% данных, и превосходит его на сложных мультизадачных тестах.'}, 'en': {'title': 'Unlocking Complex Tasks with Efficient Compositional Training', 'desc': "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."}, 'zh': {'title': '提升复杂视觉语言任务的能力', 'desc': '多模态大型语言模型（MLLMs）在简单的视觉语言任务中表现出色，但在需要多种能力的复杂任务中却面临挑战。传统的视觉指令调优（VIT）主要关注数据量的扩大，而忽视了训练示例的组合复杂性。我们提出了COMPACT（组合原子到复杂视觉能力调优），它生成一个明确控制训练示例组合复杂性的训练数据集。COMPACT使得MLLMs能够更高效地学习复杂能力，并在多个基准测试中表现出色，尤其是在涉及复杂多能力任务时。'}}}, {'id': 'https://huggingface.co/papers/2504.21233', 'title': 'Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math', 'url': 'https://huggingface.co/papers/2504.21233', 'abstract': 'Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.', 'score': 19, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '0b800a9195884db4', 'authors': ['Haoran Xu', 'Baolin Peng', 'Hany Awadalla', 'Dongdong Chen', 'Yen-Chun Chen', 'Mei Gao', 'Young Jin Kim', 'Yunsheng Li', 'Liliang Ren', 'Yelong Shen', 'Shuohang Wang', 'Weijian Xu', 'Jianfeng Gao', 'Weizhu Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21233.jpg', 'data': {'categories': ['#training', '#rl', '#transfer_learning', '#small_models', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие потенциала малых языковых моделей в задачах рассуждения', 'desc': 'Статья представляет систематический подход к улучшению способностей рассуждения в малых языковых моделях (SLM). Авторы предлагают четырехэтапный рецепт обучения, включающий дистилляцию данных, дообучение на высококачественных данных с цепочками рассуждений, оптимизацию прямого предпочтения и обучение с подкреплением. Применение этого метода к модели Phi-4-Mini (3.8 млрд параметров) позволило превзойти более крупные модели в задачах математических рассуждений. Результаты подтверждают эффективность предложенного подхода для развития сильных способностей рассуждения даже в ресурсно-ограниченных малых моделях.'}, 'en': {'title': 'Unlocking Reasoning Power in Small Models', 'desc': 'This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning.'}, 'zh': {'title': '小模型也能强推理！', 'desc': '本论文探讨了如何通过链式思维（CoT）来提升小型语言模型（SLM）的推理能力。尽管大型语言模型（LLM）在生成中间推理步骤方面表现良好，但小型模型由于容量限制，提升推理能力仍然具有挑战性。研究表明，通过从LLM生成的合成数据进行蒸馏，可以显著改善SLM的推理能力。我们提出了一种系统的训练方案，包括四个步骤，最终在Phi-4-Mini模型上实现了超越更大模型的推理表现。'}}}, {'id': 'https://huggingface.co/papers/2504.20708', 'title': 'Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think', 'url': 'https://huggingface.co/papers/2504.20708', 'abstract': "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.", 'score': 17, 'issue_id': 3532, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'b26e58cf1cee464f', 'authors': ['Hasan Abed Al Kader Hammoud', 'Hani Itani', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2504.20708.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#interpretability', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Подмысли в LLM: путь к повышению точности рассуждений', 'desc': 'Исследование посвящено анализу промежуточных шагов рассуждения (подмыслей) в крупных языковых моделях (LLM) при решении сложных задач. Авторы предлагают метод сегментации цепочки рассуждений на подмысли и генерации продолжений из каждой промежуточной точки. Агрегирование ответов, полученных из разных подмыслей, часто дает более высокую точность по сравнению с использованием только финального ответа. Эксперименты на различных LLM и наборах данных по математическим рассуждениям показали значительное улучшение точности, достигающее 13%.'}, 'en': {'title': 'Unlocking Better Answers Through Subthoughts in LLMs', 'desc': 'This paper investigates the reasoning process of Large Language Models (LLMs) by focusing on intermediate reasoning steps, called subthoughts, rather than just the final answer. It questions whether the final answer is the best conclusion and explores if different reasoning paths can lead to varied results. The authors propose a method that segments reasoning into subthoughts and generates multiple potential answers from these segments, aggregating them to find the most frequent answer for improved accuracy. Their experiments demonstrate that this approach can enhance the accuracy of LLMs by up to 13% on challenging mathematical reasoning tasks.'}, 'zh': {'title': '优化推理路径，提升模型准确性', 'desc': '本文探讨了大型语言模型（LLMs）在解决复杂问题时的推理过程。我们提出了两个问题：最终答案是否可靠地代表模型的最佳结论？不同的推理路径是否会产生不同的结果？为了解答这些问题，我们分析了中间推理步骤，称为子思维，并提出了一种基于这些发现的方法。实验结果表明，通过选择最频繁的答案（众数）来聚合不同子思维的答案，准确性显著提高，最高可达13%。'}}}, {'id': 'https://huggingface.co/papers/2504.21318', 'title': 'Phi-4-reasoning Technical Report', 'url': 'https://huggingface.co/papers/2504.21318', 'abstract': 'We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.', 'score': 14, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '7004ae060f674e9a', 'authors': ['Marah Abdin', 'Sahaj Agarwal', 'Ahmed Awadallah', 'Vidhisha Balachandran', 'Harkirat Behl', 'Lingjiao Chen', 'Gustavo de Rosa', 'Suriya Gunasekar', 'Mojan Javaheripi', 'Neel Joshi', 'Piero Kauffmann', 'Yash Lara', 'Caio César Teodoro Mendes', 'Arindam Mitra', 'Besmira Nushi', 'Dimitris Papailiopoulos', 'Olli Saarikivi', 'Shital Shah', 'Vaishnavi Shrivastava', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi', 'Guoqing Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21318.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#math', '#transfer_learning', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Мощная модель рассуждений на основе тщательно отобранных данных', 'desc': 'Исследователи представили Phi-4-reasoning - модель с 14 миллиардами параметров, обученную для сложных задач рассуждения. Модель была обучена с помощью контролируемой тонкой настройки на тщательно отобранных обучающих примерах и демонстрациях рассуждений. Phi-4-reasoning генерирует подробные цепочки рассуждений, эффективно используя вычислительные ресурсы во время вывода. Модель превосходит по производительности значительно более крупные модели с открытыми весами на различных задачах рассуждения, включая математику, научное мышление, программирование и пространственное понимание.'}, 'en': {'title': 'Unlocking Complex Reasoning with Phi-4-Reasoning', 'desc': 'The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models.'}, 'zh': {'title': '推理模型的新突破：Phi-4-reasoning', 'desc': '本文介绍了Phi-4-reasoning，这是一个拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过对精心挑选的“可教”提示进行监督微调训练，生成详细的推理链，有效利用推理时的计算能力。我们还开发了Phi-4-reasoning-plus，通过基于结果的强化学习进一步增强，能够生成更长的推理轨迹，从而提高性能。综合评估显示，这两个模型在数学、科学推理、编码等多个任务上均优于更大的开放权重模型。'}}}, {'id': 'https://huggingface.co/papers/2504.20966', 'title': 'Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax', 'url': 'https://huggingface.co/papers/2504.20966', 'abstract': 'We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.', 'score': 14, 'issue_id': 3526, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'cb610c1427bdf307', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#architecture', '#inference', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Softpick: улучшенная активация для эффективных трансформеров', 'desc': "Статья представляет softpick - новый метод активации для механизмов внимания в трансформерах, заменяющий softmax. Softpick устраняет проблему 'attention sink' и чрезмерных активаций, сохраняя производительность на уровне softmax. Эксперименты показывают, что softpick создает более разреженные карты внимания и состояния с меньшим эксцессом. Модели с softpick превосходят softmax при квантовании, особенно при низкой битовой точности."}, 'en': {'title': 'Softpick: A Smarter Alternative to Softmax for Transformers', 'desc': 'This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability.'}, 'zh': {'title': 'softpick：提升Transformer注意力的新选择', 'desc': '本文介绍了一种名为softpick的新方法，它可以替代transformer注意力机制中的softmax。softpick不需要将权重归一化为1，能够消除注意力沉没和大规模激活。实验表明，使用softpick的模型在标准基准测试中与softmax表现相当，但注意力沉没率为0%，并且生成的隐藏状态具有更低的峰度。softpick在量化和低精度训练中表现优越，尤其在较低位数精度下具有明显优势，展示了其在稀疏性优化和可解释性方面的潜力。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.19720', 'title': 'Taming the Titans: A Survey of Efficient LLM Inference Serving', 'url': 'https://huggingface.co/papers/2504.19720', 'abstract': 'Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.', 'score': 9, 'issue_id': 3526, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': 'e74f8b7af65e09fd', 'authors': ['Ranran Zhen', 'Juntao Li', 'Yixin Ji', 'Zhenlin Yang', 'Tong Liu', 'Qingrong Xia', 'Xinyu Duan', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang'], 'affiliations': ['Huawei Cloud', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2504.19720.jpg', 'data': {'categories': ['#survey', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение инференса больших языковых моделей: от экземпляра до кластера', 'desc': 'Статья представляет собой обзор методов оптимизации инференса больших языковых моделей (LLM). Авторы рассматривают подходы на уровне отдельных экземпляров, включая размещение модели и управление запросами. Также описываются стратегии на уровне кластеров, такие как балансировка нагрузки и облачные решения. Особое внимание уделяется новым сценариям применения LLM и вспомогательным методам оптимизации.'}, 'en': {'title': 'Optimizing LLMs: Balancing Performance and Efficiency in Generative AI', 'desc': 'This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services.'}, 'zh': {'title': '推动大型语言模型推理服务的研究进展', 'desc': '大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其庞大的参数量和注意力机制的高计算需求导致了内存开销大，影响了推理服务的低延迟和高吞吐量。本文全面调查了应对这些挑战的方法，包括实例级和集群级的策略，以及新兴场景的方向。我们讨论了模型部署、请求调度、解码长度预测等实例级方法，以及GPU集群部署和多实例负载均衡等集群级策略。最后，我们提出了未来研究的潜在方向，以推动LLM推理服务的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.21855', 'title': 'ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction', 'url': 'https://huggingface.co/papers/2504.21855', 'abstract': 'In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '5d8989ce0c77aa23', 'authors': ['Qihao Liu', 'Ju He', 'Qihang Yu', 'Liang-Chieh Chen', 'Alan Yuille'], 'affiliations': ['Independent Researcher', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21855.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games', '#video', '#small_models'], 'emoji': '🎥', 'ru': {'title': 'ReVision: физика в помощь ИИ для создания реалистичных видео', 'desc': 'ReVision - это новый фреймворк для улучшения генерации видео с использованием трехмерных физических знаний. Он состоит из трех этапов: генерация грубого видео, извлечение 2D и 3D признаков для создания объектно-ориентированного представления, и уточнение движения с помощью параметризованной физической модели. ReVision значительно повышает качество генерируемых видео с точки зрения сложных движений и взаимодействий, превосходя даже более крупные модели.'}, 'en': {'title': 'ReVision: Enhancing Video Generation with 3D Physical Knowledge', 'desc': 'The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation.'}, 'zh': {'title': '通过3D物理知识提升视频生成的真实感与可控性', 'desc': '近年来，视频生成技术取得了显著进展，但在生成复杂动作和交互方面仍面临挑战。为了解决这些问题，我们提出了ReVision，这是一个可插拔的框架，能够将参数化的三维物理知识集成到预训练的条件视频生成模型中，从而显著提升生成高质量视频的能力。ReVision包括三个阶段：首先使用视频扩散模型生成粗略视频，然后提取2D和3D特征构建三维物体中心表示，最后通过参数化物理先验模型精炼运动序列，反馈到视频扩散模型中以生成一致的运动视频。我们的实验表明，ReVision在复杂视频生成上表现优异，甚至以较少的参数超越了大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.19056', 'title': 'Generative AI for Character Animation: A Comprehensive Survey of\n  Techniques, Applications, and Future Directions', 'url': 'https://huggingface.co/papers/2504.19056', 'abstract': 'Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.', 'score': 7, 'issue_id': 3535, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': 'deba947a8e69f6ee', 'authors': ['Mohammad Mahdi Abootorabi', 'Omid Ghahroodi', 'Pardis Sadat Zahraei', 'Hossein Behzadasl', 'Alireza Mirrokni', 'Mobina Salimipanah', 'Arash Rasouli', 'Bahar Behzadipour', 'Sara Azarnoush', 'Benyamin Maleki', 'Erfan Sadraiye', 'Kiarash Kiani Feriz', 'Mahdi Teymouri Nahad', 'Ali Moghadasi', 'Abolfazl Eshagh Abianeh', 'Nizi Nazar', 'Hamid R. Rabiee', 'Mahdieh Soleymani Baghshah', 'Meisam Ahmadi', 'Ehsaneddin Asgari'], 'affiliations': ['Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Iran University of Science and Technology', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2504.19056.jpg', 'data': {'categories': ['#survey', '#diffusion', '#cv', '#games', '#multimodal', '#video'], 'emoji': '🤖', 'ru': {'title': 'Генеративный ИИ революционизирует анимацию персонажей', 'desc': 'Это обзорная статья о применении генеративного искусственного интеллекта в анимации персонажей. Авторы рассматривают последние достижения в области лицевой анимации, синтеза движений, создания аватаров и других аспектов. В статье представлен комплексный обзор всех основных приложений генеративного ИИ для анимации персонажей, включая ведущие исследования, практические внедрения и используемые наборы данных. Также приводится справочная информация о фундаментальных моделях и метриках оценки для новичков в этой области.'}, 'en': {'title': 'Revolutionizing Character Animation with Generative AI', 'desc': 'This paper reviews the recent advancements in generative AI technologies specifically for character animation, which includes facial animation, gesture modeling, and motion synthesis. It highlights how foundation and diffusion models have significantly lowered the costs and time required for creating animated content. The survey provides a comprehensive overview of the state-of-the-art techniques, practical applications, and datasets used in the field, making it a valuable resource for newcomers. Additionally, it discusses ongoing challenges and suggests future research directions to enhance AI-driven character animation.'}, 'zh': {'title': '生成性AI：重塑角色动画的未来', 'desc': '这篇论文探讨了生成性人工智能在角色动画领域的应用，特别是在面部动画、表情渲染和动作合成等方面的最新进展。通过整合不同的生成模型和扩散模型，研究者们显著降低了动画内容的制作时间和成本。论文还提供了对当前研究、实际应用、常用数据集和新兴趋势的全面回顾，帮助新手了解基础模型和评估指标。最后，作者讨论了该领域面临的挑战，并为未来的研究方向提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2504.18904', 'title': 'RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning', 'url': 'https://huggingface.co/papers/2504.18904', 'abstract': 'Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': 'c724dcb5ceb5df7b', 'authors': ['Haoran Geng', 'Feishi Wang', 'Songlin Wei', 'Yuyang Li', 'Bangjun Wang', 'Boshi An', 'Charlie Tianyue Cheng', 'Haozhe Lou', 'Peihao Li', 'Yen-Jen Wang', 'Yutong Liang', 'Dylan Goetting', 'Chaoyi Xu', 'Haozhe Chen', 'Yuxi Qian', 'Yiran Geng', 'Jiageng Mao', 'Weikang Wan', 'Mingtong Zhang', 'Jiangran Lyu', 'Siheng Zhao', 'Jiazhao Zhang', 'Jialiang Zhang', 'Chengyang Zhao', 'Haoran Lu', 'Yufei Ding', 'Ran Gong', 'Yuran Wang', 'Yuxuan Kuang', 'Ruihai Wu', 'Baoxiong Jia', 'Carlo Sferrazza', 'Hao Dong', 'Siyuan Huang', 'Yue Wang', 'Jitendra Malik', 'Pieter Abbeel'], 'affiliations': ['BIGAI', 'CMU', 'PKU', 'Stanford', 'UC Berkeley', 'UCLA', 'UIUC', 'UMich', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2504.18904.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#synthetic', '#optimization', '#transfer_learning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboVerse: универсальная платформа для масштабирования и стандартизации обучения роботов', 'desc': 'RoboVerse - это комплексная платформа для робототехники, включающая симуляционную среду, синтетический набор данных и унифицированные бенчмарки. Платформа поддерживает множество симуляторов и роботизированных воплощений, обеспечивая плавный переход между различными средами. Синтетический набор данных отличается высокой точностью физики и фотореалистичным рендерингом. RoboVerse предлагает унифицированные бенчмарки для имитационного обучения и обучения с подкреплением, позволяющие оценивать различные уровни обобщения.'}, 'en': {'title': 'RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks', 'desc': 'This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field.'}, 'zh': {'title': 'RoboVerse：推动机器人学习的强大框架', 'desc': '本论文介绍了RoboVerse，这是一个综合框架，旨在解决机器人领域数据收集和评估的挑战。RoboVerse包括一个模拟平台、一个合成数据集和统一的基准测试，支持多种模拟器和机器人形态。通过高保真物理和逼真的渲染，合成数据集提供了高质量和多样性的数据。实验结果表明，RoboVerse显著提升了模仿学习、强化学习和从模拟到现实的转移性能，验证了其数据集和基准的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2504.21039', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report', 'url': 'https://huggingface.co/papers/2504.21039', 'abstract': 'As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.', 'score': 5, 'issue_id': 3528, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': 'a66fbdd0c4cc7250', 'authors': ['Paul Kassianik', 'Baturay Saglam', 'Alexander Chen', 'Blaine Nelson', 'Anu Vellore', 'Massimo Aufiero', 'Fraser Burch', 'Dhruv Kedia', 'Avi Zohary', 'Sajana Weerawardhena', 'Aman Priyanshu', 'Adam Swanda', 'Amy Chang', 'Hyrum Anderson', 'Kojin Oshiba', 'Omar Santos', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Foundation AI Cisco Systems Inc.', 'Security & Trust Organization Cisco Systems Inc.', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21039.jpg', 'data': {'categories': ['#architecture', '#data', '#open_source', '#training', '#dataset', '#benchmark', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Специализированная языковая модель открывает новые горизонты в кибербезопасности', 'desc': 'Статья представляет Foundation-Sec-8B - языковую модель, специализированную на кибербезопасности. Модель основана на архитектуре Llama 3.1 и дообучена на специально подобранном корпусе текстов по кибербезопасности. Foundation-Sec-8B показывает результаты, сравнимые с Llama 3.1-70B и GPT-4o-mini в некоторых задачах кибербезопасности. Авторы публикуют модель, чтобы ускорить развитие и внедрение ИИ-инструментов в сфере кибербезопасности.'}, 'en': {'title': 'Empowering Cybersecurity with Foundation-Sec-8B', 'desc': 'This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors.'}, 'zh': {'title': '推动网络安全的AI工具进步', 'desc': '随着基于变换器的大型语言模型（LLMs）在社会中的广泛应用，它们在软件工程、创意写作和数字艺术等领域带来了革命性的变化。然而，由于缺乏专业的训练数据和表示网络安全特定知识的复杂性，它们在网络安全领域的应用仍然有限。为了解决这些问题，我们提出了Foundation-Sec-8B，这是一个专注于网络安全的LLM，基于Llama 3.1架构，并通过在精心策划的网络安全语料库上进行持续预训练来增强。我们在多个网络安全基准测试中评估了Foundation-Sec-8B，结果显示它在某些网络安全特定任务上与Llama 3.1-70B和GPT-4o-mini相匹配。'}}}, {'id': 'https://huggingface.co/papers/2504.21336', 'title': 'UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation', 'url': 'https://huggingface.co/papers/2504.21336', 'abstract': 'Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.', 'score': 2, 'issue_id': 3532, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'b4a0872fb3eb8547', 'authors': ['Linshan Wu', 'Yuxiang Nie', 'Sunan He', 'Jiaxin Zhuang', 'Hao Chen'], 'affiliations': ['Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China', 'Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China', 'State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21336.jpg', 'data': {'categories': ['#optimization', '#cv', '#science', '#multimodal', '#dataset', '#healthcare', '#interpretability', '#agi'], 'emoji': '🧬', 'ru': {'title': 'UniBiomed: революция в анализе биомедицинских изображений с помощью ИИ', 'desc': 'UniBiomed - это универсальная модель для интерпретации биомедицинских изображений, объединяющая мультимодальную языковую модель и модель сегментации. Она обучена на 27 миллионах триплетов изображений, аннотаций и текстовых описаний из 10 различных модальностей медицинской визуализации. UniBiomed демонстрирует передовые результаты в задачах сегментации, распознавания заболеваний, диагностики с привязкой к регионам, визуальных вопросов-ответов и генерации отчетов. Модель обеспечивает автоматизированную интерпретацию биомедицинских изображений без необходимости в предварительной диагностике экспертами.'}, 'en': {'title': 'UniBiomed: Revolutionizing Biomedical Image Interpretation', 'desc': "This paper presents UniBiomed, a groundbreaking universal foundation model designed for interpreting biomedical images by integrating Multi-modal Large Language Models (MLLM) and segmentation techniques. Unlike traditional AI methods that treat text generation and image segmentation separately, UniBiomed combines these processes to provide a cohesive understanding of biomedical data. It utilizes a large-scale dataset of over 27 million image-text pairs across various imaging modalities, enabling it to perform multiple tasks such as segmentation, disease recognition, and report generation. The model's ability to automate grounded interpretation marks a significant advancement in clinical workflows, enhancing diagnostic efficiency and accuracy."}, 'zh': {'title': 'UniBiomed：生物医学图像分析的新突破', 'desc': '多模态生物医学图像的解释为生物医学图像分析开辟了新的机会。传统的人工智能方法通常依赖于分离的训练，导致在实际应用中缺乏灵活性，无法充分利用整体生物医学信息。为此，我们提出了UniBiomed，这是首个用于生物医学图像解释的通用基础模型，结合了多模态大语言模型和分割模型，能够统一生成临床文本和相应生物医学对象的分割。UniBiomed在多个生物医学任务中表现出色，显著提高了诊断效率。'}}}, {'id': 'https://huggingface.co/papers/2504.19043', 'title': 'Selecting Optimal Candidate Profiles in Adversarial Environments Using\n  Conjoint Analysis and Machine Learning', 'url': 'https://huggingface.co/papers/2504.19043', 'abstract': 'Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.', 'score': 2, 'issue_id': 3538, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': 'c7a7b2771c1e5c1f', 'authors': ['Connor T. Jerzak', 'Priyanshi Chandra', 'Rishi Hazra'], 'affiliations': ['Department of Government, University of Texas at Austin', 'Department of Statistics, Harvard College', 'Faculty of Informatics, Università della Svizzera Italiana'], 'pdf_title_img': 'assets/pdf/title_img/2504.19043.jpg', 'data': {'categories': [], 'emoji': '🗳️', 'ru': {'title': 'Оптимизация профилей политических кандидатов с помощью машинного обучения', 'desc': 'Статья рассматривает применение совместного анализа в политических исследованиях для оптимизации профилей кандидатов. Авторы предлагают метод стохастической интервенции для определения оптимального распределения атрибутов кандидата в условиях ограниченных данных. Исследование учитывает как одностороннюю, так и состязательную оптимизацию между двумя партиями. Результаты показывают, что состязательный подход дает более реалистичные прогнозы и лучше соответствует наблюдаемым кандидатам по сравнению с несостязательным методом.'}, 'en': {'title': 'Optimizing Political Candidate Selection through Adversarial Conjoint Analysis', 'desc': 'This paper explores the use of conjoint analysis in political candidate selection, focusing on how to identify optimal candidate profiles. It highlights the challenge of having too many possible candidate features compared to the limited number of observations in typical experiments. To solve this, the authors propose a stochastic intervention that generates a probability distribution of candidate attributes to maximize favorable outcomes. The study shows that considering adversarial dynamics between political parties leads to more accurate predictions of candidate success compared to traditional non-adversarial methods.'}, 'zh': {'title': '对抗性动态提升联合分析的洞察力', 'desc': '本文探讨了联合分析在政治候选人选择中的应用，特别是如何识别最佳候选人特征组合。由于特征组合的数量远超观察样本，无法精确确定最佳配置。为了解决这一问题，作者提出了一种最优随机干预方法，旨在通过概率分布实现最有利的平均结果。研究表明，在对抗性环境中，所建议的策略更可能与实际候选人匹配，揭示了对抗动态在社会科学实验数据分析中的重要性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi (2)', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (13)', '#cv (5)', '#data (3)', '#dataset (9)', '#diffusion (2)', '#ethics (1)', '#games (4)', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (3)', '#interpretability (5)', '#leakage (1)', '#long_context', '#low_resource (1)', '#machine_translation (1)', '#math (4)', '#multilingual (1)', '#multimodal (5)', '#open_source (3)', '#optimization (12)', '#plp', '#rag', '#reasoning (8)', '#rl (6)', '#rlhf (2)', '#robotics (2)', '#science (2)', '#security (1)', '#small_models (2)', '#story_generation (1)', '#survey (3)', '#synthetic (2)', '#training (10)', '#transfer_learning (4)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-02 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-02 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-02 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    