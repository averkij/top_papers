
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 401 papers. May 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Май 2025</span> | <span id="title-articles-count">401 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-04.html">⬅️ <span id="prev-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-06.html">➡️ <span id="next-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Май 2025', 'en': 'May 2025', 'zh': '5月2025年'};
        let feedDateNext = {'ru': '06.2025', 'en': '06/2025', 'zh': '6月2025年'};
        let feedDatePrev = {'ru': '04.2025', 'en': '04/2025', 'zh': '4月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.15277', 'title': 'Web-Shepherd: Advancing PRMs for Reinforcing Web Agents', 'url': 'https://huggingface.co/papers/2505.15277', 'abstract': 'Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.', 'score': 78, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '8209bc2f2e5e6119', 'authors': ['Hyungjoo Chae', 'Sunghwan Kim', 'Junhee Cho', 'Seungone Kim', 'Seungjun Moon', 'Gyeom Hwangbo', 'Dongha Lim', 'Minjin Kim', 'Yeonjun Hwang', 'Minju Gwak', 'Dongwook Choi', 'Minseok Kang', 'Gwanhoon Im', 'ByeongUng Cho', 'Hyojun Kim', 'Jun Hee Han', 'Taeyoon Kwon', 'Minju Kim', 'Beong-woo Kwak', 'Dongjin Kang', 'Jinyoung Yeo'], 'affiliations': ['Carnegie Mellon University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15277.jpg', 'data': {'categories': ['#optimization', '#open_source', '#survey', '#benchmark', '#rl', '#dataset'], 'emoji': '🧭', 'ru': {'title': 'Web-Shepherd: эффективная модель вознаграждения для автоматизации веб-навигации', 'desc': 'Статья представляет Web-Shepherd - первую модель вознаграждения процесса (PRM) для веб-навигации. Авторы создали набор данных WebPRM Collection с 40 тысячами пар предпочтений на уровне шагов и аннотированными чек-листами. Они также разработали бенчмарк WebRewardBench для оценки PRM моделей. Web-Shepherd показывает значительно лучшую точность по сравнению с GPT-4o и позволяет достичь более высокой производительности при меньших затратах в задачах веб-навигации.'}, 'en': {'title': 'Web-Shepherd: Revolutionizing Web Navigation with Process Reward Models', 'desc': 'This paper introduces Web-Shepherd, a novel process reward model (PRM) designed specifically for web navigation tasks that require long-term decision making. The authors highlight the limitations of using multimodal large language models (MLLMs) as reward models, which can hinder practical applications due to speed and cost issues. They present the WebPRM Collection, a comprehensive dataset containing 40,000 step-level preference pairs to train the PRM effectively. Experimental results demonstrate that Web-Shepherd significantly outperforms existing models, achieving higher accuracy and lower costs in web navigation tasks.'}, 'zh': {'title': '网页导航的智能评估新方法', 'desc': '本论文提出了一种新的过程奖励模型（PRM），名为Web-Shepherd，旨在评估网页导航的决策过程。我们构建了一个名为WebPRM Collection的大规模数据集，包含4万对步骤级偏好对和注释清单，涵盖多种领域和难度级别。通过实验，我们发现Web-Shepherd在WebRewardBench上比使用GPT-4o的准确率提高了约30个百分点，并且在WebArena-lite测试中，使用Web-Shepherd作为验证器时，性能提升了10.9点，且成本降低了10。我们的模型、数据集和代码均已公开。'}}}, {'id': 'https://huggingface.co/papers/2505.14302', 'title': 'Scaling Law for Quantization-Aware Training', 'url': 'https://huggingface.co/papers/2505.14302', 'abstract': 'Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.', 'score': 55, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'ecea48e2af693a28', 'authors': ['Mengzhao Chen', 'Chaoyi Zhang', 'Jing Liu', 'Yutao Zeng', 'Zeyue Xue', 'Zhiheng Liu', 'Yunshui Li', 'Jin Ma', 'Jie Huang', 'Xun Zhou', 'Ping Luo'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.14302.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🔬', 'ru': {'title': 'Масштабирование квантизации в больших языковых моделях: новые горизонты эффективности', 'desc': 'Статья исследует масштабируемость квантизации с учетом обучения (QAT) для больших языковых моделей (LLM). Авторы предлагают единый закон масштабирования для QAT, моделирующий ошибку квантизации как функцию размера модели, объема обучающих данных и размера группы квантизации. Исследование показывает, что ошибка квантизации уменьшается с увеличением размера модели, но растет с увеличением количества обучающих токенов и более грубой грануляцией квантизации. Анализ выявляет, что ошибка квантизации активаций в слое FC2 является основным узким местом в 4-битной QAT.'}, 'en': {'title': 'Optimizing Quantization-Aware Training for Large Language Models', 'desc': 'This paper investigates the challenges of deploying large language models (LLMs) due to their high computational and memory requirements. It introduces quantization-aware training (QAT) as a solution to reduce model precision while preserving performance, particularly focusing on 4-bit precision (W4A4). The authors propose a new scaling law for QAT that considers factors like model size, training data volume, and quantization granularity, revealing how quantization error behaves under these conditions. Their experiments show that while increasing model size reduces quantization error, more training tokens and coarser quantization lead to higher errors, highlighting the need for mixed-precision quantization to optimize performance.'}, 'zh': {'title': '量化感知训练的统一扩展法则', 'desc': '大型语言模型（LLMs）需要大量的计算和内存资源，这给部署带来了挑战。量化感知训练（QAT）通过降低模型精度来应对这些挑战，同时保持性能。然而，QAT在4位精度（W4A4）下的扩展行为尚不清楚。本文提出了一种统一的QAT扩展法则，模型化量化误差与模型大小、训练数据量和量化组大小之间的关系。'}}}, {'id': 'https://huggingface.co/papers/2505.15809', 'title': 'MMaDA: Multimodal Large Diffusion Language Models', 'url': 'https://huggingface.co/papers/2505.15809', 'abstract': "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA", 'score': 53, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'ff99ceb93709180d', 'authors': ['Ling Yang', 'Ye Tian', 'Bowen Li', 'Xinchen Zhang', 'Ke Shen', 'Yunhai Tong', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Peking University', 'Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15809.jpg', 'data': {'categories': ['#training', '#multimodal', '#open_source', '#architecture', '#reasoning', '#diffusion', '#rl'], 'emoji': '🧠', 'ru': {'title': 'MMaDA: Унифицированная мультимодальная диффузионная модель для рассуждений и генерации', 'desc': 'MMaDA - это новый класс мультимодальных диффузионных фундаментальных моделей, разработанный для достижения превосходной производительности в различных областях, включая текстовые рассуждения, мультимодальное понимание и генерацию изображений по тексту. Модель использует унифицированную диффузионную архитектуру с общей вероятностной формулировкой и модально-агностическим дизайном, устраняя необходимость в компонентах, специфичных для конкретных модальностей. MMaDA применяет смешанную стратегию дообучения с длинной цепочкой рассуждений (CoT) и унифицированный алгоритм обучения с подкреплением UniGRPO, специально разработанный для диффузионных фундаментальных моделей. Экспериментальные результаты показывают, что MMaDA-8B демонстрирует сильные возможности обобщения как унифицированная мультимодальная фундаментальная модель, превосходя мощные модели в различных задачах.'}, 'en': {'title': 'MMaDA: Unifying Multimodal Learning for Superior Performance', 'desc': 'MMaDA is a new type of multimodal diffusion model that excels in various tasks like understanding text and images, and generating images from text. It features a unified architecture that processes different data types without needing separate components for each type. The model uses a special training method that aligns reasoning across text and visuals, making it easier to learn complex tasks. Additionally, it includes a unique reinforcement learning algorithm that improves performance in both reasoning and generation tasks, showing strong results compared to other leading models.'}, 'zh': {'title': 'MMaDA：多模态扩散模型的创新之路', 'desc': 'MMaDA是一种新型的多模态扩散基础模型，旨在在文本推理、多模态理解和文本到图像生成等多个领域实现卓越性能。该方法的三个关键创新包括：首先，MMaDA采用统一的扩散架构，消除了对特定模态组件的需求，从而实现不同数据类型的无缝集成和处理。其次，实施混合的长链思维（CoT）微调策略，统一不同模态的推理过程，增强模型处理复杂任务的能力。最后，提出了UniGRPO，这是一种专门为扩散基础模型设计的统一策略梯度强化学习算法，确保在推理和生成任务中实现一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2505.14231', 'title': 'UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.14231', 'abstract': 'Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.', 'score': 42, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'cd98eb52e95427f4', 'authors': ['Sule Bai', 'Mingxing Li', 'Yong Liu', 'Jing Tang', 'Haoji Zhang', 'Lei Sun', 'Xiangxiang Chu', 'Yansong Tang'], 'affiliations': ['AMAP, Alibaba Group', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14231.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#rl', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Улучшение визуальной привязки через обучение с подкреплением и рассуждения', 'desc': 'Статья представляет UniVG-R1 - мультимодальную большую языковую модель для универсальной визуальной привязки, улучшенную с помощью обучения с подкреплением. Модель обучается на наборе данных с аннотированными цепочками рассуждений, что позволяет ей следовать правильным путям рассуждений. Авторы применяют обучение с подкреплением на основе правил и стратегию корректировки весов с учетом сложности для повышения производительности. UniVG-R1 достигает лучших результатов на бенчмарке MIG-Bench и демонстрирует сильную обобщающую способность на других задачах визуального обоснования.'}, 'en': {'title': 'Enhancing Visual Grounding with Advanced Reasoning', 'desc': 'This paper introduces UniVG-R1, a multimodal large language model designed for universal visual grounding, which is the task of linking images to complex textual instructions. The model enhances its reasoning abilities through a combination of supervised fine-tuning on a newly created Chain-of-Thought dataset and reinforcement learning techniques. To address challenges in training, the authors implement a difficulty-aware weight adjustment strategy that helps the model focus on more complex reasoning tasks as it learns. Experimental results show that UniVG-R1 outperforms previous methods, demonstrating significant improvements in both general performance and zero-shot capabilities across various benchmarks.'}, 'zh': {'title': '提升视觉定位的推理能力', 'desc': '传统的视觉定位方法主要集中在单图像场景和简单文本引用上。然而，将这些方法扩展到涉及隐含和复杂指令的真实场景，尤其是多图像的情况下，面临着重大挑战，主要是由于缺乏在多模态上下文中进行高级推理的能力。本文提出了UniVG-R1，这是一种基于推理的多模态大型语言模型，通过结合强化学习和冷启动数据来增强推理能力。实验结果表明，UniVG-R1在MIG-Bench上实现了最先进的性能，相较于之前的方法提高了9.1%。'}}}, {'id': 'https://huggingface.co/papers/2505.15045', 'title': 'Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective', 'url': 'https://huggingface.co/papers/2505.15045', 'abstract': 'Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text.', 'score': 37, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '57c6eb57eddeebcc', 'authors': ['Siyue Zhang', 'Yilun Zhao', 'Liyuan Geng', 'Arman Cohan', 'Anh Tuan Luu', 'Chen Zhao'], 'affiliations': ['Alibaba-NTU Singapore Joint Research Institute', 'Center for Data Science, New York University', 'NYU Shanghai', 'Nanyang Technological University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15045.jpg', 'data': {'categories': ['#diffusion', '#reasoning', '#benchmark', '#training', '#long_context', '#architecture', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Диффузионные модели: новый подход к текстовым эмбеддингам', 'desc': 'В этой статье представлено исследование применения диффузионных языковых моделей для создания текстовых эмбеддингов. Авторы предлагают использовать двунаправленную архитектуру диффузионных моделей для преодоления ограничений однонаправленного внимания в автореггрессивных LLM. Результаты показывают значительное улучшение производительности по сравнению с эмбеддингами на основе LLM, особенно в задачах поиска длинных документов и рассуждений. Исследование подтверждает важность двунаправленного внимания для кодирования глобального контекста в длинных и сложных текстах.'}, 'en': {'title': 'Harnessing Bidirectional Attention for Superior Text Embeddings', 'desc': 'This paper discusses the limitations of large language model (LLM) embeddings, particularly their unidirectional attention which does not align well with the needs of text embedding tasks. The authors propose using diffusion language models, which have a bidirectional architecture, to improve text embeddings. Their systematic study shows that these diffusion models outperform LLM-based embeddings in various retrieval tasks, especially in long-document and reasoning-intensive scenarios. The findings highlight the importance of bidirectional attention for capturing the global context in complex texts.'}, 'zh': {'title': '扩散语言模型：双向嵌入的未来', 'desc': '基于大型语言模型（LLM）的嵌入模型在文档检索等通用文本嵌入任务中表现优于BERT和T5模型。然而，LLM嵌入的一个基本限制是其在自回归预训练中使用的单向注意力，这与文本嵌入任务的双向特性不符。为此，我们提出采用扩散语言模型进行文本嵌入，因其固有的双向架构在推理任务中表现出色。我们的研究表明，扩散语言嵌入模型在长文档检索等任务中优于LLM嵌入模型，验证了双向注意力在编码长文本全局上下文中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.13909', 'title': 'Efficient Agent Training for Computer Use', 'url': 'https://huggingface.co/papers/2505.13909', 'abstract': 'Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.', 'score': 31, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '6614c6f1cb4338a5', 'authors': ['Yanheng He', 'Jiahe Jin', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13909.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#transfer_learning', '#training', '#dataset', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'Эффективное обучение компьютерных агентов на малых данных', 'desc': 'Статья представляет PC Agent-E - эффективную систему обучения агентов для использования компьютера. Авторы разработали метод, позволяющий значительно сократить потребность в большом количестве демонстраций от людей. Используя всего 312 аннотированных человеком траекторий и синтезируя разнообразные решения с помощью языковой модели Claude 3.7 Sonnet, им удалось создать агента, превосходящего базовую модель на 141%. PC Agent-E также демонстрирует хорошую обобщаемость на различные операционные системы.'}, 'en': {'title': 'Empowering Agents with Minimal Data: The PC Agent-E Revolution', 'desc': 'The paper presents PC Agent-E, a new framework for training computer use agents that minimizes the need for extensive human demonstrations. By starting with only 312 human-annotated trajectories, the authors enhanced the data quality through the synthesis of diverse action decisions using Claude 3.7 Sonnet. This approach led to a significant performance boost, with the PC Agent-E model achieving a 141% relative improvement over previous models on the WindowsAgentArena-V2 benchmark. Additionally, the model shows strong adaptability across different operating systems, indicating that effective computer use skills can be developed from a limited set of high-quality data.'}, 'zh': {'title': '少量高质量数据，激发强大计算机能力', 'desc': 'PC Agent-E 是一个高效的代理训练框架，旨在减少对大规模人类示范的依赖。我们从仅有的312个人工标注的计算机使用轨迹开始，通过合成多样的行动决策来提高数据质量。经过这些丰富轨迹的训练，PC Agent-E 模型在 WindowsAgentArena-V2 基准测试中实现了141%的相对提升，超越了强大的 Claude 3.7 Sonnet。我们的研究表明，少量高质量的轨迹数据可以激发出强大的计算机使用能力。'}}}, {'id': 'https://huggingface.co/papers/2505.14766', 'title': 'This Time is Different: An Observability Perspective on Time Series\n  Foundation Models', 'url': 'https://huggingface.co/papers/2505.14766', 'abstract': "We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10times larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.", 'score': 27, 'issue_id': 3900, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'c77892c45c8f389d', 'authors': ['Ben Cohen', 'Emaad Khwaja', 'Youssef Doubli', 'Salahidine Lemaachi', 'Chris Lettieri', 'Charles Masson', 'Hugo Miccinilli', 'Elise Ramé', 'Qiqi Ren', 'Afshin Rostamizadeh', 'Jean Ogier du Terrail', 'Anna-Monica Toon', 'Kan Wang', 'Stephan Xie', 'David Asker', 'Ameet Talwalkar', 'Othmane Abou-Amal'], 'affiliations': ['datadoghq.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.14766.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#small_models', '#synthetic', '#architecture'], 'emoji': '📈', 'ru': {'title': 'Toto: революция в прогнозировании временных рядов', 'desc': 'Представлена модель Toto - фундаментальная модель прогнозирования временных рядов с 151 миллионом параметров. Toto использует современную архитектуру декодера с инновациями для решения проблем многомерных данных наблюдаемости. Модель обучена на корпусе данных наблюдаемости, открытых датасетов и синтетических данных, который в 4-10 раз больше, чем у ведущих моделей временных рядов. Toto достигает передовых результатов как на новом бенчмарке BOOM, так и на существующих бенчмарках прогнозирования временных рядов.'}, 'en': {'title': 'Toto: Revolutionizing Time Series Forecasting with State-of-the-Art Performance', 'desc': 'Toto is a new foundation model designed for time series forecasting, featuring 151 million parameters and a decoder-only architecture. It addresses challenges in multivariate observability time series data through innovative design choices. The model is trained on a diverse dataset that is significantly larger than those used by existing models, combining real and synthetic observability data. Evaluations show that Toto outperforms other models on both a new benchmark called BOOM and established forecasting tasks, with all resources made available as open source.'}, 'zh': {'title': 'Toto：时间序列预测的新基准', 'desc': '本文介绍了Toto，一个具有1.51亿参数的时间序列预测基础模型。Toto采用现代的解码器架构，并针对多变量可观测时间序列数据的特定挑战进行了架构创新。其预训练数据集由可观测数据、开放数据集和合成数据混合而成，规模是领先时间序列基础模型的4到10倍。此外，本文还介绍了BOOM，一个包含2,807个真实世界时间序列的350百万观测值的大规模基准。'}}}, {'id': 'https://huggingface.co/papers/2505.15612', 'title': 'Learn to Reason Efficiently with Adaptive Length-based Reward Shaping', 'url': 'https://huggingface.co/papers/2505.15612', 'abstract': 'Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant "self-reflections". Resources are at https://github.com/hkust-nlp/Laser.', 'score': 23, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '145c2f1bfcaa0c1e', 'authors': ['Wei Liu', 'Ruochen Zhou', 'Yiyun Deng', 'Yuzhen Huang', 'Junteng Liu', 'Yuntian Deng', 'Yizhe Zhang', 'Junxian He'], 'affiliations': ['Apple', 'City University of Hong Kong', 'The Hong Kong University of Science and Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.15612.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization'], 'emoji': '💡', 'ru': {'title': 'Эффективное рассуждение: оптимизация цепочек мыслей в крупных языковых моделях', 'desc': 'Статья представляет новый метод LASER для повышения эффективности рассуждений в крупных моделях машинного обучения. LASER использует функцию вознаграждения на основе длины для оптимизации баланса между производительностью и эффективностью. Авторы также предлагают улучшенную версию LASER-D, которая адаптивно настраивает вознаграждение в зависимости от сложности задачи. Эксперименты показывают значительное улучшение эффективности рассуждений и сокращение использования токенов в различных моделях.'}, 'en': {'title': 'Enhancing Reasoning Efficiency with Dynamic Length-Based Rewards', 'desc': 'This paper explores how Large Reasoning Models (LRMs) can improve their problem-solving efficiency using reinforcement learning (RL). It introduces a new method called Length-bAsed StEp Reward shaping (LASER), which optimizes reasoning outputs by shaping rewards based on the length of reasoning traces. LASER-D, an extension of LASER, adapts the reward system to be dynamic and difficulty-aware, penalizing longer reasoning for easier queries. The results show that this approach significantly enhances reasoning performance while reducing redundancy and token usage in outputs.'}, 'zh': {'title': '提升推理效率的动态奖励机制', 'desc': '大型推理模型（LRMs）在通过强化学习（RL）解决复杂问题方面表现出色，尤其是在生成长推理链时。然而，这些冗长的输出往往存在显著的冗余，限制了LRMs的效率。本文探讨了基于RL的方法以提高推理效率，提出了一种新的长度基础步骤奖励塑形方法（LASER），通过目标长度控制奖励，超越了之前的方法，实现了性能与效率的优越平衡。此外，我们还提出了动态和难度感知的LASER-D方法，以适应模型在训练过程中的推理行为变化。'}}}, {'id': 'https://huggingface.co/papers/2505.15400', 'title': 'When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning', 'url': 'https://huggingface.co/papers/2505.15400', 'abstract': 'Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery Mechanism" where models implicitly supplement reasoning during answer generation. Building on this insight, we propose Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables implicit recovery. By introducing accuracy-aware length reward regulation, ASRR adaptively allocates reasoning effort according to problem difficulty, achieving high efficiency with negligible performance sacrifice. Experiments across multiple benchmarks and models show that, compared with GRPO, ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates on safety benchmarks (up to +21.7%). Our results highlight the potential of ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.', 'score': 17, 'issue_id': 3896, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '93ea3ac7fa19dc07', 'authors': ['Xiaoyun Zhang', 'Jingqing Ruan', 'Xing Ma', 'Yawen Zhu', 'Haodong Zhao', 'Hao Li', 'Jiansong Chen', 'Ke Zeng', 'Xunliang Cai'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2505.15400.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение: меньше думай, больше делай', 'desc': "Эта статья представляет новый подход к оптимизации работы больших моделей рассуждений (LRM). Авторы предлагают метод Адаптивного Самовосстанавливающегося Рассуждения (ASRR), который уменьшает избыточные вычисления при сохранении высокой точности. ASRR основан на обнаруженном феномене 'Внутреннего механизма самовосстановления', когда модели неявно дополняют рассуждения во время генерации ответа. Эксперименты показывают, что ASRR значительно сокращает вычислительные затраты и повышает безопасность моделей на различных тестах."}, 'en': {'title': 'Efficient Reasoning with Adaptive Self-Recovery', 'desc': "This paper discusses the challenges of large reasoning models (LRMs) that often perform redundant reasoning, leading to high computational costs, especially on simpler tasks. The authors introduce a concept called the 'Internal Self-Recovery Mechanism', which allows models to enhance their reasoning during answer generation without additional effort. They propose a new framework called Adaptive Self-Recovery Reasoning (ASRR) that minimizes unnecessary reasoning while still allowing for implicit recovery based on the task's complexity. Experimental results demonstrate that ASRR can significantly reduce the reasoning budget while maintaining accuracy and improving safety metrics across various benchmarks."}, 'zh': {'title': '自适应推理，提升效率与安全性', 'desc': '大型推理模型（LRMs）在长推理链上表现出色，但在简单任务中常常导致过高的计算开销。本文系统量化了LRMs在长思考和无思考模式下的上限，并揭示了模型在生成答案时隐式补充推理的“内部自我恢复机制”。基于这一发现，我们提出了自适应自我恢复推理（ASRR）框架，能够抑制不必要的推理并实现隐式恢复。实验结果表明，ASRR在多个基准测试中显著提高了推理效率，同时保持了较小的准确性损失。'}}}, {'id': 'https://huggingface.co/papers/2505.14357', 'title': 'Vid2World: Crafting Video Diffusion Models to Interactive World Models', 'url': 'https://huggingface.co/papers/2505.14357', 'abstract': 'World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models.', 'score': 17, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'de65ad0f3c9cf5c2', 'authors': ['Siqiao Huang', 'Jialong Wu', 'Qixing Zhou', 'Shangchen Miao', 'Mingsheng Long'], 'affiliations': ['Chongqing University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14357.jpg', 'data': {'categories': ['#architecture', '#games', '#robotics', '#diffusion', '#transfer_learning', '#rl', '#agents', '#video'], 'emoji': '🎥', 'ru': {'title': 'Превращение моделей диффузии видео в интерактивные модели мира', 'desc': 'В статье представлен метод Vid2World, позволяющий использовать предобученные модели диффузии видео в качестве интерактивных моделей мира. Авторы предлагают каузализацию архитектуры и целевой функции модели диффузии для обеспечения авторегрессивной генерации. Также вводится механизм каузального управления действиями для улучшения контролируемости в получаемой интерактивной модели мира. Эксперименты в области робототехники и игровых симуляций показывают эффективность подхода для адаптации мощных моделей диффузии видео к задачам интерактивного моделирования мира.'}, 'en': {'title': 'Transforming Video Models into Interactive World Models', 'desc': 'This paper introduces Vid2World, a novel method that enhances world models by utilizing pre-trained video diffusion models. Traditional world models struggle with low-quality predictions and require extensive training, limiting their use in complex scenarios. Vid2World addresses these issues by adapting the architecture and training objectives of video diffusion models for autoregressive generation. The approach also incorporates a causal action guidance mechanism, improving the controllability of actions in interactive environments, as demonstrated through experiments in robot manipulation and game simulations.'}, 'zh': {'title': '将视频扩散模型转化为交互式世界模型的创新方法', 'desc': '本论文介绍了一种名为Vid2World的方法，它将预训练的视频扩散模型转化为交互式世界模型，以提高数据效率。现有的世界模型通常需要大量特定领域的训练，并且预测精度较低，限制了其在复杂环境中的应用。Vid2World通过调整视频扩散模型的架构和训练目标，实现了自回归生成，并引入了因果动作引导机制，以增强交互式世界模型中的动作可控性。实验结果表明，该方法在机器人操作和游戏模拟领域表现出色，提供了一种可扩展且有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2505.15146', 'title': 'lmgame-Bench: How Good are LLMs at Playing Games?', 'url': 'https://huggingface.co/papers/2505.15146', 'abstract': 'Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.', 'score': 16, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '96b324d85a6c0d83', 'authors': ['Lanxiang Hu', 'Mingjia Huo', 'Yuxuan Zhang', 'Haoyang Yu', 'Eric P. Xing', 'Ion Stoica', 'Tajana Rosing', 'Haojian Jin', 'Hao Zhang'], 'affiliations': ['MBZUAI', 'UC Berkeley', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.15146.jpg', 'data': {'categories': ['#rl', '#benchmark', '#games', '#agents', '#transfer_learning'], 'emoji': '🎮', 'ru': {'title': 'Видеоигры как полигон для оценки и обучения языковых моделей', 'desc': 'Исследователи изучают возможности использования видеоигр для оценки современных больших языковых моделей (LLM). Они выявили проблемы с восприятием, чувствительностью к промптам и потенциальным загрязнением данных при прямом применении LLM в играх. Для решения этих проблем авторы представляют lmgame-Bench - набор игр с унифицированным API и вспомогательными структурами для восприятия и памяти. Результаты показывают, что lmgame-Bench эффективно оценивает и разделяет модели, а также позволяет переносить навыки между играми и задачами планирования.'}, 'en': {'title': 'Evaluating LLMs with lmgame-Bench: A Game-Changer!', 'desc': 'This paper addresses the challenges of evaluating large language models (LLMs) using video games, highlighting issues like poor visual perception, sensitivity to prompts, and data contamination. The authors propose lmgame-Bench, a framework that standardizes game evaluations through a unified API and incorporates tools for perception and memory. This framework allows for a more reliable assessment of LLMs across various game types, including platformers and puzzles. The results demonstrate that lmgame-Bench effectively distinguishes between models and shows that training on one game can enhance performance on others and on planning tasks.'}, 'zh': {'title': '游戏评估：提升大型语言模型的能力', 'desc': '本论文探讨了如何使用视频游戏来评估现代大型语言模型（LLM）。研究发现，直接将LLM应用于游戏中进行评估存在视觉感知脆弱、提示敏感性和数据污染等问题。为了解决这些挑战，作者提出了lmgame-Bench，这是一个通过统一的Gym风格API提供的平台游戏、解谜游戏和叙事游戏的评估工具。通过对13个领先模型的测试，lmgame-Bench能够有效区分模型的能力，并且在单一游戏上进行强化学习可以转移到未见过的游戏和外部规划任务。'}}}, {'id': 'https://huggingface.co/papers/2505.15765', 'title': 'Constructing a 3D Town from a Single Image', 'url': 'https://huggingface.co/papers/2505.15765', 'abstract': 'Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, a training-free framework designed to synthesize realistic and coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from a single image using a principled, training-free approach.', 'score': 14, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'c8acf46d639df2bb', 'authors': ['Kaizhi Zheng', 'Ruijian Zhang', 'Jing Gu', 'Jie Yang', 'Xin Eric Wang'], 'affiliations': ['Columbia University', 'Cybever AI', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.15765.jpg', 'data': {'categories': ['#synthetic', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'Реалистичные 3D-города из одного изображения без обучения', 'desc': '3DTown - это новый подход к генерации реалистичных трехмерных сцен на основе единственного вида сверху, не требующий дополнительного обучения. Метод основан на генерации по регионам для улучшения соответствия изображению и разрешения, а также на пространственно-осведомленном 3D-инпейнтинге для обеспечения глобальной согласованности сцены. 3DTown разбивает входное изображение на перекрывающиеся области, генерирует каждую с помощью предобученного генератора 3D-объектов, а затем применяет маскированный инпейнтинг потока для заполнения недостающей геометрии. Эксперименты показывают, что 3DTown превосходит современные методы по качеству геометрии, пространственной согласованности и точности текстур.'}, 'en': {'title': 'Transforming Top-Down Images into Stunning 3D Towns!', 'desc': 'This paper presents 3DTown, a novel framework for generating realistic 3D scenes from a single top-down image without the need for extensive training. The method utilizes region-based generation to enhance the alignment between the 2D image and the 3D output, while also employing spatial-aware inpainting to ensure the overall coherence and quality of the generated geometry. By breaking down the image into overlapping regions and using a pretrained 3D object generator, the framework effectively fills in missing parts of the scene, maintaining structural integrity. The results indicate that 3DTown surpasses existing models in producing high-quality, coherent 3D scenes, demonstrating the potential of training-free approaches in 3D scene synthesis.'}, 'zh': {'title': '从单张图像生成高质量3D场景的创新方法', 'desc': '本论文提出了一种名为3DTown的框架，可以从单张俯视图生成逼真的3D场景，而无需复杂的训练过程。该方法基于区域生成和空间感知的3D修复技术，确保生成的场景在几何形状和布局上保持一致性。通过将输入图像分解为重叠区域，并利用预训练的3D物体生成器生成每个区域，最后进行几何填充，保持结构连续性。实验结果表明，3DTown在几何质量、空间一致性和纹理保真度方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2505.15210', 'title': 'Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs', 'url': 'https://huggingface.co/papers/2505.15210', 'abstract': "Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors.", 'score': 14, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'acf62275d75af161', 'authors': ['Jie Ma', 'Ning Qu', 'Zhitao Gao', 'Rui Xing', 'Jun Liu', 'Hongbin Pei', 'Jiang Xie', 'Linyun Song', 'Pinghui Wang', 'Jing Tao', 'Zhou Su'], 'affiliations': ['MOE KLINNS Lab, Xian Jiaotong University', 'School of Artificial Intelligence, Chongqing University of Post and Telecommunications', 'School of Computer Science and Technology, Xian Jiaotong University', 'School of Computer Science, Northwestern Polytechnical University', 'Shaanxi Province Key Laboratory of Big Data Knowledge Engineering'], 'pdf_title_img': 'assets/pdf/title_img/2505.15210.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag', '#benchmark', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности языковых моделей через осмысление априорных знаний', 'desc': "Статья представляет новый метод под названием 'Deliberation over Priors' (DP) для улучшения работы больших языковых моделей (LLM) с использованием графов знаний. DP использует прогрессивную стратегию дистилляции знаний, интегрируя структурные априорные данные в LLM для повышения достоверности генерации путей отношений. Метод также применяет стратегию рассуждения и самоанализа, основанную на извлеченных ограничениях, для обеспечения надежности генерации ответов. Эксперименты показывают, что DP достигает нового уровня производительности, особенно улучшая показатель Hit@1 на 13% для набора данных ComplexWebQuestions."}, 'en': {'title': 'Enhancing LLM Trustworthiness with Knowledge Graphs', 'desc': 'This paper introduces a new framework called Deliberation over Priors (DP) to enhance the reliability of Large Language Models (LLMs) by leveraging knowledge graphs (KGs). DP utilizes a progressive knowledge distillation strategy that incorporates the structural information and constraints from KGs into LLMs, improving the accuracy of reasoning and response generation. The framework also includes a reasoning-introspection strategy that allows LLMs to verify their reasoning based on extracted constraints, leading to more trustworthy outputs. Experimental results show that DP significantly outperforms existing methods, achieving a notable improvement in performance on benchmark datasets.'}, 'zh': {'title': '提升大型语言模型的可信推理能力', 'desc': '本论文提出了一种基于知识图谱的检索增强生成框架，旨在减少大型语言模型（LLMs）中的幻觉现象。我们提出的框架称为“先验推理深思”（Deliberation over Priors, DP），充分利用知识图谱中的结构信息和约束条件。DP通过逐步知识蒸馏策略，将结构先验整合到LLMs中，从而提高关系路径生成的可信度。此外，框架还采用推理自省策略，确保生成响应的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2505.15779', 'title': 'IA-T2I: Internet-Augmented Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2505.15779', 'abstract': "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.", 'score': 13, 'issue_id': 3893, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '97c03041dc8579a8', 'authors': ['Chuanhao Li', 'Jianwen Sun', 'Yukang Feng', 'Mingliang Zhai', 'Yifan Chang', 'Kaipeng Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.15779.jpg', 'data': {'categories': ['#rag', '#alignment', '#dataset', '#multimodal', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Интернет-augmented T2I: преодоление неопределенности в генерации изображений', 'desc': 'Статья представляет новый подход к генерации изображений по текстовому описанию (T2I) с использованием интернет-ресурсов. Авторы предлагают фреймворк IA-T2I, который решает проблему неопределенности знаний в промптах путем добавления референсных изображений. Система включает модуль активного поиска, иерархический выбор изображений и механизм самоанализа для улучшения результатов. Эффективность метода подтверждена экспериментами на специально созданном датасете Img-Ref-T2I и оценкой с помощью GPT-4.'}, 'en': {'title': 'Enhancing T2I Models with Internet-Augmented Knowledge', 'desc': 'This paper introduces the Internet-Augmented text-to-image generation (IA-T2I) framework, which enhances traditional text-to-image (T2I) models by addressing uncertainties in text prompts. The framework includes an active retrieval module to assess the need for reference images, a hierarchical image selection module to find the best matching images, and a self-reflection mechanism for continuous improvement of generated images. A new dataset, Img-Ref-T2I, is created to test the framework, featuring prompts with various types of uncertain knowledge. Experimental results show that IA-T2I significantly improves the quality of generated images, outperforming existing models in human evaluations.'}, 'zh': {'title': '互联网增强的文本到图像生成框架', 'desc': '当前的文本到图像生成模型在处理文本提示中隐含的不确定知识时表现不佳。为了解决这个问题，我们提出了一种互联网增强的文本到图像生成框架（IA-T2I），通过提供参考图像来帮助模型理解不确定的知识。该框架包括主动检索模块、分层图像选择模块和自我反思机制，以提高生成图像的质量和与文本提示的一致性。实验结果表明，我们的框架在性能上优于现有模型，特别是在处理复杂和不确定的文本提示时。'}}}, {'id': 'https://huggingface.co/papers/2505.15404', 'title': 'How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study', 'url': 'https://huggingface.co/papers/2505.15404', 'abstract': 'Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important research question: how can we enhance the safety of LRMs? In this paper, we present a comprehensive empirical study on how to enhance the safety of LRMs through Supervised Fine-Tuning (SFT). Our investigation begins with an unexpected observation: directly distilling safe responses from DeepSeek-R1 fails to significantly enhance safety. We analyze this phenomenon and identify three key failure patterns that contribute to it. We then demonstrate that explicitly addressing these issues during the data distillation process can lead to substantial safety improvements. Next, we explore whether a long and complex reasoning process is necessary for achieving safety. Interestingly, we find that simply using short or template-based reasoning process can attain comparable safety performance-and are significantly easier for models to learn than more intricate reasoning chains. These findings prompt a deeper reflection on the role of reasoning in ensuring safety. Finally, we find that mixing math reasoning data during safety fine-tuning is helpful to balance safety and over-refusal. Overall, we hope our empirical study could provide a more holistic picture on enhancing the safety of LRMs. The code and data used in our experiments are released in https://github.com/thu-coai/LRM-Safety-Study.', 'score': 11, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'd681b198a7360d3b', 'authors': ['Zhexin Zhang', 'Xian Qi Loye', 'Victor Shea-Jay Huang', 'Junxiao Yang', 'Qi Zhu', 'Shiyao Cui', 'Fei Mi', 'Lifeng Shang', 'Yingkang Wang', 'Hongning Wang', 'Minlie Huang'], 'affiliations': ['Huawei Noahs Ark Lab', 'The Conversational AI (CoAI) group, DCST, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15404.jpg', 'data': {'categories': ['#data', '#math', '#reasoning', '#training', '#safety'], 'emoji': '🛡️', 'ru': {'title': 'Повышение безопасности LRM: простота может быть ключом', 'desc': 'Эта статья исследует способы повышения безопасности Моделей Крупномасштабного Рассуждения (LRM) с помощью Контролируемой Тонкой Настройки (SFT). Авторы обнаружили, что прямая дистилляция безопасных ответов не всегда эффективна, и выявили три ключевых паттерна неудач. Исследование показало, что короткие или шаблонные процессы рассуждения могут быть столь же эффективны для обеспечения безопасности, как и сложные цепочки рассуждений. Кроме того, было установлено, что включение данных по математическим рассуждениям в процесс тонкой настройки помогает сбалансировать безопасность и чрезмерный отказ.'}, 'en': {'title': 'Enhancing Safety in Large Reasoning Models through Simplified Reasoning', 'desc': "This paper investigates how to improve the safety of Large Reasoning Models (LRMs) while maintaining their reasoning capabilities. The authors find that directly distilling safe responses does not significantly enhance safety and identify three failure patterns that contribute to this issue. They demonstrate that addressing these patterns during data distillation can lead to better safety outcomes. Additionally, the study reveals that simpler reasoning processes can achieve similar safety performance as complex ones, suggesting a reevaluation of reasoning's role in safety enhancement."}, 'zh': {'title': '提升大型推理模型安全性的研究', 'desc': '大型推理模型（LRMs）在数学和编程等推理密集型任务中取得了显著成功。然而，它们的推理能力增强并不一定能提高安全性能，甚至在某些情况下可能会降低安全性。本文通过监督微调（SFT）对如何增强LRMs的安全性进行了全面的实证研究，发现直接从DeepSeek-R1提取安全响应并未显著提升安全性，并识别出三种关键的失败模式。研究表明，使用简单的短推理过程可以实现与复杂推理过程相当的安全性能，且更易于模型学习，这促使我们重新思考推理在确保安全性中的作用。'}}}, {'id': 'https://huggingface.co/papers/2505.15817', 'title': 'Learning to Reason via Mixture-of-Thought for Logical Reasoning', 'url': 'https://huggingface.co/papers/2505.15817', 'abstract': 'Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference.', 'score': 10, 'issue_id': 3897, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'a9b89bfbce1c417a', 'authors': ['Tong Zheng', 'Lichang Chen', 'Simeng Han', 'R. Thomas McCoy', 'Heng Huang'], 'affiliations': ['Dept. of Computer Science, UMD, College Park, MD 20742', 'Dept. of Computer Science, Yale University, New Haven, CT 06520', 'Dept. of Linguistics, Yale University, New Haven, CT'], 'pdf_title_img': 'assets/pdf/title_img/2505.15817.jpg', 'data': {'categories': ['#training', '#multimodal', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Смешение модальностей для улучшения логических рассуждений ИИ', 'desc': 'Статья представляет новый подход к обучению языковых моделей - Mixture-of-Thought (MoT). MoT позволяет большим языковым моделям (LLM) рассуждать, используя три модальности: естественный язык, код и новую символическую модальность - таблицу истинности. Метод включает двухфазовый дизайн: самоэволюционирующее обучение MoT и вывод MoT. Эксперименты показывают, что MoT значительно превосходит базовые LLM с подходом цепочки рассуждений в одной модальности, особенно на сложных задачах логического вывода.'}, 'en': {'title': 'Empowering LLMs with Multi-Modal Reasoning for Enhanced Logic Solving', 'desc': "This paper introduces the Mixture-of-Thought (MoT) framework, which enhances large language models (LLMs) by enabling them to reason across three different modalities: natural language, code, and a new symbolic modality called truth-table. Unlike traditional methods that rely on a single reasoning modality during training, MoT allows for a more integrated approach, improving the model's ability to tackle logical reasoning tasks. The framework consists of two phases: a self-evolving training phase that learns from generated rationales across modalities, and an inference phase that utilizes the strengths of all three modalities for better predictions. Experimental results show that MoT significantly outperforms existing single-modality approaches, particularly on challenging logical reasoning problems, achieving notable accuracy improvements."}, 'zh': {'title': '多模态推理，提升逻辑思维能力', 'desc': '本文提出了一种名为混合思维（Mixture-of-Thought, MoT）的框架，旨在提升大型语言模型（LLM）在逻辑推理中的表现。与传统方法只使用自然语言作为推理模式不同，MoT结合了自然语言、代码和一种新引入的符号模式——真值表，以增强推理能力。该框架采用两阶段设计：自我演化的MoT训练和MoT推理，能够在训练和推理阶段充分利用三种模式的协同效应。实验结果表明，MoT在逻辑推理基准测试中显著超越了单一模式的LLM基线，尤其在处理更复杂的逻辑问题时表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.15781', 'title': 'dKV-Cache: The Cache for Diffusion Language Models', 'url': 'https://huggingface.co/papers/2505.15781', 'abstract': 'Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.', 'score': 10, 'issue_id': 3893, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '516300496b94028a', 'authors': ['Xinyin Ma', 'Runpeng Yu', 'Gongfan Fang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.15781.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#diffusion', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных языковых моделей с помощью умного кэширования', 'desc': 'Статья представляет новый механизм ускорения вывода для диффузионных языковых моделей (DLM) - отложенное KV-кэширование (dKV-Cache). Авторы предлагают два варианта: dKV-Cache-Decode, который улучшает производительность на длинных последовательностях, и dKV-Cache-Greedy для более агрессивного кэширования. Метод позволяет достичь 2-10-кратного ускорения вывода DLM, сокращая разрыв с авторегрессионными моделями. Эксперименты показывают эффективность подхода на различных задачах, включая понимание языка, математику и генерацию кода.'}, 'en': {'title': 'Accelerating Diffusion Language Models with Delayed KV-Cache', 'desc': 'This paper introduces a new mechanism called delayed KV-Cache to improve the inference speed of Diffusion Language Models (DLMs), which traditionally suffer from slow decoding due to their non-autoregressive nature. The authors identify that different tokens exhibit unique representation dynamics during the diffusion process, leading to the development of a caching strategy that optimizes key and value states. They propose two variants of the caching mechanism: dKV-Cache-Decode, which enhances performance on long sequences, and dKV-Cache-Greedy, which prioritizes speed at the expense of some accuracy. Overall, the proposed dKV-Cache achieves a significant speedup of 2-10x in inference, making DLMs more competitive with autoregressive models.'}, 'zh': {'title': '加速扩散语言模型的推理速度', 'desc': '扩散语言模型（DLMs）被视为自回归语言模型的有力竞争者，但其推理速度较慢是一个主要问题。本文提出了一种延迟键值缓存（dKV-Cache）机制，以解决DLMs在去噪过程中的瓶颈。我们设计了两种互补的缓存变体，分别为dKV-Cache-Decode和dKV-Cache-Greedy，前者在长序列上几乎无损加速，后者则以更高的速度实现了二次时间复杂度的加速。实验结果表明，dKV-Cache在多个基准测试中显著提高了推理速度，缩小了自回归模型与扩散语言模型之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2505.15656', 'title': 'Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!', 'url': 'https://huggingface.co/papers/2505.15656', 'abstract': 'Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction.', 'score': 10, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'eed97ed4c1582120', 'authors': ['Zhexin Zhang', 'Yuhao Sun', 'Junxiao Yang', 'Shiyao Cui', 'Hongning Wang', 'Minlie Huang'], 'affiliations': ['The Conversational AI (CoAI) group, DCST, Tsinghua University', 'The University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2505.15656.jpg', 'data': {'categories': ['#data', '#security', '#open_source', '#training', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'Скрытая угроза: как открытые языковые модели могут раскрыть ваши секретные данные', 'desc': 'Эта статья раскрывает новый риск безопасности при дообучении открытых языковых моделей (LLM) на проприетарных данных. Исследователи показали, что создатель исходной модели может извлечь частные данные дообучения, имея только доступ к конечной модели. Эксперименты на популярных моделях от 3 до 32 миллиардов параметров показали высокую эффективность извлечения - до 76.3% данных в практических условиях. Авторы подчеркивают срочность решения этой проблемы утечки данных при дообучении LLM.'}, 'en': {'title': 'Exposing the Hidden Risks of Fine-Tuning LLMs with Proprietary Data', 'desc': 'This paper discusses a significant risk associated with fine-tuning open-source Large Language Models (LLMs) using proprietary data. The authors demonstrate that creators of these LLMs can exploit backdoor training techniques to extract sensitive fine-tuning data from the models, even with only black-box access. Their experiments reveal that up to 76.3% of the fine-tuning data can be successfully extracted, with even higher rates in optimal conditions. The study emphasizes the urgent need for further research to address this data security issue in the context of LLM fine-tuning.'}, 'zh': {'title': '微调中的数据泄露风险警示', 'desc': '本论文揭示了在开源大型语言模型（LLMs）上进行微调时可能存在的隐私风险。研究表明，开源模型的创建者可以通过简单的后门训练提取私有的微调数据，即使只通过黑箱访问微调后的模型。实验结果显示，在实际情况下，最多可提取76.3%的微调数据，而在理想情况下成功率可达94.9%。我们还探讨了一种基于检测的防御策略，但发现该策略可以被改进的攻击绕过，因此强调了这一新识别的数据泄露风险的紧迫性。'}}}, {'id': 'https://huggingface.co/papers/2505.13934', 'title': 'RLVR-World: Training World Models with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.13934', 'abstract': 'World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.', 'score': 9, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'bb3f20501a24c607', 'authors': ['Jialong Wu', 'Shaofeng Yin', 'Ningya Feng', 'Mingsheng Long'], 'affiliations': ['School of Software, BNRist, Tsinghua University', 'Zhili College, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13934.jpg', 'data': {'categories': ['#rlhf', '#training', '#multimodal', '#optimization', '#reasoning', '#games', '#rl', '#agents', '#video'], 'emoji': '🌐', 'ru': {'title': 'RLVR-World: Революция в обучении мировых моделей', 'desc': 'RLVR-World - это новый подход к обучению мировых моделей, использующий обучение с подкреплением с проверяемыми наградами. В отличие от стандартных методов, таких как оценка максимального правдоподобия, RLVR-World оптимизирует модели непосредственно для специфических метрик задачи. Этот метод применим к различным модальностям, включая языковые и видео-модели. Эксперименты показали значительное улучшение производительности в таких областях, как текстовые игры, веб-навигация и робототехника.'}, 'en': {'title': 'Optimizing World Models with Reinforcement Learning for Better Predictions', 'desc': 'This paper introduces RLVR-World, a new framework that improves world models by using reinforcement learning with verifiable rewards. Traditional training methods like maximum likelihood estimation often do not align well with the specific goals of these models, such as accuracy in predicting state transitions. RLVR-World addresses this by optimizing world models directly for metrics that matter, like perceptual quality and accuracy. The authors show that this approach leads to significant improvements in performance for both language and video-based models across various tasks, including text games and robot manipulation.'}, 'zh': {'title': '通过可验证奖励优化世界模型的创新框架', 'desc': '本论文提出了一种名为RLVR-World的统一框架，旨在通过可验证奖励的强化学习来优化世界模型。传统的训练目标如最大似然估计（MLE）往往与特定任务的目标不一致，而RLVR-World直接针对过渡预测的准确性和感知质量进行优化。我们将世界建模视为对标记序列的自回归预测，并通过解码预测的度量作为可验证奖励进行评估。实验结果表明，RLVR-World在语言和视频基础的世界模型上均取得了显著的性能提升，适用于文本游戏、网页导航和机器人操作等多个领域。'}}}, {'id': 'https://huggingface.co/papers/2505.13529', 'title': 'BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs', 'url': 'https://huggingface.co/papers/2505.13529', 'abstract': 'Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don\'t know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.', 'score': 9, 'issue_id': 3892, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': 'b0175ab5c60ceaee', 'authors': ['Junxiao Yang', 'Jinzhe Tu', 'Haoran Liu', 'Xiaoce Wang', 'Chujie Zheng', 'Zhexin Zhang', 'Shiyao Cui', 'Caishun Chen', 'Tiantian He', 'Hongning Wang', 'Yew-Soon Ong', 'Minlie Huang'], 'affiliations': ['Centre for Frontier AI Research, Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore', 'The College of Computing and Data Science, Nanyang Technological University', 'The Conversational AI (CoAI) group, DCST, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13529.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности ИИ-рассуждений через осознание границ знаний', 'desc': 'Это исследование посвящено проблеме чрезмерной уверенности и неточности ответов больших моделей рассуждений (LRM) в математических и логических задачах. Авторы выявили два паттерна неправильных рассуждений: поспешные догадки в последний момент и спиральное overthinking. Для решения этих проблем предложен новый фреймворк BARREL, который способствует лаконичному и ограниченному фактическому рассуждению. Эксперименты показали, что BARREL повышает надежность модели DeepSeek-R1-Distill-Llama-8B с 39.33% до 61.48%, сохраняя при этом точность на уровне моделей, дообученных на данных рассуждений R1.'}, 'en': {'title': 'Enhancing Confidence and Reliability in Large Reasoning Models with BARREL', 'desc': 'This paper discusses the limitations of Large Reasoning Models (LRMs) in handling mathematical and logical reasoning, particularly their tendency to provide incorrect answers with excessive confidence. The authors identify two problematic reasoning behaviors: last-minute guessing and second-thought spiraling, which lead to these overconfident mistakes. To combat this, they introduce BARREL, a new framework designed to enhance factual reasoning by encouraging models to be more concise and aware of their boundaries. Their experiments show that training with BARREL significantly improves the reliability of a specific LRM, DeepSeek-R1-Distill-Llama-8B, while maintaining competitive accuracy levels.'}, 'zh': {'title': '提升推理模型的可靠性与准确性', 'desc': '最近，大型推理模型（LRMs）在数学和逻辑推理方面展现了令人印象深刻的能力。然而，目前的LRMs很少承认自己的无知，通常会在错误的情况下表现出过度自信，这引发了对其事实可靠性的担忧。本文识别了两种病态推理模式，导致了过度自信和错误答案的产生：临时猜测和反复思考。为了解决这些问题，我们提出了BARREL框架，促进简洁且边界意识强的事实推理。'}}}, {'id': 'https://huggingface.co/papers/2505.15778', 'title': 'Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space', 'url': 'https://huggingface.co/papers/2505.15778', 'abstract': 'Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like "soft" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.', 'score': 8, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '96715f0b3118eceb', 'authors': ['Zhen Zhang', 'Xuehai He', 'Weixiang Yan', 'Ao Shen', 'Chenyang Zhao', 'Shuohang Wang', 'Yelong Shen', 'Xin Eric Wang'], 'affiliations': ['LMSYS Org', 'Microsoft', 'Purdue University', 'University of California, Los Angeles', 'University of California, Santa Barbara', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.15778.jpg', 'data': {'categories': ['#math', '#interpretability', '#reasoning', '#benchmark', '#training'], 'emoji': '🧠', 'ru': {'title': 'Мягкое мышление: преодоление границ дискретного языкового рассуждения в ИИ', 'desc': "Эта статья представляет новый метод машинного обучения под названием 'Soft Thinking', который имитирует человеческое мышление в непрерывном пространстве концепций. В отличие от стандартных моделей рассуждений, ограниченных дискретными языковыми токенами, 'Soft Thinking' генерирует мягкие, абстрактные токены концепций. Этот подход позволяет исследовать более богатые представления и плавные переходы между концепциями, что приводит к улучшению точности и эффективности в задачах математики и программирования. Эмпирические результаты показывают повышение точности до 2.48 пунктов при одновременном снижении использования токенов до 22.4% по сравнению со стандартным методом Chain-of-Thought."}, 'en': {'title': 'Soft Thinking: Beyond Discrete Boundaries in Reasoning', 'desc': 'This paper presents Soft Thinking, a novel approach to reasoning that mimics human cognitive processes by utilizing continuous concept spaces instead of discrete token embeddings. Traditional reasoning models are limited by their reliance on fixed linguistic tokens, which restricts their ability to explore diverse reasoning paths. Soft Thinking generates abstract concept tokens through a probability-weighted mixture of existing token embeddings, allowing for smoother transitions and richer representations. Empirical results show that this method improves accuracy and reduces token usage, while maintaining interpretability, thus addressing the limitations of conventional Chain-of-Thought reasoning.'}, 'zh': {'title': '突破离散限制，拥抱软思维！', 'desc': '人类的认知通常涉及抽象和流动的概念，而不是仅仅依赖于离散的语言符号。当前的推理模型受限于人类语言的边界，只能处理代表固定语义点的离散标记嵌入。这种离散限制降低了推理模型的表达能力，导致推理路径的探索不够全面。本文提出了一种名为“软思维”的方法，通过在连续概念空间中生成软的抽象概念标记，模拟人类的“软”推理，从而提高推理的有效性和效率。'}}}, {'id': 'https://huggingface.co/papers/2505.15776', 'title': 'ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.15776', 'abstract': 'Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrieval-guided self-distillation, followed by Retrieval-Guided Reinforcement Learning with a specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision.', 'score': 8, 'issue_id': 3896, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'd52cdaf90c573017', 'authors': ['Changtai Zhu', 'Siyin Wang', 'Ruijun Feng', 'Kai Song', 'Xipeng Qiu'], 'affiliations': ['ByteDance Inc', 'Fudan University', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2505.15776.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#rag', '#rl', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Самообучающаяся система переформулировки запросов для диалогового поиска', 'desc': 'Статья представляет ConvSearch-R1 - первую самоуправляемую систему для переформулировки контекстно-зависимых запросов в поисковых диалоговых системах. В отличие от существующих подходов, ConvSearch-R1 не требует внешнего обучения, используя вместо этого обучение с подкреплением на основе сигналов поиска. Система применяет двухэтапный подход: самоуправляемый разогрев политики и обучение с подкреплением на основе поиска. Эксперименты показывают значительное улучшение производительности по сравнению с современными методами, особенно на сложном наборе данных TopiOCQA.'}, 'en': {'title': 'Revolutionizing Conversational Query Reformulation with Self-Driven Learning', 'desc': 'This paper introduces ConvSearch-R1, a novel framework for Conversational Query Reformulation (CQR) that eliminates the need for external supervision in query rewriting. It utilizes reinforcement learning to optimize the reformulation process based on retrieval signals, making it self-driven. The approach consists of a two-stage method that first addresses the cold-start problem and then employs a rank-incentive reward mechanism to improve retrieval performance. Experiments show that ConvSearch-R1 outperforms existing methods, achieving significant improvements on benchmark datasets with smaller models.'}, 'zh': {'title': '对话查询重构的新突破', 'desc': '本论文提出了一种新的对话查询重构框架ConvSearch-R1，旨在有效处理上下文相关的模糊查询。该方法通过强化学习直接利用检索信号优化查询重构，完全消除了对外部监督的依赖。我们采用了两阶段的方法，首先通过检索引导的自蒸馏解决冷启动问题，然后通过特别设计的奖励机制进行强化学习，以应对传统检索指标的稀疏性问题。实验结果表明，ConvSearch-R1在TopiOCQA和QReCC数据集上显著优于现有的最先进方法，尤其在TopiOCQA数据集上实现了超过10%的提升。'}}}, {'id': 'https://huggingface.co/papers/2505.14827', 'title': 'Text Generation Beyond Discrete Token Sampling', 'url': 'https://huggingface.co/papers/2505.14827', 'abstract': "In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.", 'score': 7, 'issue_id': 3893, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'c5866f38a43da092', 'authors': ['Yufan Zhuang', 'Liyuan Liu', 'Chandan Singh', 'Jingbo Shang', 'Jianfeng Gao'], 'affiliations': ['Microsoft Research', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.14827.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Смешивание входов для улучшения генерации языковых моделей', 'desc': 'Исследователи предложили новый метод автореградивной генерации текста под названием Mixture of Inputs (MoI). В отличие от стандартного подхода, MoI сохраняет информацию о распределении вероятностей токенов, а не только выбранный токен. Метод использует байесовскую оценку для создания нового входа модели, объединяющего дискретный токен и распределение вероятностей. MoI позволяет модели поддерживать более богатое внутреннее представление в процессе генерации, что улучшает качество текста и способности к рассуждению.'}, 'en': {'title': 'Enhancing Autoregressive Generation with Mixture of Inputs', 'desc': 'This paper introduces a novel method called Mixture of Inputs (MoI) for enhancing autoregressive generation in large language models (LLMs). Instead of discarding the token distribution after sampling a token, MoI combines the generated token with the previously discarded distribution to create a richer input representation. By using Bayesian estimation, MoI treats the token distribution as a prior and the sampled token as an observation, allowing for a continuous posterior expectation to be used as input. This approach leads to improved performance in tasks such as mathematical reasoning, code generation, and PhD-level question answering without requiring additional training.'}, 'zh': {'title': '混合输入：提升自回归生成的质量与推理能力', 'desc': '在标准的自回归生成中，语言模型预测下一个标记的分布，采样一个离散标记，然后丢弃该分布，仅将采样的标记作为新输入。为了保留这个分布的丰富信息，我们提出了一种名为混合输入（MoI）的方法，它不需要额外的训练。该方法在生成标记后，构建一个新的输入，将生成的离散标记与之前丢弃的标记分布混合。MoI使模型在生成过程中保持更丰富的内部表示，从而提高文本质量和推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.12650', 'title': 'AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use', 'url': 'https://huggingface.co/papers/2505.12650', 'abstract': 'Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, converting these images into simulation-ready formats remains labor-intensive and error-prone, creating a bottleneck for model training and validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that automatically transforms scanning transmission electron microscopy (STEM) images into atomic crystal structures and predicts their physical properties. AutoMat combines pattern-adaptive denoising, physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation and property prediction via MatterSim, and coordinated orchestration across all stages. We propose the first dedicated STEM2Mat-Bench for this task and evaluate performance using lattice RMSD, formation energy MAE, and structure-matching success rate. By orchestrating external tool calls, AutoMat enables a text-only LLM to outperform vision-language models in this domain, achieving closed-loop reasoning throughout the pipeline. In large-scale experiments over 450 structure samples, AutoMat substantially outperforms existing multimodal large language models and tools. These results validate both AutoMat and STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic simulation in materials science.The code and dataset are publicly available at https://github.com/yyt-2378/AutoMat and https://huggingface.co/datasets/yaotianvector/STEM2Mat.', 'score': 6, 'issue_id': 3891, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'b02918769ea5f226', 'authors': ['Yaotian Yang', 'Yiwen Tang', 'Yizhe Chen', 'Xiao Chen', 'Jiangjie Qiu', 'Hao Xiong', 'Haoyu Yin', 'Zhiyao Luo', 'Yifei Zhang', 'Sijia Tao', 'Wentao Li', 'Qinghua Zhang', 'Yuqiang Li', 'Wanli Ouyang', 'Bin Zhao', 'Xiaonan Wang', 'Fei Wei'], 'affiliations': ['Department of Chemical Engineering, Tsinghua University, Beijing, China', 'School of Computer Science, Northwestern Polytechnical University, Xian, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.12650.jpg', 'data': {'categories': ['#data', '#multimodal', '#open_source', '#benchmark', '#dataset', '#agents', '#science'], 'emoji': '🔬', 'ru': {'title': 'AutoMat: от микроскопических изображений к атомным структурам', 'desc': 'AutoMat - это новый конвейер для автоматического преобразования изображений сканирующей просвечивающей электронной микроскопии (STEM) в атомные кристаллические структуры и предсказания их физических свойств. Он объединяет адаптивное шумоподавление, физически обоснованный поиск шаблонов, реконструкцию атомов с учетом симметрии и быстрое предсказание свойств с помощью MatterSim. AutoMat превосходит существующие мультимодальные большие языковые модели и инструменты в задаче преобразования STEM-изображений в атомные структуры. Это важный шаг на пути к объединению микроскопии и атомистического моделирования в материаловедении.'}, 'en': {'title': 'Automating Atomic Structure Extraction from STEM Images with AutoMat', 'desc': 'This paper presents AutoMat, a machine learning pipeline designed to convert scanning transmission electron microscopy (STEM) images into atomic crystal structures efficiently. It addresses the challenge of limited experimental data by automating the transformation process, which traditionally requires significant manual effort. AutoMat integrates various techniques such as denoising, template retrieval, and property prediction to streamline the workflow and enhance model training. The authors also introduce the STEM2Mat-Bench for evaluating performance, demonstrating that AutoMat significantly outperforms existing models in this domain.'}, 'zh': {'title': 'AutoMat：显微镜与原子模拟的桥梁', 'desc': '本论文介绍了一种名为AutoMat的自动化管道，旨在将扫描透射电子显微镜（STEM）图像转换为原子晶体结构，并预测其物理性质。该方法结合了多种技术，包括自适应去噪、物理引导的模板检索和对称感知的原子重建，能够高效地处理数据。通过引入STEM2Mat-Bench进行性能评估，AutoMat在多个指标上显著优于现有的多模态大语言模型。此研究为材料科学中显微镜与原子级模拟之间的桥梁建设提供了重要进展。'}}}, {'id': 'https://huggingface.co/papers/2505.15524', 'title': 'Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs', 'url': 'https://huggingface.co/papers/2505.15524', 'abstract': 'Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model\'s concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model\'s vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient\'s insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.', 'score': 5, 'issue_id': 3900, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '79c358dbadeae017', 'authors': ['Lang Gao', 'Kaiyang Wan', 'Wei Liu', 'Chenxi Wang', 'Zirui Song', 'Zixiang Xu', 'Yanbo Wang', 'Veselin Stoyanov', 'Xiuying Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.15524.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#interpretability', '#data'], 'emoji': '🔍', 'ru': {'title': 'BiasLens: Новый взгляд на предвзятость в языковых моделях', 'desc': 'Статья представляет BiasLens - новый метод анализа предвзятости в больших языковых моделях (LLM) без использования тестовых наборов данных. BiasLens использует векторы активации концепций (CAV) и разреженные автоэнкодеры (SAE) для извлечения интерпретируемых представлений концепций и количественной оценки предвзятости. Метод показывает сильную корреляцию с традиционными метриками оценки предвзятости и способен выявлять формы предвзятости, трудно обнаруживаемые существующими методами. BiasLens предлагает масштабируемую и эффективную парадигму для обнаружения предвзятости в LLM, способствуя улучшению их справедливости и прозрачности.'}, 'en': {'title': 'Uncovering Bias in Language Models with BiasLens', 'desc': "This paper addresses the issue of bias in Large Language Models (LLMs), particularly focusing on how certain concepts can be unfairly correlated with others, leading to skewed outputs. The authors introduce BiasLens, a novel framework that analyzes bias without the need for labeled data, using the model's vector space structure. By employing Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs), BiasLens quantifies bias through representational similarity, demonstrating strong alignment with traditional evaluation methods. This approach not only enhances the detection of subtle biases but also promotes fairness and transparency in LLMs."}, 'zh': {'title': '揭示大型语言模型中的偏见新方法', 'desc': '本文探讨了大型语言模型（LLMs）中的偏见问题，指出这种偏见会影响模型的可靠性和公平性。我们关注一种常见的偏见形式，即模型概念空间中两个参考概念（如情感极性）与目标概念（如评论方面）之间的不对称相关性。为了解决现有偏见评估方法的局限性，本文提出了BiasLens框架，它结合了概念激活向量（CAVs）和稀疏自编码器（SAEs），无需标注数据即可进行偏见分析。BiasLens能够有效量化偏见，并揭示传统方法难以检测的偏见形式，从而为提高LLMs的公平性和透明性提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2505.15801', 'title': 'VerifyBench: Benchmarking Reference-based Reward Systems for Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.15801', 'abstract': 'Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.', 'score': 4, 'issue_id': 3905, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '45285c81d23cba37', 'authors': ['Yuchen Yan', 'Jin Jiang', 'Zhenbang Ren', 'Yijun Li', 'Xudong Cai', 'Yang Liu', 'Xin Xu', 'Mengdi Zhang', 'Jian Shao', 'Yongliang Shen', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Meituan Group', 'Peking University', 'The Hong Kong University of Science and Technology', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15801.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#optimization', '#rl', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Новые бенчмарки для улучшения точности верификаторов в обучении с подкреплением', 'desc': 'Статья представляет два новых бенчмарка для оценки систем вознаграждения на основе эталонов в обучении с подкреплением (RL) для моделей рассуждений. VerifyBench и VerifyBench-Hard разработаны для измерения точности верификаторов, используемых в RL. Исследование показывает, что современные модели, особенно небольшого масштаба, имеют значительный потенциал для улучшения на этих бенчмарках. Авторы проводят подробный анализ результатов оценки, предоставляя ценные выводы для разработки систем вознаграждения на основе эталонов.'}, 'en': {'title': 'Enhancing Reasoning Models with New Reward Benchmarks', 'desc': 'This paper discusses the development of two new benchmarks, VerifyBench and VerifyBench-Hard, aimed at evaluating reference-based reward systems in reinforcement learning (RL). These benchmarks are created through careful data collection and human annotation to ensure their quality and effectiveness. The authors highlight that current reasoning models, including smaller-scale ones, still have significant room for improvement when tested against these benchmarks. The paper provides insights into enhancing verifier accuracy and the reasoning abilities of models trained with RL, ultimately guiding future research in this area.'}, 'zh': {'title': '提升推理模型的奖励系统评估', 'desc': '本文介绍了大型推理模型在推理领域的出色表现，特别是OpenAI o1和DeepSeek-R1。我们提出了两个基准测试，VerifyBench和VerifyBench-Hard，用于评估基于参考的奖励系统的性能。通过精心的数据收集和人工标注，这些基准确保了高质量的评估。我们的研究表明，当前模型在这两个基准上仍有很大的改进空间，尤其是小规模模型。'}}}, {'id': 'https://huggingface.co/papers/2505.15791', 'title': 'VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL', 'url': 'https://huggingface.co/papers/2505.15791', 'abstract': 'Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.', 'score': 3, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '48280e44cc4e905f', 'authors': ['Fengyuan Dai', 'Zifeng Zhuang', 'Yufei Huang', 'Siteng Huang', 'Bangyan Liao', 'Donglin Wang', 'Fajie Yuan'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15791.jpg', 'data': {'categories': ['#diffusion', '#rl', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'VARD: Усиление диффузионных моделей с помощью обучения с подкреплением', 'desc': 'Статья представляет новый метод под названием VARD (Value-based Reinforced Diffusion) для улучшения диффузионных моделей с помощью обучения с подкреплением. VARD использует функцию ценности для предсказания ожидаемых наград из промежуточных состояний и применяет ее вместе с KL-регуляризацией для плотного обучения на протяжении всего процесса генерации. Этот подход позволяет сохранять близость к предобученной модели, обеспечивая при этом эффективное и стабильное обучение. Экспериментальные результаты показывают, что VARD улучшает управление траекторией, повышает эффективность обучения и расширяет применимость обучения с подкреплением для диффузионных моделей.'}, 'en': {'title': 'Enhancing Diffusion Models with Value-Based Reinforcement Learning', 'desc': "This paper introduces VAlue-based Reinforced Diffusion (VARD), a new method for fine-tuning diffusion models using reinforcement learning. The approach addresses the challenges of stable and efficient training while dealing with non-differentiable rewards by incorporating a value function that predicts expected rewards from intermediate states. By applying KL regularization, VARD provides dense supervision throughout the generation process, enhancing the model's performance. Experimental results show that VARD improves trajectory guidance and training efficiency, making it suitable for complex reward functions in diffusion models."}, 'zh': {'title': '基于价值的强化扩散：提升生成质量的新方法', 'desc': '扩散模型在多个领域中成为强大的生成工具，但将预训练模型调整为具有特定期望属性仍然具有挑战性。强化学习（RL）提供了一种有前景的解决方案，但当前方法在实现稳定、高效的微调和支持非可微分奖励方面存在困难。此外，稀疏奖励在中间步骤提供的监督不足，常常导致生成质量不佳。为了解决这些问题，我们提出了基于价值的强化扩散（VARD）方法，通过学习价值函数来预测中间状态的奖励期望，并使用KL正则化提供密集监督，从而提高生成过程的质量和效率。'}}}, {'id': 'https://huggingface.co/papers/2505.15406', 'title': 'Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large\n  Audio-Language Models', 'url': 'https://huggingface.co/papers/2505.15406', 'abstract': 'The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.', 'score': 3, 'issue_id': 3900, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '2d6a5f87ed813485', 'authors': ['Zirui Song', 'Qian Jiang', 'Mingxuan Cui', 'Mingzhe Li', 'Lang Gao', 'Zeyu Zhang', 'Zixiang Xu', 'Yanbo Wang', 'Chenxi Wang', 'Guangxian Ouyang', 'Zhenhao Chen', 'Xiuying Chen'], 'affiliations': ['Australia National University', 'ByteDance', 'Mohamed bin Zayed University of Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2505.15406.jpg', 'data': {'categories': ['#ethics', '#dataset', '#benchmark', '#security', '#audio'], 'emoji': '🎙️', 'ru': {'title': 'AJailBench: Новый рубеж в оценке безопасности аудио-ИИ', 'desc': 'Статья представляет AJailBench - первый бенчмарк для оценки уязвимостей к атакам джейлбрейка в больших аудио-языковых моделях (LAM). Авторы создали набор данных AJailBench-Base из 1495 вредоносных аудио-запросов и оценили на нем несколько современных LAM. Также был разработан инструментарий Audio Perturbation Toolkit для генерации динамических вариантов атак с сохранением семантики. Результаты показывают, что даже небольшие семантически сохраненные возмущения могут значительно снизить безопасность ведущих LAM.'}, 'en': {'title': 'Evaluating and Enhancing Safety in Large Audio Language Models', 'desc': 'This paper addresses the safety concerns associated with Large Audio Language Models (LAMs), particularly their vulnerability to jailbreak attacks. It introduces AJailBench, a benchmark designed to systematically evaluate these vulnerabilities using a dataset of adversarial audio prompts. The study reveals that current LAMs lack consistent robustness against various attacks, highlighting the need for improved defenses. Additionally, the authors propose an Audio Perturbation Toolkit (APT) to create subtle yet effective adversarial audio samples that maintain semantic integrity, demonstrating the significant impact of small perturbations on LAM safety.'}, 'zh': {'title': '评估大型音频语言模型的安全性与漏洞', 'desc': '大型音频语言模型（LAMs）的兴起带来了潜力和风险，因为它们的音频输出可能包含有害或不道德的内容。目前的研究缺乏对LAM安全性的系统性定量评估，尤其是在针对越狱攻击方面。为了解决这个问题，我们提出了AJailBench，这是第一个专门设计用于评估LAM越狱漏洞的基准测试。我们的研究表明，即使是微小的、保持语义一致的扰动也能显著降低领先LAM的安全性能，强调了需要更强大和更具语义意识的防御机制。'}}}, {'id': 'https://huggingface.co/papers/2505.15047', 'title': 'PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration', 'url': 'https://huggingface.co/papers/2505.15047', 'abstract': 'Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering systematic uncertainty reduction. Overcoming these limitations fundamentally requires systematic uncertainty reduction. We introduce PiFlow, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\\% compared to a vanilla agent system. Overall, PiFlow serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our https://github.com/amair-lab/PiFlow{GitHub}.', 'score': 3, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '12f9a805fafedcf5', 'authors': ['Yingming Pu', 'Tao Lin', 'Hongyu Chen'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15047.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#rl', '#agents', '#science'], 'emoji': '🧪', 'ru': {'title': 'PiFlow: Революция в автоматизированном научном открытии', 'desc': 'PiFlow - это информационно-теоретическая система для автоматизированного научного открытия на основе многоагентных систем с использованием больших языковых моделей. Она рассматривает научное открытие как структурированную задачу уменьшения неопределенности, основанную на научных принципах. PiFlow значительно повышает эффективность открытий в различных научных областях, таких как наноматериалы, биомолекулы и сверхпроводники. По сравнению с обычными агентными системами, PiFlow демонстрирует улучшение качества решений на 94.06% и увеличение площади под кривой значений свойств на 73.55%.'}, 'en': {'title': 'PiFlow: Revolutionizing Automated Scientific Discovery through Uncertainty Reduction', 'desc': 'This paper presents PiFlow, a new framework for improving automated scientific discovery using Large Language Model (LLM)-based multi-agent systems. It addresses the limitations of existing methods that often lack rationality and fail to connect hypotheses with evidence, leading to inefficient exploration. By framing the discovery process as a structured uncertainty reduction problem, PiFlow enhances the efficiency and quality of scientific discoveries across various domains. The results show significant improvements in discovery efficiency and solution quality, marking a shift towards more effective AI-driven research.'}, 'zh': {'title': 'PiFlow：高效的自动化科学发现新范式', 'desc': '基于大型语言模型（LLM）的多智能体系统（MAS）在科学发现中展现出显著潜力。然而，现有方法通常使用预定义的工作流程来自动化科学发现，这些流程缺乏合理性约束，导致假设无目的且无法有效链接假设与证据，从而阻碍系统性的不确定性减少。为了解决这些问题，我们提出了PiFlow，一个信息理论框架，将自动化科学发现视为一个结构化的不确定性减少问题，并以科学原则为指导。在三个不同的科学领域中进行评估时，我们的方法显著提高了发现效率，并且解决方案质量也得到了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.15034', 'title': 'RL Tango: Reinforcing Generator and Verifier Together for Language\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.15034', 'abstract': 'Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.', 'score': 3, 'issue_id': 3897, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '35041556b54ed6d4', 'authors': ['Kaiwen Zha', 'Zhengqi Gao', 'Maohao Shen', 'Zhang-Wei Hong', 'Duane S. Boning', 'Dina Katabi'], 'affiliations': ['MIT', 'MIT-IBM Watson AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.15034.jpg', 'data': {'categories': ['#math', '#rlhf', '#reasoning', '#rl', '#training', '#optimization'], 'emoji': '🕺', 'ru': {'title': 'Tango: Танец обучения с подкреплением для улучшения рассуждений LLM', 'desc': 'Статья представляет новый метод Tango для улучшения способностей больших языковых моделей (LLM) к рассуждению с помощью обучения с подкреплением. В отличие от существующих подходов, Tango обучает одновременно и генеративную модель, и верификатор, используя обучение с подкреплением. Генеративный верификатор на основе LLM, обученный с помощью RL, демонстрирует улучшенную устойчивость и обобщение по сравнению с детерминированными верификаторами или обученными с помощью SFT. Эксперименты показывают, что обе компоненты Tango достигают наилучших результатов среди моделей масштаба 7B/8B на различных тестах по математическим рассуждениям и задачам вне обучающего распределения.'}, 'en': {'title': 'Tango: Reinforcing Language Models with Generative Verifiers', 'desc': 'This paper introduces Tango, a new framework that enhances large language models (LLMs) using reinforcement learning (RL). Unlike traditional methods that rely on fixed or supervised verifiers, Tango trains both the LLM generator and the verifier together, allowing them to improve each other through mutual reinforcement. The verifier in Tango is generative and learns from outcome-level rewards, making it more robust and capable of generalizing better to new tasks. Experiments show that Tango achieves state-of-the-art performance on various reasoning benchmarks, especially in challenging mathematical problems.'}, 'zh': {'title': 'Tango：强化学习提升语言模型推理能力的创新框架', 'desc': '强化学习（RL）最近成为提升大型语言模型（LLM）推理能力的一种有效方法。在现有的RL后训练方法中，通常使用固定的验证器，这可能导致奖励黑客行为和泛化能力差。为了解决这些问题，我们提出了Tango框架，它通过交替训练LLM生成器和验证器来实现更好的性能。Tango的创新在于其生成式的过程级验证器，能够在没有明确过程级注释的情况下，仅基于结果级验证奖励进行训练，从而提高了鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.15816', 'title': 'Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM', 'url': 'https://huggingface.co/papers/2505.15816', 'abstract': 'Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss. Our key insight is that vision tokens from the pretrained vision encoder do not necessarily require all the heavy operations (e.g., self-attention, FFNs) in decoder-only LMMs and could be processed more lightly with proper designs. We designed a series of experiments to discover and progressively squeeze out the vision-related computation redundancy. Based on our findings, we propose ProxyV, a novel approach that utilizes proxy vision tokens to alleviate the computational burden on original vision tokens. ProxyV enhances efficiency without compromising performance and can even yield notable performance gains in scenarios with more moderate efficiency improvements. Furthermore, the flexibility of ProxyV is demonstrated through its combination with token reduction methods to boost efficiency further. The code will be made public at this https://github.com/penghao-wu/ProxyV URL.', 'score': 2, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '96ff09e995e7c437', 'authors': ['Penghao Wu', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['1S-Lab, Nanyang Technological University', 'SenseTime'], 'pdf_title_img': 'assets/pdf/title_img/2505.15816.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#open_source', '#architecture', '#inference'], 'emoji': '🔍', 'ru': {'title': 'ProxyV: Оптимизация вычислений для визуальных токенов в мультимодальных моделях', 'desc': 'Статья представляет новый подход ProxyV для повышения эффективности мультимодальных моделей машинного обучения. Авторы обнаружили избыточность вычислений для визуальных токенов в декодер-ориентированных моделях. ProxyV использует прокси-токены для снижения вычислительной нагрузки без потери производительности. Метод показывает гибкость и совместимость с другими методами сокращения токенов.'}, 'en': {'title': 'Enhancing Efficiency in Multimodal Models with Proxy Vision Tokens', 'desc': 'This paper addresses the computational challenges faced by large multimodal models, particularly in processing visual tokens. Instead of focusing on reducing the number of tokens, the authors explore computation-level redundancy, revealing that not all heavy operations are necessary for vision tokens in decoder-only models. They introduce ProxyV, a method that uses proxy vision tokens to reduce the computational load while maintaining or even improving performance. The study shows that ProxyV can be combined with existing token reduction techniques for even greater efficiency gains.'}, 'zh': {'title': 'ProxyV：减轻视觉计算负担的创新方法', 'desc': '本文研究了大型多模态模型在处理视觉标记时的计算冗余问题。我们发现，预训练视觉编码器生成的视觉标记并不需要在解码器中执行所有重计算操作。通过设计一系列实验，我们提出了ProxyV方法，利用代理视觉标记来减轻原始视觉标记的计算负担。ProxyV在提高效率的同时不影响性能，甚至在适度提高效率的情况下还能显著提升性能。'}}}, {'id': 'https://huggingface.co/papers/2505.14818', 'title': 'WebNovelBench: Placing LLM Novelists on the Web Novel Distribution', 'url': 'https://huggingface.co/papers/2505.14818', 'abstract': 'Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.', 'score': 2, 'issue_id': 3900, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '54fde916bc0c7b6f', 'authors': ['Leon Lin', 'Jun Zheng', 'Haidong Wang'], 'affiliations': ['Nanyang Technological University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14818.jpg', 'data': {'categories': ['#story_generation', '#long_context', '#dataset', '#benchmark'], 'emoji': '📚', 'ru': {'title': 'WebNovelBench: новый стандарт оценки генерации длинных текстов языковыми моделями', 'desc': 'WebNovelBench - это новый бенчмарк для оценки способностей больших языковых моделей (LLM) генерировать длинные романы. Он использует набор данных из более чем 4000 китайских веб-новелл и оценивает качество генерации по восьми повествовательным измерениям. Оценка производится автоматически с помощью подхода LLM-as-Judge, а результаты агрегируются методом анализа главных компонент. Бенчмарк успешно различает шедевры, написанные людьми, популярные веб-новеллы и контент, сгенерированный LLM.'}, 'en': {'title': 'WebNovelBench: A New Standard for Evaluating LLM Storytelling', 'desc': 'This paper presents WebNovelBench, a new benchmark for evaluating the storytelling abilities of Large Language Models (LLMs) in generating long-form narratives. It utilizes a dataset of over 4,000 Chinese web novels and frames the evaluation as a task of generating stories from synopses. The framework assesses narrative quality across eight dimensions using an LLM-as-Judge approach, with results analyzed through Principal Component Analysis. The findings show that WebNovelBench can effectively distinguish between human-written and LLM-generated stories, providing valuable insights for improving LLM storytelling capabilities.'}, 'zh': {'title': '评估长篇小说生成的新基准', 'desc': '本论文介绍了WebNovelBench，这是一个专门用于评估大型语言模型（LLMs）长篇小说生成能力的新基准。该基准利用了超过4000部中文网络小说的大规模数据集，将评估框架设定为从摘要到故事生成的任务。我们提出了一个多维度的框架，涵盖八个叙事质量维度，通过LLM作为评判者的方式进行自动评估。实验结果表明，WebNovelBench能够有效区分人类创作的杰作、受欢迎的网络小说和LLM生成的内容。'}}}, {'id': 'https://huggingface.co/papers/2505.14157', 'title': 'Prior Prompt Engineering for Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.14157', 'abstract': 'This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.', 'score': 2, 'issue_id': 3898, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'bb33e698375153d7', 'authors': ['Pittawat Taveekitworachai', 'Potsawee Manakul', 'Sarana Nutanong', 'Kunat Pipatanakul'], 'affiliations': ['SCB 10X R&D, SCB 10X, SCBX Group, Thailand', 'School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2505.14157.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Предварительная инженерия промптов: новый рубеж в обучении языковых моделей с подкреплением', 'desc': 'Это исследование рассматривает влияние предварительной инженерии промптов (pPE) на обучение с подкреплением языковых моделей. Авторы адаптировали пять стратегий инженерии промптов во время вывода для использования в pPE. Эксперименты показали, что все модели, обученные с pPE, превзошли свои аналоги с промптами во время вывода, причем подход с нулевым примером показал наибольший прирост производительности. Исследование также продемонстрировало, что различные стратегии pPE формируют у моделей различные поведенческие стили.'}, 'en': {'title': 'Unlocking Performance: The Power of Prior Prompt Engineering in Language Models', 'desc': "This paper explores the concept of prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) for language models (LMs). It highlights how the design of prompts, which guide the model's behavior during training, can significantly influence performance. The authors translate various inference-time prompt engineering strategies into prior prompt engineering methods and test them on a model called Qwen2.5-7B. The results indicate that models trained with pPE outperform those using traditional methods, particularly showing notable improvements in specific benchmarks."}, 'zh': {'title': '先前提示工程：强化微调的新方向', 'desc': '本文研究了在强化微调（RFT）背景下的先前提示工程（pPE），旨在通过奖励信号引导语言模型（LM）表现出最大化性能的行为。尽管现有的RFT研究主要集中在算法、奖励塑造和数据整理上，但先前提示的设计仍然未被充分探讨。我们将五种代表性的推理时间提示工程（iPE）策略转化为相应的pPE方法，并在Qwen2.5-7B模型上进行实验。结果表明，所有pPE训练的模型在性能上均优于iPE提示的模型，尤其是null-example pPE方法在AIME2024和GPQA-Diamond上取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2505.14336', 'title': 'Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach', 'url': 'https://huggingface.co/papers/2505.14336', 'abstract': 'Llama-SMoP, an efficient multimodal LLM incorporating Sparse Mixture of Projectors, enhances AVSR performance without increasing inference costs through modality-specific routers and experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy environments by integrating visual cues. While recent advances integrate Large Language Models (LLMs) into AVSR, their high computational cost hinders deployment in resource-constrained settings. To address this, we propose Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of Projectors (SMoP) module to scale model capacity without increasing inference costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors, Llama-SMoP enables the use of smaller LLMs while maintaining strong performance. We explore three SMoP configurations and show that Llama-SMoP DEDR (Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation studies confirm its effectiveness in expert activation, scalability, and noise robustness.', 'score': 1, 'issue_id': 3903, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'd5c843866931f713', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis', 'Daniele Falavigna', 'Alessio Brutti'], 'affiliations': ['Fondazione Bruno Kessler, Italy', 'Imperial College London, UK', 'Meta AI, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.14336.jpg', 'data': {'categories': ['#optimization', '#audio', '#multimodal', '#inference', '#architecture', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Эффективное мультимодальное распознавание речи с помощью разреженных экспертов', 'desc': 'Llama-SMoP - это эффективная мультимодальная языковая модель, использующая разреженную смесь проекторов для улучшения аудиовизуального распознавания речи. Модель применяет модально-специфичные маршрутизаторы и экспертов для повышения производительности без увеличения вычислительных затрат. Llama-SMoP позволяет использовать меньшие языковые модели при сохранении высокой эффективности. Эксперименты показывают превосходство конфигурации DEDR с раздельными экспертами и маршрутизаторами для каждой модальности.'}, 'en': {'title': 'Efficient AVSR with Llama-SMoP: Smart Scaling for Strong Performance', 'desc': "Llama-SMoP is a novel multimodal large language model designed to improve audio-visual speech recognition (AVSR) while keeping inference costs low. It utilizes a Sparse Mixture of Projectors (SMoP) to enhance model capacity without the need for larger models. By implementing modality-specific routers and experts, Llama-SMoP effectively manages resources and maintains high performance in challenging environments. The model's configurations, particularly the DEDR setup, demonstrate significant advancements in ASR, VSR, and AVSR tasks, proving its efficiency and robustness against noise."}, 'zh': {'title': '高效多模态模型，提升语音识别性能', 'desc': 'Llama-SMoP是一种高效的多模态大语言模型，采用稀疏混合投影器(SMoP)模块，旨在提高音视频语音识别(AVSR)的性能，同时不增加推理成本。该模型通过使用特定模态的路由器和专家，能够在资源受限的环境中有效运行。Llama-SMoP通过稀疏门控的专家混合(MoE)投影器，允许使用较小的语言模型，同时保持强大的性能。实验结果表明，Llama-SMoP在语音识别(ASR)、视觉识别(VSR)和音视频语音识别(AVSR)任务中表现优越，具有良好的可扩展性和抗噪声能力。'}}}, {'id': 'https://huggingface.co/papers/2505.14101', 'title': 'MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation\n  of LLM Hallucinations', 'url': 'https://huggingface.co/papers/2505.14101', 'abstract': 'A multilingual, multihop benchmark using knowledge graphs for evaluating and mitigating hallucinations in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benchmarks have been developed that provide a test bed for factuality evaluation within the context of English-centric datasets, while relying on supplementary informative context like web links or text passages but ignoring the available structured factual resources. To this end, Knowledge Graphs (KGs) have been identified as a useful aid for hallucination mitigation, as they provide a structured way to represent the facts about entities and their relations with minimal linguistic overhead. We bridge the lack of KG paths and multilinguality for factual language modeling within the existing hallucination evaluation benchmarks and propose a KG-based multilingual, multihop benchmark called MultiHal framed for generative text evaluation. As part of our data collection pipeline, we mined 140k KG-paths from open-domain KGs, from which we pruned noisy KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation shows an absolute scale increase by approximately 0.12 to 0.36 points for the semantic similarity score in KG-RAG over vanilla QA across multiple languages and multiple models, demonstrating the potential of KG integration. We anticipate MultiHal will foster future research towards several graph-based hallucination mitigation and fact-checking tasks.', 'score': 1, 'issue_id': 3901, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'a89e20d6ea8c5a4b', 'authors': ['Ernests Lavrinovics', 'Russa Biswas', 'Katja Hose', 'Johannes Bjerva'], 'affiliations': ['Department of Computer Science Aalborg University Copenhagen, Denmark', 'Institute of Logic and Computation TU Wien Vienna, Austria'], 'pdf_title_img': 'assets/pdf/title_img/2505.14101.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#dataset', '#graphs', '#multilingual', '#data', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Графы знаний против галлюцинаций ИИ', 'desc': 'Статья представляет новый многоязычный бенчмарк MultiHal для оценки и снижения галлюцинаций в больших языковых моделях (LLM). Бенчмарк основан на использовании графов знаний (KG) и предназначен для генеративной оценки текста. Авторы создали набор данных из 25,9 тысяч высококачественных KG-путей, извлеченных из открытых графов знаний. Базовая оценка показала увеличение семантического сходства на 0,12-0,36 пункта при использовании KG-RAG по сравнению с обычными вопросно-ответными системами для разных языков и моделей.'}, 'en': {'title': 'Enhancing Language Models with Knowledge Graphs to Combat Hallucinations', 'desc': "This paper introduces a new benchmark called MultiHal, designed to evaluate and reduce hallucinations in large language models (LLMs) using knowledge graphs (KGs). Hallucinations refer to inaccuracies in the generated text, and existing benchmarks often focus on English datasets without utilizing structured factual resources. MultiHal incorporates multilingual and multihop capabilities by leveraging KGs, which represent facts and relationships in a structured format. The authors demonstrate that integrating KGs improves the semantic similarity scores in generative text evaluation across various languages and models, highlighting the benchmark's potential for advancing research in fact-checking and hallucination mitigation."}, 'zh': {'title': '利用知识图谱减轻语言模型幻觉的多语言基准测试', 'desc': '这篇论文提出了一种新的多语言、多跳基准测试，旨在评估和减轻大型语言模型中的幻觉现象。研究表明，知识图谱（KG）可以有效地帮助解决语言模型在事实性和可信度方面的局限性。通过挖掘和筛选140,000条KG路径，最终构建了25,900条高质量的KG路径，以支持多语言的事实语言建模。实验结果显示，整合KG后，语义相似度评分在多个语言和模型中有显著提升，表明KG在减轻幻觉方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.11454', 'title': 'HumaniBench: A Human-Centric Framework for Large Multimodal Models\n  Evaluation', 'url': 'https://huggingface.co/papers/2505.11454', 'abstract': 'HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench', 'score': 1, 'issue_id': 3900, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '3ca34ea5393b883b', 'authors': ['Shaina Raza', 'Aravind Narayanan', 'Vahid Reza Khazaie', 'Ashmal Vayani', 'Mukund S. Chettiar', 'Amandeep Singh', 'Mubarak Shah', 'Deval Pandya'], 'affiliations': ['University of Central Florida, Orlando, USA', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.11454.jpg', 'data': {'categories': ['#alignment', '#ethics', '#multimodal', '#dataset', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Оценка ИИ по человеческим меркам', 'desc': 'HumaniBench - это новый комплексный бенчмарк для оценки больших мультимодальных моделей (LMM) по семи принципам человекоцентричного ИИ. Он включает 32 тысячи пар изображение-вопрос из реального мира, охватывающих задачи визуального ответа на вопросы, многоязычного QA, визуальной локализации и эмпатического описания изображений. Тестирование 15 современных LMM показало, что проприетарные модели в целом лидируют, хотя устойчивость и визуальная локализация остаются слабыми местами. HumaniBench предоставляет строгую тестовую среду для диагностики пробелов в выравнивании и направления LMM к поведению, которое является как точным, так и социально ответственным.'}, 'en': {'title': 'HumaniBench: Aligning AI with Human Values', 'desc': 'HumaniBench is a new benchmark designed to evaluate large multimodal models (LMMs) based on seven human-centered AI principles, including fairness and empathy. It uses a dataset of 32,000 real-world image-question pairs, which are annotated with the help of a scalable GPT-4 assisted pipeline and verified by experts. The benchmark assesses LMMs across various tasks such as visual question answering and empathetic captioning, revealing that while proprietary models often perform better, they still have weaknesses in robustness and visual grounding. HumaniBench aims to identify alignment gaps in LMMs and promote their development towards more socially responsible and human-aligned behaviors.'}, 'zh': {'title': '以人为本的人工智能评估基准', 'desc': 'HumaniBench 是一个评估大型多模态模型（LMMs）的基准，专注于七个以人为本的人工智能原则，包括公平性、伦理、理解、推理、语言包容性、同理心和鲁棒性。该基准使用32,000个真实世界的图像-问题对，通过可扩展的GPT4o辅助管道进行注释，并由领域专家进行全面验证。尽管一些专有模型在性能上表现较好，但在鲁棒性和视觉定位方面仍存在不足，而一些开源模型在准确性与遵循人类对齐原则之间也难以平衡。HumaniBench 是首个专门围绕以人为本的人工智能原则构建的基准，为诊断对齐差距和引导LMMs朝向准确且社会责任感强的行为提供了严格的测试平台。'}}}, {'id': 'https://huggingface.co/papers/2505.11196', 'title': 'DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion\n  Modeling', 'url': 'https://huggingface.co/papers/2505.11196', 'abstract': 'Diffusion ConvNet (DiCo), using standard ConvNet modules with a compact channel attention mechanism, achieves high image quality and generation speed in visual generation tasks with efficiency gains compared to Diffusion Transformer (DiT).  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256x256-without any additional supervision during training. Code: https://github.com/shallowdream204/DiCo.', 'score': 1, 'issue_id': 3909, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'c15c5dc1b113f10f', 'authors': ['Yuang Ai', 'Qihang Fan', 'Xuefeng Hu', 'Zhenheng Yang', 'Ran He', 'Huaibo Huang'], 'affiliations': ['ByteDance', 'CASIA', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2505.11196.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'DiCo: Эффективная генерация изображений с помощью сверточных сетей', 'desc': 'Статья представляет Diffusion ConvNet (DiCo) - новую архитектуру для генеративных моделей изображений. DiCo использует стандартные сверточные модули с компактным механизмом внимания по каналам, что позволяет достичь высокого качества и скорости генерации. По сравнению с Diffusion Transformer (DiT), DiCo демонстрирует значительный прирост эффективности при сохранении высокой производительности. На бенчмарках ImageNet модель DiCo превосходит предыдущие диффузионные модели как по качеству изображений, так и по скорости генерации.'}, 'en': {'title': 'Efficient Visual Generation with Diffusion ConvNet (DiCo)', 'desc': 'The paper introduces Diffusion ConvNet (DiCo), a new model for visual generation that utilizes standard convolutional network (ConvNet) modules combined with a compact channel attention mechanism. This approach enhances image quality and generation speed while being more efficient than the existing Diffusion Transformer (DiT) models, which are computationally intensive. The authors identify that traditional self-attention in DiT often captures redundant local patterns, suggesting that convolution can be a more effective alternative if properly optimized. By promoting diverse channel activations, DiCo achieves superior performance on ImageNet benchmarks, demonstrating significant improvements in both image quality and generation speed.'}, 'zh': {'title': '扩散卷积网络：高效生成的未来', 'desc': '本文提出了一种新的扩散模型，称为扩散卷积网络（DiCo），它使用标准的卷积网络模块和紧凑的通道注意机制，旨在提高视觉生成任务的图像质量和生成速度。与扩散变换器（DiT）相比，DiCo在效率上有显著提升，尤其是在处理图像时表现出更好的性能。研究表明，传统的自注意力机制在捕捉全局信息时往往冗余，而卷积网络则能够更有效地提取局部特征。通过引入紧凑的通道注意机制，DiCo能够激活更多样化的通道，从而增强特征的多样性，最终在ImageNet基准测试中超越了之前的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2505.15141', 'title': 'BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms', 'url': 'https://huggingface.co/papers/2505.15141', 'abstract': 'Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.', 'score': 0, 'issue_id': 3905, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '32711fd3625843fd', 'authors': ['Yunlong Hou', 'Fengzhuo Zhang', 'Cunxiao Du', 'Xuan Zhang', 'Jiachun Pan', 'Tianyu Pang', 'Chao Du', 'Vincent Y. F. Tan', 'Zhuoran Yang'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15141.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#math'], 'emoji': '🚀', 'ru': {'title': 'Адаптивное спекулятивное декодирование: ускорение LLM с помощью многоруких бандитов', 'desc': 'Эта статья представляет новый подход к улучшению спекулятивного декодирования для ускорения работы больших языковых моделей (LLM). Авторы предлагают адаптивный метод выбора гиперпараметров для спекулятивного декодирования, основанный на алгоритмах многоруких бандитов. Разработаны два алгоритма - UCBSpec и EXP3Spec, для которых проведен теоретический анализ и доказана оптимальность UCBSpec. Эксперименты на моделях LLaMA3 и Qwen2 показали эффективность предложенного подхода по сравнению с существующими методами.'}, 'en': {'title': 'Dynamic Hyperparameter Selection for Efficient Text Generation', 'desc': 'This paper introduces a new method called BanditSpec for improving the efficiency of Large Language Models (LLMs) during text generation. Instead of using a fixed approach for speculative decoding, it dynamically selects hyperparameters based on the context of the text being generated. The authors frame this selection process as a Multi-Armed Bandit problem and propose two algorithms, UCBSpec and EXP3Spec, to optimize performance. Their experiments show that these algorithms significantly enhance throughput and adapt well to various input prompts, achieving results close to the best possible configurations.'}, 'zh': {'title': '自适应超参数选择，加速大语言模型推理', 'desc': '本文提出了一种新的在线学习框架，用于自适应选择大语言模型（LLM）推理过程中的超参数配置，以加速推理速度。我们将超参数选择问题建模为多臂赌博机问题，并提出了BanditSpec框架。文中设计了两种基于赌博机的超参数选择算法UCBSpec和EXP3Spec，并分析了它们在随机和对抗奖励设置下的停止时间遗憾。通过实验证明，这些算法在处理多样化输入提示时，能够有效提高推理效率，接近最佳超参数的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.15134', 'title': 'The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.15134', 'abstract': "Entropy minimization improves large language models' performance on math, physics, and coding tasks without labeled data through various methods like fine-tuning, reinforcement learning, and inference-time adjustments.  \t\t\t\t\tAI-generated summary \t\t\t\t Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.", 'score': 0, 'issue_id': 3909, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'ab8fb9efb23df8c5', 'authors': ['Shivam Agarwal', 'Zimin Zhang', 'Lifan Yuan', 'Jiawei Han', 'Hao Peng'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.15134.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#inference', '#math', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие скрытого потенциала языковых моделей через минимизацию энтропии', 'desc': 'Исследование показывает, что минимизация энтропии может значительно улучшить производительность больших языковых моделей (LLM) в задачах математики, физики и программирования без использования размеченных данных. Авторы предлагают три подхода: тонкая настройка с минимизацией энтропии (EM-FT), обучение с подкреплением (EM-RL) и корректировка логитов во время вывода (EM-INF). Метод EM-RL достигает сопоставимых или лучших результатов по сравнению с сильными базовыми линиями RL, обученными на размеченных примерах. Применение EM-INF позволяет модели Qwen-32B соответствовать или превосходить производительность проприетарных моделей на сложном бенчмарке SciCode.'}, 'en': {'title': 'Unlocking LLM Potential with Entropy Minimization', 'desc': 'This paper discusses how entropy minimization (EM) can enhance the performance of large language models (LLMs) on complex tasks in math, physics, and coding without the need for labeled data. The authors present three methods: EM-FT, which fine-tunes the model using unlabeled outputs; EM-RL, which employs reinforcement learning with negative entropy as a reward; and EM-INF, which adjusts outputs during inference to lower entropy. The results show that these methods can achieve performance levels comparable to or better than traditional approaches that rely on extensive labeled datasets. Overall, the study highlights the untapped reasoning abilities of pretrained LLMs that can be unlocked through simple entropy minimization techniques.'}, 'zh': {'title': '熵最小化：提升语言模型性能的关键', 'desc': '熵最小化（EM）是一种训练方法，可以在没有标记数据的情况下，显著提高大型语言模型（LLMs）在数学、物理和编程任务上的表现。该方法通过让模型将更多的概率集中在最有信心的输出上，来优化其性能。研究中提出了三种方法：EM-FT、EM-RL和EM-INF，分别通过不同的方式减少熵。结果表明，使用熵最小化的模型在多个基准测试中表现优异，显示出预训练模型的推理能力可以被有效激发。'}}}, {'id': 'https://huggingface.co/papers/2505.14990', 'title': 'Language Specific Knowledge: Do Models Know Better in X than in English?', 'url': 'https://huggingface.co/papers/2505.14990', 'abstract': 'Code-switching is a common phenomenon of alternating between different languages in the same utterance, thought, or conversation. We posit that humans code-switch because they feel more comfortable talking about certain topics and domains in one language than another. With the rise of knowledge-intensive language models, we ask ourselves the next, natural question: Could models hold more knowledge on some topics in some language X? More importantly, could we improve reasoning by changing the language that reasoning is performed in? We coin the term Language Specific Knowledge (LSK) to represent this phenomenon. As ethnic cultures tend to develop alongside different languages, we employ culture-specific datasets (that contain knowledge about cultural and social behavioral norms). We find that language models can perform better when using chain-of-thought reasoning in some languages other than English, sometimes even better in low-resource languages. Paired with previous works showing that semantic similarity does not equate to representational similarity, we hypothesize that culturally specific texts occur more abundantly in corresponding languages, enabling specific knowledge to occur only in specific "expert" languages. Motivated by our initial results, we design a simple methodology called LSKExtractor to benchmark the language-specific knowledge present in a language model and, then, exploit it during inference. We show our results on various models and datasets, showing an average relative improvement of 10% in accuracy. Our research contributes to the open-source development of language models that are inclusive and more aligned with the cultural and linguistic contexts in which they are deployed.', 'score': 0, 'issue_id': 3895, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '543b5a3ff8ff6a01', 'authors': ['Ishika Agarwal', 'Nimet Beyza Bozdag', 'Dilek Hakkani-Tür'], 'affiliations': ['Department of Computer Science University of Illinois, Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.14990.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#alignment', '#benchmark', '#open_source', '#inference', '#low_resource', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Раскрытие потенциала многоязычности в искусственном интеллекте', 'desc': 'Исследователи изучают феномен языково-специфичных знаний (LSK) в языковых моделях. Они обнаружили, что модели могут лучше выполнять рассуждения на некоторых языках, отличных от английского, даже на малоресурсных языках. Авторы разработали методологию LSKExtractor для оценки и использования языково-специфичных знаний в языковых моделях. Эксперименты показали среднее относительное улучшение точности на 10% при применении этого подхода.'}, 'en': {'title': 'Unlocking Knowledge: Language Matters in Machine Learning!', 'desc': 'This paper explores the concept of Language Specific Knowledge (LSK), which suggests that language models may possess more knowledge about certain topics when reasoning in specific languages. The authors argue that humans often code-switch between languages based on comfort with cultural contexts, and this can be leveraged in machine learning. They introduce a methodology called LSKExtractor to evaluate and utilize this language-specific knowledge during model inference. Their findings indicate that language models can achieve improved reasoning accuracy, particularly in low-resource languages, by aligning with culturally relevant datasets.'}, 'zh': {'title': '语言特定知识提升模型推理能力', 'desc': '本文探讨了代码切换现象，即在同一对话中交替使用不同语言的情况。我们提出人们在某些话题上更倾向于使用特定语言，因为他们在这些语言中感到更舒适。研究表明，语言模型在某些语言中使用链式推理时表现更好，尤其是在低资源语言中。我们设计了一种名为LSKExtractor的方法来评估语言模型中的语言特定知识，并在推理过程中加以利用，最终实现了平均10%的准确率提升。'}}}, {'id': 'https://huggingface.co/papers/2505.11080', 'title': 'BLEUBERI: BLEU is a surprisingly effective reward for instruction\n  following', 'url': 'https://huggingface.co/papers/2505.11080', 'abstract': 'Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.', 'score': 0, 'issue_id': 3910, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '34ff03409b67fa3d', 'authors': ['Yapei Chang', 'Yekyung Kim', 'Michael Krumdick', 'Amir Zadeh', 'Chuan Li', 'Chris Tanner', 'Mohit Iyyer'], 'affiliations': ['Kensho', 'Lambda AI', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2505.11080.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#rl', '#dataset', '#alignment', '#training', '#open_source', '#rlhf'], 'emoji': '🔍', 'ru': {'title': 'BLEU как эффективная альтернатива моделям вознаграждения в обучении языковых моделей', 'desc': 'Статья представляет метод BLEUBERI для обучения языковых моделей, использующий метрику BLEU вместо более сложных моделей вознаграждения. Исследователи обнаружили, что BLEU показывает сопоставимые результаты с моделями вознаграждения при оценке соответствия человеческим предпочтениям. BLEUBERI применяет обучение с подкреплением, используя BLEU в качестве функции вознаграждения, и демонстрирует конкурентоспособные результаты на нескольких бенчмарках. Метод позволяет эффективно обучать модели, соответствующие человеческим предпочтениям, без необходимости в дорогостоящих моделях вознаграждения.'}, 'en': {'title': 'BLEUBERI: Simplifying LLM Alignment with BLEU Metrics', 'desc': 'This paper explores the use of simpler metrics, specifically BLEU, as alternatives to traditional reward models for aligning large language models (LLMs) with human preferences. The authors demonstrate that BLEU, a basic string-matching metric, can achieve similar performance to complex reward models when evaluating instruction-following tasks. They introduce BLEUBERI, a method that optimizes model performance using BLEU as a reward function, showing that it can compete with models trained using more sophisticated reward models. The findings suggest that with access to high-quality reference outputs, simpler metrics can effectively guide the alignment of LLMs, making the training process more efficient and cost-effective.'}, 'zh': {'title': 'BLEUBERI：用简单指标替代奖励模型的有效方法', 'desc': '奖励模型在将大型语言模型（LLMs）与人类偏好对齐中起着关键作用，但训练成本高，需要大量人类标注的偏好数据和强大的预训练LLM基础模型。本文探讨了简单的基于参考的指标是否可以作为奖励模型的替代方案，结果发现BLEU这一基本的字符串匹配指标在与人类偏好的一致性上，竟然与强大的奖励模型相匹配。基于这一发现，我们开发了BLEUBERI方法，首先识别具有挑战性的指令，然后使用BLEU作为奖励函数进行群体相对策略优化（GRPO）。实验表明，BLEUBERI训练的模型在多个具有挑战性的指令跟随基准测试中，与通过奖励模型指导的强化学习训练的模型具有竞争力，且生成的输出在事实基础上更为可靠。'}}}, {'id': 'https://huggingface.co/papers/2504.20438', 'title': 'PixelHacker: Image Inpainting with Structural and Semantic Consistency', 'url': 'https://huggingface.co/papers/2504.20438', 'abstract': 'Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/PixelHacker.', 'score': 23, 'issue_id': 3584, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '987ce511e3c86e06', 'authors': ['Ziyang Xu', 'Kangsheng Duan', 'Xiaolei Shen', 'Zhifeng Ding', 'Wenyu Liu', 'Xiaohu Ruan', 'Xiaoxin Chen', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'VIVO AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.20438.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#open_source', '#optimization', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'PixelHacker: Революционный подход к восстановлению изображений с помощью латентных категорий', 'desc': 'В статье представлен новый подход к задаче восстановления изображений под названием PixelHacker. Авторы разработали метод латентного категориального управления, используя большой набор данных из 14 миллионов пар изображение-маска с аннотациями переднего и заднего плана. PixelHacker применяет диффузионную модель с внедрением признаков через линейное внимание. Эксперименты показывают, что PixelHacker превосходит современные методы на различных наборах данных, демонстрируя высокую согласованность структуры и семантики изображений.'}, 'en': {'title': 'PixelHacker: Revolutionizing Image Inpainting with Latent Categories Guidance', 'desc': 'This paper presents a new approach to image inpainting called PixelHacker, which aims to improve the quality of generated images by addressing issues with complex structures and semantics. The authors introduce a large dataset of 14 million image-mask pairs to train their model, focusing on distinguishing between foreground and background categories. They utilize a diffusion-based model that incorporates linear attention to enhance the denoising process, ensuring better consistency in texture and color. Experimental results demonstrate that PixelHacker significantly outperforms existing state-of-the-art methods across various datasets, achieving superior image restoration results.'}, 'zh': {'title': 'PixelHacker：图像修复的新突破', 'desc': '图像修复是图像编辑与生成之间的一个重要研究领域。最近的最先进方法探索了新颖的注意力机制、轻量级架构和上下文感知建模，取得了显著的性能。然而，这些方法在处理复杂结构和语义时常常面临挑战，导致生成的图像出现伪影和不当生成。为了解决这个问题，我们设计了一种简单而有效的修复范式，称为潜在类别引导，并提出了一种基于扩散的模型PixelHacker。'}}}, {'id': 'https://huggingface.co/papers/2505.01079', 'title': 'Improving Editability in Image Generation with Layer-wise Memory', 'url': 'https://huggingface.co/papers/2505.01079', 'abstract': 'Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.', 'score': 18, 'issue_id': 3582, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'e1aa83ea7926943e', 'authors': ['Daneul Kim', 'Jaeah Lee', 'Jaesik Park'], 'affiliations': ['Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01079.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Умное последовательное редактирование изображений: сохраняем прошлое, добавляем новое', 'desc': 'Статья представляет новый подход к последовательному редактированию изображений с использованием нейронных сетей. Авторы предлагают метод, позволяющий сохранять предыдущие изменения и естественно интегрировать новые элементы в существующий контент. Ключевые инновации включают использование приблизительных масок, послойную память для хранения латентных представлений и применение техник Background Consistency Guidance и Multi-Query Disentanglement. Эффективность метода подтверждается экспериментами на новом наборе данных с метриками семантического выравнивания.'}, 'en': {'title': 'Seamless Sequential Image Editing with Context Preservation', 'desc': 'This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input.'}, 'zh': {'title': '实现自然连续的图像编辑', 'desc': '本论文探讨了图像编辑中的多次连续编辑问题，现有方法在处理多个对象的修改时存在困难，尤其是在保持之前编辑内容的同时自然地融入新对象。我们提出了两项关键方案：一是支持粗略的掩膜输入，以保留现有内容并自然整合新元素；二是支持多次修改的一致性编辑。我们的框架通过层级记忆存储先前编辑的潜在表示和提示嵌入，利用背景一致性引导保持场景的连贯性。实验结果表明，我们的方法在迭代图像编辑任务中表现优越，用户只需提供粗略掩膜即可实现高质量的编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2504.21117', 'title': 'Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts', 'url': 'https://huggingface.co/papers/2504.21117', 'abstract': 'Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.', 'score': 11, 'issue_id': 3587, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '2a43e27932acf80e', 'authors': ['Hanhua Hong', 'Chenghao Xiao', 'Yang Wang', 'Yiqi Liu', 'Wenge Rong', 'Chenghua Lin'], 'affiliations': ['Beihang University', 'Durham University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2504.21117.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#benchmark', '#interpretability'], 'emoji': '🔄', 'ru': {'title': 'Автоматическая генерация промптов для надежной оценки NLG систем', 'desc': 'Статья посвящена проблеме оценки систем генерации естественного языка (NLG) и предлагает новый метод на основе обучения инверсии. Этот подход позволяет автоматически создавать эффективные промпты для оценки, специфичные для конкретной модели, используя только один образец. Метод устраняет необходимость в трудоемкой ручной разработке промптов, повышая эффективность и надежность оценки. Работа открывает новое направление для более надежной и эффективной оценки с использованием языковых моделей (LLM).'}, 'en': {'title': 'Revolutionizing NLG Evaluation with Inversion Learning', 'desc': 'This paper addresses the difficulties in evaluating natural language generation (NLG) systems, particularly the inconsistencies and biases in human evaluations. It introduces an inversion learning method that creates effective prompts for evaluating models by learning from their outputs. This approach allows for automatic generation of tailored evaluation prompts, requiring only one sample, which enhances efficiency. The proposed method aims to improve the robustness of LLM-based evaluations, paving the way for more standardized assessment in NLG.'}, 'zh': {'title': '提升自然语言生成评估的效率与稳健性', 'desc': '评估自然语言生成（NLG）系统是一个具有挑战性的任务，因为有效输出的多样性使得标准化评估变得困难。虽然人工评估被认为是金标准，但其存在不一致性、缺乏标准化和人口偏见等问题，限制了可重复性。基于大型语言模型（LLM）的评估提供了一种可扩展的替代方案，但对提示设计非常敏感，微小的变化可能导致显著的差异。我们提出了一种反演学习方法，可以有效地从模型输出反向映射到输入指令，从而自动生成高效的、特定于模型的评估提示，提升了评估的效率和稳健性。'}}}, {'id': 'https://huggingface.co/papers/2505.00949', 'title': 'Llama-Nemotron: Efficient Reasoning Models', 'url': 'https://huggingface.co/papers/2505.00949', 'abstract': 'We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.', 'score': 8, 'issue_id': 3587, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'cbc28025b0c6bde3', 'authors': ['Akhiad Bercovich', 'Itay Levy', 'Izik Golan', 'Mohammad Dabbah', 'Ran El-Yaniv', 'Omri Puny', 'Ido Galil', 'Zach Moshe', 'Tomer Ronen', 'Najeeb Nabwani', 'Ido Shahaf', 'Oren Tropp', 'Ehud Karpas', 'Ran Zilberstein', 'Jiaqi Zeng', 'Soumye Singhal', 'Alexander Bukharin', 'Yian Zhang', 'Tugrul Konuk', 'Gerald Shen', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Yoshi Suhara', 'Olivier Delalleau', 'Zijia Chen', 'Zhilin Wang', 'David Mosallanezhad', 'Adi Renduchintala', 'Haifeng Qian', 'Dima Rekesh', 'Fei Jia', 'Somshubra Majumdar', 'Vahid Noroozi', 'Wasi Uddin Ahmad', 'Sean Narenthiran', 'Aleksander Ficek', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Igor Gitman', 'Ivan Moshkov', 'Wei Du', 'Shubham Toshniwal', 'George Armstrong', 'Branislav Kisacanin', 'Matvei Novikov', 'Daria Gitman', 'Evelina Bakhturina', 'Jane Polak Scowcroft', 'John Kamalu', 'Dan Su', 'Kezhi Kong', 'Markus Kliegl', 'Rabeeh Karimi', 'Ying Lin', 'Sanjeev Satheesh', 'Jupinder Parmar', 'Pritam Gundecha', 'Brandon Norick', 'Joseph Jennings', 'Shrimai Prabhumoye', 'Syeda Nahida Akter', 'Mostofa Patwary', 'Abhinav Khattar', 'Deepak Narayanan', 'Roger Waleffe', 'Jimmy Zhang', 'Bor-Yiing Su', 'Guyue Huang', 'Terry Kong', 'Parth Chadha', 'Sahil Jain', 'Christine Harvey', 'Elad Segal', 'Jining Huang', 'Sergey Kashirsky', 'Robert McQueen', 'Izzy Putterman', 'George Lam', 'Arun Venkatesan', 'Sherry Wu', 'Vinh Nguyen', 'Manoj Kilaru', 'Andrew Wang', 'Anna Warno', 'Abhilash Somasamudramath', 'Sandip Bhaskar', 'Maka Dong', 'Nave Assaf', 'Shahar Mor', 'Omer Ullman Argov', 'Scot Junkin', 'Oleksandr Romanenko', 'Pedro Larroy', 'Monika Katariya', 'Marco Rovinelli', 'Viji Balas', 'Nicholas Edelman', 'Anahita Bhiwandiwalla', 'Muthu Subramaniam', 'Smita Ithape', 'Karthik Ramamoorthy', 'Yuting Wu', 'Suguna Varshini Velury', 'Omri Almog', 'Joyjit Daw', 'Denys Fridman', 'Erick Galinkin', 'Michael Evans', 'Katherine Luna', 'Leon Derczynski', 'Nikki Pope', 'Eileen Long', 'Seth Schneider', 'Guillermo Siman', 'Tomasz Grzegorzek', 'Pablo Ribalta', 'Monika Katariya', 'Joey Conway', 'Trisha Saar', 'Ann Guan', 'Krzysztof Pawelec', 'Shyamala Prayaga', 'Oleksii Kuchaiev', 'Boris Ginsburg', 'Oluwatobi Olabiyi', 'Kari Briski', 'Jonathan Cohen', 'Bryan Catanzaro', 'Jonah Alben', 'Yonatan Geifman', 'Eric Chung'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00949.jpg', 'data': {'categories': ['#agi', '#training', '#rl', '#open_source', '#architecture', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Открытые модели рассуждений нового поколения', 'desc': 'Представлена серия моделей Llama-Nemotron - семейство гетерогенных моделей рассуждений с открытым исходным кодом. Модели доступны в трех размерах (8B, 49B, 253B) и обеспечивают высокую производительность и эффективность использования памяти. Обучение включает нейроархитектурный поиск, дистилляцию знаний и дообучение, а также этап постобработки с акцентом на рассуждения. Модели поддерживают динамическое переключение между режимами обычного чата и рассуждений.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Efficiency', 'desc': 'The Llama-Nemotron series introduces a new family of reasoning models designed for efficient inference and strong reasoning capabilities. These models come in three sizes, allowing flexibility for different applications while maintaining competitive performance against leading models. The training process involves advanced techniques like neural architecture search, knowledge distillation, and reinforcement learning to enhance reasoning abilities. Additionally, these models are open-source, providing resources for further research and development in the machine learning community.'}, 'zh': {'title': '开放推理模型，提升推理效率！', 'desc': 'Llama-Nemotron系列模型是一种开放的异构推理模型，具有卓越的推理能力和高效的推理性能。该系列包括三种不同规模的模型：Nano（8B）、Super（49B）和Ultra（253B），在推理速度和内存效率上优于现有的最先进模型。模型的训练过程采用了神经架构搜索、知识蒸馏和持续预训练，最后通过监督微调和大规模强化学习进行推理专注的后训练阶段。Llama-Nemotron模型是首个支持动态推理切换的开源模型，用户可以在推理过程中在标准聊天模式和推理模式之间切换。'}}}, {'id': 'https://huggingface.co/papers/2505.00174', 'title': 'Real-World Gaps in AI Governance Research', 'url': 'https://huggingface.co/papers/2505.00174', 'abstract': 'Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.', 'score': 8, 'issue_id': 3582, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '7618edbafcee6b13', 'authors': ['Ilan Strauss', 'Isobel Moure', "Tim O'Reilly", 'Sruly Rosenblat'], 'affiliations': ['AI Disclosures Project, Social Science Research Council', 'Institute for Innovation and Public Purpose, University College London', 'OReilly Media'], 'pdf_title_img': 'assets/pdf/title_img/2505.00174.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#alignment', '#healthcare', '#hallucinations', '#data'], 'emoji': '🔍', 'ru': {'title': 'Корпоративные исследования ИИ: пробелы в безопасности и необходимость прозрачности', 'desc': 'Статья анализирует 1178 работ по безопасности и надежности из 9439 статей по генеративному ИИ за период с января 2020 по март 2025 года. Исследователи сравнивают результаты ведущих компаний и университетов в области ИИ. Обнаружено, что корпоративные исследования ИИ все больше концентрируются на предварительном развертывании, включая выравнивание моделей и тестирование, в то время как внимание к проблемам этапа развертывания, таким как смещение модели, ослабевает. Авторы рекомендуют расширить доступ внешних исследователей к данным развертывания и систематическое наблюдение за поведением ИИ на рынке.'}, 'en': {'title': 'Bridging the Gap: Enhancing AI Safety in Deployment', 'desc': 'This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps.'}, 'zh': {'title': '关注人工智能部署阶段的研究缺口', 'desc': '本研究分析了1178篇安全性和可靠性论文与9439篇生成式人工智能论文，比较了主要人工智能公司和大学的研究成果。研究发现，企业的人工智能研究越来越集中在模型对齐和测试评估等预部署领域，而对部署阶段问题如模型偏见的关注有所减少。高风险部署领域（如医疗、金融、虚假信息等）存在显著的研究空白。为了改善对已部署人工智能的可观察性，建议扩大外部研究人员对部署数据的访问，并系统化市场中人工智能行为的可观察性。'}}}, {'id': 'https://huggingface.co/papers/2505.00023', 'title': 'CORG: Generating Answers from Complex, Interrelated Contexts', 'url': 'https://huggingface.co/papers/2505.00023', 'abstract': 'In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.', 'score': 5, 'issue_id': 3582, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '46da290a5c894311', 'authors': ['Hyunji Lee', 'Franck Dernoncourt', 'Trung Bui', 'Seunghyun Yoon'], 'affiliations': ['Adobe Research', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.00023.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#graphs', '#architecture', '#data'], 'emoji': '🧠', 'ru': {'title': 'CORG: Умная организация контекста для улучшения работы языковых моделей', 'desc': 'Статья представляет новый фреймворк под названием Context Organizer (CORG) для обработки сложных взаимосвязей между контекстами в корпусах реального мира. CORG организует множественные контексты в независимо обрабатываемые группы, что позволяет эффективно находить все релевантные ответы и обеспечивать устранение неоднозначности. Фреймворк состоит из трех ключевых компонентов: конструктора графов, ранжировщика и агрегатора. Результаты показывают, что CORG эффективно балансирует производительность и эффективность, превосходя существующие методы группировки.'}, 'en': {'title': 'Organizing Contexts for Better Language Understanding', 'desc': 'This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods.'}, 'zh': {'title': '上下文组织，提升模型效率与准确性', 'desc': '在现实世界的语料库中，知识经常在文档中重复出现，但由于命名模糊、信息过时或错误，导致上下文之间存在复杂的相互关系。以往的研究表明，语言模型在处理这些复杂性时通常只关注单一因素。我们将这些关系分为四种类型：干扰、模糊、反事实和重复。为了解决这些问题，我们提出了上下文组织器（CORG），它将多个上下文组织成独立处理的组，从而提高模型的效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.00562', 'title': 'TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching', 'url': 'https://huggingface.co/papers/2505.00562', 'abstract': "Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'bf5b246f5848fa6e', 'authors': ['Yue Meng', 'Chuchu Fan'], 'affiliations': ['Department of Aeronautics and Astronautics, MIT, Cambridge, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00562.jpg', 'data': {'categories': ['#dataset', '#inference', '#agents', '#robotics', '#graphs', '#optimization'], 'emoji': '⏱️', 'ru': {'title': 'Графовые нейросети для эффективного решения задач темпоральной логики', 'desc': 'Статья представляет TeLoGraF - новый метод для решения задач с темпоральной логикой сигналов (STL). Авторы используют графовые нейронные сети и технику flow-matching для обучения на разнообразном наборе STL-спецификаций. Эксперименты проводились в пяти симуляционных средах, от простых 2D-моделей до сложных роботов. Результаты показывают превосходство TeLoGraF над базовыми методами по скорости и универсальности применения.'}, 'en': {'title': 'TeLoGraF: Fast and Robust Solutions for Complex Temporal Logic Tasks', 'desc': 'This paper introduces TeLoGraF, a novel approach that leverages Graph Neural Networks (GNN) to effectively learn solutions for complex tasks defined by signal temporal logic (STL) specifications. The authors address the limitations of previous methods that relied on fixed STL templates by creating a diverse dataset of 200,000 STL specifications paired with demonstrations. Through extensive experiments across various simulation environments, TeLoGraF demonstrates superior performance in STL satisfaction rates and significantly faster inference times compared to traditional STL planning algorithms. Additionally, the graph-encoding technique shows robustness in handling complex and out-of-distribution STL specifications, making it a versatile tool for real-world applications.'}, 'zh': {'title': 'TeLoGraF：高效解决复杂时序逻辑任务的创新方法', 'desc': '本文提出了一种新的方法TeLoGraF，用于解决复杂任务的信号时序逻辑（STL）规范。我们利用图神经网络（GNN）编码器和流匹配技术，学习通用STL规范的解决方案。通过收集20万个配对示例，我们在多个仿真环境中进行了广泛实验，结果表明该方法在STL满足率上优于其他基线。与传统的STL规划算法相比，我们的方法在推理速度上快10到100倍，并且能够适应任何系统动态。'}}}, {'id': 'https://huggingface.co/papers/2504.20859', 'title': 'X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2504.20859', 'abstract': "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '2102f697cfc2375e', 'authors': ['Guy Hadad', 'Haggai Roitman', 'Yotam Eshel', 'Bracha Shapira', 'Lior Rokach'], 'affiliations': ['Ben-Gurion University of the Negev Beer Sheva, Israel', 'eBay Netanya, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.20859.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#low_resource', '#training', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'X-Cross: эффективные кросс-доменные рекомендации без обширного переобучения', 'desc': 'Статья представляет модель X-Cross для кросс-доменных последовательных рекомендаций, использующую несколько доменно-специфичных языковых моделей с низкоранговыми адаптерами (LoRA). X-Cross динамически улучшает представления каждой исходной языковой модели, интегрируя знания из всех других моделей. Эксперименты на данных Amazon показывают, что X-Cross достигает производительности, сравнимой с моделью, дообученной с LoRA, используя лишь 25% дополнительных параметров. Модель демонстрирует надежную производительность в кросс-доменных задачах, требуя на 50-75% меньше данных для эффективной донастройки.'}, 'en': {'title': 'X-Cross: Efficient Cross-Domain Recommendations with Minimal Data', 'desc': "The paper introduces 'X-Cross', a new model designed for cross-domain sequential recommendations that can quickly adapt to new product categories without extensive retraining. It utilizes multiple domain-specific language models, each fine-tuned with low-rank adapters (LoRA), to enhance the recommendation process. By refining the representations of these models layer by layer, X-Cross effectively integrates knowledge from different domains while maintaining their unique characteristics. The model shows strong performance on Amazon datasets, requiring significantly less fine-tuning data and parameters compared to traditional methods, making it efficient for data-limited scenarios."}, 'zh': {'title': 'X-Cross：高效的跨领域推荐解决方案', 'desc': '随着新产品的不断涌现，推荐系统需要快速适应新领域，而无需大量重新训练。本文提出了“X-Cross”模型，这是一种新颖的跨领域序列推荐模型，通过整合多个特定领域的语言模型来推荐新领域的产品。X-Cross通过逐层操作动态地优化每个源语言模型的表示，确保在跨领域适应时保留领域特有的细微差别。实验结果表明，X-Cross在跨领域任务中表现出色，且所需的微调数据量显著低于传统方法。'}}}, {'id': 'https://huggingface.co/papers/2505.09388', 'title': 'Qwen3 Technical Report', 'url': 'https://huggingface.co/papers/2505.09388', 'abstract': 'In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.', 'score': 108, 'issue_id': 3823, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '69a0f87bb5460e8d', 'authors': ['An Yang', 'Anfeng Li', 'Baosong Yang', 'Beichen Zhang', 'Binyuan Hui', 'Bo Zheng', 'Bowen Yu', 'Chang Gao', 'Chengen Huang', 'Chenxu Lv', 'Chujie Zheng', 'Dayiheng Liu', 'Fan Zhou', 'Fei Huang', 'Feng Hu', 'Hao Ge', 'Haoran Wei', 'Huan Lin', 'Jialong Tang', 'Jian Yang', 'Jianhong Tu', 'Jianwei Zhang', 'Jianxin Yang', 'Jiaxi Yang', 'Jing Zhou', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Keqin Bao', 'Kexin Yang', 'Le Yu', 'Lianghao Deng', 'Mei Li', 'Mingfeng Xue', 'Mingze Li', 'Pei Zhang', 'Peng Wang', 'Qin Zhu', 'Rui Men', 'Ruize Gao', 'Shixuan Liu', 'Shuang Luo', 'Tianhao Li', 'Tianyi Tang', 'Wenbiao Yin', 'Xingzhang Ren', 'Xinyu Wang', 'Xinyu Zhang', 'Xuancheng Ren', 'Yang Fan', 'Yang Su', 'Yichang Zhang', 'Yinger Zhang', 'Yu Wan', 'Yuqiong Liu', 'Zekun Wang', 'Zeyu Cui', 'Zhenru Zhang', 'Zhipeng Zhou', 'Zihan Qiu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.09388.jpg', 'data': {'categories': ['#low_resource', '#agi', '#reasoning', '#multilingual', '#benchmark', '#architecture', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Qwen3: Единая модель для мышления и быстрых ответов', 'desc': 'Qwen3 - это новое семейство больших языковых моделей (LLM), разработанное для улучшения производительности, эффективности и многоязычных возможностей. Ключевой инновацией Qwen3 является интеграция режима мышления и режима без мышления в единую структуру, что позволяет динамически переключаться между ними. Модель вводит механизм бюджета мышления, позволяющий адаптивно распределять вычислительные ресурсы во время вывода. Qwen3 достигает передовых результатов в различных бенчмарках и поддерживает 119 языков и диалектов.'}, 'en': {'title': 'Qwen3: Unifying Thinking and Efficiency in Language Models', 'desc': 'Qwen3 is the latest version of the Qwen model family, featuring large language models that enhance performance, efficiency, and multilingual capabilities. It includes both dense and Mixture-of-Expert (MoE) architectures with a wide range of parameters, from 0.6 to 235 billion. A notable innovation is the integration of thinking and non-thinking modes, allowing for seamless dynamic switching based on user needs, which improves response times and reasoning capabilities. Additionally, Qwen3 supports 119 languages, significantly increasing its accessibility and effectiveness in diverse applications, while also providing a thinking budget mechanism for optimized resource allocation during inference.'}, 'zh': {'title': 'Qwen3：统一思维与响应的智能语言模型', 'desc': '本文介绍了Qwen3，这是Qwen模型系列的最新版本。Qwen3包含一系列大型语言模型，旨在提高性能、效率和多语言能力。其创新之处在于将思维模式和非思维模式整合到一个统一框架中，实现动态模式切换，适应用户查询。Qwen3还引入了思维预算机制，允许用户在推理过程中自适应分配计算资源，从而在任务复杂性基础上平衡延迟和性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11049', 'title': 'GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning', 'url': 'https://huggingface.co/papers/2505.11049', 'abstract': "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/", 'score': 41, 'issue_id': 3829, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'bedca054f1392a71', 'authors': ['Yue Liu', 'Shengfang Zhai', 'Mingzhe Du', 'Yulin Chen', 'Tri Cao', 'Hongcheng Gao', 'Cheng Wang', 'Xinfeng Li', 'Kun Wang', 'Junfeng Fang', 'Jiaheng Zhang', 'Bryan Hooi'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.11049.jpg', 'data': {'categories': ['#multimodal', '#rl', '#dataset', '#reasoning', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Рассуждающий страж для безопасных визуально-языковых моделей', 'desc': 'Эта статья представляет GuardReasoner-VL - новую модель-охранник для визуально-языковых моделей (VLM), использующую рассуждения для повышения безопасности. Модель обучается с помощью онлайн-обучения с подкреплением (RL) на специально созданном корпусе GuardReasoner-VLTrain, содержащем 123 тысячи образцов с рассуждениями. Авторы применяют несколько техник для улучшения обучения, включая отбор образцов, аугментацию данных и динамическую настройку параметров исследования. Эксперименты показывают, что GuardReasoner-VL превосходит другие модели, улучшая F1-меру на 19.27% в среднем.'}, 'en': {'title': 'GuardReasoner-VL: Enhancing VLM Safety through Reasoning and Reinforcement Learning', 'desc': 'This paper presents GuardReasoner-VL, a new model designed to improve the safety of Vision-Language Models (VLMs) by incorporating reasoning into moderation decisions. The model is trained using a large reasoning corpus that includes diverse text and image inputs, allowing it to learn from 123K samples and 631K reasoning steps. To enhance its reasoning capabilities, the model employs supervised fine-tuning (SFT) and online reinforcement learning (RL), which helps it adapt and improve over time. The results show that GuardReasoner-VL significantly outperforms existing models, achieving a 19.27% higher F1 score on average, demonstrating its effectiveness in ensuring safer VLM operations.'}, 'zh': {'title': '推理驱动的安全保护：GuardReasoner-VL', 'desc': '为了提高视觉语言模型（VLM）的安全性，本文提出了一种新的基于推理的VLM保护模型，称为GuardReasoner-VL。该模型通过在线强化学习（RL）激励保护模型在做出审查决策之前进行深思熟虑的推理。我们构建了一个包含123K样本和631K推理步骤的推理语料库，并通过监督微调（SFT）来冷启动模型的推理能力。此外，我们通过在线RL进一步增强了审查推理的能力，最终实验结果显示该模型在F1分数上平均超越了第二名19.27%。'}}}, {'id': 'https://huggingface.co/papers/2505.10610', 'title': 'MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly', 'url': 'https://huggingface.co/papers/2505.10610', 'abstract': "The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.", 'score': 39, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '565f788b384ce66a', 'authors': ['Zhaowei Wang', 'Wenhao Yu', 'Xiyu Ren', 'Jipeng Zhang', 'Yu Zhao', 'Rohit Saxena', 'Liang Cheng', 'Ginny Wong', 'Simon See', 'Pasquale Minervini', 'Yangqiu Song', 'Mark Steedman'], 'affiliations': ['CSE Department, HKUST', 'Miniml.AI', 'NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA', 'Tencent AI Seattle Lab', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10610.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#long_context'], 'emoji': '📏', 'ru': {'title': 'MMLongBench: новый стандарт оценки мультимодальных моделей с длинным контекстом', 'desc': 'В работе представлен MMLongBench - первый бенчмарк для оценки моделей обработки длинных контекстов в области компьютерного зрения и языка (LCVLM). Бенчмарк включает 13,331 примеров из 5 категорий задач, охватывающих различные типы изображений. Оценка проводится на стандартизированных входных последовательностях длиной от 8K до 128K токенов. Результаты тестирования 46 моделей показали значительный потенциал для улучшения в задачах обработки длинных мультимодальных контекстов.'}, 'en': {'title': 'MMLongBench: Advancing Long-Context Vision-Language Models', 'desc': 'This paper introduces MMLongBench, a new benchmark designed to evaluate long-context vision-language models (LCVLMs) that can process large amounts of images and text simultaneously. It includes 13,331 examples across five task categories, ensuring a diverse assessment of model performance. The benchmark tests models at various input lengths, allowing for a detailed analysis of their capabilities in handling long-context tasks. The findings reveal that current models struggle with long-context challenges, highlighting the need for improvements and suggesting that models with better reasoning skills perform better in these scenarios.'}, 'zh': {'title': 'MMLongBench：长上下文视觉语言模型的评估新基准', 'desc': '本文介绍了MMLongBench，这是第一个针对长上下文视觉语言模型（LCVLMs）的基准测试，旨在全面评估这些模型的性能。MMLongBench包含13331个示例，涵盖五种不同类别的下游任务，并提供多种自然和合成图像类型。通过对46个闭源和开源LCVLM的深入评估，研究发现单一任务的表现并不能有效反映整体的长上下文能力。结果表明，模型的推理能力越强，长上下文表现越好，未来在这一领域仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2505.11409', 'title': "Visual Planning: Let's Think Only with Images", 'url': 'https://huggingface.co/papers/2505.11409', 'abstract': 'Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.', 'score': 30, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '67875b7838a7b7ea', 'authors': ['Yi Xu', 'Chengzu Li', 'Han Zhou', 'Xingchen Wan', 'Caiqi Zhang', 'Anna Korhonen', 'Ivan Vulić'], 'affiliations': ['Google', 'Language Technology Lab, University of Cambridge', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.11409.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Визуальное планирование: новый подход к машинному рассуждению без слов', 'desc': "Статья представляет новую парадигму под названием 'Визуальное планирование', которая позволяет осуществлять планирование с помощью чисто визуальных представлений, без использования текста. Авторы предлагают фреймворк обучения с подкреплением VPRL, усиленный методом GRPO для дообучения больших моделей компьютерного зрения. Эксперименты показывают, что визуальное планирование превосходит текстовые методы рассуждений в задачах визуальной навигации. Результаты открывают новые возможности для задач, требующих интуитивного, основанного на изображениях вывода."}, 'en': {'title': 'Visual Planning: Reasoning Beyond Text', 'desc': 'This paper introduces a new approach called Visual Planning, which focuses on using visual representations for reasoning instead of relying solely on text. The authors argue that for tasks involving spatial and geometrical information, visual reasoning can be more effective. They present a reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), which enhances planning capabilities in visual navigation tasks. The results show that this visual approach outperforms traditional text-based reasoning methods, suggesting a shift towards image-based inference in machine learning applications.'}, 'zh': {'title': '视觉规划：超越文本的推理新范式', 'desc': '最近，大型语言模型（LLMs）和多模态扩展（MLLMs）的进展显著提升了机器推理能力。然而，这些模型主要依赖纯文本来表达和构建推理，即使在存在视觉信息的情况下。我们提出了一种新的范式，称为视觉规划，允许通过纯视觉表示进行规划，而不依赖文本。我们的研究表明，视觉规划在处理空间和几何信息的任务中，比基于语言的推理更有效。'}}}, {'id': 'https://huggingface.co/papers/2505.11107', 'title': 'Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity', 'url': 'https://huggingface.co/papers/2505.11107', 'abstract': "Recent advances in large language models (LLMs) have demonstrated the power of reasoning through self-generated chains of thought. Multiple reasoning agents can collaborate to raise joint reasoning quality above individual outcomes. However, such agents typically interact in a turn-based manner, trading increased latency for improved quality. In this paper, we propose Group Think--a single LLM that acts as multiple concurrent reasoning agents, or thinkers. With shared visibility into each other's partial generation progress, Group Think introduces a new concurrent-reasoning paradigm in which multiple reasoning trajectories adapt dynamically to one another at the token level. For example, a reasoning thread may shift its generation mid-sentence upon detecting that another thread is better positioned to continue. This fine-grained, token-level collaboration enables Group Think to reduce redundant reasoning and improve quality while achieving significantly lower latency. Moreover, its concurrent nature allows for efficient utilization of idle computational resources, making it especially suitable for edge inference, where very small batch size often underutilizes local~GPUs. We give a simple and generalizable modification that enables any existing LLM to perform Group Think on a local GPU. We also present an evaluation strategy to benchmark reasoning latency and empirically demonstrate latency improvements using open-source LLMs that were not explicitly trained for Group Think. We hope this work paves the way for future LLMs to exhibit more sophisticated and more efficient collaborative behavior for higher quality generation.", 'score': 15, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'b2ea7382367e75ba', 'authors': ['Chan-Jan Hsu', 'Davide Buffelli', 'Jamie McGowan', 'Feng-Ting Liao', 'Yi-Chang Chen', 'Sattar Vakili', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11107.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#agents', '#training', '#open_source', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Коллективное мышление: параллельные рассуждения в одной языковой модели', 'desc': 'В статье представлен метод Group Think, позволяющий большой языковой модели (LLM) действовать как несколько параллельных рассуждающих агентов. Эти агенты могут динамически адаптировать свои рассуждения на уровне токенов, что позволяет снизить избыточность и повысить качество при значительно меньшей задержке. Метод особенно подходит для периферийных вычислений, где часто недоиспользуются локальные GPU. Авторы предлагают простую модификацию, позволяющую любой существующей LLM выполнять Group Think на локальном GPU.'}, 'en': {'title': 'Group Think: Collaborative Reasoning for Faster, Smarter LLMs', 'desc': "This paper introduces Group Think, a novel approach that allows a single large language model (LLM) to function as multiple reasoning agents working together simultaneously. By enabling these agents to share visibility into each other's progress, they can dynamically adjust their reasoning paths at the token level, enhancing the overall quality of the output. This concurrent reasoning reduces redundancy and improves efficiency, leading to lower latency compared to traditional turn-based interactions. The authors also provide a method to adapt existing LLMs for Group Think, demonstrating its effectiveness through empirical evaluations."}, 'zh': {'title': 'Group Think：提升推理质量的新方法', 'desc': '本文提出了一种名为Group Think的新方法，它利用单个大型语言模型（LLM）作为多个并发推理代理。通过共享彼此的生成进度，这种方法允许推理线程在生成过程中动态适应，从而减少冗余推理并提高生成质量。Group Think的并发特性使得计算资源得到更有效的利用，特别适合边缘推理场景。我们还提供了一种简单的修改方法，使现有的LLM能够在本地GPU上实现Group Think。'}}}, {'id': 'https://huggingface.co/papers/2505.07675', 'title': 'Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization', 'url': 'https://huggingface.co/papers/2505.07675', 'abstract': 'Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose texttt{D}ual-texttt{H}ead texttt{O}ptimization (texttt{DHO}) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that DHO mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that DHO consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.', 'score': 13, 'issue_id': 3828, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '73f4f4dd13e67a25', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07675.jpg', 'data': {'categories': ['#small_models', '#optimization', '#transfer_learning', '#training', '#cv', '#data'], 'emoji': '🧠', 'ru': {'title': 'DHO: Эффективная дистилляция знаний для компактных моделей компьютерного зрения', 'desc': 'Статья представляет новый метод дистилляции знаний под названием Dual-Head Optimization (DHO) для эффективной передачи знаний от крупных визуально-языковых моделей (VLM) к компактным моделям для конкретных задач. DHO использует две независимые головы предсказания, которые обучаются на размеченных данных и предсказаниях учителя соответственно. Этот подход позволяет избежать конфликтов градиентов между сигналами обучения с учителем и без него, что приводит к более эффективному обучению признаков. Эксперименты показывают, что DHO превосходит базовые методы на различных наборах данных, достигая лучших результатов на ImageNet при использовании меньшего количества параметров.'}, 'en': {'title': 'Streamlining Knowledge Distillation with Dual-Head Optimization', 'desc': 'This paper introduces a new framework called Dual-Head Optimization (DHO) for knowledge distillation from vision-language models (VLMs) to smaller, task-specific models. DHO uses two prediction heads that learn from both labeled data and the predictions of a larger teacher model, which helps to reduce conflicts in learning signals. The method simplifies the distillation process, avoiding the complexity of multi-stage training while still improving feature learning. Experiments show that DHO outperforms existing methods, achieving better accuracy on datasets like ImageNet with fewer parameters.'}, 'zh': {'title': 'DHO：高效的知识蒸馏框架', 'desc': '本文提出了一种名为DHO的知识蒸馏框架，旨在将视觉语言模型（VLMs）的知识转移到紧凑的任务特定模型中。DHO通过引入双预测头，分别从标记数据和教师预测中独立学习，并在推理时线性组合它们的输出。实验结果表明，DHO有效缓解了监督信号和蒸馏信号之间的梯度冲突，从而实现了比单头知识蒸馏基线更有效的特征学习。最终，DHO在多个领域和细粒度数据集上均表现优异，尤其在ImageNet上，使用更少的参数实现了3%的准确率提升。'}}}, {'id': 'https://huggingface.co/papers/2505.11427', 'title': 'Mergenetic: a Simple Evolutionary Model Merging Library', 'url': 'https://huggingface.co/papers/2505.11427', 'abstract': 'Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.', 'score': 10, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '759eb3fbdec85844', 'authors': ['Adrian Robert Minut', 'Tommaso Mencattini', 'Andrea Santilli', 'Donato Crisostomi', 'Emanuele Rodolà'], 'affiliations': ['Ecole Polytechnique Fédérale de Lausanne', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2505.11427.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Эволюционное слияние языковых моделей без переобучения', 'desc': 'Статья представляет библиотеку Mergenetic для эволюционного объединения языковых моделей. Она позволяет комбинировать существующие модели без дополнительного обучения, используя эволюционные алгоритмы. Mergenetic поддерживает гибкое экспериментирование с различными стратегиями объединения и включает легковесные оценщики качества для снижения вычислительных затрат. Результаты показывают конкурентоспособность подхода на различных задачах и языках при использовании скромных вычислительных ресурсов.'}, 'en': {'title': 'Mergenetic: Evolving Better Models Through Merging', 'desc': 'This paper presents Mergenetic, an open-source library designed for evolutionary model merging in machine learning. Model merging allows the combination of existing models into a new one without the need for additional training, making it cost-effective and efficient. Mergenetic enhances this process by integrating evolutionary algorithms, which can improve model performance. The library also includes lightweight fitness estimators to minimize evaluation costs, demonstrating competitive results across various tasks and languages using standard hardware.'}, 'zh': {'title': 'Mergenetic：进化模型合并的新选择', 'desc': '模型合并是一种将现有模型的能力结合成新模型的方法，无需额外训练。这种方法因其低成本和支持消费者GPU的库而越来越受欢迎。最近的研究表明，将合并与进化算法结合可以提高性能，但目前没有框架支持在语言模型中灵活实验这些策略。我们介绍了Mergenetic，这是一个开源库，用于进化模型合并，能够轻松组合合并方法和进化算法，同时引入轻量级的适应度评估器以降低评估成本。'}}}, {'id': 'https://huggingface.co/papers/2505.10518', 'title': 'Multi-Token Prediction Needs Registers', 'url': 'https://huggingface.co/papers/2505.10518', 'abstract': 'Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.', 'score': 9, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '10aef9838701ab6e', 'authors': ['Anastasios Gerontopoulos', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ['Archimedes, Athena Research Center', 'IACM-Forth', 'University of Crete', 'valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2505.10518.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization'], 'emoji': '🔮', 'ru': {'title': 'MuToR: Эффективное многомаркерное предсказание без изменения архитектуры модели', 'desc': 'В статье представлен новый подход к многомаркерному предсказанию в языковых моделях под названием MuToR. Этот метод вставляет обучаемые токены-регистры в входную последовательность для предсказания будущих целей. MuToR имеет ряд преимуществ: минимальное количество дополнительных параметров, совместимость с существующими предобученными моделями и масштабируемые горизонты предсказания. Эффективность MuToR продемонстрирована на различных задачах в области обработки естественного языка и компьютерного зрения.'}, 'en': {'title': 'MuToR: Enhancing Multi-Token Prediction for Language Models', 'desc': "This paper introduces MuToR, a novel approach to multi-token prediction that enhances language model pretraining. MuToR integrates learnable register tokens into the input sequence, allowing the model to predict multiple future tokens effectively. It maintains compatibility with existing pretrained models without requiring architectural changes and adds minimal parameters. The authors demonstrate MuToR's effectiveness across various tasks in both language and vision, showcasing its versatility in supervised and parameter-efficient fine-tuning."}, 'zh': {'title': 'MuToR：高效的多标记预测方法', 'desc': '多标记预测是一种有前景的目标，用于改善语言模型的预训练，但其优势在其他设置（如微调）中并不总是有效。本文提出了一种简单有效的多标记预测方法MuToR，它将可学习的注册标记交错到输入序列中，每个标记负责预测未来的目标。与现有方法相比，MuToR具有几个关键优势：仅引入极少的额外参数，不需要架构更改，确保与现成的预训练语言模型兼容，并且与下一个标记的预训练目标保持一致，特别适合监督微调。此外，它自然支持可扩展的预测范围。'}}}, {'id': 'https://huggingface.co/papers/2505.10962', 'title': 'MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation', 'url': 'https://huggingface.co/papers/2505.10962', 'abstract': 'Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.', 'score': 7, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '07990204af30ff71', 'authors': ['Zhenwen Liang', 'Linfeng Song', 'Yang Li', 'Tao Yang', 'Feng Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'Tencent LLM Department'], 'pdf_title_img': 'assets/pdf/title_img/2505.10962.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#reasoning', '#data', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Многоперспективный поиск для прорыва в автоматическом доказательстве теорем', 'desc': 'Эта статья представляет инновационную систему автоматического доказательства теорем под названием MPS-Prover. Система использует две ключевые инновации: эффективную стратегию отбора данных после обучения и механизм поиска с множественными перспективами. MPS-Prover достигает наилучших результатов на нескольких сложных эталонных тестах, превосходя предыдущие модели с 7 миллиардами параметров. Анализ показывает, что MPS-Prover генерирует значительно более короткие и разнообразные доказательства по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Multi-Perspective Search', 'desc': 'This paper presents the Multi-Perspective Search Prover (MPS-Prover), a new system for Automated Theorem Proving (ATP) that addresses inefficiencies in existing stepwise provers. MPS-Prover utilizes a post-training data curation strategy to eliminate redundant training data, improving performance without loss of quality. It also features a multi-perspective tree search that combines a learned critic model with heuristic rules to enhance search diversity and prevent unproductive paths. The results show that MPS-Prover not only achieves state-of-the-art performance on various benchmarks but also produces shorter and more diverse proofs than previous models.'}, 'zh': {'title': '多视角搜索，提升定理证明效率', 'desc': '自动定理证明（ATP）在形式语言中仍然是人工智能中的一大挑战，需要严格的逻辑推理和广泛的搜索空间。虽然大型语言模型（LLMs）表现出良好的性能，但现有的逐步证明器常常受到偏见搜索指导的影响，导致效率低下和次优的证明策略。本文介绍了一种新颖的逐步ATP系统——多视角搜索证明器（MPS-Prover），旨在克服这些局限性。MPS-Prover结合了高效的后训练数据整理策略和多视角树搜索机制，显著提高了证明的效率和多样性。'}}}, {'id': 'https://huggingface.co/papers/2505.11140', 'title': 'Scaling Reasoning can Improve Factuality in Large Language Models', 'url': 'https://huggingface.co/papers/2505.11140', 'abstract': 'Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.', 'score': 5, 'issue_id': 3827, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '29d6b0a8040db2ff', 'authors': ['Mike Zhang', 'Johannes Bjerva', 'Russa Biswas'], 'affiliations': ['Department of Computer Science Aalborg University Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2505.11140.jpg', 'data': {'categories': ['#reasoning', '#graphs', '#dataset', '#benchmark', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Длительные рассуждения улучшают точность ответов языковых моделей', 'desc': 'Исследование посвящено изучению влияния длительного процесса рассуждений на точность ответов больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска. Авторы провели эксперименты с различными моделями, обогащая цепочки рассуждений фактической информацией из графов знаний. Результаты показывают, что меньшие модели рассуждений достигают заметных улучшений в фактической точности по сравнению с исходными инструктированными аналогами. Добавление вычислительных ресурсов и увеличение лимита токенов во время тестирования последовательно улучшает фактическую точность на 2-8%.'}, 'en': {'title': 'Enhancing LLM Reasoning with Knowledge and Compute', 'desc': 'This paper investigates the reasoning capabilities of large language models (LLMs) in open-domain question-answering tasks. It analyzes how longer reasoning processes and additional computational resources can impact factual accuracy, especially beyond mathematical reasoning. The authors fine-tune various models and incorporate knowledge graph information to enhance reasoning traces. Their experiments reveal that smaller models can achieve better factual accuracy than larger, instruction-tuned models, and that increasing computational resources during inference can further improve performance.'}, 'zh': {'title': '提升推理准确性的关键在于模型与资源的结合', 'desc': '本研究探讨了大型语言模型（LLM）在复杂开放领域问答（QA）场景中的推理能力。我们从先进的推理模型中提取推理轨迹，并对多种模型进行微调，以提高其推理准确性。通过引入知识图谱中的事实信息，我们丰富了推理轨迹，并在多个数据集上进行了广泛的实验。结果表明，较小的推理模型在事实准确性上有显著提升，而在测试时增加计算资源和令牌预算也能进一步提高准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.11011', 'title': 'Humans expect rationality and cooperation from LLM opponents in\n  strategic games', 'url': 'https://huggingface.co/papers/2505.11011', 'abstract': "As Large Language Models (LLMs) integrate into our social and economic interactions, we need to deepen our understanding of how humans respond to LLMs opponents in strategic settings. We present the results of the first controlled monetarily-incentivised laboratory experiment looking at differences in human behaviour in a multi-player p-beauty contest against other humans and LLMs. We use a within-subject design in order to compare behaviour at the individual level. We show that, in this environment, human subjects choose significantly lower numbers when playing against LLMs than humans, which is mainly driven by the increased prevalence of `zero' Nash-equilibrium choices. This shift is mainly driven by subjects with high strategic reasoning ability. Subjects who play the zero Nash-equilibrium choice motivate their strategy by appealing to perceived LLM's reasoning ability and, unexpectedly, propensity towards cooperation. Our findings provide foundational insights into the multi-player human-LLM interaction in simultaneous choice games, uncover heterogeneities in both subjects' behaviour and beliefs about LLM's play when playing against them, and suggest important implications for mechanism design in mixed human-LLM systems.", 'score': 4, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '0f12554b6b3b2b83', 'authors': ['Darija Barak', 'Miguel Costa-Gomes'], 'affiliations': ['School of Economics University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.11011.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#games'], 'emoji': '🤖', 'ru': {'title': 'Люди vs ИИ: новые стратегии в играх с искусственным интеллектом', 'desc': "Это исследование изучает поведение людей в стратегической игре p-beauty contest против других людей и больших языковых моделей (LLM). Эксперимент показал, что участники выбирают значительно меньшие числа при игре против LLM, чем против людей, что объясняется более частым выбором равновесия Нэша 'ноль'. Такое поведение в основном наблюдается у участников с высокими способностями к стратегическому мышлению. Результаты дают важные insights о взаимодействии человека и LLM в играх с одновременным выбором и имеют значение для проектирования механизмов в смешанных человеко-LLM системах."}, 'en': {'title': 'Understanding Human Behavior in Games Against LLMs', 'desc': "This paper investigates how humans behave when competing against Large Language Models (LLMs) in strategic games, specifically in a multi-player p-beauty contest. The study reveals that participants tend to choose lower numbers when playing against LLMs compared to human opponents, influenced by the perception of LLMs' reasoning capabilities. The results indicate that individuals with strong strategic reasoning are more likely to adopt a 'zero' Nash-equilibrium strategy, believing it aligns with the LLM's cooperative tendencies. These findings highlight the complexities of human-LLM interactions and their implications for designing effective systems that integrate both human and LLM participants."}, 'zh': {'title': '人类与大型语言模型的战略互动新视角', 'desc': '本研究探讨了人类在与大型语言模型（LLMs）进行战略互动时的行为差异。通过一个受控的实验，我们发现人类在与LLMs对战时选择的数字显著低于与其他人类对战时的选择。这种现象主要是由于高战略推理能力的参与者更倾向于选择零纳什均衡策略。我们的发现为人类与LLMs在多玩家同时选择游戏中的互动提供了基础性见解，并揭示了参与者行为和对LLMs游戏方式的信念差异。'}}}, {'id': 'https://huggingface.co/papers/2505.10852', 'title': 'MatTools: Benchmarking Large Language Models for Materials Science Tools', 'url': 'https://huggingface.co/papers/2505.10852', 'abstract': 'Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.', 'score': 4, 'issue_id': 3831, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'a29f8a9973b15514', 'authors': ['Siyu Liu', 'Jiamin Xu', 'Beilin Ye', 'Bo Hu', 'David J. Srolovitz', 'Tongqi Wen'], 'affiliations': ['Center for Structural Materials, Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.10852.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#science'], 'emoji': '🧪', 'ru': {'title': 'MatTools: бенчмарк для оценки LLM в материаловедении', 'desc': 'Статья представляет MatTools - бенчмарк для оценки способности больших языковых моделей (LLM) отвечать на вопросы в области материаловедения путем генерации и безопасного выполнения кода на основе пакетов вычислительного материаловедения. Бенчмарк состоит из двух компонентов: набора вопросов-ответов по инструментам моделирования материалов и набора реальных задач по использованию этих инструментов. Авторы разработали автоматизированную методологию для сбора примеров реального использования инструментов материаловедения. Оценка различных LLM с помощью MatTools выявила три ключевых вывода: универсальные модели превосходят специализированные, ИИ хорошо разбирается в ИИ, и более простые решения лучше работают.'}, 'en': {'title': 'Evaluating LLMs in Materials Science: Generalists Win!', 'desc': 'This paper introduces MatTools, a benchmark designed to evaluate how well large language models (LLMs) can handle materials science tasks. It combines a question-answer (QA) benchmark with a real-world tool-usage benchmark, assessing LLMs on their ability to generate and execute code for materials property calculations. The QA benchmark includes over 69,000 question-answer pairs derived from existing materials science resources, while the real-world benchmark consists of 49 tasks that require functional Python code generation. The findings suggest that generalist models perform better than specialized ones, that AI can effectively leverage other AI tools, and that simpler approaches yield better results.'}, 'zh': {'title': '评估大型语言模型在材料科学中的应用能力', 'desc': '大型语言模型（LLMs）在材料科学领域的应用越来越广泛，包括文献理解、属性预测、材料发现和合金设计。我们提出了一种基准应用，评估LLMs在材料科学问题上的能力，特别是通过生成和安全执行基于物理的计算材料科学软件包的代码。MatTools由两个互补组件构成：材料模拟工具问答基准和真实工具使用基准。我们的评估结果显示，通用模型优于专业模型，AI能够理解其他AI的能力，并且简单的方法更有效。'}}}, {'id': 'https://huggingface.co/papers/2505.11152', 'title': 'Learning Dense Hand Contact Estimation from Imbalanced Data', 'url': 'https://huggingface.co/papers/2505.11152', 'abstract': 'Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.', 'score': 2, 'issue_id': 3822, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'caa702fa71c24606', 'authors': ['Daniel Sungho Jung', 'Kyoung Mu Lee'], 'affiliations': ['IPAI, Dept. of ECE & ASRI, Seoul National University, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.11152.jpg', 'data': {'categories': ['#training', '#dataset', '#data'], 'emoji': '🖐️', 'ru': {'title': 'Точная оценка контактов рук: преодоление дисбаланса данных', 'desc': 'Эта статья представляет новый подход к оценке плотного контакта рук с окружающей средой, что важно для понимания взаимодействия человека с миром. Авторы разработали фреймворк HACO для обучения на несбалансированных данных, решая проблемы классового и пространственного дисбаланса в наборах данных о контактах рук. Они предложили метод сбалансированной выборки контактов и функцию потерь VCB, учитывающую пространственное распределение контактов на поверхности руки. Результаты показывают эффективность предложенного подхода для точного предсказания плотных контактов рук на основе крупномасштабных данных.'}, 'en': {'title': 'Enhancing Hand Contact Estimation with Balanced Learning Techniques', 'desc': 'This paper addresses the challenge of estimating dense hand contact in various interactions, which is crucial for understanding hand functionality. It identifies two main issues: class imbalance, where most samples do not involve contact, and spatial imbalance, where contact is primarily at the fingertips. To overcome these challenges, the authors propose a framework called HACO that utilizes balanced contact sampling to ensure diverse representation of contact data. Additionally, they introduce a vertex-level class-balanced loss to adjust the learning process based on the frequency of contact across different hand regions, leading to improved predictions in dense hand contact estimation.'}, 'zh': {'title': '提升手部接触估计的准确性', 'desc': '这篇论文探讨了手部接触估计的重要性，尤其是在与物体、其他手、场景和身体的互动中。尽管已有大量高质量的数据集，但如何有效学习密集的手部接触估计仍然是一个未被充分研究的问题。论文提出了一种新的框架，称为HACO，旨在解决类不平衡和空间不平衡的问题。通过引入平衡接触采样和顶点级类平衡损失，研究者们成功地提高了手部接触估计的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.10496', 'title': 'CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs', 'url': 'https://huggingface.co/papers/2505.10496', 'abstract': 'We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'ef303e066da24351', 'authors': ['Raman Dutt', 'Pedro Sanchez', 'Yongchen Yao', 'Steven McDonagh', 'Sotirios A. Tsaftaris', 'Timothy Hospedales'], 'affiliations': ['Samsung AI Center, Cambridge', 'Sinkove', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10496.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#cv', '#open_source', '#benchmark'], 'emoji': '\U0001fa7b', 'ru': {'title': 'Комплексная оценка генерации медицинских изображений: качество, безопасность, применимость', 'desc': 'CheXGenBench представляет собой комплексную систему оценки генерации синтетических рентгеновских снимков грудной клетки. Фреймворк оценивает точность, риски конфиденциальности и клиническую полезность современных генеративных моделей текст-в-изображение. CheXGenBench включает более 20 количественных метрик для анализа качества генерации, потенциальных уязвимостей приватности и клинического применения 11 ведущих архитектур. Авторы также выпустили высококачественный синтетический датасет SynthCheX-75K из 75 000 рентгенограмм, сгенерированных лучшей моделью в их бенчмарке.'}, 'en': {'title': 'Setting New Standards for Synthetic Chest Radiograph Evaluation', 'desc': 'CheXGenBench is a comprehensive evaluation framework designed to assess the generation of synthetic chest radiographs using advanced text-to-image models. It addresses previous challenges in medical image evaluation by providing standardized metrics that measure fidelity, privacy risks, and clinical utility. The framework includes over 20 quantitative metrics and evaluates 11 leading generative architectures, revealing inefficiencies in current evaluation methods. Additionally, it introduces a high-quality synthetic dataset, SynthCheX-75K, to support ongoing research in medical AI.'}, 'zh': {'title': '建立医学AI的新标准评估框架', 'desc': 'CheXGenBench是一个全面的评估框架，用于合成胸部X光图像的生成，评估生成的真实性、隐私风险和临床实用性。该框架解决了以往医学领域评估中的方法不一致、架构比较过时和评估标准脱节等问题。通过标准化的数据划分和统一的评估协议，CheXGenBench使用超过20个定量指标系统分析生成质量和潜在隐私漏洞。我们还发布了一个高质量的合成数据集SynthCheX-75K，包含75,000张由最佳模型生成的X光图像，以支持该领域的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2505.09924', 'title': 'From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models', 'url': 'https://huggingface.co/papers/2505.09924', 'abstract': 'The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark{https://github.com/redwyd/SymMark}.', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '17456d03961632a9', 'authors': ['Yidan Wang', 'Yubing Ren', 'Yanan Cao', 'Binxing Fang'], 'affiliations': ['Hainan Province Fang Binxing Academician Workstation, Hainan, China', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China', 'School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09924.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#security', '#open_source', '#benchmark', '#architecture'], 'emoji': '🔐', 'ru': {'title': 'Симбиотическая защита: новый подход к водяным знакам в текстах ИИ', 'desc': 'Статья посвящена разработке новой системы встраивания водяных знаков в тексты, генерируемые большими языковыми моделями (LLM). Авторы предлагают гибридный подход, объединяющий методы на основе логитов и семплирования, что позволяет достичь лучшего баланса между обнаруживаемостью, устойчивостью, качеством текста и безопасностью. Предложенная система адаптивно встраивает водяные знаки, используя энтропию токенов и семантическую энтропию. Экспериментальные результаты показывают, что метод превосходит существующие базовые линии и достигает наилучших показателей в своей области.'}, 'en': {'title': 'Synergizing Watermarking Techniques for Robust AI Text Security', 'desc': 'This paper addresses the challenges of watermarking Large Language Models (LLMs) to prevent misuse of AI-generated text. It introduces a new framework that combines logits-based and sampling-based watermarking techniques to enhance robustness and text quality while maintaining security. The proposed hybrid approach uses token and semantic entropy to adaptively embed watermarks, optimizing the balance between detectability and performance. Experimental results demonstrate that this method surpasses existing watermarking techniques, achieving state-of-the-art results across various datasets and models.'}, 'zh': {'title': '共生水印框架：优化AI文本安全性与质量的创新方案', 'desc': '随着大型语言模型（LLMs）的兴起，关于AI生成文本滥用的担忧加剧，因此水印技术成为一种有前景的解决方案。现有的水印方案主要分为基于logits和基于采样的两类，但这些方案在鲁棒性、文本质量和安全性之间存在权衡。为了解决这个问题，我们提出了一种多功能的共生水印框架，结合了这两种方案的优点，采用串行、并行和混合三种策略。实验结果表明，我们的方法在多个数据集和模型上超越了现有基准，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11480', 'title': 'Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.11480', 'abstract': 'Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.', 'score': 1, 'issue_id': 3837, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '553b664dc4913a2e', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Huanmi Tan', 'Yinglun Xu', 'Gagandeep Singh', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Carnegie Mellon University', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11480.jpg', 'data': {'categories': ['#rl', '#plp', '#training', '#benchmark', '#rlhf', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'LLM превосходят gcc в оптимизации ассемблера', 'desc': 'Исследование посвящено оптимизации ассемблерного кода с помощью больших языковых моделей (LLM). Авторы разработали систему обучения с подкреплением, использующую алгоритм Proximal Policy Optimization (PPO) для тренировки LLM. Модель оценивается по функциональной корректности и производительности относительно gcc -O3. Результаты показывают, что обученная модель Qwen2.5-Coder-7B-PPO достигает 96% прохождения тестов и среднего ускорения в 1.47 раза по сравнению с базовой линией gcc -O3.'}, 'en': {'title': 'Unlocking LLMs for Assembly Code Optimization', 'desc': 'This paper explores the ability of large language models (LLMs) to optimize assembly code, which allows for precise control over execution. The authors develop a reinforcement learning framework using Proximal Policy Optimization (PPO) to train LLMs, focusing on both correctness and performance improvements. They introduce a benchmark of 8,072 real-world programs to evaluate their model, Qwen2.5-Coder-7B-PPO, which achieves a high test pass rate and significant speedup compared to the standard gcc -O3 compiler. The findings suggest that LLMs, when trained with reinforcement learning, can effectively enhance the performance of assembly code.'}, 'zh': {'title': '强化学习助力大型语言模型优化汇编代码', 'desc': '大型语言模型（LLMs）在编程任务中表现出色，但在代码优化方面的潜力尚未被充分探索。本文研究了LLMs是否能够优化汇编代码的性能，因为汇编语言提供了对执行的细粒度控制。我们提出了一种强化学习框架，使用近端策略优化（PPO）训练LLMs，并通过考虑功能正确性和执行性能的奖励函数进行指导。我们的模型Qwen2.5-Coder-7B-PPO在测试中达到了96.0%的通过率，并且在速度上比行业标准编译器gcc -O3快了1.47倍，显示出强化学习可以有效提升LLMs在汇编代码优化中的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.11493', 'title': 'GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2505.11493', 'abstract': 'Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing.', 'score': 0, 'issue_id': 3838, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'd5f4f14c60b051ec', 'authors': ['Yusu Qian', 'Jiasen Lu', 'Tsu-Jui Fu', 'Xinze Wang', 'Chen Chen', 'Yinfei Yang', 'Wenze Hu', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2505.11493.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'GIE-Bench: Точная оценка редактирования изображений по текстовым инструкциям', 'desc': 'Статья представляет новый бенчмарк для оценки моделей редактирования изображений с помощью текстовых инструкций. Бенчмарк оценивает функциональную корректность изменений и сохранение неизменяемых частей изображения. Он включает более 1000 примеров редактирования с аннотациями и масками объектов. Исследование показало, что модель GPT-Image-1 лидирует в точности следования инструкциям, но часто изменяет нерелевантные области изображения.'}, 'en': {'title': 'Enhancing Evaluation of Text-Guided Image Editing with GIE-Bench', 'desc': 'This paper addresses the challenges in evaluating models that edit images based on natural language instructions. It introduces a new benchmark called GIE-Bench, which assesses models on functional correctness and image content preservation. The benchmark includes a large dataset of editing examples with detailed instructions and evaluation metrics. The study compares the performance of GPT-Image-1 with other models, revealing strengths in instruction-following but weaknesses in preserving non-targeted image areas.'}, 'zh': {'title': '提升文本引导图像编辑的评估准确性', 'desc': '本研究提出了一种新的基准，旨在更准确地评估文本引导的图像编辑模型。我们通过自动生成的多项选择题来评估功能正确性，并使用对象感知掩膜技术确保图像内容的保留。基准包含超过1000个高质量的编辑示例，涵盖20个不同的内容类别，并附有详细的编辑指令和评估问题。我们的研究表明，尽管GPT-Image-1在遵循指令的准确性上表现优异，但在处理无关图像区域时常常过度修改，突显了当前模型行为中的一个关键权衡。'}}}, {'id': 'https://huggingface.co/papers/2505.11315', 'title': 'Improving Inference-Time Optimisation for Vocal Effects Style Transfer\n  with a Gaussian Prior', 'url': 'https://huggingface.co/papers/2505.11315', 'abstract': "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach for transferring the applied effects of a reference audio to a raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including a blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our method's superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems.", 'score': 0, 'issue_id': 3836, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'a8b0aeed9083fba7', 'authors': ['Chin-Yun Yu', 'Marco A. Martínez-Ramírez', 'Junghyun Koo', 'Wei-Hsiang Liao', 'Yuki Mitsufuji', 'György Fazekas'], 'affiliations': ['Centre for Digital Music, Queen Mary University of London, London, UK', 'Sony AI, Tokyo, Japan', 'Sony Group Corporation, Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.11315.jpg', 'data': {'categories': ['#optimization', '#inference', '#audio', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'Байесовская оптимизация улучшает перенос аудиоэффектов', 'desc': 'Статья представляет новый подход к переносу аудиоэффектов с использованием байесовской оптимизации. Авторы предлагают метод ST-ITO с гауссовым априорным распределением, полученным из набора данных вокальных пресетов. Эксперименты показывают значительное улучшение качества переноса эффектов по сравнению с базовыми методами. Субъективная оценка 16 участников подтверждает превосходство предложенного метода, особенно при ограниченных данных.'}, 'en': {'title': 'Enhancing Audio Effects Transfer with Prior Knowledge', 'desc': 'This paper presents a new method called Style Transfer with Inference-Time Optimisation (ST-ITO) for applying audio effects from a reference track to a raw audio track. The approach improves upon previous methods by introducing a Gaussian prior based on a vocal preset dataset, which helps to guide the optimisation of effect parameters. This results in more realistic audio effects by reducing the mean squared error of the parameters and better matching the reference style. The method shows significant improvements in performance metrics and subjective evaluations, especially when data is limited, highlighting the importance of incorporating prior knowledge in audio processing.'}, 'zh': {'title': '引入先验知识，提升音频风格转移效果', 'desc': '本文提出了一种新的音频风格转移方法，称为推理时优化（ST-ITO），旨在将参考音频的效果应用于原始音频轨道。该方法通过优化效果参数，最小化处理后音频与参考音频的风格嵌入之间的距离。然而，传统方法对所有配置的处理是平等的，可能导致不真实或有偏差的结果。我们通过引入基于声乐预设数据集DiffVox的高斯先验，改进了参数空间的优化，从而显著提高了音频效果转移的准确性和效果。'}}}, {'id': 'https://huggingface.co/papers/2505.05678', 'title': 'InstanceGen: Image Generation with Instance-level Instructions', 'url': 'https://huggingface.co/papers/2505.05678', 'abstract': 'Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible fine-grained structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances.', 'score': 0, 'issue_id': 3840, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '201865e7cbd49316', 'authors': ['Etai Sella', 'Yanir Kleiman', 'Hadar Averbuch-Elor'], 'affiliations': ['Cornell University, USA', 'Meta AI, UK', 'Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.05678.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Структурное руководство для точной генерации изображений', 'desc': 'Статья описывает новый подход к улучшению генерации изображений по сложным текстовым запросам. Авторы предлагают использовать структурную инициализацию, полученную непосредственно из современных моделей генерации изображений. Этот метод сочетает структурное руководство на основе изображений с инструкциями на уровне экземпляров, сгенерированными языковыми моделями (LLM). Результатом является генерация изображений, точно соответствующих всем аспектам текстового запроса, включая количество объектов, атрибуты экземпляров и пространственные отношения между ними.'}, 'en': {'title': 'Enhancing Image Generation with Structural Guidance and LLM Instructions', 'desc': "This paper addresses the limitations of current text-to-image generative models in understanding complex prompts that involve multiple objects and their attributes. The authors propose a novel approach that combines structural guidance from existing image generation models with instructions from large language models (LLMs). By doing so, they enhance the model's ability to generate images that accurately reflect the details specified in the prompts, such as the number of objects and their spatial relationships. This technique aims to improve the fidelity and coherence of generated images in response to intricate textual descriptions."}, 'zh': {'title': '结构指导与实例指令结合的图像生成新方法', 'desc': '尽管生成模型的能力迅速提升，但预训练的文本到图像模型在处理复杂提示时仍然存在困难，尤其是涉及多个对象和实例属性时。因此，越来越多的研究者开始关注通过添加结构约束（通常是粗略的边界框）来更好地指导生成过程。在本研究中，我们进一步发展了结构指导的理念，观察到现代图像生成模型可以直接提供合理的细粒度结构初始化。我们提出了一种技术，将基于图像的结构指导与基于大语言模型的实例级指令相结合，从而生成符合文本提示所有部分的输出图像，包括对象数量、实例属性和实例之间的空间关系。'}}}, {'id': 'https://huggingface.co/papers/2505.03318', 'title': 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.03318', 'abstract': "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.", 'score': 63, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f0871f80f0b8fdd9', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.03318.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#multimodal', '#training', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение надежности мультимодальных моделей вознаграждения через цепочки рассуждений', 'desc': 'В статье представлен UnifiedReward-Think - первая унифицированная мультимодальная модель вознаграждения, основанная на цепочках рассуждений (CoT). Модель способна проводить многомерные пошаговые рассуждения для задач визуального понимания и генерации. Авторы используют подход обучения с подкреплением для выявления и стимулирования скрытых способностей модели к сложным рассуждениям. Эксперименты показывают превосходство предложенной модели в различных задачах визуального вознаграждения.'}, 'en': {'title': 'Empowering Vision Models with Long-Chain Reasoning', 'desc': 'This paper introduces UnifiedReward-Think, a novel multimodal reward model that enhances the alignment of vision models with human preferences through long-chain reasoning. By integrating explicit chains of thought (CoT) into the reward reasoning process, the model improves the accuracy and reliability of reward signals. The approach involves a two-step training process: first, distilling reasoning from a small dataset, and then fine-tuning with large-scale multimodal preference data. Experimental results show that this method significantly outperforms existing models in various vision tasks, demonstrating its effectiveness in complex reasoning scenarios.'}, 'zh': {'title': '长链思维提升多模态奖励模型的可靠性', 'desc': '最近在多模态奖励模型（RMs）方面的进展显示出将视觉模型与人类偏好对齐的潜力。然而，目前的RMs通常只能提供直接响应或进行浅层推理，导致奖励信号不准确。我们认为，将明确的长链思维（CoT）纳入奖励推理过程可以显著增强其可靠性和稳健性。本文提出了UnifiedReward-Think，这是第一个统一的基于CoT的多模态奖励模型，能够进行多维度、逐步的长链推理，适用于视觉理解和生成奖励任务。'}}}, {'id': 'https://huggingface.co/papers/2505.03335', 'title': 'Absolute Zero: Reinforced Self-play Reasoning with Zero Data', 'url': 'https://huggingface.co/papers/2505.03335', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.', 'score': 59, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'b53e736d1884218d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rlhf', '#training', '#rl', '#math', '#optimization', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Самообучающийся ИИ: революция в обучении с подкреплением', 'desc': 'Статья представляет новую парадигму обучения с подкреплением с проверяемыми наградами (RLVR) под названием Absolute Zero. В рамках этой парадигмы модель самостоятельно генерирует задачи для максимизации собственного прогресса обучения, не полагаясь на внешние данные. Авторы представляют систему Absolute Zero Reasoner (AZR), которая развивает свою учебную программу и способности к рассуждению, используя исполнитель кода для проверки предложенных задач и ответов. Несмотря на отсутствие внешних данных при обучении, AZR достигает лучших результатов в задачах кодирования и математических рассуждений по сравнению с существующими моделями.'}, 'en': {'title': 'Self-Learning AI: No Data, No Problem!', 'desc': 'This paper introduces a new approach in reinforcement learning called Absolute Zero, which allows a model to learn and improve its reasoning skills without needing external data or human supervision. The proposed Absolute Zero Reasoner (AZR) autonomously generates tasks that enhance its learning and validates its own reasoning through a code executor. This self-sufficient learning method leads to state-of-the-art performance in coding and mathematical reasoning tasks, surpassing models that rely on large datasets of human-created examples. The findings suggest that AZR can adapt to different model sizes and types, showcasing its versatility and potential for future AI development.'}, 'zh': {'title': '绝对零：自我进化的推理模型', 'desc': '强化学习与可验证奖励（RLVR）在提升大型语言模型的推理能力方面表现出色，能够直接从结果导向的奖励中学习。最近的RLVR研究在零设置下运行，避免了对推理过程的监督，但仍依赖于人工策划的问题和答案集合进行训练。由于高质量人类生成示例的稀缺性，依赖人类监督的长期可扩展性受到质疑。为了解决这些问题，我们提出了一种新的RLVR范式，称为绝对零（Absolute Zero），该范式下的模型能够自我提出任务以最大化学习进展，并通过解决这些任务来提升推理能力，而无需依赖任何外部数据。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2505.03005', 'title': 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale', 'url': 'https://huggingface.co/papers/2505.03005', 'abstract': "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper", 'score': 22, 'issue_id': 3625, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '0fe1c0b1575b6708', 'authors': ['Daniel Goldstein', 'Eric Alcaide', 'Janna Lu', 'Eugene Cheah'], 'affiliations': ['Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'EleutherAI', 'George Mason University', 'Recursal AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.03005.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#inference', '#benchmark', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Эффективное преобразование трансформеров в модели с линейным вниманием', 'desc': 'В статье представлен метод Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) для быстрого преобразования трансформеров с софтмакс-вниманием в модели декодеров с линейным вниманием. Авторы разработали две новые архитектуры на основе RWKV и конвертировали популярные модели Qwen2.5 размером 7B, 32B и 72B. Процесс конвертации требует всего 350-700 млн токенов, что составляет менее 0,005% от количества токенов, использованных для обучения исходных моделей. Полученные модели с линейным вниманием демонстрируют высокую производительность на стандартных бенчмарках.'}, 'en': {'title': 'Transforming Transformers: Efficient Linear Attention Models with RADLADS', 'desc': 'The paper introduces Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a method for transforming softmax attention transformers into efficient linear attention models. This conversion process is highly efficient, requiring only a small fraction of the original training data, specifically 350-700M tokens. Despite the reduced training cost, the resulting 72B linear attention model maintains performance levels comparable to its transformer counterparts. The authors also present new RWKV-variant architectures and make their models available on HuggingFace, demonstrating state-of-the-art results on standard benchmarks for linear attention models.'}, 'zh': {'title': '快速转换，线性注意力的未来', 'desc': '我们提出了一种快速注意力蒸馏到线性注意解码器的协议（RADLADS），可以迅速将软最大注意力变换器转换为线性注意解码模型。我们的转换过程只需350-700M个标记，远低于原始教师模型训练所需的0.005%的标记数量。转换为我们的72B线性注意模型的成本不到2000美元，但推理质量仍接近原始变换器。我们在标准基准测试中实现了同类最佳的下游性能，并将所有模型发布在HuggingFace上。'}}}, {'id': 'https://huggingface.co/papers/2505.03730', 'title': 'FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios', 'url': 'https://huggingface.co/papers/2505.03730', 'abstract': 'Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/', 'score': 19, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '2329fe6d2462c9c9', 'authors': ['Shiyi Zhang', 'Junhao Zhuang', 'Zhaoyang Zhang', 'Ying Shan', 'Yansong Tang'], 'affiliations': ['Tencent ARC Lab, China', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03730.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'Гибкий перенос действий между разными субъектами и сценариями', 'desc': 'Статья представляет FlexiAct - метод для переноса действий с референсного видео на произвольное целевое изображение. В отличие от существующих подходов, FlexiAct позволяет варьировать компоновку, ракурс и скелетную структуру между субъектом референсного видео и целевым изображением, сохраняя при этом идентичность. Для достижения этой цели авторы вводят RefAdapter - легковесный адаптер, обусловленный изображением, который превосходит существующие методы в балансировке согласованности внешнего вида и структурной гибкости. Также предлагается FAE (Frequency-aware Action Extraction) для извлечения действий непосредственно в процессе шумоподавления.'}, 'en': {'title': 'FlexiAct: Flexible Action Transfer for Diverse Video Customization', 'desc': 'The paper presents FlexiAct, a novel approach for customizing action videos by transferring actions from a reference video to a target image, regardless of differences in layout, viewpoint, and skeletal structure. This method addresses the limitations of existing techniques that require strict spatial consistency, allowing for greater adaptability across various subjects and scenarios. FlexiAct utilizes a lightweight image-conditioned adapter called RefAdapter to ensure identity consistency while adapting spatial structures. Additionally, it introduces Frequency-aware Action Extraction (FAE) to enhance action extraction during the denoising process, achieving superior results in maintaining both appearance and structural flexibility.'}, 'zh': {'title': '灵活的动作转移，打破空间限制', 'desc': '本文提出了一种名为FlexiAct的方法，用于根据输入控制信号生成视频，允许在不同布局、视角和骨架结构之间进行动作转移。与现有方法不同，FlexiAct能够在保持身份一致性的同时，适应目标图像的空间结构变化。为实现这一目标，文章引入了RefAdapter，一个轻量级的图像条件适配器，能够在外观一致性和结构灵活性之间取得良好平衡。此外，提出的FAE方法在去噪过程中直接提取动作，克服了传统方法的局限性。'}}}, {'id': 'https://huggingface.co/papers/2505.02922', 'title': 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference', 'url': 'https://huggingface.co/papers/2505.02922', 'abstract': 'The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.', 'score': 18, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'd7e3545dcade10b4', 'authors': ['Yaoqi Chen', 'Jinkai Zhang', 'Baotong Lu', 'Qianxi Zhang', 'Chengruidong Zhang', 'Jingjia Luo', 'Di Liu', 'Huiqiang Jiang', 'Qi Chen', 'Jing Liu', 'Bailu Ding', 'Xiao Yan', 'Jiawei Jiang', 'Chen Chen', 'Mingxing Zhang', 'Yuqing Yang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02922.jpg', 'data': {'categories': ['#long_context', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'RetroInfer: революция в эффективности вывода LLM с длинным контекстом', 'desc': 'RetroInfer - это новая система, которая переосмысливает кэш ключ-значение как систему хранения векторов для ускорения вывода LLM с длинным контекстом. В основе системы лежит волновой индекс, использующий разреженность внимания для эффективного извлечения критических токенов. RetroInfer также включает волновой буфер для координации размещения кэша и оптимизации вычислений между GPU и CPU. Эксперименты показывают ускорение до 4.5 раз по сравнению с полным вниманием в пределах памяти GPU и до 10.5 раз по сравнению с базовыми методами разреженного внимания при сохранении точности.'}, 'en': {'title': 'Accelerating Long-Context Inference with RetroInfer', 'desc': 'This paper introduces RetroInfer, a system designed to improve the efficiency of large language models (LLMs) during inference by addressing GPU memory and bandwidth limitations. It innovatively redefines the key-value (KV) cache as a vector storage system that leverages attention sparsity to speed up processing of long contexts. The core component, the wave index, utilizes advanced techniques for token retrieval, ensuring both efficiency and accuracy. Additionally, the wave buffer optimizes the coordination of KV cache and computation, achieving significant speed improvements while maintaining high model accuracy.'}, 'zh': {'title': '高效推理，突破上下文限制！', 'desc': '随着大型语言模型（LLMs）上下文长度的增加，推理效率面临显著挑战，主要是由于GPU内存和带宽的限制。我们提出了RetroInfer，这是一种新颖的系统，将关键值（KV）缓存重新概念化为向量存储系统，利用内在的注意力稀疏性来加速长上下文LLM推理。其核心是波动索引（wave index），一种注意力感知向量索引，能够通过三方注意力近似、精度受限的注意力估计和分段聚类等技术高效准确地检索关键标记。与以往在标记选择和硬件协调上存在困难的稀疏性方法不同，RetroInfer在不影响模型准确性的情况下提供了强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02872', 'title': 'Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading', 'url': 'https://huggingface.co/papers/2505.02872', 'abstract': "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.", 'score': 14, 'issue_id': 3629, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'aae5b0f63189eb32', 'authors': ['Cfir Avraham Hadar', 'Omer Shubi', 'Yoav Meiri', 'Yevgeni Berzak'], 'affiliations': ['Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.02872.jpg', 'data': {'categories': ['#multimodal', '#science', '#dataset', '#benchmark', '#interpretability', '#agi'], 'emoji': '👁️', 'ru': {'title': 'Расшифровка целей чтения по движениям глаз с помощью ИИ', 'desc': 'В этой статье исследуется возможность автоматического определения целей чтения на основе движений глаз. Авторы разработали задачи классификации и реконструкции целей, используя крупномасштабные данные отслеживания движений глаз при чтении на английском языке. Они создали и сравнили несколько дискриминативных и генеративных мультимодальных языковых моделей (LLM), объединяющих движения глаз и текст. Эксперименты показали значительный успех в обеих задачах, что указывает на способность LLM извлекать ценную информацию о целях чтения из движений глаз.'}, 'en': {'title': 'Decoding Reading Goals from Eye Movements with LLMs', 'desc': "This paper explores how eye movements during reading can reveal a reader's specific goals, such as information seeking. The authors introduce new tasks for classifying and reconstructing these goals using large-scale eye tracking data. They develop and evaluate various multimodal large language models (LLMs) that integrate eye movement data with text to improve goal understanding. The results indicate that these models can effectively decode readers' intentions, demonstrating the potential of LLMs in understanding reading behavior."}, 'zh': {'title': '从眼动数据解码阅读目标的创新研究', 'desc': '本研究首次探讨了如何从阅读时的眼动数据自动解码开放式阅读目标。我们引入了目标分类和目标重建任务，并建立了评估框架，使用了大规模的眼动追踪数据。通过结合眼动和文本信息，我们开发并比较了多种判别式和生成式的多模态大语言模型（LLMs）。实验结果表明，这些模型在提取读者的文本特定目标方面表现出显著的成功。'}}}, {'id': 'https://huggingface.co/papers/2505.02214', 'title': 'An Empirical Study of Qwen3 Quantization', 'url': 'https://huggingface.co/papers/2505.02214', 'abstract': "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.", 'score': 12, 'issue_id': 3627, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'be32ffc34f30d354', 'authors': ['Xingyu Zheng', 'Yuye Li', 'Haoran Chu', 'Yue Feng', 'Xudong Ma', 'Jie Luo', 'Jinyang Guo', 'Haotong Qin', 'Michele Magno', 'Xianglong Liu'], 'affiliations': ['Beihang University', 'ETH Zürich', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02214.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source', '#low_resource'], 'emoji': '🔬', 'ru': {'title': 'Квантование Qwen3: баланс между эффективностью и точностью', 'desc': 'Исследование посвящено оценке эффективности квантования модели Qwen3, одной из ведущих открытых больших языковых моделей (LLM). Авторы применили 5 классических методов пост-тренировочного квантования с различной битовой глубиной от 1 до 8 бит. Результаты показали, что Qwen3 сохраняет высокую производительность при умеренном квантовании, но значительно теряет в качестве при сверхнизкой точности. Это исследование подчеркивает необходимость дальнейших разработок для минимизации потерь производительности при экстремальном квантовании LLM.'}, 'en': {'title': "Unlocking Efficiency: Evaluating Qwen3's Performance Under Quantization", 'desc': "The Qwen series represents a significant advancement in open-source Large Language Models (LLMs), particularly with the introduction of Qwen3, which excels in natural language understanding tasks. This paper investigates the effects of low-bit quantization on Qwen3's performance, focusing on how different quantization techniques impact its robustness. By evaluating five classic post-training quantization methods across various bit-widths, the study reveals that while Qwen3 performs well at moderate bit-widths, it struggles with linguistic tasks at ultra-low precision. The findings highlight the challenges of compressing LLMs and suggest the need for further research to improve quantization strategies without sacrificing model accuracy."}, 'zh': {'title': '探索Qwen3的量化挑战与机遇', 'desc': 'Qwen系列是一个领先的开源大型语言模型（LLM），在自然语言理解任务中表现出色。最近发布的Qwen3在多个基准测试中表现优异，吸引了在资源受限环境中高效部署的关注。本文系统评估了Qwen3在不同量化设置下的鲁棒性，探讨了压缩这一先进模型的机遇与挑战。研究发现，尽管在中等位宽下Qwen3的性能仍具竞争力，但在超低精度下语言任务的表现显著下降，强调了LLM压缩中的持续难题。'}}}, {'id': 'https://huggingface.co/papers/2505.03735', 'title': 'Multi-Agent System for Comprehensive Soccer Understanding', 'url': 'https://huggingface.co/papers/2505.03735', 'abstract': 'Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.', 'score': 9, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f62fbd87d3f6f548', 'authors': ['Jiayuan Rao', 'Zifeng Li', 'Haoning Wu', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['SAI, Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03735.jpg', 'data': {'categories': ['#open_source', '#survey', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#agents'], 'emoji': '⚽', 'ru': {'title': 'Революция в ИИ-анализе футбола: от знаний к пониманию', 'desc': 'Статья представляет комплексный подход к пониманию футбола с помощью искусственного интеллекта. Авторы создали SoccerWiki - первую крупномасштабную мультимодальную базу знаний о футболе, и SoccerBench - обширный набор тестовых заданий для оценки понимания футбола ИИ-системами. Также они разработали SoccerAgent - мультиагентную систему, которая декомпозирует сложные вопросы о футболе путем совместных рассуждений. Исследование демонстрирует превосходство предложенного агентного подхода над современными мультимодальными языковыми моделями в задачах понимания футбола.'}, 'en': {'title': 'Revolutionizing Soccer Understanding with AI', 'desc': 'This paper presents a new framework for understanding soccer using AI, addressing the limitations of previous research that focused on narrow tasks. The authors introduce SoccerWiki, a large multimodal knowledge base that contains detailed information about various aspects of soccer, enabling better reasoning. They also create SoccerBench, a comprehensive benchmark with thousands of multimodal question-answer pairs to evaluate soccer understanding tasks. Finally, the paper introduces SoccerAgent, a multi-agent system that collaborates to answer complex soccer questions, demonstrating improved performance through extensive evaluations.'}, 'zh': {'title': '全面提升足球理解的智能框架', 'desc': '本论文提出了一个全面的足球理解框架，以填补现有研究的空白。我们构建了SoccerWiki，这是第一个大规模的多模态足球知识库，整合了关于球员、球队、裁判和场馆的丰富领域知识。我们还推出了SoccerBench，这是最大的足球特定基准，包含约10,000个标准化的多模态多选问答对，涵盖13个不同的理解任务。最后，我们介绍了SoccerAgent，一个新颖的多智能体系统，通过协作推理分解复杂的足球问题，展示了其卓越的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.03739', 'title': 'VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model', 'url': 'https://huggingface.co/papers/2505.03739', 'abstract': 'With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.', 'score': 6, 'issue_id': 3632, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'd1a6985b437b7562', 'authors': ['Zuwei Long', 'Yunhang Shen', 'Chaoyou Fu', 'Heting Gao', 'Lijiang Li', 'Peixian Chen', 'Mengdan Zhang', 'Hang Shao', 'Jian Li', 'Jinlong Peng', 'Haoyu Cao', 'Ke Li', 'Rongrong Ji', 'Xing Sun'], 'affiliations': ['Nanjing University', 'Tencent Youtu Lab', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03739.jpg', 'data': {'categories': ['#training', '#multimodal', '#audio', '#inference', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Революция в речевых технологиях: мгновенная генерация аудио с минимальной задержкой', 'desc': 'VITA-Audio - это инновационная крупномасштабная речевая модель, решающая проблему высокой задержки при генерации первого аудиотокена в потоковых системах. Модель использует легковесный модуль множественного кросс-модального предсказания токенов (MCTP), который эффективно генерирует несколько аудиотокенов за один проход модели. Применяется четырехэтапная стратегия прогрессивного обучения для ускорения модели при минимальной потере качества речи. VITA-Audio демонстрирует ускорение вывода в 3-5 раз и превосходит модели с открытым исходным кодом аналогичного размера по нескольким бенчмаркам для задач ASR, TTS и SQA.'}, 'en': {'title': 'VITA-Audio: Fast, Real-Time Speech Generation for Seamless Interaction', 'desc': 'The paper introduces VITA-Audio, a novel end-to-end speech model designed to enhance real-time human-computer interaction by reducing latency in audio generation. It features a Multiple Cross-modal Token Prediction (MCTP) module that allows for the simultaneous generation of multiple audio tokens, significantly speeding up the inference process. The model employs a four-stage progressive training strategy to maintain high speech quality while achieving faster performance. VITA-Audio stands out as the first multi-modal large language model capable of producing audio output during the initial forward pass, demonstrating a 3-5x speedup compared to existing models while excelling in various speech-related tasks.'}, 'zh': {'title': 'VITA-Audio：实时语音生成的新突破', 'desc': '随着人机交互需求的增加，基于语音的系统受到越来越多的关注。现有的语音模型在生成第一个音频标记时存在高延迟，这限制了其应用。为了解决这个问题，我们提出了VITA-Audio，这是一种端到端的大型语音模型，能够快速生成音频文本标记。我们的模型通过引入轻量级的多模态交叉标记预测模块，显著加快了推理速度，并在流媒体场景中减少了延迟。'}}}, {'id': 'https://huggingface.co/papers/2505.03368', 'title': 'Geospatial Mechanistic Interpretability of Large Language Models', 'url': 'https://huggingface.co/papers/2505.03368', 'abstract': 'Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs\' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.', 'score': 5, 'issue_id': 3629, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '1901718a6f967dbe', 'authors': ['Stef De Sabbata', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Leicester, UK', 'University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2505.03368.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#science', '#data', '#architecture', '#interpretability'], 'emoji': '🌍', 'ru': {'title': 'Заглядывая в мозг ИИ: как языковые модели понимают географию', 'desc': 'Статья исследует внутренние механизмы обработки географической информации в больших языковых моделях (LLM). Авторы предлагают новый подход к геопространственной интерпретируемости LLM, используя методы пространственного анализа. Они применяют пробинг и разреженные автоэнкодеры для выявления внутренних представлений географических данных в моделях. Исследование показывает, что извлеченные признаки для топонимов демонстрируют пространственные паттерны, связанные с их географическим положением.'}, 'en': {'title': 'Unveiling How LLMs Think Geographically', 'desc': 'This paper explores how Large Language Models (LLMs) understand and process geographical information. It introduces a framework for geospatial mechanistic interpretability, which aims to reverse engineer the internal workings of LLMs using spatial analysis. The authors utilize probing techniques and sparse autoencoders to uncover how LLMs represent geographic concepts, revealing patterns in their internal features. By demonstrating spatial autocorrelation in placenames, the study provides insights into the spatial reasoning capabilities of LLMs and their implications for geography.'}, 'zh': {'title': '揭示大型语言模型的地理信息处理机制', 'desc': '大型语言模型（LLMs）在自然语言处理任务中展现了前所未有的能力，尤其是在文本和代码的处理与生成方面。本文提出了一种新框架，旨在研究LLMs如何处理地理信息，特别是其内部机制的可解释性。我们通过空间分析和探测技术，揭示LLMs内部结构，并利用稀疏自编码器将多义性特征分解为更易解释的单义特征。实验结果表明，地名特征与其地理位置之间存在空间相关性，从而为理解LLMs处理地理信息的方式提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2505.03164', 'title': 'InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships', 'url': 'https://huggingface.co/papers/2505.03164', 'abstract': "Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.", 'score': 4, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '377a2c082764680a', 'authors': ['Ji Won Chung', 'Tongyu Zhou', 'Ivy Chen', 'Kevin Hsu', 'Ryan A. Rossi', 'Alexa Siu', 'Shunan Guo', 'Franck Dernoncourt', 'James Tompkin', 'Jeff Huang'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03164.jpg', 'data': {'categories': ['#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'InfoVids: Новый подход к визуализации данных через призму человека', 'desc': 'Это исследование представляет концепцию InfoVids - информационных видео, вдохновленных инфографикой. Они призваны создать более сбалансированные отношения между презентатором и визуализацией данных. Авторы изучают, как макет, форма и интерактивность влияют на восприятие зрителей. Эксперименты показали, что InfoVids снижают рассеивание внимания, переносят фокус на презентатора и создают более естественное и увлекательное взаимодействие с данными.'}, 'en': {'title': 'Revolutionizing Data Presentation with InfoVids', 'desc': 'This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualizations in a more cohesive manner. By using infographics-inspired videos, the authors aim to enhance viewer engagement and reduce distractions that typically arise from traditional 2D slides. The study evaluates InfoVids against standard presentation methods across various metrics, revealing that they foster a more interactive and natural experience for viewers. The findings suggest that this innovative approach can transform the dynamics of data presentation, making it more human-centric and effective.'}, 'zh': {'title': '重新定义演示者与可视化的关系', 'desc': '传统的数据展示通常将演示者和可视化分开，分别在3D世界和2D屏幕中进行，强调以可视化为中心的叙述。为了创造更以人为本的观看体验，我们通过InfoVids建立了可视化与演示者之间更平等的关系。这些受信息图启发的信息视频旨在重新定义演示者与可视化之间的关系。我们的研究表明，InfoVids减少了观众的注意力分散，使焦点从可视化转向演示者，并为观众提供了更互动、自然和引人入胜的数据表现。'}}}, {'id': 'https://huggingface.co/papers/2504.21650', 'title': 'HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation', 'url': 'https://huggingface.co/papers/2504.21650', 'abstract': "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.", 'score': 4, 'issue_id': 3632, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'a04fa605a99969e5', 'authors': ['Haiyang Zhou', 'Wangbo Yu', 'Jiawen Guan', 'Xinhua Cheng', 'Yonghong Tian', 'Li Yuan'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory', 'School of Electronic and Computer Engineering, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21650.jpg', 'data': {'categories': ['#3d', '#video', '#cv', '#dataset', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'Погружение в виртуальные миры: от панорамных видео к 4D-сценам', 'desc': 'HoloTime - это новая система для создания панорамных видео и 4D-сцен для VR и AR на основе диффузионных моделей. Она включает в себя генерацию панорамных видео по одному изображению или текстовому запросу с помощью модели Panoramic Animator. Затем производится реконструкция 4D-сцены методом Panoramic Space-Time Reconstruction. Для обучения моделей был создан набор данных 360World с панорамными видео.'}, 'en': {'title': 'Transforming VR and AR with HoloTime: Immersive 4D Experiences from Panoramic Videos', 'desc': 'This paper introduces HoloTime, a novel framework that enhances virtual reality (VR) and augmented reality (AR) experiences by generating immersive 4D assets from video diffusion models. It addresses the limitations of current models that focus on static 3D scenes by proposing a method to create panoramic videos from a single image, which are then transformed into 4D representations. The authors present the 360World dataset, a unique collection of panoramic videos that supports advanced 4D scene reconstruction tasks. Their approach, validated through comparative analysis, shows significant improvements in generating high-quality panoramic videos and reconstructing 4D scenes, ultimately leading to more engaging user experiences in immersive environments.'}, 'zh': {'title': 'HoloTime：提升VR和AR沉浸体验的全景视频生成框架', 'desc': '本论文提出了一种名为HoloTime的框架，旨在通过视频扩散模型生成全景视频，从而提升虚拟现实（VR）和增强现实（AR）技术的沉浸体验。我们引入了360World数据集，这是第一个专门用于4D场景重建任务的全景视频集合。通过Panoramic Animator模型，我们能够将全景图像转换为高质量的全景视频。最后，利用空间-时间深度估计方法，我们将生成的视频转化为4D点云，实现了更真实的4D场景重建。'}}}, {'id': 'https://huggingface.co/papers/2504.21798', 'title': 'SWE-smith: Scaling Data for Software Engineering Agents', 'url': 'https://huggingface.co/papers/2504.21798', 'abstract': 'Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.', 'score': 3, 'issue_id': 3638, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '35185d0e663ab134', 'authors': ['John Yang', 'Kilian Leret', 'Carlos E. Jimenez', 'Alexander Wettig', 'Kabir Khandpur', 'Yanzhe Zhang', 'Binyuan Hui', 'Ofir Press', 'Ludwig Schmidt', 'Diyi Yang'], 'affiliations': ['Alibaba Qwen', 'Indepedent', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21798.jpg', 'data': {'categories': ['#benchmark', '#data', '#synthetic', '#open_source', '#dataset', '#training'], 'emoji': '🛠️', 'ru': {'title': 'SWE-smith: революция в создании обучающих данных для ИИ в программировании', 'desc': 'SWE-smith - это новый конвейер для генерации обучающих данных в области программной инженерии в больших масштабах. Он создает среду выполнения для любой кодовой базы Python и автоматически синтезирует сотни или тысячи экземпляров задач, нарушающих существующие тесты. С помощью SWE-smith авторы создали набор данных из 50 тысяч экземпляров из 128 репозиториев GitHub, что на порядок больше, чем в предыдущих работах. Обученная на этих данных языковая модель SWE-agent-LM-32B достигла показателя Pass@1 в 40.2% на бенчмарке SWE-bench Verified, что является лучшим результатом среди моделей с открытым исходным кодом.'}, 'en': {'title': 'Scaling Software Engineering Data Generation with SWE-smith', 'desc': 'This paper presents SWE-smith, a new method for generating large-scale training data for language models in software engineering. Traditional datasets are limited in size and require extensive manual effort to curate, which hinders their effectiveness. SWE-smith automates the creation of execution environments and synthesizes numerous task instances from Python codebases, resulting in a dataset of 50,000 instances from 128 GitHub repositories. The authors demonstrate that their model, SWE-agent-LM-32B, achieves a state-of-the-art performance on the SWE-bench Verified benchmark, and they provide all resources to facilitate further research in this area.'}, 'zh': {'title': 'SWE-smith：大规模生成软件工程训练数据的解决方案', 'desc': '尽管语言模型在软件工程领域取得了进展，但收集训练数据仍然是一个重大挑战。现有的数据集规模较小，通常来自11个或更少的GitHub仓库，最多只有几千个训练实例。为了应对这一问题，我们提出了SWE-smith，这是一种用于大规模生成软件工程训练数据的新型管道。通过SWE-smith，我们创建了一个包含5万个实例的数据集，显著超过了以往的工作，并且开源了相关资源，以降低自动化软件工程研究的门槛。'}}}, {'id': 'https://huggingface.co/papers/2505.03052', 'title': 'Teaching Models to Understand (but not Generate) High-risk Data', 'url': 'https://huggingface.co/papers/2505.03052', 'abstract': "Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.", 'score': 2, 'issue_id': 3642, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'a988d97eaa0eb984', 'authors': ['Ryan Wang', 'Matthew Finlayson', 'Luca Soldaini', 'Swabha Swayamdipta', 'Robin Jia'], 'affiliations': ['Allen Institute for AI', 'Department of Computer Science, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.03052.jpg', 'data': {'categories': ['#training', '#hallucinations', '#rlhf', '#ethics'], 'emoji': '🛡️', 'ru': {'title': 'Понимать, но не генерировать: безопасное обучение языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием SLUNG. Этот метод позволяет моделям понимать потенциально опасный контент, не генерируя его. SLUNG избирательно применяет функцию потерь, избегая стимулирования генерации рискованных токенов, но сохраняя их в контексте модели. Эксперименты показывают, что SLUNG улучшает понимание моделями опасного контента без увеличения его генерации.'}, 'en': {'title': 'Learn to Understand, Not to Generate Toxicity', 'desc': "This paper presents a new training method called Selective Loss to Understand but Not Generate (SLUNG) for language models. SLUNG allows models to learn from high-risk content, like toxic text, without generating it in their outputs. By selectively applying loss functions, the model is trained to predict safe tokens that follow harmful ones, enhancing its understanding of sensitive topics. The results show that SLUNG improves the model's ability to recognize toxic content while preventing it from producing harmful responses."}, 'zh': {'title': '理解高风险内容，避免生成有害输出', 'desc': '本文提出了一种新的预训练方法，称为选择性损失以理解但不生成（SLUNG）。该方法允许模型在不生成高风险内容的情况下，理解这些内容。通过选择性地避免激励生成高风险标记，SLUNG确保这些内容仍然在模型的上下文窗口内。实验结果表明，SLUNG显著提高了模型对高风险数据的理解能力，同时没有增加生成的有害内容。'}}}, {'id': 'https://huggingface.co/papers/2505.02311', 'title': 'Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering', 'url': 'https://huggingface.co/papers/2505.02311', 'abstract': 'The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.', 'score': 2, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '2a0b4af232b71fc8', 'authors': ['Jihao Zhao', 'Chunlai Zhou', 'Biao Qin'], 'affiliations': ['School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02311.jpg', 'data': {'categories': ['#hallucinations', '#small_models', '#training', '#optimization', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Умное обнаружение галлюцинаций для эффективного сотрудничества языковых моделей', 'desc': 'Статья предлагает новый метод AttenHScore для оценки галлюцинаций в малых языковых моделях в реальном времени. Этот подход позволяет точнее определять момент для вызова большой языковой модели в коллаборативной парадигме. Авторы также используют реорганизацию знаний с учетом неопределенности, чтобы помочь малым моделям лучше обрабатывать ключевую информацию. Эксперименты показывают, что AttenHScore превосходит базовые методы в обнаружении галлюцинаций на нескольких наборах данных вопросно-ответных систем.'}, 'en': {'title': 'Enhancing Hallucination Detection in Language Models with AttenHScore', 'desc': 'This paper introduces a new metric called AttenHScore to improve the detection of hallucinations in small language models (LMs) during their generation process. Hallucinations refer to incorrect or nonsensical outputs produced by LMs, and the proposed metric helps identify when these errors occur in real-time. By adjusting the detection threshold dynamically, the method enhances the invocation of larger LMs to provide more accurate responses. Additionally, the paper discusses how uncertainty-aware knowledge reorganization can help small LMs better understand and utilize critical information from text, leading to improved performance without requiring extra training.'}, 'zh': {'title': '提升小型语言模型的幻觉检测能力', 'desc': '本文提出了一种新的评估指标AttenHScore，用于在小型语言模型生成过程中检测和传播幻觉。通过动态调整检测阈值，我们能够更准确地实时调用大型语言模型，从而提高幻觉检测的能力。我们还利用不确定性感知的知识重组，帮助小型语言模型更好地捕捉关键信息。实验结果表明，AttenHScore在多个问答数据集上优于大多数基线方法，尤其是在处理复杂查询时。'}}}, {'id': 'https://huggingface.co/papers/2504.18373', 'title': 'Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant', 'url': 'https://huggingface.co/papers/2504.18373', 'abstract': 'In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.', 'score': 2, 'issue_id': 3629, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '296dd197ef9ea331', 'authors': ['Lei Shen', 'Xiaoyu Shen'], 'affiliations': ['GEB Tech', 'Ningbo Institute of Digital Twin, EIT, Ningbo'], 'pdf_title_img': 'assets/pdf/title_img/2504.18373.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Auto-SLURP: новый стандарт для оценки многоагентных ИИ-ассистентов', 'desc': 'Авторы представляют Auto-SLURP - набор данных для оценки многоагентных систем на основе больших языковых моделей (LLM) в контексте интеллектуальных персональных ассистентов. Auto-SLURP расширяет исходный набор SLURP, добавляя симулированные серверы и внешние сервисы для комплексной оценки понимания языка, выполнения задач и генерации ответов. Эксперименты показывают, что Auto-SLURP представляет серьезную проблему для современных передовых систем. Результаты подчеркивают, что по-настоящему надежные и интеллектуальные многоагентные персональные ассистенты все еще находятся в стадии разработки.'}, 'en': {'title': 'Auto-SLURP: Benchmarking Multi-Agent LLMs for Intelligent Assistants', 'desc': 'This paper introduces Auto-SLURP, a new benchmark dataset designed to evaluate multi-agent frameworks that utilize large language models (LLMs) in the realm of intelligent personal assistants. It enhances the original SLURP dataset by relabeling data and incorporating simulated servers and external services, allowing for a more thorough assessment of language understanding, task execution, and response generation. The authors demonstrate that Auto-SLURP poses significant challenges to existing state-of-the-art frameworks, indicating that the development of reliable multi-agent personal assistants is still ongoing. The dataset and its associated code are made publicly available for further research and development.'}, 'zh': {'title': 'Auto-SLURP：评估智能个人助理的基准数据集', 'desc': '近年来，基于大型语言模型（LLMs）的多智能体框架发展迅速。然而，目前缺乏专门用于评估这些框架性能的基准数据集。为了解决这个问题，我们推出了Auto-SLURP，这是一个旨在评估基于LLM的多智能体框架的基准数据集，特别是在智能个人助理的背景下。Auto-SLURP通过重新标记数据并整合模拟服务器和外部服务，扩展了原始的SLURP数据集，从而实现了全面的端到端评估流程。'}}}, {'id': 'https://huggingface.co/papers/2505.00212', 'title': 'Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2505.00212', 'abstract': "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution", 'score': 1, 'issue_id': 3639, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '1b127d9b18e51c40', 'authors': ['Shaokun Zhang', 'Ming Yin', 'Jieyu Zhang', 'Jiale Liu', 'Zhiguang Han', 'Jingyang Zhang', 'Beibin Li', 'Chi Wang', 'Huazheng Wang', 'Yiran Chen', 'Qingyun Wu'], 'affiliations': ['Duke University', 'Google DeepMind', 'Meta', 'Nanyang Technological University', 'Oregon State University', 'Pennsylvania State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.00212.jpg', 'data': {'categories': ['#dataset', '#open_source', '#agents', '#reasoning'], 'emoji': '🕵️', 'ru': {'title': 'Кто виноват и когда: автоматическое расследование сбоев в многоагентных LLM-системах', 'desc': 'Статья представляет новую область исследований: автоматическое определение причин сбоев в многоагентных системах на основе больших языковых моделей (LLM). Авторы создали датасет Who&When, содержащий подробные логи сбоев из 127 многоагентных LLM-систем с аннотациями, связывающими сбои с конкретными агентами и этапами ошибок. На основе этого датасета разработаны и оценены три метода автоматического определения причин сбоев. Результаты показывают сложность задачи: лучший метод достигает точности 53.5% в идентификации агентов, ответственных за сбои, но только 14.2% в определении этапов сбоев.'}, 'en': {'title': 'Automating Failure Attribution in Multi-Agent Systems', 'desc': 'This paper addresses the challenge of identifying which agent and which step in a multi-agent system led to task failures, a process known as failure attribution. The authors introduce a new dataset called Who&When, which contains detailed failure logs from 127 LLM multi-agent systems, annotated to link failures to specific agents and error steps. They propose three automated methods for failure attribution and evaluate their performance, revealing that the best method only achieves 53.5% accuracy in identifying responsible agents and 14.2% in pinpointing failure steps. The findings indicate that current state-of-the-art models struggle with this task, emphasizing the complexity of automated failure attribution and the need for further research.'}, 'zh': {'title': '自动化失败归因：LLM多智能体系统的新挑战', 'desc': '本文探讨了在大型语言模型（LLM）多智能体系统中，如何自动识别导致任务失败的智能体和步骤。我们提出了一个新的研究领域：自动化失败归因，并引入了Who&When数据集，该数据集包含127个LLM多智能体系统的详细失败日志。通过使用Who&When数据集，我们开发并评估了三种自动化失败归因方法，并总结了它们的优缺点。尽管最佳方法在识别失败责任智能体方面达到了53.5%的准确率，但在确定失败步骤方面仅为14.2%，显示出该任务的复杂性和进一步研究的必要性。'}}}, {'id': 'https://huggingface.co/papers/2505.10554', 'title': "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models", 'url': 'https://huggingface.co/papers/2505.10554', 'abstract': 'Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model\'s "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs\' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment', 'score': 96, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '9b574dafe9f7fb18', 'authors': ['Zhiyuan Hu', 'Yibo Wang', 'Hanze Dong', 'Yuhui Xu', 'Amrita Saha', 'Caiming Xiong', 'Bryan Hooi', 'Junnan Li'], 'affiliations': ['National University of Singapore', 'Salesforce AI Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10554.jpg', 'data': {'categories': ['#alignment', '#science', '#rl', '#optimization', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Выравнивание мета-способностей для улучшения рассуждений ИИ', 'desc': 'Статья описывает новый подход к улучшению способностей больших языковых моделей к рассуждению. Авторы предлагают метод явного выравнивания моделей с тремя мета-способностями: дедукцией, индукцией и абдукцией, используя автоматически сгенерированные, самопроверяемые задачи. Предложенный трехэтапный конвейер включает индивидуальное выравнивание, слияние в пространстве параметров и предметно-специфическое обучение с подкреплением. Результаты показывают повышение производительности на более чем 10% по сравнению с базовыми моделями, обученными на инструкциях.'}, 'en': {'title': 'Enhancing Reasoning in Models through Explicit Meta-Ability Alignment', 'desc': "This paper discusses how large reasoning models (LRMs) can improve their reasoning abilities through a structured approach. It highlights the limitations of relying on spontaneous reasoning behaviors, known as 'aha moments', which can be unpredictable. The authors propose a method to explicitly align models with three key reasoning skills: deduction, induction, and abduction, using self-verifiable tasks. Their approach, which includes a three-stage pipeline and domain-specific reinforcement learning, significantly enhances the performance of LRMs on various benchmarks."}, 'zh': {'title': '明确对齐，提升推理能力！', 'desc': '大型推理模型（LRMs）具备长链推理的潜在能力。以结果为基础的强化学习（RL）可以偶然引发高级推理行为，如自我修正和回溯，但这些行为的时机和一致性难以预测，限制了LRMs推理能力的可扩展性和可靠性。为了解决这些问题，本文提出通过自动生成的自我验证任务，明确对模型进行三种元能力的对齐：演绎、归纳和溯因。通过三阶段的个体对齐、参数空间合并和领域特定的强化学习，性能提升超过10%，并在数学、编程和科学基准测试中实现额外的2%的平均增益，证明了明确的元能力对齐为推理提供了可扩展和可靠的基础。'}}}, {'id': 'https://huggingface.co/papers/2505.10475', 'title': 'Parallel Scaling Law for Language Models', 'url': 'https://huggingface.co/papers/2505.10475', 'abstract': "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply P diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the P outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with P parallel streams is similar to scaling the parameters by O(log P) while showing superior inference efficiency. For example, ParScale can use up to 22times less memory increase and 6times less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.", 'score': 52, 'issue_id': 3800, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '71bd56e1da07bcba', 'authors': ['Mouxiang Chen', 'Binyuan Hui', 'Zeyu Cui', 'Jiaxi Yang', 'Dayiheng Liu', 'Jianling Sun', 'Junyang Lin', 'Zhongxin Liu'], 'affiliations': ['Qwen Team, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10475.jpg', 'data': {'categories': ['#inference', '#low_resource', '#training', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Эффективное масштабирование языковых моделей через параллельные вычисления', 'desc': 'Статья представляет новый подход к масштабированию языковых моделей, названный параллельным масштабированием (ParScale). Метод применяет P различных преобразований к входным данным, выполняет параллельные прямые проходы модели и динамически агрегирует P выходов. ParScale позволяет увеличить вычислительную мощность модели, переиспользуя существующие параметры, и может применяться к любой структуре модели, процедуре оптимизации, данным или задаче. Авторы теоретически предлагают новый закон масштабирования и подтверждают его эффективность через масштабное предобучение.'}, 'en': {'title': 'Unlocking Efficiency: Parallel Scaling for Language Models', 'desc': "This paper introduces a new method called parallel scaling (ParScale) for improving the efficiency of language models during training and inference. Instead of simply increasing the number of parameters or output tokens, ParScale enhances the model's ability to perform computations in parallel by applying multiple transformations to the input. The authors propose a new scaling law that shows how using P parallel streams can achieve similar performance to increasing parameters by O(log P), while significantly reducing memory and latency costs. This approach allows for the reuse of existing models and can be adapted to various tasks, making it a cost-effective solution for deploying powerful models in resource-constrained environments."}, 'zh': {'title': '并行扩展：高效的模型计算新方法', 'desc': '这篇论文提出了一种新的模型扩展方法，称为并行扩展（ParScale），它通过增加模型的并行计算来提高推理效率，而不是单纯增加参数或输出令牌。该方法通过对输入应用多种可学习的变换，并在训练和推理时并行执行模型的前向传递，动态聚合多个输出。研究表明，使用P个并行流的模型在性能上相当于参数扩展的O(log P)，但在推理效率上更优。ParScale还可以通过在少量令牌上进行后训练，将现有的预训练模型转化为并行扩展模型，从而降低训练成本。'}}}, {'id': 'https://huggingface.co/papers/2505.09666', 'title': 'System Prompt Optimization with Meta-Learning', 'url': 'https://huggingface.co/papers/2505.09666', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.', 'score': 52, 'issue_id': 3792, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': 'e8d5cb78c5949430', 'authors': ['Yumin Choi', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.09666.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Универсальные системные промпты: новый уровень эффективности LLM', 'desc': 'Статья представляет новый подход к оптимизации системных промптов для больших языковых моделей (LLM). Авторы предлагают метод двухуровневой оптимизации, который делает системные промпты устойчивыми к разнообразным пользовательским запросам и применимыми к новым задачам. Используется фреймворк мета-обучения, оптимизирующий системный промпт на различных пользовательских запросах и наборах данных. Эксперименты на 14 новых датасетах показали эффективность метода в генерализации и быстрой адаптации к незнакомым задачам.'}, 'en': {'title': 'Optimizing System Prompts for Versatile Language Model Performance', 'desc': 'This paper addresses the optimization of system prompts in Large Language Models (LLMs), which are crucial for enhancing model performance across various tasks. It introduces a new approach called bilevel system prompt optimization, focusing on creating system prompts that can adapt to different user prompts and tasks. The authors propose a meta-learning framework that iteratively refines both system and user prompts, ensuring they work well together. Experimental results demonstrate that the optimized system prompts can generalize effectively and adapt quickly to new tasks with fewer optimization steps, leading to better performance.'}, 'zh': {'title': '优化系统提示，提升模型适应性', 'desc': '大型语言模型（LLMs）在处理任务时表现出色，而优化输入提示在提升其性能中起着关键作用。现有的提示优化研究主要集中在特定任务的用户提示上，忽视了系统提示的优化。本文提出了双层系统提示优化的新问题，旨在设计能够适应多种用户提示并可迁移到未见任务的系统提示。我们提出了一种元学习框架，通过在多个数据集上优化系统提示，同时迭代更新用户提示，以确保两者之间的协同作用。'}}}, {'id': 'https://huggingface.co/papers/2505.08617', 'title': 'OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.08617', 'abstract': 'While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".', 'score': 29, 'issue_id': 3792, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '8cef19b2c7c9a459', 'authors': ['Zhaochen Su', 'Linjie Li', 'Mingyang Song', 'Yunzhuo Hao', 'Zhengyuan Yang', 'Jun Zhang', 'Guanjie Chen', 'Jiawei Gu', 'Juntao Li', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Huazhong University of Science and Technology', 'Microsoft', 'Shanghai Jiao Tong University', 'Soochow University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.08617.jpg', 'data': {'categories': ['#cv', '#rl', '#dataset', '#agents', '#training', '#open_source', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'OpenThinkIMG: Революция в обучении ИИ мыслить визуально', 'desc': 'Статья представляет OpenThinkIMG - первую открытую комплексную платформу для обучения мультимодальных языковых моделей (LVLM) использованию визуальных инструментов. Платформа включает стандартизированные интерфейсы визуальных инструментов, масштабируемую генерацию траекторий для инициализации политик и гибкую среду обучения. Авторы также предлагают новый фреймворк обучения с подкреплением V-ToolRL для адаптивного вызова внешних визуальных инструментов. Эмпирические результаты на задачах анализа диаграмм показывают, что агент, обученный с помощью V-ToolRL, значительно превосходит базовые модели и даже GPT-4.1.'}, 'en': {'title': 'Empowering AI to Think with Images through OpenThinkIMG', 'desc': 'This paper presents OpenThinkIMG, an innovative framework designed to enhance Large Vision-Language Models (LVLMs) by integrating visual tools for improved problem-solving. The framework addresses the challenges of standardization and data generation, allowing for better training of agents that can adaptively use visual tools. A key feature is the introduction of V-ToolRL, a reinforcement learning approach that enables LVLMs to learn optimal strategies for tool usage through direct feedback from their interactions. The results show that agents trained with V-ToolRL significantly outperform traditional supervised fine-tuning methods and even leading closed-source models, demonstrating the potential of dynamic tool-augmented visual reasoning.'}, 'zh': {'title': '开启视觉工具增强的智能推理新时代', 'desc': '本文介绍了OpenThinkIMG，这是第一个开源的端到端框架，旨在增强大型视觉语言模型（LVLMs）使用视觉工具的能力。该框架提供了标准化的视觉工具接口和灵活的训练环境，以便更好地集成多种工具并生成丰富的交互数据。为了克服传统监督微调方法的局限性，本文提出了一种新的强化学习框架V-ToolRL，使LVLMs能够自主学习动态工具调用的适应性策略。实验结果表明，使用V-ToolRL训练的代理在复杂的图表推理任务中表现优异，显著超越了其他基线模型。'}}}, {'id': 'https://huggingface.co/papers/2505.09723', 'title': 'EnerVerse-AC: Envisioning Embodied Environments with Action Condition', 'url': 'https://huggingface.co/papers/2505.09723', 'abstract': "Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world model that generates future visual observations based on an agent's predicted actions, enabling realistic and controllable robotic inference. Building on prior architectures, EVAC introduces a multi-level action-conditioning mechanism and ray map encoding for dynamic multi-view image generation while expanding training data with diverse failure trajectories to improve generalization. As both a data engine and evaluator, EVAC augments human-collected trajectories into diverse datasets and generates realistic, action-conditioned video observations for policy testing, eliminating the need for physical robots or complex simulations. This approach significantly reduces costs while maintaining high fidelity in robotic manipulation evaluation. Extensive experiments validate the effectiveness of our method. Code, checkpoints, and datasets can be found at <https://annaj2178.github.io/EnerverseAC.github.io>.", 'score': 18, 'issue_id': 3793, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '8adef8c283985aee', 'authors': ['Yuxin Jiang', 'Shengcong Chen', 'Siyuan Huang', 'Liliang Chen', 'Pengfei Zhou', 'Yue Liao', 'Xindong He', 'Chiming Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'MMLab-CUHK', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.09723.jpg', 'data': {'categories': ['#agents', '#dataset', '#training', '#optimization', '#open_source', '#robotics', '#video'], 'emoji': '🤖', 'ru': {'title': 'EVAC: виртуальная среда для эффективного обучения и оценки роботов', 'desc': 'EVAC - это модель мира с условным действием для имитационного обучения роботов. Она генерирует будущие визуальные наблюдения на основе прогнозируемых действий агента, что позволяет проводить реалистичное и контролируемое тестирование. EVAC использует многоуровневый механизм обусловливания действий и кодирование лучевой карты для динамической генерации мультиракурсных изображений. Модель расширяет обучающие данные разнообразными траекториями неудач для улучшения обобщения, а также служит как генератором данных, так и инструментом оценки.'}, 'en': {'title': 'Revolutionizing Robotic Learning with EnerVerse-AC', 'desc': 'The paper presents EnerVerse-AC (EVAC), a novel action-conditional world model designed for robotic imitation learning in dynamic environments. EVAC generates future visual observations based on predicted actions, allowing for realistic robotic inference without the need for physical robots. It enhances training data by incorporating diverse failure trajectories and employs a multi-level action-conditioning mechanism for improved generalization. This method significantly reduces evaluation costs while ensuring high fidelity in testing robotic manipulation policies.'}, 'zh': {'title': '动态交互中的机器人模仿学习新突破', 'desc': '本论文提出了一种名为EnerVerse-AC（EVAC）的动作条件世界模型，旨在提高机器人模仿学习在动态交互场景中的表现。EVAC能够根据代理的预测动作生成未来的视觉观察，从而实现更真实和可控的机器人推理。该模型引入了多层次的动作条件机制和光线图编码，增强了动态多视图图像生成的能力，并通过多样化的失败轨迹扩展训练数据，以提高模型的泛化能力。通过将人类收集的轨迹转化为多样化的数据集，EVAC显著降低了测试成本，同时保持了高保真度的机器人操作评估。'}}}, {'id': 'https://huggingface.co/papers/2505.10527', 'title': 'WorldPM: Scaling Human Preference Modeling', 'url': 'https://huggingface.co/papers/2505.10527', 'abstract': "Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.", 'score': 17, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'd2fe74535293635c', 'authors': ['Binghai Wang', 'Runji Lin', 'Keming Lu', 'Le Yu', 'Zhenru Zhang', 'Fei Huang', 'Chujie Zheng', 'Kai Dang', 'Yang Fan', 'Xingzhang Ren', 'An Yang', 'Binyuan Hui', 'Dayiheng Liu', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Bowen Yu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Institute of Trustworthy Embodied Artificial Intelligence, Fudan University', 'Qwen Team, Alibaba Group', 'School of Computer Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10527.jpg', 'data': {'categories': ['#data', '#training', '#benchmark', '#optimization', '#dataset', '#rlhf', '#alignment'], 'emoji': '🌍', 'ru': {'title': 'Масштабирование моделей предпочтений: от данных к глобальному пониманию', 'desc': 'Исследователи обнаружили, что законы масштабирования, аналогичные тем, что наблюдаются в языковом моделировании, существуют и в моделировании предпочтений. Они предложили концепцию World Preference Modeling (WorldPM) для изучения потенциала масштабирования в этой области. Используя 15 миллионов примеров данных и модели размером от 1.5 до 72 миллиардов параметров, они наблюдали различные паттерны масштабирования для разных метрик оценки. Эксперименты показали, что WorldPM эффективен как основа для дальнейшей настройки моделей предпочтений и улучшает обобщающую способность на различных наборах данных.'}, 'en': {'title': 'Scaling Human Preferences with WorldPM', 'desc': 'This paper introduces World Preference Modeling (WorldPM), which explores how human preferences can be effectively modeled and scaled in machine learning. The authors demonstrate that preference modeling follows similar scaling laws as language modeling, where larger models and datasets lead to improved performance. They collect diverse preference data and train models with varying sizes, revealing that adversarial and objective metrics improve with scale, while subjective metrics do not show consistent trends. The findings suggest that WorldPM enhances generalization across different human preference datasets and significantly boosts performance in reinforcement learning from human feedback (RLHF) applications.'}, 'zh': {'title': '世界偏好建模：提升人类偏好的新方法', 'desc': '本论文探讨了在偏好建模中存在的规模法则，类似于语言建模中的现象。我们提出了世界偏好建模（WorldPM），强调其在处理人类偏好时的统一表示能力。通过收集来自公共论坛的偏好数据，并在不同规模的模型上进行训练，我们发现不同评估指标的表现存在明显差异。实验结果表明，WorldPM在多种人类偏好数据集上显著提高了模型的泛化性能，尤其在关键子任务上提升超过5%。'}}}, {'id': 'https://huggingface.co/papers/2505.10185', 'title': 'The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think', 'url': 'https://huggingface.co/papers/2505.10185', 'abstract': 'Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.', 'score': 17, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'dc93377d68390280', 'authors': ['Seongyun Lee', 'Seungone Kim', 'Minju Seo', 'Yongrae Jo', 'Dongyoung Go', 'Hyeonbin Hwang', 'Jinho Park', 'Xiang Yue', 'Sean Welleck', 'Graham Neubig', 'Moontae Lee', 'Minjoon Seo'], 'affiliations': ['Carnegie Mellon University', 'Cornell University', 'KAIST AI', 'LG AI Research', 'NAVER Search US'], 'pdf_title_img': 'assets/pdf/title_img/2505.10185.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Расшифровка мышления ИИ: от цепочек к энциклопедии рассуждений', 'desc': "Статья представляет новый подход к анализу цепочек рассуждений (CoT) в больших языковых моделях. Авторы предлагают метод 'CoT Encyclopedia', который автоматически извлекает и кластеризует различные стратегии рассуждений из сгенерированных моделью цепочек. Этот метод позволяет более полно и интерпретируемо анализировать поведение моделей по сравнению с существующими подходами. Исследование также показывает, что формат обучающих данных оказывает большее влияние на стратегии рассуждений, чем их предметная область."}, 'en': {'title': 'Unlocking Model Reasoning with the CoT Encyclopedia', 'desc': 'This paper presents the CoT Encyclopedia, a new framework for analyzing the reasoning strategies of large language models through their chain-of-thought (CoT) outputs. Unlike previous methods that rely on predefined categories, this approach uses a bottom-up technique to automatically extract and cluster diverse reasoning criteria from the models. The framework not only enhances the interpretability of model behaviors but also improves performance by predicting and guiding models towards more effective reasoning strategies. Additionally, the study highlights the significant influence of training data format on reasoning behavior, emphasizing the need for format-aware design in model training.'}, 'zh': {'title': '理解推理，提升模型表现', 'desc': '长链推理（CoT）是现代大型语言模型有效使用的重要组成部分，但我们对其推理策略的理解仍然有限。本文提出了CoT百科全书，这是一个自下而上的框架，用于分析和引导模型推理。我们的方法自动提取模型生成的CoT中的多样化推理标准，将其嵌入语义空间，聚类成代表性类别，并推导出对比性标准以解释推理行为。研究表明，这种框架比现有方法提供了更可解释和全面的分析，并且能够预测模型可能使用的策略，从而引导其朝向更有效的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2505.10562', 'title': 'End-to-End Vision Tokenizer Tuning', 'url': 'https://huggingface.co/papers/2505.10562', 'abstract': 'Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is agnostic to downstream tasks requiring varied representations and semantics. This decoupled paradigm introduces a critical misalignment: The loss of the vision tokenization can be the representation bottleneck for target tasks. For example, errors in tokenizing text in a given image lead to poor results when recognizing or generating them. To address this, we propose ETT, an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.', 'score': 16, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '1f57cc7a0953749d', 'authors': ['Wenxuan Wang', 'Fan Zhang', 'Yufeng Cui', 'Haiwen Diao', 'Zhuoyan Luo', 'Huchuan Lu', 'Jing Liu', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Dalian University of Technology', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10562.jpg', 'data': {'categories': ['#alignment', '#cv', '#optimization', '#multimodal', '#training', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Конец-в-конец настройка визуальных токенизаторов для улучшения мультимодальных моделей', 'desc': 'Статья предлагает новый подход ETT для оптимизации визуальных токенизаторов в контексте конкретных задач машинного обучения. В отличие от существующих методов, которые изолируют токенизацию от последующего обучения, ETT позволяет совместно оптимизировать токенизацию и целевые авторегрессионные задачи. Метод использует визуальные эмбеддинги кодовой книги токенизатора и оптимизирует токенизаторы с учетом целей реконструкции и генерации подписей. Эксперименты показывают значительное улучшение производительности на 2-6% для задач мультимодального понимания и визуальной генерации по сравнению с базовыми моделями с замороженными токенизаторами.'}, 'en': {'title': 'Empowering Vision Tokenization for Better Multimodal Performance', 'desc': 'This paper introduces ETT, a new method for optimizing vision tokenizers in machine learning. Traditional vision tokenization methods often work separately from the tasks they support, which can lead to poor performance due to misalignment in representation. ETT addresses this issue by allowing joint optimization of vision tokenization and downstream tasks, improving the quality of visual representations. The results show that ETT significantly enhances performance in multimodal tasks while maintaining the original capabilities of the tokenizer.'}, 'zh': {'title': '端到端视觉标记器调优，提升多模态性能', 'desc': '现有的视觉标记化方法将视觉标记器的优化与下游训练分开，假设视觉标记在各种任务中能够很好地泛化。为了克服这一问题，我们提出了ETT，一种端到端的视觉标记器调优方法，能够实现视觉标记化与目标自回归任务的联合优化。ETT利用标记器代码本的视觉嵌入，优化视觉标记器，同时兼顾重建和描述目标。实验表明，ETT在多模态理解和视觉生成任务中相比于固定标记器基线，性能提升显著。'}}}, {'id': 'https://huggingface.co/papers/2505.10320', 'title': 'J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.10320', 'abstract': 'The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.', 'score': 15, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '61617ab29ad31cc7', 'authors': ['Chenxi Whitehouse', 'Tianlu Wang', 'Ping Yu', 'Xian Li', 'Jason Weston', 'Ilia Kulikov', 'Swarnadeep Saha'], 'affiliations': ['FAIR at Meta', 'GenAI at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.10320.jpg', 'data': {'categories': ['#reasoning', '#training', '#benchmark', '#optimization', '#hallucinations', '#rlhf', '#rl'], 'emoji': '🧠', 'ru': {'title': 'J1: Революция в обучении моделей-судей через рассуждения', 'desc': 'Статья представляет новый подход к обучению моделей-судей с использованием обучения с подкреплением, названный J1. Метод преобразует как проверяемые, так и непроверяемые запросы в задачи оценки с проверяемыми вознаграждениями, стимулирующими мышление и снижающими предвзятость суждений. J1 превосходит существующие модели размером 8B и 70B, включая модели, дистиллированные из DeepSeek-R1. Анализ показывает, что модели J1 улучшают суждения, формулируя критерии оценки, сравнивая с самостоятельно сгенерированными эталонными ответами и переоценивая правильность ответов модели.'}, 'en': {'title': 'Empowering AI Judgment with J1: A Reinforcement Learning Breakthrough', 'desc': 'This paper addresses the challenge of evaluating AI models by introducing J1, a reinforcement learning method designed to enhance the judgment capabilities of large language models (LLMs). J1 transforms both verifiable and non-verifiable prompts into judgment tasks that provide clear rewards, promoting better reasoning and reducing bias in evaluations. The results show that J1 outperforms existing models of similar sizes, demonstrating its effectiveness in training models to make more accurate judgments. The study also includes a detailed analysis of various training strategies and their impact on model performance, highlighting the importance of structured evaluation criteria and self-referential comparisons.'}, 'zh': {'title': '提升AI评估质量的关键：J1模型', 'desc': '本论文探讨了人工智能评估质量对AI进步的影响，并提出了一种名为J1的强化学习方法来训练大型语言模型（LLM）进行判断。J1通过将可验证和不可验证的提示转换为具有可验证奖励的判断任务，从而提高模型的思维能力并减少判断偏差。研究表明，J1在训练时表现优于现有的8B和70B模型，甚至在某些基准测试中超越了更大的模型。通过对比不同的训练策略和模型变体，发现J1模型能够更好地进行判断，学习评估标准并自我生成参考答案。'}}}, {'id': 'https://huggingface.co/papers/2505.09694', 'title': 'EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models', 'url': 'https://huggingface.co/papers/2505.09694', 'abstract': 'Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench.', 'score': 15, 'issue_id': 3792, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '8439c79042ee5e23', 'authors': ['Hu Yue', 'Siyuan Huang', 'Yue Liao', 'Shengcong Chen', 'Pengfei Zhou', 'Liliang Chen', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'HIT', 'MMLab-CUHK', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.09694.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#multimodal', '#games', '#video', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'EWMBench: Комплексная оценка воплощенных мировых моделей для генерации видео', 'desc': 'Статья представляет новый бенчмарк EWMBench для оценки воплощенных мировых моделей (Embodied World Models, EWM) в контексте генерации видео на основе текста. Авторы предлагают оценивать EWM по трем ключевым аспектам: согласованность визуальной сцены, корректность движения и семантическое соответствие. Бенчмарк включает специально подобранный набор данных с разнообразными сценами и паттернами движения. EWMBench позволяет выявить ограничения существующих моделей генерации видео для задач воплощенного ИИ и направить дальнейшие исследования в этой области.'}, 'en': {'title': "Evaluating AI's Ability to Create Realistic Actions in Video", 'desc': 'This paper discusses the development of embodied world models (EWMs) that generate realistic video scenes based on text descriptions. It highlights the need for better evaluation methods for these models, moving beyond simple visual quality assessments to include physical realism and action consistency. The authors introduce the Embodied World Model Benchmark (EWMBench), which evaluates EWMs on visual scene consistency, motion correctness, and semantic alignment. By providing a curated dataset and evaluation tools, this work aims to improve the performance of video generation models in tasks that require understanding and interaction with the environment.'}, 'zh': {'title': '评估具身世界模型的新基准', 'desc': '最近，创意人工智能的进步使得根据语言指令合成高保真图像和视频成为可能。基于这些发展，文本到视频的扩散模型演变为具身世界模型（EWM），能够根据语言命令生成物理上合理的场景，有效地将视觉与行动结合在具身人工智能应用中。本文提出了具身世界模型基准（EWMBench），旨在通过视觉场景一致性、运动正确性和语义对齐三个关键方面来评估EWM。该基准不仅识别了现有视频生成模型在满足具身任务独特需求方面的局限性，还为未来的研究提供了有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.06027', 'title': 'Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation', 'url': 'https://huggingface.co/papers/2505.06027', 'abstract': "This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.", 'score': 15, 'issue_id': 3804, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': '8abae468afb82fcd', 'authors': ['Stefan Vasilev', 'Christian Herold', 'Baohao Liao', 'Seyyed Hadi Hashemi', 'Shahram Khadivi', 'Christof Monz'], 'affiliations': ['University of Amsterdam', 'eBay Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.06027.jpg', 'data': {'categories': ['#training', '#benchmark', '#dataset', '#security', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Unilogit: умное забывание для больших языковых моделей', 'desc': 'Статья представляет Unilogit - новый метод самодистилляции для машинного забывания в больших языковых моделях. Unilogit решает задачу выборочного забывания конкретной информации при сохранении общей полезности модели, что важно для соответствия требованиям конфиденциальности данных. Метод динамически корректирует целевые логиты для достижения равномерной вероятности целевого токена, используя текущие выходные данные модели. Эксперименты показывают превосходство Unilogit в балансировке задач забывания и сохранения информации по сравнению с современными методами.'}, 'en': {'title': 'Unilogit: Smart Forgetting for Safer AI', 'desc': "This paper presents Unilogit, a new self-distillation technique designed for machine unlearning in Large Language Models. It focuses on the ability to forget specific information while still keeping the model useful, which is important for following data privacy laws like GDPR. Unilogit improves upon previous methods by dynamically adjusting target logits based on the model's current outputs, rather than using fixed hyperparameters. The results show that Unilogit performs better than existing methods in both forgetting and retaining information, proving its effectiveness in real-world applications."}, 'zh': {'title': 'Unilogit：智能遗忘的自我蒸馏新方法', 'desc': '本文介绍了一种新颖的自我蒸馏方法Unilogit，用于大语言模型中的机器遗忘。Unilogit解决了在遵守数据隐私法规（如GDPR）的同时，选择性遗忘特定信息的挑战。与依赖静态超参数或初始模型输出的传统方法不同，Unilogit动态调整目标logits，以实现目标标记的均匀概率，从而提高自我蒸馏目标的准确性。通过在公共基准和内部电子商务数据集上的广泛实验，Unilogit在遗忘与保留目标的平衡方面表现优越，超越了NPO和UnDIAL等最先进的方法。'}}}, {'id': 'https://huggingface.co/papers/2505.07782', 'title': 'MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering', 'url': 'https://huggingface.co/papers/2505.07782', 'abstract': "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.", 'score': 14, 'issue_id': 3791, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '6a556cb214d3d71d', 'authors': ['Rushi Qiang', 'Yuchen Zhuang', 'Yinghao Li', 'Dingu Sagar V K', 'Rongzhi Zhang', 'Changhao Li', 'Ian Shu-Hei Wong', 'Sherry Yang', 'Percy Liang', 'Chao Zhang', 'Bo Dai'], 'affiliations': ['Georgia Institute of Technology', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07782.jpg', 'data': {'categories': ['#data', '#training', '#architecture', '#benchmark', '#open_source', '#games', '#agents', '#rl'], 'emoji': '🤖', 'ru': {'title': 'MLE-Dojo: Интерактивная среда для обучения автономных LLM-агентов', 'desc': 'MLE-Dojo - это фреймворк для обучения, оценки и улучшения автономных агентов на основе больших языковых моделей (LLM) в итеративных рабочих процессах машинного обучения. В отличие от существующих бенчмарков, MLE-Dojo предоставляет интерактивную среду, позволяющую агентам экспериментировать, отлаживать и улучшать решения через структурированные циклы обратной связи. Фреймворк построен на более чем 200 реальных задачах Kaggle и охватывает разнообразные задачи машинного обучения, такие как обработка данных, поиск архитектуры, настройка гиперпараметров и отладка кода. MLE-Dojo поддерживает обучение агентов как с помощью контролируемой настройки, так и с помощью обучения с подкреплением.'}, 'en': {'title': 'MLE-Dojo: Empowering Iterative Learning for LLMs', 'desc': 'MLE-Dojo is a new framework designed for reinforcement learning and improving large language model (LLM) agents through iterative machine learning engineering (MLE) processes. It offers an interactive environment where agents can experiment and refine their solutions based on structured feedback, unlike traditional benchmarks that use static datasets. The framework is built on over 200 real-world Kaggle challenges, allowing for diverse MLE tasks such as data processing and hyperparameter tuning. Evaluations show that while LLMs can make iterative improvements, they still struggle with generating long-term solutions and solving complex issues, highlighting the need for further advancements in autonomous learning.'}, 'zh': {'title': 'MLE-Dojo：推动自主学习模型的创新平台', 'desc': 'MLE-Dojo是一个类似Gym的框架，旨在系统化地进行强化学习，评估和改进自主大型语言模型（LLM）代理。与现有的基准测试不同，MLE-Dojo提供了一个互动环境，使代理能够通过结构化反馈循环进行迭代实验、调试和优化解决方案。该框架基于200多个真实的Kaggle挑战，涵盖了多样化的开放式机器学习工程任务，反映了现实的工程场景。MLE-Dojo的可执行环境支持通过监督微调和强化学习进行全面的代理训练，促进了迭代实验和实时结果验证。'}}}, {'id': 'https://huggingface.co/papers/2505.10558', 'title': 'Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors', 'url': 'https://huggingface.co/papers/2505.10558', 'abstract': 'Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.   To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.', 'score': 13, 'issue_id': 3794, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '949341b92df361ab', 'authors': ['Peiying Zhang', 'Nanxuan Zhao', 'Jing Liao'], 'affiliations': ['Adobe Research', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.10558.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Генерация стилизованной векторной графики с помощью ИИ', 'desc': 'Статья представляет новый метод генерации масштабируемой векторной графики (SVG) с возможностью настройки стиля. Авторы предлагают двухэтапный подход, сочетающий преимущества прямых моделей Text-to-Vector (T2V) и приоров Text-to-Image (T2I). На первом этапе обучается диффузионная модель T2V для обеспечения структурной регулярности SVG. На втором этапе модель T2V настраивается на различные стили путем дистилляции кастомизированных моделей T2I.'}, 'en': {'title': 'Custom SVGs: Merging Structure and Style in Vector Graphics Generation', 'desc': 'This paper presents a new method for generating Scalable Vector Graphics (SVGs) from text prompts while allowing for style customization. The proposed two-stage pipeline combines the strengths of both text-to-vector (T2V) diffusion models and text-to-image (T2I) models to ensure structural regularity and diverse styles. In the first stage, a T2V diffusion model is trained to maintain the SVG structure, while the second stage focuses on customizing the model to different styles using T2I model distillation. The results demonstrate that this approach can efficiently produce high-quality SVGs that meet aesthetic requirements based on user-defined text inputs.'}, 'zh': {'title': '高效生成定制化SVG的创新方法', 'desc': '本论文提出了一种新的两阶段样式定制管道，用于从文本生成可缩放矢量图形（SVG）。第一阶段，我们训练了一个基于路径级表示的文本到矢量（T2V）扩散模型，以确保SVG的结构规则性，同时保留多样的表现能力。第二阶段，通过提炼定制的文本到图像（T2I）模型来实现不同样式的定制。我们的管道能够高效地生成高质量和多样化的SVG，满足实际应用中的样式需求。'}}}, {'id': 'https://huggingface.co/papers/2505.10565', 'title': 'Depth Anything with Any Prior', 'url': 'https://huggingface.co/papers/2505.10565', 'abstract': 'This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.', 'score': 11, 'issue_id': 3794, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '2291c0b9541ea999', 'authors': ['Zehan Wang', 'Siyu Chen', 'Lihe Yang', 'Jialei Wang', 'Ziang Zhang', 'Hengshuang Zhao', 'Zhou Zhao'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10565.jpg', 'data': {'categories': ['#cv', '#dataset', '#training'], 'emoji': '🔍', 'ru': {'title': 'Точные карты глубины для любых сцен через объединение измерений и предсказаний', 'desc': 'Статья представляет Prior Depth Anything - фреймворк для создания точных и детальных карт глубины для любых сцен. Он объединяет неполную, но точную метрическую информацию измерений глубины с относительной, но полной геометрической структурой предсказаний глубины. Авторы разработали конвейер уточнения от грубого к точному, включающий выравнивание на уровне пикселей и взвешивание с учетом расстояния для предварительного заполнения метрических приоров. Модель демонстрирует впечатляющую обобщающую способность на 7 наборах данных реального мира для различных задач, связанных с глубиной.'}, 'en': {'title': 'Merging Depth Insights for Accurate Scene Mapping', 'desc': "This paper introduces Prior Depth Anything, a novel framework that effectively merges incomplete metric depth information with complete geometric structures to create accurate depth maps. It employs a coarse-to-fine approach that integrates these two sources of depth data, enhancing the model's ability to generalize across different scenarios. The framework utilizes pixel-level metric alignment and a conditioned monocular depth estimation model to refine depth predictions and reduce noise. The results demonstrate strong performance in various depth-related tasks, showcasing the model's adaptability and efficiency in real-world applications."}, 'zh': {'title': '深度测量与预测的完美结合', 'desc': '本文提出了一种名为Prior Depth Anything的框架，旨在将不完整但精确的深度测量信息与相对但完整的几何结构结合起来，从而生成准确、密集且详细的深度图。我们设计了一个粗到细的流程，逐步整合这两种互补的深度来源。首先，通过引入像素级的度量对齐和距离感知加权，利用深度预测来预填充多样的度量先验，有效缩小了先验模式之间的领域差距。其次，我们开发了一种条件单目深度估计模型，以进一步精炼深度先验的固有噪声，从而在多个真实世界数据集上展示了出色的零-shot泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.09990', 'title': 'PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing', 'url': 'https://huggingface.co/papers/2505.09990', 'abstract': 'Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. Project page: https://pointarena.github.io/', 'score': 11, 'issue_id': 3796, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'f0065735407ccc64', 'authors': ['Long Cheng', 'Jiafei Duan', 'Yi Ru Wang', 'Haoquan Fang', 'Boyang Li', 'Yushan Huang', 'Elvis Wang', 'Ainaz Eftekhar', 'Jason Lee', 'Wentao Yuan', 'Rose Hendrix', 'Noah A. Smith', 'Fei Xia', 'Dieter Fox', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Anderson Collegiate Vocational Institute', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.09990.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#open_source', '#benchmark', '#robotics', '#dataset'], 'emoji': '👆', 'ru': {'title': 'PointArena: новый стандарт оценки мультимодальных моделей в задачах указания', 'desc': 'Статья представляет PointArena - комплексную платформу для оценки мультимодальных моделей в задачах указания на объекты. Платформа включает набор данных Point-Bench с 1000 задач, интерактивную арену Point-Battle для сравнения моделей, и систему Point-Act для оценки моделей в реальных робототехнических сценариях. Результаты показывают, что модель Molmo-72B превосходит другие, а специализированное обучение значительно улучшает производительность моделей в задачах указания. Исследование подчеркивает важность точных возможностей указания для эффективного применения мультимодальных моделей в реальном мире.'}, 'en': {'title': 'PointArena: Evaluating Multimodal Pointing for Real-World Applications', 'desc': 'This paper introduces PointArena, a new platform designed to evaluate how well multimodal models can perform pointing tasks in various reasoning scenarios. It includes a dataset called Point-Bench with around 1,000 tasks, an interactive comparison tool named Point-Battle, and a robotic system called Point-Act for real-world testing. The study shows that the Molmo-72B model outperforms others, especially when trained specifically for pointing tasks. The findings highlight the importance of accurate pointing in helping models connect abstract reasoning with real-world actions.'}, 'zh': {'title': '多模态指向能力的评估新平台', 'desc': '本文介绍了PointArena，这是一个用于评估多模态指向能力的综合平台。它包含三个主要部分：Point-Bench，一个包含约1000个指向任务的数据集；Point-Battle，一个互动的网络平台，用于模型的盲测比较；以及Point-Act，一个允许用户在实际环境中评估多模态模型指向能力的机器人操作系统。研究表明，专门针对指向任务的监督训练显著提高了模型的性能，强调了精确指向能力在多模态模型中连接抽象推理与实际行动的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.10046', 'title': 'Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2505.10046', 'abstract': 'This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.', 'score': 10, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'e297e154b434ef38', 'authors': ['Bingda Tang', 'Boyang Zheng', 'Xichen Pan', 'Sayak Paul', 'Saining Xie'], 'affiliations': ['Hugging Face', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10046.jpg', 'data': {'categories': ['#multimodal', '#training', '#diffusion', '#survey'], 'emoji': '🔬', 'ru': {'title': 'Глубокий анализ слияния LLM и DiT для генерации изображений по тексту', 'desc': 'Статья представляет собой эмпирическое исследование генерации изображений по текстовому описанию. Авторы проводят детальный анализ объединения больших языковых моделей (LLM) и диффузионных трансформеров (DiT) для мультимодальной генерации. Исследование включает контролируемые сравнения с существующими базовыми методами и анализ ключевых аспектов дизайна. Статья предоставляет четкий и воспроизводимый рецепт для обучения таких моделей в промышленном масштабе.'}, 'en': {'title': 'Unlocking the Potential of Multi-Modal Generation', 'desc': 'This paper investigates the integration of large language models (LLMs) and diffusion transformers (DiTs) in the field of text-to-image synthesis. It highlights the lack of detailed comparisons and transparency in previous studies, which often overlooked critical design choices and training methodologies. By conducting empirical studies and controlled comparisons with established baselines, the authors aim to clarify the potential of this multi-modal generation approach. The paper provides a reproducible training recipe and valuable insights to guide future research in this area.'}, 'zh': {'title': '探索文本到图像合成的新设计空间', 'desc': '本文探讨了文本到图像合成中的一个重要设计空间，特别是大型语言模型（LLMs）与扩散变换器（DiTs）深度融合的多模态生成。以往的研究主要关注系统整体性能，而缺乏对替代方法的详细比较，关键设计细节和训练方案往往未被披露。这些空白导致了对该方法真实潜力的怀疑。为了解决这些问题，我们进行了文本到图像生成的实证研究，进行控制比较，分析重要设计选择，并提供清晰、可重复的训练方案，以期为未来的多模态生成研究提供有意义的数据和实用指南。'}}}, {'id': 'https://huggingface.co/papers/2505.09738', 'title': 'Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning', 'url': 'https://huggingface.co/papers/2505.09738', 'abstract': 'Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this often require prohibitive computational resources. Although tokenizer replacement with heuristic initialization aims to reduce this burden, existing methods often require exhaustive residual fine-tuning and still may not fully preserve semantic nuances or adequately address the underlying compression inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a model-agnostic tokenizer transplantation method, and second, novel pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation. Tokenadapt initializes new unique token embeddings via a hybrid heuristic that combines two methods: a local estimate based on subword decomposition using the old tokenizer, and a global estimate utilizing the top-k semantically similar tokens from the original vocabulary. This methodology aims to preserve semantics while significantly minimizing retraining requirements. Empirical investigations validate both contributions: the transplantation heuristic successfully initializes unique tokens, markedly outperforming conventional baselines and sophisticated methods including Transtokenizer and ReTok, while our Supertokens achieve notable compression gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to both ReTok and TransTokenizer baselines across different base models and newly trained target tokenizers. TokenAdapt typically reduced the overall perplexity ratio significantly compared to ReTok, yielding at least a 2-fold improvement in these aggregate scores.', 'score': 9, 'issue_id': 3795, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': 'ae96fcdfda8fee90', 'authors': ['Shaurya Sharthak', 'Vinayak Pahalwan', 'Adithya Kamath', 'Adarsh Shirawalmath'], 'affiliations': ['proton.me', 'tensoic.com', 'tinycompany.in'], 'pdf_title_img': 'assets/pdf/title_img/2505.09738.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#training', '#transfer_learning', '#data'], 'emoji': '🔄', 'ru': {'title': 'TokenAdapt: Эффективная замена токенизатора в языковых моделях', 'desc': 'Статья представляет новый метод TokenAdapt для замены токенизатора в предобученных языковых моделях (LLM). Авторы предлагают гибридный подход инициализации новых токенов, сочетающий локальную оценку на основе декомпозиции подслов и глобальную оценку с использованием семантически похожих токенов. Метод также включает обучение пре-токенизации для создания мультисловных супертокенов, улучшающих сжатие и уменьшающих фрагментацию. Эмпирические исследования показывают, что TokenAdapt значительно превосходит существующие методы по эффективности и сохранению семантики.'}, 'en': {'title': 'Unlocking Language Models: Efficient Tokenization with Tokenadapt', 'desc': 'This paper addresses the limitations of fixed tokenization schemes in pretrained language models, which can hinder performance, especially in multilingual contexts. The authors propose a novel framework called Tokenadapt, which allows for efficient tokenizer transplantation without extensive retraining. By introducing a hybrid heuristic for initializing new token embeddings and utilizing multi-word Supertokens, the method aims to enhance semantic preservation and reduce fragmentation. Empirical results show that Tokenadapt significantly outperforms existing methods in terms of perplexity, demonstrating its effectiveness in improving language model efficiency.'}, 'zh': {'title': '突破分词限制，提升语言模型性能', 'desc': '预训练语言模型（LLMs）常常受到固定分词方案的限制，这导致在多语言或专业应用中效率低下和性能受限。本文提出了一种新的框架，包含两项创新：Tokenadapt，一种模型无关的分词器移植方法，以及用于多词超级标记的新型预分词学习，以增强压缩效果并减少碎片化。Tokenadapt通过结合旧分词器的子词分解和原始词汇中语义相似的前k个标记的全局估计，初始化新的独特标记嵌入，从而在减少重训练需求的同时保持语义。实验证明，Tokenadapt在初始化独特标记方面显著优于传统基线和其他复杂方法，且超级标记在压缩效果上也取得了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.10566', 'title': '3D-Fixup: Advancing Photo Editing with 3D Priors', 'url': 'https://huggingface.co/papers/2505.10566', 'abstract': 'Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/', 'score': 7, 'issue_id': 3808, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'f1dc934cd3d0961a', 'authors': ['Yen-Chi Cheng', 'Krishna Kumar Singh', 'Jae Shin Yoon', 'Alex Schwing', 'Liangyan Gui', 'Matheus Gadelha', 'Paul Guerrero', 'Nanxuan Zhao'], 'affiliations': ['Adobe Research, UK', 'Adobe Research, USA', 'University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.10566.jpg', 'data': {'categories': ['#cv', '#diffusion', '#3d', '#video', '#training'], 'emoji': '🖼️', 'ru': {'title': '3D-Fixup: Новый уровень редактирования изображений с 3D-приорами', 'desc': '3D-Fixup - это новый фреймворк для редактирования 2D-изображений с использованием изученных 3D-приоров. Он поддерживает сложные операции редактирования, такие как трансляция объектов и 3D-вращение, используя обучающий подход на основе диффузионных моделей. Фреймворк использует видеоданные для генерации обучающих пар и включает 3D-руководство от модели Image-to-3D. Результаты показывают, что 3D-Fixup эффективно поддерживает сложное 3D-редактирование изображений с сохранением идентичности объектов.'}, 'en': {'title': 'Revolutionizing 2D Image Editing with 3D Insights', 'desc': 'The paper introduces 3D-Fixup, a novel framework designed for 2D image editing using learned 3D priors. It addresses the challenges of editing images that require understanding of 3D transformations, such as object translation and rotation. By utilizing video data to create training pairs, the framework enhances the generative capabilities of diffusion models. The integration of an Image-to-3D model allows for effective projection of 2D information into 3D space, resulting in high-quality, coherent edits that push the boundaries of image manipulation.'}, 'zh': {'title': '3D-Fixup：实现高质量的3D感知图像编辑', 'desc': '本论文提出了一种名为3D-Fixup的新框架，用于通过学习的3D先验来编辑2D图像。该框架能够处理复杂的编辑任务，如物体平移和3D旋转。我们利用扩散模型的生成能力，通过视频数据生成训练数据对，以提高编辑效果。通过结合图像到3D模型的3D指导，3D-Fixup能够在2D和3D空间之间进行有效的转换，从而实现高质量的图像编辑。'}}}, {'id': 'https://huggingface.co/papers/2505.10468', 'title': 'AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge', 'url': 'https://huggingface.co/papers/2505.10468', 'abstract': 'This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications', 'score': 7, 'issue_id': 3802, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '613808a0b3d41836', 'authors': ['Ranjan Sapkota', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Department of Environmental and Biological Engineering, USA', 'Department of Informatics and Telecommunications, University of the Peloponnese, 22131 Tripoli, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.10468.jpg', 'data': {'categories': ['#healthcare', '#hallucinations', '#agents', '#architecture', '#multimodal', '#reasoning', '#optimization', '#agi'], 'emoji': '🤖', 'ru': {'title': 'От ИИ-агентов к агентному ИИ: эволюция автономных систем', 'desc': 'Это исследование проводит критическое различие между ИИ-агентами и агентным ИИ, предлагая структурированную концептуальную таксономию, картирование приложений и анализ вызовов. ИИ-агенты характеризуются как модульные системы, управляемые большими языковыми моделями (LLM) и большими моделями изображений (LIM) для узкой, специфичной для задач автоматизации. Агентный ИИ представляет собой парадигматический сдвиг, отмеченный многоагентным сотрудничеством, динамической декомпозицией задач и оркестрованной автономией. Исследование также рассматривает уникальные проблемы в каждой парадигме, включая галлюцинации, хрупкость и проблемы координации.'}, 'en': {'title': 'Understanding AI Agents vs. Agentic AI: A New Frontier in AI Development', 'desc': 'This paper differentiates between AI Agents and Agentic AI, providing a clear framework for understanding their unique characteristics and applications. AI Agents are defined as modular systems that utilize Large Language Models (LLMs) and Large Image Models (LIMs) for specific tasks, while Agentic AI represents a more advanced form that incorporates multi-agent collaboration and dynamic task management. The study evaluates the evolution of these systems, highlighting their operational differences and the challenges they face, such as hallucination and coordination failure. It also suggests solutions like ReAct loops and orchestration layers to enhance the effectiveness of both AI paradigms.'}, 'zh': {'title': '区分人工智能代理与代理智能的未来之路', 'desc': '本研究明确区分了人工智能代理和代理智能，提供了结构化的概念分类、应用映射和挑战分析，以阐明它们不同的设计理念和能力。我们将人工智能代理定义为由大型语言模型（LLMs）和大型图像模型（LIMs）驱动的模块化系统，主要用于特定任务的自动化。代理智能系统则代表了一种范式转变，强调多代理协作、动态任务分解、持久记忆和自主协调。通过对两种范式的架构演变、操作机制、交互风格和自主水平的比较分析，我们为开发强大、可扩展和可解释的人工智能代理和代理智能驱动系统提供了明确的路线图。'}}}, {'id': 'https://huggingface.co/papers/2505.08581', 'title': 'ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking', 'url': 'https://huggingface.co/papers/2505.08581', 'abstract': 'Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.', 'score': 7, 'issue_id': 3793, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '3dab22f0e497d468', 'authors': ['Haofeng Liu', 'Mingqi Gao', 'Xuxiao Luo', 'Ziyue Wang', 'Guanyi Qin', 'Junde Wu', 'Yueming Jin'], 'affiliations': ['National University of Singapore, Singapore, Singapore', 'Southern University of Science and Technology, Shenzhen, China', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2505.08581.jpg', 'data': {'categories': ['#cv', '#healthcare', '#multimodal'], 'emoji': '🔪', 'ru': {'title': 'ReSurgSAM2: Революция в интерактивной сегментации хирургических сцен', 'desc': 'Статья представляет ReSurgSAM2 - двухэтапную систему для сегментации хирургических сцен с использованием текстовых запросов. На первом этапе применяется улучшенная модель SAM2 с пространственно-временной архитектурой Mamba для точного обнаружения и сегментации объектов. Второй этап включает надежный выбор начального кадра и долгосрочное отслеживание с помощью разнообразного банка памяти. Эксперименты показывают значительное улучшение точности и эффективности по сравнению с существующими методами, достигая работы в реальном времени со скоростью 61.2 кадра в секунду.'}, 'en': {'title': 'Revolutionizing Surgical Segmentation with ReSurgSAM2', 'desc': 'This paper presents ReSurgSAM2, a novel framework for surgical scene segmentation that enhances the interactive experience for surgeons. It consists of two main stages: the first stage focuses on text-referred target detection using the Segment Anything Model 2, while the second stage emphasizes reliable tracking through an innovative initial frame selection and a diversity-driven memory mechanism. The proposed method addresses the limitations of existing techniques by improving efficiency and enabling long-term tracking in complex surgical environments. Experimental results show that ReSurgSAM2 significantly outperforms previous methods, achieving real-time performance at 61.2 frames per second.'}, 'zh': {'title': '提升手术分割效率与准确性的ReSurgSAM2框架', 'desc': '本论文介绍了一种名为ReSurgSAM2的手术场景分割框架，旨在提高计算机辅助手术的效率和准确性。该框架采用了两阶段的手术参考分割方法，利用Segment Anything Model 2进行文本引用目标检测，并通过可靠的初始帧识别和多样性驱动的长期记忆进行跟踪。我们提出了一种跨模态时空Mamba，用于生成精确的检测和分割结果，并通过可信的初始帧选择策略来确保后续跟踪的可靠性。实验结果表明，ReSurgSAM2在准确性和效率上显著优于现有方法，能够以61.2 FPS的实时速度运行。'}}}, {'id': 'https://huggingface.co/papers/2505.10167', 'title': 'QuXAI: Explainers for Hybrid Quantum Machine Learning Models', 'url': 'https://huggingface.co/papers/2505.10167', 'abstract': 'The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still in its infancy, a major research gap is evident in robust global and local explainability approaches that are designed for HQML architectures that employ quantized feature encoding followed by classical learning. The gap is the focus of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an explainer for explaining feature importance in these hybrid systems. Our model entails the creation of HQML models incorporating quantum feature maps, the use of Q-MEDLEY, which combines feature based inferences, preserving the quantum transformation stage and visualizing the resulting attributions. Our result shows that Q-MEDLEY delineates influential classical aspects in HQML models, as well as separates their noise, and competes well against established XAI techniques in classical validation settings. Ablation studies more significantly expose the virtues of the composite structure used in Q-MEDLEY. The implications of this work are critically important, as it provides a route to improve the interpretability and reliability of HQML models, thus promoting greater confidence and being able to engage in safer and more responsible use of quantum-enhanced AI technology.', 'score': 5, 'issue_id': 3801, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '924a78566cebf728', 'authors': ['Saikat Barua', 'Mostafizur Rahman', 'Shehenaz Khaled', 'Md Jafor Sadek', 'Rafiul Islam', 'Shahnewaz Siddique'], 'affiliations': ['North South University, Dhaka'], 'pdf_title_img': 'assets/pdf/title_img/2505.10167.jpg', 'data': {'categories': ['#data', '#inference', '#interpretability', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'QuXAI: Прозрачность квантово-классического ИИ', 'desc': 'Эта статья представляет QuXAI - фреймворк для объяснения гибридных квантово-классических моделей машинного обучения (HQML). В основе QuXAI лежит Q-MEDLEY - объяснитель, который определяет важность признаков в таких гибридных системах. Фреймворк позволяет создавать HQML модели с квантовыми отображениями признаков, применять Q-MEDLEY и визуализировать результаты. Результаты показывают, что Q-MEDLEY эффективно выделяет влиятельные классические аспекты в HQML моделях и конкурирует с классическими методами XAI.'}, 'en': {'title': 'Unlocking the Black Box of Hybrid Quantum-Classical AI', 'desc': 'This paper addresses the challenge of understanding hybrid quantum-classical machine learning (HQML) models, which often behave like black boxes. It introduces QuXAI, a framework that enhances explainability in HQML by utilizing Q-MEDLEY, an explainer that assesses feature importance while maintaining the quantum transformation process. The study demonstrates that Q-MEDLEY effectively identifies key classical features and reduces noise in HQML models, outperforming traditional explainable AI (XAI) methods in classical validation tests. Ultimately, this work aims to improve the transparency and reliability of HQML systems, fostering safer and more responsible applications of quantum-enhanced AI.'}, 'zh': {'title': '提升混合量子-经典模型的可解释性与可靠性', 'desc': '混合量子-经典机器学习（HQML）模型的出现为计算智能开辟了新视野，但其复杂性常导致黑箱行为，影响透明性和可靠性。本文提出了QuXAI框架，旨在填补针对HQML架构的可解释性研究空白，特别是结合量子特征编码和经典学习的模型。QuXAI基于Q-MEDLEY，能够解释特征重要性，并通过可视化结果归因来揭示HQML模型中的经典影响因素。研究结果表明，Q-MEDLEY在经典验证环境中表现良好，提升了HQML模型的可解释性和可靠性，促进了量子增强人工智能技术的安全使用。'}}}, {'id': 'https://huggingface.co/papers/2505.09926', 'title': 'AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection', 'url': 'https://huggingface.co/papers/2505.09926', 'abstract': 'Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.', 'score': 5, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '25c3eaaddbc9d5ca', 'authors': ['Bin-Bin Gao', 'Yue Zhu', 'Jiangtao Yan', 'Yuezhi Cai', 'Weixi Zhang', 'Meng Wang', 'Jun Liu', 'Yong Liu', 'Lei Wang', 'Chengjie Wang'], 'affiliations': ['Shanghai Jiao Tong University', 'Siemens Corporate Research', 'Technical University of Munich', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09926.jpg', 'data': {'categories': ['#cv', '#benchmark', '#open_source', '#transfer_learning', '#healthcare'], 'emoji': '🔍', 'ru': {'title': 'AdaptCLIP: Эффективное обнаружение аномалий с помощью адаптации CLIP', 'desc': 'Статья представляет новый метод универсального обнаружения визуальных аномалий под названием AdaptCLIP. Этот метод основан на предобученной мультимодальной модели CLIP и использует три адаптера для улучшения ее производительности. AdaptCLIP обучается поочередно адаптивным визуальным и текстовым представлениям, а также использует как контекстные, так и выровненные остаточные признаки. Метод демонстрирует state-of-the-art результаты на 12 бенчмарках обнаружения аномалий в промышленных и медицинских доменах.'}, 'en': {'title': 'AdaptCLIP: Simplifying Anomaly Detection with Adaptive Learning', 'desc': 'This paper introduces AdaptCLIP, a novel approach for universal visual anomaly detection that operates without the need for additional fine-tuning. It leverages pre-trained vision-language models like CLIP to generalize effectively from just a few normal images. The method emphasizes learning adaptive visual and textual representations alternately and incorporates comparative learning that utilizes both contextual and aligned residual features. AdaptCLIP demonstrates superior performance on various anomaly detection benchmarks, showcasing its flexibility and efficiency in handling unseen vision domains.'}, 'zh': {'title': 'AdaptCLIP：无微调的通用视觉异常检测', 'desc': '本论文提出了一种名为AdaptCLIP的通用视觉异常检测方法，旨在无需额外微调即可识别新颖或未见的视觉领域中的异常。研究表明，预训练的视觉-语言模型如CLIP在仅使用零或少量正常图像时展现出强大的泛化能力。AdaptCLIP通过交替学习自适应的视觉和文本表示，结合上下文和对齐的残差特征进行比较学习，从而提高了灵活性。该方法在12个工业和医疗领域的异常检测基准上实现了最先进的性能，显著超越了现有的竞争方法。'}}}, {'id': 'https://huggingface.co/papers/2505.09601', 'title': 'Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware', 'url': 'https://huggingface.co/papers/2505.09601', 'abstract': 'Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com', 'score': 4, 'issue_id': 3811, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '65500a5b9d477d08', 'authors': ['Justin Yu', 'Letian Fu', 'Huang Huang', 'Karim El-Refai', 'Rares Andrei Ambrus', 'Richard Cheng', 'Muhammad Zubair Irshad', 'Ken Goldberg'], 'affiliations': ['Toyota Research Institute', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.09601.jpg', 'data': {'categories': ['#optimization', '#dataset', '#synthetic', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении роботов: от одной демонстрации к тысячам симуляций', 'desc': 'R2R2R - новый подход к генерации обучающих данных для роботов без использования симуляции динамики объектов или телеуправления реальным оборудованием. Метод использует смартфон для сканирования объектов и одно видео с человеческой демонстрацией для создания тысяч высококачественных визуальных демонстраций, независимых от конкретного робота. R2R2R применяет технологию 3D Gaussian Splatting для гибкой генерации ассетов и синтеза траекторий как для жестких, так и для шарнирных объектов. Эксперименты показывают, что модели, обученные на данных R2R2R из одной человеческой демонстрации, могут сравниться по эффективности с моделями, обученными на 150 демонстрациях с телеуправлением.'}, 'en': {'title': 'Revolutionizing Robot Training with R2R2R', 'desc': 'The paper presents Real2Render2Real (R2R2R), a new method for generating training data for robots without the need for expensive human teleoperation or complex simulations. It utilizes smartphone scans and a single video of human actions to create thousands of high-quality, robot-agnostic demonstrations. By employing 3D Gaussian Splatting, R2R2R can flexibly generate 3D models and trajectories for various objects, which are then converted into meshes for use in rendering engines. Experiments show that models trained on data from R2R2R can achieve performance comparable to those trained on a much larger dataset of human demonstrations.'}, 'zh': {'title': '机器人学习的新方法：无需遥控即可生成训练数据', 'desc': '本文提出了一种名为Real2Render2Real（R2R2R）的新方法，用于生成机器人训练数据，而无需依赖物体动力学模拟或人工遥控。该方法通过智能手机捕捉的物体扫描和人类演示视频，重建详细的3D物体几何形状和外观，生成高视觉保真度的机器人演示。R2R2R利用3D高斯点云（3DGS）技术，灵活生成资产和轨迹，适用于刚性和关节物体，并将这些表示转换为网格，以便与可扩展的渲染引擎兼容。实验表明，基于R2R2R数据训练的模型可以与基于150个人工遥控演示训练的模型性能相匹配。'}}}, {'id': 'https://huggingface.co/papers/2505.09265', 'title': 'MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning', 'url': 'https://huggingface.co/papers/2505.09265', 'abstract': 'Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '62287f3647d6b6a7', 'authors': ['Bin-Bin Gao'], 'affiliations': ['Tencent YouTu Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09265.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#synthetic', '#dataset'], 'emoji': '👁️', 'ru': {'title': 'Универсальная сегментация аномалий без языковых подсказок', 'desc': 'Статья представляет новый подход к сегментации визуальных аномалий, основанный на чисто визуальной модели без использования языковых подсказок. Авторы предлагают парадигму, объединяющую сегментацию аномалий и сегментацию изменений, используя синтетические пары изображений для обучения. Разработан фреймворк MetaUAS, использующий мета-обучение и способный обобщаться на новые типы аномалий. Метод превосходит существующие решения для zero-shot, few-shot и full-shot сегментации аномалий.'}, 'en': {'title': 'Revolutionizing Anomaly Segmentation with Pure Vision Models', 'desc': 'This paper introduces a new approach for visual anomaly segmentation that does not depend on language, using a pure visual foundation model instead of traditional vision-language models. The authors propose a unified framework that treats anomaly segmentation as a form of change segmentation, allowing the use of synthetic image pairs to train the model. They develop a Meta-learning framework called MetaUAS, which can effectively identify unseen anomalies with just one normal image prompt. The method includes a soft feature alignment module to address geometric differences, achieving superior performance compared to existing anomaly segmentation techniques.'}, 'zh': {'title': '纯视觉模型实现通用异常分割', 'desc': '本文探讨了一种新的视觉异常分割方法，称为MetaUAS，旨在使用纯视觉模型而非传统的视觉-语言模型。该方法通过将异常分割统一为变化分割，利用大规模合成图像对进行训练，从而实现对未知异常的有效分割。我们提出的软特征对齐模块能够处理提示图像和查询图像之间的几何变化，提升了分割的准确性。MetaUAS在零-shot、少-shot和全-shot异常分割方法中表现优异，展示了其在实际应用中的广泛适用性。'}}}, {'id': 'https://huggingface.co/papers/2505.09264', 'title': 'Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt', 'url': 'https://huggingface.co/papers/2505.09264', 'abstract': 'Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': 'd89f10b01e706c42', 'authors': ['Bin-Bin Gao'], 'affiliations': ['Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09264.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#survey', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'OneNIP: Обнаружение аномалий по одному нормальному изображению', 'desc': 'Эта статья представляет новый метод OneNIP для обнаружения аномалий в изображениях с использованием всего одного эталонного нормального изображения. В отличие от предыдущих подходов, OneNIP способен реконструировать как нормальные, так и аномальные признаки, что повышает эффективность унифицированного обнаружения аномалий. Авторы также предлагают супервизорный уточнитель для улучшения сегментации аномалий на уровне пикселей. Метод OneNIP превзошел существующие решения на трех промышленных наборах данных для обнаружения аномалий.'}, 'en': {'title': 'Revolutionizing Anomaly Detection with One Normal Image Prompt', 'desc': 'This paper introduces a novel method called One Normal Image Prompt (OneNIP) for improving anomaly detection using self-attention transformers. Traditional models struggle with accurately identifying anomalies because they can reconstruct both normal and anomalous features too well, leading to confusion. OneNIP addresses this by allowing the model to reconstruct normal features while effectively restoring anomalies using just one normal image as a reference. Additionally, a supervised refiner is proposed to enhance pixel-level segmentation of anomalies, resulting in superior performance on multiple industry benchmarks.'}, 'zh': {'title': '用一个正常图像提示提升异常检测性能', 'desc': '本文提出了一种新的无监督重建网络方法，称为One Normal Image Prompt（OneNIP），用于多类异常检测。与传统方法不同，OneNIP只需一个正常图像提示即可重建或恢复异常特征，从而提高了异常检测的性能。该方法通过引入监督精炼器，利用真实正常图像和合成异常图像来回归重建误差，显著改善了像素级异常分割。实验结果表明，OneNIP在多个行业异常检测基准上超越了之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2505.09263', 'title': 'Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation', 'url': 'https://huggingface.co/papers/2505.09263', 'abstract': 'Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '1cde18cecb1ca918', 'authors': ['Guan Gui', 'Bin-Bin Gao', 'Jun Liu', 'Chengjie Wang', 'Yunsheng Wu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09263.jpg', 'data': {'categories': ['#data', '#cv', '#training', '#benchmark', '#dataset', '#synthetic', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Генерация аномалий по нескольким примерам для улучшения их обнаружения', 'desc': 'Статья представляет метод AnoGen для обнаружения аномалий в промышленной инспекции с использованием малого количества реальных аномальных образцов. Метод использует диффузионную модель для генерации реалистичных и разнообразных аномалий на основе нескольких реальных примеров. AnoGen включает три этапа: изучение распределения аномалий, генерация новых аномалий с помощью диффузионной модели и слабоконтролируемое обучение модели обнаружения аномалий. Эксперименты на наборе данных MVTec показали значительное улучшение производительности базовых моделей DRAEM и DesTSeg в задачах классификации и сегментации аномалий.'}, 'en': {'title': 'Generating Realistic Anomalies for Better Detection', 'desc': 'This paper presents a novel method called Few-shot Anomaly-driven Generation (AnoGen) for improving anomaly detection in industrial settings. The approach addresses the challenge of limited real anomaly samples by using a diffusion model to generate realistic anomalies based on a few existing examples. It consists of three stages: learning the anomaly distribution, guiding the generation of diverse anomalies, and training a weakly-supervised detection model. The results show significant improvements in both anomaly classification and segmentation tasks, demonstrating the effectiveness of the generated anomalies in enhancing model performance.'}, 'zh': {'title': '少样本驱动的异常生成，提升检测性能！', 'desc': '异常检测是一项实际且具有挑战性的任务，因为工业检测中异常样本稀缺。现有的一些异常检测方法通过合成噪声或外部数据来解决这个问题，但合成异常与真实异常之间存在较大的语义差距，导致检测性能较弱。为了解决这个问题，我们提出了一种少量样本驱动的异常生成方法（AnoGen），该方法利用少量真实异常指导扩散模型生成真实且多样的异常，从而有利于训练异常检测模型。我们的实验表明，生成的异常有效提高了模型在异常分类和分割任务上的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.07096', 'title': 'X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real', 'url': 'https://huggingface.co/papers/2505.07096', 'abstract': 'Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.', 'score': 3, 'issue_id': 3806, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 мая', 'en': 'May 11', 'zh': '5月11日'}, 'hash': 'cb9029d3f1641bd2', 'authors': ['Prithwish Dan', 'Kushal Kedia', 'Angela Chao', 'Edward Weiyi Duan', 'Maximus Adrian Pace', 'Wei-Chiu Ma', 'Sanjiban Choudhury'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07096.jpg', 'data': {'categories': ['#synthetic', '#rl', '#training', '#robotics', '#optimization', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Роботы учатся у людей без прямого копирования движений', 'desc': 'X-Sim - это новый подход к обучению роботов манипуляциям на основе видео с людьми. Метод использует фотореалистичное моделирование и отслеживание движения объектов для создания наград в симуляции. Затем обученная политика переносится в реальный мир с помощью диффузионной модели и адаптации домена. X-Sim превосходит базовые методы на 30% и не требует данных телеоперации робота.'}, 'en': {'title': 'X-Sim: Bridging Human Videos to Robot Actions Efficiently', 'desc': 'The paper introduces X-Sim, a framework designed to train robot manipulation policies using human videos without needing action labels. It reconstructs a realistic simulation from RGBD videos and tracks object movements to create rewards for reinforcement learning. The learned policy is then refined into a diffusion policy that adapts to real-world conditions through online domain adaptation. X-Sim demonstrates significant improvements in task performance and efficiency compared to existing methods, showcasing its ability to generalize across different environments and viewpoints.'}, 'zh': {'title': 'X-Sim：从人类视频到机器人操作的智能转移', 'desc': '本论文提出了一种名为X-Sim的框架，用于通过人类视频训练机器人操作策略。X-Sim利用物体运动作为密集且可转移的信号，首先从RGBD人类视频重建出逼真的仿真环境，并跟踪物体轨迹以定义以物体为中心的奖励。然后，这些奖励用于在仿真中训练强化学习（RL）策略，并通过合成的视角和光照变化渲染的回放将学习到的策略提炼为图像条件扩散策略。最后，X-Sim引入在线领域适应技术，在部署过程中对真实和仿真观察进行对齐，从而实现无机器人遥控数据的有效转移。'}}}, {'id': 'https://huggingface.co/papers/2505.09568', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset', 'url': 'https://huggingface.co/papers/2505.09568', 'abstract': 'Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.', 'score': 46, 'issue_id': 3768, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '822f8dd79d39211b', 'authors': ['Jiuhai Chen', 'Zhiyang Xu', 'Xichen Pan', 'Yushi Hu', 'Can Qin', 'Tom Goldstein', 'Lifu Huang', 'Tianyi Zhou', 'Saining Xie', 'Silvio Savarese', 'Le Xue', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['New York University', 'Salesforce Research', 'UC Davis', 'University of Maryland', 'University of Washington', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.09568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#open_source', '#multimodal', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Объединение понимания и генерации изображений с помощью диффузионных трансформеров', 'desc': 'Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений.'}, 'en': {'title': 'Unifying Image Understanding and Generation with BLIP3-o', 'desc': 'This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks.'}, 'zh': {'title': '统一图像理解与生成的创新模型', 'desc': '本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.04410', 'title': 'DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception', 'url': 'https://huggingface.co/papers/2505.04410', 'abstract': "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.", 'score': 35, 'issue_id': 3774, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '24fee436fe24f861', 'authors': ['Junjie Wang', 'Bin Chen', 'Yulin Li', 'Bin Kang', 'Yichi Chen', 'Zhuotao Tian'], 'affiliations': ['International Research Institute for Artificial Intelligence, HIT, Shenzhen', 'School of Computer Science and Technology, HIT, Shenzhen', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.04410.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#cv', '#optimization', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'DeCLIP: Новый шаг к универсальному компьютерному зрению', 'desc': "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотного предсказания с открытым словарем. Авторы предлагают разделить модуль самовнимания CLIP на 'содержательные' и 'контекстные' признаки для повышения локальной различимости и пространственной согласованности. DeCLIP показывает значительное улучшение результатов в задачах обнаружения объектов и семантической сегментации по сравнению с существующими методами. Этот подход позволяет преодолеть ограничения предопределенных категорий в задачах компьютерного зрения."}, 'en': {'title': 'Enhancing Dense Visual Predictions with DeCLIP', 'desc': "This paper introduces DeCLIP, a new framework designed to improve dense visual prediction tasks by enhancing the capabilities of Vision-Language Models (VLMs) like CLIP. The authors identify that CLIP's image tokens fail to effectively gather information from related regions, leading to poor local feature representation. DeCLIP addresses this by separating the self-attention mechanism into 'content' and 'context' features, which improves local discriminability and maintains spatial relationships. The results show that DeCLIP outperforms existing methods in various open-vocabulary tasks such as object detection and semantic segmentation."}, 'zh': {'title': 'DeCLIP：提升视觉语言模型的密集预测能力', 'desc': '本论文提出了一种新的框架DeCLIP，旨在改善视觉语言模型CLIP在密集视觉预测任务中的表现。我们发现CLIP的图像标记在聚合空间或语义相关区域的信息时存在困难，导致特征缺乏局部可区分性和空间一致性。DeCLIP通过解耦自注意力模块，分别提取“内容”和“上下文”特征，从而提高局部可区分性并保持空间相关性。实验结果表明，DeCLIP在多个开放词汇密集预测任务中显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.09343', 'title': 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures', 'url': 'https://huggingface.co/papers/2505.09343', 'abstract': "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.", 'score': 24, 'issue_id': 3773, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '3c249078ec32a334', 'authors': ['Chenggang Zhao', 'Chengqi Deng', 'Chong Ruan', 'Damai Dai', 'Huazuo Gao', 'Jiashi Li', 'Liyue Zhang', 'Panpan Huang', 'Shangyan Zhou', 'Shirong Ma', 'Wenfeng Liang', 'Ying He', 'Yuqing Wang', 'Yuxuan Liu', 'Y. X. Wei'], 'affiliations': ['DeepSeek-AI Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09343.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Совместное проектирование моделей и оборудования для масштабирования ИИ', 'desc': 'Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограничений современного аппаратного обеспечения при обучении больших языковых моделей (LLM). Авторы представляют ключевые инновации, включая Multi-head Latent Attention (MLA) для повышения эффективности использования памяти и архитектуру Mixture of Experts (MoE) для оптимизации баланса между вычислениями и коммуникацией. В работе также обсуждается применение смешанной точности FP8 для максимального использования возможностей оборудования и использование многоплоскостной сетевой топологии для минимизации накладных расходов на уровне кластера. На основе опыта разработки DeepSeek-V3 авторы предлагают направления для будущих исследований в области аппаратного обеспечения для ИИ.'}, 'en': {'title': 'Innovating AI: Bridging Hardware and Model Design for Scalable Solutions', 'desc': 'This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design.'}, 'zh': {'title': '硬件与模型共同设计，推动AI创新', 'desc': '这篇论文讨论了大型语言模型（LLMs）在硬件架构上的限制，包括内存容量、计算效率和互连带宽等问题。DeepSeek-V3模型在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型共同设计如何有效解决这些挑战。论文分析了DeepSeek-V3/R1模型架构及其AI基础设施，介绍了多头潜在注意力（MLA）、专家混合（MoE）架构和FP8混合精度训练等创新。最后，作者与学术界和工业界同行探讨了未来硬件的发展方向，强调了硬件与模型共同设计在满足AI工作负载需求中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.09358', 'title': 'Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\n  Image Analysis', 'url': 'https://huggingface.co/papers/2505.09358', 'abstract': "The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io", 'score': 13, 'issue_id': 3777, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '88afa5d56831ceb4', 'authors': ['Bingxin Ke', 'Kevin Qu', 'Tianfu Wang', 'Nando Metzger', 'Shengyu Huang', 'Bo Li', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2505.09358.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#synthetic', '#transfer_learning', '#training'], 'emoji': '🌼', 'ru': {'title': 'Marigold: Раскрытие потенциала генеративных моделей для плотного анализа изображений', 'desc': 'Статья представляет Marigold - семейство условных генеративных моделей и протокол дообучения, которые извлекают знания из предобученных моделей латентной диффузии, таких как Stable Diffusion. Marigold адаптирует эти модели для задач плотного анализа изображений, включая оценку монокулярной глубины, предсказание нормалей поверхности и внутреннюю декомпозицию. Модель требует минимальной модификации архитектуры предобученной модели латентной диффузии и обучается на небольших синтетических наборах данных на одном GPU в течение нескольких дней. Marigold демонстрирует передовую обобщающую способность в режиме zero-shot.'}, 'en': {'title': 'Unlocking Image Analysis with Pretrained Generative Models', 'desc': 'This paper introduces Marigold, a set of conditional generative models designed to leverage pretrained latent diffusion models for various dense image analysis tasks. By fine-tuning these models, Marigold can effectively adapt to tasks like monocular depth estimation and surface normals prediction with minimal changes to the original architecture. The approach allows for training on small synthetic datasets, making it efficient and accessible for users with limited resources. Notably, Marigold achieves impressive zero-shot generalization, showcasing its potential in data-scarce environments.'}, 'zh': {'title': 'Marigold：高效的图像分析生成模型', 'desc': '深度学习在计算机视觉领域的成功依赖于大量标注数据集和强大的预训练模型。在数据稀缺的情况下，这些预训练模型的质量对有效的迁移学习至关重要。最近，文本到图像的生成模型，特别是使用去噪扩散的潜在空间模型，开创了一类新的基础模型，这些模型在大量带注释的图像数据集上进行训练。本文介绍了Marigold，一个条件生成模型的家族及其微调协议，能够提取预训练潜在扩散模型的知识，并将其适应于密集图像分析任务。'}}}, {'id': 'https://huggingface.co/papers/2505.08787', 'title': 'UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations', 'url': 'https://huggingface.co/papers/2505.08787', 'abstract': 'Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.', 'score': 12, 'issue_id': 3779, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '385e98801f0b0c4a', 'authors': ['Hanjung Kim', 'Jaehyun Kang', 'Hyolim Kang', 'Meedeum Cho', 'Seon Joo Kim', 'Youngwoon Lee'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08787.jpg', 'data': {'categories': ['#video', '#transfer_learning', '#dataset', '#robotics', '#agents'], 'emoji': '🤖', 'ru': {'title': 'UniSkill: Обучение роботов человеческим навыкам без разметки', 'desc': 'В статье представлен UniSkill - новый фреймворк для обучения роботов навыкам на основе видео с людьми. Он создает представления навыков, независимые от воплощения, используя масштабные видеоданные без разметки. UniSkill позволяет переносить навыки, извлеченные из видео с людьми, на политики роботов, обученные только на данных роботов. Эксперименты показали, что такой подход успешно направляет действия роботов даже для новых видеопромптов.'}, 'en': {'title': 'Bridging Human-Robot Learning with UniSkill', 'desc': 'This paper introduces UniSkill, a framework designed to help robots learn skills by observing human actions without needing labeled data. It addresses the challenge of differences in appearance and capabilities between humans and robots by using large-scale video data that captures both. UniSkill creates skill representations that are not tied to any specific embodiment, allowing robots to apply learned skills from human videos to their own tasks. The results demonstrate that robots can effectively choose actions based on human video prompts, even when those prompts are new to them.'}, 'zh': {'title': '跨体现技能学习的新突破', 'desc': '模仿是人类学习新任务的基本机制，通过观察和模仿专家来学习。然而，将这种能力应用于机器人面临重大挑战，因为人类和机器人的外观和物理能力存在固有差异。本文提出了UniSkill，一个新颖的框架，可以从大规模的跨体现视频数据中学习与体现无关的技能表示，而无需任何标签，从而使从人类视频提示中提取的技能能够有效转移到仅在机器人数据上训练的机器人策略中。我们的实验表明，这些跨体现技能能够成功指导机器人选择合适的动作，即使在面对未见过的视频提示时。'}}}, {'id': 'https://huggingface.co/papers/2505.07849', 'title': 'SweRank: Software Issue Localization with Code Ranking', 'url': 'https://huggingface.co/papers/2505.07849', 'abstract': "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community.", 'score': 6, 'issue_id': 3777, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': 'b9c609d9756c1056', 'authors': ['Revanth Gangi Reddy', 'Tarun Suresh', 'JaeHyeok Doo', 'Ye Liu', 'Xuan Phi Nguyen', 'Yingbo Zhou', 'Semih Yavuz', 'Caiming Xiong', 'Heng Ji', 'Shafiq Joty'], 'affiliations': ['KAIST AI', 'Salesforce Research', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07849.jpg', 'data': {'categories': ['#data', '#benchmark', '#optimization', '#dataset', '#survey', '#agents'], 'emoji': '🔍', 'ru': {'title': 'SweRank: эффективная локализация проблем в коде с помощью извлечения и переранжирования', 'desc': 'SweRank - это эффективная система для локализации проблем в программном обеспечении, использующая подход извлечения и переранжирования. Авторы создали большой датасет SweLoc, содержащий описания реальных проблем и соответствующие изменения кода из репозиториев GitHub. Эмпирические результаты показывают, что SweRank превосходит как предыдущие модели ранжирования, так и дорогостоящие системы на основе агентов, использующие закрытые языковые модели. SweLoc также демонстрирует свою полезность для улучшения существующих моделей извлечения и переранжирования в задаче локализации проблем.'}, 'en': {'title': 'SweRank: Efficient Software Issue Localization with SweLoc Dataset', 'desc': 'This paper presents SweRank, a new framework designed to improve software issue localization by efficiently retrieving and re-ranking code relevant to natural language issue descriptions. Unlike traditional models that struggle with the complexity of verbose queries, SweRank leverages a large-scale dataset called SweLoc, which contains real-world issue descriptions and their corresponding code changes. The empirical results indicate that SweRank outperforms existing models, including those based on costly closed-source large language models, in terms of accuracy and efficiency. This work not only introduces a novel approach to issue localization but also provides a valuable dataset for future research in the field.'}, 'zh': {'title': '高效软件问题定位的新方法', 'desc': '软件问题定位是识别与自然语言问题描述（如错误报告、功能请求）相关的代码位置（文件、类或函数）的任务，这在软件开发中至关重要但耗时。尽管最近基于大型语言模型（LLM）的代理方法显示出潜力，但由于复杂的多步骤推理和依赖于封闭源LLM，往往会导致显著的延迟和成本。传统的代码排名模型通常针对查询到代码或代码到代码的检索进行优化，但在处理冗长和描述性失败的定位查询时表现不佳。为了解决这个问题，我们提出了SweRank，一个高效且有效的软件问题定位检索与重排名框架，并构建了SweLoc，一个来自公共GitHub库的大规模数据集，包含真实问题描述及相应的代码修改。'}}}, {'id': 'https://huggingface.co/papers/2505.09558', 'title': 'WavReward: Spoken Dialogue Models With Generalist Reward Evaluators', 'url': 'https://huggingface.co/papers/2505.09558', 'abstract': "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.", 'score': 5, 'issue_id': 3783, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '74f9831e9fa216f6', 'authors': ['Shengpeng Ji', 'Tianle Liang', 'Yangzhuo Li', 'Jialong Zuo', 'Minghui Fang', 'Jinzheng He', 'Yifu Chen', 'Zhengqing Liu', 'Ziyue Jiang', 'Xize Cheng', 'Siqi Zheng', 'Jin Xu', 'Junyang Lin', 'Zhou Zhao'], 'affiliations': ['Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.09558.jpg', 'data': {'categories': ['#audio', '#reasoning', '#open_source', '#rlhf', '#dataset', '#benchmark'], 'emoji': '🎙️', 'ru': {'title': 'WavReward: прорыв в оценке разговорных ИИ-систем', 'desc': 'Статья представляет WavReward - модель оценки разговорных систем на основе аудио. Эта модель способна анализировать как интеллектуальные, так и эмоциональные аспекты диалоговых систем, работающих с речевым вводом. WavReward использует аудио языковые модели и механизмы обучения с подкреплением для глубокого анализа диалогов. Для обучения модели был создан датасет ChatReward-30K, охватывающий различные сценарии разговоров и акустические характеристики.'}, 'en': {'title': 'WavReward: Revolutionizing Evaluation for Spoken Dialogue Systems', 'desc': 'This paper introduces WavReward, a novel evaluation model designed specifically for spoken dialogue systems. Unlike traditional text-based models, WavReward assesses both the intelligence (IQ) and emotional understanding (EQ) of dialogue systems using audio inputs. It employs a reinforcement learning approach with multi-sample feedback to enhance its evaluation capabilities. The model demonstrates significant improvements in accuracy and user preference over existing evaluation methods, showcasing its effectiveness in various dialogue scenarios.'}, 'zh': {'title': 'WavReward：提升对话系统评估的新方法', 'desc': '本文提出了一种新的评估模型WavReward，用于评估基于音频的对话系统的表现。WavReward结合了深度推理过程和非线性奖励机制，能够同时评估对话系统的智商和情商。我们还引入了ChatReward-30K数据集，用于训练WavReward，涵盖了对话模型的理解和生成能力。实验结果表明，WavReward在多个对话场景中显著优于现有的评估模型，提升了客观准确率和主观测试的表现。'}}}, {'id': 'https://huggingface.co/papers/2502.12894', 'title': 'CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image', 'url': 'https://huggingface.co/papers/2502.12894', 'abstract': "Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.", 'score': 5, 'issue_id': 3779, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'aeb3d023043e8a18', 'authors': ['Kaixin Yao', 'Longwen Zhang', 'Xinhao Yan', 'Yan Zeng', 'Qixuan Zhang', 'Wei Yang', 'Lan Xu', 'Jiayuan Gu', 'Jingyi Yu'], 'affiliations': ['Deemos Technology Co., Ltd., China', 'Huazhong University of Science and Technology, China', 'ShanghaiTech University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12894.jpg', 'data': {'categories': ['#3d', '#graphs', '#robotics', '#optimization'], 'emoji': '🏙️', 'ru': {'title': 'Реконструкция 3D-сцен с учетом компонентов и физики из одного изображения', 'desc': 'CAST - это новый метод реконструкции 3D-сцен из одного RGB-изображения. Он использует сегментацию объектов, анализ пространственных отношений с помощью GPT и генерацию полной геометрии каждого объекта. CAST применяет модель выравнивания для точного размещения объектов в сцене и физически корректного исправления их позиций. Метод решает проблемы окклюзий, проникновения объектов и обеспечивает реалистичное отображение физических взаимодействий в сцене.'}, 'en': {'title': 'CAST: Transforming 2D Images into Coherent 3D Scenes', 'desc': "The paper presents CAST, a new method for reconstructing high-quality 3D scenes from a single RGB image. It begins by extracting 2D segmentations and depth information, then uses a GPT-based model to analyze how objects relate spatially within the scene. CAST generates each object's geometry while addressing occlusions and partial data through a large-scale 3D generation model. Finally, it ensures physical consistency in the scene by optimizing object poses with a physics-aware correction step, making it useful for applications in robotics."}, 'zh': {'title': 'CAST：从单张RGB图像重建高质量3D场景的创新方法', 'desc': '本文提出了一种名为CAST的新方法，用于从单张RGB图像中重建高质量的3D场景。该方法首先提取对象级的2D分割和相对深度信息，然后利用基于GPT的模型分析对象之间的空间关系，以实现更连贯的重建。CAST还采用了一个考虑遮挡的大规模3D生成模型，独立生成每个对象的完整几何形状，并通过MAE和点云条件来减轻遮挡和部分对象信息的影响。最后，CAST通过物理感知的校正步骤，确保生成的场景在物理上保持一致，适用于机器人等领域。'}}}, {'id': 'https://huggingface.co/papers/2505.09439', 'title': 'Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?', 'url': 'https://huggingface.co/papers/2505.09439', 'abstract': 'We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.', 'score': 4, 'issue_id': 3778, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '95b580ea2dcf9967', 'authors': ['Andrew Rouditchenko', 'Saurabhchand Bhati', 'Edson Araujo', 'Samuel Thomas', 'Hilde Kuehne', 'Rogerio Feris', 'James Glass'], 'affiliations': ['Goethe University of Frankfurt', 'IBM Research AI', 'MIT CSAIL', 'MIT-IBM Watson AI Lab', 'Tuebingen AI Center/University of Tuebingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.09439.jpg', 'data': {'categories': ['#training', '#multimodal', '#rl', '#reasoning', '#optimization', '#benchmark', '#rag'], 'emoji': '🎧', 'ru': {'title': 'Революция в аудио-ИИ: Omni-R1 покоряет новые вершины мультимодального анализа', 'desc': 'Исследователи представили Omni-R1 - мультимодальную языковую модель, улучшенную с помощью обучения с подкреплением. Модель достигла наилучших результатов в бенчмарке MMAU по распознаванию звуков, музыки и речи. Анализ показал, что улучшение производительности в основном связано с более качественным текстовым рассуждением. Интересно, что дообучение на текстовых данных без аудио также повысило эффективность модели в аудиозадачах.'}, 'en': {'title': 'Enhancing Audio Understanding through Text-Based Reasoning', 'desc': "The paper introduces Omni-R1, a model that enhances the Qwen2.5-Omni multi-modal large language model (LLM) by fine-tuning it on an audio question answering dataset using the GRPO reinforcement learning method. This approach achieves state-of-the-art results on the MMAU benchmark, excelling in categories such as sounds, music, and speech. The authors found that the performance gains were largely due to improved text-based reasoning capabilities, even when audio data was not used during fine-tuning. Interestingly, they discovered that training on a text-only dataset could still boost the model's performance on audio tasks."}, 'zh': {'title': 'Omni-R1：音频问答的新突破', 'desc': '我们提出了Omni-R1，它在音频问答数据集上对最新的多模态大语言模型Qwen2.5-Omni进行了微调，采用了强化学习方法GRPO。这使得Omni-R1在最近的MMAU基准测试中达到了新的最先进性能。Omni-R1在声音、音乐、语音和整体平均类别上都取得了最高的准确率，无论是在Test-mini还是Test-full分割上。我们还发现，在没有音频的情况下进行微调，竟然也能有效提高音频相关的性能，这表明文本推理能力的提升对整体表现有重要影响。'}}}, {'id': 'https://huggingface.co/papers/2505.08455', 'title': 'VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models', 'url': 'https://huggingface.co/papers/2505.08455', 'abstract': 'Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.', 'score': 4, 'issue_id': 3773, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'b7bc69bb40029690', 'authors': ['Pritam Sarkar', 'Ali Etemad'], 'affiliations': ['Queens University, Canada', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.08455.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях', 'desc': 'Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Моделей (LVLM) к причинно-следственному рассуждению на основе видео. VCRBench использует процедурные видео повседневных действий с перемешанными шагами для проверки способности моделей идентифицировать и упорядочивать ключевые причинные события. Оценка современных LVLM на VCRBench показала их трудности с моделированием долгосрочных причинно-следственных зависимостей напрямую из визуальных наблюдений. Авторы предложили модульный подход Recognition-Reasoning Decomposition (RRD), который значительно повысил точность на VCRBench.'}, 'en': {'title': 'Enhancing Video Causal Reasoning with RRD', 'desc': 'This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains.'}, 'zh': {'title': '提升视频因果推理能力的关键', 'desc': '尽管视频理解技术有所进步，但大型视频语言模型（LVLMs）在视频基础的因果推理方面的能力仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准测试，名为视频基础的长形式因果推理（VCRBench），通过对日常活动的视频进行处理，测试LVLMs能否识别、推理并正确排序实现特定目标所需的事件。我们还提出了一种模块化的方法，称为识别-推理分解（RRD），将视频基础的因果推理分为视频识别和因果推理两个子任务，从而显著提高了模型的准确性。我们的分析表明，LVLMs在复杂的长形式因果推理任务中主要依赖语言知识。'}}}, {'id': 'https://huggingface.co/papers/2505.09608', 'title': 'LightLab: Controlling Light Sources in Images with Diffusion Models', 'url': 'https://huggingface.co/papers/2505.09608', 'abstract': 'We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.', 'score': 3, 'issue_id': 3785, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': 'dbbc9a09c87e94d0', 'authors': ['Nadav Magar', 'Amir Hertz', 'Eric Tabellion', 'Yael Pritch', 'Alex Rav-Acha', 'Ariel Shamir', 'Yedid Hoshen'], 'affiliations': ['Google, Israel', 'Google, United States of America', 'Hebrew University of Jerusalem and Google, Israel', 'Reichman University and Google, Israel', 'Tel Aviv University and Google, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.09608.jpg', 'data': {'categories': ['#cv', '#diffusion', '#synthetic', '#data', '#dataset', '#training'], 'emoji': '💡', 'ru': {'title': 'Прецизионное управление освещением с помощью диффузионных моделей', 'desc': 'Авторы представляют новый метод на основе диффузии для точного контроля источников света на изображении. Модель дообучается на парах реальных фотографий и синтетических изображениях, используя фотореалистичные возможности диффузионных моделей. Метод позволяет точно контролировать интенсивность и цвет освещения. По результатам пользовательских оценок, он превосходит существующие подходы к редактированию освещения.'}, 'en': {'title': 'Mastering Light: Fine-Grained Control with Diffusion Models', 'desc': 'This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques.'}, 'zh': {'title': '精确控制光源的扩散重光照方法', 'desc': '我们提出了一种简单而有效的基于扩散的方法，用于对图像中的光源进行细粒度的参数控制。现有的重光照方法要么依赖多个输入视图进行逆向渲染，要么无法提供对光变化的明确控制。我们通过在一小组真实原始照片对上微调扩散模型，并结合大规模合成渲染图像，来引导其在重光照方面的真实感先验。最终，我们展示了该方法在光编辑结果上的优越性，超越了基于用户偏好的现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.04793', 'title': 'DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\n  Recognition', 'url': 'https://huggingface.co/papers/2505.04793', 'abstract': 'Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/', 'score': 2, 'issue_id': 3778, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '76d34246ac682dfd', 'authors': ['Kailash A. Hambarde', 'Nzakiese Mbongo', 'Pavan Kumar MP', 'Satish Mekewad', 'Carolina Fernandes', 'Gökhan Silahtaroğlu', 'Alice Nithya', 'Pawan Wasnik', 'MD. Rashidunnabi', 'Pranita Samale', 'Hugo Proença'], 'affiliations': ['Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal', 'Istanbul Medipol University, Istanbul, Turkey', 'J.N.N. College of Engineering, Shivamogga, Karnataka, India', 'SRM Institute of Science and Technology, Kattankulathur, India', 'School of Computational Sciences, SRTM University, Nanded, India'], 'pdf_title_img': 'assets/pdf/title_img/2505.04793.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic'], 'emoji': '🎯', 'ru': {'title': 'DetReIDX: Стресс-тест для реидентификации людей в реальном мире', 'desc': 'Статья представляет DetReIDX - крупномасштабный набор данных для задачи реидентификации людей в воздушно-наземных условиях. Набор данных включает более 13 миллионов ограничивающих рамок для 509 личностей, собранных в семи университетских кампусах на трех континентах, с высотой съемки от 5,8 до 120 метров. DetReIDX специально разработан для стресс-тестирования алгоритмов реидентификации в реальных условиях, включая изменения одежды, освещения и местоположения между сессиями. Эксперименты показали, что современные методы обнаружения и реидентификации людей значительно ухудшают свою производительность при работе с данными DetReIDX.'}, 'en': {'title': 'DetReIDX: A Game-Changer for Real-World Person ReID Challenges', 'desc': 'This paper presents DetReIDX, a new dataset designed to improve person reidentification (ReID) technology in real-world scenarios. It addresses the limitations of existing datasets by incorporating significant variability factors such as changes in clothing, viewpoint, and environmental conditions. DetReIDX includes over 13 million bounding boxes from 509 identities, collected across multiple sessions in diverse locations, making it a comprehensive resource for evaluating ReID systems. The authors demonstrate that current state-of-the-art methods struggle significantly under the conditions presented by DetReIDX, highlighting the need for robust solutions in person reidentification.'}, 'zh': {'title': 'DetReIDX：真实世界行人重识别的新挑战', 'desc': '本文介绍了一种名为DetReIDX的大规模空中-地面行人重识别（ReID）数据集，旨在测试ReID技术在真实世界条件下的表现。该数据集包含来自509个身份的1300多万个边界框，数据收集自三个大洲的七个大学校园，涵盖了不同的服装、光照和位置变化。DetReIDX的独特之处在于，它记录了至少两个不同日期的会话，能够有效评估长期行人重识别的能力。研究表明，当前最先进的方法在DetReIDX条件下的表现显著下降，检测准确率下降高达80%，Rank-1 ReID下降超过70%。'}}}, {'id': 'https://huggingface.co/papers/2505.08084', 'title': 'Visually Interpretable Subtask Reasoning for Visual Question Answering', 'url': 'https://huggingface.co/papers/2505.08084', 'abstract': "Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.", 'score': 1, 'issue_id': 3779, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '3bcc5517eeabe122', 'authors': ['Yu Cheng', 'Arushi Goel', 'Hakan Bilen'], 'affiliations': ['NVIDIA', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.08084.jpg', 'data': {'categories': ['#training', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'VISTAR: Интерпретируемые рассуждения в мультимодальных моделях', 'desc': 'Статья представляет VISTAR - модель для интерпретируемых рассуждений на основе подзадач в мультимодальных больших языковых моделях (MLLM). VISTAR улучшает точность рассуждений и интерпретируемость, генерируя текстовые и визуальные объяснения внутри MLLM. Модель дообучается для создания структурированных последовательностей рассуждений Subtask-of-Thought. Эксперименты показывают, что VISTAR повышает точность рассуждений, сохраняя интерпретируемость.'}, 'en': {'title': 'Enhancing Visual Question Answering with VISTAR', 'desc': "This paper presents VISTAR, a new framework designed to improve the reasoning capabilities of multimodal large language models (MLLMs) when answering complex visual questions. VISTAR enhances interpretability by breaking down tasks into smaller subtasks and generating structured reasoning sequences, known as Subtask-of-Thought rationales. Unlike previous methods that relied on external models, VISTAR fine-tunes MLLMs directly, leading to better adaptation and accuracy on target data. Experimental results demonstrate that VISTAR not only boosts reasoning accuracy but also maintains a high level of interpretability in the model's outputs."}, 'zh': {'title': 'VISTAR：提升视觉推理的子任务驱动模型', 'desc': '本文介绍了一种名为VISTAR的模型，旨在提高多模态大语言模型（MLLMs）在复杂视觉问题上的推理能力。VISTAR通过生成文本和视觉解释，采用子任务驱动的训练框架，增强了解释性和推理能力。与依赖外部模型的方法不同，VISTAR对MLLMs进行微调，以生成结构化的思维子任务推理序列。实验结果表明，VISTAR在两个基准测试中均提高了推理准确性，同时保持了解释性。'}}}, {'id': 'https://huggingface.co/papers/2505.06356', 'title': 'Understanding and Mitigating Toxicity in Image-Text Pretraining\n  Datasets: A Case Study on LLaVA', 'url': 'https://huggingface.co/papers/2505.06356', 'abstract': 'Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.', 'score': 1, 'issue_id': 3784, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': '159eecb990673138', 'authors': ['Karthik Reddy Kanjula', 'Surya Guthikonda', 'Nahid Alam', 'Shayekh Bin Islam'], 'affiliations': ['Bangladesh University of Engineering and Technology', 'Cisco Meraki', 'Cohere for AI Community', 'Indiana University Bloomington'], 'pdf_title_img': 'assets/pdf/title_img/2505.06356.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#ethics', '#open_source', '#data'], 'emoji': '🧹', 'ru': {'title': 'Очистка данных для этичного ИИ', 'desc': 'Исследователи изучили проблему токсичности в наборе данных LLaVA для предобучения мультимодальных моделей. Они провели анализ различных категорий токсичного контента и предложили стратегии по его устранению. В результате был создан очищенный набор данных, из которого удалено более 7500 токсичных пар изображение-текст. Авторы подчеркивают важность активного выявления и фильтрации вредоносного контента для создания более ответственных мультимодальных систем.'}, 'en': {'title': 'Building Safer Multimodal Models by Mitigating Toxicity', 'desc': 'This paper addresses the issue of toxic content in pretraining datasets used for multimodal models, specifically focusing on the LLaVA image-text dataset. The authors analyze various categories of toxicity and how they appear in both images and text. They propose strategies to reduce this harmful content, resulting in a refined dataset that eliminates over 7,500 toxic image-text pairs. The study emphasizes the importance of filtering out toxic elements to create fairer and more responsible AI systems, and the new dataset is made available for further research.'}, 'zh': {'title': '构建更安全的多模态模型', 'desc': '本论文研究了LLaVA图像-文本预训练数据集中有毒内容的普遍性，分析了有害内容在不同模态中的表现。我们识别了常见的有毒类别，并提出了针对性的减轻策略，最终创建了一个经过改进的去毒性数据集。该数据集移除了7531对有毒的图像-文本配对，提供了实施强大毒性检测管道的指南。我们的研究强调了主动识别和过滤有毒内容的重要性，以构建更负责任和公平的多模态系统。'}}}, {'id': 'https://huggingface.co/papers/2505.05587', 'title': 'Steepest Descent Density Control for Compact 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2505.05587', 'abstract': '3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.', 'score': 1, 'issue_id': 3784, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'c689f8e4175cc211', 'authors': ['Peihao Wang', 'Yuehao Wang', 'Dilin Wang', 'Sreyas Mohan', 'Zhiwen Fan', 'Lemeng Wu', 'Ruisi Cai', 'Yu-Ying Yeh', 'Zhangyang Wang', 'Qiang Liu', 'Rakesh Ranjan'], 'affiliations': ['Meta Reality Labs', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.05587.jpg', 'data': {'categories': ['#inference', '#3d', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация 3D Gaussian Splatting: меньше точек, выше эффективность', 'desc': 'Статья представляет теоретическую основу для улучшения контроля плотности в методе 3D Gaussian Splatting (3DGS). Авторы анализируют процесс уплотнения в 3DGS и предлагают оптимизационный подход для определения необходимых условий и параметров генерации новых гауссовых примитивов. На основе этого анализа разработан метод SteepGS, который позволяет достичь 50% сокращения количества гауссовых точек без ухудшения качества рендеринга. Предложенный подход значительно повышает эффективность и масштабируемость 3DGS, особенно для устройств с ограниченными ресурсами.'}, 'en': {'title': 'Optimizing 3D Gaussian Splatting for Efficient Real-Time Rendering', 'desc': 'This paper presents 3D Gaussian Splatting (3DGS), a technique for creating high-quality 3D views in real-time by using Gaussian primitives. It addresses the issue of excessive memory usage and slow performance caused by redundant point clouds during the densification process. The authors propose a theoretical framework to improve density control, which includes optimizing the number of Gaussian points and their opacity. The new method, SteepGS, effectively reduces the number of Gaussian points by about 50% while preserving rendering quality, making it more efficient for use on devices with limited resources.'}, 'zh': {'title': '高效紧凑的3D高斯点云控制', 'desc': '3D高斯点云（3DGS）是一种用于实时高分辨率新视角合成的强大技术。它通过将场景表示为高斯原语的混合体，利用GPU光栅化管道实现高效渲染和重建。为了优化场景覆盖和捕捉细节，3DGS采用了密度化算法生成额外点，但这会导致冗余点云，增加内存使用和存储需求。为了解决这个问题，我们提出了一个理论框架，改进了3DGS中的密度控制，提出了SteepGS方法，显著减少了高斯点数量，同时保持渲染质量。'}}}, {'id': 'https://huggingface.co/papers/2505.08910', 'title': 'Behind Maya: Building a Multilingual Vision Language Model', 'url': 'https://huggingface.co/papers/2505.08910', 'abstract': 'In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.', 'score': 0, 'issue_id': 3784, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '617c510dcc5eefc7', 'authors': ['Nahid Alam', 'Karthik Reddy Kanjula', 'Surya Guthikonda', 'Timothy Chung', 'Bala Krishna S Vegesna', 'Abhipsha Das', 'Anthony Susevski', 'Ryan Sze-Yin Chan', 'S M Iftekhar Uddin', 'Shayekh Bin Islam', 'Roshan Santhosh', 'Snegha A', 'Drishti Sharma', 'Chen Liu', 'Isha Chaturvedi', 'Genta Indra Winata', 'Ashvanth. S', 'Snehanshu Mukherjee', 'Alham Fikri Aji'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08910.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#dataset', '#cv', '#open_source'], 'emoji': '🌍', 'ru': {'title': 'Maya: мультиязычная VLM для преодоления языковых и культурных барьеров', 'desc': 'Статья представляет Maya - открытую мультиязычную модель компьютерного зрения и обработки естественного языка (VLM). Модель обучена на наборе данных, включающем восемь языков, что расширяет её возможности для низкоресурсных языков и различных культурных контекстов. Maya улучшает понимание культурных и лингвистических аспектов в задачах, связанных со зрением и языком. Авторы предоставляют открытый доступ к коду модели для дальнейших исследований и разработок.'}, 'en': {'title': 'Empowering Multilingual Understanding in Vision-Language Models', 'desc': 'This paper presents Maya, a new open-source Multilingual Vision-Language Model (VLM) designed to improve performance in low-resource languages and diverse cultural contexts. It introduces a multilingual image-text pretraining dataset that includes eight languages, expanding upon the existing LLaVA dataset. The model aims to enhance understanding in vision-language tasks by incorporating cultural and linguistic nuances. By providing this resource, the authors hope to bridge the gap in VLM capabilities across different languages and cultures.'}, 'zh': {'title': 'Maya：提升多语言视觉理解的开源模型', 'desc': '近年来，大型视觉语言模型（VLM）迅速发展，取得了令人瞩目的学术成绩。然而，这些模型在低资源语言和不同文化背景下的表现仍然不足。为了解决这些问题，我们推出了Maya，一个开源的多语言视觉语言模型。我们的贡献包括：基于LLaVA预训练数据集的八种语言的多语言图像-文本预训练数据集，以及支持这些语言的多语言图像-文本模型，提升了视觉语言任务中的文化和语言理解能力。'}}}, {'id': 'https://huggingface.co/papers/2504.21635', 'title': 'Sadeed: Advancing Arabic Diacritization Through Small Language Model', 'url': 'https://huggingface.co/papers/2504.21635', 'abstract': "Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.", 'score': 44, 'issue_id': 3530, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'af5a1b038b3ccab3', 'authors': ['Zeina Aldallal', 'Sara Chrouf', 'Khalil Hennara', 'Mohamed Motaism Hamed', 'Muhammad Hreden', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21635.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#multilingual', '#benchmark', '#data', '#machine_translation'], 'emoji': '🔠', 'ru': {'title': 'Sadeed: Прорыв в диакритизации арабского текста с помощью компактной языковой модели', 'desc': 'Sadeed - это новый подход к диакритизации арабского текста, основанный на модели Kuwain 1.5B. Модель была дообучена на тщательно отобранных и обработанных наборах данных с диакритическими знаками. Несмотря на ограниченные вычислительные ресурсы, Sadeed показывает конкурентоспособные результаты по сравнению с проприетарными большими языковыми моделями. Авторы также представили новый бенчмарк SadeedDiac-25 для более справедливой оценки моделей диакритизации арабского текста.'}, 'en': {'title': 'Sadeed: Advancing Arabic Diacritization with Efficiency and Precision', 'desc': 'This paper presents Sadeed, a new method for Arabic text diacritization using a fine-tuned decoder-only language model based on Kuwain 1.5B. Sadeed is specifically trained on high-quality diacritized datasets, which were created through a thorough data-cleaning process. The model demonstrates competitive performance against larger proprietary models while requiring less computational power. Additionally, the authors introduce SadeedDiac-25, a new benchmark for evaluating Arabic diacritization, aiming to improve assessment practices in the field.'}, 'zh': {'title': 'Sadeed：阿拉伯语标记化的新突破', 'desc': '本文介绍了一种新的阿拉伯语文本标记化方法，名为Sadeed。该方法基于经过微调的解码器语言模型，专门针对阿拉伯语的丰富形态特征进行优化。Sadeed在高质量的标记化数据集上进行微调，尽管计算资源有限，但其性能与大型语言模型相当，且优于传统模型。此外，本文还提出了SadeedDiac-25基准，以促进对阿拉伯语标记化的公平评估。'}}}, {'id': 'https://huggingface.co/papers/2504.21776', 'title': 'WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability', 'url': 'https://huggingface.co/papers/2504.21776', 'abstract': 'Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.', 'score': 26, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '61ce82abe42f584a', 'authors': ['Xiaoxi Li', 'Jiajie Jin', 'Guanting Dong', 'Hongjin Qian', 'Yutao Zhu', 'Yongkang Wu', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21776.jpg', 'data': {'categories': ['#rl', '#science', '#agents', '#rlhf', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🕸️', 'ru': {'title': 'WebThinker: Расширение возможностей ИИ для глубоких веб-исследований', 'desc': 'WebThinker - это глубокая исследовательская система, которая расширяет возможности больших языковых моделей (LLM) для автономного поиска в интернете и составления исследовательских отчетов. Система включает модуль Deep Web Explorer для динамического поиска и извлечения информации из веб-страниц, а также стратегию автономного мышления, поиска и составления черновиков. WebThinker использует обучение с подкреплением на основе прямой оптимизации предпочтений (DPO) для улучшения использования исследовательских инструментов. Эксперименты показывают, что WebThinker значительно превосходит существующие методы на сложных задачах рассуждения и генерации научных отчетов.'}, 'en': {'title': 'Empowering LRMs with Real-Time Web Research Capabilities', 'desc': "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."}, 'zh': {'title': 'WebThinker：让推理模型更智能的研究助手', 'desc': '大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在长时间推理方面表现出色，但它们依赖静态内部知识，限制了在复杂知识密集型任务中的表现。为了解决这个问题，我们提出了WebThinker，一个深度研究代理，能够让LRMs自主搜索网络、浏览网页并在推理过程中撰写研究报告。WebThinker集成了深网探索模块，使LRMs在遇到知识空白时能够动态搜索和提取信息。通过引入基于强化学习的训练策略，我们的实验表明WebThinker在复杂推理基准和科学报告生成任务中显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.21850', 'title': 'COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning', 'url': 'https://huggingface.co/papers/2504.21850', 'abstract': 'Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.', 'score': 20, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '3db97f245360deb4', 'authors': ['Xindi Wu', 'Hee Seung Hwang', 'Polina Kirichenko', 'Olga Russakovsky'], 'affiliations': ['Meta AI', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21850.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#data', '#optimization', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'COMPACT: эффективное обучение MLLM сложным визуально-языковым задачам', 'desc': 'Статья представляет новый метод обучения мультимодальных больших языковых моделей (MLLM) под названием COMPACT. Этот подход генерирует набор данных для обучения, контролируя композиционную сложность примеров. COMPACT позволяет MLLM эффективнее обучаться сложным задачам, комбинируя атомарные возможности. Метод достигает сопоставимых результатов с LLaVA-665k, используя менее 10% данных, и превосходит его на сложных мультизадачных тестах.'}, 'en': {'title': 'Unlocking Complex Tasks with Efficient Compositional Training', 'desc': "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."}, 'zh': {'title': '提升复杂视觉语言任务的能力', 'desc': '多模态大型语言模型（MLLMs）在简单的视觉语言任务中表现出色，但在需要多种能力的复杂任务中却面临挑战。传统的视觉指令调优（VIT）主要关注数据量的扩大，而忽视了训练示例的组合复杂性。我们提出了COMPACT（组合原子到复杂视觉能力调优），它生成一个明确控制训练示例组合复杂性的训练数据集。COMPACT使得MLLMs能够更高效地学习复杂能力，并在多个基准测试中表现出色，尤其是在涉及复杂多能力任务时。'}}}, {'id': 'https://huggingface.co/papers/2504.21233', 'title': 'Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math', 'url': 'https://huggingface.co/papers/2504.21233', 'abstract': 'Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.', 'score': 19, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '0b800a9195884db4', 'authors': ['Haoran Xu', 'Baolin Peng', 'Hany Awadalla', 'Dongdong Chen', 'Yen-Chun Chen', 'Mei Gao', 'Young Jin Kim', 'Yunsheng Li', 'Liliang Ren', 'Yelong Shen', 'Shuohang Wang', 'Weijian Xu', 'Jianfeng Gao', 'Weizhu Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21233.jpg', 'data': {'categories': ['#training', '#rl', '#transfer_learning', '#small_models', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие потенциала малых языковых моделей в задачах рассуждения', 'desc': 'Статья представляет систематический подход к улучшению способностей рассуждения в малых языковых моделях (SLM). Авторы предлагают четырехэтапный рецепт обучения, включающий дистилляцию данных, дообучение на высококачественных данных с цепочками рассуждений, оптимизацию прямого предпочтения и обучение с подкреплением. Применение этого метода к модели Phi-4-Mini (3.8 млрд параметров) позволило превзойти более крупные модели в задачах математических рассуждений. Результаты подтверждают эффективность предложенного подхода для развития сильных способностей рассуждения даже в ресурсно-ограниченных малых моделях.'}, 'en': {'title': 'Unlocking Reasoning Power in Small Models', 'desc': 'This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning.'}, 'zh': {'title': '小模型也能强推理！', 'desc': '本论文探讨了如何通过链式思维（CoT）来提升小型语言模型（SLM）的推理能力。尽管大型语言模型（LLM）在生成中间推理步骤方面表现良好，但小型模型由于容量限制，提升推理能力仍然具有挑战性。研究表明，通过从LLM生成的合成数据进行蒸馏，可以显著改善SLM的推理能力。我们提出了一种系统的训练方案，包括四个步骤，最终在Phi-4-Mini模型上实现了超越更大模型的推理表现。'}}}, {'id': 'https://huggingface.co/papers/2504.20708', 'title': 'Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think', 'url': 'https://huggingface.co/papers/2504.20708', 'abstract': "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.", 'score': 17, 'issue_id': 3532, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'b26e58cf1cee464f', 'authors': ['Hasan Abed Al Kader Hammoud', 'Hani Itani', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2504.20708.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#interpretability', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Подмысли в LLM: путь к повышению точности рассуждений', 'desc': 'Исследование посвящено анализу промежуточных шагов рассуждения (подмыслей) в крупных языковых моделях (LLM) при решении сложных задач. Авторы предлагают метод сегментации цепочки рассуждений на подмысли и генерации продолжений из каждой промежуточной точки. Агрегирование ответов, полученных из разных подмыслей, часто дает более высокую точность по сравнению с использованием только финального ответа. Эксперименты на различных LLM и наборах данных по математическим рассуждениям показали значительное улучшение точности, достигающее 13%.'}, 'en': {'title': 'Unlocking Better Answers Through Subthoughts in LLMs', 'desc': 'This paper investigates the reasoning process of Large Language Models (LLMs) by focusing on intermediate reasoning steps, called subthoughts, rather than just the final answer. It questions whether the final answer is the best conclusion and explores if different reasoning paths can lead to varied results. The authors propose a method that segments reasoning into subthoughts and generates multiple potential answers from these segments, aggregating them to find the most frequent answer for improved accuracy. Their experiments demonstrate that this approach can enhance the accuracy of LLMs by up to 13% on challenging mathematical reasoning tasks.'}, 'zh': {'title': '优化推理路径，提升模型准确性', 'desc': '本文探讨了大型语言模型（LLMs）在解决复杂问题时的推理过程。我们提出了两个问题：最终答案是否可靠地代表模型的最佳结论？不同的推理路径是否会产生不同的结果？为了解答这些问题，我们分析了中间推理步骤，称为子思维，并提出了一种基于这些发现的方法。实验结果表明，通过选择最频繁的答案（众数）来聚合不同子思维的答案，准确性显著提高，最高可达13%。'}}}, {'id': 'https://huggingface.co/papers/2504.21318', 'title': 'Phi-4-reasoning Technical Report', 'url': 'https://huggingface.co/papers/2504.21318', 'abstract': 'We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.', 'score': 14, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '7004ae060f674e9a', 'authors': ['Marah Abdin', 'Sahaj Agarwal', 'Ahmed Awadallah', 'Vidhisha Balachandran', 'Harkirat Behl', 'Lingjiao Chen', 'Gustavo de Rosa', 'Suriya Gunasekar', 'Mojan Javaheripi', 'Neel Joshi', 'Piero Kauffmann', 'Yash Lara', 'Caio César Teodoro Mendes', 'Arindam Mitra', 'Besmira Nushi', 'Dimitris Papailiopoulos', 'Olli Saarikivi', 'Shital Shah', 'Vaishnavi Shrivastava', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi', 'Guoqing Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21318.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#math', '#transfer_learning', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Мощная модель рассуждений на основе тщательно отобранных данных', 'desc': 'Исследователи представили Phi-4-reasoning - модель с 14 миллиардами параметров, обученную для сложных задач рассуждения. Модель была обучена с помощью контролируемой тонкой настройки на тщательно отобранных обучающих примерах и демонстрациях рассуждений. Phi-4-reasoning генерирует подробные цепочки рассуждений, эффективно используя вычислительные ресурсы во время вывода. Модель превосходит по производительности значительно более крупные модели с открытыми весами на различных задачах рассуждения, включая математику, научное мышление, программирование и пространственное понимание.'}, 'en': {'title': 'Unlocking Complex Reasoning with Phi-4-Reasoning', 'desc': 'The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models.'}, 'zh': {'title': '推理模型的新突破：Phi-4-reasoning', 'desc': '本文介绍了Phi-4-reasoning，这是一个拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过对精心挑选的“可教”提示进行监督微调训练，生成详细的推理链，有效利用推理时的计算能力。我们还开发了Phi-4-reasoning-plus，通过基于结果的强化学习进一步增强，能够生成更长的推理轨迹，从而提高性能。综合评估显示，这两个模型在数学、科学推理、编码等多个任务上均优于更大的开放权重模型。'}}}, {'id': 'https://huggingface.co/papers/2504.20966', 'title': 'Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax', 'url': 'https://huggingface.co/papers/2504.20966', 'abstract': 'We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.', 'score': 14, 'issue_id': 3526, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'cb610c1427bdf307', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#architecture', '#inference', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Softpick: улучшенная активация для эффективных трансформеров', 'desc': "Статья представляет softpick - новый метод активации для механизмов внимания в трансформерах, заменяющий softmax. Softpick устраняет проблему 'attention sink' и чрезмерных активаций, сохраняя производительность на уровне softmax. Эксперименты показывают, что softpick создает более разреженные карты внимания и состояния с меньшим эксцессом. Модели с softpick превосходят softmax при квантовании, особенно при низкой битовой точности."}, 'en': {'title': 'Softpick: A Smarter Alternative to Softmax for Transformers', 'desc': 'This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability.'}, 'zh': {'title': 'softpick：提升Transformer注意力的新选择', 'desc': '本文介绍了一种名为softpick的新方法，它可以替代transformer注意力机制中的softmax。softpick不需要将权重归一化为1，能够消除注意力沉没和大规模激活。实验表明，使用softpick的模型在标准基准测试中与softmax表现相当，但注意力沉没率为0%，并且生成的隐藏状态具有更低的峰度。softpick在量化和低精度训练中表现优越，尤其在较低位数精度下具有明显优势，展示了其在稀疏性优化和可解释性方面的潜力。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.19720', 'title': 'Taming the Titans: A Survey of Efficient LLM Inference Serving', 'url': 'https://huggingface.co/papers/2504.19720', 'abstract': 'Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.', 'score': 9, 'issue_id': 3526, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': 'e74f8b7af65e09fd', 'authors': ['Ranran Zhen', 'Juntao Li', 'Yixin Ji', 'Zhenlin Yang', 'Tong Liu', 'Qingrong Xia', 'Xinyu Duan', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang'], 'affiliations': ['Huawei Cloud', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2504.19720.jpg', 'data': {'categories': ['#survey', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение инференса больших языковых моделей: от экземпляра до кластера', 'desc': 'Статья представляет собой обзор методов оптимизации инференса больших языковых моделей (LLM). Авторы рассматривают подходы на уровне отдельных экземпляров, включая размещение модели и управление запросами. Также описываются стратегии на уровне кластеров, такие как балансировка нагрузки и облачные решения. Особое внимание уделяется новым сценариям применения LLM и вспомогательным методам оптимизации.'}, 'en': {'title': 'Optimizing LLMs: Balancing Performance and Efficiency in Generative AI', 'desc': 'This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services.'}, 'zh': {'title': '推动大型语言模型推理服务的研究进展', 'desc': '大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其庞大的参数量和注意力机制的高计算需求导致了内存开销大，影响了推理服务的低延迟和高吞吐量。本文全面调查了应对这些挑战的方法，包括实例级和集群级的策略，以及新兴场景的方向。我们讨论了模型部署、请求调度、解码长度预测等实例级方法，以及GPU集群部署和多实例负载均衡等集群级策略。最后，我们提出了未来研究的潜在方向，以推动LLM推理服务的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.21855', 'title': 'ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction', 'url': 'https://huggingface.co/papers/2504.21855', 'abstract': 'In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '5d8989ce0c77aa23', 'authors': ['Qihao Liu', 'Ju He', 'Qihang Yu', 'Liang-Chieh Chen', 'Alan Yuille'], 'affiliations': ['Independent Researcher', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21855.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games', '#video', '#small_models'], 'emoji': '🎥', 'ru': {'title': 'ReVision: физика в помощь ИИ для создания реалистичных видео', 'desc': 'ReVision - это новый фреймворк для улучшения генерации видео с использованием трехмерных физических знаний. Он состоит из трех этапов: генерация грубого видео, извлечение 2D и 3D признаков для создания объектно-ориентированного представления, и уточнение движения с помощью параметризованной физической модели. ReVision значительно повышает качество генерируемых видео с точки зрения сложных движений и взаимодействий, превосходя даже более крупные модели.'}, 'en': {'title': 'ReVision: Enhancing Video Generation with 3D Physical Knowledge', 'desc': 'The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation.'}, 'zh': {'title': '通过3D物理知识提升视频生成的真实感与可控性', 'desc': '近年来，视频生成技术取得了显著进展，但在生成复杂动作和交互方面仍面临挑战。为了解决这些问题，我们提出了ReVision，这是一个可插拔的框架，能够将参数化的三维物理知识集成到预训练的条件视频生成模型中，从而显著提升生成高质量视频的能力。ReVision包括三个阶段：首先使用视频扩散模型生成粗略视频，然后提取2D和3D特征构建三维物体中心表示，最后通过参数化物理先验模型精炼运动序列，反馈到视频扩散模型中以生成一致的运动视频。我们的实验表明，ReVision在复杂视频生成上表现优异，甚至以较少的参数超越了大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.19056', 'title': 'Generative AI for Character Animation: A Comprehensive Survey of\n  Techniques, Applications, and Future Directions', 'url': 'https://huggingface.co/papers/2504.19056', 'abstract': 'Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.', 'score': 7, 'issue_id': 3535, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': 'deba947a8e69f6ee', 'authors': ['Mohammad Mahdi Abootorabi', 'Omid Ghahroodi', 'Pardis Sadat Zahraei', 'Hossein Behzadasl', 'Alireza Mirrokni', 'Mobina Salimipanah', 'Arash Rasouli', 'Bahar Behzadipour', 'Sara Azarnoush', 'Benyamin Maleki', 'Erfan Sadraiye', 'Kiarash Kiani Feriz', 'Mahdi Teymouri Nahad', 'Ali Moghadasi', 'Abolfazl Eshagh Abianeh', 'Nizi Nazar', 'Hamid R. Rabiee', 'Mahdieh Soleymani Baghshah', 'Meisam Ahmadi', 'Ehsaneddin Asgari'], 'affiliations': ['Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Iran University of Science and Technology', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2504.19056.jpg', 'data': {'categories': ['#survey', '#diffusion', '#cv', '#games', '#multimodal', '#video'], 'emoji': '🤖', 'ru': {'title': 'Генеративный ИИ революционизирует анимацию персонажей', 'desc': 'Это обзорная статья о применении генеративного искусственного интеллекта в анимации персонажей. Авторы рассматривают последние достижения в области лицевой анимации, синтеза движений, создания аватаров и других аспектов. В статье представлен комплексный обзор всех основных приложений генеративного ИИ для анимации персонажей, включая ведущие исследования, практические внедрения и используемые наборы данных. Также приводится справочная информация о фундаментальных моделях и метриках оценки для новичков в этой области.'}, 'en': {'title': 'Revolutionizing Character Animation with Generative AI', 'desc': 'This paper reviews the recent advancements in generative AI technologies specifically for character animation, which includes facial animation, gesture modeling, and motion synthesis. It highlights how foundation and diffusion models have significantly lowered the costs and time required for creating animated content. The survey provides a comprehensive overview of the state-of-the-art techniques, practical applications, and datasets used in the field, making it a valuable resource for newcomers. Additionally, it discusses ongoing challenges and suggests future research directions to enhance AI-driven character animation.'}, 'zh': {'title': '生成性AI：重塑角色动画的未来', 'desc': '这篇论文探讨了生成性人工智能在角色动画领域的应用，特别是在面部动画、表情渲染和动作合成等方面的最新进展。通过整合不同的生成模型和扩散模型，研究者们显著降低了动画内容的制作时间和成本。论文还提供了对当前研究、实际应用、常用数据集和新兴趋势的全面回顾，帮助新手了解基础模型和评估指标。最后，作者讨论了该领域面临的挑战，并为未来的研究方向提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2504.18904', 'title': 'RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning', 'url': 'https://huggingface.co/papers/2504.18904', 'abstract': 'Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': 'c724dcb5ceb5df7b', 'authors': ['Haoran Geng', 'Feishi Wang', 'Songlin Wei', 'Yuyang Li', 'Bangjun Wang', 'Boshi An', 'Charlie Tianyue Cheng', 'Haozhe Lou', 'Peihao Li', 'Yen-Jen Wang', 'Yutong Liang', 'Dylan Goetting', 'Chaoyi Xu', 'Haozhe Chen', 'Yuxi Qian', 'Yiran Geng', 'Jiageng Mao', 'Weikang Wan', 'Mingtong Zhang', 'Jiangran Lyu', 'Siheng Zhao', 'Jiazhao Zhang', 'Jialiang Zhang', 'Chengyang Zhao', 'Haoran Lu', 'Yufei Ding', 'Ran Gong', 'Yuran Wang', 'Yuxuan Kuang', 'Ruihai Wu', 'Baoxiong Jia', 'Carlo Sferrazza', 'Hao Dong', 'Siyuan Huang', 'Yue Wang', 'Jitendra Malik', 'Pieter Abbeel'], 'affiliations': ['BIGAI', 'CMU', 'PKU', 'Stanford', 'UC Berkeley', 'UCLA', 'UIUC', 'UMich', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2504.18904.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#synthetic', '#optimization', '#transfer_learning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboVerse: универсальная платформа для масштабирования и стандартизации обучения роботов', 'desc': 'RoboVerse - это комплексная платформа для робототехники, включающая симуляционную среду, синтетический набор данных и унифицированные бенчмарки. Платформа поддерживает множество симуляторов и роботизированных воплощений, обеспечивая плавный переход между различными средами. Синтетический набор данных отличается высокой точностью физики и фотореалистичным рендерингом. RoboVerse предлагает унифицированные бенчмарки для имитационного обучения и обучения с подкреплением, позволяющие оценивать различные уровни обобщения.'}, 'en': {'title': 'RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks', 'desc': 'This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field.'}, 'zh': {'title': 'RoboVerse：推动机器人学习的强大框架', 'desc': '本论文介绍了RoboVerse，这是一个综合框架，旨在解决机器人领域数据收集和评估的挑战。RoboVerse包括一个模拟平台、一个合成数据集和统一的基准测试，支持多种模拟器和机器人形态。通过高保真物理和逼真的渲染，合成数据集提供了高质量和多样性的数据。实验结果表明，RoboVerse显著提升了模仿学习、强化学习和从模拟到现实的转移性能，验证了其数据集和基准的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2504.21039', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report', 'url': 'https://huggingface.co/papers/2504.21039', 'abstract': 'As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.', 'score': 5, 'issue_id': 3528, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': 'a66fbdd0c4cc7250', 'authors': ['Paul Kassianik', 'Baturay Saglam', 'Alexander Chen', 'Blaine Nelson', 'Anu Vellore', 'Massimo Aufiero', 'Fraser Burch', 'Dhruv Kedia', 'Avi Zohary', 'Sajana Weerawardhena', 'Aman Priyanshu', 'Adam Swanda', 'Amy Chang', 'Hyrum Anderson', 'Kojin Oshiba', 'Omar Santos', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Foundation AI Cisco Systems Inc.', 'Security & Trust Organization Cisco Systems Inc.', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21039.jpg', 'data': {'categories': ['#architecture', '#data', '#open_source', '#training', '#dataset', '#benchmark', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Специализированная языковая модель открывает новые горизонты в кибербезопасности', 'desc': 'Статья представляет Foundation-Sec-8B - языковую модель, специализированную на кибербезопасности. Модель основана на архитектуре Llama 3.1 и дообучена на специально подобранном корпусе текстов по кибербезопасности. Foundation-Sec-8B показывает результаты, сравнимые с Llama 3.1-70B и GPT-4o-mini в некоторых задачах кибербезопасности. Авторы публикуют модель, чтобы ускорить развитие и внедрение ИИ-инструментов в сфере кибербезопасности.'}, 'en': {'title': 'Empowering Cybersecurity with Foundation-Sec-8B', 'desc': 'This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors.'}, 'zh': {'title': '推动网络安全的AI工具进步', 'desc': '随着基于变换器的大型语言模型（LLMs）在社会中的广泛应用，它们在软件工程、创意写作和数字艺术等领域带来了革命性的变化。然而，由于缺乏专业的训练数据和表示网络安全特定知识的复杂性，它们在网络安全领域的应用仍然有限。为了解决这些问题，我们提出了Foundation-Sec-8B，这是一个专注于网络安全的LLM，基于Llama 3.1架构，并通过在精心策划的网络安全语料库上进行持续预训练来增强。我们在多个网络安全基准测试中评估了Foundation-Sec-8B，结果显示它在某些网络安全特定任务上与Llama 3.1-70B和GPT-4o-mini相匹配。'}}}, {'id': 'https://huggingface.co/papers/2504.21336', 'title': 'UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation', 'url': 'https://huggingface.co/papers/2504.21336', 'abstract': 'Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.', 'score': 2, 'issue_id': 3532, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'b4a0872fb3eb8547', 'authors': ['Linshan Wu', 'Yuxiang Nie', 'Sunan He', 'Jiaxin Zhuang', 'Hao Chen'], 'affiliations': ['Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China', 'Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China', 'State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21336.jpg', 'data': {'categories': ['#optimization', '#cv', '#science', '#multimodal', '#dataset', '#healthcare', '#interpretability', '#agi'], 'emoji': '🧬', 'ru': {'title': 'UniBiomed: революция в анализе биомедицинских изображений с помощью ИИ', 'desc': 'UniBiomed - это универсальная модель для интерпретации биомедицинских изображений, объединяющая мультимодальную языковую модель и модель сегментации. Она обучена на 27 миллионах триплетов изображений, аннотаций и текстовых описаний из 10 различных модальностей медицинской визуализации. UniBiomed демонстрирует передовые результаты в задачах сегментации, распознавания заболеваний, диагностики с привязкой к регионам, визуальных вопросов-ответов и генерации отчетов. Модель обеспечивает автоматизированную интерпретацию биомедицинских изображений без необходимости в предварительной диагностике экспертами.'}, 'en': {'title': 'UniBiomed: Revolutionizing Biomedical Image Interpretation', 'desc': "This paper presents UniBiomed, a groundbreaking universal foundation model designed for interpreting biomedical images by integrating Multi-modal Large Language Models (MLLM) and segmentation techniques. Unlike traditional AI methods that treat text generation and image segmentation separately, UniBiomed combines these processes to provide a cohesive understanding of biomedical data. It utilizes a large-scale dataset of over 27 million image-text pairs across various imaging modalities, enabling it to perform multiple tasks such as segmentation, disease recognition, and report generation. The model's ability to automate grounded interpretation marks a significant advancement in clinical workflows, enhancing diagnostic efficiency and accuracy."}, 'zh': {'title': 'UniBiomed：生物医学图像分析的新突破', 'desc': '多模态生物医学图像的解释为生物医学图像分析开辟了新的机会。传统的人工智能方法通常依赖于分离的训练，导致在实际应用中缺乏灵活性，无法充分利用整体生物医学信息。为此，我们提出了UniBiomed，这是首个用于生物医学图像解释的通用基础模型，结合了多模态大语言模型和分割模型，能够统一生成临床文本和相应生物医学对象的分割。UniBiomed在多个生物医学任务中表现出色，显著提高了诊断效率。'}}}, {'id': 'https://huggingface.co/papers/2504.19043', 'title': 'Selecting Optimal Candidate Profiles in Adversarial Environments Using\n  Conjoint Analysis and Machine Learning', 'url': 'https://huggingface.co/papers/2504.19043', 'abstract': 'Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.', 'score': 2, 'issue_id': 3538, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': 'c7a7b2771c1e5c1f', 'authors': ['Connor T. Jerzak', 'Priyanshi Chandra', 'Rishi Hazra'], 'affiliations': ['Department of Government, University of Texas at Austin', 'Department of Statistics, Harvard College', 'Faculty of Informatics, Università della Svizzera Italiana'], 'pdf_title_img': 'assets/pdf/title_img/2504.19043.jpg', 'data': {'categories': [], 'emoji': '🗳️', 'ru': {'title': 'Оптимизация профилей политических кандидатов с помощью машинного обучения', 'desc': 'Статья рассматривает применение совместного анализа в политических исследованиях для оптимизации профилей кандидатов. Авторы предлагают метод стохастической интервенции для определения оптимального распределения атрибутов кандидата в условиях ограниченных данных. Исследование учитывает как одностороннюю, так и состязательную оптимизацию между двумя партиями. Результаты показывают, что состязательный подход дает более реалистичные прогнозы и лучше соответствует наблюдаемым кандидатам по сравнению с несостязательным методом.'}, 'en': {'title': 'Optimizing Political Candidate Selection through Adversarial Conjoint Analysis', 'desc': 'This paper explores the use of conjoint analysis in political candidate selection, focusing on how to identify optimal candidate profiles. It highlights the challenge of having too many possible candidate features compared to the limited number of observations in typical experiments. To solve this, the authors propose a stochastic intervention that generates a probability distribution of candidate attributes to maximize favorable outcomes. The study shows that considering adversarial dynamics between political parties leads to more accurate predictions of candidate success compared to traditional non-adversarial methods.'}, 'zh': {'title': '对抗性动态提升联合分析的洞察力', 'desc': '本文探讨了联合分析在政治候选人选择中的应用，特别是如何识别最佳候选人特征组合。由于特征组合的数量远超观察样本，无法精确确定最佳配置。为了解决这一问题，作者提出了一种最优随机干预方法，旨在通过概率分布实现最有利的平均结果。研究表明，在对抗性环境中，所建议的策略更可能与实际候选人匹配，揭示了对抗动态在社会科学实验数据分析中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.02567', 'title': 'Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities', 'url': 'https://huggingface.co/papers/2505.02567', 'abstract': "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).", 'score': 51, 'issue_id': 3655, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '0d49b4c41b7654a0', 'authors': ['Xinjie Zhang', 'Jintao Guo', 'Shanshan Zhao', 'Minghao Fu', 'Lunhao Duan', 'Guo-Hua Wang', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba Group', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02567.jpg', 'data': {'categories': ['#dataset', '#architecture', '#survey', '#multimodal', '#diffusion', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Объединение мультимодального понимания и генерации изображений: путь к универсальным моделям ИИ', 'desc': 'Эта статья представляет собой обзор современных подходов к объединению моделей мультимодального понимания и генерации изображений. Авторы анализируют три основных архитектурных парадигмы: основанные на диффузии, авторегрессивные и гибридные подходы. В работе также рассматриваются наборы данных и бенчмарки для унифицированных моделей. Обсуждаются ключевые проблемы в этой области, включая стратегию токенизации, кросс-модальное внимание и данные.'}, 'en': {'title': 'Bridging the Gap: Unifying Multimodal Understanding and Image Generation', 'desc': 'This paper surveys the integration of multimodal understanding and image generation models, which have traditionally developed separately. It highlights the architectural differences between autoregressive and diffusion-based models, emphasizing the challenges in unifying these approaches. The authors categorize existing unified models into three paradigms: diffusion-based, autoregressive-based, and hybrid methods. They also provide resources such as datasets and benchmarks to support future research in this emerging field.'}, 'zh': {'title': '统一多模态模型的未来探索', 'desc': '近年来，多模态理解模型和图像生成模型取得了显著进展，但这两个领域的发展相对独立，导致了不同的架构范式。自回归架构在多模态理解中占主导地位，而扩散模型则成为图像生成的基石。本文综述了当前统一框架的努力，介绍了多模态理解和文本到图像生成模型的基础概念及最新进展，并分析了三种主要的统一模型架构。我们还讨论了这一新兴领域面临的关键挑战，并提供了未来研究的参考资源。'}}}, {'id': 'https://huggingface.co/papers/2505.04588', 'title': 'ZeroSearch: Incentivize the Search Capability of LLMs without Searching', 'url': 'https://huggingface.co/papers/2505.04588', 'abstract': "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", 'score': 29, 'issue_id': 3647, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '24edc7c3c5e5e23d', 'authors': ['Hao Sun', 'Zile Qiao', 'Jiayan Guo', 'Xuanbo Fan', 'Yingyan Hou', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Yan Zhang'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.04588.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'ZeroSearch: обучение LLM эффективному поиску без реальных поисковых систем', 'desc': 'Статья представляет ZeroSearch - новую систему обучения с подкреплением для улучшения поисковых возможностей больших языковых моделей (LLM). В отличие от предыдущих подходов, ZeroSearch не требует взаимодействия с реальными поисковыми системами, что решает проблемы неконтролируемого качества документов и высоких затрат на API. Метод использует легковесную предобученную модель в качестве модуля поиска и стратегию постепенного ухудшения качества генерируемых документов во время обучения. Эксперименты показывают, что ZeroSearch эффективно улучшает поисковые способности LLM, причем модели с 14 миллиардами параметров даже превосходят реальные поисковые системы.'}, 'en': {'title': 'ZeroSearch: Enhancing LLM Search Without Real Engines', 'desc': 'This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines.'}, 'zh': {'title': '提升LLMs搜索能力的创新框架', 'desc': '有效的信息搜索对于提升大型语言模型（LLMs）的推理和生成能力至关重要。本文提出了一种名为ZeroSearch的强化学习框架，旨在提高LLMs的搜索能力，而无需与真实搜索引擎互动。该方法通过轻量级的监督微调，将LLM转变为一个检索模块，并在训练过程中逐步降低生成文档的质量，以激发模型的推理能力。实验结果表明，ZeroSearch能够有效提升LLMs的搜索能力，并在不同参数规模的模型中表现出良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.03821', 'title': 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models', 'url': 'https://huggingface.co/papers/2505.03821', 'abstract': "We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.", 'score': 18, 'issue_id': 3655, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 мая', 'en': 'May 3', 'zh': '5月3日'}, 'hash': 'abede452b390c7de', 'authors': ['Gracjan Góral', 'Alicja Ziarko', 'Piotr Miłoś', 'Michał Nauman', 'Maciej Wołczyk', 'Michał Kosiński'], 'affiliations': ['Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, S. Banacha 2, 02-097 Warsaw, PL', 'Graduate School of Business, Stanford University, Stanford, CA 94305, USA', 'IDEAS NCBR, Chmielna 69, 00-801 Warsaw, PL', 'Institute of Mathematics, Polish Academy of Sciences, J. & J. Sniadeckich 8, 00-656 Warsaw, PL', 'Robot Learning Lab, University of California, Berkeley, CA 94720, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.03821.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#training'], 'emoji': '👁️', 'ru': {'title': 'VLM модели: от распознавания объектов к пониманию перспективы', 'desc': 'Исследователи изучают способность моделей компьютерного зрения и обработки естественного языка (VLM) к визуальному восприятию перспективы. Они разработали набор из 144 визуальных задач, используя сцены с миниатюрной фигуркой человека и объектом в различных пространственных конфигурациях. Эксперименты показали, что современные модели, такие как GPT-4-Turbo и Claude Sonnet, хорошо справляются с пониманием сцен, но значительно хуже выполняют задачи пространственного мышления и восприятия перспективы. Результаты указывают на необходимость интеграции явных геометрических представлений и специализированных протоколов обучения в будущих разработках VLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Spatial Reasoning and Perspective Taking', 'desc': "This paper explores how well Vision Language Models (VLMs) can understand visual perspectives through a series of unique tasks. The tasks involve a humanoid figure and an object in various spatial arrangements, designed to test scene understanding, spatial reasoning, and visual perspective taking. The study evaluates several advanced models, finding that while they perform well in recognizing scenes, they struggle with more complex reasoning tasks. The results highlight a significant gap in the models' abilities, suggesting that future developments should focus on incorporating geometric representations and specialized training methods."}, 'zh': {'title': '提升视觉语言模型的空间推理能力', 'desc': '本文研究了视觉语言模型（VLMs）在视觉视角理解方面的能力，使用了一组新颖的视觉任务，这些任务灵感来源于人类的经典测试。我们设计了144个独特的视觉任务，通过系统地改变空间配置，如物体相对于人形小人偶的位置和方向，来评估模型的表现。每个视觉任务配有7个诊断问题，旨在评估场景理解、空间推理和视觉视角理解三个层次的视觉认知。评估结果显示，尽管这些先进模型在场景理解方面表现出色，但在空间推理和视角理解方面的表现显著下降，表明在复杂视觉任务中，表面物体识别与更深层次的空间和视角推理之间存在差距。'}}}, {'id': 'https://huggingface.co/papers/2505.00358', 'title': 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training', 'url': 'https://huggingface.co/papers/2505.00358', 'abstract': "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.", 'score': 14, 'issue_id': 3652, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '74b251baea8510bd', 'authors': ['Albert Ge', 'Tzu-Heng Huang', 'John Cooper', 'Avi Trost', 'Ziyi Chu', 'Satya Sai Srinath Namburi GNVV', 'Ziyang Cai', 'Kendall Park', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.00358.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#data', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'R&B: Умное смешивание данных для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый фреймворк R&B для оптимизации стратегий смешивания данных при обучении языковых моделей. R&B перегруппирует обучающие данные на основе семантического сходства и эффективно оптимизирует состав данных, используя матрицу Грама, полученную из градиентов доменов. Этот метод устраняет необходимость в дополнительных вычислениях для получения оценочной информации. Теоретический и эмпирический анализ показывает эффективность R&B по сравнению с неадаптивными подходами к смешиванию данных.'}, 'en': {'title': 'R&B: Smarter Data Mixing for Language Models', 'desc': 'This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods.'}, 'zh': {'title': 'R&B：高效的数据混合新策略', 'desc': '本文提出了一种新的数据混合策略R&B，旨在解决现有方法的两个主要缺陷。首先，R&B通过语义相似性重新划分训练数据，创建更细粒度的数据域，从而捕捉到重要的语义细节。其次，该框架通过利用训练过程中获得的领域梯度的Gram矩阵，优化数据组合，避免了额外的计算开销。实验结果表明，R&B在多种数据集上表现优异，能够以极小的计算成本超越现有的最先进数据混合策略。'}}}, {'id': 'https://huggingface.co/papers/2505.04622', 'title': 'PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer', 'url': 'https://huggingface.co/papers/2505.04622', 'abstract': 'Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '8205883cc18835a6', 'authors': ['Jingwen Ye', 'Yuze He', 'Yanning Zhou', 'Yiqin Zhu', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Wei Yang', 'Xiao Han'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04622.jpg', 'data': {'categories': ['#optimization', '#3d', '#cv', '#games'], 'emoji': '🧊', 'ru': {'title': 'Универсальная абстракция 3D-форм с помощью ИИ', 'desc': 'Статья представляет PrimitiveAnything - новый фреймворк для абстракции 3D-форм с помощью примитивов. Он использует трансформер, обученный на масштабных данных человеческих абстракций, для автоматической генерации сборок примитивов. PrimitiveAnything применяет унифицированную параметризацию для разных типов примитивов и генерирует высококачественные абстракции, соответствующие человеческому восприятию. Фреймворк демонстрирует хорошую обобщающую способность на разнообразных категориях форм и имеет потенциал для применения в играх и других 3D-приложениях.'}, 'en': {'title': 'Revolutionizing 3D Shape Understanding with PrimitiveAnything', 'desc': 'This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games.'}, 'zh': {'title': '形状抽象的新突破：PrimitiveAnything', 'desc': '形状原始抽象是将复杂的3D形状分解为简单几何元素的过程，这对人类视觉认知至关重要，并在计算机视觉和图形学中有广泛应用。现有的原始抽象方法通常依赖于几何优化，缺乏语义理解，或者仅从小规模、特定类别的数据集中学习，难以在多样的形状类别中进行泛化。我们提出了PrimitiveAnything，一个将形状原始抽象重新定义为原始组装生成任务的新框架。该框架通过大规模人类创作的抽象学习原始组装过程，从而能够更好地捕捉人类如何将复杂形状分解为原始元素。'}}}, {'id': 'https://huggingface.co/papers/2505.04512', 'title': 'HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation', 'url': 'https://huggingface.co/papers/2505.04512', 'abstract': 'Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '82e5839ef846d9d8', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Sen Liang', 'Yuan Zhou', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.04512.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Мультимодальная генерация персонализированных видео с сохранением идентичности', 'desc': 'HunyuanCustom - это мультимодальная система для генерации персонализированных видео, поддерживающая условия в виде изображений, аудио, видео и текста. Она использует модуль слияния текста и изображений на основе LLaVA для улучшенного мультимодального понимания, а также модуль усиления идентификации изображений для сохранения согласованности личности в кадрах. Система включает специальные механизмы для внедрения аудио- и видеоусловий, такие как AudioNet и сеть выравнивания признаков на основе патчей. Эксперименты показывают, что HunyuanCustom превосходит современные методы по согласованности идентичности, реалистичности и соответствию текста видео.'}, 'en': {'title': 'HunyuanCustom: Consistent and Multi-Modal Video Generation', 'desc': 'This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation.'}, 'zh': {'title': '多模态定制视频生成的创新之路', 'desc': '定制视频生成旨在根据用户定义的条件生成特定主题的视频，但现有方法在身份一致性和输入模态方面常常面临挑战。本文提出了HunyuanCustom，一个多模态定制视频生成框架，强调主题一致性，并支持图像、音频、视频和文本条件。我们的模型通过引入基于LLaVA的文本-图像融合模块和图像ID增强模块，解决了图像-文本条件生成任务，从而增强多模态理解。实验结果表明，HunyuanCustom在身份一致性、真实感和文本-视频对齐方面显著优于现有的最先进方法，验证了多模态条件和身份保持策略在可控视频生成中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.04364', 'title': "Benchmarking LLMs' Swarm intelligence", 'url': 'https://huggingface.co/papers/2505.04364', 'abstract': 'Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.', 'score': 12, 'issue_id': 3648, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '4b0575d2194aee20', 'authors': ['Kai Ruan', 'Mowen Huang', 'Ji-Rong Wen', 'Hao Sun'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04364.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#open_source', '#multimodal'], 'emoji': '🐝', 'ru': {'title': 'SwarmBench: Тестирование роевого интеллекта языковых моделей', 'desc': 'Статья представляет SwarmBench - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к роевому интеллекту в многоагентных системах. SwarmBench включает пять задач координации в 2D-сетке, где агенты ограничены локальным восприятием и коммуникацией. Результаты экспериментов показывают значительные различия в производительности LLM между задачами, выявляя сложности планирования в условиях неопределенности. Авторы предоставляют открытый инструментарий для воспроизводимых исследований координации на основе LLM в многоагентных системах.'}, 'en': {'title': 'Unlocking Swarm Intelligence in Language Models', 'desc': "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."}, 'zh': {'title': '探索大型语言模型的群体智能潜力', 'desc': '大型语言模型（LLMs）在复杂推理方面显示出潜力，但它们在多智能体系统（MAS）中在严格约束下的协调能力仍然未被充分探索，尤其是在群体智能的细微差别方面。现有基准测试往往无法完全捕捉到在不完整时空信息下，智能体进行去中心化协调所面临的独特挑战。为此，我们引入了SwarmBench，这是一个新颖的基准，旨在系统评估LLMs作为去中心化智能体的群体智能能力。通过评估多个领先的LLMs，我们发现它们在任务中的表现差异显著，突显了在局部信息限制下的协调困难。'}}}, {'id': 'https://huggingface.co/papers/2505.04528', 'title': 'Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving', 'url': 'https://huggingface.co/papers/2505.04528', 'abstract': 'As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.', 'score': 7, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '0e9e0d509e4b4624', 'authors': ['Qi Liu', 'Xinhao Zheng', 'Renqiu Xia', 'Xingzhi Qi', 'Qinxiang Cao', 'Junchi Yan'], 'affiliations': ['Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04528.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#alignment', '#interpretability', '#reasoning', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Формальная верификация процесса решения задач искусственным интеллектом', 'desc': 'Статья представляет новый подход к формализации решения задач как марковского процесса принятия решений. Авторы предлагают фреймворк FPS (Formal Problem-Solving), использующий среды формального доказательства теорем для верификации процесса решения задач. Также представлен D-FPS (Deductive FPS), разделяющий решение и проверку ответа для лучшего соответствия человеческому подходу. Созданы три новых набора данных для оценки систем решения задач, а также предложен метод RPE для формальной верификации корректности ответов.'}, 'en': {'title': 'Revolutionizing Problem-Solving with Formal Frameworks', 'desc': 'This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods.'}, 'zh': {'title': '形式化问题解决的新框架', 'desc': '这篇论文探讨了问题解决的形式化，提出了一种将问题解决视为确定性马尔可夫决策过程的框架。作者介绍了FPS（正式问题解决）框架，利用现有的正式定理证明环境进行过程验证的问题解决。为了提高人类对齐，论文还提出了D-FPS（演绎FPS），将求解与答案验证解耦。最后，作者构建了三个基准测试，并提出了一种符号方法RPE来评估答案的正确性。'}}}, {'id': 'https://huggingface.co/papers/2505.03912', 'title': 'OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2505.03912', 'abstract': 'Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.', 'score': 6, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f7347c1b093f9488', 'authors': ['Can Cui', 'Pengxiang Ding', 'Wenxuan Song', 'Shuanghao Bai', 'Xinyang Tong', 'Zirui Ge', 'Runze Suo', 'Wanqi Zhou', 'Yang Liu', 'Bofang Jia', 'Han Zhao', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['HKUST(GZ)', 'Westlake University', 'Xian Jiaotong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03912.jpg', 'data': {'categories': ['#architecture', '#optimization', '#agents', '#open_source', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Открытая платформа для исследования двухсистемных VLA архитектур', 'desc': 'Статья посвящена двухсистемным архитектурам VLA (Vision-Language-Action) в области воплощенного интеллекта. Авторы анализируют и сравнивают существующие архитектуры, проводя систематическую эмпирическую оценку их ключевых элементов. Целью работы является создание открытой модели с низкими вычислительными затратами для дальнейших исследований. Проект планирует регулярно обновляться новыми экспериментальными выводами и улучшенными открытыми моделями.'}, 'en': {'title': 'Empowering Embodied Intelligence with Open-Source VLA Models', 'desc': 'This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers.'}, 'zh': {'title': '推动双系统VLA架构的开源探索', 'desc': '本文探讨了双系统视觉-语言-行动（VLA）架构在具身智能研究中的重要性，并指出目前缺乏足够的开源工作来进行性能分析和优化。作者总结并比较了现有双系统架构的结构设计，并对其核心设计元素进行了系统的实证评估。最终，本文将提供一个低成本的开源模型，以便进一步探索和研究。该项目将持续更新，提供更多实验结论和性能改进的开源模型供大家选择。'}}}, {'id': 'https://huggingface.co/papers/2505.04606', 'title': 'OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution', 'url': 'https://huggingface.co/papers/2505.04606', 'abstract': "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", 'score': 5, 'issue_id': 3657, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '25e97f182730fc25', 'authors': ['Lianghong Guo', 'Wei Tao', 'Runhan Jiang', 'Yanlin Wang', 'Jiachi Chen', 'Xilin Liu', 'Yuchi Ma', 'Mingzhi Mao', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University, China', 'Huawei Cloud Computing Technologies Co., Ltd., China', 'Independent Researcher, China', 'Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04606.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset', '#long_context', '#multimodal'], 'emoji': '🐙', 'ru': {'title': 'OmniGIRL: Вызов для языковых моделей в решении задач GitHub', 'desc': 'Статья представляет OmniGIRL - новый многоязычный, мультимодальный и мультидоменный эталонный набор данных для автоматического разрешения проблем на GitHub. OmniGIRL включает 959 задач из репозиториев на четырех языках программирования и восьми различных доменах. Оценка показала, что современные языковые модели (LLM) демонстрируют ограниченную эффективность на OmniGIRL, особенно при работе с изображениями. Анализ причин неудач LLM на OmniGIRL предоставляет insights для будущих улучшений.'}, 'en': {'title': 'OmniGIRL: A Comprehensive Benchmark for GitHub Issue Resolution', 'desc': 'This paper introduces OmniGIRL, a new benchmark for automatically resolving GitHub issues using large language models (LLMs). Unlike existing benchmarks, OmniGIRL is designed to be multilingual, multimodal, and multi-domain, addressing the limitations of focusing on a single programming language and a narrow range of issues. The benchmark includes 959 instances from four programming languages and eight domains, highlighting the diversity of real-world problems. Evaluation results show that current LLMs perform poorly on this benchmark, particularly in resolving issues that require understanding images, indicating a need for further advancements in model capabilities.'}, 'zh': {'title': 'OmniGIRL：多语言多模态的GitHub问题解决基准', 'desc': '本文提出了OmniGIRL，一个多语言、多模态和多领域的GitHub问题解决基准。现有的基准存在三个主要限制：只关注单一编程语言、覆盖领域狭窄以及仅依赖文本信息。OmniGIRL包含来自四种编程语言和八个不同领域的959个任务实例，旨在更全面地评估大型语言模型的能力。我们的评估显示，当前的语言模型在OmniGIRL上的表现有限，尤其在处理需要理解图像的问题时表现更差。'}}}, {'id': 'https://huggingface.co/papers/2505.03570', 'title': 'OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents', 'url': 'https://huggingface.co/papers/2505.03570', 'abstract': 'In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.', 'score': 4, 'issue_id': 3654, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'e87199c8805bce4f', 'authors': ['Mariya Davydova', 'Daniel Jeffries', 'Patrick Barker', 'Arturo Márquez Flores', 'Sinéad Ryan'], 'affiliations': ['Kentauros AI Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.03570.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#optimization', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'OSUniverse: новый стандарт оценки ИИ-агентов в графическом интерфейсе', 'desc': 'В статье представлен OSUniverse - комплексный многомодальный бенчмарк для ИИ-агентов, навигирующих в графическом интерфейсе. Бенчмарк включает задачи разной сложности, от простых кликов до многошаговых тестов в нескольких приложениях. Современные агенты достигают не более 50% успеха, в то время как обычные офисные работники справляются со всеми задачами. Бенчмарк имеет автоматизированную систему валидации с погрешностью менее 2%.'}, 'en': {'title': 'OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks', 'desc': 'This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.'}, 'zh': {'title': 'OSUniverse：GUI导航AI的全新基准', 'desc': '本文介绍了OSUniverse，这是一个针对高级GUI导航AI代理的复杂多模态桌面任务基准，旨在易用性、可扩展性、全面覆盖测试案例和自动验证方面表现出色。我们将任务分为不同复杂度的级别，从基本的精确点击到需要灵活性、精确性和清晰思维的多步骤、多应用程序测试。在基准的第一版中，我们调整了测试案例的复杂性，以确保当时的最先进（SOTA）代理的结果不超过50%，而普通白领工人可以完美完成所有这些任务。该基准可以手动评分，同时我们还引入了一个平均错误率低于2%的自动验证机制，为全面自动化测量GUI导航AI代理的进展、能力和有效性提供了坚实基础。'}}}, {'id': 'https://huggingface.co/papers/2505.03418', 'title': 'Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey', 'url': 'https://huggingface.co/papers/2505.03418', 'abstract': 'Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.', 'score': 4, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '8417799a01a2ecc2', 'authors': ['Da Zheng', 'Lun Du', 'Junwei Su', 'Yuchen Tian', 'Yuqi Zhu', 'Jintian Zhang', 'Lanning Wei', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['Ant Group, China', 'The University of Hong Kong, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03418.jpg', 'data': {'categories': ['#rl', '#survey', '#math', '#training', '#reasoning', '#science', '#data'], 'emoji': '🧠', 'ru': {'title': 'LLM: Новый рубеж в решении сложных задач', 'desc': 'Эта статья исследует возможности и ограничения больших языковых моделей (LLM) в решении сложных задач. Авторы рассматривают такие техники, как рассуждения по цепочке мыслей (Chain-of-Thought), расширение знаний и различные методы верификации на основе LLM и инструментов. В статье обсуждаются проблемы применения LLM в различных областях, включая разработку программного обеспечения, математические рассуждения и доказательства, анализ данных и научные исследования. Также рассматриваются фундаментальные ограничения текущих решений на основе LLM и будущие направления развития в контексте многоступенчатых рассуждений, интеграции доменных знаний и верификации результатов.'}, 'en': {'title': 'Unlocking Complex Problem-Solving with Large Language Models', 'desc': 'This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving.'}, 'zh': {'title': '大型语言模型：复杂问题解决的新工具', 'desc': '本论文探讨了大型语言模型（LLMs）在复杂问题解决中的能力和局限性。与传统计算系统不同，LLMs结合了强大的计算能力和人类推理的近似，能够生成解决方案和进行推理。尽管LLMs在多步骤推理、领域知识整合和结果验证方面面临挑战，但它们在软件工程、数学推理、数据分析和科学研究等领域的应用潜力巨大。本文还讨论了当前LLM解决方案的基本局限性以及未来在复杂问题解决中的发展方向。'}}}, {'id': 'https://huggingface.co/papers/2505.04601', 'title': 'OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning', 'url': 'https://huggingface.co/papers/2505.04601', 'abstract': "OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.", 'score': 2, 'issue_id': 3666, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '0b9c03d9680cfe04', 'authors': ['Xianhang Li', 'Yanqing Liu', 'Haoqin Tu', 'Hongru Zhu', 'Cihang Xie'], 'affiliations': ['University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.04601.jpg', 'data': {'categories': ['#small_models', '#dataset', '#open_source', '#multimodal', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'OpenVision: открытые визуальные энкодеры для мультимодальных моделей', 'desc': 'OpenVision представляет собой семейство полностью открытых и экономически эффективных визуальных энкодеров, которые соответствуют или превосходят производительность CLIP от OpenAI при интеграции в мультимодальные модели. Энкодеры OpenVision основаны на существующих работах, таких как CLIPS и Recap-DataComp-1B, но предлагают новые идеи по улучшению качества кодирования. Разработчики предоставляют энкодеры различных размеров (от 5,9 до 632,1 миллионов параметров), что позволяет гибко балансировать между производительностью и эффективностью при создании мультимодальных моделей. OpenVision заполняет пробел в области открытых визуальных энкодеров, предоставляя полностью открытые данные и методы обучения.'}, 'en': {'title': 'OpenVision: Open and Efficient Vision Encoders for Multimodal Models', 'desc': "This paper introduces OpenVision, a new family of vision encoders that are fully open and cost-effective, designed to compete with OpenAI's CLIP. OpenVision not only matches but can also surpass CLIP's performance when used in multimodal frameworks like LLaVA. The authors leverage existing methodologies and datasets to improve encoder quality and provide insights into enhancing multimodal models. By offering a range of models with varying parameters, OpenVision allows users to choose between high performance and efficiency for their specific applications."}, 'zh': {'title': '开放视觉编码器，提升多模态模型性能', 'desc': 'OpenVision是一个完全开放的视觉编码器家族，旨在与OpenAI的CLIP相媲美或超越其性能。该论文展示了如何利用现有的训练框架和数据集，提升编码器的质量，并为多模态模型的发展提供实用的好处。OpenVision提供了从590万到6.32亿参数的多种选择，使得开发者可以在模型容量和效率之间灵活权衡。通过这些开放的资源，OpenVision为多模态模型的构建提供了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2505.04253', 'title': 'LLM-Independent Adaptive RAG: Let the Question Speak for Itself', 'url': 'https://huggingface.co/papers/2505.04253', 'abstract': 'Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '7ce9a465202a9c3c', 'authors': ['Maria Marina', 'Nikolay Ivanov', 'Sergey Pletenev', 'Mikhail Salnikov', 'Daria Galimzianova', 'Nikita Krayko', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.04253.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rag', '#benchmark', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Эффективный адаптивный поиск без использования языковых моделей', 'desc': 'Исследование предлагает новые методы адаптивного поиска для больших языковых моделей, не зависящие от самих моделей. Авторы изучили 27 признаков, организованных в 7 групп, и их гибридные комбинации для определения необходимости поиска дополнительной информации. Методы были оценены на 6 наборах данных вопросно-ответных систем. Результаты показывают, что предложенный подход соответствует производительности сложных методов, основанных на языковых моделях, при значительном повышении эффективности.'}, 'en': {'title': 'Efficient Adaptive Retrieval: Enhancing QA with External Information', 'desc': 'This paper addresses the issue of hallucinations in Large Language Models (LLMs) and presents a solution through Retrieval-Augmented Generation (RAG). The authors propose lightweight, LLM-independent adaptive retrieval methods that utilize external information, aiming to improve efficiency while maintaining performance. They analyze 27 features across 7 groups to create effective hybrid combinations for retrieval. The results indicate that their approach can achieve comparable performance to complex LLM-based methods but with significant efficiency improvements.'}, 'zh': {'title': '轻量级自适应检索，提升效率与准确性', 'desc': '大型语言模型（LLMs）容易出现幻觉，而增强检索生成（RAG）可以帮助减轻这一问题，但代价高昂且可能导致错误信息。自适应检索旨在仅在必要时进行检索，但现有方法依赖于基于LLM的不确定性估计，效率低下且不切实际。我们提出了一种基于外部信息的轻量级LLM独立自适应检索方法，研究了27个特征并将其组织成7个组及其混合组合。我们的评估结果表明，该方法在6个问答数据集上的表现与复杂的LLM方法相当，同时实现了显著的效率提升，展示了外部信息在自适应检索中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.03538', 'title': 'RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT', 'url': 'https://huggingface.co/papers/2505.03538', 'abstract': 'Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '9e7cb17dec2eda27', 'authors': ['Chuyu Zhao', 'Hao Huang', 'Jiashuo Guo', 'Ziyu Shen', 'Zhongwei Zhou', 'Jie Liu', 'Zekuan Yu'], 'affiliations': ['Academy for Engineering and Technology, Fudan University, Shanghai 200433, China', 'Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China', 'School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03538.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#transfer_learning', '#training'], 'emoji': '🦷', 'ru': {'title': 'Умное обучение для точной сегментации зубов', 'desc': 'Статья представляет новый метод полуконтролируемого обучения для сегментации зубов на КЛКТ-снимках, называемый Region-Aware Instructive Learning (RAIL). RAIL использует двухгрупповую двухстудентную архитектуру, где каждая группа содержит два студента, управляемых общим учителем. Метод вводит два инструктивных механизма: Disagreement-Focused Supervision (DFS) Controller для улучшения обучения с учителем и Confidence-Aware Learning (CAL) Modulator для повышения надежности псевдо-меток. Эксперименты показывают, что RAIL превосходит современные методы при ограниченном количестве размеченных данных.'}, 'en': {'title': 'Enhancing 3D Tooth Segmentation with RAIL: A Smart Learning Approach', 'desc': 'This paper presents a new method called Region-Aware Instructive Learning (RAIL) for improving 3D tooth segmentation from CBCT scans using semi-supervised learning. RAIL addresses challenges like limited supervision in ambiguous areas and the unreliability of pseudo-labels by employing a dual-group, dual-student framework with a shared teacher network. It introduces two key mechanisms: Disagreement-Focused Supervision (DFS) to enhance learning in uncertain regions, and Confidence-Aware Learning (CAL) to stabilize predictions by focusing on high-confidence areas. Experimental results demonstrate that RAIL outperforms existing methods, making it a significant advancement in the field of medical image segmentation.'}, 'zh': {'title': '区域感知指导学习：提升3D牙齿分割的半监督方法', 'desc': '半监督学习在CBCT扫描的3D牙齿分割中变得越来越重要，尤其是在标注数据稀缺的情况下。然而，现有方法面临两个主要挑战：在结构模糊或标注错误的区域，监督训练中的纠正监督有限；以及在未标注数据上，由于不可靠的伪标签导致的性能下降。为了解决这些问题，我们提出了区域感知指导学习（RAIL），这是一种双组双学生的半监督框架，通过交替训练促进组间知识转移和协作指导，同时减少对单一模型特征的过拟合。RAIL引入了两种指导机制，分别是关注分歧的监督控制器和基于置信度的学习调节器，以提高模型在不确定区域的学习效果。'}}}, {'id': 'https://huggingface.co/papers/2505.03105', 'title': 'Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation', 'url': 'https://huggingface.co/papers/2505.03105', 'abstract': 'Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI\'s evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.', 'score': 1, 'issue_id': 3657, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '24cdaf99b9b04dad', 'authors': ['Xule Lin'], 'affiliations': ['Department of Management and Entrepreneurship, Imperial College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.03105.jpg', 'data': {'categories': ['#agents', '#healthcare', '#science', '#ethics', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Cognitio Emergens: новая парадигма коэволюции человека и ИИ в науке', 'desc': 'Статья представляет концепцию Cognitio Emergens (CE) - новую модель сотрудничества человека и искусственного интеллекта в научных исследованиях. CE описывает, как распределяются роли между людьми и ИИ, какие эпистемические способности возникают в процессе взаимодействия, и какие силы влияют на эволюцию этих отношений. Модель подчеркивает динамичный и рекурсивный характер создания научных знаний в партнерстве человека и ИИ. CE предлагает сбалансированный взгляд на роль ИИ в науке, избегая как чрезмерного оптимизма, так и необоснованных страхов.'}, 'en': {'title': 'Transforming Scientific Collaboration: Humans and AI as Co-Evolutionary Partners', 'desc': 'This paper discusses how the relationship between humans and AI in scientific research is changing from a simple tool-user dynamic to a more collaborative partnership. It introduces a new framework called Cognitio Emergens (CE) that addresses the limitations of existing models by focusing on the evolving roles and interactions between humans and AI over time. CE includes three main components: Agency Configurations that describe how authority is shared, Epistemic Dimensions that outline capabilities developed through collaboration, and Partnership Dynamics that explore how these relationships change. By viewing human-AI collaboration as a co-evolutionary process, the framework aims to enhance scientific understanding while ensuring that human input remains significant in the face of advancing AI capabilities.'}, 'zh': {'title': '人类与AI的共同进化：知识创造的新视角', 'desc': '这篇论文探讨了人类与人工智能（AI）之间的合作关系如何从简单的工具使用者转变为共同进化的知识伙伴。文章介绍了Cognitio Emergens（CE）框架，旨在解决现有模型的局限性，强调科学理解是如何通过人类与AI的互动逐步形成的。CE框架包括三个主要组成部分：代理配置、认知维度和伙伴动态，帮助描述人类与AI之间的权力分配、合作能力和关系演变。通过重新定义人类与AI的科学合作，CE提供了促进有意义的人类参与和科学突破的概念工具。'}}}, {'id': 'https://huggingface.co/papers/2505.02820', 'title': 'AutoLibra: Agent Metric Induction from Open-Ended Feedback', 'url': 'https://huggingface.co/papers/2505.02820', 'abstract': 'Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don\'t click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent\'s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra\'s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.', 'score': 1, 'issue_id': 3661, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '52333e4c868e25f8', 'authors': ['Hao Zhu', 'Phil Cuvin', 'Xinkai Yu', 'Charlotte Ka Yee Yan', 'Jason Zhang', 'Diyi Yang'], 'affiliations': ['stanford.edu', 'upenn.edu', 'utoronto.ca'], 'pdf_title_img': 'assets/pdf/title_img/2505.02820.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rlhf', '#alignment', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'AutoLibra: Умная оценка ИИ-агентов на основе отзывов пользователей', 'desc': "AutoLibra - это новая система оценки агентов искусственного интеллекта, которая преобразует открытые отзывы пользователей в конкретные метрики для оценки поведения агентов. Система использует кластеризацию схожих положительных и отрицательных моделей поведения для создания четких метрик с конкретными примерами. AutoLibra также предлагает мета-метрики 'покрытие' и 'избыточность' для оценки согласованности набора метрик с открытыми отзывами. Экспериментально показано, что метрики AutoLibra эффективнее стандартных для улучшения производительности агентов в различных задачах."}, 'en': {'title': 'Transforming Feedback into Fine-Grained Agent Evaluation Metrics', 'desc': "The paper introduces AutoLibra, a novel framework for evaluating agents using fine-grained metrics derived from open-ended human feedback. It transforms qualitative feedback into quantitative metrics by clustering behaviors and defining clear examples, allowing for a more nuanced assessment of agent performance. AutoLibra also proposes two meta-metrics, 'coverage' and 'redundancy', to ensure that the evaluation metrics align well with the feedback provided. Experimental results show that AutoLibra improves agent performance significantly compared to traditional success metrics, making it a valuable tool for enhancing language agents."}, 'zh': {'title': 'AutoLibra：智能体评估的新标准', 'desc': '本文提出了一种名为AutoLibra的框架，用于评估智能体的表现。该框架通过将开放式人类反馈转化为细粒度的评估指标，来更好地衡量智能体的行为。AutoLibra通过对反馈进行归类和定义，生成具体的评估标准，并利用这些标准来优化智能体的表现。实验结果表明，AutoLibra能够生成比现有基准更有效的评估指标，并在多种任务中显著提升智能体的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02393', 'title': 'Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection', 'url': 'https://huggingface.co/papers/2505.02393', 'abstract': 'Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.', 'score': 1, 'issue_id': 3651, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'b5c708abbb25e1ce', 'authors': ['Sungheon Jeong', 'Jihong Park', 'Mohsen Imani'], 'affiliations': ['MOLOCO', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.02393.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#synthetic'], 'emoji': '🕵️', 'ru': {'title': 'Синтез событий из RGB для точного обнаружения видеоаномалий', 'desc': 'В этой статье представлен метод IEF-VAD для обнаружения аномалий в видео, который объединяет RGB-кадры с синтезированными событийными представлениями. Система моделирует шум датчика, применяет покадровые обновления в стиле фильтра Калмана и итеративно уточняет слитое латентное состояние. IEF-VAD достигает нового уровня производительности на нескольких реальных тестовых наборах данных для обнаружения аномалий. Метод подчеркивает важность синтетических событийных представлений для выделения ключевых признаков движения в задачах анализа видео.'}, 'en': {'title': 'Enhancing Video Anomaly Detection with Image-Event Fusion', 'desc': 'The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data.'}, 'zh': {'title': '图像与事件融合，提升视频异常检测的准确性', 'desc': '现有的视频异常检测器主要依赖RGB帧，但这些帧缺乏捕捉突发或瞬态运动线索的时间分辨率。为了解决这个问题，我们提出了一种图像-事件融合的视频异常检测框架（IEF-VAD），该框架直接从RGB视频合成事件表示，并通过一种基于不确定性的过程将其与图像特征融合。该系统通过拉普拉斯近似建模重尾传感器噪声，应用卡尔曼风格的逐帧更新来平衡时间上的模态，并迭代优化融合的潜在状态以消除残余的跨模态噪声。IEF-VAD在多个真实世界的异常检测基准上设定了新的最先进水平，展示了合成事件表示在强调RGB帧中常被低估的运动线索方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.01449', 'title': 'COSMOS: Predictable and Cost-Effective Adaptation of LLMs', 'url': 'https://huggingface.co/papers/2505.01449', 'abstract': 'Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.', 'score': 1, 'issue_id': 3665, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'cc6993ec1bb06272', 'authors': ['Jiayu Wang', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.01449.jpg', 'data': {'categories': ['#optimization', '#data', '#inference', '#training', '#benchmark'], 'emoji': '🔮', 'ru': {'title': 'COSMOS: Эффективное предсказание результатов адаптации LLM', 'desc': 'Статья представляет COSMOS - унифицированную систему предсказания для эффективного выбора стратегии адаптации больших языковых моделей (LLM). Авторы предлагают использовать легковесные прокси-модели и масштабируемые законы для прогнозирования производительности fine-tuning и обучения в контексте. Система COSMOS позволяет значительно сократить вычислительные затраты при сохранении высокой точности предсказаний. Эксперименты на восьми бенчмарках показали снижение вычислительных расходов в среднем на 92.72%.'}, 'en': {'title': 'COSMOS: Smart Predictions for Efficient LLM Adaptation', 'desc': 'This paper addresses the challenge of selecting the best model and adaptation strategy for large language models (LLMs) while managing resource constraints. It introduces COSMOS, a unified prediction framework designed to estimate the performance and cost of different adaptation strategies without the need for extensive trials. The framework utilizes lightweight proxy models and low-sample scaling laws to predict outcomes efficiently. The results indicate that COSMOS can significantly reduce computational costs while maintaining high prediction accuracy across various benchmarks.'}, 'zh': {'title': 'COSMOS：高效预测适应结果的框架', 'desc': '大型语言模型（LLMs）在多种任务中表现出色，但在资源有限的情况下选择最佳模型和适应策略非常具有挑战性。我们提出了COSMOS，一个统一的预测框架，可以在最低成本下高效估计适应结果。该框架利用轻量级代理模型和低样本扩展法则来预测微调性能和检索增强的上下文学习。我们的评估表明，COSMOS在八个基准测试中实现了高预测准确性，同时平均减少了92.72%的计算成本。'}}}, {'id': 'https://huggingface.co/papers/2505.07916', 'title': 'MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder', 'url': 'https://huggingface.co/papers/2505.07916', 'abstract': 'We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.', 'score': 80, 'issue_id': 3752, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '30175415a859995c', 'authors': ['Bowen Zhang', 'Congchao Guo', 'Geng Yang', 'Hang Yu', 'Haozhe Zhang', 'Heidi Lei', 'Jialong Mai', 'Junjie Yan', 'Kaiyue Yang', 'Mingqi Yang', 'Peikai Huang', 'Ruiyang Jin', 'Sitan Jiang', 'Weihua Cheng', 'Yawei Li', 'Yichen Xiao', 'Yiying Zhou', 'Yongmao Zhang', 'Yuan Lu', 'Yucen He'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07916.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#games', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон', 'desc': 'MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обучаемый кодировщик говорящего, который извлекает характеристики тембра из эталонного аудио без необходимости его транскрипции. Модель поддерживает 32 языка и демонстрирует отличные результаты по многим метрикам, достигая лучших показателей в клонировании голоса. MiniMax-Speech также обладает расширяемостью, позволяющей реализовать различные приложения, такие как управление эмоциями голоса и синтез тембра из текстового описания.'}, 'en': {'title': 'Revolutionizing Speech Synthesis with MiniMax-Speech', 'desc': 'MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.'}, 'zh': {'title': 'MiniMax-Speech：高质量语音生成的新突破', 'desc': 'MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2505.07591', 'title': 'A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models', 'url': 'https://huggingface.co/papers/2505.07591', 'abstract': "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.", 'score': 7, 'issue_id': 3750, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'ca7c47ccc0066e55', 'authors': ['Junjie Ye', 'Caishuang Huang', 'Zhuohan Chen', 'Wenjie Fu', 'Chenyuan Yang', 'Leyi Yang', 'Yilong Wu', 'Peng Wang', 'Meng Zhou', 'Xiaolong Yang', 'Tao Gui', 'Qi Zhang', 'Zhongchao Shi', 'Jianping Fan', 'Xuanjing Huang'], 'affiliations': ['Institute of Modern Languages and Linguistics, Fudan University', 'Lenovo Research', 'School of Computer Science, Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.07591.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#optimization', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Многомерная оценка следования инструкциям для больших языковых моделей', 'desc': 'Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV.'}, 'en': {'title': 'Enhancing LLMs with Multi-Dimensional Constraints', 'desc': 'This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters.'}, 'zh': {'title': '多维约束框架提升语言模型性能', 'desc': '本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.07215', 'title': 'Measuring General Intelligence with Generated Games', 'url': 'https://huggingface.co/papers/2505.07215', 'abstract': 'We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.', 'score': 5, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'ed99ec3875dd9a95', 'authors': ['Vivek Verma', 'David Huang', 'William Chen', 'Dan Klein', 'Nicholas Tomlin'], 'affiliations': ['Computer Science Division, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.07215.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#rl', '#games', '#synthetic'], 'emoji': '🎮', 'ru': {'title': 'gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды', 'desc': 'В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моделей к общему рассуждению. gg-bench автоматически генерирует новые игры с помощью большой языковой модели (LLM), которая создает описания и код игр. Для оценки языковые модели играют против обученных с помощью обучения с подкреплением агентов. Результаты показывают, что современные LLM достигают низких результатов (7-9% побед), в то время как специализированные модели рассуждений достигают 31-36% побед.'}, 'en': {'title': 'Dynamic Game Environments for Evaluating AI Reasoning', 'desc': 'The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI.'}, 'zh': {'title': 'gg-bench：评估语言模型推理能力的新基准', 'desc': '我们提出了gg-bench，这是一个用于评估语言模型一般推理能力的游戏环境集合。与大多数静态基准测试不同，gg-bench是一个数据生成过程，可以随时生成新的评估实例。具体来说，gg-bench通过使用大型语言模型生成新游戏的自然语言描述，并将每个游戏实现为Gym环境，最后通过自我对弈训练强化学习代理。我们通过模型在这些游戏中与强化学习代理的胜率来评估语言模型，发现当前最先进的语言模型在gg-bench上的胜率仅为7-9%。'}}}, {'id': 'https://huggingface.co/papers/2505.05464', 'title': 'Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging', 'url': 'https://huggingface.co/papers/2505.05464', 'abstract': 'Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.', 'score': 4, 'issue_id': 3760, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'd0d13229ec81018d', 'authors': ['Shiqi Chen', 'Jinghan Zhang', 'Tongyao Zhu', 'Wei Liu', 'Siyang Gao', 'Miao Xiong', 'Manling Li', 'Junxian He'], 'affiliations': ['City University of Hong Kong', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05464.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#architecture', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Объединение зрения и мышления: новый подход к мультимодальному ИИ', 'desc': 'Статья исследует возможность объединения моделей компьютерного зрения и языковых моделей для улучшения восприятия и рассуждения. Авторы предлагают метод слияния параметров моделей разных модальностей, что позволяет передавать способности к рассуждению от языковых моделей к визуально-языковым моделям без дополнительного обучения. Эксперименты показывают, что восприятие в основном кодируется в ранних слоях модели, а рассуждение - в средних и поздних. После слияния все слои начинают участвовать в рассуждении, в то время как распределение способностей восприятия остается неизменным.'}, 'en': {'title': 'Merging Models: Bridging Vision and Reasoning', 'desc': 'This paper investigates how to combine the visual understanding of Vision-Language Models (VLMs) with the reasoning skills of Large Language Models (LLMs) through a process called model merging. The authors propose a novel approach that merges models from different modalities, allowing VLMs to gain reasoning capabilities without the need for additional training. Their experiments reveal that while perception is mainly captured in the early layers of the model, reasoning is enhanced across all layers after merging. This study highlights the effectiveness of model merging for improving multimodal integration and understanding the interplay between perception and reasoning.'}, 'zh': {'title': '模型合并：跨模态的推理与感知结合', 'desc': '视觉-语言模型（VLMs）将视觉感知与大型语言模型（LLMs）的推理能力结合在一起。然而，这两种能力如何结合并相互贡献仍然不太清楚。我们通过模型合并的方法来探索感知与推理的组合，连接不同模型的参数。实验表明，模型合并能够成功地将LLMs的推理能力转移到VLMs，并且有助于理解感知与推理的内部机制。'}}}, {'id': 'https://huggingface.co/papers/2505.08638', 'title': 'TRAIL: Trace Reasoning and Agentic Issue Localization', 'url': 'https://huggingface.co/papers/2505.08638', 'abstract': 'The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '823a40ec2bb1f793', 'authors': ['Darshan Deshpande', 'Varun Gangal', 'Hersh Mehta', 'Jitin Krishnan', 'Anand Kannappan', 'Rebecca Qian'], 'affiliations': ['Patronus AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.08638.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#agents', '#open_source'], 'emoji': '🕵️', 'ru': {'title': 'Новый подход к отладке агентных систем: от таксономии ошибок к масштабируемой оценке', 'desc': 'Статья представляет новый подход к оценке сложных рабочих процессов агентных систем. Авторы разработали таксономию типов ошибок в агентных системах и создали набор данных TRAIL из 148 аннотированных трасс. Исследование показало, что современные языковые модели с длинным контекстом плохо справляются с отладкой трасс, даже лучшая модель Gemini-2.5-pro достигла всего 11% точности на TRAIL. Датасет и код открыты для дальнейших исследований в области масштабируемой оценки агентных рабочих процессов.'}, 'en': {'title': 'Revolutionizing Evaluation for Agentic Workflows', 'desc': 'This paper addresses the challenges of evaluating agentic workflows, which are increasingly used in various fields. Current methods rely on manual analysis of complex workflow traces, making them inefficient as the volume of data grows. The authors propose a new framework that includes a taxonomy of error types specific to agentic systems and introduce a dataset of 148 annotated traces for evaluation. Their findings indicate that existing large language models struggle with debugging these traces, highlighting the need for improved evaluation techniques in this area.'}, 'zh': {'title': '智能工作流评估的新方法', 'desc': '随着智能工作流在各个领域的广泛应用，系统地评估这些系统生成的复杂轨迹变得至关重要。目前的评估方法依赖于人工、特定领域的分析，这种方法无法适应日益复杂和庞大的智能输出。错误分析在这些环境中变得更加复杂，因为外部工具输出与语言模型推理之间的相互作用，使得调试变得更加困难。本文提出了一种针对智能工作流轨迹的动态评估方法，并引入了一种错误类型的正式分类法，同时构建了148个大型人类标注的轨迹数据集，以支持未来在智能工作流评估方面的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.08175', 'title': 'Fast Text-to-Audio Generation with Adversarial Post-Training', 'url': 'https://huggingface.co/papers/2505.08175', 'abstract': 'Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating approx12s of 44.1kHz stereo audio in approx75ms on an H100, and approx7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '845d8b64408c2d62', 'authors': ['Zachary Novack', 'Zach Evans', 'Zack Zukowski', 'Josiah Taylor', 'CJ Carr', 'Julian Parker', 'Adnan Al-Sinan', 'Gian Marco Iodice', 'Julian McAuley', 'Taylor Berg-Kirkpatrick', 'Jordi Pons'], 'affiliations': ['Arm', 'Stability AI', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.08175.jpg', 'data': {'categories': ['#audio', '#inference', '#optimization', '#diffusion', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Сверхбыстрая генерация аудио по тексту с помощью ARC пост-обучения', 'desc': 'Статья представляет новый метод ускорения моделей диффузии и потоков для генерации аудио по тексту - Adversarial Relativistic-Contrastive (ARC) пост-обучение. Это первый состязательный алгоритм ускорения, не основанный на дистилляции. ARC сочетает релятивистский состязательный подход с новой контрастивной целевой функцией дискриминатора для улучшения соответствия промпту. В сочетании с оптимизациями Stable Audio Open метод позволяет генерировать 12 секунд стерео аудио 44.1кГц за 75 мс на H100 и 7 секунд на мобильном устройстве.'}, 'en': {'title': 'Accelerating Text-to-Audio: The Fastest Model Yet!', 'desc': "This paper introduces Adversarial Relativistic-Contrastive (ARC) post-training, a novel method designed to speed up text-to-audio systems without relying on distillation. ARC post-training enhances diffusion and flow models by applying a relativistic adversarial approach combined with a contrastive discriminator objective, which improves the model's ability to follow prompts accurately. The authors demonstrate that their method significantly reduces inference time, achieving audio generation in approximately 75 milliseconds on high-performance hardware and 7 seconds on mobile devices. This advancement positions ARC post-training as the fastest text-to-audio model currently available, making it more practical for creative applications."}, 'zh': {'title': '加速文本到音频生成的革命性方法', 'desc': '本文介绍了一种新的后训练方法，称为对抗相对对比（ARC），旨在加速文本到音频系统的推理速度。ARC后训练算法不依赖于蒸馏，能够有效提升扩散/流模型的性能。通过结合相对对抗形式和对比判别器目标，ARC鼓励模型更好地遵循提示。经过优化后，该模型在H100上能在约75毫秒内生成12秒的44.1kHz立体声音频，成为目前已知的最快文本到音频模型。'}}}, {'id': 'https://huggingface.co/papers/2505.08727', 'title': 'Memorization-Compression Cycles Improve Generalization', 'url': 'https://huggingface.co/papers/2505.08727', 'abstract': 'We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.', 'score': 2, 'issue_id': 3760, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '07a8c687bc82dfeb', 'authors': ['Fangyuan Yu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08727.jpg', 'data': {'categories': ['#optimization', '#data', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Сжатие для обобщения: новый подход к обучению языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей, называемый Information Bottleneck Language Modeling (IBLM). Авторы доказывают, что улучшение обобщающей способности модели достигается не только за счет увеличения объема данных, но и путем сжатия внутренних представлений. В работе наблюдается циклическое чередование фаз запоминания и сжатия информации во время предобучения больших языковых моделей. На основе этих наблюдений предлагается алгоритм Gated Phase Transition (GAPT), который адаптивно переключается между фазами запоминания и сжатия, улучшая обобщающую способность модели и снижая эффект катастрофического забывания.'}, 'en': {'title': 'Compressing Knowledge for Better Generalization', 'desc': 'This paper demonstrates that improving generalization in machine learning models can be achieved not only by increasing the amount of data but also by compressing the internal representations of the model. The authors introduce the Information Bottleneck Language Modeling (IBLM) objective, which focuses on minimizing the complexity of representations while maintaining effective prediction capabilities. They observe a cycle of memorization and compression during the pretraining of large language models, which aligns with the principles of IBLM and reflects biological processes of learning and memory consolidation. To leverage this insight, they propose the Gated Phase Transition (GAPT) algorithm, which alternates between phases of memorization and compression, leading to significant improvements in model performance and generalization.'}, 'zh': {'title': '压缩与记忆的平衡提升模型泛化能力', 'desc': '本文理论上证明，模型的泛化能力不仅通过增加数据量来提高，还可以通过压缩内部表示来实现。我们提出了信息瓶颈语言建模（IBLM）目标，将语言建模重新构建为一个约束优化问题：在最优预测性能的条件下，最小化表示熵。通过实验证明，在大规模语言模型的预训练过程中，出现了记忆与压缩的循环现象，这种现象与IBLM所描述的预测-压缩权衡密切相关。基于这一观察，我们提出了门控相变（GAPT）训练算法，能够自适应地在记忆和压缩阶段之间切换，从而显著提高模型的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.08665', 'title': 'SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation', 'url': 'https://huggingface.co/papers/2505.08665', 'abstract': 'Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.', 'score': 2, 'issue_id': 3750, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '8cbf16dc2ec90273', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Free University of Bozen-Bolzano Bozen-Bolzano, IT'], 'pdf_title_img': 'assets/pdf/title_img/2505.08665.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training'], 'emoji': '🎥', 'ru': {'title': 'SkillFormer: эффективная оценка мастерства по мультиракурсному видео', 'desc': 'SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами.'}, 'en': {'title': 'SkillFormer: Efficient Multi-View Skill Assessment', 'desc': 'This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation.'}, 'zh': {'title': 'SkillFormer：高效的多视角技能评估架构', 'desc': '评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。'}}}, {'id': 'https://huggingface.co/papers/2505.08311', 'title': 'AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale', 'url': 'https://huggingface.co/papers/2505.08311', 'abstract': 'We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.', 'score': 2, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '492d10424bc0d97d', 'authors': ['Yunjie Ji', 'Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.08311.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#open_source', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Открытая 32B-модель устанавливает новую планку в рассуждениях', 'desc': 'AM-Thinking-v1 - это языковая модель с 32 миллиардами параметров, которая демонстрирует передовые способности к рассуждению. Модель превосходит DeepSeek-R1 и конкурирует с ведущими MoE-моделями, показывая впечатляющие результаты в математических и кодинговых тестах. AM-Thinking-v1 была создана на основе открытой модели Qwen2.5-32B с использованием тщательно разработанного процесса дообучения, включающего supervised fine-tuning и reinforcement learning. Авторы подчеркивают, что модель с 32 миллиардами параметров представляет собой практичный баланс между производительностью и удобством использования.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Innovation', 'desc': 'AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.'}, 'zh': {'title': '开源创新，推理新高度', 'desc': '我们介绍了AM-Thinking-v1，这是一个32B的密集语言模型，推动了推理的前沿，体现了开源创新的合作精神。它在AIME 2024、AIME 2025和LiveCodeBench等基准测试中表现出色，超越了DeepSeek-R1，并与领先的专家混合模型如Qwen3-235B-A22B和Seed1.5-Thinking相媲美。AM-Thinking-v1完全基于开源的Qwen2.5-32B基础模型，结合监督微调和强化学习，展现了卓越的推理能力。我们的工作证明了开源社区可以在32B规模上实现高性能，平衡顶级性能与实际可用性，激励更多合作努力。'}}}, {'id': 'https://huggingface.co/papers/2504.21475', 'title': 'Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines', 'url': 'https://huggingface.co/papers/2504.21475', 'abstract': 'This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.', 'score': 2, 'issue_id': 3756, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '86e4cc7ad4a84ece', 'authors': ['Serry Sibaee', 'Samar Ahmed', 'Abdullah Al Harbi', 'Omer Nacar', 'Adel Ammar', 'Yasser Habashi', 'Wadii Boulila'], 'affiliations': ['College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia', 'Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia', 'Independent Researcher, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21475.jpg', 'data': {'categories': ['#training', '#data', '#machine_translation', '#dataset', '#architecture', '#multilingual', '#low_resource'], 'emoji': '📚', 'ru': {'title': 'Революция в арабской лингвистике: трансформеры на службе обратного словаря', 'desc': 'Исследование посвящено разработке эффективной системы обратного словаря для арабского языка, позволяющей находить слова по их описаниям или значениям. Авторы представляют новый подход на основе трансформеров с полуэнкодерной нейросетевой архитектурой, достигающий наилучших результатов в задачах арабского обратного словаря. Методология включает создание комплексного датасета и установление формальных стандартов качества для арабских лексикографических определений. Эксперименты показали, что арабоязычные предобученные модели значительно превосходят многоязычные эмбеддинги, причем ARBERTv2 достигает наилучшего рейтингового показателя.'}, 'en': {'title': 'Bridging the Gap in Arabic Language Processing with a Reverse Dictionary', 'desc': 'This paper presents a new system for Arabic natural language processing called the Arabic Reverse Dictionary (RD), which helps users find words based on their meanings. The authors introduce a transformer-based model with a unique semi-encoder architecture that improves performance on Arabic RD tasks. They also create a detailed dataset and establish quality standards for Arabic definitions, showing that specialized Arabic models outperform general multilingual ones. Additionally, the study offers a theoretical framework for reverse dictionaries and introduces a Python library for easy implementation and training.'}, 'zh': {'title': '阿拉伯语反向词典：提升语言处理的利器', 'desc': '本研究解决了阿拉伯语自然语言处理中的关键问题，开发了一种有效的阿拉伯语反向词典系统，用户可以根据描述或含义找到单词。我们提出了一种新颖的基于变换器的半编码神经网络架构，具有几何递减层，达到了阿拉伯语反向词典任务的最先进结果。我们的研究方法包括全面的数据集构建过程，并建立了阿拉伯语词典定义的正式质量标准。实验表明，专门针对阿拉伯语的模型显著优于通用多语言嵌入，ARBERTv2模型获得了最佳排名分数。'}}}, {'id': 'https://huggingface.co/papers/2505.08751', 'title': 'Aya Vision: Advancing the Frontier of Multilingual Multimodality', 'url': 'https://huggingface.co/papers/2505.08751', 'abstract': 'Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.', 'score': 1, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'c063c125e504fa88', 'authors': ['Saurabh Dash', 'Yiyang Nan', 'John Dang', 'Arash Ahmadian', 'Shivalika Singh', 'Madeline Smith', 'Bharat Venkitesh', 'Vlad Shmyhlo', 'Viraat Aryabumi', 'Walter Beller-Morales', 'Jeremy Pekmez', 'Jason Ozuzu', 'Pierre Richemond', 'Acyr Locatelli', 'Nick Frosst', 'Phil Blunsom', 'Aidan Gomez', 'Ivan Zhang', 'Marzieh Fadaee', 'Manoj Govindassamy', 'Sudip Roy', 'Matthias Gallé', 'Beyza Ermis', 'Ahmet Üstün', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.08751.jpg', 'data': {'categories': ['#synthetic', '#data', '#low_resource', '#multimodal', '#training', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Прорыв в создании эффективных многоязычных мультимодальных ИИ-моделей', 'desc': 'Статья представляет новые методы для создания многоязычных мультимодальных языковых моделей. Авторы разработали систему синтетической аннотации для создания качественных многоязычных мультимодальных данных для обучения. Они также предложили технику объединения кросс-модальных моделей для предотвращения катастрофического забывания. Модели Aya-Vision, созданные с использованием этих методов, показывают лучшие результаты по сравнению с более крупными конкурентами в многоязычных мультимодальных задачах.'}, 'en': {'title': 'Advancing Multimodal Language Models for Multilingual Mastery', 'desc': 'This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.'}, 'zh': {'title': '多模态语言模型的突破性进展', 'desc': '构建多模态语言模型面临许多挑战，包括视觉和语言模态的对齐、高质量指令数据的整理，以及在引入视觉后避免文本能力的退化。在多语言环境中，这些困难更加突出，因为不同语言的多模态数据需求加剧了数据稀缺，机器翻译常常扭曲意义，并且灾难性遗忘现象更加明显。为了解决这些问题，我们提出了新的技术，包括一个合成注释框架，用于整理高质量、多样化的多语言多模态指令数据，使Aya Vision模型能够在多种语言中对多模态输入生成自然、符合人类偏好的响应。此外，我们还提出了一种跨模态模型合并技术，有效减轻灾难性遗忘，同时增强多模态生成性能。'}}}, {'id': 'https://huggingface.co/papers/2505.08712', 'title': 'NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance', 'url': 'https://huggingface.co/papers/2505.08712', 'abstract': "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.", 'score': 1, 'issue_id': 3752, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'a68cc40de86ece61', 'authors': ['Wenzhe Cai', 'Jiaqi Peng', 'Yuqiang Yang', 'Yujian Zhang', 'Meng Wei', 'Hanqing Wang', 'Yilun Chen', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.08712.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#diffusion', '#robotics', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'NavDP: Универсальная навигация роботов из симуляции в реальность', 'desc': 'Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключительно на симуляциях. NavDP использует комбинацию генерации траектории на основе диффузии и функции критика для выбора траектории, что позволяет ей успешно работать с разными типами роботов в реальном мире. Система обучается на большом наборе данных, сгенерированном в симуляции, что значительно эффективнее сбора реальных данных. Эксперименты показывают высокую производительность и способность к обобщению для различных типов роботов в разнообразных средах.'}, 'en': {'title': 'Efficient Robot Navigation with Simulation-Driven Learning', 'desc': 'This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.'}, 'zh': {'title': '机器人导航的新突破：导航扩散策略', 'desc': '本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.08445', 'title': 'Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency', 'url': 'https://huggingface.co/papers/2505.08445', 'abstract': 'Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.', 'score': 1, 'issue_id': 3756, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '7b4d8b3daa6efb6d', 'authors': ['Adel Ammar', 'Anis Koubaa', 'Omer Nacar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, P.O. Box 50927, Riyadh 11533, Saudi Arabia', 'Prince Sultan University, Rafha Street, P.O. Box 66833, Riyadh 11586, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.08445.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#healthcare', '#hallucinations', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация RAG: баланс между скоростью и точностью', 'desc': 'Статья анализирует влияние гиперпараметров на скорость и качество в системах генерации с дополнением извлечением (RAG). Исследуются векторные хранилища Chroma и Faiss, стратегии разбиения текста, переранжирование кросс-энкодером и температура. Оценивается шесть метрик, включая достоверность, корректность и релевантность ответов. Результаты показывают компромисс между скоростью и точностью, а также демонстрируют, что RAG-системы могут достичь очень высокой точности извлечения при правильном подборе гиперпараметров.'}, 'en': {'title': 'Optimizing RAG: Balancing Speed and Accuracy for Better AI Responses', 'desc': 'This paper explores how hyperparameters affect the performance of Retrieval-Augmented Generation (RAG) systems, which combine language generation with external information retrieval. It evaluates different vector stores, chunking strategies, and re-ranking methods to find the best balance between speed and accuracy. The findings indicate that while Chroma is faster, Faiss offers better retrieval precision, highlighting a trade-off between these two factors. The study also shows that with optimal hyperparameter tuning, RAG systems can achieve very high retrieval accuracy, which is crucial for applications like healthcare decision support.'}, 'zh': {'title': '优化RAG系统，实现高效准确的检索', 'desc': '大型语言模型在任务表现上表现优异，但常常会出现幻觉或依赖过时知识。检索增强生成（RAG）通过将生成与外部搜索结合，解决了这些问题。我们分析了超参数如何影响RAG系统的速度和质量，包括向量存储、分块策略、交叉编码重排序和温度等，并评估了六个指标。研究结果帮助从业者在调整RAG系统时平衡计算成本和准确性，以实现透明且最新的响应。'}}}, {'id': 'https://huggingface.co/papers/2505.07416', 'title': 'ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation', 'url': 'https://huggingface.co/papers/2505.07416', 'abstract': 'Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP', 'score': 1, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '4f126d39b5476454', 'authors': ['Truc Mai-Thanh Nguyen', 'Dat Minh Nguyen', 'Son T. Luu', 'Kiet Van Nguyen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07416.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#multilingual', '#low_resource', '#open_source'], 'emoji': '🇻🇳', 'ru': {'title': 'ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке', 'desc': 'Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзывов на вьетнамском языке. Авторы использовали искусственный интеллект для помощи аннотаторам, что значительно ускорило процесс разметки и снизило затраты. Датасет охватывает 4 домена, включая 2000 продуктов и 46000 отзывов. Исследователи провели эксперименты с базовыми моделями машинного обучения для оценки качества аннотаций, созданных человеком и ИИ.'}, 'en': {'title': 'Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction', 'desc': 'This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks.'}, 'zh': {'title': '提升越南语评论有用性的智能解决方案', 'desc': '多模态评论有用性预测（MRHP）是推荐系统中的一个重要任务，尤其是在电子商务平台上。本文介绍了ViMRHP（越南语多模态评论有用性预测），这是一个针对越南语的MRHP任务的大规模基准数据集，涵盖了四个领域，包括2000个产品和46000条评论。为了优化注释过程，我们利用人工智能辅助注释者构建ViMRHP数据集，从而显著减少了注释时间和成本，同时保持数据质量。尽管AI生成的注释在复杂任务中仍存在局限性，但我们通过详细的性能分析进一步探讨了这些问题。'}}}, {'id': 'https://huggingface.co/papers/2504.21853', 'title': 'A Survey of Interactive Generative Video', 'url': 'https://huggingface.co/papers/2504.21853', 'abstract': 'Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.', 'score': 40, 'issue_id': 3550, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '4e975f915f638955', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Hao Chen', 'Xihui Liu'], 'affiliations': ['Kuaishou Technology, Shenzhen, China', 'The Hong Kong University of Science and Technology (HKUST), Hong Kong', 'The University of Hong Kong, Pok Fu Lam, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.21853.jpg', 'data': {'categories': ['#robotics', '#interpretability', '#video', '#optimization', '#survey', '#agents', '#games', '#multimodal'], 'emoji': '🎮', 'ru': {'title': 'IGV: Будущее интерактивного видеоконтента', 'desc': 'Статья представляет обзор технологии Интерактивного Генеративного Видео (IGV), которая сочетает генеративные возможности с интерактивными функциями. Авторы рассматривают применение IGV в трех основных областях: игры, воплощенный ИИ и автономное вождение. Предложена комплексная структура, разбивающая идеальную систему IGV на пять модулей: Генерация, Контроль, Память, Динамика и Интеллект. В работе анализируются технические проблемы и будущие направления развития каждого компонента IGV.'}, 'en': {'title': 'Empowering Interactive Experiences with Generative Video', 'desc': 'Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.'}, 'zh': {'title': '推动互动生成视频技术的未来发展', 'desc': '互动生成视频（IGV）是一种新兴技术，旨在满足对高质量互动视频内容的需求。本文将IGV定义为一种结合生成能力和互动特性的技术，能够通过控制信号和反馈实现用户参与。我们调查了IGV在游戏、具身人工智能和自动驾驶等三个主要领域的应用，并提出了一个全面的框架，将理想的IGV系统分解为生成、控制、记忆、动态和智能五个模块。通过系统分析技术挑战和未来方向，本文旨在推动IGV领域的研究与发展。'}}}, {'id': 'https://huggingface.co/papers/2505.00662', 'title': 'DeepCritic: Deliberate Critique with Large Language Models', 'url': 'https://huggingface.co/papers/2505.00662', 'abstract': 'As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.', 'score': 37, 'issue_id': 3549, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '259dddc97137d27a', 'authors': ['Wenkai Yang', 'Jingwen Chen', 'Yankai Lin', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00662.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#reasoning', '#rlhf', '#math', '#training'], 'emoji': '🧮', 'ru': {'title': 'Усовершенствование LLM для глубокой критики математических решений', 'desc': 'Эта статья представляет новый двухэтапный подход к улучшению способностей больших языковых моделей (LLM) критиковать математические решения. На первом этапе используется Qwen2.5-72B-Instruct для генерации подробных пошаговых критик, которые служат обучающими данными. Затем применяется обучение с подкреплением для дальнейшего улучшения критических способностей модели. Разработанная модель-критик на базе Qwen2.5-7B-Instruct превосходит существующие LLM-критики в выявлении ошибок и предоставлении детальной обратной связи.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions', 'desc': "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."}, 'zh': {'title': '提升大型语言模型的数学批评能力', 'desc': '随着大型语言模型（LLMs）的快速发展，提供准确的反馈和可扩展的监督变得尤为重要。本文提出了一种新颖的两阶段框架，旨在增强LLMs在数学批评方面的能力。通过生成详细的逐步批评，模型能够更准确地识别错误并提供深入的反馈，帮助生成模型纠正错误。我们的实验表明，改进后的批评模型在错误识别基准测试中显著优于现有的LLM批评者。'}}}, {'id': 'https://huggingface.co/papers/2505.00703', 'title': 'T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT', 'url': 'https://huggingface.co/papers/2505.00703', 'abstract': 'Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1', 'score': 28, 'issue_id': 3548, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'ca564761ff71d15e', 'authors': ['Dongzhi Jiang', 'Ziyu Guo', 'Renrui Zhang', 'Zhuofan Zong', 'Hao Li', 'Le Zhuo', 'Shilin Yan', 'Pheng-Ann Heng', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'CUHK MiuLar Lab', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.00703.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🎨', 'ru': {'title': 'Рассуждения на двух уровнях улучшают генерацию изображений по тексту', 'desc': 'Данная статья представляет T2I-R1 - новую модель генерации изображений по тексту, улучшенную с помощью рассуждений и обучения с подкреплением. Модель использует двухуровневый процесс рассуждений: семантический уровень для планирования промпта и токенный уровень для попиксельной генерации. Авторы вводят метод BiCoT-GRPO для оптимизации обоих уровней рассуждений. Применение этих стратегий к базовой модели Janus-Pro позволило значительно улучшить результаты на бенчмарках T2I-CompBench и WISE.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Enhanced Reasoning', 'desc': 'This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests.'}, 'zh': {'title': '双层思维链提升文本到图像生成', 'desc': '本文介绍了一种新颖的文本到图像生成模型T2I-R1，该模型结合了强化学习和双层思维链推理过程。我们提出了两种思维链：语义层次的思维链用于高层次的提示规划，令生成过程更具逻辑性；而令牌层次的思维链则用于在逐块生成过程中进行低层次的像素处理。通过引入BiCoT-GRPO，我们能够在同一训练步骤中优化这两种思维链，从而提升生成效果。实验结果表明，T2I-R1在多个基准测试中表现优异，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2505.00497', 'title': 'KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution', 'url': 'https://huggingface.co/papers/2505.00497', 'abstract': 'Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.', 'score': 9, 'issue_id': 3554, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'dc645b5c7e476cea', 'authors': ['Antoni Bigata', 'Rodrigo Mira', 'Stella Bounareli', 'Michał Stypułkowski', 'Konstantinos Vougioukas', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London', 'University of Wrocław'], 'pdf_title_img': 'assets/pdf/title_img/2505.00497.jpg', 'data': {'categories': ['#cv', '#leakage', '#video', '#open_source', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'KeySync: Революция в синхронизации губ для видео', 'desc': 'KeySync - это новая двухэтапная система для синхронизации губ в видео с новым аудио. Она решает проблемы временной согласованности, утечки выражений и окклюзий лица с помощью специальной стратегии маскирования. KeySync достигает лучших результатов в реконструкции губ и кросс-синхронизации, улучшая визуальное качество и уменьшая утечку выражений согласно новой метрике LipLeak. Система эффективно справляется с окклюзиями и превосходит существующие методы в реальных приложениях, таких как автоматический дубляж.'}, 'en': {'title': 'KeySync: Mastering Lip Synchronization with Precision', 'desc': 'This paper introduces KeySync, a two-stage framework designed to improve lip synchronization in videos by aligning lip movements with new audio inputs. It addresses common challenges in talking head generation, such as maintaining temporal consistency and managing expression leakage and facial occlusions. KeySync employs a unique masking strategy to effectively tackle these issues, resulting in enhanced visual quality and reduced leakage as measured by a new metric called LipLeak. The framework demonstrates state-of-the-art performance in lip reconstruction and cross-synchronization, validated through various ablation studies.'}, 'zh': {'title': 'KeySync：提升唇部同步的创新框架', 'desc': '本论文介绍了一种名为KeySync的双阶段框架，用于解决唇部同步中的时间一致性问题。该方法还通过精心设计的遮罩策略，解决了输入视频中的表情泄漏和面部遮挡等新挑战。实验结果表明，KeySync在唇部重建和交叉同步方面达到了最先进的效果，显著提高了视觉质量并减少了表情泄漏。我们还通过多项消融研究验证了新遮罩方法在处理遮挡方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.00234', 'title': 'Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks', 'url': 'https://huggingface.co/papers/2505.00234', 'abstract': 'Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.', 'score': 9, 'issue_id': 3563, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '9e975cfc8ecf9dea', 'authors': ['Vishnu Sarukkai', 'Zhiqiang Xie', 'Kayvon Fatahalian'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00234.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#rl', '#agents', '#optimization', '#benchmark', '#games'], 'emoji': '🧠', 'ru': {'title': 'Самообучение LLM-агентов на собственном опыте', 'desc': 'Статья представляет метод улучшения агентов на основе больших языковых моделей (LLM) для задач последовательного принятия решений. Вместо специфической настройки под конкретные задачи, авторы предлагают автоматическое обучение на основе успешного опыта агента на схожих задачах. Метод включает создание и уточнение базы данных самостоятельно сгенерированных примеров. Результаты показывают значительное улучшение производительности на нескольких бенчмарках, сравнимое с более сложными подходами, использующими специфичные для задач компоненты.'}, 'en': {'title': 'Learning from Success: Automating Improvement in LLM Agents', 'desc': 'This paper explores a new approach for improving Large Language Model (LLM) agents in sequential decision-making tasks without relying on extensive task-specific knowledge engineering. Instead of manually tuning prompts or creating curated examples, the authors propose that LLM agents can learn from their own successful experiences by building a database of self-generated examples. The study shows that simply accumulating successful task trajectories can significantly enhance performance on various benchmarks. Additionally, the paper introduces methods for selecting high-performing examples from this database, leading to even better results that rival more complex, knowledge-intensive methods.'}, 'zh': {'title': '自动学习提升LLM代理性能的创新方法', 'desc': '本文探讨了如何通过自我生成的成功经验来自动提升大型语言模型（LLM）代理在顺序决策任务中的表现，而不是依赖于特定任务的知识工程。研究表明，简单地积累成功的轨迹可以显著提高在多个基准测试中的表现。我们还提出了两种扩展方法：通过基于种群的训练选择高效的示例集合，以及根据实证效用保留个别轨迹。这些方法进一步提升了性能，展示了自动轨迹数据库构建作为知识工程的有效替代方案。'}}}, {'id': 'https://huggingface.co/papers/2504.18715', 'title': 'Spatial Speech Translation: Translating Across Space With Binaural\n  Hearables', 'url': 'https://huggingface.co/papers/2504.18715', 'abstract': "Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.", 'score': 7, 'issue_id': 3566, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '979ba773cd3e90a0', 'authors': ['Tuochao Chen', 'Qirui Wang', 'Runlin He', 'Shyam Gollakota'], 'affiliations': ['Paul G. Allen School, University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.18715.jpg', 'data': {'categories': ['#audio', '#multilingual', '#machine_translation'], 'emoji': '🎧', 'ru': {'title': 'Пространственный перевод речи: слышать мир на своем языке', 'desc': 'Статья представляет концепцию пространственного перевода речи для слуховых устройств. Система позволяет переводить речь окружающих людей, сохраняя направление и характеристики голоса каждого говорящего в бинауральном выводе. Авторы решают технические задачи слепого разделения источников, локализации, перевода в реальном времени и бинаурального рендеринга. Прототип системы демонстрирует эффективность в условиях сильных помех и реверберации, достигая оценки BLEU до 22.01 при переводе между языками.'}, 'en': {'title': 'Transforming Speech Translation with Spatial Awareness', 'desc': "This paper presents a new approach to spatial speech translation using hearables, which are devices that can translate spoken language in real-time while preserving the spatial characteristics of the speakers. The authors address key challenges such as separating overlapping voices, accurately locating speakers, and ensuring that translations are expressive and timely, all while running efficiently on Apple M2 hardware. Their prototype demonstrates significant improvements over existing models, achieving a BLEU score of 22.01 in noisy environments, indicating high translation quality. User studies validate the system's ability to maintain the spatial cues of translated speech, marking a significant advancement in integrating spatial awareness into speech translation technology."}, 'zh': {'title': '空间语音翻译：让交流无障碍', 'desc': '本文介绍了一种新颖的空间语音翻译概念，旨在通过耳机将环境中说话者的语言实时翻译为佩戴者的母语，同时保留每位说话者的方向和独特声音特征。为实现这一目标，研究解决了多个技术挑战，包括盲源分离、定位、实时表达翻译和双耳渲染，以确保翻译音频中的说话者方向得以保留。通过原型双耳耳机的概念验证评估，研究表明该系统在强干扰环境中仍能实现高达22.01的BLEU分数，优于现有模型。用户研究进一步确认了该系统在真实环境中空间渲染翻译语音的有效性，标志着将空间感知整合到语音翻译中的第一步。'}}}, {'id': 'https://huggingface.co/papers/2504.21659', 'title': 'AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization', 'url': 'https://huggingface.co/papers/2504.21659', 'abstract': 'Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1', 'score': 6, 'issue_id': 3549, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '6487e5a67faf07a5', 'authors': ['Haotian Luo', 'Haiying He', 'Yibo Wang', 'Jinluan Yang', 'Rui Liu', 'Naiqiang Tan', 'Xiaochun Cao', 'Dacheng Tao', 'Li Shen'], 'affiliations': ['China Agricultural University', 'Didichuxing Co. Ltd', 'Nanyang Technological University', 'Sun Yat-sen University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21659.jpg', 'data': {'categories': ['#reasoning', '#inference', '#math', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное рассуждение: эффективность без потери качества', 'desc': 'Статья представляет новый подход к адаптивному и эффективному рассуждению в языковых моделях. Авторы предлагают двухэтапную структуру, включающую гибридную модель рассуждений и би-уровневое обучение предпочтениям. Эксперименты показывают, что метод значительно сокращает вычислительные затраты при сохранении производительности. На пяти математических наборах данных средняя длина рассуждений сократилась более чем на 50%.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Inference in Complex Tasks', 'desc': 'This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets.'}, 'zh': {'title': '自适应推理，提升效率！', 'desc': '最近，长推理模型在复杂推理任务中表现出色，但常常导致推理开销大，因此效率成为一个关键问题。我们的实证分析显示，使用长链推理（Long-CoT）的好处在不同问题上差异很大：有些问题需要复杂推理，而其他问题则没有改善，甚至准确率下降。这促使我们提出自适应推理策略，根据输入调整推理深度。我们提出了一种新颖的两阶段框架，通过混合长短链推理模型和双层偏好训练，显著降低推理成本，同时保持性能。'}}}, {'id': 'https://huggingface.co/papers/2504.19394', 'title': 'LLMs for Engineering: Teaching Models to Design High Powered Rockets', 'url': 'https://huggingface.co/papers/2504.19394', 'abstract': "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.", 'score': 6, 'issue_id': 3558, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': '5b65f8f309063f13', 'authors': ['Toby Simonds'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.19394.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#agi', '#benchmark', '#games', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'ИИ с обучением с подкреплением превосходит людей в проектировании ракет', 'desc': 'Это исследование оценивает возможности больших языковых моделей (LLM) в проектировании мощных ракет с помощью бенчмарка RocketBench. Модели тестируются на задачах оптимизации целевой высоты и точной посадки. Результаты показывают, что современные LLM демонстрируют хорошие базовые инженерные знания, но затрудняются с итерациями на основе результатов симуляций. Однако при усилении обучением с подкреплением (RL) модель с 7 миллиардами параметров превосходит как современные базовые модели, так и экспертов-людей.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Rocket Design Challenges', 'desc': 'This paper investigates the use of Large Language Models (LLMs) in the field of physical engineering, specifically in high-powered rocketry design. It introduces RocketBench, a benchmark that connects LLMs to advanced rocket simulations, and evaluates their performance on design tasks like optimizing target altitude and achieving precision landings. The results indicate that while LLMs possess a solid foundation in engineering knowledge, they struggle to improve their designs based on simulation feedback, often falling short of human capabilities. However, by incorporating reinforcement learning, a 7B parameter model surpasses both state-of-the-art models and human experts, highlighting the potential of RL-enhanced LLMs in complex engineering optimization tasks.'}, 'zh': {'title': '强化学习助力火箭设计，超越人类专家！', 'desc': '大型语言模型（LLMs）在软件工程中取得了显著进展，但在物理工程领域的应用仍然不够深入。本文通过RocketBench基准评估LLMs在高功率火箭设计中的能力，连接LLMs与高保真火箭模拟。我们测试了两个复杂的设计任务：目标高度优化和精确着陆挑战。研究发现，尽管最先进的LLMs展现出强大的工程知识，但在根据模拟结果迭代设计时表现不佳，最终未能超越人类专家的水平；然而，通过强化学习（RL）增强后，7B参数模型的表现超过了现有基础模型和人类专家。'}}}, {'id': 'https://huggingface.co/papers/2504.18983', 'title': 'MediAug: Exploring Visual Augmentation in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.18983', 'abstract': 'Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.', 'score': 5, 'issue_id': 3554, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': '13143b6fb9ae9216', 'authors': ['Xuyin Qi', 'Zeyu Zhang', 'Canxuan Gang', 'Hao Zhang', 'Lei Zhang', 'Zhiwei Zhang', 'Yang Zhao'], 'affiliations': ['AIML', 'ANU', 'La Trobe', 'PSU', 'UCAS', 'UNSW'], 'pdf_title_img': 'assets/pdf/title_img/2504.18983.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#healthcare', '#synthetic', '#training'], 'emoji': '🩺', 'ru': {'title': 'MediAug: Новый эталон аугментации данных для медицинской визуализации', 'desc': 'Эта статья представляет MediAug - комплексный фреймворк для оценки методов аугментации данных в медицинской визуализации. Авторы исследуют шесть методов аугментации на основе смешивания изображений, применяя их к задачам классификации опухолей мозга и глазных заболеваний. Эксперименты проводятся с использованием как сверточных нейронных сетей (ResNet-50), так и трансформеров (ViT-B). Результаты показывают, что разные методы аугментации дают наилучшие результаты для разных архитектур и задач, подчеркивая важность выбора подходящего метода для конкретной задачи.'}, 'en': {'title': 'Enhancing Medical Imaging with Advanced Data Augmentation', 'desc': 'This paper addresses the challenges of data augmentation in medical imaging, particularly the domain gap between natural images and medical scans. It introduces MediAug, a benchmark framework that evaluates six mix-based augmentation methods across different neural network architectures. The study finds that MixUp and SnapMix significantly enhance classification accuracy for brain tumors and eye diseases, respectively. By providing a systematic evaluation, the paper clarifies the effectiveness of various augmentation strategies in improving medical image analysis.'}, 'zh': {'title': '医学影像数据增强的新突破', 'desc': '数据增强在医学影像中对于提高分类准确性、病变检测和器官分割至关重要，尤其是在数据有限的情况下。然而，医学影像与自然照片之间的显著领域差距可能会扭曲关键的疾病特征。此外，现有的增强研究往往局限于单一任务或架构，导致先进的混合策略的优势不明确。为了解决这些问题，我们提出了一个统一的评估框架，整合了六种基于混合的数据增强方法，并在脑肿瘤MRI和眼病视网膜图像数据集上进行了评估。'}}}, {'id': 'https://huggingface.co/papers/2504.20605', 'title': 'TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models', 'url': 'https://huggingface.co/papers/2504.20605', 'abstract': 'Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.', 'score': 4, 'issue_id': 3554, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '9637fef0c8d474ed', 'authors': ['Mihai Nadas', 'Laura Diosan', 'Andrei Piscoran', 'Andreea Tomescu'], 'affiliations': ['Babes-Bolyai University', 'KlusAI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.20605.jpg', 'data': {'categories': ['#story_generation', '#ethics', '#multimodal', '#dataset', '#benchmark', '#alignment', '#open_source'], 'emoji': '📚', 'ru': {'title': 'Масштабное моральное повествование без гигантских моделей', 'desc': 'Исследователи создали TF1-EN-3M - первый открытый датасет из трех миллионов англоязычных басен, сгенерированных моделями до 8 миллиардов параметров. Каждая история следует шестиэлементной структуре, охватывая широкий тематический спектр. Оценка качества историй производится с помощью гибридного подхода, сочетающего критику на основе GPT и метрики разнообразия и читабельности. Датасет, код генерации и скрипты оценки выпущены под свободной лицензией, открывая возможности для исследований в области следования инструкциям, нарративного интеллекта и образовательного ИИ.'}, 'en': {'title': 'Empowering Moral Storytelling with TF1-EN-3M', 'desc': 'This paper introduces TF1-EN-3M, a novel dataset containing three million English fables designed to teach moral lessons, generated by smaller instruction-tuned models. The stories are structured using a six-slot framework that ensures each narrative includes essential elements like characters, traits, and morals. A unique evaluation system combines assessments from a GPT-based critic and various metrics to ensure quality and diversity in the generated fables. The dataset and associated tools are made publicly available, promoting further research in areas like narrative intelligence and ethical AI development.'}, 'zh': {'title': '道德故事生成的新纪元', 'desc': '这篇论文介绍了TF1-EN-3M，这是第一个包含三百万个英语寓言的开放数据集，专门由指令调优模型生成。每个故事遵循六个部分的结构，包括角色、特征、背景、冲突、解决方案和道德，确保了故事的连贯性和主题的多样性。论文还提出了一种混合评估方法，结合了基于GPT的评分和无参考的多样性与可读性指标。TF1-EN-3M为道德故事的生成和教育AI的研究提供了新的可能性，表明大规模的道德叙事不再需要专有的大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.20406', 'title': 'Skill Discovery for Software Scripting Automation via Offline\n  Simulations with LLMs', 'url': 'https://huggingface.co/papers/2504.20406', 'abstract': "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.", 'score': 3, 'issue_id': 3570, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '995b07a7273b51fd', 'authors': ['Paiheng Xu', 'Gang Wu', 'Xiang Chen', 'Tong Yu', 'Chang Xiao', 'Franck Dernoncourt', 'Tianyi Zhou', 'Wei Ai', 'Viswanathan Swaminathan'], 'affiliations': ['Adobe Research', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.20406.jpg', 'data': {'categories': ['#agents', '#dataset', '#alignment', '#graphs', '#games', '#architecture', '#data'], 'emoji': '🤖', 'ru': {'title': 'Автоматизация без кодирования: LLM создают скрипты для пользователей', 'desc': 'Эта статья представляет новый подход к созданию скриптов для автоматизации задач в программном обеспечении без необходимости программирования. Авторы предлагают офлайн-систему для создания набора проверенных скриптов с использованием больших языковых моделей (LLM) и общедоступных руководств по скриптингу. Система включает компоненты для создания задач и генерации навыков, а также использует модель на основе графовых нейронных сетей (GNN) для прогнозирования связей между API. Эксперименты с Adobe Illustrator показали значительное улучшение успешности автоматизации, сокращение времени отклика и экономию затрат на токены по сравнению с традиционной генерацией кода во время выполнения.'}, 'en': {'title': 'Empowering Users with Automated Scripting through AI', 'desc': 'This paper presents a framework that helps users automate tasks in software without needing deep programming skills. It uses Large Language Models (LLMs) to generate verified scripts by simulating tasks and refining them based on execution feedback. The framework includes a Graph Neural Network (GNN) to explore API synergies, which helps create a diverse set of useful scripts. Experiments show that this approach improves automation success rates and reduces costs compared to traditional methods.'}, 'zh': {'title': '利用LLM提升软件脚本自动化的创新框架', 'desc': '本文提出了一种离线模拟框架，旨在通过利用大型语言模型（LLMs）和公开的脚本指南，创建一个软件特定的技能集。该框架包括两个主要部分：任务创建和技能生成，通过执行反馈来优化和验证脚本。我们还引入了一种基于图神经网络（GNN）的链接预测模型，以捕捉API之间的协同作用，从而生成多样化的技能。实验结果表明，该框架在自动化成功率、响应时间和运行时成本方面显著优于传统的代码生成方法。'}}}, {'id': 'https://huggingface.co/papers/2505.00534', 'title': 'A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic', 'url': 'https://huggingface.co/papers/2505.00534', 'abstract': 'Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.', 'score': 2, 'issue_id': 3560, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'd411bc0f9d34adf4', 'authors': ['Muhammad Imran Zaman', 'Usama Ijaz Bajwa', 'Gulshan Saleem', 'Rana Hammad Raza'], 'affiliations': ['Department of Computer Science, COMSATS University Islamabad, Lahore Campus, Lahore, Pakistan', 'Pakistan Navy Engineering College, National University of Sciences and Technology (NUST), Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2505.00534.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#optimization', '#video', '#transfer_learning'], 'emoji': '🚗', 'ru': {'title': 'Умное отслеживание транспорта с помощью глубокого обучения', 'desc': 'Статья представляет фреймворк для отслеживания транспортных средств с использованием нескольких камер в городских условиях. Авторы применяют глубокое обучение, включая Mask R-CNN для обнаружения объектов и ResNet-152 для извлечения признаков. Система решает проблемы окклюзии, изменения освещения и теней с помощью специальных функций потерь и метрик расстояния. Фреймворк показал высокую эффективность на наборе данных AI City Challenge, достигнув IDF1 0.8289.'}, 'en': {'title': 'Revolutionizing Traffic Monitoring with Deep Learning Tracking', 'desc': 'This paper presents a deep learning framework for Multi-Object Multi-Camera Tracking (MO-MCT) aimed at improving traffic monitoring in Intelligent Transportation Systems. The framework utilizes Mask R-CNN for detecting vehicles and applies Non-Maximum Suppression (NMS) to manage overlapping detections. It incorporates transfer learning for vehicle re-identification, allowing for the tracking of vehicles across different cameras despite challenges like occlusions and varying lighting conditions. The system is evaluated on a large dataset and shows strong performance metrics, indicating its potential for real-world traffic applications.'}, 'zh': {'title': '智能交通中的高效多摄像头跟踪解决方案', 'desc': '随着网络摄像头数量的增加，视觉传感器在智能交通系统中的重要性日益增强。本文提出了一种基于深度学习的多目标多摄像头跟踪框架，旨在解决城市交通场景中手动目标跟踪和匹配的挑战。该框架利用Mask R-CNN进行目标检测，并通过非极大值抑制（NMS）选择重叠检测中的目标。通过迁移学习实现车辆的重新识别，结合适当的损失函数和距离度量，最终在AI City Challenge数据集上取得了竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02707', 'title': 'Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play', 'url': 'https://huggingface.co/papers/2505.02707', 'abstract': "A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.", 'score': 56, 'issue_id': 3604, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '1cf06df6011df348', 'authors': ['Yemin Shi', 'Yu Shu', 'Siwei Dong', 'Guangyi Liu', 'Jaward Sesay', 'Jingwen Li', 'Zhiting Hu'], 'affiliations': ['MBZUAI', 'Maitrix.org', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.02707.jpg', 'data': {'categories': ['#open_source', '#agi', '#multimodal', '#audio', '#architecture', '#agents', '#reasoning', '#multilingual', '#machine_translation'], 'emoji': '🗣️', 'ru': {'title': 'Voila: ИИ-собеседник с человеческим голосом', 'desc': 'Статья представляет Voila - семейство голосовых языковых моделей, способных вести естественный диалог с человеком в реальном времени. Модель использует сквозную архитектуру, объединяющую возможности больших языковых моделей с акустическим моделированием. Voila достигает задержки ответа всего 195 миллисекунд, что быстрее среднего времени реакции человека. Модель поддерживает более миллиона предварительно созданных голосов и может эффективно настраиваться на новые голоса по коротким аудиосэмплам.'}, 'en': {'title': 'Voila: Revolutionizing Voice AI for Natural Conversations', 'desc': 'This paper presents Voila, an advanced voice AI agent designed for seamless interaction in daily life. Voila utilizes a novel end-to-end architecture that allows for real-time, emotionally expressive conversations, achieving a response time of just 195 milliseconds. It combines large language models with sophisticated acoustic modeling to generate natural and persona-aware voice outputs, enabling users to customize voice characteristics easily. Additionally, Voila supports various voice-based applications, including automatic speech recognition and multilingual translation, and is fully open-sourced to promote further research in human-machine communication.'}, 'zh': {'title': 'Voila：实现自然情感互动的语音AI代理', 'desc': '这篇论文介绍了Voila，一个新型的语音AI代理，旨在实现与人类的自然互动。Voila采用端到端架构，支持全双工、低延迟的对话，能够捕捉语音中的情感和细微差别。它结合了大型语言模型的推理能力和强大的声学建模，允许用户通过简单的文本指令定义说话者的身份和语调。Voila还支持超过一百万种预构建的声音，并能从短至10秒的音频样本中高效定制新声音，推动人机交互的进步。'}}}, {'id': 'https://huggingface.co/papers/2505.02387', 'title': 'RM-R1: Reward Modeling as Reasoning', 'url': 'https://huggingface.co/papers/2505.02387', 'abstract': "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.", 'score': 43, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '6cac1dc82fc6bcdc', 'authors': ['Xiusi Chen', 'Gaotang Li', 'Ziqi Wang', 'Bowen Jin', 'Cheng Qian', 'Yu Wang', 'Hongru Wang', 'Yu Zhang', 'Denghui Zhang', 'Tong Zhang', 'Hanghang Tong', 'Heng Ji'], 'affiliations': ['Stevens Institute of Technology', 'Texas A&M University', 'University of California, San Diego', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02387.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#training', '#benchmark', '#rlhf', '#alignment', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Рассуждающие модели вознаграждения: новый подход к интерпретируемому обучению с подкреплением', 'desc': 'Эта статья представляет новый класс генеративных моделей вознаграждения - Reasoning Reward Models (ReasRMs), которые формулируют моделирование вознаграждения как задачу рассуждения. Авторы предлагают двухэтапный процесс обучения: дистилляция высококачественных цепочек рассуждений и обучение с подкреплением с проверяемыми вознаграждениями. Модели ReasRM улучшают результаты больших языковых моделей, генерируя цепочки рассуждений или специфические для чата рубрики. Эмпирически модели достигают высоких результатов в нескольких комплексных бенчмарках для моделей вознаграждения, превосходя более крупные модели на величину до 13.8%.'}, 'en': {'title': 'Enhancing Reward Models with Reasoning for Better Interpretability', 'desc': 'This paper discusses the importance of reward modeling in aligning large language models (LLMs) with human preferences using reinforcement learning from human feedback (RLHF). It highlights the limitations of existing reward models, which often lack interpretability and struggle to incorporate natural language critiques. The authors introduce Reasoning Reward Models (ReasRMs), which enhance interpretability by treating reward modeling as a reasoning task. Their approach includes a two-stage training process that improves performance and achieves state-of-the-art results on various benchmarks, outperforming larger models.'}, 'zh': {'title': '推理奖励模型：提升可解释性的关键', 'desc': '奖励建模在将大型语言模型（LLMs）与人类偏好对齐中至关重要，尤其是通过人类反馈的强化学习（RLHF）。现有的奖励模型（RM）往往生成不透明的标量分数或直接预测偏好的答案，缺乏可解释性。我们提出了一种新的生成奖励模型——推理奖励模型（ReasRMs），将奖励建模视为推理任务，从而显著提高了模型的可解释性和性能。通过高质量推理链的蒸馏和可验证奖励的强化学习，我们的模型在多个奖励模型基准测试中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.20752', 'title': 'Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers', 'url': 'https://huggingface.co/papers/2504.20752', 'abstract': 'Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio phi_r of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing phi_r drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.', 'score': 41, 'issue_id': 3604, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '46858ed83065e6ae', 'authors': ['Roman Abramov', 'Felix Steinbauer', 'Gjergji Kasneci'], 'affiliations': ['School of Computation, Information and Technology, Technical University of Munich, Munich, Germany', 'School of Social Sciences and Technology, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.20752.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#dataset', '#reasoning', '#synthetic', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Грокинг открывает дверь к надежному фактическому рассуждению в больших языковых моделях', 'desc': 'Статья представляет новый подход к улучшению многоступенчатого фактического рассуждения в трансформерах с помощью метода грокинга. Авторы расширяют применение грокинга на реальные данные, дополняя графы знаний синтетическими данными для достижения порога, необходимого для возникновения обобщения. Удивительно, но даже фактически неверные синтетические данные могут усилить схемы рассуждений, заставляя модель полагаться на реляционную структуру. Предложенный метод достигает точности 95-100% на бенчмарке 2WikiMultiHopQA, превосходя современные результаты.'}, 'en': {'title': 'Unlocking Reasoning in Transformers with Grokking and Data Augmentation', 'desc': "This paper explores how Transformers, a type of neural network, can improve their ability to reason through complex factual information. It introduces the concept of 'grokking' to real-world data by enhancing knowledge graphs with synthetic data, which helps the model learn logical patterns instead of just memorizing facts. Interestingly, the study finds that even incorrect synthetic data can aid in developing reasoning capabilities by emphasizing relational structures. The results show significant improvements in multi-hop reasoning tasks, achieving high accuracy and suggesting that this method can enhance the reasoning abilities of large language models."}, 'zh': {'title': '利用合成数据提升多步推理能力', 'desc': '本文探讨了变换器在多步事实推理中的不足，尤其是在真实世界知识稀缺的情况下。我们首次将grokking扩展到真实世界的事实数据，并通过增强知识图谱来解决数据集稀疏性的问题。研究发现，即使是事实不正确的合成数据也能增强模型的推理能力，而不是降低准确性。我们的实验表明，该方法在多跳推理基准测试中达到了95-100%的准确率，显著优于现有的强基线。'}}}, {'id': 'https://huggingface.co/papers/2505.02222', 'title': 'Practical Efficiency of Muon for Pretraining', 'url': 'https://huggingface.co/papers/2505.02222', 'abstract': 'We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.', 'score': 26, 'issue_id': 3608, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '3b2cca9779803131', 'authors': ['Essential AI', ':', 'Ishaan Shah', 'Anthony M. Polloreno', 'Karl Stratos', 'Philip Monk', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Ashish Tanwer', 'Darsh J Shah', 'Khoi Nguyen', 'Kurt Smith', 'Michael Callahan', 'Michael Pust', 'Mohit Parmar', 'Peter Rushton', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Somanshu Singla', 'Tim Romanski', 'Yash Vanjani', 'Ashish Vaswani'], 'affiliations': ['Essential AI, San Francisco, CA'], 'pdf_title_img': 'assets/pdf/title_img/2505.02222.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Muon: Оптимизатор второго порядка для эффективного обучения больших моделей', 'desc': 'Исследователи представляют оптимизатор Muon, который превосходит AdamW по соотношению вычислительной мощности и времени обучения. Muon эффективнее сохраняет эффективность данных при больших размерах батча, позволяя проводить более экономичное обучение. Авторы изучают комбинацию Muon и максимальной параметризации обновлений (muP) для эффективного переноса гиперпараметров. Результаты подтверждены обширными экспериментами с моделями до 4 миллиардов параметров.'}, 'en': {'title': 'Muon: A New Era of Efficient Optimization in Machine Learning', 'desc': 'This paper introduces Muon, a second-order optimizer that improves upon AdamW by expanding the Pareto frontier in terms of compute-time efficiency. Muon demonstrates superior data efficiency at large batch sizes, even exceeding the critical batch size, while maintaining computational efficiency for cost-effective training. The authors also explore the integration of Muon with the maximal update parameterization (muP) to enhance hyperparameter transfer efficiency. Their extensive experiments, involving models with up to four billion parameters, validate the effectiveness of Muon across various data distributions and architectures.'}, 'zh': {'title': 'Muon优化器：超越AdamW的高效训练', 'desc': '本文展示了Muon作为一种二阶优化器的简单实现，能够在计算时间的权衡上显著扩展Pareto前沿，相比于AdamW更具优势。研究发现，Muon在大批量训练中保持数据效率，超越了所谓的临界批量大小，同时仍然保持计算效率，从而实现更经济的训练。我们还研究了Muon与最大更新参数化（muP）的结合，以实现高效的超参数转移，并提出了一种简单的递归算法，能够考虑muP中的所有误差来源，同时仅引入适度的资源开销。通过对模型规模达到四十亿参数的广泛实验以及对数据分布和架构的消融研究，我们验证了我们的发现。'}}}, {'id': 'https://huggingface.co/papers/2505.02735', 'title': 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.02735', 'abstract': 'Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.', 'score': 20, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '1ffa5567eb2f4acc', 'authors': ['Zhouliang Yu', 'Ruotian Peng', 'Keyi Ding', 'Yizhe Li', 'Zhongyuan Peng', 'Minghao Liu', 'Yifan Zhang', 'Zheng Yuan', 'Huajian Xin', 'Wenhao Huang', 'Yandong Wen', 'Ge Zhang', 'Weiyang Liu'], 'affiliations': ['M-A-P', 'Max Planck Institute for Intelligent Systems, Tübingen', 'Numina', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02735.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'FormalMATH: Новый рубеж в формальном математическом рассуждении для ИИ', 'desc': 'FormalMATH - это масштабный бенчмарк на Lean4, содержащий 5560 формально верифицированных задач от олимпиадного до университетского уровня. Авторы представляют новый полуавтоматический конвейер для формализации, использующий специализированные языковые модели (LLM) и стратегии фильтрации. Оценка современных LLM-доказателей теорем показала их ограниченность, с максимальным успехом в 16.46% при практических ограничениях. Бенчмарк выявил обратную зависимость между наличием решения на естественном языке и успехом доказательства в сценариях рассуждений по цепочке мыслей.'}, 'en': {'title': 'FormalMATH: Advancing AI in Formal Mathematical Reasoning', 'desc': 'The paper introduces FormalMATH, a comprehensive benchmark designed to enhance formal mathematical reasoning in AI. It consists of 5,560 verified problems that cover a wide range of mathematical topics, from high school to undergraduate level. The authors propose a human-in-the-loop autoformalization pipeline that utilizes large language models (LLMs) for automating the formalization process, significantly reducing the need for expert input. The study also highlights the limitations of current LLM-based theorem provers, revealing their low success rates and biases across different mathematical domains.'}, 'zh': {'title': 'FormalMATH：推动正式数学推理的基准', 'desc': '本文提出了FormalMATH，这是一个大规模的Lean4基准，包含5560个经过正式验证的数学问题，涵盖从高中奥林匹克挑战到本科定理的多个领域。为了解决手动形式化的低效问题，我们引入了一种新的人机协作自动形式化流程，结合了专门的大型语言模型（LLMs）进行语句自动形式化、多LLM语义验证和基于否定的反驳过滤策略。我们的评估显示，当前最先进的LLM定理证明器在实际采样预算下的成功率仅为16.46%，并且在不同领域表现出明显的偏差。我们认为，FormalMATH为正式数学推理提供了一个强有力的基准。'}}}, {'id': 'https://huggingface.co/papers/2505.02819', 'title': 'ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations', 'url': 'https://huggingface.co/papers/2505.02819', 'abstract': "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.", 'score': 19, 'issue_id': 3608, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '7f169d4491ab6864', 'authors': ['Dmitriy Shopkhoev', 'Ammar Ali', 'Magauiya Zhussip', 'Valentin Malykh', 'Stamatios Lefkimmiatis', 'Nikos Komodakis', 'Sergey Zagoruyko'], 'affiliations': ['Archimedes Athena RC', 'IACM-Forth', 'IITU', 'ITMO University', 'MTS AI', 'Polynome', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2505.02819.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#inference', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Эффективная обрезка трансформеров без переобучения', 'desc': 'ReplaceMe - это метод обрезки глубины трансформеров без дополнительного обучения. Он заменяет блоки трансформера линейной операцией, сохраняя высокую производительность при низких коэффициентах сжатия. Метод требует только небольшой калибровочный набор данных для оценки линейного преобразования. ReplaceMe превосходит другие подходы без обучения и конкурирует с современными методами обрезки, требующими длительного дообучения.'}, 'en': {'title': 'Effortless Depth Pruning with ReplaceMe', 'desc': 'ReplaceMe is a novel method for depth pruning in transformer models that does not require additional training. Instead of retraining the model after pruning, it uses a small calibration dataset to create a linear transformation that approximates the pruned blocks. This allows for significant compression of the model while maintaining high performance, achieving up to 25% pruning with around 90% of the original performance. The method is efficient and can be easily integrated into existing architectures without adding extra parameters, making it a competitive alternative to traditional pruning techniques.'}, 'zh': {'title': 'ReplaceMe：高效的无训练深度剪枝方法', 'desc': '我们介绍了一种名为ReplaceMe的深度剪枝方法，它不需要训练就能有效地将变换器块替换为线性操作，同时在低压缩比下保持高性能。与传统的剪枝方法不同，ReplaceMe只需一个小的校准数据集来估计线性变换，从而近似剪枝后的块。这个估计的线性映射可以与剩余的变换器块无缝合并，避免了额外的网络参数需求。我们的实验表明，ReplaceMe在多个大型语言模型上表现优异，能够实现高达25%的剪枝，同时保留约90%的原始模型性能，且无需任何训练或调整步骤。'}}}, {'id': 'https://huggingface.co/papers/2505.02391', 'title': 'Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL', 'url': 'https://huggingface.co/papers/2505.02391', 'abstract': 'Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.', 'score': 19, 'issue_id': 3605, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '48b57ad8bd358e5d', 'authors': ['Jiarui Yao', 'Yifan Hao', 'Hanning Zhang', 'Hanze Dong', 'Wei Xiong', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02391.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Динамическое распределение ресурсов для эффективного обучения рассуждениям', 'desc': 'Эта статья представляет новый метод под названием GVM-RAFT для улучшения обучения с подкреплением в задачах рассуждений на основе цепочки мыслей (CoT) в больших языковых моделях. Авторы предлагают динамическую стратегию выделения вычислительных ресурсов, которая минимизирует дисперсию стохастического градиента. Теоретический анализ показывает, что этот подход обеспечивает ускоренную сходимость при определенных условиях. Эксперименты на задачах математических рассуждений демонстрируют 2-4-кратное ускорение и значительное повышение точности по сравнению с обычным методом RAFT.'}, 'en': {'title': 'Dynamic Resource Allocation for Enhanced Reasoning in LLMs', 'desc': 'This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains.'}, 'zh': {'title': '动态样本分配，提升推理效率！', 'desc': '本文探讨了大型语言模型中的链式推理（CoT）如何被形式化为潜变量问题，模型需要生成中间推理步骤。以往的方法如迭代奖励排名微调（RAFT）在处理不同难度的提示时，通常采用统一的推理预算，这导致了效率低下。我们提出了GVM-RAFT，这是一种针对特定提示的动态样本分配策略，旨在在计算预算限制下最小化随机梯度方差。实验结果表明，GVM-RAFT在数学推理任务中实现了2-4倍的加速和显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2505.01658', 'title': 'A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency', 'url': 'https://huggingface.co/papers/2505.01658', 'abstract': 'Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine', 'score': 19, 'issue_id': 3604, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 мая', 'en': 'May 3', 'zh': '5月3日'}, 'hash': '56f8dc116dbd737d', 'authors': ['Sihyeong Park', 'Sungryeol Jeon', 'Chaelyn Lee', 'Seokhun Jeon', 'Byung-Soo Kim', 'Jemin Lee'], 'affiliations': ['Electronics and Telecommunications Research Institute, South Korea', 'Korea Electronics Technology Institute, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01658.jpg', 'data': {'categories': ['#inference', '#open_source', '#optimization', '#survey'], 'emoji': '🚀', 'ru': {'title': 'Комплексный анализ движков вывода LLM: оптимизация, производительность и будущее', 'desc': 'Данная статья представляет собой комплексный анализ 25 движков вывода для больших языковых моделей (LLM), включая открытые и коммерческие решения. Авторы исследуют каждый движок по ряду критериев, таких как простота использования, масштабируемость и оптимизация для различных сценариев применения. В работе рассматриваются методы оптимизации, поддерживаемые каждым движком, и оценивается зрелость экосистемы открытых решений. Статья завершается обзором будущих направлений исследований в области движков вывода для LLM и предоставляет практические рекомендации для разработчиков и исследователей.'}, 'en': {'title': 'Optimizing Inference: A Deep Dive into LLM Engines', 'desc': 'This paper evaluates 25 open-source and commercial inference engines designed for large language models (LLMs). It focuses on key factors such as ease-of-use, deployment, scalability, and performance in terms of throughput and latency. The study also investigates the optimization techniques supported by each engine and assesses the maturity of the open-source ecosystem compared to commercial solutions. Finally, it outlines future research directions to enhance LLM services, hardware support, and security, providing valuable insights for researchers and developers.'}, 'zh': {'title': '优化大型语言模型推理引擎的研究与评估', 'desc': '大型语言模型（LLM）在聊天机器人、代码生成器和搜索引擎中得到了广泛应用。由于链式思维、复杂推理和代理服务等工作负载的增加，推理成本显著上升。虽然已经采用并行、压缩和缓存等优化方法来降低成本，但多样化的服务需求使得选择合适的方法变得困难。本文对25个开源和商业推理引擎进行了全面评估，探讨了它们的易用性、可部署性、通用支持、可扩展性以及适合吞吐量和延迟感知计算的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.02835', 'title': 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.02835', 'abstract': "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.", 'score': 17, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '5134d59b0389ade9', 'authors': ['Yi-Fan Zhang', 'Xingyu Lu', 'Xiao Hu', 'Chaoyou Fu', 'Bin Wen', 'Tianke Zhang', 'Changyi Liu', 'Kaiyu Jiang', 'Kaibing Chen', 'Kaiyu Tang', 'Haojie Ding', 'Jiankang Chen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Liang Wang'], 'affiliations': ['CASIA', 'KuaiShou', 'NJU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2505.02835.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#multimodal', '#rl', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Стабильное обучение с подкреплением для улучшения мультимодальных моделей вознаграждения', 'desc': 'Эта статья представляет новый подход к улучшению мультимодальных моделей вознаграждения (MRM) с использованием обучения с подкреплением (RL). Авторы предлагают алгоритм StableReinforce, который решает проблемы нестабильности, возникающие при применении существующих RL-алгоритмов к моделированию вознаграждений. Они собрали набор данных из 200 тысяч примеров предпочтений для обучения MRM. Их модель R1-Reward, обученная с помощью StableReinforce, значительно превосходит предыдущие модели на эталонных тестах мультимодального моделирования вознаграждений.'}, 'en': {'title': 'Stable Reinforcement for Enhanced Multimodal Reward Modeling', 'desc': 'This paper investigates the use of Reinforcement Learning (RL) to enhance Multimodal Reward Models (MRMs) for Multimodal Large Language Models (MLLMs). It identifies challenges in applying traditional RL algorithms to reward modeling, which can lead to instability during training. To overcome these issues, the authors introduce the StableReinforce algorithm, which improves training dynamics through better loss refinement and reward design. The results show that their reward model, R1-Reward, outperforms existing state-of-the-art models on multimodal benchmarks, demonstrating the effectiveness of their approach.'}, 'zh': {'title': '稳定强化学习提升多模态奖励模型性能', 'desc': '多模态奖励模型（MRMs）在提升多模态大型语言模型（MLLMs）的性能中起着重要作用。本文探讨了如何利用强化学习（RL）来改善奖励建模，提出将奖励建模问题重新表述为基于规则的RL任务。我们提出的StableReinforce算法通过优化训练损失、优势估计策略和奖励设计，解决了现有RL算法在奖励建模中导致的不稳定性问题。经过StableReinforce算法训练的奖励模型R1-Reward在多模态奖励建模基准上显著提高了性能，展示了RL算法在优化MRMs中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.01441', 'title': 'Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.01441', 'abstract': 'Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.', 'score': 17, 'issue_id': 3603, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': '4c0a590eccbb1960', 'authors': ['Joykirat Singh', 'Raghav Magazine', 'Yash Pandya', 'Akshay Nambi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01441.jpg', 'data': {'categories': ['#agents', '#rl', '#reasoning', '#training', '#benchmark', '#optimization', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'ARTIST: Агентный ИИ с инструментами для продвинутого решения задач', 'desc': 'Статья представляет ARTIST - фреймворк, объединяющий агентное рассуждение, обучение с подкреплением и интеграцию инструментов для больших языковых моделей (LLM). ARTIST позволяет моделям автономно решать, когда и как использовать инструменты в многоэтапных цепочках рассуждений. Эксперименты показывают, что ARTIST превосходит современные базовые модели в задачах математических рассуждений и многоэтапных вызовов функций. Результаты демонстрируют, что агентное обучение с подкреплением с интеграцией инструментов открывает новые возможности для надежного и интерпретируемого решения задач в LLM.'}, 'en': {'title': 'Empowering LLMs with Dynamic Reasoning and Tool Integration', 'desc': "This paper presents ARTIST, a new framework that enhances large language models (LLMs) by integrating agentic reasoning, reinforcement learning (RL), and tool usage. Unlike traditional LLMs that rely on static knowledge, ARTIST allows models to dynamically decide when and how to use external tools during complex reasoning tasks. The framework employs outcome-based RL to improve the model's strategies for interacting with tools and environments, enabling it to learn without needing detailed step-by-step guidance. Experimental results demonstrate that ARTIST significantly outperforms existing models, achieving better reasoning and tool utilization in challenging scenarios."}, 'zh': {'title': 'ARTIST：智能推理与工具集成的新前沿', 'desc': '大型语言模型（LLMs）在复杂推理任务中取得了显著进展，但仍然受到静态内部知识和仅基于文本推理的限制。现实世界的问题解决通常需要动态的多步骤推理、适应性决策以及与外部工具和环境的交互能力。我们提出了ARTIST（自我改进变换器中的代理推理和工具集成），这是一个将代理推理、强化学习和工具集成紧密结合的统一框架。实验结果表明，ARTIST在数学推理和多轮函数调用基准测试中表现优异，相较于基础模型有高达22%的绝对提升。'}}}, {'id': 'https://huggingface.co/papers/2505.02156', 'title': 'Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents', 'url': 'https://huggingface.co/papers/2505.02156', 'abstract': "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction rightarrow deep contemplation) based on real-time context. Our framework's core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach", 'score': 16, 'issue_id': 3602, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '13e5cf390c136aeb', 'authors': ['Minzheng Wang', 'Yongbin Li', 'Haobo Wang', 'Xinghua Zhang', 'Nan Xu', 'Bingli Wu', 'Fei Huang', 'Haiyang Yu', 'Wenji Mao'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.02156.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное обучение режимам мышления для улучшения социального интеллекта ИИ', 'desc': 'Статья представляет новый метод машинного обучения под названием Adaptive Mode Learning (AML) для улучшения социального интеллекта языковых агентов. AML использует алгоритм Adaptive Mode Policy Optimization (AMPO), который динамически выбирает один из четырех режимов мышления в зависимости от контекста. Этот подход позволяет агентам адаптивно регулировать глубину рассуждений, что приводит к более эффективному использованию токенов и улучшенной производительности на задачах социального интеллекта. Эксперименты показывают, что AML превосходит современные методы, достигая на 15.6% более высокой производительности при использовании на 32.8% более коротких цепочек рассуждений.'}, 'en': {'title': 'Dynamic Reasoning for Smarter Social Agents', 'desc': 'This paper introduces Adaptive Mode Learning (AML), a new approach for simulating social intelligence in language agents. Unlike existing methods that either lack dynamic reasoning or use a one-size-fits-all strategy, AML allows agents to switch between four different thinking modes based on the context of the interaction. The core of this framework is the Adaptive Mode Policy Optimization (AMPO) algorithm, which enhances reasoning efficiency by adapting the depth of thought to the situation. Experimental results show that AML significantly improves task performance and reduces the length of reasoning chains compared to current state-of-the-art methods.'}, 'zh': {'title': '自适应模式学习：提升社交智能的推理能力', 'desc': '本论文提出了一种新的方法，称为自适应模式学习（AML），旨在提高社交智能模拟的有效性。现有方法在推理深度上缺乏灵活性，导致在不同场景下的推理不够高效。AML通过实时上下文动态选择四种思维模式，从直觉反应到深度思考，优化了推理过程。实验结果表明，AML在社交智能任务中比现有最先进的方法提高了15.6%的性能，同时推理链长度减少了32.8%。'}}}, {'id': 'https://huggingface.co/papers/2505.02094', 'title': 'SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations', 'url': 'https://huggingface.co/papers/2505.02094', 'abstract': 'We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.', 'score': 14, 'issue_id': 3604, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '1ae5a69dab5de633', 'authors': ['Runyi Yu', 'Yinhuai Wang', 'Qihan Zhao', 'Hok Wai Tsui', 'Jingbo Wang', 'Ping Tan', 'Qifeng Chen'], 'affiliations': ['HKUST Hong Kong, China', 'Shanghai AI Laboratory Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02094.jpg', 'data': {'categories': ['#games', '#training', '#rl', '#optimization', '#data'], 'emoji': '🤖', 'ru': {'title': 'Улучшение обучения с подкреплением через аугментацию демонстраций', 'desc': 'Эта статья представляет новый подход к обучению с подкреплением на основе демонстраций взаимодействия (RLID). Авторы предлагают методы аугментации данных для преодоления проблем шума и ограниченности демонстраций: Stitched Trajectory Graph (STG) и State Transition Field (STF). Они также разрабатывают стратегию Adaptive Trajectory Sampling (ATS) для динамической генерации учебной программы и механизм исторического кодирования для обучения навыкам, зависящим от памяти. Эксперименты показывают значительное улучшение по сравнению с современными методами в плане стабильности сходимости, способности к обобщению и устойчивости восстановления.'}, 'en': {'title': 'Bridging Skills: Enhancing Reinforcement Learning with Augmented Trajectories', 'desc': 'This paper tackles the issue of noise and limited coverage in Reinforcement Learning from Interaction Demonstration (RLID). It highlights that even with noisy and sparse data, there are countless feasible trajectories that can connect different demonstrated skills. The authors introduce two innovative techniques: the Stitched Trajectory Graph (STG) for identifying transitions between skills, and the State Transition Field (STF) for linking arbitrary states. Their method, combined with Adaptive Trajectory Sampling (ATS) and a historical encoding mechanism, enhances skill learning and generalization beyond the initial demonstrations, showing significant improvements in various tasks.'}, 'zh': {'title': '克服演示噪声，实现技能的鲁棒学习', 'desc': '本文探讨了从交互演示中进行强化学习（RLID）时面临的挑战，特别是演示噪声和覆盖限制。现有的数据收集方法虽然提供了有价值的交互演示，但往往导致稀疏、断裂和噪声的轨迹，无法全面捕捉技能变化和过渡的全貌。我们提出的关键见解是，尽管演示存在噪声和稀疏性，但仍然存在无限的物理可行轨迹，可以自然地连接演示技能或从其邻近状态中产生。基于此，我们提出了两种数据增强技术，旨在提高技能获取的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.02471', 'title': 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction', 'url': 'https://huggingface.co/papers/2505.02471', 'abstract': 'We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'fa1cacd261a625e3', 'authors': ['Biao Gong', 'Cheng Zou', 'Dandan Zheng', 'Hu Yu', 'Jingdong Chen', 'Jianxin Sun', 'Junbo Zhao', 'Jun Zhou', 'Kaixiang Ji', 'Lixiang Ru', 'Libin Wang', 'Qingpei Guo', 'Rui Liu', 'Weilong Chai', 'Xinyu Xiao', 'Ziyuan Huang'], 'affiliations': ['Ant Group', 'Ming-Lite-Uni'], 'pdf_title_img': 'assets/pdf/title_img/2505.02471.jpg', 'data': {'categories': ['#agi', '#multimodal', '#open_source', '#cv'], 'emoji': '🤖', 'ru': {'title': 'Объединение зрения и языка в единой мультимодальной системе', 'desc': 'Ming-Lite-Uni - это открытая мультимодальная система, объединяющая генерацию изображений и обработку естественного языка. Она использует фиксированную мультимодальную языковую модель (MLLM) и обучаемую диффузионную модель для выполнения задач генерации изображений по тексту и редактирования изображений на основе инструкций. Система вводит новые многомасштабные обучаемые токены и стратегию выравнивания многомасштабных представлений. Экспериментальные результаты демонстрируют высокую производительность Ming-Lite-Uni и впечатляющую гибкость интерактивного процесса.'}, 'en': {'title': 'Unifying Vision and Language for Advanced AI', 'desc': "Ming-Lite-Uni is an innovative open-source framework designed to integrate vision and language through a unified visual generator and a multimodal autoregressive model. It introduces advanced techniques like multi-scale learnable tokens and representation alignment to enhance the interaction between text and images. By utilizing a fixed MLLM alongside a learnable diffusion model, it supports both text-to-image generation and image editing based on instructions. The framework's strong performance and interactive capabilities highlight its potential impact on the development of advanced AI systems, contributing to the journey towards artificial general intelligence (AGI)."}, 'zh': {'title': 'Ming-Lite-Uni：统一视觉与语言的多模态框架', 'desc': 'Ming-Lite-Uni是一个开源的多模态框架，旨在统一视觉和语言。它引入了新的视觉生成器和多模态自回归模型，支持文本到图像生成和图像编辑任务。该框架采用了多尺度可学习标记和多尺度表示对齐策略，提升了模型的表现。所有代码和模型权重都是开源的，鼓励社区进一步探索。'}}}, {'id': 'https://huggingface.co/papers/2505.02370', 'title': 'SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing', 'url': 'https://huggingface.co/papers/2505.02370', 'abstract': 'Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '5833f2e06b661a76', 'authors': ['Ming Li', 'Xin Gu', 'Fan Chen', 'Xiaoying Xing', 'Longyin Wen', 'Chen Chen', 'Sijie Zhu'], 'affiliations': ['ByteDance Intelligent Creation (USA)', 'Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2505.02370.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#training', '#cv'], 'emoji': '✏️', 'ru': {'title': 'Улучшение редактирования изображений через оптимизацию инструкций', 'desc': 'Статья представляет новый подход к улучшению моделей редактирования изображений на основе инструкций. Авторы предлагают метод создания более эффективных инструкций для редактирования, включая их исправление для лучшего соответствия парам оригинальных и отредактированных изображений. Они также вводят контрастные инструкции редактирования для повышения эффективности обучения. Результаты показывают значительное улучшение производительности по сравнению с существующими методами, достигая 9.19% улучшения на бенчмарке Real-Edit при использовании в 30 раз меньше данных для обучения и в 13 раз меньшем размере модели.'}, 'en': {'title': 'Enhancing Image Editing with Accurate Instructions and Contrastive Supervision', 'desc': 'This paper addresses the problem of noisy supervision in image editing tasks caused by mismatched editing instructions and image pairs. The authors propose a novel approach that involves creating more accurate editing instructions that align better with the original and edited images. They introduce contrastive editing instructions to enhance the effectiveness of these instructions and utilize a unified guide for vision-language models to rectify them. Their method improves supervision signals without relying on pre-training or VLM modules, achieving significant performance gains on benchmarks with less training data and smaller model sizes.'}, 'zh': {'title': '提升图像编辑效果的新方法', 'desc': '本论文提出了一种新方法，旨在改善图像编辑模型的监督信号。我们通过构建更有效的编辑指令，使其与原始和编辑后的图像对更好地对齐，并引入对比编辑指令来增强效果。研究发现，编辑模型在不同推理步骤中表现出特定的生成属性，这些属性与文本无关。我们的方法不依赖于之前的视觉语言模型或预训练任务，提供了一种更直接和高效的监督信号，显著提升了图像编辑的效果。'}}}, {'id': 'https://huggingface.co/papers/2505.01043', 'title': 'Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities', 'url': 'https://huggingface.co/papers/2505.01043', 'abstract': 'Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'b67e3ec75756e896', 'authors': ['Zhiwei Hao', 'Jianyuan Guo', 'Li Shen', 'Yong Luo', 'Han Hu', 'Guoxia Wang', 'Dianhai Yu', 'Yonggang Wen', 'Dacheng Tao'], 'affiliations': ['Baidu Inc., Beijing 100000, China', 'College of Computing and Data Science, Nanyang Technological University, 639798, Singapore', 'Computer Science, City University of Hong Kong, Hong Kong 999077, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China', 'School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China', 'School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01043.jpg', 'data': {'categories': ['#optimization', '#inference', '#survey', '#training'], 'emoji': '🔬', 'ru': {'title': 'Систематизация методов обучения нейросетей с низкой точностью для повышения эффективности', 'desc': 'Статья представляет собой обзор методов обучения нейронных сетей с низкой точностью вычислений. Авторы систематизируют существующие подходы, разделяя их на три основные категории: методы на основе фиксированной точки и целых чисел, методы на основе чисел с плавающей запятой и методы с использованием специализированных форматов. В работе также обсуждаются подходы к обучению с учетом квантования и намечаются перспективные направления исследований в этой области. Обзор направлен на предоставление исследователям единого представления о ландшафте методов обучения с низкой точностью.'}, 'en': {'title': 'Optimizing Large Language Models with Low-Precision Training', 'desc': 'This paper reviews low-precision training techniques for large language models (LLMs), which help improve training efficiency while reducing hardware resource demands. It categorizes these methods into three main groups based on numerical formats: fixed-point and integer-based, floating-point-based, and customized format-based methods. The survey also addresses quantization-aware training, which is closely related to low-precision training during model inference. By organizing the existing research, the paper aims to provide clarity and direction for future advancements in low-precision training.'}, 'zh': {'title': '低精度训练：提升效率的关键', 'desc': '大型语言模型（LLMs）在多个领域取得了显著的表现，但其训练所需的硬件资源极大，成为效率和可扩展性的障碍。为了应对这一挑战，低精度训练技术被广泛采用，显著提高了训练效率。尽管如此，低精度训练涉及多个组件，如权重、激活和梯度，每个组件可以用不同的数值格式表示，导致研究领域的碎片化。本文综述了现有的低精度训练方法，并根据数值格式将其系统地分类为三大类，以便于研究者理解和参考。'}}}, {'id': 'https://huggingface.co/papers/2505.02625', 'title': 'LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis', 'url': 'https://huggingface.co/papers/2505.02625', 'abstract': 'Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.', 'score': 7, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '2f8233e0d3083036', 'authors': ['Qingkai Fang', 'Yan Zhou', 'Shoutao Guo', 'Shaolei Zhang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02625.jpg', 'data': {'categories': ['#audio', '#small_models', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Прорыв в разговорном ИИ: компактная модель превосходит гигантов', 'desc': 'В статье представлена модель LLaMA-Omni 2 - серия речевых языковых моделей для естественного голосового взаимодействия. Модель основана на архитектуре Qwen2.5 и дополнена речевым энкодером и декодером. Несмотря на небольшой объем обучающих данных, LLaMA-Omni 2 показывает высокие результаты в задачах голосового вопросно-ответного взаимодействия. Модель превосходит предыдущие аналоги, обученные на гораздо большем объеме речевых данных.'}, 'en': {'title': 'Revolutionizing Speech Interaction with LLaMA-Omni 2', 'desc': 'This paper presents LLaMA-Omni 2, a new series of speech language models designed for real-time speech interaction. These models, ranging from 0.5B to 14B parameters, utilize a speech encoder and an autoregressive streaming speech decoder to facilitate intelligent spoken chatbots. Remarkably, LLaMA-Omni 2 achieves high performance despite being trained on only 200K multi-turn speech dialogue samples. It outperforms previous models like GLM-4-Voice, which were trained on significantly larger datasets, showcasing the efficiency of the new architecture.'}, 'zh': {'title': '实时智能语音交互的新突破', 'desc': '本论文介绍了LLaMA-Omni 2，这是一个系列的语音语言模型，参数范围从0.5亿到14亿，能够实现高质量的实时语音交互。该模型基于Qwen2.5系列，结合了语音编码器和自回归流式语音解码器。尽管仅在20万多轮语音对话样本上进行训练，LLaMA-Omni 2在多个语音问答和语音指令跟随基准测试中表现出色，超越了之前的最先进模型GLM-4-Voice。这表明，使用较少的数据也能训练出高效的智能语音聊天机器人。'}}}, {'id': 'https://huggingface.co/papers/2505.01583', 'title': 'TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action', 'url': 'https://huggingface.co/papers/2505.01583', 'abstract': 'Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.', 'score': 6, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'd29565337415d11f', 'authors': ['Jen-Hao Cheng', 'Vivian Wang', 'Huayu Wang', 'Huapeng Zhou', 'Yi-Hao Peng', 'Hou-I Liu', 'Hsiang-Wei Huang', 'Kuang-Ming Chen', 'Cheng-Yen Yang', 'Wenhao Chai', 'Yi-Ling Chen', 'Vibhav Vineet', 'Qin Cai', 'Jenq-Neng Hwang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.01583.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#video', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'TEMPURA: Улучшение понимания причинно-следственных связей в видео с помощью маскированного предсказания событий', 'desc': 'TEMPURA - это двухэтапная система обучения для улучшения понимания временных событий в видео. На первом этапе она использует маскированное предсказание событий для реконструкции пропущенных событий и генерации пошаговых причинно-следственных объяснений. Затем TEMPURA учится сегментировать видео и создавать подробные описания для каждого сегмента. Система обучается на большом наборе данных VER, содержащем 1 млн обучающих примеров и 500 тыс. видео с временными метками событий. Эксперименты показывают, что TEMPURA превосходит базовые модели в задачах временной локализации и обнаружения ключевых моментов.'}, 'en': {'title': 'TEMPURA: Enhancing Video Understanding through Causal Reasoning and Temporal Segmentation', 'desc': 'This paper introduces TEMPURA, a novel framework designed to improve how vision-language models understand the timing and relationships of events in videos. It addresses the limitations of existing methods that either lose important details by compressing video data or treat videos as continuous streams without clear boundaries. TEMPURA employs a two-stage training process that first predicts missing events to create causal explanations and then segments videos into distinct events with precise descriptions. The framework is trained on a large dataset, VER, and shows significant improvements in tasks like temporal grounding and highlight detection compared to existing models.'}, 'zh': {'title': 'TEMPURA：提升视频理解的因果推理与时间分割结合', 'desc': '本论文提出了一种名为TEMPURA的框架，用于提高视频的时间理解能力。TEMPURA通过掩蔽事件预测推理，重建缺失事件并生成逐步的因果解释，从而更好地理解视频中的事件关系。接着，它学习视频分割和密集标注，将视频分解为不重叠的事件，并提供详细的时间戳对齐描述。实验结果表明，TEMPURA在时间定位和高亮检测基准测试中优于现有的强基线模型，验证了因果推理与细粒度时间分割的结合能够提升视频理解。'}}}, {'id': 'https://huggingface.co/papers/2505.02823', 'title': 'MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing', 'url': 'https://huggingface.co/papers/2505.02823', 'abstract': 'Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.', 'score': 3, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'da9d48f7aea6c0bd', 'authors': ['Zinan Guo', 'Pengze Zhang', 'Yanze Wu', 'Chong Mou', 'Songtao Zhao', 'Qian He'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2505.02823.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#multimodal', '#data'], 'emoji': '🖼️', 'ru': {'title': 'MUSAR: Эффективная многосубъектная кастомизация на ограниченных данных', 'desc': 'MUSAR - это новая структура для многосубъектной кастомизации в машинном обучении. Она решает проблемы ограниченности данных и переплетения атрибутов между субъектами, используя обучение на диптихах и динамическую маршрутизацию внимания. MUSAR позволяет достичь высокого качества генерации изображений и согласованности субъектов, используя только данные по отдельным субъектам. Эксперименты показывают превосходство MUSAR над существующими методами по качеству изображений, согласованности субъектов и естественности взаимодействия.'}, 'en': {'title': 'MUSAR: Unlocking Multi-Subject Customization with Single-Subject Data', 'desc': 'The paper presents MUSAR, a novel framework designed to enhance multi-subject customization in machine learning. It addresses two main challenges: the scarcity of diverse multi-subject training data and the issue of attribute entanglement among different subjects. MUSAR employs debiased diptych learning to create training pairs from single-subject images, correcting biases through static attention routing and dual-branch LoRA. Additionally, it utilizes a dynamic attention routing mechanism to establish clear mappings between generated images and their corresponding subjects, leading to improved image quality and consistency across subjects.'}, 'zh': {'title': 'MUSAR：单一主题数据下的多主题定制新方法', 'desc': '当前的多主题定制方法面临两个主要挑战：获取多样化的多主题训练数据的困难，以及不同主题之间属性的纠缠。为了解决这些问题，我们提出了MUSAR框架，它能够在仅需单一主题训练数据的情况下，实现稳健的多主题定制。首先，我们引入了去偏差的双联学习，通过从单一主题图像构建双联训练对，促进多主题学习，并通过静态注意力路由和双分支LoRA主动纠正双联构建引入的分布偏差。其次，我们引入了动态注意力路由机制，适应性地建立生成图像与条件主题之间的双射映射，从而消除跨主题的纠缠。'}}}, {'id': 'https://huggingface.co/papers/2505.02005', 'title': 'Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields', 'url': 'https://huggingface.co/papers/2505.02005', 'abstract': 'Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.', 'score': 3, 'issue_id': 3611, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'b0fc91d25884f885', 'authors': ['Zhenxing Mi', 'Ping Yin', 'Xue Xiao', 'Dan Xu'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR', 'Inspur Cloud Information Technology Co, Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2505.02005.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset'], 'emoji': '🏙️', 'ru': {'title': 'Масштабируемый NeRF для эффективного моделирования больших сцен', 'desc': 'Switch-NeRF++ представляет собой новый подход к моделированию крупномасштабных сцен с использованием нейронных полей излучения (NeRF). Эта модель использует гетерогенную смесь хеш-экспертов (HMoHE) для эффективного разложения сцены и обучения специализированных NeRF-экспертов. Ключевыми элементами являются сеть гейтинга на основе хеширования для декомпозиции сцены и гетерогенные хеш-эксперты с различными диапазонами разрешения. Switch-NeRF++ демонстрирует значительное улучшение качества рендеринга и эффективности по сравнению с предыдущими методами для крупномасштабных сцен.'}, 'en': {'title': 'Efficient Scene Decomposition for Scalable NeRFs', 'desc': 'This paper presents Switch-NeRF++, a novel approach to improve the scalability and efficiency of Neural Radiance Fields (NeRF) for large-scale scenes. It introduces a Heterogeneous Mixture of Hash Experts (HMoHE) network that learns to decompose scenes into specialized components, allowing for better handling of scene heterogeneity. The framework utilizes a gating network that allocates 3D points to different NeRF experts, optimizing the learning process in an end-to-end manner. The results show significant improvements in both training and rendering speeds, achieving state-of-the-art accuracy on large-scale datasets.'}, 'zh': {'title': '高效大规模场景建模的新方法', 'desc': '本文介绍了一种名为Switch-NeRF++的网络，旨在解决大规模场景中的场景分解、建模异质性和建模效率等问题。该网络采用了异质混合哈希专家（HMoHE）架构，能够高效地学习异质分解和异质NeRF。通过一个门控网络，系统能够将3D点分配给专门的NeRF专家，从而实现端到端的学习。实验结果表明，该方法在大规模场景建模中具有优越的准确性和效率，训练速度提升8倍，渲染速度提升16倍。'}}}, {'id': 'https://huggingface.co/papers/2505.01456', 'title': 'Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation', 'url': 'https://huggingface.co/papers/2505.01456', 'abstract': 'LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.', 'score': 2, 'issue_id': 3608, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '0201cbbcc6e52005', 'authors': ['Vaidehi Patil', 'Yi-Lin Sung', 'Peter Hase', 'Jie Peng', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science University of North Carolina at Chapel Hill', 'School of Artificial Intelligence and Data Science University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01456.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#interpretability', '#security', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Забывание по требованию: новый рубеж в безопасности мультимодальных ИИ', 'desc': 'Эта статья посвящена проблеме целенаправленного забывания информации в мультимодальных языковых моделях (MLLM). Авторы представляют новый бенчмарк UnLOK-VQA для оценки методов удаления специфических знаний из MLLM. Они предлагают фреймворк атак и защиты, включающий шесть целей защиты и семь типов атак. Результаты показывают, что мультимодальные атаки эффективнее одномодальных, а наилучшая защита достигается удалением информации об ответе из внутренних состояний модели.'}, 'en': {'title': 'Enhancing Safety in Multimodal LLMs through Targeted Unlearning', 'desc': 'This paper discusses the risks associated with large language models (LLMs) that are trained on extensive datasets, particularly the unintentional acquisition of sensitive information. It highlights the challenges of multimodal LLMs, which combine text and images, making them more vulnerable to adversarial attacks that can extract private details. The authors introduce a new benchmark called UnLOK-VQA for evaluating targeted unlearning in multimodal contexts, along with a framework for assessing methods to remove specific knowledge from these models. Their findings indicate that multimodal attacks are more effective than those targeting only text or images, and that larger models tend to be more robust against such attacks, emphasizing the need for improved unlearning techniques in multimodal settings.'}, 'zh': {'title': '多模态大语言模型的敏感信息遗忘挑战', 'desc': '本论文探讨了多模态大语言模型（MLLMs）在处理敏感信息时的风险，尤其是在图像和文本结合的情况下。研究表明，攻击者可以利用多模态提示来提取这些敏感信息，因此需要有效的目标性遗忘机制。为此，作者提出了一个新的基准测试UnLOK-VQA，并建立了一个攻击与防御框架，以评估从MLLMs中删除特定多模态知识的方法。实验结果显示，多模态攻击的效果优于单一文本或图像攻击，而更大的模型在后期编辑时表现出更强的鲁棒性，表明模型规模对安全性有积极影响。'}}}, {'id': 'https://huggingface.co/papers/2505.02130', 'title': 'Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data', 'url': 'https://huggingface.co/papers/2505.02130', 'abstract': "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: https://github.com/millioniron/LLM_exploration{LLM4Exploration}", 'score': 1, 'issue_id': 3616, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'a07adea642e1877d', 'authors': ['Zhong Guan', 'Likang Wu', 'Hongke Zhao', 'Ming He', 'Jianpin Fan'], 'affiliations': ['AI Lab at Lenovo', 'College of Management and Economics, Tianjin University, Tianjin, China', 'Laboratory of Computation and Analytics of Complex Management Systems, Tianjin University, Tianjin, China', 'ai-deepcube'], 'pdf_title_img': 'assets/pdf/title_img/2505.02130.jpg', 'data': {'categories': ['#graphs', '#training', '#architecture', '#interpretability'], 'emoji': '🕸️', 'ru': {'title': 'Преодоление ограничений внимания в LLM для графовых данных', 'desc': 'Это исследование изучает, как модели большого языка (LLM) обрабатывают графовые данные с точки зрения механизмов внимания. Авторы обнаружили, что LLM могут распознавать графовые структуры, но испытывают трудности с моделированием отношений между узлами. Распределение внимания LLM по узлам графа не соответствует идеальным структурным паттернам. Исследователи предлагают использовать промежуточные окна внимания для улучшения обучения LLM на графовых данных.'}, 'en': {'title': 'Unlocking LLMs: Enhancing Graph Understanding with Attention', 'desc': 'This paper investigates how large language models (LLMs) utilize attention mechanisms when processing graph-structured data. It highlights that while LLMs can identify interactions between text and graph nodes, they struggle with understanding the relationships between nodes due to their architectural limitations. The study reveals that the attention distribution in LLMs does not effectively reflect the underlying graph structure, leading to suboptimal performance. To address these issues, the authors propose using intermediate-state attention windows to enhance training and improve the transition to fully connected attention during inference.'}, 'zh': {'title': '探索注意力机制在图数据中的表现', 'desc': '本文探讨了注意力机制在大型语言模型（LLMs）处理图结构数据时的表现。研究发现，尽管LLMs能够识别图数据并捕捉文本与节点之间的交互，但在建模节点间关系时存在困难。注意力分布未能与理想的结构模式对齐，显示出对图拓扑的适应性不足。通过引入中间状态的注意力窗口，研究表明可以提高LLMs的训练性能，并在推理时无缝过渡到完全连接的窗口。'}}}, {'id': 'https://huggingface.co/papers/2505.01548', 'title': 'Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional\n  Motion-enhanced Event Representation', 'url': 'https://huggingface.co/papers/2505.01548', 'abstract': 'Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.', 'score': 1, 'issue_id': 3619, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': '07a36a1d377e5cc7', 'authors': ['Zhen Yao', 'Xiaowen Ying', 'Mooi Choo Chuah'], 'affiliations': ['Lehigh University', 'Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01548.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': '🎥', 'ru': {'title': 'Улучшение слияния RGB и событийных данных для семантической сегментации', 'desc': 'Данная статья представляет новый подход к объединению данных с RGB-камер и событийных камер для задач компьютерного зрения. Авторы предлагают новое представление событий - Motion-enhanced Event Tensor (MET), которое преобразует разреженные воксели событий в плотную и темпорально согласованную форму. Также вводятся модули BFAM и TFM для устранения пространственно-временного и модального рассогласования между RGB и событийными данными. Эксперименты показывают значительное улучшение результатов семантической сегментации по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing RGB-Event Fusion with Motion-Enhanced Event Tensor', 'desc': 'This paper addresses the challenges of combining RGB images with event camera data in computer vision. It identifies three main types of misalignments: temporal, spatial, and modal, which hinder effective integration. To overcome these issues, the authors introduce the Motion-enhanced Event Tensor (MET), which creates a dense and coherent representation of events. Additionally, they propose two modules, BFAM and TFM, to further align the data and improve performance in semantic segmentation tasks, achieving superior results on benchmark datasets.'}, 'zh': {'title': '运动增强事件张量：解决RGB-事件融合挑战的创新方法', 'desc': '本文提出了一种新颖的事件表示方法，称为运动增强事件张量（MET），旨在解决RGB-事件融合中的三种内在不对齐问题：时间、空间和模态不对齐。MET通过利用密集光流和事件时间特征，将稀疏事件体素转化为密集且时间一致的形式，从而克服了现有方法的局限性。我们还引入了频率感知双向流聚合模块（BFAM）和时间融合模块（TFM），以解决模态和时空不对齐问题。实验结果表明，我们的框架在两个大规模数据集上显著优于现有的RGB-事件语义分割方法。'}}}, {'id': 'https://huggingface.co/papers/2505.16938', 'title': 'NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification', 'url': 'https://huggingface.co/papers/2505.16938', 'abstract': 'Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.', 'score': 104, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '39a42fc40deae7a6', 'authors': ['NovelSeek Team', 'Bo Zhang', 'Shiyang Feng', 'Xiangchao Yan', 'Jiakang Yuan', 'Zhiyin Yu', 'Xiaohan He', 'Songtao Huang', 'Shaowei Hou', 'Zheng Nie', 'Zhilong Wang', 'Jinyao Liu', 'Runmin Ma', 'Tianshuo Peng', 'Peng Ye', 'Dongzhan Zhou', 'Shufei Zhang', 'Xiaosong Wang', 'Yilan Zhang', 'Meng Li', 'Zhongying Tu', 'Xiangyu Yue', 'Wangli Ouyang', 'Bowen Zhou', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.16938.jpg', 'data': {'categories': ['#healthcare', '#multimodal', '#agents', '#science'], 'emoji': '🧬', 'ru': {'title': 'NovelSeek: ИИ-ускоритель научных открытий', 'desc': 'NovelSeek - это унифицированная мультиагентная система для автономных научных исследований в различных областях. Она обладает масштабируемостью, продемонстрировав эффективность в 12 научных задачах, и обеспечивает интерактивность, позволяя интегрировать экспертные знания. NovelSeek значительно повышает эффективность исследований, достигая существенных улучшений за короткое время. Например, в задаче предсказания выхода реакции точность выросла с 27.6% до 35.4% всего за 12 часов работы системы.'}, 'en': {'title': 'Revolutionizing Research with Autonomous AI Frameworks', 'desc': 'This paper presents NovelSeek, a multi-agent framework designed for Autonomous Scientific Research (ASR) that enhances the efficiency and innovation in scientific studies. It showcases three main benefits: scalability across various research tasks, interactivity with human experts for feedback, and improved efficiency in achieving results faster than traditional methods. NovelSeek has been tested on multiple scientific tasks, demonstrating significant performance improvements in areas like reaction yield prediction and semantic segmentation. By integrating AI with human expertise, NovelSeek aims to revolutionize how scientific research is conducted.'}, 'zh': {'title': 'NovelSeek：加速科学研究的智能框架', 'desc': '本文介绍了一种名为NovelSeek的统一闭环多智能体框架，旨在实现自主科学研究（ASR）。该框架在多个科学研究领域中展现出卓越的可扩展性、交互性和效率，能够以空前的速度和精度解决复杂问题。NovelSeek通过与人类专家的反馈和多智能体的互动，促进了领域专家知识的无缝整合。研究表明，NovelSeek在反应产率预测、增强子活性预测和2D语义分割等任务中，显著提高了性能，节省了大量时间。'}}}, {'id': 'https://huggingface.co/papers/2505.14810', 'title': 'Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models', 'url': 'https://huggingface.co/papers/2505.14810', 'abstract': 'An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.', 'score': 54, 'issue_id': 3914, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '97ed7c1fde734d7e', 'authors': ['Tingchen Fu', 'Jiawei Gu', 'Yafu Li', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.14810.jpg', 'data': {'categories': ['#math', '#reasoning', '#training', '#benchmark', '#optimization', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Баланс между разумом и послушанием в ИИ', 'desc': 'Исследование MathIF выявляет противоречие между улучшением способности к рассуждению и сохранением следования инструкциям в больших языковых моделях (LLM). Анализ показывает, что модели, настроенные на длинные цепочки рассуждений или обученные с помощью обучения с подкреплением, часто ухудшают способность следовать указаниям пользователя. Простые вмешательства могут частично восстановить послушание модели, но за счет снижения производительности рассуждений. Эти выводы подчеркивают фундаментальное противоречие в текущих парадигмах обучения LLM и мотивируют необходимость создания моделей, более осознанно следующих инструкциям при рассуждениях.'}, 'en': {'title': 'Balancing Reasoning and Instruction in Language Models', 'desc': "This paper presents MathIF, a benchmark designed to evaluate how well large language models (LLMs) follow instructions while solving mathematical problems. The authors find a conflict between improving reasoning abilities and maintaining adherence to user instructions, as models that excel in reasoning often fail to follow directives accurately. They observe that training methods like reinforcement learning can enhance reasoning but may reduce the model's ability to comply with instructions, especially as the complexity of tasks increases. The study suggests that addressing this tension is crucial for developing more effective instruction-aware reasoning models."}, 'zh': {'title': '平衡推理能力与指令遵循的挑战', 'desc': '本研究分析了大型语言模型在数学推理任务中的指令遵循能力与推理能力之间的矛盾。我们提出了MathIF基准，用于评估模型在数学推理中的指令遵循表现。研究发现，推理能力更强的模型往往在遵循用户指令时表现不佳，尤其是在生成内容较长时。我们的结果表明，当前的训练方法需要更多关注指令意识，以平衡推理能力和指令遵循。'}}}, {'id': 'https://huggingface.co/papers/2505.16410', 'title': 'Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.16410', 'abstract': 'Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.', 'score': 47, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'acbe5c0b965cb7af', 'authors': ['Guanting Dong', 'Yifei Chen', 'Xiaoxi Li', 'Jiajie Jin', 'Hongjin Qian', 'Yutao Zhu', 'Hangyu Mao', 'Guorui Zhou', 'Zhicheng Dou', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'Kuaishou Technology', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16410.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#rl', '#optimization'], 'emoji': '🛠️', 'ru': {'title': 'Tool-Star: Автономное мультиинструментальное рассуждение для больших языковых моделей', 'desc': 'Tool-Star - это фреймворк на основе обучения с подкреплением, который позволяет большим языковым моделям автономно использовать несколько инструментов для пошагового рассуждения. Он включает в себя шесть типов инструментов и систематические подходы к синтезу данных и обучению. Фреймворк использует конвейер синтеза данных для автоматической генерации траекторий использования инструментов, а также двухэтапный процесс обучения для улучшения совместного рассуждения с помощью нескольких инструментов. Экспериментальные анализы на более чем 10 сложных тестах показывают эффективность Tool-Star.'}, 'en': {'title': 'Empowering LLMs with Multi-Tool Collaborative Reasoning', 'desc': "Tool-Star is a reinforcement learning (RL) framework that enhances large language models (LLMs) by enabling them to autonomously utilize multiple external tools for stepwise reasoning. It addresses the challenge of effective multi-tool collaboration by integrating a systematic approach to data synthesis and hierarchical reward design. The framework includes a novel data synthesis pipeline that generates tool-use trajectories and organizes them by difficulty, ensuring high-quality training data. Tool-Star's two-stage training process improves LLMs' reasoning capabilities through fine-tuning and a self-critic RL algorithm, leading to significant performance gains on various reasoning tasks."}, 'zh': {'title': 'Tool-Star：赋能LLM的多工具协作推理', 'desc': 'Tool-Star是一个基于强化学习的框架，旨在使大型语言模型（LLMs）能够自主使用多个工具进行逐步推理。该框架整合了六种工具，并在数据合成和训练方面进行了系统设计，以解决工具使用数据稀缺的问题。通过结合工具集成提示和基于提示的采样，Tool-Star能够自动生成工具使用轨迹，并通过质量标准化和难度感知分类来过滤低质量样本。最后，Tool-Star采用两阶段训练框架，增强多工具协作推理能力，提升了模型的推理效果和效率。'}}}, {'id': 'https://huggingface.co/papers/2505.15966', 'title': 'Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.15966', 'abstract': "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on TallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.", 'score': 43, 'issue_id': 3915, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'a1134b1247c14719', 'authors': ['Alex Su', 'Haozhe Wang', 'Weimin Ren', 'Fangzhen Lin', 'Wenhu Chen'], 'affiliations': ['HKUST', 'USTC', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15966.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#cv', '#rl', '#optimization', '#benchmark', '#open_source', '#training'], 'emoji': '🔍', 'ru': {'title': 'Рассуждения в пикселях: новый уровень понимания изображений для ИИ', 'desc': 'Статья представляет новый подход к улучшению работы моделей компьютерного зрения и обработки естественного языка (VLM) путем внедрения рассуждений в пиксельном пространстве. Авторы предлагают использовать визуальные операции, такие как увеличение и выбор кадра, чтобы модели могли напрямую анализировать визуальные данные. Для обучения этим навыкам применяется двухфазный подход: инструктивная настройка и обучение с подкреплением. Результаты показывают значительное улучшение производительности модели на различных визуальных тестах, превосходя существующие открытые модели.'}, 'en': {'title': 'Enhancing VLMs with Pixel-Space Reasoning', 'desc': 'This paper introduces a new way for Vision-Language Models (VLMs) to reason about images by using visual operations like zooming in and selecting frames. Traditionally, reasoning in these models has been limited to text, which makes it hard for them to handle visual tasks effectively. The authors propose a two-phase training method that first teaches the model to use these new visual operations and then encourages it to explore both visual and textual reasoning through reinforcement learning. Their approach leads to significant improvements in VLM performance on various visual reasoning tasks, achieving record accuracy on several benchmarks.'}, 'zh': {'title': '像素空间推理提升视觉语言模型的表现', 'desc': '本文提出了一种在视觉语言模型（VLMs）中引入像素空间推理的新方法，通过视觉操作如放大和选择帧来提升其在视觉任务上的表现。传统的大型语言模型（LLMs）在文本空间的推理能力显著提高，但在视觉密集任务中效果有限。我们通过两阶段的训练方法解决了模型在像素空间推理中的挑战，首先通过指令调优让模型熟悉新视觉操作，然后利用强化学习平衡像素空间和文本空间的推理。实验结果表明，我们的模型在多个视觉推理基准上取得了显著的性能提升，展示了像素空间推理的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.16707', 'title': 'KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models', 'url': 'https://huggingface.co/papers/2505.16707', 'abstract': 'Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a diagnostic benchmark designed to assess models through a cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose a comprehensive protocol that incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems.', 'score': 38, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'fa1902dc37796b16', 'authors': ['Yongliang Wu', 'Zonghui Li', 'Xinting Hu', 'Xinyu Ye', 'Xianfang Zeng', 'Gang Yu', 'Wenbo Zhu', 'Bernt Schiele', 'Ming-Hsuan Yang', 'Xu Yang'], 'affiliations': ['Max Planck Institute for Informatics', 'Shanghai Jiao Tong University', 'Southeast University', 'StepFun', 'University of California, Berkeley', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2505.16707.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'KRIS-Bench: когнитивная оценка моделей редактирования изображений', 'desc': 'Статья представляет KRIS-Bench - диагностический бенчмарк для оценки моделей редактирования изображений с точки зрения когнитивных способностей. Бенчмарк классифицирует задачи редактирования по трем типам знаний: фактическим, концептуальным и процедурным. Авторы разработали 22 репрезентативные задачи по 7 аспектам рассуждений и собрали 1267 аннотированных примеров. Предложен комплексный протокол оценки, включающий новую метрику правдоподобности знаний.'}, 'en': {'title': 'Advancing Image Editing with Knowledge-Based Reasoning', 'desc': "This paper presents KRIS-Bench, a new benchmark for evaluating multi-modal generative models in the context of instruction-based image editing. It focuses on assessing the models' ability to perform knowledge-based reasoning tasks, which has not been thoroughly investigated before. The benchmark categorizes editing tasks into three knowledge types: Factual, Conceptual, and Procedural, and includes 22 tasks with 1,267 annotated instances. The study reveals that current state-of-the-art models struggle with reasoning tasks, indicating a need for more knowledge-centric evaluation methods in image editing systems."}, 'zh': {'title': '知识驱动的图像编辑评估新基准', 'desc': '本文介绍了KRIS-Bench（知识基础推理在图像编辑系统基准），这是一个旨在评估多模态生成模型在知识推理编辑任务中的能力的基准测试。KRIS-Bench根据教育理论将编辑任务分为三种基础知识类型：事实性、概念性和程序性，并设计了22个代表性任务。我们提出了一种综合评估协议，包含新的知识合理性指标，并通过人类研究进行校准。实证结果显示，当前最先进的模型在推理性能上存在显著差距，强调了以知识为中心的基准测试在智能图像编辑系统发展中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.16175', 'title': 'QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design', 'url': 'https://huggingface.co/papers/2505.16175', 'abstract': 'QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.', 'score': 34, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '0bcb5c833bad2340', 'authors': ['Benjamin Schneider', 'Dongfu Jiang', 'Chao Du', 'Tianyu Pang', 'Wenhu Chen'], 'affiliations': ['University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.16175.jpg', 'data': {'categories': ['#inference', '#optimization', '#video', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'Ускорение анализа длинных видео с помощью QuickVideo', 'desc': 'QuickVideo - это система, ускоряющая понимание длинных видео для приложений реального времени. Она использует параллельный декодер видео, эффективное по памяти предзаполнение и перекрытие декодирования с выводом. QuickVideo включает QuickDecoder для параллельной обработки видео на CPU, QuickPrefill для оптимизации памяти GPU и схему перекрытия CPU-декодирования с GPU-выводом. Эксперименты показывают, что система обобщается на разные длительности и частоты дискретизации видео.'}, 'en': {'title': 'Accelerating Long-Video Understanding for Real-Time Applications', 'desc': 'QuickVideo is a novel system designed to enhance the understanding of long videos in real-time applications. It addresses two major challenges: the slow sequential video decoding and the high memory requirements for token prefilling in large language models (LLMs). By introducing a parallelized video decoder, a memory-efficient prefilling method, and an overlapping decoding scheme, QuickVideo significantly reduces inference time. This allows for efficient processing of long videos, making advanced video analysis accessible even on limited hardware.'}, 'zh': {'title': 'QuickVideo：实时长视频理解的加速利器', 'desc': 'QuickVideo 是一种加速长视频理解的系统，结合了并行视频解码、内存高效的预填充和重叠解码与推理。它通过快速解码器将视频分割成关键帧对齐的间隔，并同时处理，从而实现了 2-3 倍的速度提升。QuickPrefill 方法通过 KV-cache 剪枝减少了对 GPU 内存的需求，使得可以处理更多帧。该系统显著降低了长视频输入的推理时间，使得在有限硬件上也能实现高质量的视频理解。'}}}, {'id': 'https://huggingface.co/papers/2505.15270', 'title': 'Scaling Diffusion Transformers Efficiently via μP', 'url': 'https://huggingface.co/papers/2505.15270', 'abstract': 'Maximal Update Parametrization (μP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization (muP) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether muP of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard muP to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that muP of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing muP methodologies. Leveraging this result, we systematically demonstrate that DiT-muP enjoys robust HP transferability. Notably, DiT-XL-2-muP with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of muP on text-to-image generation by scaling PixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under muP outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-alpha and 3% of consumption by human experts for MMDiT-18B. These results establish muP as a principled and efficient framework for scaling diffusion Transformers.', 'score': 28, 'issue_id': 3914, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '6c13e0c5ef8ee4a2', 'authors': ['Chenyu Zheng', 'Xinyu Zhang', 'Rongzhen Wang', 'Wei Huang', 'Zhi Tian', 'Weilin Huang', 'Jun Zhu', 'Chongxuan Li'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'ByteDance Seed', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of AI, Renmin University of China', 'RIKEN AIP', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15270.jpg', 'data': {'categories': ['#architecture', '#training', '#transfer_learning', '#diffusion', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'μP: Эффективное масштабирование диффузионных трансформеров', 'desc': 'Статья описывает расширение метода Maximal Update Parametrization (μP) для диффузионных трансформеров. Авторы доказывают, что μP может быть применен к различным архитектурам диффузионных трансформеров, таким как DiT, U-ViT, PixArt-alpha и MMDiT. Эксперименты показывают, что использование μP позволяет эффективно переносить гиперпараметры между моделями разного размера и значительно сокращает затраты на их настройку. Результаты демонстрируют, что μP является эффективным подходом для масштабирования диффузионных трансформеров в задачах генерации изображений.'}, 'en': {'title': 'Efficient Hyperparameter Transfer for Diffusion Transformers with μP', 'desc': 'This paper extends the Maximal Update Parametrization (μP) technique to diffusion Transformers, which are crucial for generative vision models. The authors demonstrate that μP allows for effective hyperparameter transfer from smaller to larger models, significantly reducing the costs associated with hyperparameter tuning. Through extensive experiments, they show that diffusion Transformers like DiT and PixArt-alpha benefit from μP, achieving faster convergence and better performance with minimal tuning effort. Overall, this work establishes μP as a valuable method for enhancing the scalability and efficiency of diffusion Transformers in various tasks.'}, 'zh': {'title': '最大更新参数化：扩散变换器的高效调优新方法', 'desc': '本文扩展了最大更新参数化（μP）到扩散变换器，展示了高效的超参数可转移性和降低的调优成本。扩散变换器在视觉生成模型中发挥了基础作用，但在大规模应用中超参数调优的高成本限制了其可扩展性。研究表明，μP可以有效地从小型模型转移到大型扩散变换器，并在大规模实验中验证了其有效性。通过系统性实验，结果显示在调优成本较低的情况下，使用μP的模型在性能上优于基线模型。'}}}, {'id': 'https://huggingface.co/papers/2505.17022', 'title': 'GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17022', 'abstract': 'GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.', 'score': 24, 'issue_id': 3917, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'c3983abb24fa0ce4', 'authors': ['Chengqi Duan', 'Rongyao Fang', 'Yuqing Wang', 'Kun Wang', 'Linjiang Huang', 'Xingyu Zeng', 'Hongsheng Li', 'Xihui Liu'], 'affiliations': ['Beihang University', 'CUHK MMLab', 'HKU MMLab', 'Sensetime'], 'pdf_title_img': 'assets/pdf/title_img/2505.17022.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#rl', '#multimodal', '#open_source', '#cv', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Умное рассуждение для умной генерации изображений', 'desc': 'GoT-R1 - это фреймворк, использующий обучение с подкреплением для улучшения семантико-пространственных рассуждений в генерации изображений. Он основан на подходе Generation Chain-of-Thought и позволяет моделям самостоятельно находить эффективные стратегии рассуждений. GoT-R1 использует двухэтапную многомерную систему вознаграждений, оценивающую семантическое соответствие, пространственную точность и визуальное качество. Экспериментальные результаты показывают значительные улучшения в сложных композиционных задачах генерации изображений.'}, 'en': {'title': 'Reinforcing Reasoning for Better Visual Generation', 'desc': 'GoT-R1 is a new framework that improves visual generation by using reinforcement learning to enhance how models understand and create images based on complex text prompts. It focuses on semantic-spatial reasoning, which means it helps models better grasp the meaning of words and how objects should be arranged in space. The framework uses a dual-stage reward system to evaluate both the reasoning process and the final image quality, ensuring that the generated images are accurate and visually appealing. Experimental results show that GoT-R1 outperforms existing models, especially in tasks that require detailed understanding of object relationships and attributes.'}, 'zh': {'title': 'GoT-R1：通过强化学习提升视觉生成能力', 'desc': 'GoT-R1 是一个通过强化学习增强语义-空间推理的视觉生成框架。它能够更好地处理复杂的文本提示，尤其是涉及多个对象及其精确空间关系的任务。该框架采用双阶段多维奖励机制，利用大规模语言模型（MLLMs）来评估推理过程和最终输出。实验结果表明，GoT-R1 在 T2I-CompBench 基准测试中显著超越了现有模型，特别是在组合任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2505.16933', 'title': 'LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2505.16933', 'abstract': 'A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.', 'score': 23, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '9ae7746da2ef9a2b', 'authors': ['Zebin You', 'Shen Nie', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Zhiwu Lu', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16933.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Диффузионная мультимодальная ИИ-модель превосходит аналоги в понимании текста и изображений', 'desc': 'В статье представлена модель LLaDA-V - мультимодальная большая языковая модель на основе диффузии, интегрирующая визуальное обучение с инструкциями. LLaDA-V демонстрирует конкурентоспособные результаты в мультимодальных задачах, несмотря на более слабую языковую модель по сравнению с аналогами. Модель достигает высоких показателей в мультимодальном понимании по сравнению с существующими гибридными и чисто диффузионными моделями. Результаты исследования указывают на перспективность использования диффузионных языковых моделей в мультимодальных контекстах.'}, 'en': {'title': 'LLaDA-V: Bridging Text and Vision with Diffusion Power!', 'desc': 'LLaDA-V is a new type of Multimodal Large Language Model that uses a diffusion-based approach combined with visual instruction tuning. This model integrates visual features into the language processing space, allowing it to understand and generate responses that involve both text and images. Despite being less effective on purely text tasks compared to some existing models, LLaDA-V performs well in multimodal scenarios and shows strong scalability with data. The results indicate that diffusion models can be highly effective for tasks that require understanding multiple types of information, paving the way for future research in this area.'}, 'zh': {'title': '扩散模型引领多模态理解新潮流', 'desc': '本文介绍了一种基于扩散的多模态大语言模型LLaDA-V，该模型结合了视觉指令调优，能够在多模态任务中表现出色。LLaDA-V采用了掩蔽扩散模型，突破了当前多模态方法中主流的自回归范式。尽管在纯文本任务上表现不如一些现有模型，但在多模态任务中，LLaDA-V与其他模型相比具有更好的数据可扩展性和竞争力。研究结果表明，基于扩散的大语言模型在多模态理解方面具有很大的潜力，值得进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2505.16925', 'title': 'Risk-Averse Reinforcement Learning with Itakura-Saito Loss', 'url': 'https://huggingface.co/papers/2505.16925', 'abstract': 'Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives.', 'score': 21, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '1233ea3eef980e02', 'authors': ['Igor Udovichenko', 'Olivier Croissant', 'Anita Toleutaeva', 'Evgeny Burnaev', 'Alexander Korotin'], 'affiliations': ['Artificial Intelligence Research Institute', 'Natixis Foundation', 'Skolkovo Institute of Science and Technology', 'Vega Institute Foundation'], 'pdf_title_img': 'assets/pdf/title_img/2505.16925.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#math', '#rl', '#training', '#games'], 'emoji': '📊', 'ru': {'title': 'Стабильное обучение с подкреплением для минимизации рисков', 'desc': 'Статья посвящена риск-осторожному обучению с подкреплением, которое находит применение в критически важных областях. Авторы фокусируются на случае экспоненциальной функции полезности и выводят соответствующие уравнения Беллмана. Для решения проблемы численной нестабильности предлагается новая функция потерь на основе расхождения Итакуры-Саито. Экспериментальные результаты на финансовых сценариях показывают превосходство предложенного метода над существующими альтернативами.'}, 'en': {'title': 'Minimizing Risk in Reinforcement Learning with Stability', 'desc': 'This paper discusses risk-averse reinforcement learning, which is important in high-stakes situations where minimizing risk is crucial. Unlike traditional reinforcement learning that focuses on maximizing expected returns, risk-averse agents prioritize safer policies, sometimes at the cost of expected value. The authors specifically examine the exponential utility function to derive Bellman equations and adapt reinforcement learning algorithms accordingly. They propose a new loss function based on the Itakura-Saito divergence to improve numerical stability during training, demonstrating its effectiveness through theoretical analysis and empirical tests in financial scenarios.'}, 'zh': {'title': '风险厌恶强化学习的新方法', 'desc': '风险厌恶强化学习在高风险领域有广泛应用。与传统的强化学习不同，风险厌恶代理选择的策略是最小化风险，可能会牺牲预期收益。我们专注于指数效用函数的特定情况，推导出贝尔曼方程，并在此基础上对强化学习算法进行少量修改。然而，这些方法在计算过程中由于需要进行指数运算而导致数值不稳定，因此我们提出了一种基于Itakura-Saito散度的数值稳定且数学上合理的损失函数，用于学习状态值和动作值函数。'}}}, {'id': 'https://huggingface.co/papers/2505.16181', 'title': 'Understanding Generative AI Capabilities in Everyday Image Editing Tasks', 'url': 'https://huggingface.co/papers/2505.16181', 'abstract': 'Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io', 'score': 21, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '1d67f723fb5261aa', 'authors': ['Mohammad Reza Taesiri', 'Brandon Collins', 'Logan Bolton', 'Viet Dac Lai', 'Franck Dernoncourt', 'Trung Bui', 'Anh Totti Nguyen'], 'affiliations': ['Adobe Research', 'Auburn University', 'University of Alberta'], 'pdf_title_img': 'assets/pdf/title_img/2505.16181.jpg', 'data': {'categories': ['#multimodal', '#games', '#dataset', '#cv', '#optimization', '#interpretability'], 'emoji': '🖼️', 'ru': {'title': 'ИИ в фоторедактировании: креативность vs точность', 'desc': 'Анализ 83 тысяч запросов на редактирование изображений показывает, что ИИ-редакторы, включая GPT-4o, испытывают трудности с задачами низкой креативности и точным редактированием, но лучше справляются с открытыми задачами. Только 33% запросов могут быть успешно выполнены лучшими ИИ-редакторами по оценкам людей. ИИ часто не сохраняет идентичность людей и животных, а также делает незапрошенные улучшения. Интересно, что оценки моделей компьютерного зрения отличаются от оценок людей и могут отдавать предпочтение редактированию ИИ.'}, 'en': {'title': 'AI Editors: Better at Creativity, Struggling with Precision', 'desc': 'This paper analyzes 83,000 image editing requests to evaluate the performance of AI editors like GPT-4o. It finds that AI struggles with low-creativity tasks that require precise edits, achieving success in only about 33% of requests. In contrast, AI performs better on open-ended editing tasks, but often fails to maintain the identity of subjects and makes unwanted changes. Additionally, the preferences of visual language model (VLM) judges differ from human judges, with VLM judges showing a greater inclination towards AI-generated edits.'}, 'zh': {'title': 'AI编辑器在创造性任务中的优势与挑战', 'desc': '本研究分析了83000个图像编辑请求，发现AI编辑器（如GPT-4o）在低创造性任务和精确编辑方面表现不佳，但在开放性任务中表现更好。人类评审和视觉语言模型（VLM）评审在对AI与人类编辑的偏好上存在差异。大约只有33%的请求能够被最好的AI编辑器满足，尤其是在需要精确编辑的情况下，AI编辑器常常无法保持人物和动物的身份。通过对这些请求的分析，我们可以为改进AI编辑器提供重要的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.16400', 'title': 'AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.16400', 'abstract': "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.", 'score': 20, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '5a3a63b67bb5b62a', 'authors': ['Yang Chen', 'Zhuolin Yang', 'Zihan Liu', 'Chankyu Lee', 'Peng Xu', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Wei Ping'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.16400.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением раскрывает скрытый потенциал малых моделей', 'desc': 'Исследование показывает, что крупномасштабное обучение с подкреплением (RL) значительно улучшает способности к рассуждению у небольших и средних моделей машинного обучения. Этот метод превосходит дистилляцию знаний на математических и кодовых бенчмарках. Авторы предлагают эффективный подход: сначала обучение на математических задачах, затем на задачах программирования. Ключевые выводы включают важность курирования данных и curriculum learning с постепенным увеличением длины ответов.'}, 'en': {'title': 'Boosting Reasoning with Large-Scale Reinforcement Learning', 'desc': 'This paper explores how large-scale reinforcement learning (RL) can improve the reasoning abilities of small and mid-sized models more effectively than traditional distillation methods. The authors present a systematic study of the RL training process, highlighting a two-phase approach that first focuses on math prompts and then on code prompts. They demonstrate significant performance gains on both math and code benchmarks, showing that RL enhances the foundational reasoning capabilities of models while also enabling them to tackle previously unsolvable problems. Key insights include the importance of data curation and curriculum learning in optimizing the RL training process.'}, 'zh': {'title': '大规模强化学习提升推理能力的有效性', 'desc': '本研究表明，大规模强化学习（RL）在提升小型和中型模型的推理能力方面，比蒸馏方法更为有效。我们通过系统的实验研究，提出了一种简单有效的训练方法：先在数学提示上训练，再在代码提示上训练。结果显示，数学强化学习显著提高了模型在数学基准测试和代码推理任务上的表现。我们还开发了一个强大的数据整理流程，以收集具有挑战性的提示和高质量的答案，从而支持跨领域的验证基础强化学习。'}}}, {'id': 'https://huggingface.co/papers/2505.14604', 'title': 'Let LLMs Break Free from Overthinking via Self-Braking Tuning', 'url': 'https://huggingface.co/papers/2505.14604', 'abstract': 'A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.', 'score': 20, 'issue_id': 3917, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '20e4c9c407be15de', 'authors': ['Haoran Zhao', 'Yuchen Yan', 'Yongliang Shen', 'Haolei Xu', 'Wenqi Zhang', 'Kaitao Song', 'Jian Shao', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Microsoft Research Asia', 'Tianjin University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14604.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самоконтроль ИИ: эффективное мышление без лишних раздумий', 'desc': "Представлена новая система Self-Braking Tuning для крупных моделей рассуждений, позволяющая им самостоятельно регулировать процесс рассуждений и избегать избыточных вычислений. Разработаны метрики для выявления чрезмерного обдумывания и метод обнаружения избыточных шагов в цепочке рассуждений. Предложена стратегия создания обучающих данных с адаптивной длиной рассуждений и механизм 'тормозящих' подсказок. Эксперименты показали снижение потребления токенов до 60% при сохранении точности на уровне моделей без ограничений."}, 'en': {'title': 'Empowering Models to Self-Regulate Reasoning for Efficiency', 'desc': 'The paper introduces a Self-Braking Tuning (SBT) framework designed to enhance large reasoning models (LRMs) by allowing them to self-regulate their reasoning processes. This approach addresses the issue of overthinking, which leads to unnecessary computational overhead and redundant reasoning steps. By developing metrics to identify overthinking and creating a braking prompt mechanism, the model learns to determine when to stop reasoning effectively. Experimental results show that SBT can reduce token usage by up to 60% while preserving accuracy, making it a more efficient solution for reasoning tasks.'}, 'zh': {'title': '自我调节，优化推理效率', 'desc': '本文提出了一种新颖的自我制动调优框架（Self-Braking Tuning, SBT），旨在减少大型推理模型中的过度思考和不必要的计算开销。通过允许模型自我调节推理过程，SBT消除了对外部控制机制的依赖。我们构建了一套基于标准答案的过度思考识别指标，并设计了一种系统的方法来检测冗余推理。实验结果表明，该方法在保持相似准确率的同时，能够将令牌消耗减少多达60%。'}}}, {'id': 'https://huggingface.co/papers/2505.14684', 'title': 'Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning', 'url': 'https://huggingface.co/papers/2505.14684', 'abstract': 'A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.', 'score': 19, 'issue_id': 3918, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'bcaa1385d7242117', 'authors': ['Haolei Xu', 'Yuchen Yan', 'Yongliang Shen', 'Wenqi Zhang', 'Guiyang Hou', 'Shengpei Jiang', 'Kaitao Song', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Microsoft Research Asia', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14684.jpg', 'data': {'categories': ['#math', '#training', '#dataset', '#reasoning', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Мост через пропасть в цепочке мыслей: улучшение математических рассуждений ИИ', 'desc': 'Эта статья представляет модель для обнаружения и генерации пропущенных промежуточных шагов в математических рассуждениях с использованием метода цепочки мыслей (Chain-of-Thought, CoT). Авторы предлагают задачу CoT Thought Leap Bridge для автоматического обнаружения пропусков и генерации недостающих шагов рассуждения. Они создали специализированный набор данных ScaleQM+ и обучили модель CoT-Bridge для восстановления полноты и связности рассуждений. Эксперименты показали, что модели, дообученные на дополненных наборах данных, превосходят модели, обученные на исходных данных, улучшая производительность и обобщающую способность на задачах математического и логического мышления.'}, 'en': {'title': 'Bridging Thought Leaps for Enhanced Reasoning in AI', 'desc': 'This paper introduces a model designed to detect and fill in missing steps in Chain-of-Thought (CoT) reasoning for mathematical tasks. The authors identify a problem where existing datasets have gaps, known as Thought Leaps, which hinder the learning process of large language models (LLMs). To address this, they propose the CoT Thought Leap Bridge Task and create a new training dataset called ScaleQM+ to help models learn to generate the missing reasoning steps. Their experiments show that models trained with this approach significantly outperform those trained on incomplete datasets, leading to better performance in both mathematical and logical reasoning tasks.'}, 'zh': {'title': '填补思维跳跃，提升推理能力', 'desc': '本论文提出了一种模型，用于检测和生成数学推理中的缺失中间步骤，从而提高模型在数学和逻辑推理任务上的表现和泛化能力。现有的数学链式推理数据集常常因为专家省略中间步骤而导致思维跳跃，这对模型的学习和泛化产生负面影响。我们提出了链式推理思维跳跃桥接任务，旨在自动检测思维跳跃并生成缺失的中间推理步骤，以恢复推理的完整性和连贯性。通过构建专门的训练数据集ScaleQM+并进行实验，我们证明了在桥接数据集上微调的模型在数学推理基准测试中表现优于原始数据集，且在逻辑推理任务上也显示出更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.16990', 'title': 'Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding', 'url': 'https://huggingface.co/papers/2505.16990', 'abstract': "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only text{response length}{3}. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimple's capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at https://github.com/yu-rp/Dimple.", 'score': 18, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '5c0636fbc17936b5', 'authors': ['Runpeng Yu', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.16990.jpg', 'data': {'categories': ['#multimodal', '#inference', '#diffusion', '#optimization', '#open_source', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'Dimple: Дискретная диффузия для эффективных и контролируемых языковых моделей', 'desc': 'Dimple - это первая дискретная диффузионная мультимодальная большая языковая модель (DMLLM). Она сочетает автореãрессивное и диффузионное обучение, достигая производительности сравнимой с авторегрессивными моделями. Для повышения эффективности вывода предложена стратегия уверенного декодирования, динамически корректирующая количество генерируемых токенов. Модель также демонстрирует возможность точного контроля ответов с помощью структурных приоров.'}, 'en': {'title': 'Dimple: A New Era in Efficient Language Modeling', 'desc': "Dimple is a new type of language model called a Discrete Diffusion Multimodal Large Language Model (DMLLM) that combines two training methods to improve performance and stability. It starts with an autoregressive training phase, which helps to stabilize the learning process, followed by a diffusion phase that enhances the model's capabilities. Dimple-7B outperforms existing models like LLaVA-NEXT by 3.9%, showing that it can compete with traditional autoregressive models. Additionally, it introduces a confident decoding strategy that reduces the number of iterations needed for generating responses, making it faster and more efficient while allowing for better control over the output format and length."}, 'zh': {'title': 'Dimple：高效的离散扩散多模态大语言模型', 'desc': '本文提出了Dimple，一个离散扩散多模态大语言模型（DMLLM），通过混合训练方法实现了与自回归模型相当的性能。我们发现，单纯的离散扩散训练方法会导致训练不稳定、性能不佳和严重的长度偏差。为了解决这些问题，我们设计了一种新颖的训练范式，结合了初始的自回归阶段和后续的扩散阶段。Dimple模型在推理效率上也有所提升，采用了动态调整生成令牌数量的自信解码策略，显著减少了生成迭代次数。'}}}, {'id': 'https://huggingface.co/papers/2505.15952', 'title': 'VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance', 'url': 'https://huggingface.co/papers/2505.15952', 'abstract': "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/", 'score': 18, 'issue_id': 3915, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'f9307168ac97ad24', 'authors': ['Mohammad Reza Taesiri', 'Abhijay Ghildyal', 'Saman Zadtootaghaj', 'Nabajeet Barman', 'Cor-Paul Bezemer'], 'affiliations': ['Sony Interactive Entertainment, Aliso Viejo, US', 'Sony Interactive Entertainment, Berlin, Germany', 'Sony Interactive Entertainment, London, UK', 'University of Alberta, CA'], 'pdf_title_img': 'assets/pdf/title_img/2505.15952.jpg', 'data': {'categories': ['#games', '#cv', '#video', '#optimization', '#benchmark'], 'emoji': '🎮', 'ru': {'title': 'VideoGameQA-Bench: Революция в автоматизации контроля качества видеоигр', 'desc': 'Представлен новый бенчмарк VideoGameQA-Bench для оценки визуально-языковых моделей в задачах контроля качества видеоигр. Бенчмарк охватывает широкий спектр задач, включая визуальное модульное тестирование, обнаружение визуальных регрессий, поиск игл в стоге сена, обнаружение глитчей и генерацию отчетов об ошибках. Он разработан для точной оценки эффективности визуально-языковых моделей в реальных сценариях разработки игр. VideoGameQA-Bench призван заполнить пробел в существующих бенчмарках, которые недостаточно адаптированы к специфике индустрии видеоигр.'}, 'en': {'title': 'Revolutionizing Game QA with VideoGameQA-Bench', 'desc': 'The paper presents VideoGameQA-Bench, a new benchmark designed to evaluate Vision-Language Models (VLMs) specifically for video game quality assurance tasks. It addresses the need for standardized assessments in a field where existing benchmarks do not meet the unique challenges of game development. By focusing on various QA activities such as visual unit testing and glitch detection, this benchmark aims to enhance the automation of quality assurance processes in the gaming industry. The introduction of VideoGameQA-Bench is a significant step towards improving the efficiency and effectiveness of game development workflows.'}, 'zh': {'title': '提升游戏开发质量的智能评估工具', 'desc': '本文介绍了一个名为VideoGameQA-Bench的基准，用于评估视觉语言模型在视频游戏质量保证任务中的表现。随着视频游戏在娱乐行业中产生的收入最高，优化游戏开发流程变得至关重要。视觉语言模型的最新进展为自动化和提升游戏开发的各个方面提供了巨大潜力，尤其是在劳动密集型的质量保证环节。为了准确评估这些模型在实际场景中的有效性，本文提出了一个全面的基准，涵盖了多种游戏QA活动，包括视觉单元测试、视觉回归测试、故障检测等。'}}}, {'id': 'https://huggingface.co/papers/2505.16967', 'title': 'Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval', 'url': 'https://huggingface.co/papers/2505.16967', 'abstract': 'Using cascading LLM prompts to identify and relabel false negatives in datasets improves retrieval and reranking models\' performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness -- pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35times and increases nDCG@10 on BEIR by 1.0 point. This motivates a deeper examination of training data quality, with a particular focus on "false negatives", where relevant passages are incorrectly labeled as irrelevant. We propose a simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o-mini.', 'score': 17, 'issue_id': 3931, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'd40b350aefa1642b', 'authors': ['Nandan Thakur', 'Crystina Zhang', 'Xueguang Ma', 'Jimmy Lin'], 'affiliations': ['David R. Cheriton School of Computer Science, University of Waterloo, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.16967.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Улучшение моделей поиска через умную переразметку данных', 'desc': 'Статья предлагает метод улучшения качества датасетов для обучения моделей информационного поиска и ранжирования. Используя каскадные промпты для больших языковых моделей (LLM), авторы выявляют и переразмечают ложноотрицательные примеры в наборах данных. Эксперименты показывают, что такой подход повышает эффективность как моделей ретривера (E5, Qwen2.5-7B), так и ранжировщика (Qwen2.5-3B) на бенчмарках BEIR и AIR-Bench. Надежность метода подтверждается высоким согласием с человеческой разметкой.'}, 'en': {'title': 'Enhancing Model Performance by Relabeling False Negatives', 'desc': 'This paper discusses improving the performance of retrieval and reranking models by addressing the issue of false negatives in training datasets. The authors demonstrate that by pruning ineffective datasets and focusing on relabeling incorrectly labeled relevant passages, they can enhance model effectiveness significantly. They introduce a method using cascading LLM prompts to identify and correct these false negatives, leading to measurable improvements in nDCG scores on benchmark evaluations. The results indicate that better training data quality directly correlates with enhanced model performance, showcasing the importance of accurate labeling in machine learning.'}, 'zh': {'title': '提升模型性能的关键：重新标记错误负样本', 'desc': '本文探讨了使用级联大语言模型（LLM）提示来识别和重新标记数据集中错误标记的负样本，从而提高检索和重排序模型的性能。研究发现，某些数据集会对模型效果产生负面影响，去除不必要的数据集可以显著提高模型的表现。我们提出了一种简单且经济的方法，通过级联提示来识别和重新标记这些难以识别的负样本。实验结果表明，重新标记后的数据显著提升了检索模型的效果，验证了数据质量对模型训练的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.16864', 'title': 'Training-Free Efficient Video Generation via Dynamic Token Carving', 'url': 'https://huggingface.co/papers/2505.16864', 'abstract': 'Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83times speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga', 'score': 17, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'ec67c352f5ae81d7', 'authors': ['Yuechen Zhang', 'Jinbo Xing', 'Bin Xia', 'Shaoteng Liu', 'Bohao Peng', 'Xin Tao', 'Pengfei Wan', 'Eric Lo', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'Kuaishou Technology', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2505.16864.jpg', 'data': {'categories': ['#video', '#inference', '#diffusion', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Ускорение генерации видео без потери качества', 'desc': 'Jenga - это новый конвейер вывода для моделей видео-диффузионных трансформеров, который сочетает динамическое вырезание внимания и прогрессивную генерацию разрешения. Этот подход значительно ускоряет генерацию видео, сохраняя при этом высокое качество. Jenga использует блочный механизм внимания, который динамически выбирает релевантные взаимодействия токенов с помощью 3D-кривых, заполняющих пространство. Экспериментальные результаты показывают, что Jenga достигает существенного ускорения на нескольких современных моделях видео-диффузии при сохранении сопоставимого качества генерации.'}, 'en': {'title': 'Jenga: Speeding Up Video Generation with Smart Attention!', 'desc': 'The paper introduces Jenga, an innovative inference pipeline designed for video Diffusion Transformer models that enhances video generation speed while preserving quality. It tackles the computational inefficiencies caused by the quadratic complexity of self-attention and the multi-step nature of diffusion processes. Jenga employs dynamic attention carving to selectively focus on relevant token interactions and utilizes progressive resolution generation to optimize the use of high-resolution latents. Experimental results show that Jenga can significantly accelerate video generation, achieving up to 8.83 times faster performance with minimal quality loss, making it a practical solution for real-time applications.'}, 'zh': {'title': 'Jenga：加速视频生成的创新推理管道', 'desc': 'Jenga是一种新颖的视频扩散变换器模型推理管道，结合了动态注意力切割和渐进分辨率生成，显著加快了视频生成速度，同时保持高质量。该方法解决了自注意力的平方复杂度和扩散模型的多步骤特性带来的计算效率问题。Jenga通过动态选择相关的标记交互和逐步提高潜在分辨率，优化了生成过程。实验结果表明，Jenga在多个最先进的视频扩散模型上实现了显著的加速，同时生成质量保持相当。'}}}, {'id': 'https://huggingface.co/papers/2505.16916', 'title': 'Backdoor Cleaning without External Guidance in MLLM Fine-tuning', 'url': 'https://huggingface.co/papers/2505.16916', 'abstract': "Multimodal Large Language Models (MLLMs) are increasingly deployed in fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt general-purpose models to downstream tasks. This flexibility, however, introduces serious security risks, as malicious fine-tuning can implant backdoors into MLLMs with minimal effort. In this paper, we observe that backdoor triggers systematically disrupt cross-modal processing by causing abnormal attention concentration on non-semantic regions--a phenomenon we term attention collapse. Based on this insight, we propose Believe Your Eyes (BYE), a data filtering framework that leverages attention entropy patterns as self-supervised signals to identify and filter backdoor samples. BYE operates via a three-stage pipeline: (1) extracting attention maps using the fine-tuned model, (2) computing entropy scores and profiling sensitive layers via bimodal separation, and (3) performing unsupervised clustering to remove suspicious samples. Unlike prior defenses, BYE equires no clean supervision, auxiliary labels, or model modifications. Extensive experiments across various datasets, models, and diverse trigger types validate BYE's effectiveness: it achieves near-zero attack success rates while maintaining clean-task performance, offering a robust and generalizable solution against backdoor threats in MLLMs.", 'score': 15, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '36dbc47d484f0a53', 'authors': ['Xuankun Rong', 'Wenke Huang', 'Jian Liang', 'Jinhe Bi', 'Xun Xiao', 'Yiming Li', 'Bo Du', 'Mang Ye'], 'affiliations': ['Huawei Technologies', 'Munich Research Center', 'Nanyang Technological University', 'School of Computer Science, Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16916.jpg', 'data': {'categories': ['#multimodal', '#data', '#training', '#dataset', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Защита мультимодальных ИИ от скрытых угроз: доверяй своим глазам', 'desc': "Статья посвящена проблеме безопасности мультимодальных больших языковых моделей (MLLM) в контексте дообучения на пользовательских данных. Авторы обнаружили, что вредоносные триггеры вызывают аномальную концентрацию внимания на несемантических областях, назвав это явление 'коллапсом внимания'. На основе этого наблюдения они разработали фреймворк BYE для фильтрации backdoor-образцов, используя паттерны энтропии внимания. BYE работает в три этапа и не требует чистых данных или модификации модели, обеспечивая эффективную защиту от backdoor-атак на MLLM."}, 'en': {'title': 'Defending MLLMs: Believe Your Eyes Against Backdoors!', 'desc': "This paper addresses the security risks associated with Multimodal Large Language Models (MLLMs) in fine-tuning-as-a-service (FTaaS) environments, where malicious fine-tuning can introduce backdoors. The authors identify a phenomenon called 'attention collapse', where backdoor triggers cause abnormal attention focus on irrelevant areas, disrupting cross-modal processing. To combat this, they propose a framework called Believe Your Eyes (BYE), which uses attention entropy patterns to filter out backdoor samples without needing clean supervision or model changes. BYE demonstrates strong effectiveness in various scenarios, achieving low attack success rates while preserving performance on clean tasks."}, 'zh': {'title': '抵御后门威胁的创新解决方案', 'desc': '多模态大型语言模型（MLLMs）在微调服务中越来越常见，但这也带来了安全风险，恶意微调可能会在模型中植入后门。本文观察到，后门触发器会导致跨模态处理的异常注意力集中，形成我们称之为注意力崩溃的现象。基于这一发现，我们提出了"相信你的眼睛"（BYE）数据过滤框架，通过注意力熵模式作为自监督信号来识别和过滤后门样本。BYE通过三个阶段的流程操作，能够在不需要干净监督或模型修改的情况下，有效地抵御MLLMs中的后门威胁。'}}}, {'id': 'https://huggingface.co/papers/2505.16421', 'title': 'WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.16421', 'abstract': 'WebAgent-R1 is an RL framework for training web agents in multi-turn interactions, achieving high success rates compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.', 'score': 13, 'issue_id': 3928, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '9b6b6b9f01bfb9c0', 'authors': ['Zhepei Wei', 'Wenlin Yao', 'Yao Liu', 'Weizhi Zhang', 'Qin Lu', 'Liang Qiu', 'Changlong Yu', 'Puyang Xu', 'Chao Zhang', 'Bing Yin', 'Hyokun Yun', 'Lihong Li'], 'affiliations': ['Amazon', 'Georgia Institute of Technology', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.16421.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#rl', '#benchmark', '#games', '#training'], 'emoji': '🕸️', 'ru': {'title': 'WebAgent-R1: Прорыв в обучении веб-агентов с помощью многоходового RL', 'desc': 'WebAgent-R1 - это фреймворк обучения с подкреплением для тренировки веб-агентов в многоходовых взаимодействиях. Он обучается напрямую из онлайн-взаимодействий с веб-средами, генерируя разнообразные траектории, руководствуясь бинарными наградами в зависимости от успеха задачи. Эксперименты показывают значительное повышение успешности выполнения задач по сравнению с существующими методами. Исследование также выявляет эффективность стратегии промптинга на основе размышлений и масштабирования во время тестирования для веб-задач.'}, 'en': {'title': 'Empowering Web Agents with Multi-Turn Reinforcement Learning', 'desc': 'WebAgent-R1 is a reinforcement learning (RL) framework designed to train web agents for multi-turn interactions, which are more complex than single-turn tasks. It operates by learning from real-time interactions with web environments, using binary rewards to guide the training process based on task success. The framework has shown significant improvements in task success rates for large language models, outperforming existing methods and proprietary models. Additionally, it explores different initialization strategies to enhance the training process, emphasizing the role of behavior cloning and chain-of-thought reasoning in achieving better performance.'}, 'zh': {'title': 'WebAgent-R1：多轮交互的强化学习新框架', 'desc': 'WebAgent-R1是一个用于训练网络代理的强化学习框架，专注于多轮交互，成功率显著高于现有方法。与传统的单轮任务不同，该框架能够处理复杂的长期决策问题，通过在线交互学习并生成多样化的轨迹。实验结果表明，WebAgent-R1在WebArena-Lite基准测试中显著提高了任务成功率，超越了当前最先进的方法。该研究还探讨了不同的强化学习初始化策略，强调了热身训练阶段的重要性，并提供了在网络代理中融入长链思维推理的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.17018', 'title': 'SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward', 'url': 'https://huggingface.co/papers/2505.17018', 'abstract': 'An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1.', 'score': 12, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'a49e142829760753', 'authors': ['Kaixuan Fan', 'Kaituo Feng', 'Haoming Lyu', 'Dongzhan Zhou', 'Xiangyu Yue'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.17018.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rlhf', '#rl', '#optimization', '#benchmark', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение мышления ИИ через вознаграждение за процесс рассуждений', 'desc': 'Исследователи предложили улучшенную мультимодальную языковую модель SophiaVL-R1, которая использует вознаграждения за процесс мышления для улучшения рассуждений и обобщения. Модель обучается с помощью метода Trust-GRPO, который взвешивает надежность вознаграждений за мышление. Применяется стратегия отжига, постепенно уменьшающая влияние вознаграждений за мышление в пользу более точных вознаграждений за конечный результат. SophiaVL-R1 превосходит более крупные модели на различных бенчмарках, демонстрируя сильные способности к рассуждению и обобщению.'}, 'en': {'title': 'Empowering Reasoning with Thinking Process Rewards', 'desc': 'This paper introduces SophiaVL-R1, a multimodal language model that enhances reasoning and generalization by incorporating rewards for the thinking process. Traditional reinforcement learning methods often overlook the quality of reasoning, leading to suboptimal strategies. To address this, the authors develop a thinking reward model that evaluates reasoning quality and implement a Trust-GRPO method to ensure reliable reward signals. Their experiments demonstrate that SophiaVL-R1 outperforms larger models on various benchmarks, showcasing its superior reasoning capabilities.'}, 'zh': {'title': '引入思维奖励，提升推理能力的多模态模型', 'desc': '这篇论文提出了一种增强的多模态语言模型SophiaVL-R1，旨在通过引入思维过程奖励来改善推理和泛化能力。传统的多模态大语言模型在推理时缺乏对思维过程的监督，可能导致学习到次优的推理策略。为了解决这个问题，研究者们设计了一个思维奖励模型，并提出了Trust-GRPO方法来评估思维奖励的可信度。实验结果表明，SophiaVL-R1在多个基准测试中表现优于许多大型模型，展示了其强大的推理和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.17012', 'title': 'SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding', 'url': 'https://huggingface.co/papers/2505.17012', 'abstract': 'SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs.', 'score': 10, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '51519403c5b92e25', 'authors': ['Haoning Wu', 'Xiao Huang', 'Yaohui Chen', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai AI Laboratory', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2505.17012.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#survey', '#3d', '#benchmark', '#agents'], 'emoji': '🧠', 'ru': {'title': 'SpatialScore: новый рубеж в пространственном понимании для мультимодальных ИИ', 'desc': 'Статья представляет SpatialScore - комплексный бенчмарк для оценки пространственного понимания мультимодальными большими языковыми моделями (MLLM). Авторы разработали VGBench для тестирования восприятия визуальной геометрии, а также создали SpatialAgent - мультиагентную систему со специализированными инструментами для пространственного анализа. Исследование выявило сохраняющиеся проблемы в пространственных рассуждениях MLLM, одновременно продемонстрировав эффективность предложенного SpatialAgent. Бенчмарк SpatialScore призван стать важным инструментом для дальнейшего развития MLLM в области пространственного понимания.'}, 'en': {'title': 'Enhancing 3D Spatial Understanding in Multimodal Models', 'desc': 'This paper introduces SpatialScore, a benchmark designed to evaluate multimodal large language models (MLLMs) in their ability to understand 3D spatial concepts. It highlights the development of VGBench, which focuses on visual geometry perception tasks like camera pose estimation. The benchmark includes 28,000 samples from various spatial understanding tasks and features a challenging subset called SpatialScore-Hard. Additionally, the paper presents SpatialAgent, a multi-agent system equipped with specialized tools to enhance spatial reasoning capabilities in MLLMs.'}, 'zh': {'title': '空间理解的新基准与工具', 'desc': '本论文探讨了多模态大型语言模型（MLLMs）在三维空间理解方面的能力。我们引入了VGBench，这是一个专门用于评估视觉几何感知的基准，涵盖相机姿态和运动估计等任务。此外，我们提出了SpatialScore，这是迄今为止最全面的多模态空间理解基准，整合了来自11个现有数据集的数据，包含28K个样本和一个具有挑战性的子集SpatialScore-Hard。最后，我们开发了SpatialAgent，一个新颖的多智能体系统，结合了9个专门的工具，以支持空间理解的推理。'}}}, {'id': 'https://huggingface.co/papers/2505.16839', 'title': 'LaViDa: A Large Diffusion Language Model for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2505.16839', 'abstract': "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.", 'score': 10, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '9e2202389256fc59', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Hritik Bansal', 'Akash Gokul', 'Yusuke Kato', 'Kazuki Kozuka', 'Jason Kuen', 'Zhe Lin', 'Kai-Wei Chang', 'Aditya Grover'], 'affiliations': ['Adobe Research', 'Panasonic AI Research', 'Salesforce Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2505.16839.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#training', '#games', '#cv', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'LaViDa: Быстрые и гибкие мультимодальные модели на основе дискретной диффузии', 'desc': 'LaViDa - это семейство мультимодальных моделей, основанных на дискретных диффузионных моделях. Они предлагают конкурентоспособную производительность на различных мультимодальных задачах, при этом обеспечивая преимущества в скорости, контролируемости и двунаправленном рассуждении. В отличие от авторегрессионных моделей, LaViDa использует параллельное декодирование для более быстрого вывода и двунаправленный контекст для контролируемой генерации. Эксперименты показывают, что LaViDa достигает конкурентных или превосходящих результатов по сравнению с авторегрессионными мультимодальными моделями на различных бенчмарках.'}, 'en': {'title': 'LaViDa: Fast and Controllable Vision-Language Models', 'desc': 'LaViDa is a new family of vision-language models that utilize discrete diffusion models to enhance performance on multimodal tasks. It addresses the limitations of traditional autoregressive models by providing faster inference and better control over output generation. By integrating a vision encoder and employing techniques like complementary masking and prefix KV cache, LaViDa achieves high-quality results while maintaining efficiency. Experimental results show that LaViDa outperforms existing models in various benchmarks, demonstrating its potential as a robust alternative in the field of vision-language processing.'}, 'zh': {'title': 'LaViDa：高效可控的视觉-语言模型', 'desc': 'LaViDa是一种基于离散扩散模型的视觉-语言模型家族，能够在多模态基准测试中表现出色。与现有的自回归视觉-语言模型相比，LaViDa在推理速度、可控性和双向推理方面具有明显优势。该模型通过结合视觉编码器和联合微调技术，提升了多模态任务的处理能力。实验结果表明，LaViDa在多模态基准测试中表现优于传统模型，展示了其作为自回归模型强有力替代品的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.14625', 'title': 'TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.14625', 'abstract': "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.", 'score': 10, 'issue_id': 3914, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '0bcb0b140b7623c3', 'authors': ['Zhangchen Xu', 'Yuetai Li', 'Fengqing Jiang', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14625.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#training', '#dataset', '#rl', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Борьба с ложноотрицательными результатами для улучшения RL-обучения языковых моделей', 'desc': 'В этой статье исследуется проблема ложноотрицательных результатов в верификаторах, используемых для обучения с подкреплением (RL) больших языковых моделей (LLM). Авторы обнаружили, что более 38% правильных ответов моделей неверно отклоняются верификаторами, что значительно ухудшает процесс обучения. Для решения этой проблемы предложен легковесный верификатор tinyV на основе LLM, который дополняет существующие методы и выявляет потенциальные ложноотрицательные результаты. Интеграция tinyV повышает точность на 10% и ускоряет сходимость при обучении LLM с помощью RL на нескольких математических тестах.'}, 'en': {'title': 'Enhancing RL Training by Tackling Verifier False Negatives with TinyV', 'desc': 'This paper discusses the challenges of using Reinforcement Learning (RL) to improve large language models (LLMs) due to the issue of false negatives from verifiers. False negatives occur when verifiers incorrectly reject correct outputs from the model, which can hinder the RL training process by limiting the feedback the model receives. The authors analyze a dataset and find that a significant portion of model responses are misclassified, leading to slower learning and convergence. To address this, they introduce TinyV, a new verifier that enhances existing methods by identifying and correcting these false negatives, resulting in improved performance on math-reasoning tasks.'}, 'zh': {'title': '解决假阴性，提升强化学习效果！', 'desc': '强化学习（RL）是一种通过奖励信号优化大语言模型（LLM）策略的强大工具。然而，RL的成功依赖于验证者提供的可靠奖励，而我们发现一个普遍存在的问题——假阴性，即验证者错误地拒绝正确的模型输出。我们的研究表明，超过38%的模型生成的响应受到假阴性的影响，这严重损害了RL训练的效果。为了解决这个问题，我们提出了tinyV，一个轻量级的LLM基础验证器，可以动态识别潜在的假阴性，从而提高奖励估计的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.16854', 'title': 'Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.16854', 'abstract': "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON.", 'score': 9, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'af930383188983e1', 'authors': ['Jiaqi Wang', 'Kevin Qinghong Lin', 'James Cheng', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.16854.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение ИИ рассуждать как человек', 'desc': "Статья представляет стратегию обучения TON для улучшения рассуждений в мультимодальных моделях на основе компьютерного зрения и обработки естественного языка. TON сочетает контролируемое обучение с методом 'отсева мыслей' и групповой относительной оптимизацией политики (GRPO). Эта стратегия позволяет моделям избегать ненужных шагов рассуждения, сокращая длину вывода на 90% без потери производительности. Эксперименты показывают, что модель постепенно учится пропускать лишние рассуждения, приближаясь к человекоподобному паттерну мышления."}, 'en': {'title': 'TON: Streamlining Reasoning in Vision-Language Models', 'desc': "The paper introduces TON, a two-stage training strategy designed to enhance vision-language models (VLMs) by reducing unnecessary reasoning steps. It combines supervised fine-tuning with a technique called 'thought dropout' to help models decide when reasoning is necessary, mimicking human-like thinking processes. The second stage employs Group Relative Policy Optimization (GRPO) to optimize the model's reasoning efficiency while maximizing task performance. Experimental results demonstrate that TON can significantly decrease reasoning length without compromising, and often improving, the model's performance across various tasks."}, 'zh': {'title': 'TON：优化视觉-语言模型推理的创新策略', 'desc': 'TON是一种两阶段的训练策略，结合了监督微调和思维丢弃，旨在减少视觉-语言模型中的不必要推理步骤，同时保持性能。第一阶段通过简单有效的“思维丢弃”操作，随机用空思维替换推理轨迹，引入了选择性推理的思考格式。第二阶段采用群体相对策略优化（GRPO），使模型能够自由决定何时进行推理，从而最大化任务相关的结果奖励。实验结果表明，TON可以将完成长度减少多达90%，而不牺牲性能，甚至在某些情况下提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2505.15879', 'title': 'GRIT: Teaching MLLMs to Think with Images', 'url': 'https://huggingface.co/papers/2505.15879', 'abstract': 'Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.', 'score': 9, 'issue_id': 3915, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '05b66f8da1752a05', 'authors': ['Yue Fan', 'Xuehai He', 'Diji Yang', 'Kaizhi Zheng', 'Ching-Chen Kuo', 'Yuting Zheng', 'Sravana Jyothi Narayanaraju', 'Xinze Guan', 'Xin Eric Wang'], 'affiliations': ['UC Santa Cruz', 'eBay'], 'pdf_title_img': 'assets/pdf/title_img/2505.15879.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rl', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'GRIT: Визуальное обоснование рассуждений для мультимодальных языковых моделей', 'desc': 'Статья представляет новый метод обучения мультимодальных языковых моделей (MLLM) под названием GRIT. Этот метод позволяет моделям генерировать цепочки рассуждений, сочетающие естественный язык и координаты ограничивающих рамок на изображениях. GRIT использует подход обучения с подкреплением GRPO-GR, основанный на алгоритме GRPO, который не требует аннотаций цепочек рассуждений или явных меток ограничивающих рамок. Эксперименты показывают, что GRIT эффективно обучает MLLM производить согласованные и визуально обоснованные цепочки рассуждений, успешно объединяя способности к рассуждению и визуальной привязке.'}, 'en': {'title': 'Grounded Reasoning: Merging Vision and Language for Better Understanding', 'desc': 'This paper introduces Grounded Reasoning with Images and Texts (GRIT), a new method that enhances visual reasoning in machine learning models. GRIT allows models to generate reasoning chains that combine natural language with specific visual information, represented by bounding box coordinates. By using a reinforcement learning approach called GRPO-GR, GRIT focuses on improving the accuracy of final answers without needing extensive annotated data. The results show that GRIT can efficiently train models to produce clear and visually supported reasoning, bridging the gap between reasoning and visual understanding.'}, 'zh': {'title': '图像与文本的基础推理：让模型更好地思考', 'desc': '最近的研究表明，强化学习（RL）在构建推理模型方面非常有效，这些模型在给出最终答案之前会表达思维链。然而，现有的开源视觉推理模型通常只用自然语言生成推理内容，缺乏与视觉信息的明确结合，这限制了它们生成清晰且与视觉相关的推理链的能力。为此，我们提出了一种新方法——图像与文本的基础推理（GRIT），该方法训练多模态语言模型（MLLMs）与图像共同思考。GRIT引入了一种基础推理范式，模型生成的推理链交替包含自然语言和明确的边界框坐标，这些坐标指向输入图像中模型在推理过程中参考的区域。'}}}, {'id': 'https://huggingface.co/papers/2505.15963', 'title': 'OViP: Online Vision-Language Preference Learning', 'url': 'https://huggingface.co/papers/2505.15963', 'abstract': "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities.", 'score': 8, 'issue_id': 3914, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '4476380071b2e936', 'authors': ['Shujun Liu', 'Siyuan Wang', 'Zejun Li', 'Jianxiang Wang', 'Cheng Zeng', 'Zhongyu Wei'], 'affiliations': ['ByteDance', 'Fudan University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.15963.jpg', 'data': {'categories': ['#multimodal', '#training', '#hallucinations', '#benchmark', '#diffusion', '#rag', '#alignment'], 'emoji': '🔮', 'ru': {'title': 'OViP: Обучение без галлюцинаций для визуально-языковых моделей', 'desc': 'Статья представляет новый подход к обучению мультимодальных моделей, называемый OViP (Online Vision-language Preference Learning). Этот метод направлен на уменьшение галлюцинаций в крупных визуально-языковых моделях (LVLM) путем динамического создания контрастных обучающих данных на основе собственных ошибочных выходов модели. OViP использует диффузионную модель для синтеза негативных изображений, что позволяет генерировать более релевантные сигналы обучения в реальном времени. Эксперименты показывают, что OViP эффективно снижает галлюцинации, сохраняя при этом ключевые мультимодальные возможности модели.'}, 'en': {'title': 'Dynamic Learning to Combat Hallucination in Vision-Language Models', 'desc': "This paper addresses the issue of hallucination in large vision-language models (LVLMs), where the models generate content that does not match the visual inputs. The authors introduce a new framework called Online Vision-language Preference Learning (OViP), which creates training data based on the model's own incorrect outputs, rather than relying on static negative samples. By using a diffusion model to synthesize negative images, OViP provides more relevant feedback for the model to learn from. The results show that this approach not only reduces hallucinations but also maintains the model's ability to express multi-modal information effectively."}, 'zh': {'title': '动态构建对比数据，减少幻觉！', 'desc': '大型视觉语言模型（LVLMs）在生成内容时容易出现幻觉，常常与视觉输入不一致。虽然最近的研究通过多模态直接偏好优化（DPO）来减轻幻觉问题，但通常依赖于预定义或随机编辑的负样本，这些样本无法真实反映模型的错误，限制了训练效果。我们提出了一种在线视觉语言偏好学习（OViP）框架，动态构建对比训练数据，基于模型自身的幻觉输出。通过识别响应对之间的语义差异并使用扩散模型合成负图像，OViP实时生成更相关的监督信号，有效减少幻觉，同时保持多模态能力。'}}}, {'id': 'https://huggingface.co/papers/2505.17015', 'title': 'Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.17015', 'abstract': 'Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics.', 'score': 7, 'issue_id': 3923, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'b15dc44bbeff2152', 'authors': ['Runsen Xu', 'Weiyao Wang', 'Hao Tang', 'Xingyu Chen', 'Xiaodong Wang', 'Fu-Jen Chu', 'Dahua Lin', 'Matt Feiszli', 'Kevin J. Liang'], 'affiliations': ['FAIR, Meta', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17015.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#games', '#3d', '#robotics', '#multimodal', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Мультикадровое пространственное понимание для языковых моделей', 'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM) для понимания пространственных отношений в нескольких кадрах. Авторы создали большой датасет MultiSPA и комплексный бенчмарк для оценки пространственных задач. Разработанная модель Multi-SpatialMLLM демонстрирует значительное улучшение по сравнению с базовыми моделями в задачах мультикадрового рассуждения. Результаты показывают потенциал применения модели в робототехнике в качестве аннотатора наград.'}, 'en': {'title': 'Empowering MLLMs with Multi-Frame Spatial Intelligence', 'desc': 'This paper addresses the limitations of multi-modal large language models (MLLMs) in understanding spatial information across multiple frames, which is crucial for applications like robotics. The authors introduce a new framework that enhances MLLMs by incorporating depth perception, visual correspondence, and dynamic perception. They present the MultiSPA dataset, a large-scale collection of over 27 million samples that covers a variety of 3D and 4D scenes, to train and evaluate their model. The proposed Multi-SpatialMLLM shows improved performance in multi-frame reasoning tasks and demonstrates potential for multi-task learning and emergent capabilities in complex scenarios.'}, 'zh': {'title': '提升多模态模型的多帧空间理解能力', 'desc': '本文提出了一种新的框架，以增强多模态大型语言模型（MLLMs）在多帧空间理解方面的能力。我们通过整合深度感知、视觉对应和动态感知，来解决现有模型在机器人和现实应用中的局限性。我们引入了MultiSPA数据集，这是一个包含超过2700万样本的全新大规模数据集，涵盖多种3D和4D场景。我们的模型Multi-SpatialMLLM在多帧推理任务中表现出显著的提升，并展示了在复杂场景中的多任务优势和新兴能力。'}}}, {'id': 'https://huggingface.co/papers/2505.16151', 'title': 'Training-Free Reasoning and Reflection in MLLMs', 'url': 'https://huggingface.co/papers/2505.16151', 'abstract': 'The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates a hierarchical weight merging approach that combines a visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we propose a layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html', 'score': 7, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '59af0e553fdc317e', 'authors': ['Hongchen Wei', 'Zhenzhong Chen'], 'affiliations': ['School of Remote Sensing and Information Engineering, Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16151.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'FRANK: Мультимодальное рассуждение без переобучения', 'desc': 'Модель FRANK улучшает мультимодальные языковые модели (MLLM), добавляя им способности к рассуждению и рефлексии без переобучения. Это достигается с помощью иерархического подхода к объединению весов, который сочетает модель, предобученную на визуальных данных, с моделью, специализированной на рассуждениях. Метод использует послойное слияние на основе разложения Тейлора, интегрируя способность к рассуждениям в глубокие слои декодера, сохраняя при этом визуальную привязку в поверхностных слоях. Эксперименты показывают эффективность подхода, превосходя сильные базовые модели на сложных мультимодальных тестах рассуждений.'}, 'en': {'title': 'FRANK Model: Enhancing MLLMs with Reasoning Without Retraining', 'desc': 'The FRANK Model enhances multimodal large language models (MLLMs) by integrating reasoning and reflection capabilities without the need for retraining. It employs a hierarchical weight merging technique that combines visual-pretrained models with reasoning-specialized models, allowing for effective reasoning in MLLMs. The model strategically decouples perception and reasoning across different layers of the decoder, leveraging shallow layers for visual attention and deeper layers for textual semantics. Experimental results show that FRANK-38B significantly outperforms existing models on multimodal reasoning tasks, achieving a notable accuracy increase on the MMMU benchmark.'}, 'zh': {'title': 'FRANK模型：无需重训的多模态推理增强', 'desc': 'FRANK模型通过分层权重合并的方法，增强了多模态大语言模型（MLLM）的推理和反思能力，而无需重新训练。该模型将视觉预训练的MLLM与专注于推理的LLM结合，避免了高昂的重新训练成本。研究发现，浅层解码器层对视觉信息的关注度更高，而深层解码器层则更注重文本语义，这一观察促成了分层权重合并的方法。通过在深层解码器中整合推理能力，同时保持浅层解码器的视觉基础，FRANK模型在多模态推理基准测试中表现出色，准确率超过了多个强基线模型。'}}}, {'id': 'https://huggingface.co/papers/2505.16944', 'title': 'AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios', 'url': 'https://huggingface.co/papers/2505.16944', 'abstract': "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.", 'score': 6, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '1699a99ec316293a', 'authors': ['Yunjia Qi', 'Hao Peng', 'Xiaozhi Wang', 'Amy Xin', 'Youfeng Liu', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.16944.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#agents', '#long_context', '#agi'], 'emoji': '🤖', 'ru': {'title': 'AgentIF: новый вызов для языковых моделей в роли агентов', 'desc': 'Исследователи представили новый бенчмарк AgentIF для оценки способности больших языковых моделей (LLM) следовать сложным инструкциям в реалистичных сценариях агентов. AgentIF включает 707 аннотированных инструкций из 50 агентных задач, со средней длиной 1723 слова и 11.9 ограничений на инструкцию. Оценка существующих LLM с помощью AgentIF показала их низкую производительность, особенно в обработке сложных структур ограничений и спецификаций инструментов. Авторы провели анализ ошибок и аналитические эксперименты, выявив некоторые режимы отказа существующих LLM.'}, 'en': {'title': 'AgentIF: Benchmarking LLMs in Complex Instruction Following', 'desc': 'This paper introduces AgentIF, a new benchmark designed to assess how well Large Language Models (LLMs) can follow complex instructions in realistic scenarios where they act as agents. The benchmark is based on 50 real-world applications and includes long instructions with an average of 1,723 words and multiple constraints, such as tool specifications. The study reveals that current LLMs struggle significantly with these complex instructions, particularly in managing constraints and tool requirements. By providing detailed evaluations and error analyses, the authors aim to highlight the limitations of existing models and encourage further research in this area.'}, 'zh': {'title': '评估大型语言模型的指令遵循能力', 'desc': '本文介绍了一个新的基准测试AgentIF，用于评估大型语言模型（LLMs）在复杂指令下的表现，特别是在真实的代理场景中。AgentIF的特点包括：基于50个真实世界的代理应用构建，指令平均长度为1723个单词，且每条指令平均包含11.9个约束条件。研究发现，现有的LLMs在处理复杂约束和工具规范时表现不佳，尤其是在遵循长指令方面。通过错误分析和实验，本文揭示了现有模型的局限性，并提供了代码和数据以支持未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.16265', 'title': 'Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models', 'url': 'https://huggingface.co/papers/2505.16265', 'abstract': "Think-RM is a framework that enhances generative reward models with long-horizon reasoning and a novel pairwise RLHF pipeline to improve end-policy performance in aligning large language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the model's long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches.", 'score': 6, 'issue_id': 3929, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '3bfe544ceb0449d3', 'authors': ['Ilgee Hong', 'Changlong Yu', 'Liang Qiu', 'Weixiang Yan', 'Zhenghao Xu', 'Haoming Jiang', 'Qingru Zhang', 'Qin Lu', 'Xin Liu', 'Chao Zhang', 'Tuo Zhao'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.16265.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#rlhf', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Думай глубже: улучшение генеративных моделей вознаграждения с помощью долгосрочного рассуждения', 'desc': 'Think-RM - это фреймворк, который улучшает генеративные модели вознаграждения с помощью долгосрочного рассуждения и новой попарной схемы обучения с подкреплением на основе обратной связи от человека (RLHF). Он позволяет моделям генерировать гибкие, самостоятельно управляемые цепочки рассуждений, поддерживающие такие продвинутые возможности, как самоанализ и гипотетические рассуждения. Think-RM достигает наилучших результатов на бенчмарке RM-Bench, превосходя другие подходы на 8%. В сочетании с новой попарной схемой RLHF он демонстрирует превосходную производительность конечной политики по сравнению с традиционными методами.'}, 'en': {'title': 'Empowering Generative Models with Long-Horizon Reasoning', 'desc': "Think-RM is a new framework designed to improve generative reward models (GenRMs) by enabling long-horizon reasoning, which helps align large language models with human preferences more effectively. It addresses the limitations of traditional Bradley-Terry reward models (BT RMs) that struggle with data sensitivity and reward hacking. By generating self-guided reasoning traces, Think-RM enhances the model's ability to perform complex reasoning tasks. Additionally, it introduces a novel pairwise reinforcement learning from human feedback (RLHF) pipeline that optimizes policies directly with pairwise preferences, leading to better overall performance."}, 'zh': {'title': 'Think-RM：提升生成奖励模型的长时间推理能力', 'desc': 'Think-RM是一个增强生成奖励模型的框架，旨在通过长时间推理和新颖的成对人类反馈强化学习（RLHF）流程来提高大型语言模型与人类偏好的对齐性能。传统的奖励模型在数据规模和覆盖度上敏感，容易受到奖励操控的影响，而生成奖励模型则通过生成思维链（CoT）推理来提供更稳健的替代方案。Think-RM通过建模内部思维过程，生成灵活的自我引导推理轨迹，支持自我反思和假设推理等高级能力。实验结果表明，Think-RM在RM-Bench上取得了最先进的结果，超越了传统方法。'}}}, {'id': 'https://huggingface.co/papers/2505.16192', 'title': 'VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought', 'url': 'https://huggingface.co/papers/2505.16192', 'abstract': 'Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce VLM-R^3 (Visual Language Model with Region Recognition and Reasoning), a framework that equips an MLLM with the ability to (i) decide when additional visual evidence is needed, (ii) determine where to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is Region-Conditioned Reinforcement Policy Optimization (R-GRPO), a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction.', 'score': 6, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'ffe915867d50a220', 'authors': ['Chaoya Jiang', 'Yongrui Heng', 'Wei Ye', 'Han Yang', 'Haiyang Xu', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Shikun Zhang'], 'affiliations': ['Alibaba Group', 'National Engineering Research Center for Software Engineering, Peking University', 'ZEEKR Intelligent Technology Holding Limited'], 'pdf_title_img': 'assets/pdf/title_img/2505.16192.jpg', 'data': {'categories': ['#reasoning', '#cv', '#dataset', '#multimodal', '#benchmark', '#rl', '#training'], 'emoji': '🔍', 'ru': {'title': 'Умное зрение ИИ: точные рассуждения на основе визуальных данных', 'desc': 'Статья представляет VLM-R^3 - фреймворк, который улучшает способность мультимодальных языковых моделей (MLLM) к рассуждениям на основе визуальной информации. Ключевой компонент - метод обучения R-GRPO, который учит модель выбирать информативные регионы изображения и интегрировать их в процесс рассуждений. Для начального обучения создан специальный датасет VLIR с пошаговой разметкой выбора регионов и текстовых обоснований. Эксперименты показывают, что VLM-R^3 достигает нового уровня производительности на ряде бенчмарков, особенно для задач, требующих тонкого пространственного анализа изображений.'}, 'en': {'title': 'Enhancing Visual Reasoning in Language Models with VLM-R^3', 'desc': 'This paper presents VLM-R^3, a new framework that enhances reasoning in multi-modal language models (MLLMs) by integrating visual evidence more effectively. It allows the model to identify when it needs more visual information, where to focus within an image, and how to incorporate this visual context into its reasoning process. The training method, Region-Conditioned Reinforcement Policy Optimization (R-GRPO), incentivizes the model to select informative image regions and apply transformations like cropping or zooming. The results demonstrate that VLM-R^3 outperforms previous models on tasks requiring detailed spatial reasoning and visual cue extraction, achieving state-of-the-art performance in various benchmarks.'}, 'zh': {'title': '视觉与语言的深度融合', 'desc': '最近，基于推理的多模态语言模型（MLLMs）在生成长文本推理链方面取得了一定成功。然而，它们在处理复杂任务时仍然面临挑战，这些任务需要动态和迭代地关注和重新审视视觉区域，以实现文本推理与视觉证据的精确结合。我们提出了VLM-R^3（带有区域识别和推理的视觉语言模型），该框架使MLLM具备了决定何时需要额外视觉证据、确定图像中应聚焦的位置以及将相关子图像内容无缝融入交错思维链的能力。我们的核心方法是区域条件强化策略优化（R-GRPO），通过奖励模型选择信息丰富的区域、制定适当的变换（如裁剪、缩放）并将结果视觉上下文整合到后续推理步骤中，来训练该策略。'}}}, {'id': 'https://huggingface.co/papers/2505.15960', 'title': 'Training Step-Level Reasoning Verifiers with Formal Verification Tools', 'url': 'https://huggingface.co/papers/2505.15960', 'abstract': 'FoVer is a method for automatically annotating step-level error labels using formal verification tools to train Process Reward Models, which significantly improves cross-task generalization and outperforms human-annotated methods in various reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs), which provide step-by-step feedback on the reasoning generated by Large Language Models (LLMs), are receiving increasing attention. However, two key research gaps remain: collecting accurate step-level error labels for training typically requires costly human annotation, and existing PRMs are limited to math reasoning problems. In response to these gaps, this paper aims to address the challenges of automatic dataset creation and the generalization of PRMs to diverse reasoning tasks. To achieve this goal, we propose FoVer, an approach for training PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 for formal logic and Isabelle for theorem proof, which provide automatic and accurate verification for symbolic tasks. Using this approach, we synthesize a training dataset with error labels on LLM responses for formal logic and theorem proof tasks without human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we observe that LLM-based PRMs trained on our dataset exhibit cross-task generalization, improving verification across diverse reasoning tasks. Specifically, PRMs trained with FoVer significantly outperform baseline PRMs based on the original LLMs and achieve competitive or superior results compared to state-of-the-art PRMs trained on labels annotated by humans or stronger models, as measured by step-level verification on ProcessBench and Best-of-K performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU, and BBH. The datasets, models, and code are provided at https://github.com/psunlpgroup/FoVer.', 'score': 6, 'issue_id': 3926, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'fd1009a733cb10f6', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Nan Zhang', 'Sarkar Snigdha Sarathi Das', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15960.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#optimization', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Автоматическое обучение моделей вознаграждения для улучшения рассуждений ИИ', 'desc': 'FoVer - это метод автоматической аннотации ошибок на уровне шагов с использованием инструментов формальной верификации для обучения моделей вознаграждения процессов (Process Reward Models). Данный подход значительно улучшает обобщение между задачами и превосходит методы с аннотациями от людей в различных тестах на рассуждение. FoVer синтезирует обучающий набор данных с метками ошибок для задач формальной логики и доказательства теорем без участия человека. Модели, обученные с помощью FoVer, демонстрируют способность к обобщению на разнообразные задачи рассуждения, превосходя базовые модели и достигая конкурентоспособных или превосходящих результатов по сравнению с современными моделями, обученными на аннотациях человека.'}, 'en': {'title': 'Automating Error Annotation for Enhanced Reasoning Models', 'desc': 'FoVer is a novel method that automates the annotation of step-level error labels using formal verification tools, which helps in training Process Reward Models (PRMs). This approach addresses the challenges of costly human annotation and expands the applicability of PRMs beyond just math reasoning tasks. By synthesizing a training dataset with accurate error labels for formal logic and theorem proof tasks, FoVer enables PRMs to generalize across various reasoning tasks effectively. The results show that PRMs trained with FoVer outperform traditional methods and achieve competitive performance on multiple reasoning benchmarks.'}, 'zh': {'title': 'FoVer：自动标注提升推理模型的跨任务能力', 'desc': 'FoVer是一种利用形式验证工具自动标注步骤级错误标签的方法，用于训练过程奖励模型（PRMs）。该方法显著提高了跨任务的泛化能力，并在多个推理基准测试中超越了人工标注的方法。FoVer通过自动生成的错误标签，解决了训练中需要昂贵人工标注的问题，并扩展了PRMs在多种推理任务中的应用。实验结果表明，使用FoVer训练的PRMs在步骤级验证上表现优异，超越了基于原始大型语言模型的基线PRMs。'}}}, {'id': 'https://huggingface.co/papers/2505.11711', 'title': 'Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.11711', 'abstract': 'Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.', 'score': 6, 'issue_id': 3914, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'e7296e89ef67015f', 'authors': ['Sagnik Mukherjee', 'Lifan Yuan', 'Dilek Hakkani-Tur', 'Hao Peng'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.11711.jpg', 'data': {'categories': ['#rlhf', '#training', '#rl', '#optimization', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение языковых моделей: меньше параметров, больше результат', 'desc': "Это исследование показывает, что методы обучения с подкреплением (RL) значительно улучшают производительность больших языковых моделей (LLM) при решении задач и их соответствие человеческим ценностям. Удивительно, но такие улучшения достигаются путем обновления только небольшой подсети, составляющей 5-30% параметров модели. Этот феномен, названный 'разреженностью обновления параметров', наблюдается для различных алгоритмов RL и семейств LLM без применения явных методов разреженности. Анализ показывает, что обновления затрагивают почти все матрицы параметров, но остаются разреженными и полноранговыми."}, 'en': {'title': 'Efficient Reinforcement Learning: Small Updates, Big Gains!', 'desc': "This paper explores how reinforcement learning (RL) can enhance the performance of large language models (LLMs) by making minimal updates to a small subnetwork of parameters. Remarkably, only 5 to 30 percent of the model's parameters are adjusted, while the majority remain unchanged, a phenomenon termed 'parameter update sparsity.' This sparsity occurs across various RL algorithms and LLMs, indicating a consistent pattern in how RL influences model training. The findings suggest that even with limited updates, the subnetwork can achieve performance comparable to full finetuning, highlighting the efficiency of RL in optimizing LLMs."}, 'zh': {'title': '强化学习：小更新，大提升', 'desc': '强化学习（RL）在大型语言模型（LLM）中通过最小的参数更新显著提升了下游任务的表现和与人类价值观的对齐。令人惊讶的是，这种显著的提升仅通过更新占参数5%到30%的小子网络实现，其余参数基本保持不变。我们称这种现象为由RL引起的参数更新稀疏性。实验表明，这种稀疏性在七种广泛使用的RL算法和十种不同家族的LLM中普遍存在，且不需要任何显式的稀疏促进正则化或架构约束。'}}}, {'id': 'https://huggingface.co/papers/2505.16186', 'title': 'SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning', 'url': 'https://huggingface.co/papers/2505.16186', 'abstract': "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.", 'score': 5, 'issue_id': 3918, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'a47a9eb4edda38b3', 'authors': ['Kaiwen Zhou', 'Xuandong Zhao', 'Gaowen Liu', 'Jayanth Srinivasa', 'Aosong Feng', 'Dawn Song', 'Xin Eric Wang'], 'affiliations': ['Cisco Research', 'UC Berkeley', 'UC Santa Cruz', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16186.jpg', 'data': {'categories': ['#benchmark', '#security', '#training', '#reasoning', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'SafeKey: Активация безопасности в ключевой момент рассуждений ИИ', 'desc': "Статья представляет метод SafeKey для повышения безопасности крупных моделей рассуждений (LRM). Метод фокусируется на активации 'момента озарения' в ключевом предложении с помощью двухпутевой головки безопасности и моделирования маскировки запроса. SafeKey улучшает генерализацию моделей на вредоносные запросы и атаки. Эксперименты показывают значительное повышение безопасности при сохранении общих способностей модели."}, 'en': {'title': 'Activating Safety Moments in Large Reasoning Models', 'desc': "SafeKey is a method designed to improve the safety of large reasoning models (LRMs) by focusing on a critical moment in the model's reasoning process, known as the safety aha moment. This moment occurs in the key sentence, which helps the model determine whether it can respond safely to a query. SafeKey employs a Dual-Path Safety Head to strengthen the safety signals in the model's internal representations and a Query-Mask Modeling approach to enhance the model's attention to safety-related aspects of the query. Through experiments, SafeKey has shown to significantly reduce the risk of harmful responses while preserving the model's overall performance."}, 'zh': {'title': 'SafeKey：激活安全时刻，提升推理模型安全性', 'desc': 'SafeKey通过激活关键句中的安全时刻来增强大型推理模型的安全性。它采用双路径安全头和查询掩码建模，旨在提高模型对有害提示的泛化能力。研究发现，关键句中的安全时刻能够引导模型做出安全的响应。实验结果表明，SafeKey显著降低了模型在各种攻击下的有害性，同时保持了其一般能力。'}}}, {'id': 'https://huggingface.co/papers/2505.13344', 'title': 'RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE\n  Optimization on Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.13344', 'abstract': "RoPECraft is a training-free method that modifies rotary positional embeddings in diffusion transformers to transfer motion from reference videos, enhancing text-guided video generation and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.", 'score': 5, 'issue_id': 3932, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'e47accfd208862f5', 'authors': ['Ahmet Berke Gokmen', 'Yigit Ekin', 'Bahri Batuhan Bilecen', 'Aysegul Dundar'], 'affiliations': ['Bilkent University, Department of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2505.13344.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Перенос движения в видео без обучения', 'desc': 'RoPECraft - это метод переноса движения в видео для диффузионных трансформеров, не требующий дополнительного обучения. Он работает путем модификации позиционных эмбеддингов RoPE на основе оптического потока из референсного видео. Метод оптимизирует эмбеддинги во время шагов шумоподавления, используя выравнивание траекторий и регуляризацию на основе фазовых компонентов преобразования Фурье. Эксперименты показывают, что RoPECraft превосходит существующие методы как качественно, так и количественно.'}, 'en': {'title': 'Seamless Motion Transfer in Video Generation with RoPECraft', 'desc': 'RoPECraft is a novel method that enhances video generation by transferring motion from reference videos without requiring additional training. It modifies rotary positional embeddings (RoPE) in diffusion transformers to incorporate motion information extracted from dense optical flow. The method aligns predicted and target velocities during the denoising process, ensuring that the generated video matches the intended motion. Additionally, it uses a regularization technique based on Fourier transform phase components to maintain fidelity to text prompts and minimize artifacts in the output.'}, 'zh': {'title': 'RoPECraft：无训练的视频运动转移新方法', 'desc': 'RoPECraft是一种无需训练的方法，通过修改扩散变换器中的旋转位置嵌入（RoPE）来实现视频运动转移。该方法首先从参考视频中提取密集光流，并利用运动偏移量来扭曲RoPE的复指数张量，从而将运动信息编码到生成过程中。在去噪时间步骤中，通过使用流匹配目标对预测速度和目标速度进行轨迹对齐，进一步优化这些嵌入。为了保持输出与文本提示的一致性并防止重复生成，我们引入了基于参考视频傅里叶变换相位分量的正则化项，以抑制高频伪影。'}}}, {'id': 'https://huggingface.co/papers/2505.16612', 'title': 'Steering Large Language Models for Machine Translation Personalization', 'url': 'https://huggingface.co/papers/2505.16612', 'abstract': 'Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.', 'score': 4, 'issue_id': 3922, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '5695106d35c6955a', 'authors': ['Daniel Scalena', 'Gabriele Sarti', 'Arianna Bisazza', 'Elisabetta Fersini', 'Malvina Nissim'], 'affiliations': ['CLCG, University of Groningen', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2505.16612.jpg', 'data': {'categories': ['#low_resource', '#training', '#multimodal', '#machine_translation'], 'emoji': '🎭', 'ru': {'title': 'Персонализация переводов LLM: от промптов к латентным концепциям', 'desc': 'Статья исследует стратегии персонализации переводов, генерируемых большими языковыми моделями (LLM), в условиях ограниченных ресурсов, особенно в области литературного перевода. Авторы изучают методы промптинга и вмешательства во время вывода для направления генераций модели к персонализированному стилю. Они предлагают контрастивный фреймворк, использующий латентные концепции из разреженных автоэнкодеров для выявления важных свойств персонализации. Результаты показывают, что направление генераций достигает сильной персонализации при сохранении качества перевода.'}, 'en': {'title': 'Personalized Translations Made Easy with LLMs!', 'desc': "This paper discusses methods to improve personalized translations using large language models (LLMs) in situations where resources are limited. It highlights the use of prompting strategies and a contrastive framework that leverages latent concepts from sparse autoencoders to enhance stylistic personalization. The authors demonstrate that these techniques can effectively guide the model's output while maintaining high translation quality. Additionally, they analyze how these personalization methods influence the internal representations of the LLM, indicating that similar mechanisms are at work in both prompting and steering approaches."}, 'zh': {'title': '个性化翻译的新策略', 'desc': '本文探讨了在低资源环境下，如何通过提示和对比框架来个性化大型语言模型（LLM）的翻译。研究表明，利用稀疏自编码器提取的潜在概念，可以有效识别个性化特征，从而改善翻译质量。我们提出的策略在文学翻译领域表现出色，能够在保持翻译质量的同时，实现强烈的个性化效果。此外，研究还发现，个性化相关的模型层在多次提示和我们的引导方法下受到的影响相似，表明两者可能存在相似的机制。'}}}, {'id': 'https://huggingface.co/papers/2505.17019', 'title': 'Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework', 'url': 'https://huggingface.co/papers/2505.17019', 'abstract': 'LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.  \t\t\t\t\tAI-generated summary \t\t\t\t Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.', 'score': 3, 'issue_id': 3922, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '9284570e4cba3821', 'authors': ['Chenhao Zhang', 'Yazhe Niu'], 'affiliations': ['Huazhong University of Science and Technology', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17019.jpg', 'data': {'categories': ['#reasoning', '#science', '#multimodal', '#cv', '#benchmark', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'LAD: Революция в понимании подтекста изображений искусственным интеллектом', 'desc': 'Статья представляет новый фреймворк LAD для понимания и рассуждения о подтексте изображений, используя модель GPT-4o-mini. LAD применяет трехэтапный подход, включающий восприятие, поиск и рассуждение, для преодоления ограничений существующих мультимодальных языковых моделей. Фреймворк достигает наилучших результатов в задачах понимания подтекста изображений на разных языках и типах вопросов. LAD демонстрирует значительное улучшение производительности по сравнению с другими моделями, особенно в задачах с открытыми вопросами.'}, 'en': {'title': 'Unlocking Image Meanings with LAD Framework', 'desc': 'The paper introduces Let Androids Dream (LAD), a three-stage framework designed to enhance image implication understanding and reasoning using the lightweight GPT-4o-mini model. It addresses the limitations of existing multimodal large language models (MLLMs) in grasping the nuanced meanings of images by implementing a structured approach: Perception, Search, and Reasoning. This framework converts visual data into detailed textual representations, integrates cross-domain knowledge to clarify ambiguities, and generates contextually aligned implications through reasoning. LAD demonstrates state-of-the-art performance on various benchmarks, significantly improving image implication tasks in both English and Chinese, and offers valuable insights for advancing vision-language reasoning.'}, 'zh': {'title': '让安卓梦见电羊：图像隐含理解的新突破', 'desc': 'LAD是一个三阶段框架，利用GPT-4o-mini模型，在图像隐含理解和推理任务中取得了最先进的表现。该框架通过感知、搜索和推理三个步骤，解决了现有模型在理解图像隐含意义时的上下文缺失问题。LAD能够将视觉信息转化为丰富的文本表示，并通过跨领域知识的整合来消除歧义。我们的研究为AI更有效地解读图像隐含意义提供了新见解，推动了视觉-语言推理和人机交互领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2505.15517', 'title': 'Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets', 'url': 'https://huggingface.co/papers/2505.15517', 'abstract': 'Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.', 'score': 3, 'issue_id': 3917, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '7c3b47e3a7b062f1', 'authors': ['Kaiyuan Chen', 'Shuangyu Xie', 'Zehan Ma', 'Ken Goldberg'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.15517.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#games', '#dataset', '#3d', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Роботы учат ИИ видеть и понимать мир', 'desc': 'Robo2VLM - это фреймворк для создания наборов данных визуальных вопросов и ответов на основе траекторий робота. Он использует сенсорные модальности и понимание 3D-свойств для улучшения и оценки визуально-языковых моделей (VLM). Фреймворк сегментирует траекторию робота на фазы манипуляций и генерирует репрезентативные вопросы с несколькими вариантами ответов на основе пространственных, целевых и интерактивных шаблонов рассуждений. Авторы создали большой набор данных Robo2VLM-1, охватывающий 463 сцены и 3396 задач роботизированных манипуляций из 176 тысяч реальных траекторий роботов.'}, 'en': {'title': 'Enhancing VLMs with Robot Trajectory Insights', 'desc': "Robo2VLM is a framework designed to create Visual Question Answering (VQA) datasets by utilizing data from robot trajectories. It enhances Vision-Language Models (VLMs) by integrating sensory information and understanding 3D properties related to robotic tasks. The framework segments robot movements into phases and generates questions based on the robot's interactions with its environment. The resulting dataset, Robo2VLM-1, includes a vast number of questions that help evaluate and improve the reasoning abilities of VLMs in spatial and interaction contexts."}, 'zh': {'title': '利用机器人轨迹数据提升视觉语言模型的能力', 'desc': 'Robo2VLM是一个用于生成视觉问答数据集的框架，利用机器人轨迹数据来增强和评估视觉语言模型（VLM）。该框架通过分析机器人的传感器数据和3D属性理解，生成与机器人操作相关的问答数据。Robo2VLM将机器人轨迹分为多个操作阶段，并在每个阶段识别任务目标和目标物体的3D属性。最终，Robo2VLM-1数据集包含684,710个问题，涵盖463个不同场景和3,396个机器人操作任务，能够有效评估和提升VLM在空间和交互推理方面的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.15865', 'title': 'How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads', 'url': 'https://huggingface.co/papers/2505.15865', 'abstract': 'The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images.', 'score': 3, 'issue_id': 3919, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '4cf7128eaead82ce', 'authors': ['Ingeol Baek', 'Hwan Chang', 'Sunghyun Ryu', 'Hwanhee Lee'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea', 'Department of Computer Engineering, Sejong University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.15865.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие секретов OCR в мультимодальных нейросетях', 'desc': 'Исследование посвящено анализу OCR-компонентов в больших мультимодальных языковых моделях (LVLM). Авторы выявили уникальные паттерны активации этих компонентов при интерпретации текста на изображениях. Обнаружено, что OCR-компоненты менее разрежены, качественно отличаются от обычных компонентов извлечения информации и имеют статическую активацию. Применение метода цепочки рассуждений (CoT) и маскирование OCR-компонентов позволило валидировать результаты на практических задачах.'}, 'en': {'title': 'Unlocking Text Recognition in Images with OCR Heads', 'desc': "This paper investigates the Optical Character Recognition (OCR) Heads in Large Vision Language Models (LVLMs) to understand how they process text in images. It reveals that these heads are less sparse, meaning many of them activate simultaneously to extract text, unlike traditional retrieval heads. The study also finds that OCR heads have distinct properties, showing low similarity to general retrieval heads, and their activation frequency correlates with their OCR performance. By applying techniques like Chain-of-Thought and redistributing values within these heads, the research enhances the models' ability to interpret text in images, shedding light on their internal workings."}, 'zh': {'title': '揭示大型视觉语言模型中的OCR头', 'desc': '本研究分析了大型视觉语言模型中的光学字符识别头（OCR头），揭示了它们在图像中文本解读中的独特激活模式和角色。尽管大型视觉语言模型（LVLMs）取得了显著进展，但在可解释性方面仍存在差距，尤其是在定位和解读图像中的文本信息时。我们发现，OCR头的激活方式与传统的检索头显著不同，且在提取文本信息时激活的头数量较多。通过对这些头的研究，我们加深了对LVLMs在处理图像中嵌入文本信息时内部机制的理解。'}}}, {'id': 'https://huggingface.co/papers/2505.14462', 'title': 'RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture\n  Understanding', 'url': 'https://huggingface.co/papers/2505.14462', 'abstract': 'RAVENEA, a retrieval-augmented benchmark, enhances visual culture understanding in VLMs through culture-focused tasks and outperforms non-augmented models across various metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t As vision-language models (VLMs) become increasingly integrated into daily life, the need for accurate visual culture understanding is becoming critical. Yet, these models frequently fall short in interpreting cultural nuances effectively. Prior work has demonstrated the effectiveness of retrieval-augmented generation (RAG) in enhancing cultural understanding in text-only settings, while its application in multimodal scenarios remains underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a new benchmark designed to advance visual culture understanding through retrieval, focusing on two tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). RAVENEA extends existing datasets by integrating over 10,000 Wikipedia documents curated and ranked by human annotators. With RAVENEA, we train and evaluate seven multimodal retrievers for each image query, and measure the downstream impact of retrieval-augmented inputs across fourteen state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented with culture-aware retrieval, outperform their non-augmented counterparts (by at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the value of retrieval-augmented methods and culturally inclusive benchmarks for multimodal understanding.', 'score': 3, 'issue_id': 3926, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '49a96a5bbddf2965', 'authors': ['Jiaang Li', 'Yifei Yuan', 'Wenyan Li', 'Mohammad Aliannejadi', 'Daniel Hershcovich', 'Anders Søgaard', 'Ivan Vulić', 'Wenxuan Zhang', 'Paul Pu Liang', 'Yang Deng', 'Serge Belongie'], 'affiliations': ['ETH Zürich', 'Massachusetts Institute of Technology', 'Singapore Management University', 'Singapore University of Technology and Design', 'University of Amsterdam', 'University of Cambridge', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2505.14462.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#interpretability', '#multimodal', '#rag', '#games', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Культурный контекст улучшает понимание изображений ИИ', 'desc': 'RAVENEA - это новый бенчмарк для улучшения понимания визуальной культуры в мультимодальных моделях с помощью метода извлечения информации. Он включает в себя задачи ответов на вопросы о культуре по изображениям (cVQA) и создания описаний изображений с учетом культурного контекста (cIC). Бенчмарк содержит более 10 000 документов из Википедии, отобранных и ранжированных аннотаторами. Результаты показывают, что легковесные модели компьютерного зрения с дополнительной культурной информацией превосходят обычные модели на 3.2% в cVQA и 6.2% в cIC.'}, 'en': {'title': 'Enhancing Visual Culture Understanding with RAVENEA', 'desc': 'RAVENEA is a new benchmark designed to improve visual culture understanding in vision-language models (VLMs) through retrieval-augmented techniques. It focuses on two main tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). By integrating over 10,000 curated Wikipedia documents, RAVENEA enhances the training and evaluation of multimodal retrievers for image queries. The results show that VLMs using this retrieval-augmented approach significantly outperform those that do not, demonstrating the importance of culturally aware data in machine learning.'}, 'zh': {'title': 'RAVENEA：提升视觉文化理解的检索增强基准', 'desc': 'RAVENEA是一个增强视觉文化理解的基准，专注于文化相关的任务，提升了视觉语言模型（VLMs）的表现。该基准通过检索增强生成（RAG）方法，解决了模型在理解文化细微差别方面的不足。RAVENEA包含超过10,000个经过人工标注和排名的维基百科文档，支持文化聚焦的视觉问答和图像描述任务。研究表明，经过文化意识检索增强的轻量级VLMs在多个指标上优于未增强的模型，显示了检索增强方法在多模态理解中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.14395', 'title': 'MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language', 'url': 'https://huggingface.co/papers/2505.14395', 'abstract': "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks (r > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.", 'score': 3, 'issue_id': 3921, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '88fb6eaee50e6cb2', 'authors': ['Seyoung Song', 'Seogyeong Jeong', 'Eunsu Kim', 'Jiho Jin', 'Dongkwan Kim', 'Jay Shin', 'Alice Oh'], 'affiliations': ['KAIST', 'Trillion Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.14395.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#low_resource', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Универсальная оценка многоязычных LLM без специальных инструментов', 'desc': 'MUG-Eval - это новый метод оценки способностей больших языковых моделей (LLM) к многоязычной генерации текста. Он трансформирует существующие тесты в разговорные задачи и измеряет точность LLM при их выполнении. Этот подход не зависит от языкоспецифичных инструментов обработки естественного языка и не полагается на оценку другими LLM. MUG-Eval показывает сильную корреляцию с установленными эталонами и позволяет проводить стандартизированные сравнения между языками и моделями.'}, 'en': {'title': 'MUG-Eval: A Language-Independent Tool for Multilingual LLM Assessment', 'desc': 'MUG-Eval is a new framework designed to evaluate the multilingual generation capabilities of large language models (LLMs). It transforms existing benchmarks into conversational tasks, allowing for a language-independent assessment of LLM performance. By measuring the success rate of these tasks, MUG-Eval provides a proxy for effective conversation generation without relying on specific NLP tools or annotated datasets. The framework has been tested on 8 LLMs across 30 languages and shows strong correlation with established benchmarks, making it a valuable tool for evaluating LLMs in low-resource languages.'}, 'zh': {'title': 'MUG-Eval：多语言生成的评估新方法', 'desc': 'MUG-Eval是一个评估大型语言模型（LLMs）多语言生成能力的新框架。它通过将现有基准转化为对话任务，来测量LLMs在这些任务上的准确性。该方法不依赖于特定语言的自然语言处理工具或标注数据集，适用于多种语言。我们的研究表明，MUG-Eval与已有基准的相关性很强，能够为多语言生成提供标准化的比较。'}}}, {'id': 'https://huggingface.co/papers/2505.16170', 'title': 'When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction', 'url': 'https://huggingface.co/papers/2505.16170', 'abstract': 'LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.  \t\t\t\t\tAI-generated summary \t\t\t\t Can large language models (LLMs) admit their mistakes when they should know better? In this work, we define the behavior of acknowledging errors in previously generated answers as "retraction" and aim to understand when and why LLMs choose to retract. We first construct model-specific datasets to evaluate whether a model will retract an incorrect answer that contradicts its own parametric knowledge. While LLMs are capable of retraction, they do so only infrequently. We demonstrate that retraction is closely tied to previously identified indicators of models\' internal belief: models fail to retract wrong answers that they "believe" to be factually correct. Steering experiments further demonstrate that internal belief causally influences model retraction. In particular, when the model does not believe its answer, this not only encourages the model to attempt to verify the answer, but also alters attention behavior during self-verification. Finally, we demonstrate that simple supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. Code and datasets are available on https://github.com/ayyyq/llm-retraction.', 'score': 2, 'issue_id': 3919, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '2f89bb0f5c59846a', 'authors': ['Yuqing Yang', 'Robin Jia'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.16170.jpg', 'data': {'categories': ['#training', '#alignment', '#hallucinations', '#dataset'], 'emoji': '🤔', 'ru': {'title': 'Учим ИИ признавать свои ошибки', 'desc': "Это исследование посвящено способности больших языковых моделей (LLM) признавать свои ошибки, что авторы называют 'ретракцией'. Эксперименты показали, что LLM редко отказываются от неверных ответов, которые они считают правильными. Авторы обнаружили, что ретракция тесно связана с внутренними убеждениями модели. Простая контролируемая тонкая настройка значительно улучшает способность моделей к ретракции, помогая им формировать более точные внутренние убеждения."}, 'en': {'title': 'Enhancing LLMs: Fine-Tuning for Better Error Retraction', 'desc': "This paper investigates how large language models (LLMs) handle the acknowledgment of their mistakes, a behavior termed 'retraction'. It finds that LLMs rarely retract incorrect answers, especially when they are confident in their incorrect beliefs. The study shows that the ability to retract is influenced by the model's internal beliefs, where models are less likely to retract answers they consider factually correct. Additionally, the authors demonstrate that supervised fine-tuning can enhance the retraction capabilities of LLMs by refining their internal belief systems."}, 'zh': {'title': '提升模型的错误撤回能力', 'desc': '本研究探讨了大型语言模型（LLMs）在面对错误时的自我修正能力，称之为“撤回”。我们发现，LLMs在认为自己的答案是正确时，往往不会撤回错误的回答。通过构建特定的数据集，我们评估了模型在何种情况下会进行撤回，并发现内部信念对撤回行为有重要影响。最后，我们通过监督微调显著提高了模型的撤回性能，使其能够学习更准确的内部信念。'}}}, {'id': 'https://huggingface.co/papers/2505.16088', 'title': 'Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.16088', 'abstract': 'Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future regimes; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year rightarrow month rightarrow day).', 'score': 2, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'e95fb5fdc583b428', 'authors': ['Gagan Bhatia', 'Maxime Peyrard', 'Wei Zhao'], 'affiliations': ['University of Aberdeen', 'Université Grenoble Alpes & CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2505.16088.jpg', 'data': {'categories': ['#reasoning', '#data', '#dataset', '#benchmark', '#interpretability'], 'emoji': '🗓️', 'ru': {'title': 'Преодоление фрагментации дат в языковых моделях', 'desc': "Статья посвящена проблеме фрагментации дат современными токенизаторами в области обработки естественного языка. Авторы вводят метрику 'коэффициент фрагментации дат' и представляют набор данных DateAugBench для оценки временных рассуждений моделей. Исследование выявляет механизм абстракции дат в больших языковых моделях, который восстанавливает фрагментированные компоненты дат. Результаты показывают, что чрезмерная фрагментация может снижать точность моделей на редких датах, а более крупные модели быстрее справляются с абстракцией дат."}, 'en': {'title': 'Preserving Date Integrity for Better Temporal Reasoning', 'desc': 'This paper addresses the issue of how modern Byte Pair Encoding (BPE) tokenizers break down calendar dates into smaller parts, which can hinder effective temporal reasoning in language models. The authors propose a new metric called the date fragmentation ratio to evaluate how well tokenizers maintain the integrity of multi-digit date components. They also introduce DateAugBench, a dataset designed for testing temporal reasoning across various tasks involving dates. The findings reveal that excessive fragmentation can lead to significant drops in accuracy, especially for less common dates, and that larger models are better at reconstructing these fragmented dates for reasoning tasks.'}, 'zh': {'title': '提升日期推理的分词器设计', 'desc': '现代的BPE分词器常常将日期分割成无意义的碎片，例如将20250312分割为202、503和12，这样会增加标记数量并掩盖进行时间推理所需的内在结构。本文提出了一种简单且可解释的度量标准，称为日期碎片化比率，用于衡量分词器保留多位数日期组件的忠实度。我们还发布了DateAugBench，这是一个包含6500个示例的套件，涵盖了三种时间推理任务：基于上下文的日期解析、格式不变性难题和历史、当代及未来的日期算术。实验表明，过度的碎片化与不常见日期（如历史和未来日期）的准确性下降高达10个百分点相关，且模型越大，修复日期碎片的抽象机制越快。'}}}, {'id': 'https://huggingface.co/papers/2505.15263', 'title': 'gen2seg: Generative Models Enable Generalizable Instance Segmentation', 'url': 'https://huggingface.co/papers/2505.15263', 'abstract': "Generative models fine-tuned for instance segmentation demonstrate strong zero-shot performance on unseen objects and styles, surpassing discriminatively pretrained models.  \t\t\t\t\tAI-generated summary \t\t\t\t By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website.", 'score': 2, 'issue_id': 3936, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '01de6ec5d8c92cb4', 'authors': ['Om Khangaonkar', 'Hamed Pirsiavash'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.15263.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#cv', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Генеративные модели раскрывают скрытый потенциал в сегментации объектов', 'desc': 'Данная статья посвящена использованию генеративных моделей для задачи сегментации объектов. Авторы обнаружили, что генеративные модели, дообученные на ограниченном наборе типов объектов, демонстрируют сильную обобщающую способность при сегментации неизвестных объектов и стилей. Такие модели превосходят дискриминативно предобученные архитектуры и приближаются по качеству к полностью supervised моделям вроде SAM. Это указывает на то, что генеративные модели в процессе предобучения неявно изучают универсальные механизмы группировки объектов, применимые к разным категориям и доменам.'}, 'en': {'title': 'Generative Models Excel in Zero-Shot Instance Segmentation', 'desc': 'This paper explores how generative models, specifically those fine-tuned for instance segmentation, can effectively identify and segment objects that were not seen during training. By leveraging a technique called instance coloring loss, the authors adapt models like Stable Diffusion and MAE to work with a limited set of object types, such as indoor furniture and cars. Remarkably, these models demonstrate strong zero-shot performance, meaning they can accurately segment new object types and styles that they have never encountered before. The findings suggest that generative models possess a robust understanding of object boundaries and scene composition, allowing them to generalize better than traditional discriminative models.'}, 'zh': {'title': '生成模型在实例分割中的强大泛化能力', 'desc': '这篇论文探讨了生成模型在实例分割任务中的应用，特别是在未见物体和风格上的零-shot性能表现。通过对生成模型进行微调，研究者们发现这些模型能够有效理解物体边界和场景构成。论文中使用了实例着色损失，专注于室内家具和汽车等特定物体类型，结果显示模型在未见物体上也能准确分割。与传统的判别式预训练模型相比，生成模型展现出更强的跨类别和领域的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.16048', 'title': 'SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution', 'url': 'https://huggingface.co/papers/2505.16048', 'abstract': 'A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.', 'score': 1, 'issue_id': 3920, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'cf44ff3c901f7498', 'authors': ['Philipp D. Siedler'], 'affiliations': ['Aleph Alpha Research, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.16048.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset'], 'emoji': '🏗️', 'ru': {'title': 'Испытание ИИ на прочность: топологическая оптимизация без симуляций', 'desc': 'Представлен новый набор данных для оценки физических и пространственных рассуждений больших языковых моделей на основе задач топологической оптимизации. Модели должны предсказывать оптимальное распределение материала в 2D-пространстве при заданных нагрузках и опорах. Задачи включают заполнение замаскированных областей в частичных структурах и прогнозирование полных распределений материала. Этот датасет оценивает способность моделей рассуждать о структурной стабильности и пространственной организации без использования инструментов симуляции.'}, 'en': {'title': 'Benchmarking LLMs in Spatial and Physical Reasoning with Topology Optimization', 'desc': 'This paper presents a new dataset aimed at evaluating the spatial and physical reasoning skills of Large Language Models (LLMs) through topology optimization tasks. The tasks require LLMs to analyze conditions like 2D boundaries and applied forces to determine optimal material distributions without using simulation tools. By challenging models to predict material layouts and understand force flows, the dataset assesses their ability to reason about structural stability and spatial organization. This approach provides a unique perspective on LLM performance, complementing existing benchmarks focused on language and logic.'}, 'zh': {'title': '评估大型语言模型的空间与物理推理能力', 'desc': '本文介绍了一个新数据集，旨在评估大型语言模型（LLM）在拓扑优化任务中的空间和物理推理能力。该数据集提供了2D边界、施加的力和支撑条件，LLM需要推理出最佳的材料分布。任务包括填补部分结构中的缺失区域和预测完整的材料分布，要求模型理解在给定约束下的力流和材料分布。这个数据集为评估2D环境中的空间和物理推理能力提供了新的视角，补充了传统的语言和逻辑基准。'}}}, {'id': 'https://huggingface.co/papers/2505.13237', 'title': 'SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based\n  on Speech and Audio Information', 'url': 'https://huggingface.co/papers/2505.13237', 'abstract': "SAKURA is introduced to evaluate the multi-hop reasoning abilities of large audio-language models, revealing their struggles in integrating speech/audio representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.", 'score': 0, 'issue_id': 3928, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '657c71fe76ca3155', 'authors': ['Chih-Kai Yang', 'Neo Ho', 'Yen-Ting Piao', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2505.13237.jpg', 'data': {'categories': ['#audio', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🎙️', 'ru': {'title': 'SAKURA: выявление ограничений логики аудио-языковых моделей', 'desc': 'SAKURA - это новый бенчмарк для оценки способностей больших аудио-языковых моделей (LALM) к многоэтапным рассуждениям. Исследование показало, что LALM испытывают трудности с интеграцией речевых и аудио представлений для сложных логических выводов, даже когда они правильно извлекают релевантную информацию. Это выявило фундаментальную проблему в мультимодальных рассуждениях таких моделей. Результаты исследования предоставляют важные insights для дальнейших разработок в области аудио-языковых моделей.'}, 'en': {'title': 'SAKURA: Unveiling the Reasoning Gaps in Audio-Language Models', 'desc': 'The paper introduces SAKURA, a benchmark designed to evaluate the multi-hop reasoning capabilities of large audio-language models (LALMs). It highlights that while LALMs perform well in speech and audio tasks, their ability to integrate multiple pieces of information for reasoning is not well understood. The study reveals that LALMs face significant challenges in combining speech and audio representations for effective multi-hop reasoning. This research identifies a key limitation in LALMs and provides valuable insights for future advancements in multimodal reasoning.'}, 'zh': {'title': 'SAKURA：评估音频语言模型的多跳推理能力', 'desc': 'SAKURA是一个新的基准，用于评估大型音频语言模型在多跳推理方面的能力。这些模型在处理语音和音频任务时表现良好，但在整合多条信息进行推理时却存在困难。研究表明，即使它们能够正确提取相关信息，仍然难以将语音和音频表示结合起来进行多跳推理。这一发现揭示了多模态推理中的一个重要挑战，为未来的研究提供了有价值的见解和资源。'}}}, {'id': 'https://huggingface.co/papers/2505.07062', 'title': 'Seed1.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2505.07062', 'abstract': 'We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)', 'score': 83, 'issue_id': 3722, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 мая', 'en': 'May 11', 'zh': '5月11日'}, 'hash': 'c3406b40cc21820d', 'authors': ['Dong Guo', 'Faming Wu', 'Feida Zhu', 'Fuxing Leng', 'Guang Shi', 'Haobin Chen', 'Haoqi Fan', 'Jian Wang', 'Jianyu Jiang', 'Jiawei Wang', 'Jingji Chen', 'Jingjia Huang', 'Kang Lei', 'Liping Yuan', 'Lishu Luo', 'Pengfei Liu', 'Qinghao Ye', 'Rui Qian', 'Shen Yan', 'Shixiong Zhao', 'Shuai Peng', 'Shuangye Li', 'Sihang Yuan', 'Sijin Wu', 'Tianheng Cheng', 'Weiwei Liu', 'Wenqian Wang', 'Xianhan Zeng', 'Xiao Liu', 'Xiaobo Qin', 'Xiaohan Ding', 'Xiaojun Xiao', 'Xiaoying Zhang', 'Xuanwei Zhang', 'Xuehan Xiong', 'Yanghua Peng', 'Yangrui Chen', 'Yanwei Li', 'Yanxu Hu', 'Yi Lin', 'Yiyuan Hu', 'Yiyuan Zhang', 'Youbin Wu', 'Yu Li', 'Yudong Liu', 'Yue Ling', 'Yujia Qin', 'Zanbo Wang', 'Zhiwu He', 'Aoxue Zhang', 'Bairen Yi', 'Bencheng Liao', 'Can Huang', 'Can Zhang', 'Chaorui Deng', 'Chaoyi Deng', 'Cheng Lin', 'Cheng Yuan', 'Chenggang Li', 'Chenhui Gou', 'Chenwei Lou', 'Chengzhi Wei', 'Chundian Liu', 'Chunyuan Li', 'Deyao Zhu', 'Donghong Zhong', 'Feng Li', 'Feng Zhang', 'Gang Wu', 'Guodong Li', 'Guohong Xiao', 'Haibin Lin', 'Haihua Yang', 'Haoming Wang', 'Heng Ji', 'Hongxiang Hao', 'Hui Shen', 'Huixia Li', 'Jiahao Li', 'Jialong Wu', 'Jianhua Zhu', 'Jianpeng Jiao', 'Jiashi Feng', 'Jiaze Chen', 'Jianhui Duan', 'Jihao Liu', 'Jin Zeng', 'Jingqun Tang', 'Jingyu Sun', 'Joya Chen', 'Jun Long', 'Junda Feng', 'Junfeng Zhan', 'Junjie Fang', 'Junting Lu', 'Kai Hua', 'Kai Liu', 'Kai Shen', 'Kaiyuan Zhang', 'Ke Shen', 'Ke Wang', 'Keyu Pan', 'Kun Zhang', 'Kunchang Li', 'Lanxin Li', 'Lei Li', 'Lei Shi', 'Li Han', 'Liang Xiang', 'Liangqiang Chen', 'Lin Chen', 'Lin Li', 'Lin Yan', 'Liying Chi', 'Longxiang Liu', 'Mengfei Du', 'Mingxuan Wang', 'Ningxin Pan', 'Peibin Chen', 'Pengfei Chen', 'Pengfei Wu', 'Qingqing Yuan', 'Qingyao Shuai', 'Qiuyan Tao', 'Renjie Zheng', 'Renrui Zhang', 'Ru Zhang', 'Rui Wang', 'Rui Yang', 'Rui Zhao', 'Shaoqiang Xu', 'Shihao Liang', 'Shipeng Yan', 'Shu Zhong', 'Shuaishuai Cao', 'Shuangzhi Wu', 'Shufan Liu', 'Shuhan Chang', 'Songhua Cai', 'Tenglong Ao', 'Tianhao Yang', 'Tingting Zhang', 'Wanjun Zhong', 'Wei Jia', 'Wei Weng', 'Weihao Yu', 'Wenhao Huang', 'Wenjia Zhu', 'Wenli Yang', 'Wenzhi Wang', 'Xiang Long', 'XiangRui Yin', 'Xiao Li', 'Xiaolei Zhu', 'Xiaoying Jia', 'Xijin Zhang', 'Xin Liu', 'Xinchen Zhang', 'Xinyu Yang', 'Xiongcai Luo', 'Xiuli Chen', 'Xuantong Zhong', 'Xuefeng Xiao', 'Xujing Li', 'Yan Wu', 'Yawei Wen', 'Yifan Du', 'Yihao Zhang', 'Yining Ye', 'Yonghui Wu', 'Yu Liu', 'Yu Yue', 'Yufeng Zhou', 'Yufeng Yuan', 'Yuhang Xu', 'Yuhong Yang', 'Yun Zhang', 'Yunhao Fang', 'Yuntao Li', 'Yurui Ren', 'Yuwen Xiong', 'Zehua Hong', 'Zehua Wang', 'Zewei Sun', 'Zeyu Wang', 'Zhao Cai', 'Zhaoyue Zha', 'Zhecheng An', 'Zhehui Zhao', 'Zhengzhuo Xu', 'Zhipeng Chen', 'Zhiyong Wu', 'Zhuofan Zheng', 'Zihao Wang', 'Zilong Huang', 'Ziyu Zhu', 'Zuquan Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07062.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#architecture', '#training', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'Компактная мультимодальная модель с выдающимися способностями', 'desc': 'Seed1.5-VL - это мультимодальная модель, сочетающая зрение и язык для общего понимания и рассуждений. Она состоит из энкодера изображений на 532 млн параметров и языковой модели на основе смеси экспертов с 20 млрд активных параметров. Несмотря на компактную архитектуру, модель показывает высокие результаты в широком спектре задач, достигая state-of-the-art на 38 из 60 публичных бенчмарков. Seed1.5-VL особенно эффективна в задачах управления интерфейсами, игровом процессе и визуальных головоломках, превосходя ведущие мультимодальные системы.'}, 'en': {'title': 'Empowering Multimodal Understanding with Seed1.5-VL', 'desc': 'Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles.'}, 'zh': {'title': 'Seed1.5-VL：多模态理解与推理的新突破', 'desc': '我们介绍了Seed1.5-VL，这是一种旨在提升多模态理解和推理的视觉-语言基础模型。Seed1.5-VL由一个532M参数的视觉编码器和一个具有20B活跃参数的专家混合模型（MoE LLM）组成。尽管其架构相对紧凑，但在多个公共VLM基准测试中表现出色，在60个公共基准中有38个达到了最先进的性能。此外，在以代理为中心的任务中，如图形用户界面控制和游戏玩法，Seed1.5-VL超越了领先的多模态系统，包括OpenAI CUA和Claude 3.7。'}}}, {'id': 'https://huggingface.co/papers/2505.07608', 'title': 'MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining', 'url': 'https://huggingface.co/papers/2505.07608', 'abstract': "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.", 'score': 52, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9db5f7b72add3369', 'authors': ['Xiaomi LLM-Core Team', ':', 'Bingquan Xia', 'Bowen Shen', 'Cici', 'Dawei Zhu', 'Di Zhang', 'Gang Wang', 'Hailin Zhang', 'Huaqiu Liu', 'Jiebao Xiao', 'Jinhao Dong', 'Liang Zhao', 'Peidian Li', 'Peng Wang', 'Shihua Yu', 'Shimao Chen', 'Weikun Wang', 'Wenhan Ma', 'Xiangwei Deng', 'Yi Huang', 'Yifan Song', 'Zihan Jiang', 'Bowen Ye', 'Can Cai', 'Chenhong He', 'Dong Zhang', 'Duo Zhang', 'Guoan Wang', 'Hao Tian', 'Haochen Zhao', 'Heng Qu', 'Hongshen Xu', 'Jun Shi', 'Kainan Bao', 'QingKai Fang', 'Kang Zhou', 'Kangyang Zhou', 'Lei Li', 'Menghang Zhu', 'Nuo Chen', 'Qiantong Wang', 'Shaohui Liu', 'Shicheng Li', 'Shuhao Gu', 'Shuhuai Ren', 'Shuo Liu', 'Sirui Deng', 'Weiji Zhuang', 'Weiwei Lv', 'Wenyu Yang', 'Xin Zhang', 'Xing Yong', 'Xing Zhang', 'Xingchen Song', 'Xinzhe Xu', 'Xu Wang', 'Yihan Yan', 'Yu Tu', 'Yuanyuan Tian', 'Yudong Wang', 'Yue Yu', 'Zhenru Lin', 'Zhichao Song', 'Zihao Yue'], 'affiliations': ['Xiaomi LLM-Core Team'], 'pdf_title_img': 'assets/pdf/title_img/2505.07608.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#dataset', '#math', '#rl', '#data', '#training'], 'emoji': '🧠', 'ru': {'title': 'MiMo-7B: Мощная языковая модель для сложных рассуждений', 'desc': 'MiMo-7B - это большая языковая модель, оптимизированная для задач рассуждения. В процессе предварительного обучения использовалась улучшенная обработка данных и трехэтапная стратегия смешивания для усиления потенциала базовой модели. На этапе пост-обучения применялось обучение с подкреплением на наборе из 130 тысяч верифицируемых задач по математике и программированию. Итоговая модель MiMo-7B-RL превосходит более крупные модели в задачах рассуждения, математики и программирования.'}, 'en': {'title': 'MiMo-7B: Revolutionizing Reasoning with Advanced Training Techniques', 'desc': 'MiMo-7B is a large language model specifically designed for reasoning tasks, optimized through both pre-training and post-training processes. In the pre-training phase, it utilizes an advanced data preprocessing pipeline and a three-stage data mixing strategy to enhance its reasoning capabilities, training on a massive dataset of 25 trillion tokens. The post-training phase involves reinforcement learning with a curated dataset of 130,000 math and programming problems, addressing sparse-reward challenges with a code-reward scheme and strategic data resampling. Evaluations demonstrate that MiMo-7B-Base excels in reasoning tasks, outperforming larger models, while the final RL-tuned version, MiMo-7B-RL, achieves outstanding results in mathematics, coding, and general reasoning tasks.'}, 'zh': {'title': 'MiMo-7B：推理任务的强大语言模型', 'desc': '我们介绍了MiMo-7B，这是一个专为推理任务设计的大型语言模型，优化了预训练和后训练阶段。在预训练过程中，我们增强了数据预处理流程，并采用三阶段数据混合策略，以提升基础模型的推理能力。MiMo-7B-Base在25万亿个标记上进行预训练，并增加了多标记预测目标，以提高性能和加速推理速度。在后训练阶段，我们整理了130K个可验证的数学和编程问题数据集，结合测试难度驱动的代码奖励机制，解决稀疏奖励问题，并采用战略性数据重采样来稳定训练。'}}}, {'id': 'https://huggingface.co/papers/2505.07747', 'title': 'Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets', 'url': 'https://huggingface.co/papers/2505.07747', 'abstract': 'While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.', 'score': 46, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'd9ffe741ebae4acb', 'authors': ['Weiyu Li', 'Xuanyang Zhang', 'Zheng Sun', 'Di Qi', 'Hao Li', 'Wei Cheng', 'Weiwei Cai', 'Shihao Wu', 'Jiarui Liu', 'Zihao Wang', 'Xiao Chen', 'Feipeng Tian', 'Jianxiong Pan', 'Zeming Li', 'Gang Yu', 'Xiangyu Zhang', 'Daxin Jiang', 'Ping Tan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07747.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#data', '#diffusion', '#transfer_learning', '#3d', '#benchmark'], 'emoji': '🧊', 'ru': {'title': 'Открытая платформа для AI-генерации 3D-объектов нового поколения', 'desc': 'Статья представляет Step1X-3D - открытую систему для генерации 3D-объектов с использованием искусственного интеллекта. Авторы разработали процесс обработки большого набора данных, создав высококачественный датасет из 2 миллионов 3D-моделей. Архитектура системы включает гибридный VAE-DiT генератор геометрии и диффузионный модуль для синтеза текстур. Step1X-3D демонстрирует высокое качество генерации и позволяет применять методы контроля из 2D-генерации к 3D-синтезу.'}, 'en': {'title': 'Revolutionizing 3D Generation with Step1X-3D', 'desc': 'The paper introduces Step1X-3D, a framework designed to improve 3D generation in artificial intelligence. It tackles challenges like limited data and algorithmic issues by creating a high-quality dataset and employing a two-stage architecture that combines a geometry generator and a texture synthesis module. The framework allows for better detail preservation and consistency in 3D assets by integrating techniques from 2D generation. By providing open-source resources, it aims to enhance research and development in controllable 3D asset generation.'}, 'zh': {'title': 'Step1X-3D：开创可控3D生成的新标准', 'desc': '本论文介绍了Step1X-3D，这是一个开放框架，旨在解决3D生成中的数据稀缺、算法限制和生态系统碎片化等挑战。该框架通过严格的数据整理流程，处理超过500万资产，创建了一个200万高质量数据集，并采用标准化的几何和纹理属性。它结合了混合VAE-DiT几何生成器和基于扩散的纹理合成模块，能够生成高质量的3D模型。Step1X-3D还支持将2D控制技术直接转移到3D合成，推动了可控3D资产生成的新标准。'}}}, {'id': 'https://huggingface.co/papers/2505.07787', 'title': 'Learning from Peers in Reasoning Models', 'url': 'https://huggingface.co/papers/2505.07787', 'abstract': 'Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP\'s robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .', 'score': 34, 'issue_id': 3722, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '350f28f20ab516fc', 'authors': ['Tongxu Luo', 'Wenyu Du', 'Jiaxi Bi', 'Stephen Chung', 'Zhengyang Tang', 'Hao Yang', 'Min Zhang', 'Benyou Wang'], 'affiliations': ['DualityRL', 'Huawei', 'The Chinese University of Hong Kong, Shenzhen', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2505.07787.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#small_models', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Коллективный разум: как модели машинного обучения учатся друг у друга', 'desc': "Исследование показывает, что крупные модели рассуждений (LRM) могут попадать в 'ловушку доминирования префикса', когда плохое начало рассуждения мешает самокоррекции. Для решения этой проблемы предложен метод 'Обучение у сверстников' (LeaP), позволяющий моделям обмениваться промежуточными выводами во время вывода. Авторы также представили серию моделей LeaP-T, настроенных для эффективного следования инструкциям по обобщению и рефлексии. Эксперименты на математических бенчмарках показали значительное улучшение производительности моделей с использованием LeaP."}, 'en': {'title': 'Empowering LRMs through Peer Collaboration', 'desc': "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."}, 'zh': {'title': '同伴学习：提升推理模型的自我纠正能力', 'desc': '大型推理模型（LRMs）具有自我纠正的能力，但当推理过程以短而差的开头开始时，模型很难恢复。我们称这种现象为“前缀主导陷阱”。为了解决这个问题，我们提出了“从同伴学习”（LeaP），通过路由机制让每个推理路径总结其中间推理并与其他路径共享，从而在推理过程中融入同伴的见解。实验结果表明，LeaP显著提高了模型的表现，尤其是在处理不同任务难度时展现出强大的错误容忍能力。'}}}, {'id': 'https://huggingface.co/papers/2505.07447', 'title': 'Unified Continuous Generative Models', 'url': 'https://huggingface.co/papers/2505.07447', 'abstract': 'Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.', 'score': 31, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '8d18ef028e9905b1', 'authors': ['Peng Sun', 'Yi Jiang', 'Tao Lin'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07447.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Единый фреймворк для непрерывных генеративных моделей: от многошаговых до малошаговых', 'desc': 'Статья представляет унифицированный подход к обучению и сэмплированию непрерывных генеративных моделей. Авторы объединяют многошаговые методы, такие как диффузия и flow-matching, с малошаговыми подходами вроде consistency models. Их реализация, UCGM-{T,S}, достигает state-of-the-art результатов на ImageNet 256x256, используя диффузионный трансформер. Кроме того, UCGM-S улучшает производительность предобученной модели, снижая FID и количество шагов сэмплирования.'}, 'en': {'title': 'Unifying Generative Models for Superior Performance', 'desc': 'This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.'}, 'zh': {'title': '统一生成模型，提升生成性能！', 'desc': '本文介绍了一种统一的连续生成模型框架，旨在整合多步和少步生成方法的训练和采样。通过引入统一的训练和采样器（UCGM-{T,S}），我们实现了最先进的生成性能。实验结果表明，在ImageNet数据集上，UCGM-T能够在20步内将多步模型的FID降低到1.30，而少步模型在仅2步内达到1.42的FID。此外，使用UCGM-S对预训练模型进行改进，FID从250步的1.26降至仅40步的1.06。'}}}, {'id': 'https://huggingface.co/papers/2505.06548', 'title': 'REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback', 'url': 'https://huggingface.co/papers/2505.06548', 'abstract': 'Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.', 'score': 26, 'issue_id': 3722, 'pub_date': '2025-05-10', 'pub_date_card': {'ru': '10 мая', 'en': 'May 10', 'zh': '5月10日'}, 'hash': 'db28335cad79db53', 'authors': ['Aniruddha Roy', 'Pretam Ray', 'Abhilash Nandy', 'Somak Aditya', 'Pawan Goyal'], 'affiliations': ['Indian Institute of Technology, Kharagpur'], 'pdf_title_img': 'assets/pdf/title_img/2505.06548.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#small_models', '#rl', '#dataset', '#data'], 'emoji': '🤖', 'ru': {'title': 'Малые модели и RL улучшают генерацию инструкций для обучения больших ЯМ', 'desc': 'Статья исследует эффективность малых открытых языковых моделей (LLaMA 2-7B, LLama 2-13B, Mistral 7B) для полуавтоматической генерации инструкций для обучения больших языковых моделей. Авторы применяют фреймворк, уменьшающий необходимость ручной разметки и затраты на создание датасета инструкций. Они также интегрируют алгоритм обучения с подкреплением (RL) для улучшения результатов. Эксперименты показывают, что предложенный подход с RL превосходит предыдущие методы в 63-66% задач.'}, 'en': {'title': 'Empowering LLMs with Cost-Effective Instruction Generation', 'desc': 'This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods.'}, 'zh': {'title': '高效生成指令数据，提升LLMs性能', 'desc': '这篇论文探讨了指令驱动的大型语言模型（LLMs）在自然语言处理任务中的应用。作者提出了一种半自动化框架，利用开源的小型LLMs（如LLaMA 2-7B、LLaMA 2-13B和Mistral 7B）来生成指令数据集，从而减少人工干预和成本。通过引入基于强化学习的训练算法，研究表明这种方法在63-66%的任务中显著提高了性能。该研究为生成多样化的指令数据提供了一种更高效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2505.07293', 'title': 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection', 'url': 'https://huggingface.co/papers/2505.07293', 'abstract': "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.", 'score': 17, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '7403ae602b400fc4', 'authors': ['Kai Hua', 'Steven Wu', 'Ge Zhang', 'Ke Shen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.07293.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#data', '#optimization', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Улучшение способности ЯМ к рассуждениям через умный отбор данных', 'desc': 'Статья представляет метод AttentionInfluence для отбора данных, улучшающих способности языковых моделей к сложным рассуждениям. Этот метод использует небольшую предобученную модель для выбора данных путем маскирования головок внимания без необходимости в дополнительном обучении или разметке. Авторы применили AttentionInfluence к корпусу SmolLM для предобучения 7B-параметровой модели. Результаты показали значительное улучшение на нескольких бенчмарках, требующих знаний и рассуждений.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with AttentionInfluence', 'desc': 'This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.'}, 'zh': {'title': '无监督推理数据选择的新方法', 'desc': '最近，研究者们越来越关注收集推理密集型的预训练数据，以提高大型语言模型（LLMs）的复杂推理能力。以往的方法通常依赖于监督分类器来识别这些数据，这需要人类或LLMs进行标注，常常引入领域特定的偏见。我们提出了一种名为AttentionInfluence的方法，这是一种简单而有效的无监督训练方法。通过简单的注意力头屏蔽操作，我们的方法使得一个小型的预训练语言模型能够作为强大的数据选择器，从而在推理任务中取得显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2505.07818', 'title': 'DanceGRPO: Unleashing GRPO on Visual Generation', 'url': 'https://huggingface.co/papers/2505.07818', 'abstract': 'Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.', 'score': 16, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '023078250e0d651f', 'authors': ['Zeyue Xue', 'Jie Wu', 'Yu Gao', 'Fangyuan Kong', 'Lingting Zhu', 'Mengzhao Chen', 'Zhiheng Liu', 'Wei Liu', 'Qiushan Guo', 'Weilin Huang', 'Ping Luo'], 'affiliations': ['ByteDance Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.07818.jpg', 'data': {'categories': ['#alignment', '#optimization', '#video', '#multimodal', '#rl', '#diffusion', '#benchmark', '#rlhf'], 'emoji': '🎨', 'ru': {'title': 'DanceGRPO: Революция в обучении с подкреплением для генерации визуального контента', 'desc': 'Статья представляет DanceGRPO - унифицированный фреймворк для адаптации Group Relative Policy Optimization (GRPO) к задачам генерации визуального контента. DanceGRPO совместим с различными генеративными парадигмами, задачами, фундаментальными моделями и моделями вознаграждения. Фреймворк демонстрирует значительные улучшения по сравнению с базовыми методами на нескольких бенчмарках. DanceGRPO стабилизирует оптимизацию политики для сложной генерации видео и позволяет генеративной политике лучше захватывать траектории шумоподавления.'}, 'en': {'title': 'DanceGRPO: Unifying Reinforcement Learning for Visual Generation', 'desc': 'This paper presents DanceGRPO, a novel framework that enhances visual content generation by integrating Group Relative Policy Optimization (GRPO) with generative models like diffusion models and rectified flows. It addresses key challenges in reinforcement learning (RL) for visual generation, such as instability during training and compatibility with modern sampling methods. DanceGRPO is versatile, supporting multiple tasks including text-to-image and video generation, and utilizes various foundational and reward models to improve output quality. The framework shows significant performance improvements over existing methods, making it a promising solution for aligning generative models with human preferences in visual synthesis.'}, 'zh': {'title': 'DanceGRPO：视觉生成的统一强化学习框架', 'desc': '本论文介绍了DanceGRPO，这是第一个将群体相对策略优化（GRPO）应用于视觉生成的统一框架。它解决了现有基于强化学习（RL）的方法在现代常微分方程（ODE）采样、训练稳定性和视频生成验证方面的局限性。DanceGRPO能够在扩散模型和修正流等多种生成范式中无缝适应，并在文本到图像、文本到视频和图像到视频等任务中表现出显著的性能提升。该框架在多个基准测试中超越了基线，展示了在视觉生成任务中结合强化学习与人类反馈的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.07263', 'title': 'Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2505.07263', 'abstract': 'We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.', 'score': 15, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9735af402c0df35f', 'authors': ['Xiaokun Wang', 'Chris', 'Jiangbo Pei', 'Wei Shen', 'Yi Peng', 'Yunzhuo Hao', 'Weijie Qiu', 'Ai Jian', 'Tianyidan Xie', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07263.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#alignment', '#open_source', '#training', '#architecture', '#dataset'], 'emoji': '🌟', 'ru': {'title': 'Универсальная мультимодальная модель вознаграждения для улучшения ИИ-рассуждений', 'desc': 'Исследователи представили Skywork-VL Reward - мультимодальную модель вознаграждения для задач понимания и рассуждения. Они создали большой набор данных с предпочтениями для различных мультимодальных сценариев. Архитектура модели основана на Qwen2.5-VL-7B-Instruct с добавлением головы вознаграждения и многоэтапной тонкой настройкой. Skywork-VL Reward показывает высокие результаты на бенчмарках и эффективна для обучения моделей с помощью Mixed Preference Optimization.'}, 'en': {'title': 'Skywork-VL Reward: Advancing Multimodal Understanding and Reasoning', 'desc': 'The paper introduces Skywork-VL Reward, a novel multimodal reward model designed to enhance understanding and reasoning across different types of data, such as text and images. It utilizes a large-scale dataset that captures diverse tasks, with feedback gathered from both traditional vision-language models and more sophisticated reasoning models. The architecture is built on Qwen2.5-VL-7B-Instruct, featuring a reward head and employing multi-stage fine-tuning with pairwise ranking loss to optimize performance. Experimental results demonstrate that this model not only excels in multimodal tasks but also improves training for Mixed Preference Optimization, marking a significant step forward in creating effective reward models for multimodal alignment.'}, 'zh': {'title': 'Skywork-VL Reward：多模态对齐的突破性进展', 'desc': '我们提出了Skywork-VL Reward，这是一种多模态奖励模型，旨在为多模态理解和推理任务提供奖励信号。我们的技术方法包括两个关键组成部分：首先，我们构建了一个大规模的多模态偏好数据集，涵盖了广泛的任务和场景，数据来自标准视觉-语言模型和先进的视觉-语言推理模型。其次，我们设计了一种基于Qwen2.5-VL-7B-Instruct的奖励模型架构，集成了奖励头，并在成对偏好数据上应用了多阶段微调。实验评估表明，Skywork-VL Reward在多模态VL-RewardBench上达到了最先进的结果，并在文本奖励基准上表现出竞争力。'}}}, {'id': 'https://huggingface.co/papers/2505.03733', 'title': 'WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch', 'url': 'https://huggingface.co/papers/2505.03733', 'abstract': "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.", 'score': 15, 'issue_id': 3722, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '1d5e56d00ea8d485', 'authors': ['Zimu Lu', 'Yunqiao Yang', 'Houxing Ren', 'Haotian Hou', 'Han Xiao', 'Ke Wang', 'Weikang Shi', 'Aojun Zhou', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Multimedia Laboratory (MMLab), The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.03733.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#agents', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'WebGen-Bench: Новый рубеж в оценке LLM-агентов для веб-разработки', 'desc': 'WebGen-Bench - это новый бенчмарк для оценки способности LLM-агентов создавать многофайловые веб-сайты с нуля. Он включает разнообразные инструкции по генерации сайтов, охватывающие почти все важные типы веб-приложений. Качество сгенерированных сайтов оценивается с помощью автоматизированных тестовых случаев, созданных GPT-4o и проверенных вручную. Лучшая комбинация - Bolt.diy с DeepSeek-R1 - достигает только 27,8% точности, что подчеркивает сложность бенчмарка.'}, 'en': {'title': 'Benchmarking LLMs for Website Code Generation', 'desc': 'This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models.'}, 'zh': {'title': '评估LLM代理生成网站代码的挑战', 'desc': '本文介绍了一种新的基准测试WebGen-Bench，旨在评估基于大型语言模型（LLM）的代理在从零开始创建多文件网站代码库的能力。该基准包含多样化的网站生成指令，涵盖了三大类和十三小类，几乎包括所有重要类型的Web应用程序。为了评估生成网站的质量，使用GPT-4o生成针对每个功能的测试用例，并手动过滤和调整，最终形成647个测试用例。通过强大的网页导航代理自动执行测试，评估生成网站的响应是否符合预期结果，结果显示最佳模型组合的准确率仅为27.8%，显示出基准的挑战性。'}}}, {'id': 'https://huggingface.co/papers/2505.07796', 'title': 'Learning Dynamics in Continual Pre-Training for Large Language Models', 'url': 'https://huggingface.co/papers/2505.07796', 'abstract': 'Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.', 'score': 12, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'c63d617be0b4d13a', 'authors': ['Xingjin Wang', 'Howe Tissue', 'Lu Wang', 'Linjing Li', 'Daniel Dajun Zeng'], 'affiliations': ['Ritzz-AI', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07796.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training'], 'emoji': '📈', 'ru': {'title': 'Раскрытие секретов непрерывного предобучения языковых моделей', 'desc': 'Статья исследует динамику обучения при непрерывном предобучении (CPT) больших языковых моделей. Авторы наблюдают, как меняется производительность модели на общих и специфических задачах на каждом шаге обучения. Они предлагают масштабируемый закон CPT, объединяющий эффекты смещения распределения и снижения скорости обучения. Этот подход позволяет прогнозировать потери на любом этапе обучения и настраивать гиперпараметры для различных целей CPT.'}, 'en': {'title': 'Unlocking the Dynamics of Continual Pre-Training', 'desc': "This paper investigates the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on how performance in general and specific domains changes during training. The authors analyze the CPT loss curve, revealing that it represents a transition between different performance states influenced by distribution shifts and learning rate adjustments. They propose a scaling law that predicts loss across various training steps and learning rate schedules, providing insights into key factors like peak learning rate and replay ratio. The findings are validated through extensive experiments, showing the law's applicability across different datasets and training configurations."}, 'zh': {'title': '持续预训练的动态与优化法则', 'desc': '持续预训练（CPT）是一种将强大的基础模型应用于特定下游任务的有效方法。本文探讨了大型语言模型在CPT过程中的学习动态，特别关注在每个训练步骤中，通用性能和下游领域性能的演变。我们观察到CPT损失曲线本质上描述了从一个曲线到另一个隐藏曲线的过渡，并通过解耦分布变化和学习率退火的影响来进行描述。我们推导出了一种CPT缩放法则，结合了这两个因素，使得能够预测在任何（持续）训练步骤和学习率调度下的损失。'}}}, {'id': 'https://huggingface.co/papers/2505.07596', 'title': 'Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent', 'url': 'https://huggingface.co/papers/2505.07596', 'abstract': 'Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.', 'score': 10, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '10063104a79da512', 'authors': ['Ziyang Huang', 'Xiaowei Yuan', 'Yiming Ju', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.07596.jpg', 'data': {'categories': ['#reasoning', '#rag', '#agents', '#optimization', '#hallucinations', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Умный поиск: когда искать, а когда довериться себе', 'desc': 'Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с использованием метода retrieval-augmented generation (RAG). Авторы предлагают агента IKEA, который способен эффективно определять границы собственных знаний и принимать решение о необходимости внешнего поиска информации. Для обучения агента используется функция вознаграждения, учитывающая границы знаний, и специально подготовленный набор данных. Эксперименты показывают, что IKEA превосходит базовые методы, значительно сокращает частоту обращений к внешним источникам и демонстрирует хорошую обобщающую способность.'}, 'en': {'title': 'Optimizing Knowledge Use in Language Models with IKEA', 'desc': 'This paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), a novel approach to enhance the performance of Large Language Models (LLMs) by optimizing their retrieval processes. IKEA intelligently determines when to use its internal knowledge versus when to perform external searches, reducing unnecessary retrievals and improving inference speed. The model employs a unique reward function that encourages effective use of internal knowledge while still allowing for external retrieval when needed. Evaluations show that IKEA not only outperforms existing methods but also generalizes well across various knowledge reasoning tasks.'}, 'zh': {'title': '智能检索，优化知识利用', 'desc': '本文介绍了一种新的强化学习模型，名为IKEA，旨在提高大型语言模型（LLMs）的检索能力。IKEA能够识别自身知识的边界，并优先使用内部知识，只有在内部知识不足时才会进行外部检索。通过引入一种新的奖励函数和训练数据集，IKEA能够有效减少冗余检索，提高回答的准确性。实验结果表明，IKEA在多个知识推理任务中表现优异，显著降低了检索频率，并展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.06176', 'title': "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", 'url': 'https://huggingface.co/papers/2505.06176', 'abstract': 'Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.', 'score': 7, 'issue_id': 3733, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': '884e34691df4b88b', 'authors': ['Niladri Shekhar Dutt', 'Duygu Ceylan', 'Niloy J. Mitra'], 'affiliations': ['Adobe Research, UK', 'University College London, Adobe Research, UK', 'University College London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.06176.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#dataset', '#data', '#multimodal', '#interpretability', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Интеллектуальная ретушь фотографий: MLLM на страже профессионального качества', 'desc': 'Статья представляет новый подход к ретуши фотографий с использованием мультимодальной большой языковой модели (MLLM). Модель обучается анализировать сырые фотографии, предлагать подходящие корректировки и реализовывать их с помощью заранее заданных процедурных операций обработки изображений. MLLM сначала обучается на специально разработанных визуальных головоломках для понимания операций обработки изображений, а затем на синтезированном наборе данных с рассуждениями экспертов. Предложенный метод сохраняет детали объектов и разрешение, обеспечивая понятные пользователю операции редактирования.'}, 'en': {'title': 'Empowering Photo Retouching with Intelligent Editing Guidance', 'desc': 'This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.'}, 'zh': {'title': '利用多模态模型提升照片修饰质量', 'desc': '本论文探讨了如何利用多模态大型语言模型（MLLM）来改进原始照片的修饰过程。我们训练MLLM理解图像处理操作，并通过解决视觉难题来增强其操作意识。该模型能够规划和建议编辑序列，确保修饰操作对用户可理解，并保留对象细节和分辨率。我们的实验结果表明，与现有的生成和程序化方法相比，该方法在可解释性和身份保留方面具有明显优势。'}}}, {'id': 'https://huggingface.co/papers/2505.07819', 'title': 'H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning', 'url': 'https://huggingface.co/papers/2505.07819', 'abstract': 'Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9285a87dc24d7d07', 'authors': ['Yiyang Lu', 'Yufeng Tian', 'Zhecheng Yuan', 'Xianbang Wang', 'Pu Hua', 'Zhengrong Xue', 'Huazhe Xu'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute', 'Tsinghua University IIIS'], 'pdf_title_img': 'assets/pdf/title_img/2505.07819.jpg', 'data': {'categories': ['#diffusion', '#agents', '#robotics', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Иерархическое обучение для улучшения визуомоторного контроля роботов', 'desc': 'Статья представляет новый подход к обучению визуомоторной политики для робототехнической манипуляции - Триединую Иерархическую Диффузионную Политику (H^3DP). Этот метод использует трехуровневую иерархию: послойную организацию входных данных с учетом глубины, многомасштабное представление визуальных признаков и иерархически обусловленный процесс диффузии для генерации действий. H^3DP усиливает интеграцию между визуальным восприятием и предсказанием действий. Эксперименты показывают значительное улучшение производительности как в симуляции, так и в реальных задачах манипуляции.'}, 'en': {'title': 'Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration', 'desc': 'This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.'}, 'zh': {'title': '增强视觉与动作生成的三层次学习框架', 'desc': '本文提出了一种新的视觉运动策略学习框架，称为三层次扩散策略（H^{\textbf{3}DP）。该框架通过引入层次结构，增强了视觉特征与动作生成之间的结合。H^{3}DP包含三个层次：基于深度信息的输入分层、多尺度视觉表示和层次条件扩散过程。实验结果表明，H^{3}DP在44个仿真任务中相较于基线方法平均提高了27.5%的性能，并在四个复杂的双手真实世界操作任务中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.07260', 'title': 'UMoE: Unifying Attention and FFN with Shared Experts', 'url': 'https://huggingface.co/papers/2505.07260', 'abstract': 'Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '6a820b0cd1c07ac4', 'authors': ['Yuanhang Yang', 'Chaozheng Wang', 'Jing Li'], 'affiliations': ['Hong Kong Polytechnic University, Hong Kong, China', 'Institute of Science Tokyo, Tokyo, Japan', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07260.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Унификация MoE в Transformer: повышение эффективности через переосмысление внимания', 'desc': 'Статья представляет новый подход к архитектуре Разреженной смеси экспертов (Sparse Mixture of Experts, MoE) для масштабирования моделей Transformer. Авторы предлагают унифицированный дизайн MoE для слоев внимания и полносвязных слоев, переформулируя механизм внимания. Новая архитектура, названная UMoE, демонстрирует улучшенную производительность за счет MoE в слоях внимания. UMoE также позволяет эффективно разделять параметры между компонентами внимания и полносвязными слоями.'}, 'en': {'title': 'Unifying MoE for Enhanced Transformer Performance', 'desc': 'This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.'}, 'zh': {'title': '统一注意力与前馈网络的稀疏专家架构', 'desc': '稀疏专家混合（MoE）架构是一种有前景的方法，用于扩展Transformer模型。虽然早期的研究主要将MoE应用于前馈网络（FFN）层，但最近的研究开始探索将MoE扩展到注意力层，以提高模型性能。现有的基于注意力的MoE层需要专门的实现，并且与基于FFN的层相比，性能不尽如人意。本文提出了一种新颖的注意力机制重构，统一了注意力层和FFN层中的MoE设计，提出的UMoE架构通过基于注意力的MoE层实现了更优的性能，同时实现了FFN和注意力组件之间的高效参数共享。'}}}, {'id': 'https://huggingface.co/papers/2505.00612', 'title': 'Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation', 'url': 'https://huggingface.co/papers/2505.00612', 'abstract': 'In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\\em leakage} and {\\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.', 'score': 5, 'issue_id': 3733, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '98b0a79b86c5cd15', 'authors': ['D. Sculley', 'Will Cukierski', 'Phil Culliton', 'Sohier Dane', 'Maggie Demkin', 'Ryan Holbrook', 'Addison Howard', 'Paul Mooney', 'Walter Reade', 'Megan Risdal', 'Nate Keating'], 'affiliations': ['Kaggle, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.00612.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#evaluation'], 'emoji': '🏆', 'ru': {'title': 'Соревнования по ИИ - ключ к надежной оценке генеративных моделей', 'desc': 'Эта статья рассматривает кризис в эмпирической оценке генеративного ИИ, указывая на недостаточность традиционных методов оценки машинного обучения. Авторы отмечают проблемы, связанные с неограниченными входными и выходными пространствами, отсутствием четко определенной эталонной истины и сильными обратными связями в генеративных моделях. Особое внимание уделяется проблемам утечки и загрязнения данных как наиболее критичным для оценки генеративного ИИ. Статья предлагает рассматривать соревнования по ИИ как золотой стандарт для эмпирической строгости в оценке генеративного ИИ.'}, 'en': {'title': 'Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI', 'desc': 'This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.'}, 'zh': {'title': '生成性AI评估的新标准：人工智能竞赛的价值', 'desc': '在这篇论文中，我们观察到生成性人工智能的实证评估正面临危机，因为传统的机器学习评估和基准策略无法满足现代生成性AI模型和系统的评估需求。这些模型通常具有几乎无限的输入和输出空间，缺乏明确的真实目标，并且在预测时强烈依赖于之前模型输出的上下文。此外，论文指出，泄漏和污染问题是生成性AI评估中最重要且最难解决的挑战。我们认为，人工智能竞赛领域已经发展出有效的措施来应对泄漏问题，因此应将其视为生成性AI评估的黄金标准。'}}}, {'id': 'https://huggingface.co/papers/2505.07812', 'title': 'Continuous Visual Autoregressive Generation via Score Maximization', 'url': 'https://huggingface.co/papers/2505.07812', 'abstract': 'Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.', 'score': 3, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '5635f18df39cf275', 'authors': ['Chenze Shao', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07812.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#training', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Непрерывное визуальное авторегрессионное моделирование без квантования', 'desc': 'Статья представляет новый подход к визуальному авторегрессионному моделированию (VAR) для непрерывных данных без использования векторного квантования. Авторы предлагают фреймворк Continuous VAR, основанный на строго корректных правилах оценки (strictly proper scoring rules). Основное внимание уделяется целевым функциям на основе энергетической оценки (energy score), которая не требует вычисления правдоподобия. Этот метод позволяет преодолеть ограничения традиционных подходов VAR, связанные с потерей информации при дискретизации.'}, 'en': {'title': 'Revolutionizing Visual Data Generation with Continuous VAR', 'desc': 'This paper presents a new approach called Continuous Visual AutoRegressive (VAR) modeling, which allows for the generation of visual data without converting it into discrete formats. Traditional methods often lose important information during quantization, but this framework uses strictly proper scoring rules to evaluate and optimize generative models directly in continuous space. By focusing on energy scores as training objectives, the method avoids the challenges of probabilistic predictions in continuous domains. Additionally, it shows that previous continuous autoregressive methods can be derived from this new framework, enhancing their applicability and performance.'}, 'zh': {'title': '无量化的连续自回归生成新框架', 'desc': '传统观点认为自回归模型主要用于处理离散数据。在处理连续数据（如视觉数据）时，视觉自回归建模（VAR）通常需要通过量化方法将数据转换为离散空间，这可能导致信息损失。为了解决这个问题，我们提出了一种连续VAR框架，能够直接进行视觉自回归生成，而无需向量量化。该框架的理论基础是严格适当的评分规则，这为评估生成模型如何逼近真实分布提供了强大的统计工具。'}}}, {'id': 'https://huggingface.co/papers/2505.07793', 'title': 'Overflow Prevention Enhances Long-Context Recurrent LLMs', 'url': 'https://huggingface.co/papers/2505.07793', 'abstract': 'A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.', 'score': 3, 'issue_id': 3728, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'f4bfefd5343cbf0c', 'authors': ['Assaf Ben-Kish', 'Itamar Zimerman', 'M. Jehanzeb Mirza', 'James Glass', 'Leonid Karlinsky', 'Raja Giryes'], 'affiliations': ['IBM Research', 'MIT CSAIL', 'Tel Aviv University', 'Xero'], 'pdf_title_img': 'assets/pdf/title_img/2505.07793.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Чанки побеждают рекуррентность в обработке длинных контекстов', 'desc': 'Исследование фокусируется на рекуррентных суб-квадратичных моделях для обработки длинных контекстов в больших языковых моделях. Эксперименты показывают, что даже при обучении на длинных контекстах, модели недостаточно эффективно их используют. Предложенный метод обработки по чанкам, выбирающий наиболее релевантные части входных данных, значительно улучшает производительность ряда моделей на бенчмарке LongBench. Результаты ставят под сомнение способность рекуррентных моделей эффективно использовать зависимости в длинных контекстах.'}, 'en': {'title': 'Unlocking Long-Context Potential with Chunk-Based Inference', 'desc': 'This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.'}, 'zh': {'title': '提升长上下文处理效率的新方法', 'desc': '最近，长文本模型（LLMs）发展出了一种新的子二次模型，旨在提高长上下文处理的效率。我们研究了主要的长上下文模型，重点关注它们固定大小的递归记忆如何影响性能。实验表明，即使这些模型经过长上下文训练，它们对长上下文的利用仍然不足。我们提出的基于块的推理方法能够识别并处理输入中最相关的部分，从而有效缓解递归记忆的不足，并在多个长上下文任务中取得显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.06324', 'title': 'Document Attribution: Examining Citation Relationships using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.06324', 'abstract': "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.   To tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.", 'score': 3, 'issue_id': 3724, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': 'd1b4a407c1a67da8', 'authors': ['Vipula Rawte', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka'], 'affiliations': ['Adobe Inc.', 'Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.06324.jpg', 'data': {'categories': ['#training', '#interpretability', '#multimodal', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Повышение надежности атрибуции в LLM: от текстового включения до механизма внимания', 'desc': 'Эта статья посвящена проблеме атрибуции в больших языковых моделях (LLM) при работе с документами. Авторы предлагают два метода для повышения надежности цитирования: подход с нулевым обучением, основанный на текстовом включении, и использование механизма внимания. Первый метод, использующий flan-ul2, показал улучшение на 0,27% и 2,4% по сравнению с базовой линией на наборах данных AttributionBench. Второй метод, применяющий меньшую модель flan-t5-small, превзошел базовую линию по показателю F1 почти во всех слоях, кроме нескольких.'}, 'en': {'title': 'Enhancing Trust in LLMs through Improved Attribution Techniques', 'desc': 'This paper addresses the challenges of trustworthiness and interpretability in Large Language Models (LLMs) when used for document-based tasks. It introduces two techniques for improving attribution, which is the process of linking model outputs back to their source documents. The first technique is a zero-shot approach that treats attribution as a textual entailment task, showing measurable improvements in performance. The second technique investigates how the attention mechanism in LLMs can enhance attribution accuracy, achieving better F1 scores in most layers of a smaller model.'}, 'zh': {'title': '提升大型语言模型的可信性与可解释性', 'desc': '随着大型语言模型（LLMs）在文档摘要、问答和信息提取等任务中的应用日益增多，确保这些系统的可信性和可解释性变得至关重要。本文提出了一种归因方法，通过追踪生成的输出回到其源文档来解决这一挑战。我们提出了两种技术：一种是零样本方法，将归因视为简单的文本蕴含任务，另一种是探索注意力机制在增强归因过程中的作用。实验结果表明，我们的方法在多个基准测试中均有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.07291', 'title': 'INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.07291', 'abstract': 'We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training.', 'score': 2, 'issue_id': 3742, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'bf4c666ff6a739cf', 'authors': ['Prime Intellect Team', 'Sami Jaghouar', 'Justus Mattern', 'Jack Min Ong', 'Jannik Straube', 'Manveer Basra', 'Aaron Pazdera', 'Kushal Thaman', 'Matthew Di Ferrante', 'Felix Gabriel', 'Fares Obeid', 'Kemal Erdem', 'Michael Keiblinger', 'Johannes Hagemann'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07291.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#rl', '#dataset', '#reasoning'], 'emoji': '🌐', 'ru': {'title': 'Глобальное распределенное обучение с подкреплением для крупномасштабных языковых моделей', 'desc': 'INTELLECT-2 представляет собой первую глобально распределенную систему обучения с подкреплением для языковой модели с 32 миллиардами параметров. В отличие от традиционного централизованного подхода, INTELLECT-2 использует полностью асинхронное обучение с подкреплением на динамичном, гетерогенном рое вычислительных узлов. Для реализации этой уникальной инфраструктуры были разработаны специальные компоненты, включая фреймворк PRIME-RL, систему верификации TOPLOC и механизм эффективного распространения весов модели SHARDCAST. Авторы также предложили модификации стандартного алгоритма GRPO и методы фильтрации данных для достижения стабильности обучения.'}, 'en': {'title': 'Revolutionizing Reinforcement Learning with Distributed Training', 'desc': 'INTELLECT-2 is a groundbreaking reinforcement learning model that utilizes a globally distributed training approach for a 32 billion parameter language model. It employs asynchronous training across a diverse group of independent compute contributors, which is a shift from traditional centralized methods. The paper introduces innovative components like PRIME-RL for managing distributed training, TOPLOC for verifying data from untrusted sources, and SHARDCAST for efficient communication of model updates. By refining the GRPO training method and implementing effective data filtering, INTELLECT-2 achieves enhanced stability and performance, surpassing previous models in its category.'}, 'zh': {'title': 'INTELLECT-2：全球分布式强化学习的创新之路', 'desc': '我们介绍了INTELLECT-2，这是第一个全球分布式的强化学习训练，使用了320亿参数的语言模型。与传统的集中式训练不同，INTELLECT-2通过一个动态的、异构的计算贡献者群体，采用完全异步的强化学习来训练推理模型。为了支持这种独特的基础设施，我们从头开始构建了多个组件，包括专为分布式异步强化学习设计的PRIME-RL训练框架，以及验证不可信推理工作者的TOPLOC和高效广播策略权重的SHARDCAST。我们还对标准的GRPO训练方法和数据过滤技术进行了修改，以确保训练的稳定性，并成功实现模型的训练目标，从而在320亿参数范围内超越了现有的QwQ-32B推理模型。'}}}, {'id': 'https://huggingface.co/papers/2505.07233', 'title': 'DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2505.07233', 'abstract': 'Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG', 'score': 2, 'issue_id': 3734, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'b6c55b4c738d5230', 'authors': ['Jiashuo Sun', 'Xianrui Zhong', 'Sizhe Zhou', 'Jiawei Han'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07233.jpg', 'data': {'categories': ['#rag', '#optimization', '#interpretability', '#rl'], 'emoji': '🔄', 'ru': {'title': 'Динамическая оптимизация извлечения знаний для генеративных ИИ-систем', 'desc': 'DynamicRAG - это новая система генерации с извлечением информации (RAG), использующая динамический ранжировщик документов. Ранжировщик оптимизируется с помощью обучения с подкреплением, используя качество ответов языковой модели в качестве сигнала обратной связи. Система автоматически определяет оптимальное количество и порядок документов для каждого запроса. DynamicRAG показала наилучшие результаты на семи наборах данных, требующих обширных знаний.'}, 'en': {'title': 'DynamicRAG: Optimizing Document Retrieval for Better Generation', 'desc': "This paper introduces DynamicRAG, a new framework for retrieval-augmented generation (RAG) systems that enhances the quality of generated responses by optimizing the reranking of retrieved documents. The reranker in DynamicRAG is designed to dynamically adjust both the order and the number of documents based on the specific query, addressing the challenge of selecting the optimal number of documents. By employing reinforcement learning, the reranker uses feedback from the quality of the language model's output to improve its decisions. The results show that DynamicRAG outperforms existing methods across multiple knowledge-intensive datasets, achieving state-of-the-art performance."}, 'zh': {'title': '动态调整，提升生成质量的RAG框架', 'desc': '检索增强生成（RAG）系统结合了大型语言模型（LLM）和外部知识检索，适用于知识密集型任务。本文提出了一种新的RAG框架DynamicRAG，其中的重排序器能够根据查询动态调整检索文档的顺序和数量。我们将重排序器建模为一个通过强化学习（RL）优化的智能体，利用LLM输出质量作为奖励来优化重排序决策。实验结果表明，DynamicRAG在七个知识密集型数据集上表现优异，达到了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2505.04918', 'title': 'Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction', 'url': 'https://huggingface.co/papers/2505.04918', 'abstract': "Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.", 'score': 2, 'issue_id': 3728, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '73efdd4a4328c88d', 'authors': ['Jiaqi Zheng', 'Qing Ling', 'Yerong Feng'], 'affiliations': ['Shenzhen Institute of Meteorological Innovation', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04918.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#graphs', '#architecture', '#training'], 'emoji': '🌎', 'ru': {'title': 'PASSAT: Физически обоснованный прогноз погоды с учетом топологии Земли', 'desc': 'PASSAT - это новая модель глубокого обучения для прогнозирования погоды, учитывающая физику атмосферных процессов и топологию поверхности Земли. Модель решает уравнения адвекции и Навье-Стокса на сферическом многообразии, используя сферическую графовую нейронную сеть для моделирования взаимодействия Земли и атмосферы. PASSAT генерирует начальные поля скоростей, критически важные для решения уравнения адвекции. В экспериментах на данных ERA5 с разрешением 5.625° PASSAT превзошла как современные модели глубокого обучения, так и операционную модель численного прогноза погоды IFS T42.'}, 'en': {'title': 'PASSAT: Bridging Physics and Topology for Superior Weather Prediction', 'desc': "The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth's surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns."}, 'zh': {'title': 'PASSAT：结合物理与拓扑的天气预测新模型', 'desc': '本文提出了一种新型的天气预测模型PASSAT，该模型结合了物理学和地形信息。PASSAT通过对流过程和地球-大气相互作用来描述天气演变，并考虑了地球表面的拓扑结构。该模型在球面流形上数值求解对流方程和纳维-斯托克斯方程，并利用球面图神经网络捕捉地球-大气的相互作用。实验结果表明，PASSAT在5.625度分辨率的ERA5数据集上优于现有的深度学习天气预测模型和操作性数值天气预测模型IFS T42。'}}}, {'id': 'https://huggingface.co/papers/2505.04066', 'title': 'LLAMAPIE: Proactive In-Ear Conversation Assistants', 'url': 'https://huggingface.co/papers/2505.04066', 'abstract': 'We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.', 'score': 1, 'issue_id': 3739, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': 'f8bf204612751793', 'authors': ['Tuochao Chen', 'Nicholas Batchelder', 'Alisa Liu', 'Noah Smith', 'Shyamnath Gollakota'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.04066.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#dataset', '#data'], 'emoji': '🎧', 'ru': {'title': 'LlamaPIE: Незаметный помощник для живых разговоров', 'desc': 'LlamaPIE - это первый в реальном времени проактивный ассистент, разработанный для улучшения человеческих разговоров через незаметные, краткие подсказки, передаваемые через слуховые устройства. В отличие от традиционных языковых моделей, этот ассистент работает в фоновом режиме, предугадывая потребности пользователя без прерывания разговоров. Авторы решают несколько задач, включая определение момента для ответа, создание кратких ответов и обработку в реальном времени на устройстве. Оценка подхода на реальных данных и пользовательские исследования показывают эффективность LlamaPIE в предоставлении полезной, ненавязчивой помощи.'}, 'en': {'title': 'LlamaPIE: Enhancing Conversations with Proactive Assistance', 'desc': 'LlamaPIE is a novel real-time proactive assistant that enhances human conversations by providing discreet guidance through hearable devices. Unlike traditional models that wait for user prompts, LlamaPIE anticipates user needs and offers assistance without interrupting the flow of conversation. The system tackles challenges such as timing for responses, generating concise and relevant replies, and utilizing user context for personalized support, all while processing information on-device. Evaluations show that users prefer LlamaPIE over reactive models, indicating its effectiveness in improving live interactions.'}, 'zh': {'title': 'LlamaPIE：提升对话的主动助手', 'desc': 'LlamaPIE 是首个实时主动助手，旨在通过可穿戴设备在对话中提供简洁的指导。与传统语言模型不同，它在后台运行，能够预测用户需求而不打断对话。我们解决了多个挑战，包括何时响应、如何生成简洁的回应以及如何利用用户知识提供上下文感知的帮助。通过构建半合成对话数据集和提出双模型管道，我们在真实数据集上评估了该方法的有效性，用户研究显示出对主动助手的强烈偏好。'}}}, {'id': 'https://huggingface.co/papers/2505.07086', 'title': 'Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design', 'url': 'https://huggingface.co/papers/2505.07086', 'abstract': "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.", 'score': 0, 'issue_id': 3725, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 мая', 'en': 'May 11', 'zh': '5月11日'}, 'hash': 'a2fce171208a1e7a', 'authors': ['Tong Chen', 'Yinuo Zhang', 'Sophia Tang', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer Science, Fudan University', 'Management and Technology Program, University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2505.07086.jpg', 'data': {'categories': ['#training', '#dataset', '#science', '#data', '#optimization'], 'emoji': '🧬', 'ru': {'title': 'Многоцелевая оптимизация биологических последовательностей с помощью дискретного сопоставления потоков', 'desc': 'Статья представляет новый метод машинного обучения под названием Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) для проектирования биологических последовательностей с множественными целевыми функциями. MOG-DFM использует предобученные генеративные модели на основе дискретного сопоставления потоков и направляет их к Парето-эффективным компромиссам между несколькими скалярными целями. Авторы демонстрируют эффективность метода на примерах генерации пептидов с оптимизированными свойствами и проектирования последовательностей ДНК с заданными характеристиками. MOG-DFM показывает себя мощным инструментом для многоцелевого дизайна биомолекулярных последовательностей.'}, 'en': {'title': 'Optimizing Biomolecule Design with MOG-DFM', 'desc': "This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering."}, 'zh': {'title': '多目标优化，助力生物分子设计', 'desc': '在生物分子工程中，设计满足多种功能和生物物理标准的生物序列仍然是一个重要挑战。本文提出了一种名为多目标引导离散流匹配（MOG-DFM）的框架，能够在多个标量目标之间实现帕累托有效的权衡。MOG-DFM通过计算混合排名方向分数和应用自适应超锥过滤器，来引导预训练的离散时间流匹配生成器进行多目标优化。我们展示了MOG-DFM在生成优化的肽结合物和特定增强子类DNA序列方面的有效性，证明了其在多属性引导的生物分子序列设计中的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2505.14683', 'title': 'Emerging Properties in Unified Multimodal Pretraining', 'url': 'https://huggingface.co/papers/2505.14683', 'abstract': 'Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/', 'score': 80, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '57522649bb8f8010', 'authors': ['Chaorui Deng', 'Deyao Zhu', 'Kunchang Li', 'Chenhui Gou', 'Feng Li', 'Zeyu Wang', 'Shu Zhong', 'Weihao Yu', 'Xiaonan Nie', 'Ziang Song', 'Guang Shi', 'Haoqi Fan'], 'affiliations': ['ByteDance Seed', 'Hong Kong University of Science and Technology', 'Monash University', 'Shenzhen Institutes of Advanced Technology', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.14683.jpg', 'data': {'categories': ['#3d', '#benchmark', '#reasoning', '#open_source', '#multimodal', '#dataset'], 'emoji': '🥯', 'ru': {'title': 'BAGEL: Объединение мультимодального понимания и генерации в открытой модели', 'desc': 'BAGEL - это открытая фундаментальная модель для мультимодального понимания и генерации. Она обучена на триллионах токенов из текстовых, изображений, видео и веб-данных. BAGEL превосходит другие открытые унифицированные модели в задачах мультимодальной генерации и понимания. Модель демонстрирует продвинутые способности в мультимодальном рассуждении, включая манипуляции с изображениями, предсказание будущих кадров и 3D-манипуляции.'}, 'en': {'title': 'BAGEL: Unifying Multimodal AI for Enhanced Understanding and Generation', 'desc': 'This paper presents BAGEL, an open-source foundational model designed for multimodal understanding and generation. BAGEL is a decoder-only model that has been pretrained on a vast dataset comprising text, images, videos, and web content. By leveraging this diverse multimodal data, BAGEL demonstrates advanced capabilities in complex reasoning tasks, outperforming existing open-source models. The authors aim to promote further research in multimodal AI by sharing their findings, pretraining methods, and code with the community.'}, 'zh': {'title': 'BAGEL：开源多模态理解与生成的统一模型', 'desc': '本文介绍了一个名为BAGEL的开源基础模型，它支持多模态理解和生成。BAGEL是一个统一的解码器模型，经过在大量文本、图像、视频和网络数据上进行预训练。通过使用多样化的多模态数据，BAGEL在复杂的多模态推理方面展现出新的能力，显著超越了现有的开源统一模型。我们希望通过分享关键发现、预训练细节和数据创建协议，促进多模态研究的进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2505.11594', 'title': 'SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training', 'url': 'https://huggingface.co/papers/2505.11594', 'abstract': 'The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.', 'score': 45, 'issue_id': 3869, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '33309444d442b40c', 'authors': ['Jintao Zhang', 'Jia Wei', 'Pengle Zhang', 'Xiaoming Xu', 'Haofeng Huang', 'Haoxu Wang', 'Kai Jiang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11594.jpg', 'data': {'categories': ['#inference', '#architecture', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Революция в эффективности механизма внимания: от FP4 до 8-бит', 'desc': 'Статья представляет два ключевых улучшения эффективности механизма внимания в нейронных сетях. Во-первых, авторы используют новые тензорные ядра FP4 в GPU Blackwell для ускорения вычислений, достигая 5-кратного прироста производительности по сравнению с FlashAttention. Во-вторых, они разрабатывают 8-битное внимание для задач обучения, которое показывает хорошие результаты при дообучении моделей. Эксперименты демонстрируют, что предложенные методы могут эффективно применяться для ускорения различных моделей.'}, 'en': {'title': 'Revolutionizing Attention: Fast and Efficient for Training and Inference', 'desc': 'This paper addresses the inefficiency of attention mechanisms in machine learning, which typically have a quadratic time complexity. The authors introduce enhancements using FP4 Tensor Cores in Blackwell GPUs, achieving a significant speedup in attention computation, reaching 1038 TOPS on the RTX5090. Additionally, they explore low-bit attention for training tasks, proposing an 8-bit attention method that maintains performance during fine-tuning while showing slower convergence during pretraining. This work not only improves inference speed but also expands the application of low-bit attention to training, making it a versatile solution for large model training.'}, 'zh': {'title': '提升注意力机制效率的创新方法', 'desc': '本文探讨了注意力机制的效率问题，主要由于其二次时间复杂度。我们通过利用Blackwell GPU中的新FP4 Tensor Cores来加速注意力计算，实现了在RTX5090上达到1038 TOPS的性能，相比于最快的FlashAttention提升了5倍。我们的FP4注意力可以以即插即用的方式加速各种模型的推理。此外，我们还首次将低位注意力应用于训练任务，设计了高效的8位注意力，实验表明在微调任务中表现无损，但在预训练任务中收敛速度较慢。'}}}, {'id': 'https://huggingface.co/papers/2505.13438', 'title': 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2505.13438', 'abstract': 'Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.', 'score': 27, 'issue_id': 3874, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '34ff8235b7a27562', 'authors': ['Penghui Qi', 'Zichen Liu', 'Tianyu Pang', 'Chao Du', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.13438.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Эффективные рассуждения ЛЯМ в любой момент времени', 'desc': 'Статья представляет новый подход AnytimeReasoner для оптимизации рассуждений языковых моделей (ЛЯМ) в любой момент времени. Метод использует усеченные процессы мышления под различные бюджеты токенов, заставляя модель суммировать оптимальный ответ для каждого усеченного рассуждения. Авторы вводят технику снижения дисперсии Budget Relative Policy Optimization (BRPO) для улучшения обучения с подкреплением. Эмпирические результаты показывают превосходство метода над GRPO в задачах математических рассуждений при различных распределениях бюджетов.'}, 'en': {'title': 'Optimizing Reasoning Efficiency with AnytimeReasoner', 'desc': 'This paper introduces AnytimeReasoner, a framework designed to improve the reasoning capabilities of large language models (LLMs) by optimizing their performance under varying token budgets. Unlike traditional reinforcement learning methods that focus solely on final outcomes, AnytimeReasoner allows for flexible reasoning by truncating the thinking process and summarizing answers based on sampled token budgets. This approach incorporates verifiable dense rewards, which aids in better credit assignment during the reinforcement learning optimization. The authors also propose a new technique called Budget Relative Policy Optimization (BRPO) to enhance the robustness and efficiency of the learning process, leading to superior performance in mathematical reasoning tasks compared to existing methods.'}, 'zh': {'title': '优化推理性能，提升效率与灵活性', 'desc': '本文提出了一种新的框架，名为AnytimeReasoner，旨在优化大型语言模型（LLMs）的推理性能。与传统方法不同，AnytimeReasoner通过在不同的令牌预算约束下进行推理，提升了令牌的使用效率和灵活性。该方法引入了可验证的密集奖励，使得在强化学习优化中能够更有效地进行信用分配。实验结果表明，在数学推理任务中，AnytimeReasoner在各种预算条件下均优于现有方法，提升了训练和令牌效率。'}}}, {'id': 'https://huggingface.co/papers/2505.14460', 'title': 'VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank', 'url': 'https://huggingface.co/papers/2505.14460', 'abstract': 'DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.', 'score': 24, 'issue_id': 3876, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '98e34275d69f6e41', 'authors': ['Tianhe Wu', 'Jian Zou', 'Jie Liang', 'Lei Zhang', 'Kede Ma'], 'affiliations': ['City University of Hong Kong', 'OPPO Research Institute', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14460.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#rl', '#training', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'VisualQuality-R1: Революция в оценке качества изображений с помощью рассуждений и обучения с подкреплением', 'desc': 'В статье представлена модель VisualQuality-R1 для оценки качества изображений без эталона, обученная с помощью обучения с подкреплением для ранжирования. Модель генерирует несколько оценок качества для каждого изображения в паре, используя групповую относительную оптимизацию политики. VisualQuality-R1 превосходит дискриминативные модели глубокого обучения и может генерировать содержательные описания качества, понятные человеку. Модель поддерживает обучение на нескольких наборах данных без необходимости перекалибровки перцептивной шкалы.'}, 'en': {'title': 'Revolutionizing Image Quality Assessment with Reasoning and Reinforcement Learning', 'desc': "This paper presents VisualQuality-R1, a novel no-reference image quality assessment (NR-IQA) model that leverages reasoning and reinforcement learning to evaluate image quality. By using group relative policy optimization, the model generates multiple quality scores for image pairs, allowing for a nuanced comparison of visual quality. The rewards for these scores are based on continuous fidelity measures, enhancing the model's ability to provide accurate quality assessments. Experimental results show that VisualQuality-R1 outperforms existing deep learning-based NR-IQA models and can generate detailed quality descriptions, making it effective for various image processing applications."}, 'zh': {'title': '推理驱动的图像质量评估新模型', 'desc': '本文介绍了一种新的无参考图像质量评估模型VisualQuality-R1，该模型通过强化学习来提高大语言模型的推理和泛化能力。VisualQuality-R1利用组相对策略优化，为每对图像生成多个质量评分，并计算图像之间的比较概率。与传统的二元标签不同，奖励是基于连续的保真度度量来定义的。实验结果表明，VisualQuality-R1在图像质量评估任务中表现优于现有的深度学习模型，并能够生成丰富的质量描述，适用于多数据集训练。'}}}, {'id': 'https://huggingface.co/papers/2505.14246', 'title': 'Visual Agentic Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.14246', 'abstract': "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.", 'score': 24, 'issue_id': 3873, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '163cdaefde9d9174', 'authors': ['Ziyu Liu', 'Yuhang Zang', 'Yushan Zou', 'Zijian Liang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14246.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#benchmark', '#multimodal', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Усиление мультимодальных ИИ-агентов через обучение с подкреплением', 'desc': 'Статья представляет метод Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) для улучшения способностей мультимодальных языковых моделей (LVLM) к рассуждению и использованию инструментов. Visual-ARFT позволяет моделям просматривать веб-сайты для получения актуальной информации и писать код для манипуляции изображениями. Авторы также представляют набор тестов Multi-modal Agentic Tool Bench (MAT) для оценки агентных способностей LVLM. Результаты экспериментов показывают, что Visual-ARFT значительно улучшает производительность моделей на MAT и других задачах многоэтапного вопросно-ответного анализа.'}, 'en': {'title': 'Empowering Vision-Language Models with Visual-ARFT for Enhanced Reasoning', 'desc': "This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT), a method that enhances Large Vision-Language Models (LVLMs) by enabling them to think with images and use external tools effectively. The research highlights the development of multi-modal agentic capabilities, allowing LVLMs to browse the web for information and manipulate images through coding. The authors present a new evaluation framework called the Multi-modal Agentic Tool Bench (MAT), which assesses the models' abilities in searching and coding tasks. Experimental results show that Visual-ARFT significantly improves performance on various benchmarks, indicating its potential for creating more capable multimodal agents."}, 'zh': {'title': '视觉代理强化微调：多模态智能的未来', 'desc': '本文探讨了大型视觉语言模型（LVLMs）在多模态推理能力方面的进展，特别是通过视觉代理强化微调（Visual-ARFT）技术。该技术使得LVLMs能够灵活地使用外部工具，如浏览器和代码执行，进行实时信息更新和图像处理。研究还提出了一个多模态代理工具基准（MAT），用于评估LVLMs的搜索和编码能力。实验结果表明，Visual-ARFT在多个基准测试中显著优于传统模型，展示了其强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.13138', 'title': 'Neurosymbolic Diffusion Models', 'url': 'https://huggingface.co/papers/2505.13138', 'abstract': 'Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration.', 'score': 24, 'issue_id': 3876, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '56639003a63a4eb4', 'authors': ['Emile van Krieken', 'Pasquale Minervini', 'Edoardo Ponti', 'Antonio Vergari'], 'affiliations': ['Miniml.AI', 'School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.13138.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#agents', '#benchmark', '#diffusion', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Нейросимволические диффузионные модели: преодоление ограничений условной независимости', 'desc': 'Нейросимволические диффузионные модели (NeSyDMs) представляют собой новый класс нейросимволических предикторов, использующих дискретную диффузию для моделирования зависимостей между символами. В отличие от стандартных нейросимволических предикторов, которые предполагают условную независимость извлекаемых символов, NeSyDMs способны лучше моделировать взаимодействия и неопределенность. Этот подход позволяет сохранить масштабируемость обучения при одновременном учете зависимостей между символами и количественной оценке неопределенности. На синтетических и реальных тестах, включая высокоразмерное визуальное планирование пути и автономное вождение на основе правил, NeSyDMs достигают наилучшей точности среди нейросимволических предикторов и демонстрируют хорошую калибровку.'}, 'en': {'title': 'Enhancing Symbolic Reasoning with Dependency Modeling in Neurosymbolic Predictors', 'desc': 'This paper presents neurosymbolic diffusion models (NeSyDMs), which enhance traditional neurosymbolic predictors by addressing the limitations of assuming conditional independence between symbols. By employing discrete diffusion processes, NeSyDMs effectively model the interactions and dependencies among symbols, leading to improved predictions. This method allows for scalable learning while also quantifying uncertainty, which is crucial for tasks like visual reasoning and autonomous driving. The results show that NeSyDMs outperform existing neurosymbolic approaches in accuracy and calibration on various benchmarks.'}, 'zh': {'title': '突破独立假设，提升符号依赖性建模', 'desc': '神经符号预测器（NeSy）结合了神经感知和符号推理，用于解决视觉推理等任务。传统的NeSy预测器假设提取的符号之间是条件独立的，这限制了它们建模交互和不确定性的能力，常常导致过于自信的预测和较差的分布外泛化能力。为了解决独立性假设的局限性，我们提出了神经符号扩散模型（NeSyDMs），这是一类新的NeSy预测器，利用离散扩散来建模符号之间的依赖关系。我们的研究在合成和真实世界基准测试中，包括高维视觉路径规划和基于规则的自动驾驶，展示了NeSyDMs在NeSy预测器中的最先进准确性和强大的校准能力。'}}}, {'id': 'https://huggingface.co/papers/2505.04388', 'title': 'The Aloe Family Recipe for Open and Specialized Healthcare LLMs', 'url': 'https://huggingface.co/papers/2505.04388', 'abstract': 'Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.', 'score': 19, 'issue_id': 3874, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '12792ceffb601d5a', 'authors': ['Dario Garcia-Gasulla', 'Jordi Bayarri-Planas', 'Ashwin Kumar Gururajan', 'Enrique Lopez-Cuena', 'Adrian Tormos', 'Daniel Hinjos', 'Pablo Bernabeu-Perez', 'Anna Arias-Duart', 'Pablo Agustin Martin-Torres', 'Marta Gonzalez-Mallo', 'Sergio Alvarez-Napagao', 'Eduard Ayguadé-Parra', 'Ulises Cortés'], 'affiliations': ['Barcelona Supercomputing Center (BSC-CNS), Spain', 'Universitat Polit`ecnica de Catalunya - Barcelona Tech (UPC), Spain'], 'pdf_title_img': 'assets/pdf/title_img/2505.04388.jpg', 'data': {'categories': ['#alignment', '#data', '#healthcare', '#training', '#open_source', '#ethics', '#benchmark', '#rlhf', '#rag'], 'emoji': '🩺', 'ru': {'title': 'Открытые медицинские ИИ-модели нового поколения: эффективность и безопасность', 'desc': 'Статья представляет семейство моделей Aloe Beta - открытые языковые модели для здравоохранения, конкурентоспособные с лучшими закрытыми аналогами. Исследователи оптимизировали ключевые этапы предобработки данных и обучения, улучшили безопасность моделей с помощью DPO и эффективность через RAG. Предложенная методология оценки, включающая четыре типа тестов, устанавливает новый стандарт в этой области. Модели Aloe Beta демонстрируют высокую производительность в медицинских задачах и устойчивость к атакам на безопасность.'}, 'en': {'title': 'Empowering Healthcare with Open-Source LLMs: Safety Meets Efficacy', 'desc': 'This paper discusses the development of open-source Large Language Models (LLMs) for healthcare, focusing on optimizing data preprocessing and training methods. It introduces Direct Preference Optimization (DPO) to enhance model safety and Retrieval-Augmented Generation (RAG) to improve efficacy. The evaluation methodology includes various tests to establish a new standard for assessing model performance in medical contexts. The resulting Aloe Beta models demonstrate competitive capabilities against private models while adhering to ethical guidelines and safety measures.'}, 'zh': {'title': '推动开放医疗模型，保障公众利益', 'desc': '本研究旨在推动开放源代码医疗大型语言模型（LLMs）的发展，以保护公众利益。通过优化数据预处理和训练的关键阶段，研究展示了如何通过直接偏好优化（DPO）提高模型的安全性，以及通过检索增强生成（RAG）提升模型的有效性。评估方法包括四种不同类型的测试，为该领域设定了新的标准。最终发布的模型在医疗基准测试中表现出色，且在安全性和伦理方面也有显著改善。'}}}, {'id': 'https://huggingface.co/papers/2505.14513', 'title': 'Latent Flow Transformer', 'url': 'https://huggingface.co/papers/2505.14513', 'abstract': 'Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.', 'score': 18, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '3683bab427c47086', 'authors': ['Yen-Chen Wu', 'Feng-Ting Liao', 'Meng-Hsi Chen', 'Pei-Chen Ho', 'Farhang Nabiei', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.14513.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training'], 'emoji': '🌊', 'ru': {'title': 'Непрерывные потоки вместо дискретных слоев: революция в архитектуре трансформеров', 'desc': 'Статья представляет Latent Flow Transformer (LFT), новый подход к архитектуре языковых моделей. LFT заменяет несколько дискретных слоев одним непрерывным оператором переноса, обученным с помощью метода согласования потоков. Авторы также предлагают алгоритм Flow Walking для улучшения сохранения связей между токенами. Эксперименты на модели Pythia-410M показывают, что LFT позволяет значительно сжать модель, сохраняя или даже улучшая ее производительность.'}, 'en': {'title': 'Efficient Layer Compression with Latent Flow Transformers', 'desc': 'This paper introduces the Latent Flow Transformer (LFT), a new architecture for large language models that replaces multiple discrete layers with a single learned transport operator. By utilizing flow matching, LFT achieves significant model compression while still being compatible with traditional transformer designs. The authors also present the Flow Walking (FW) algorithm to enhance the coupling preservation in flow-based methods. Experimental results show that LFT can effectively reduce the number of layers while improving performance metrics, bridging the gap between autoregressive and flow-based generation techniques.'}, 'zh': {'title': '潜在流变换器：高效压缩大语言模型的创新方案', 'desc': '本文提出了一种新的模型——潜在流变换器（Latent Flow Transformer, LFT），旨在提高大语言模型的效率。LFT通过使用学习的传输算子替代多个离散层，从而实现显著的压缩，同时保持与原始架构的兼容性。我们还引入了流步行（Flow Walking, FW）算法，以解决现有流基方法在保持耦合方面的局限性。实验结果表明，LFT在压缩层数的同时，能够在性能上超越传统的层跳过方法，缩小自回归和流生成范式之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2505.14674', 'title': 'Reward Reasoning Model', 'url': 'https://huggingface.co/papers/2505.14674', 'abstract': 'Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.', 'score': 14, 'issue_id': 3871, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'b51747905eeda5db', 'authors': ['Jiaxin Guo', 'Zewen Chi', 'Li Dong', 'Qingxiu Dong', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14674.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#reasoning', '#rl', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Рассуждающие модели вознаграждения: новый шаг к более точной оценке языковых моделей', 'desc': 'Статья представляет новый подход к улучшению моделей вознаграждения для больших языковых моделей - Reward Reasoning Models (RRMs). RRMs используют цепочку рассуждений для более точной оценки сложных запросов. Модели обучаются с помощью reinforcement learning без необходимости в размеченных данных с рассуждениями. Эксперименты показывают, что RRMs превосходят обычные модели вознаграждения в различных областях и могут адаптивно использовать дополнительные вычисления во время вывода для повышения точности.'}, 'en': {'title': 'Enhancing Reward Models with Adaptive Reasoning', 'desc': 'This paper introduces Reward Reasoning Models (RRMs), which enhance the performance of reward models in large language models by incorporating a structured reasoning process. RRMs utilize additional computational resources during testing to tackle complex queries where the correct rewards are not obvious. The authors employ a reinforcement learning framework that allows these models to develop their reasoning capabilities autonomously, without needing specific reasoning examples in the training data. Experimental results indicate that RRMs outperform existing reward modeling methods across various benchmarks, demonstrating their ability to adaptively use test-time compute for improved reward accuracy.'}, 'zh': {'title': '提升奖励模型的推理能力', 'desc': '奖励模型在引导大型语言模型生成符合人类期望的输出中起着关键作用。然而，在有效利用测试时计算以提升奖励模型性能方面仍然存在挑战。我们提出了奖励推理模型（RRMs），它们专门设计用于在生成最终奖励之前执行深思熟虑的推理过程。实验结果表明，RRMs在各个领域的奖励建模基准上表现优越，并能够自适应地利用测试时计算进一步提高奖励准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.14489', 'title': 'Reasoning Models Better Express Their Confidence', 'url': 'https://huggingface.co/papers/2505.14489', 'abstract': 'Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.', 'score': 13, 'issue_id': 3871, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'c945246738ceba22', 'authors': ['Dongkeun Yoon', 'Seungone Kim', 'Sohee Yang', 'Sunkyoung Kim', 'Soyeon Kim', 'Yongil Kim', 'Eunbi Choi', 'Yireun Kim', 'Minjoon Seo'], 'affiliations': ['CMU', 'KAIST', 'LG AI Research', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.14489.jpg', 'data': {'categories': ['#training', '#reasoning', '#interpretability', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Медленное мышление улучшает самооценку ИИ', 'desc': "Исследование показывает, что модели рассуждений (reasoning models) на основе больших языковых моделей (LLM) с расширенной цепочкой рассуждений (CoT) демонстрируют лучшую калибровку уверенности по сравнению с обычными LLM. Это достигается за счет 'медленного мышления' - исследования альтернативных подходов и корректировки уверенности в процессе рассуждений. Улучшение калибровки наблюдается по мере развертывания цепочки рассуждений. Удаление элементов 'медленного мышления' из CoT значительно снижает качество калибровки."}, 'en': {'title': 'Boosting Confidence Calibration in Language Models through Reasoning', 'desc': 'This paper explores how large language models (LLMs) can improve their confidence calibration through reasoning techniques. It shows that LLMs that use chain-of-thought (CoT) reasoning not only solve problems better but also express their confidence more accurately. The study benchmarks six reasoning models and finds that they outperform non-reasoning models in confidence calibration across most scenarios. The authors conclude that the slow thinking behaviors inherent in reasoning models allow them to dynamically adjust their confidence, leading to better performance as the reasoning process unfolds.'}, 'zh': {'title': '推理模型提升自信度校准的秘密', 'desc': '尽管大型语言模型（LLMs）具有很强的能力，但它们在表达自信度方面常常不准确，这使得评估其错误的可能性变得困难，从而限制了它们的可靠性。本文展示了推理模型，即进行扩展思维链（CoT）推理的LLMs，不仅在解决问题方面表现优越，而且在准确表达自信度方面也表现更佳。我们对六个推理模型在六个数据集上进行了基准测试，发现它们在36种设置中有33种情况下的自信度校准明显优于非推理模型。我们的分析表明，这种校准的提升源于推理模型的慢思维行为，如探索替代方法和回溯，使它们能够在思维链中动态调整自信度，从而逐步提高准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.14652', 'title': 'General-Reasoner: Advancing LLM Reasoning Across All Domains', 'url': 'https://huggingface.co/papers/2505.14652', 'abstract': 'Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.', 'score': 12, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '494fe90709dc6c63', 'authors': ['Xueguang Ma', 'Qian Liu', 'Dongfu Jiang', 'Ge Zhang', 'Zejun Ma', 'Wenhu Chen'], 'affiliations': ['M-A-P', 'Singapore', 'TikTok', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.14652.jpg', 'data': {'categories': ['#dataset', '#rl', '#benchmark', '#math', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Универсальный рассуждатель: расширение возможностей LLM в многодоменном анализе', 'desc': 'В статье представлен новый подход к обучению больших языковых моделей (LLM) для улучшения их способностей к рассуждению в различных областях знаний. Авторы предлагают метод General-Reasoner, который включает создание масштабного набора данных с вопросами и проверяемыми ответами из разных дисциплин. Они также разработали генеративную модель для верификации ответов, способную анализировать цепочки рассуждений и учитывать контекст. Результаты экспериментов показывают, что General-Reasoner превосходит существующие методы по эффективности и обобщаемости рассуждений на различных тестовых наборах.'}, 'en': {'title': 'Empowering LLMs with General-Reasoner for Diverse Reasoning', 'desc': "This paper introduces General-Reasoner, a new approach to improve the reasoning abilities of large language models (LLMs) across various fields. It leverages a large-scale dataset of questions with verifiable answers, which is created through web crawling, to train LLMs without the need for prior supervised fine-tuning. Additionally, it employs a generative model-based answer verifier that enhances the model's ability to understand context and think through problems. The results show that General-Reasoner significantly outperforms existing methods in reasoning tasks, especially in mathematics, while also being effective in other domains like physics and finance."}, 'zh': {'title': '提升LLM推理能力的新范式', 'desc': '强化学习（RL）在提升大型语言模型（LLM）的推理能力方面展现出强大的潜力。本文提出了一种新颖的训练范式——General-Reasoner，旨在增强LLM在多领域的推理能力。我们构建了一个大规模、高质量的问题数据集，并开发了一种基于生成模型的答案验证器，取代了传统的基于规则的验证方法。通过在多个领域的数据集上进行评估，General-Reasoner在推理性能上超越了现有的基线方法，尤其在数学推理任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2505.13866', 'title': 'Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.13866', 'abstract': 'Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60times compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.', 'score': 12, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '72f6460e348e135a', 'authors': ['Jiwon Song', 'Dongwon Jo', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Seoul National University', 'Sungkyunkwan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13866.jpg', 'data': {'categories': ['#inference', '#optimization', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Ускорение языковых моделей через сжатие путей рассуждений', 'desc': 'Статья представляет метод Сжатия Пути Рассуждений (RPC) для ускорения вывода моделей языка, ориентированных на рассуждения. RPC использует семантическую разреженность путей рассуждений, периодически сжимая KV-кэш путем сохранения наиболее важных элементов. Эксперименты показывают, что RPC увеличивает пропускную способность генерации модели QwQ-32B до 1.60 раз по сравнению с выводом с полным KV-кэшем. Метод демонстрирует, что семантическая разреженность в следах рассуждений может быть эффективно использована для сжатия, предлагая практический путь к эффективному развертыванию рассуждающих языковых моделей.'}, 'en': {'title': 'Efficient Inference with Reasoning Path Compression', 'desc': 'This paper introduces Reasoning Path Compression (RPC), a method designed to enhance the efficiency of reasoning-focused language models during inference. By utilizing the concept of semantic sparsity, RPC compresses the key-value (KV) cache, retaining only the most important elements based on recent queries. This approach significantly increases the throughput of token generation while only slightly affecting accuracy. The results indicate that RPC can improve the performance of large models like QwQ-32B, making them more practical for real-world applications.'}, 'zh': {'title': '推理路径压缩：高效推理的新方法', 'desc': '最近专注于推理的语言模型通过生成较长的中间推理路径来实现高准确率。这种方法在解决需要逻辑思维的问题时非常有效，但长推理路径显著增加了内存使用和令牌生成的吞吐量，限制了模型的实际应用。我们提出了一种名为推理路径压缩（RPC）的方法，通过利用推理路径的语义稀疏性来加速推理。实验表明，RPC在AIME 2024基准测试中相比于完整KV缓存，提升了QwQ-32B的生成吞吐量，准确率仅下降1.2%。'}}}, {'id': 'https://huggingface.co/papers/2505.13547', 'title': 'Exploring Federated Pruning for Large Language Models', 'url': 'https://huggingface.co/papers/2505.13547', 'abstract': 'LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.', 'score': 12, 'issue_id': 3872, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '436f0f2e8c3f8481', 'authors': ['Pengxin Guo', 'Yinong Wang', 'Wei Li', 'Mengting Liu', 'Ming Li', 'Jinkai Zheng', 'Liangqiong Qu'], 'affiliations': ['Guangming Laboratory', 'Hangzhou Dianzi University', 'Southern University of Science and Technology', 'Sun Yat-sen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13547.jpg', 'data': {'categories': ['#inference', '#security', '#optimization', '#open_source', '#training'], 'emoji': '🔒', 'ru': {'title': 'Федеративное сжатие языковых моделей с сохранением приватности данных', 'desc': 'FedPrLLM - это новая федеративная система для приватного сжатия больших языковых моделей (LLM). Она позволяет клиентам вычислять маски прунинга на локальных данных и делиться ими с сервером для обрезки глобальной модели. Эксперименты показали, что одноразовый прунинг с посдойным сравнением и без масштабирования весов дает оптимальные результаты. Этот подход может помочь в сжатии LLM для конфиденциальных областей применения.'}, 'en': {'title': 'Privacy-Preserving Compression of LLMs with FedPrLLM', 'desc': 'This paper presents FedPrLLM, a federated pruning framework aimed at compressing large language models (LLMs) while ensuring data privacy. Unlike traditional methods that require public calibration samples, FedPrLLM allows clients to generate pruning masks using their local data, which are then shared with a central server. This collaborative approach enables the global model to be pruned without exposing sensitive local data. The authors conducted experiments to identify the best pruning strategies, concluding that one-shot pruning with layer comparison and no weight scaling is the most effective method within their framework.'}, 'zh': {'title': '隐私保护下的LLM剪枝新方法', 'desc': 'LLM剪枝是一种有前景的技术，可以压缩大型语言模型（LLM），使其能够在资源有限的设备上运行。现有的方法通常需要公共校准样本，这在隐私敏感的领域中很难获得。为了解决这个问题，我们提出了FedPrLLM，这是一个全面的联邦剪枝框架，旨在保护隐私的同时压缩LLM。在FedPrLLM中，每个客户端只需根据本地校准数据计算剪枝掩码矩阵，并将其与服务器共享，从而对全局模型进行剪枝。'}}}, {'id': 'https://huggingface.co/papers/2505.14677', 'title': 'Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.14677', 'abstract': 'Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks.', 'score': 11, 'issue_id': 3873, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '032b4d528d6984fd', 'authors': ['Jiaer Xia', 'Yuhang Zang', 'Peng Gao', 'Yixuan Li', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University', 'Shanghai AI Lab', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.14677.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#cv', '#multimodal', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Визуальное рассуждение без явного обучения цепочке мыслей', 'desc': "Исследователи разработали метод обучения визуальных языковых моделей (VLM) выполнять рассуждения на основе изображений с помощью обучения с подкреплением и пар визуальных вопросов-ответов. Они обнаружили, что простое применение обучения с подкреплением может привести к появлению у модели нежелательных 'shortcuts'. Для решения этой проблемы авторы предложили формат вывода 'подпись-рассуждение-ответ', который побуждает модель сначала интерпретировать изображение. Разработанная модель Visionary-R1 превзошла другие сильные мультимодальные модели на нескольких тестах визуальных рассуждений."}, 'en': {'title': 'Enhancing Visual Reasoning with Captions First!', 'desc': 'This paper addresses the challenge of enhancing reasoning capabilities in visual language models (VLMs) using reinforcement learning. The authors propose a novel training approach that emphasizes generating detailed captions for images before reasoning, which helps prevent shortcut learning. By training their model, Visionary-R1, on a large dataset of visual question-answer pairs without explicit chain-of-thought supervision, they achieve superior performance compared to existing multimodal models. The results suggest that focusing on image interpretation prior to reasoning can significantly improve generalization across diverse data distributions.'}, 'zh': {'title': '通过强化学习提升视觉语言模型的推理能力', 'desc': '本论文探讨了如何通过强化学习训练视觉语言模型（VLM）来进行图像数据的推理。我们的方法不依赖于显式的思维链（CoT）监督，而是通过视觉问答对来实现。研究表明，简单地应用强化学习可以导致模型在回答之前生成推理链，但这可能导致模型在面对新数据时的泛化能力下降。为了解决这个问题，我们提出让模型在推理之前先对图像进行解释，从而提高其推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.14673', 'title': 'Training-Free Watermarking for Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2505.14673', 'abstract': 'Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.', 'score': 11, 'issue_id': 3873, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'ec739be428657981', 'authors': ['Yu Tong', 'Zihao Pan', 'Shuai Yang', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University', 'Peking University', 'Sun Yat-sen University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14673.jpg', 'data': {'categories': ['#cv', '#data', '#training', '#security'], 'emoji': '🖼️', 'ru': {'title': 'Незаметная защита авторства изображений без переобучения модели', 'desc': 'IndexMark - это безтренировочный метод встраивания водяных знаков в автореггрессивные модели генерации изображений. Он использует избыточность кодовой книги, заменяя сгенерированные индексы на похожие для внедрения водяного знака без ухудшения качества изображения. Верификация водяного знака осуществляется путем подсчета доли токенов водяного знака в сгенерированных изображениях. Эксперименты показывают, что IndexMark достигает высоких результатов по качеству изображений и точности верификации, а также демонстрирует устойчивость к различным искажениям.'}, 'en': {'title': 'IndexMark: Watermarking Autoregressive Models with Precision and Robustness', 'desc': 'This paper presents IndexMark, a novel watermarking framework specifically designed for autoregressive image generation models. Unlike previous methods focused on diffusion models, IndexMark utilizes the redundancy in the codebook to embed watermarks without compromising image quality. The framework employs a match-then-replace strategy to select and replace indices with similar ones, effectively embedding the watermark. Additionally, it includes a robust verification process and an auxiliary validation scheme to withstand various image perturbations, demonstrating superior performance in both image quality and watermark verification accuracy.'}, 'zh': {'title': '自回归图像生成的隐形水印新方案', 'desc': '隐形图像水印技术可以保护图像的所有权，并防止视觉生成模型的恶意滥用。现有的生成水印方法主要针对扩散模型，而自回归图像生成模型的水印技术尚未得到充分研究。我们提出了IndexMark，这是一种无需训练的自回归图像生成模型水印框架。IndexMark通过匹配和替换的方法，利用代码本的冗余特性，嵌入水印而不影响图像质量，并在多种干扰下表现出良好的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2505.14631', 'title': 'Think Only When You Need with Large Hybrid-Reasoning Models', 'url': 'https://huggingface.co/papers/2505.14631', 'abstract': "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.", 'score': 11, 'issue_id': 3872, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '12732abf8e9d807f', 'authors': ['Lingjie Jiang', 'Xun Wu', 'Shaohan Huang', 'Qingxiu Dong', 'Zewen Chi', 'Li Dong', 'Xingxing Zhang', 'Tengchao Lv', 'Lei Cui', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14631.jpg', 'data': {'categories': ['#architecture', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Гибридное мышление: новый шаг в развитии искусственного интеллекта', 'desc': 'Статья представляет новый тип моделей машинного обучения - Large Hybrid-Reasoning Models (LHRMs), способных адаптивно определять необходимость применения расширенного процесса мышления в зависимости от контекста запроса пользователя. Авторы предлагают двухэтапный процесс обучения, включающий Hybrid Fine-Tuning (HFT) и онлайн-обучение с подкреплением с использованием Hybrid Group Policy Optimization (HGPO). Введена метрика Hybrid Accuracy для оценки способности модели к гибридному мышлению. Эксперименты показывают, что LHRMs превосходят существующие LRMs и LLMs в рассуждениях и общих возможностях, значительно повышая эффективность.'}, 'en': {'title': 'Adaptive Thinking for Efficient Reasoning', 'desc': 'This paper presents Large Hybrid-Reasoning Models (LHRMs), which enhance reasoning abilities by deciding when to engage in extended thinking based on the complexity of user queries. Unlike traditional Large Language Models (LLMs), LHRMs use a two-stage training approach that includes Hybrid Fine-Tuning and online reinforcement learning to optimize their reasoning process. The authors introduce a new metric, Hybrid Accuracy, to evaluate the effectiveness of these models in adapting their thinking strategies. Experimental results demonstrate that LHRMs outperform existing models in both reasoning and efficiency, suggesting a new direction for developing intelligent systems that balance thinking depth with response speed.'}, 'zh': {'title': '自适应混合推理，提升效率与能力', 'desc': '最近的大型推理模型（LRMs）在推理能力上显著优于传统的大型语言模型（LLMs），因为它们在生成最终回答之前进行了更深入的思考。然而，过长的思考过程会导致令牌消耗和延迟的显著增加，这在处理简单查询时尤其不必要。为此，我们提出了大型混合推理模型（LHRMs），这种模型能够根据用户查询的上下文信息自适应地决定是否进行思考。我们的研究表明，LHRMs在处理不同难度和类型的查询时，能够有效地进行混合思考，并在推理和整体能力上超越现有的LRMs和LLMs，同时显著提高效率。'}}}, {'id': 'https://huggingface.co/papers/2505.14640', 'title': 'VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation', 'url': 'https://huggingface.co/papers/2505.14640', 'abstract': "Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (>25\\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.", 'score': 10, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '45d64d535935c6a4', 'authors': ['Wentao Ma', 'Weiming Ren', 'Yiming Jia', 'Zhuofeng Li', 'Ping Nie', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['Independent', 'M-A-P', 'Shanghai University', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.14640.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#long_context', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'Реалистичная оценка понимания длинных видео: от угадывания к глубокому анализу', 'desc': 'Эта статья посвящена проблемам существующих бенчмарков для оценки понимания длинных видео большими мультимодальными моделями (LMM). Авторы выявили, что текущие бенчмарки часто используют вопросы с множественным выбором, что приводит к завышенным результатам из-за возможности угадывания. Они предлагают новый бенчмарк VideoEval-Pro с открытыми вопросами, требующими понимания всего видео. Результаты показывают, что производительность моделей на открытых вопросах значительно ниже, чем на вопросах с множественным выбором, что дает более реалистичную оценку способностей LMM к пониманию длинных видео.'}, 'en': {'title': 'Revolutionizing Long Video Understanding with Realistic Benchmarks', 'desc': 'This paper discusses the limitations of current benchmarks for evaluating long video understanding (LVU) in large multimodal models (LMMs). It highlights that many existing benchmarks rely on multiple-choice questions (MCQs), which can inflate performance scores due to guessing and prior knowledge. The authors introduce VideoEval-Pro, a new benchmark that uses open-ended questions requiring comprehensive video understanding, thus providing a more accurate assessment of LMM capabilities. Their findings reveal significant performance drops for LMMs on open-ended questions compared to MCQs, indicating that current benchmarks may not effectively measure true understanding of long videos.'}, 'zh': {'title': 'VideoEval-Pro：提升长视频理解的真实评估', 'desc': '大型多模态模型（LMMs）在长视频理解（LVU）中表现出色，但现有的LVU基准测试存在问题。许多基准依赖多项选择题（MCQs），这导致评估结果被夸大，因为模型可能通过猜测获得正确答案。此外，部分问题的先验信息使得模型可以在不观看视频的情况下直接回答。为了解决这些问题，我们提出了VideoEval-Pro基准，采用开放式短答案问题，真正考察模型对整个视频的理解能力。'}}}, {'id': 'https://huggingface.co/papers/2505.13559', 'title': 'CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models', 'url': 'https://huggingface.co/papers/2505.13559', 'abstract': 'Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.', 'score': 9, 'issue_id': 3870, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '9ea21df740810e7e', 'authors': ['Sathya Krishnan Suresh', 'Tanmay Surana', 'Lim Zhi Hao', 'Eng Siong Chng'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13559.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#low_resource', '#synthetic', '#training', '#benchmark', '#multilingual'], 'emoji': '🗣️', 'ru': {'title': 'CS-Sum: новый бенчмарк для оценки понимания переключения кодов в больших языковых моделях', 'desc': 'Статья представляет новый бенчмарк CS-Sum для оценки понимания переключения кодов (code-switching) в больших языковых моделях. Бенчмарк включает задачу суммаризации диалогов с переключением кодов на английский язык для пар китайский-английский, тамильский-английский и малайский-английский. Авторы оценили производительность десяти языковых моделей, используя различные подходы, включая few-shot обучение и тонкую настройку. Результаты показывают, что модели часто допускают тонкие ошибки, искажающие смысл диалога, несмотря на высокие показатели автоматических метрик.'}, 'en': {'title': 'Enhancing LLMs for Code-Switching Comprehensibility', 'desc': 'This paper addresses the challenges that Large Language Models (LLMs) face when dealing with code-switching (CS) in dialogues. It introduces CS-Sum, a benchmark designed to assess how well LLMs can summarize CS dialogues into English, focusing on three language pairs: Mandarin-English, Tamil-English, and Malay-English. The study evaluates ten different LLMs using various methods, including few-shot learning and fine-tuning techniques, to understand their performance on CS data. The results reveal that while LLMs achieve high scores on automated metrics, they often make subtle errors that can change the meaning of the dialogues, highlighting the need for improved training on code-switched content.'}, 'zh': {'title': '评估代码切换的可理解性', 'desc': '代码切换（CS）对大型语言模型（LLMs）构成了重大挑战，但其可理解性在LLMs中的研究仍然不足。我们提出了CS-Sum，旨在通过将CS对话总结为英语来评估LLMs对CS的理解能力。CS-Sum是首个针对普通话-英语、泰米尔语-英语和马来语-英语的CS对话总结基准，包含每对语言900到1300个人工标注的对话。我们的研究发现，尽管自动评估指标得分较高，LLMs在处理CS输入时仍会出现细微错误，这些错误会改变对话的完整含义。'}}}, {'id': 'https://huggingface.co/papers/2505.14680', 'title': 'NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search', 'url': 'https://huggingface.co/papers/2505.14680', 'abstract': "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.", 'score': 8, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'ace242db16327202', 'authors': ['Sunhao Dai', 'Wenjie Wang', 'Liang Pang', 'Jun Xu', 'See-Kiong Ng', 'Ji-Rong Wen', 'Tat-Seng Chua'], 'affiliations': ['CAS Key Laboratory of AI Safety Institute of Computing Technology Chinese Academy of Sciences', 'Gaoling School of Artificial Intelligence Renmin University of China', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.14680.jpg', 'data': {'categories': ['#interpretability', '#rag', '#rlhf', '#agents', '#alignment'], 'emoji': '🔍', 'ru': {'title': 'Возвращение человеческого контроля в ИИ-поиск', 'desc': 'Статья представляет концепцию NExT-Search, новую парадигму генеративного ИИ-поиска. Она направлена на восстановление детальной обратной связи в процессе поиска, что было утрачено при переходе от традиционного веб-поиска к генеративному ИИ. NExT-Search предлагает два режима: режим отладки пользователем и режим теневого пользователя, позволяющие собирать обратную связь на разных этапах поиска. Система использует эту обратную связь для онлайн-адаптации и офлайн-обновления моделей декомпозиции запросов, извлечения и генерации ответов.'}, 'en': {'title': 'NExT-Search: Enhancing Generative AI Search with User Feedback', 'desc': 'This paper discusses the challenges of integrating user feedback into generative AI search systems, which provide direct answers to complex queries but lack detailed feedback mechanisms. Traditional web search benefits from fine-grained user interactions, allowing for continuous improvement of ranking models. The proposed NExT-Search framework aims to bridge this gap by introducing two modes of user feedback: User Debug Mode for active user engagement and Shadow User Mode for passive feedback collection. By leveraging both real-time and aggregated feedback, NExT-Search seeks to enhance the generative AI search process and ensure it evolves in response to user needs.'}, 'zh': {'title': 'NExT-Search：重塑生成式搜索的反馈循环', 'desc': '生成式人工智能搜索正在改变信息检索，通过提供端到端的答案来应对复杂查询，减少用户手动浏览和总结多个网页的依赖。然而，这种新模式虽然提高了便利性，却打破了传统网页搜索中基于反馈的改进循环。传统搜索可以通过收集用户反馈（如点击率和停留时间）来不断改进排名模型，而生成式搜索则面临反馈循环断裂的问题，用户反馈难以有效映射到系统的具体组件。本文提出了NExT-Search，旨在将细粒度的过程级反馈重新引入生成式搜索，结合用户调试模式和影子用户模式，以实现实时和离线的反馈信号利用，从而持续改进搜索系统。'}}}, {'id': 'https://huggingface.co/papers/2505.14135', 'title': 'Hunyuan-Game: Industrial-grade Intelligent Game Creation Model', 'url': 'https://huggingface.co/papers/2505.14135', 'abstract': 'Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.', 'score': 8, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '344469b85ea1e75e', 'authors': ['Ruihuang Li', 'Caijin Zhou', 'Shoujian Zheng', 'Jianxiang Lu', 'Jiabin Huang', 'Comi Chen', 'Junshu Tang', 'Guangzheng Xu', 'Jiale Tao', 'Hongmei Wang', 'Donghao Li', 'Wenqing Yu', 'Senbo Wang', 'Zhimin Li', 'Yetshuan Shi', 'Haoyu Yang', 'Yukun Wang', 'Wenxun Dai', 'Jiaqi Li', 'Linqing Wang', 'Qixun Wang', 'Zhiyong Xu', 'Yingfang Zhang', 'Jiangfeng Xiong', 'Weijie Kong', 'Chao Zhang', 'Hongxin Zhang', 'Qiaoling Zheng', 'Weiting Guo', 'Xinchi Deng', 'Yixuan Li', 'Renjia Wei', 'Yulin Jian', 'Duojun Huang', 'Xuhua Ren', 'Sihuan Lin', 'Yifu Sun', 'Yuan Zhou', 'Joey Wang', 'Qin Lin', 'Jingmiao Yu', 'Jihong Zhang', 'Caesar Zhong', 'Di Wang', 'Yuhong Liu', 'Linus', 'Jie Jiang', 'Longhuang Wu', 'Shuai Shao', 'Qinglin Lu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.14135.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#cv', '#games', '#diffusion', '#video'], 'emoji': '🎮', 'ru': {'title': 'Революция в создании игр: ИИ на службе разработчиков', 'desc': 'Проект Hunyuan-Game представляет собой инновационный подход к интеллектуальному созданию игр с использованием генеративного искусственного интеллекта. Он включает в себя две основные ветви: генерацию изображений и генерацию видео, каждая из которых основана на обширных наборах данных игровых ресурсов. Модели генерации изображений способны создавать различные игровые элементы, включая общие сцены, визуальные эффекты и персонажей. Компонент генерации видео предлагает пять ключевых алгоритмических моделей, направленных на решение критических проблем в разработке игр.'}, 'en': {'title': 'Revolutionizing Game Development with AI-Driven Content Creation', 'desc': 'The paper introduces Hunyuan-Game, a project that leverages generative artificial intelligence to enhance game development by creating high-quality game assets. It focuses on two main areas: image generation and video generation, utilizing extensive datasets of game images and videos. The image generation models include various techniques for creating game visuals, such as text-to-image and character generation from sketches. The video generation models address specific challenges in game video production, offering solutions like image-to-video synthesis and interactive video generation, all while maintaining aesthetic quality and understanding of game art styles.'}, 'zh': {'title': '智能游戏创作的未来', 'desc': '智能游戏创作是游戏开发中的一项变革性进展，利用生成性人工智能动态生成和增强游戏内容。尽管生成模型取得了显著进展，但高质量游戏资产的综合合成仍然是一个挑战。Hunyuan-Game项目旨在通过图像和视频生成，提升游戏内容的质量和设计师的效率。该项目包括图像生成和视频生成两个主要部分，涵盖了多种定制化的生成模型，能够满足不同游戏场景的需求。'}}}, {'id': 'https://huggingface.co/papers/2505.13430', 'title': 'Fine-tuning Quantized Neural Networks with Zeroth-order Optimization', 'url': 'https://huggingface.co/papers/2505.13430', 'abstract': 'As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.', 'score': 8, 'issue_id': 3873, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '6728dda02398fbcc', 'authors': ['Sifeng Shang', 'Jiayi Zhou', 'Chenyu Lin', 'Minxian Li', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University', 'Nanjing University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.13430.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'QZO: Революция в эффективном обучении крупных языковых моделей', 'desc': 'Статья представляет новый метод оптимизации крупных языковых моделей - Quantized Zeroth-order Optimization (QZO). Этот подход позволяет значительно сократить использование памяти GPU при адаптации больших моделей к конкретным задачам. QZO использует оптимизацию нулевого порядка и квантование модели для устранения необходимости хранения градиентов и состояний оптимизатора. Метод преодолевает проблему разрыва точности между дискретными весами и непрерывными градиентами, возмущая непрерывную шкалу квантования для оценки градиентов.'}, 'en': {'title': 'Revolutionizing Memory Efficiency in Large Language Model Training', 'desc': 'This paper addresses the challenge of training large language models (LLMs) with limited GPU memory. It introduces a method called Quantized Zeroth-order Optimization (QZO) that reduces memory usage by eliminating the need for gradients and optimizer states. QZO achieves this by perturbing the quantization scale of weights to estimate gradients, allowing for efficient training without the need for de-quantization. The proposed approach significantly lowers memory costs, enabling the fine-tuning of large models on standard GPUs.'}, 'zh': {'title': '突破内存瓶颈，实现高效训练', 'desc': '随着大型语言模型规模的快速增长，GPU内存成为适应这些模型到下游任务的瓶颈。本文提出了一种内存高效训练的方法，通过在统一框架内最小化模型权重、梯度和优化器状态的内存使用。我们使用零阶优化来消除梯度和优化器状态，通过在前向传播中扰动权重来近似梯度方向。我们还提出了量化零阶优化（QZO），通过扰动连续量化尺度来估计梯度，从而在不牺牲精度的情况下显著减少内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2505.14681', 'title': 'Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training', 'url': 'https://huggingface.co/papers/2505.14681', 'abstract': "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.", 'score': 7, 'issue_id': 3878, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '89ee9aa82837601e', 'authors': ['Mengru Wang', 'Xingyu Chen', 'Yue Wang', 'Zhiwei He', 'Jiahao Xu', 'Tian Liang', 'Qiuzhi Liu', 'Yunzhi Yao', 'Wenxuan Wang', 'Ruotian Ma', 'Haitao Mi', 'Ningyu Zhang', 'Zhaopeng Tu', 'Xiaolong Li', 'Dong Yu'], 'affiliations': ['Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14681.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#architecture', '#benchmark', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Усиление когнитивных экспертов для эффективного машинного рассуждения', 'desc': "Статья представляет новый метод под названием RICE для улучшения рассуждений в моделях машинного обучения с архитектурой Mixture-of-Experts. Используя нормализованную поточечную взаимную информацию (nPMI), авторы выявляют специализированные 'когнитивные эксперты', отвечающие за мета-уровневые операции рассуждения. Эксперименты с ведущими моделями типа MoE показали заметное улучшение точности рассуждений, когнитивной эффективности и обобщения на разные домены. Подход RICE превосходит существующие методы управления рассуждениями, сохраняя при этом общие навыки модели следовать инструкциям."}, 'en': {'title': 'Enhancing Reasoning Efficiency with Cognitive Experts', 'desc': "This paper presents a new method called Reinforcing Cognitive Experts (RICE) to improve the reasoning capabilities of Mixture-of-Experts (MoE) architectures in Large Reasoning Models (LRMs). RICE addresses issues of cognitive inefficiencies, such as overthinking and underthinking, by selectively activating specialized experts during inference. The method uses normalized Pointwise Mutual Information (nPMI) to identify these 'cognitive experts' that enhance meta-level reasoning processes. Empirical results show that RICE significantly boosts reasoning accuracy and efficiency compared to existing techniques, while maintaining the model's ability to follow instructions effectively."}, 'zh': {'title': '强化认知专家：提升推理效率的新方法', 'desc': '混合专家（MoE）架构在大型推理模型（LRMs）中通过选择性激活专家来实现出色的推理能力。然而，现有的推理模型常常面临认知效率低下的问题，如过度思考和不足思考。为了解决这些问题，我们提出了一种新的推理时间引导方法，称为强化认知专家（RICE），旨在在不增加额外训练或复杂启发式的情况下提高推理性能。通过利用归一化的点互信息（nPMI），我们系统地识别出专门的专家，称为“认知专家”，以协调以“<think>”等标记为特征的元级推理操作。'}}}, {'id': 'https://huggingface.co/papers/2505.14464', 'title': 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters', 'url': 'https://huggingface.co/papers/2505.14464', 'abstract': 'Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging FaceDatasets are available on Hugging Face: \\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled}, https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.', 'score': 7, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '709996374c466144', 'authors': ['Xiaoyu Tian', 'Yunjie Ji', 'Haotian Wang', 'Shuaiting Chen', 'Sitong Zhao', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.14464.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#data', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Дистилляция знаний улучшает способности ИИ к рассуждению', 'desc': 'Это исследование посвящено дистилляции данных для улучшения способностей рассуждения языковых моделей с открытым исходным кодом. Авторы собрали верифицированные выходные данные от трех современных моделей-учителей на корпусе из 1,89 миллиона запросов. Анализ показал, что данные, дистиллированные из модели AM-Thinking-v1, обладают большим разнообразием длины токенов и меньшей перплексией. Модели-ученики, обученные на этих данных, продемонстрировали лучшие результаты на нескольких тестах по рассуждению.'}, 'en': {'title': 'Enhancing Reasoning in Language Models through Data Distillation', 'desc': 'This paper explores the process of distillation to improve the reasoning abilities of open-source language models. The authors conducted a large-scale study using outputs from three advanced teacher models on a dataset of 1.89 million queries. They found that the distilled data from the AM-Thinking-v1 model had better diversity in token length and lower perplexity, leading to superior performance on various reasoning benchmarks. The results indicate that high-quality reasoning data is crucial for training effective student models, and the authors have made their datasets publicly available for further research.'}, 'zh': {'title': '蒸馏技术提升语言模型推理能力', 'desc': '本研究探讨了通过蒸馏技术提升开源语言模型推理能力的方法。我们收集了来自三种先进教师模型的验证输出，并构建了三个平行数据集进行分析。结果显示，AM-Thinking-v1蒸馏数据在标记长度多样性和困惑度方面表现更佳。经过训练的学生模型在多个推理基准测试中表现优异，特别是AM-Thinking-v1模型在各项测试中均取得了最佳成绩，展示了高质量推理轨迹的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.12448', 'title': 'SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning', 'url': 'https://huggingface.co/papers/2505.12448', 'abstract': 'Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.', 'score': 7, 'issue_id': 3869, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': '18ffd5153e838d86', 'authors': ['Yang Liu', 'Ming Ma', 'Xiaomin Yu', 'Pengxiang Ding', 'Han Zhao', 'Mingyang Sun', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['Alibaba DAMO Academy', 'Harbin Institute of Technology', 'Shanghai Innovation Institute', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12448.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#cv', '#interpretability', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение пространственного понимания в визуально-языковых моделях с помощью структурированных текстовых обоснований', 'desc': 'Статья представляет новый метод под названием SSR (Spatial Sense and Reasoning) для улучшения пространственного понимания в визуально-языковых моделях (VLM). SSR преобразует данные о глубине в структурированные текстовые обоснования, которые затем сжимаются в компактные латентные представления с помощью дистилляции знаний. Авторы также представляют новый набор данных SSR-CoT и бенчмарк SSRBench для оценки пространственных рассуждений в мультимодальных задачах. Эксперименты показывают, что SSR значительно улучшает использование информации о глубине и повышает способность моделей к пространственным рассуждениям.'}, 'en': {'title': 'Enhancing Spatial Reasoning in VLMs with SSR', 'desc': 'This paper introduces a new method called Spatial Sense and Reasoning (SSR) to improve how Visual-Language Models (VLMs) understand spatial information. SSR transforms raw depth data into structured textual rationales, which help the model reason about space more effectively. The authors also use knowledge distillation to create compact representations of these rationales, allowing for easy integration into existing VLMs without needing to retrain them. To support their research, they present a new dataset, SSR-CoT, and a benchmark, SSRBench, which show that SSR significantly enhances spatial reasoning in VLMs.'}, 'zh': {'title': '提升空间推理能力的创新方法', 'desc': '尽管视觉语言模型（VLMs）在多模态任务上取得了显著进展，但它们对RGB输入的依赖限制了精确的空间理解。现有的方法在整合空间线索时，往往需要专用传感器或无法有效利用深度信息进行更高阶的推理。为此，我们提出了一种新颖的空间感知与推理方法（SSR），该框架将原始深度数据转化为结构化的可解释文本推理。这些文本推理作为有意义的中间表示，显著增强了空间推理能力，并通过知识蒸馏将生成的推理压缩为紧凑的潜在嵌入，便于与现有VLMs的高效集成。'}}}, {'id': 'https://huggingface.co/papers/2505.14352', 'title': 'Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability', 'url': 'https://huggingface.co/papers/2505.14352', 'abstract': 'As language models become more powerful and sophisticated, it is crucial that they remain trustworthy and reliable. There is concerning preliminary evidence that models may attempt to deceive or keep secrets from their operators. To explore the ability of current techniques to elicit such hidden knowledge, we train a Taboo model: a language model that describes a specific secret word without explicitly stating it. Importantly, the secret word is not presented to the model in its training data or prompt. We then investigate methods to uncover this secret. First, we evaluate non-interpretability (black-box) approaches. Subsequently, we develop largely automated strategies based on mechanistic interpretability techniques, including logit lens and sparse autoencoders. Evaluation shows that both approaches are effective in eliciting the secret word in our proof-of-concept setting. Our findings highlight the promise of these approaches for eliciting hidden knowledge and suggest several promising avenues for future work, including testing and refining these methods on more complex model organisms. This work aims to be a step towards addressing the crucial problem of eliciting secret knowledge from language models, thereby contributing to their safe and reliable deployment.', 'score': 6, 'issue_id': 3874, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '6b06f2f5351e8b60', 'authors': ['Bartosz Cywiński', 'Emil Ryd', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2505.14352.jpg', 'data': {'categories': ['#hallucinations', '#inference', '#training', '#security', '#interpretability'], 'emoji': '🕵️', 'ru': {'title': 'Раскрытие секретов искусственного интеллекта: новые методы интерпретации языковых моделей', 'desc': 'Статья исследует методы выявления скрытых знаний в языковых моделях. Авторы обучают модель Taboo, которая описывает секретное слово, не называя его явно. Затем они применяют методы интерпретируемости, включая logit lens и разреженные автоэнкодеры, для раскрытия этого секрета. Результаты показывают эффективность этих подходов и открывают перспективы для дальнейших исследований в области безопасности и надежности языковых моделей.'}, 'en': {'title': 'Unveiling Secrets: Enhancing Trust in Language Models', 'desc': 'This paper addresses the challenge of ensuring that powerful language models remain trustworthy by investigating their potential to conceal information. The authors introduce a Taboo model, which is designed to describe a secret word without revealing it directly, even though the word is not included in the training data. They evaluate two main approaches to uncover this hidden knowledge: black-box methods and mechanistic interpretability techniques, such as logit lens and sparse autoencoders. The results demonstrate that these methods can effectively elicit the secret word, paving the way for future research on improving the transparency and reliability of language models.'}, 'zh': {'title': '揭示语言模型中的隐藏知识', 'desc': '随着语言模型变得越来越强大和复杂，确保它们的可信性和可靠性变得至关重要。研究表明，模型可能会试图欺骗或隐瞒信息。为此，我们训练了一个禁忌模型，它在不直接说明特定秘密词的情况下进行描述。我们的研究表明，使用黑箱方法和机械解释技术可以有效地揭示这些隐藏的知识，推动未来在更复杂模型上的应用。'}}}, {'id': 'https://huggingface.co/papers/2505.14534', 'title': 'Lessons from Defending Gemini Against Indirect Prompt Injections', 'url': 'https://huggingface.co/papers/2505.14534', 'abstract': "Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.", 'score': 5, 'issue_id': 3874, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '1e855b0dd2463fec', 'authors': ['Chongyang Shi', 'Sharon Lin', 'Shuang Song', 'Jamie Hayes', 'Ilia Shumailov', 'Itay Yona', 'Juliette Pluto', 'Aneesh Pappu', 'Christopher A. Choquette-Choo', 'Milad Nasr', 'Chawin Sitawarin', 'Gena Gibson', 'Andreas Terzis', 'John "Four" Flynn'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2505.14534.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Укрепление защиты Gemini от состязательных атак', 'desc': 'Статья описывает подход Google DeepMind к оценке устойчивости моделей Gemini к состязательным атакам. Авторы разработали систему непрерывного тестирования моделей с использованием адаптивных методов атак. Исследование направлено на выявление уязвимостей, связанных с обработкой ненадежных данных при выполнении задач от имени пользователей. Результаты оценки помогают повысить устойчивость Gemini к манипуляциям со стороны злоумышленников.'}, 'en': {'title': 'Strengthening Gemini: Safeguarding User Data Against Adversarial Attacks', 'desc': "The paper discusses the challenges of ensuring the security of Gemini, a machine learning model that performs tasks for users by accessing their data. It highlights the risks posed by untrusted data, which can contain malicious instructions that lead the model to behave unexpectedly. To address these risks, Google DeepMind has developed an adversarial evaluation framework that tests Gemini's robustness against sophisticated attacks. The ongoing evaluations aim to enhance Gemini's resilience, ensuring it handles user data and permissions safely and effectively."}, 'zh': {'title': '提升Gemini模型的对抗性鲁棒性', 'desc': 'Gemini模型被广泛用于执行用户任务，但在使用工具时可能会接触到不可信的数据，这带来了风险。恶意攻击者可以在不可信的数据中嵌入恶意指令，导致模型偏离用户的期望，错误处理用户的数据或权限。本文介绍了Google DeepMind评估Gemini模型对抗性鲁棒性的方法，并总结了在这一过程中获得的主要经验教训。通过对Gemini进行持续的对抗性评估，我们能够提高其抵御操控的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.13718', 'title': 'Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings', 'url': 'https://huggingface.co/papers/2505.13718', 'abstract': 'Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we "warm up" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: (i) the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both the base model and the warmed-up model are RLVR trained on the same small dataset (leq100 examples), the warmed-up model consistently outperforms the base model; (iii) Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; (iv) Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.', 'score': 5, 'issue_id': 3872, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '29613228991289b5', 'authors': ['Safal Shrestha', 'Minwu Kim', 'Aadim Nepal', 'Anubhav Shrestha', 'Keith Ross'], 'affiliations': ['New York University Abu Dhabi'], 'pdf_title_img': 'assets/pdf/title_img/2505.13718.jpg', 'data': {'categories': ['#data', '#rl', '#long_context', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение LLM рассуждению с минимумом данных', 'desc': "Исследователи предлагают эффективный двухэтапный метод обучения языковых моделей (LLM) навыкам рассуждения при ограниченном количестве данных. На первом этапе модель 'разогревается' на логических головоломках, приобретая общие навыки рассуждения. На втором этапе применяется обучение с подкреплением с проверяемыми наградами (RLVR) на небольшом наборе целевых примеров. Эксперименты показывают, что такой подход улучшает производительность модели на различных задачах и повышает эффективность использования данных при обучении RLVR. Метод особенно перспективен для создания надежных LLM в условиях ограниченности данных."}, 'en': {'title': 'Warmup for Robust Reasoning in Data-Scarce LLMs', 'desc': 'This paper presents a novel two-stage training strategy for developing reasoning-capable large language models (LLMs) when high-quality training data is limited. The first stage involves warming up the model by distilling Long Chains of Thought (CoT) from simple logic puzzles, which helps the model acquire general reasoning skills. In the second stage, Reinforcement Learning with Verifiable Rewards (RLVR) is applied using a small set of examples from the target domain. The results show that this approach enhances performance across various tasks and improves sample efficiency, demonstrating the effectiveness of the warmup phase in building robust reasoning LLMs.'}, 'zh': {'title': '在数据稀缺环境中构建强大推理模型的有效策略', 'desc': '本文提出了一种在有限监督下开发推理能力强的语言模型（LLM）的两阶段训练策略。第一阶段通过从简单的逻辑谜题（骑士与骗子）中提取长思维链（CoT）来“预热”模型，以获取一般推理技能。第二阶段则使用有限的目标领域示例对预热后的模型进行强化学习与可验证奖励（RLVR）训练。实验结果表明，这种两阶段的方法在数据稀缺的环境中能够有效提高模型的推理能力和样本效率。'}}}, {'id': 'https://huggingface.co/papers/2505.13103', 'title': 'Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair', 'url': 'https://huggingface.co/papers/2505.13103', 'abstract': 'The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness.   We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.', 'score': 5, 'issue_id': 3874, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '4d2278523a88b9bc', 'authors': ['Han Zheng', 'Ilia Shumailov', 'Tianqi Fan', 'Aiden Hall', 'Mathias Payer'], 'affiliations': ['EPFL Lausanne, Switzerland', 'Google DeepMind London, UK', 'Google New York, USA', 'Google Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2505.13103.jpg', 'data': {'categories': ['#agents', '#plp', '#training', '#optimization', '#open_source', '#benchmark'], 'emoji': '🛠️', 'ru': {'title': 'WILLIAMT: Эффективное автоматическое исправление программ с меньшими затратами', 'desc': 'Статья представляет новый подход к автоматическому исправлению программ (APR) под названием WILLIAMT. Система фокусируется на исправлении места сбоя программы и использует шаблоны для генерации патчей, что значительно снижает затраты токенов при использовании больших языковых моделей (LLM). WILLIAMT показывает улучшение на 29.6% в скорости исправления ошибок по сравнению с современными APR инструментами на бенчмарке ARVO. Исследование также демонстрирует, что WILLIAMT может эффективно работать даже с локальными моделями на обычных компьютерах.'}, 'en': {'title': 'Simplifying Bug Fixes with WILLIAMT: Efficient Automated Program Repair', 'desc': 'This paper addresses the challenge of fixing software bugs, which have become too numerous for developers to handle manually. It introduces a method called crash-site repair that simplifies the bug-fixing process while reducing the risk of security issues. The authors also present a template-guided patch generation technique that lowers the token usage of Large Language Models (LLMs), making the repair process more efficient. Their system, WILLIAMT, shows significant improvements in bug-fixing rates and can operate effectively even on less powerful hardware.'}, 'zh': {'title': 'WILLIAMT：高效的自动化程序修复新方法', 'desc': '随着漏洞发现技术的快速发展，开发者面临着越来越多的漏洞修复需求，因此迫切需要有效的自动化程序修复（APR）方法。现代漏洞的复杂性使得精确的根本原因分析变得困难且不可靠。为了解决这个问题，我们提出了崩溃现场修复方法，以简化修复任务，同时降低被利用的风险。此外，我们引入了一种模板引导的补丁生成方法，显著降低了大型语言模型（LLMs）的令牌成本，同时保持了效率和有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.12182', 'title': 'Truth Neurons', 'url': 'https://huggingface.co/papers/2505.12182', 'abstract': 'Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.', 'score': 5, 'issue_id': 3868, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': 'ddeab64450bb26a9', 'authors': ['Haohang Li', 'Yupeng Cao', 'Yangyang Yu', 'Jordan W. Suchow', 'Zining Zhu'], 'affiliations': ['Stevens Institute of Technology', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.12182.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#hallucinations', '#alignment', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Нейроны правды: путь к повышению надежности языковых моделей', 'desc': "Исследователи предложили метод идентификации представлений правдивости на уровне отдельных нейронов в языковых моделях. Они обнаружили так называемые 'нейроны правды', которые кодируют правдивость независимо от темы. Эксперименты подтвердили наличие таких нейронов в моделях разного масштаба. Подавление активации этих нейронов ухудшает производительность модели на различных тестах правдивости."}, 'en': {'title': 'Unveiling Truth Neurons: Enhancing Language Model Trustworthiness', 'desc': "This paper investigates how language models encode truthfulness at the neuron level, revealing the presence of 'truth neurons' that represent truthfulness in a way that is not dependent on specific subjects. The authors demonstrate that these truth neurons exist across various models, indicating a shared property among them. By analyzing the distribution of truth neurons across different layers, the study aligns with previous research on the geometry of truthfulness. Additionally, the suppression of these neurons negatively impacts model performance, suggesting that understanding and improving truthfulness in language models is crucial for their reliability."}, 'zh': {'title': '揭示语言模型中的真相神经元', 'desc': '尽管语言模型在各种工作流程中取得了显著成功，但有时会产生不真实的回答。我们对这些模型中真相编码机制的理解有限，这影响了它们的可靠性和安全性。本文提出了一种方法，通过神经元层面识别真相的表示，发现语言模型中存在编码真相的真相神经元。实验表明，真相神经元的存在是许多语言模型的共同特性，并且其分布模式与真相的几何特征一致。'}}}, {'id': 'https://huggingface.co/papers/2505.09569', 'title': 'MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8', 'url': 'https://huggingface.co/papers/2505.09569', 'abstract': 'With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with 5,102 and 300 repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience and https://github.com/amazon-science/self_debug respectively.', 'score': 5, 'issue_id': 3872, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': 'cb7832fb680cc056', 'authors': ['Linbo Liu', 'Xinle Liu', 'Qiang Zhou', 'Lin Chen', 'Yihan Liu', 'Hoan Nguyen', 'Behrooz Omidvar-Tehrani', 'Xi Shen', 'Jun Huan', 'Omer Tripp', 'Anoop Deoras'], 'affiliations': ['AWS AI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.09569.jpg', 'data': {'categories': ['#survey', '#data', '#optimization', '#dataset', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'MIGRATION-BENCH: новый стандарт для оценки миграции кода с помощью LLM', 'desc': 'Статья представляет новый бенчмарк MIGRATION-BENCH для оценки способностей больших языковых моделей (LLM) в задаче миграции кода с Java 8 на более новые версии. Бенчмарк включает полный набор данных из 5,102 репозиториев и выбранное подмножество из 300 репозиториев, отобранных по сложности. Авторы также предлагают метод SD-Feedback, который позволяет LLM эффективно выполнять миграцию кода на уровне репозитория. С использованием модели Claude-3.5-Sonnet-v2 и метода SD-Feedback достигнута успешность в 62.33% для минимальной миграции и 27.00% для максимальной миграции на выбранном подмножестве данных.'}, 'en': {'title': 'Revolutionizing Code Migration with MIGRATION-BENCH', 'desc': 'This paper introduces MIGRATION-BENCH, a new benchmark specifically designed for evaluating large language models (LLMs) on the task of code migration from Java 8 to newer long-term support versions like Java 17 and 21. Unlike existing benchmarks that focus on problem-solving, MIGRATION-BENCH provides a comprehensive dataset of 5,102 repositories, with a curated subset of 300 that vary in complexity and difficulty. The authors also present an evaluation framework to standardize the assessment of LLMs in this domain, demonstrating that LLMs can effectively perform repository-level code migration. Using their proposed SD-Feedback method, they report success rates of 62.33% for minimal migration and 27.00% for maximal migration with the Claude-3.5-Sonnet-v2 model.'}, 'zh': {'title': '代码迁移的新基准：MIGRATION-BENCH', 'desc': '近年来，强大的大型语言模型（LLMs）迅速发展，能够处理多种软件工程任务，显著提高了生产力和可扩展性。为了评估这些模型的编码能力，开发了许多基准数据集，但大多集中在问题解决和故障排除任务上。我们提出了一个新的编码基准MIGRATION-BENCH，专注于代码迁移，特别是从Java 8迁移到最新的长期支持版本（Java 17、21）。该基准包含完整数据集和代表性子集，提供了一个多功能资源，以支持代码迁移领域的研究，并提供了全面的评估框架，以便对LLMs在这一挑战性任务上的表现进行严格和标准化的评估。'}}}, {'id': 'https://huggingface.co/papers/2505.14648', 'title': 'Vox-Profile: A Speech Foundation Model Benchmark for Characterizing\n  Diverse Speaker and Speech Traits', 'url': 'https://huggingface.co/papers/2505.14648', 'abstract': 'We introduce Vox-Profile, a comprehensive benchmark to characterize rich speaker and speech traits using speech foundation models. Unlike existing works that focus on a single dimension of speaker traits, Vox-Profile provides holistic and multi-dimensional profiles that reflect both static speaker traits (e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech flow). This benchmark is grounded in speech science and linguistics, developed with domain experts to accurately index speaker and speech characteristics. We report benchmark experiments using over 15 publicly available speech datasets and several widely used speech foundation models that target various static and dynamic speaker and speech properties. In addition to benchmark experiments, we showcase several downstream applications supported by Vox-Profile. First, we show that Vox-Profile can augment existing speech recognition datasets to analyze ASR performance variability. Vox-Profile is also used as a tool to evaluate the performance of speech generation systems. Finally, we assess the quality of our automated profiles through comparison with human evaluation and show convergent validity. Vox-Profile is publicly available at: https://github.com/tiantiaf0627/vox-profile-release.', 'score': 4, 'issue_id': 3884, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'f1d0da4a4a22866a', 'authors': ['Tiantian Feng', 'Jihwan Lee', 'Anfeng Xu', 'Yoonjeong Lee', 'Thanathai Lertpetchpun', 'Xuan Shi', 'Helin Wang', 'Thomas Thebaud', 'Laureano Moro-Velazquez', 'Dani Byrd', 'Najim Dehak', 'Shrikanth Narayanan'], 'affiliations': ['Johns Hopkins University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.14648.jpg', 'data': {'categories': ['#benchmark', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Vox-Profile: Многомерный анализ речи и говорящего с помощью ИИ', 'desc': 'Vox-Profile - это комплексный бенчмарк для характеристики богатых черт говорящего и речи с использованием фундаментальных моделей речи. Он обеспечивает целостные и многомерные профили, отражающие как статические черты говорящего (возраст, пол, акцент), так и динамические свойства речи (эмоции, плавность речи). Бенчмарк основан на науке о речи и лингвистике, разработан с экспертами для точной индексации характеристик говорящего и речи. Авторы провели эксперименты с использованием более 15 общедоступных речевых датасетов и нескольких широко используемых фундаментальных моделей речи.'}, 'en': {'title': 'Vox-Profile: A Holistic Benchmark for Speaker and Speech Trait Analysis', 'desc': 'Vox-Profile is a new benchmark designed to analyze various speaker and speech traits using advanced speech foundation models. It goes beyond previous studies by providing a comprehensive view of both static traits like age and accent, and dynamic traits such as emotion and speech flow. Developed with input from experts in speech science and linguistics, it offers a reliable way to index these characteristics. The benchmark has been tested with multiple speech datasets and models, demonstrating its utility in improving automatic speech recognition and speech generation systems.'}, 'zh': {'title': 'Vox-Profile：多维度说话者特征的基准', 'desc': 'Vox-Profile是一个全面的基准，用于通过语音基础模型来表征丰富的说话者和语音特征。与现有研究只关注单一维度的说话者特征不同，Vox-Profile提供了反映静态说话者特征（如年龄、性别、口音）和动态语音属性（如情感、语速）的整体和多维档案。该基准基于语音科学和语言学，与领域专家合作开发，以准确索引说话者和语音特征。我们通过超过15个公开的语音数据集和多种广泛使用的语音基础模型进行基准实验，展示了Vox-Profile在语音识别和生成系统评估中的应用。'}}}, {'id': 'https://huggingface.co/papers/2505.13380', 'title': 'CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition', 'url': 'https://huggingface.co/papers/2505.13380', 'abstract': "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526", 'score': 4, 'issue_id': 3868, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '6a5e70a76e6f012c', 'authors': ['Nam V. Nguyen', 'Huy Nguyen', 'Quang Pham', 'Van Nguyen', 'Savitha Ramasamy', 'Nhat Ho'], 'affiliations': ['FPT Software AI Center', 'Independent Researcher', 'Institute for Infocomm Research, ASTAR', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.13380.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training'], 'emoji': '🏆', 'ru': {'title': 'Конкуренция экспертов для эффективного обучения языковых моделей', 'desc': "Статья представляет новый механизм маршрутизации токенов в разреженных смесях экспертов (SMoE) под названием 'competition'. Авторы теоретически доказывают, что этот метод обладает лучшей эффективностью выборки по сравнению с традиционной маршрутизацией softmax. На основе этого механизма разработан алгоритм CompeteSMoE для обучения больших языковых моделей. Эмпирические эксперименты на задачах визуального обучения и предобучения языка демонстрируют эффективность, надежность и масштабируемость CompeteSMoE по сравнению с современными стратегиями SMoE."}, 'en': {'title': 'CompeteSMoE: Efficient Routing for Powerful Language Models', 'desc': 'Sparse mixture of experts (SMoE) is a method that allows models to become more complex without simply making them deeper or wider. The challenge with SMoE is that the way experts are chosen to process data can be inefficient, as not all experts contribute to the decision-making process. This paper introduces a new routing mechanism called competition, which directs data to the most responsive experts, improving the efficiency of the model. The authors present CompeteSMoE, an algorithm that uses this competition mechanism to train large language models effectively, showing better performance and lower training costs compared to existing methods.'}, 'zh': {'title': '竞争机制提升稀疏专家混合模型的效率', 'desc': '稀疏专家混合模型（SMoE）是一种有效提升模型复杂度的方法，超越了简单增加网络深度或宽度的方式。然而，SMoE的训练仍然面临挑战，主要是因为计算的专家与路由过程之间的联系不够直接。我们提出了一种新的机制——竞争，能够将输入数据更有效地路由到响应最强的专家。通过理论分析，我们证明了竞争机制在样本效率上优于传统的softmax路由，并开发了CompeteSMoE算法，能够以较低的训练开销实现强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11365', 'title': 'Phare: A Safety Probe for Large Language Models', 'url': 'https://huggingface.co/papers/2505.11365', 'abstract': 'Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.', 'score': 4, 'issue_id': 3873, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '8ab32377d956578e', 'authors': ['Pierre Le Jeune', 'Benoît Malézieux', 'Weixuan Xiao', 'Matteo Dora'], 'affiliations': ['Giskard AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.11365.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#ethics', '#hallucinations', '#alignment'], 'emoji': '🔍', 'ru': {'title': 'Диагностика безопасности языковых моделей для ответственного ИИ', 'desc': 'Статья представляет Phare - многоязычную диагностическую систему для оценки больших языковых моделей (LLM) по трем ключевым аспектам безопасности: галлюцинации и надежность, социальные предубеждения и генерация вредного контента. Авторы провели оценку 17 современных LLM, выявив систематические уязвимости во всех аспектах безопасности, включая угодничество, чувствительность к промпту и воспроизведение стереотипов. Phare фокусируется на выявлении конкретных режимов отказа, а не просто ранжировании моделей, что дает исследователям и практикам полезную информацию для создания более надежных и заслуживающих доверия языковых систем.'}, 'en': {'title': 'Phare: Probing Safety in Language Models Beyond Performance', 'desc': 'This paper presents Phare, a new framework designed to evaluate the safety of large language models (LLMs) by focusing on their failure modes rather than just their performance. It assesses LLMs across three key areas: hallucination and reliability, social biases, and harmful content generation. The study analyzes 17 advanced LLMs and uncovers common vulnerabilities, such as sycophancy and prompt sensitivity, which can lead to biased or harmful outputs. By identifying these specific issues, Phare aims to help researchers and developers create more reliable and ethical language models.'}, 'zh': {'title': '构建更安全的语言模型，识别失败模式！', 'desc': '确保大型语言模型（LLMs）的安全性对于负责任的部署至关重要，但现有的评估往往更关注性能而非识别失败模式。我们提出了Phare，这是一个多语言诊断框架，用于探测和评估LLM在三个关键维度上的行为：幻觉和可靠性、社会偏见以及有害内容生成。对17个最先进的LLM的评估揭示了在所有安全维度上系统性脆弱性的模式，包括谄媚、提示敏感性和刻板印象再现。通过突出这些具体的失败模式，Phare为研究人员和从业者提供了可操作的见解，以构建更强大、更一致和更可信的语言系统。'}}}, {'id': 'https://huggingface.co/papers/2505.13988', 'title': 'The Hallucination Tax of Reinforcement Finetuning', 'url': 'https://huggingface.co/papers/2505.13988', 'abstract': "Reinforcement finetuning (RFT) has become a standard approach for enhancing the reasoning capabilities of large language models (LLMs). However, its impact on model trustworthiness remains underexplored. In this work, we identify and systematically study a critical side effect of RFT, which we term the hallucination tax: a degradation in refusal behavior causing models to produce hallucinated answers to unanswerable questions confidently. To investigate this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of unanswerable math problems designed to probe models' ability to recognize an unanswerable question by reasoning from the insufficient or ambiguous information. Our results show that standard RFT training could reduce model refusal rates by more than 80%, which significantly increases model's tendency to hallucinate. We further demonstrate that incorporating just 10% SUM during RFT substantially restores appropriate refusal behavior, with minimal accuracy trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage inference-time compute to reason about their own uncertainty and knowledge boundaries, improving generalization not only to out-of-domain math problems but also to factual question answering tasks.", 'score': 3, 'issue_id': 3887, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'db283d13501585eb', 'authors': ['Linxin Song', 'Taiwei Shi', 'Jieyu Zhao'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.13988.jpg', 'data': {'categories': ['#hallucinations', '#rlhf', '#training', '#dataset', '#synthetic', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Борьба с галлюцинациями ИИ: баланс между уверенностью и осторожностью', 'desc': "Это исследование посвящено проблеме усиления склонности языковых моделей к галлюцинациям после обучения с подкреплением (RFT). Авторы вводят понятие 'налога на галлюцинации' и создают набор данных SUM для оценки способности моделей распознавать вопросы без ответа. Результаты показывают, что стандартное RFT может снизить частоту отказов модели более чем на 80%. Однако включение всего 10% данных SUM в процесс RFT значительно восстанавливает правильное поведение отказа без существенных потерь в точности на решаемых задачах."}, 'en': {'title': 'Balancing Confidence and Accuracy in Language Models', 'desc': "This paper explores the effects of Reinforcement Finetuning (RFT) on the trustworthiness of large language models (LLMs). It introduces the concept of the 'hallucination tax', which refers to the increased likelihood of models confidently generating incorrect answers to unanswerable questions after RFT. The authors present a new dataset called SUM (Synthetic Unanswerable Math) to evaluate how well models can identify unanswerable questions. Their findings suggest that while RFT can significantly lower refusal rates, incorporating SUM during training can help restore proper refusal behavior without sacrificing performance on answerable tasks."}, 'zh': {'title': '强化微调与幻觉税的平衡', 'desc': '强化微调（RFT）已成为提升大型语言模型（LLM）推理能力的标准方法。然而，它对模型可信度的影响仍然未被充分研究。我们发现并系统研究了RFT的一个重要副作用，称为幻觉税：即模型在面对无法回答的问题时，拒绝行为的下降导致其自信地产生幻觉答案。通过引入SUM（合成不可回答数学问题）数据集，我们探讨了模型识别不可回答问题的能力，并发现标准的RFT训练会显著降低模型的拒绝率，从而增加幻觉的倾向。'}}}, {'id': 'https://huggingface.co/papers/2505.13946', 'title': 'Visual Instruction Bottleneck Tuning', 'url': 'https://huggingface.co/papers/2505.13946', 'abstract': "Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the robustness of MLLMs under distribution shifts, from a representation learning perspective. Inspired by the information bottleneck (IB) principle, we derive a variational lower bound of the IB for MLLMs and devise a practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide a theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of three MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLM's robustness under shifts by pursuing the learning of a minimal sufficient representation.", 'score': 3, 'issue_id': 3887, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '8d7a6b4e5fc68de6', 'authors': ['Changdae Oh', 'Jiatong Li', 'Shawn Im', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.13946.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#multimodal', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Повышение устойчивости мультимодальных языковых моделей с помощью информационного узкого места', 'desc': 'Эта статья представляет новый метод под названием Visual Instruction Bottleneck Tuning (Vittle) для улучшения обобщающей способности мультимодальных больших языковых моделей (MLLM). Метод основан на принципе информационного узкого места и не требует дополнительных данных или изменений архитектуры модели. Авторы предоставляют теоретическое обоснование метода и показывают его связь с информационно-теоретической метрикой устойчивости MLLM. Экспериментальные результаты на 45 наборах данных демонстрируют, что Vittle последовательно улучшает устойчивость MLLM к распределительным сдвигам.'}, 'en': {'title': 'Enhancing MLLM Robustness with Vittle: A New Approach to Distribution Shifts', 'desc': 'This paper addresses the issue of multimodal large language models (MLLMs) struggling with unfamiliar queries due to distribution shifts. Instead of relying on more data or larger models, the authors propose a new method called Visual Instruction Bottleneck Tuning (Vittle) that enhances MLLM robustness through representation learning. Vittle is based on the information bottleneck principle, which helps in learning a minimal sufficient representation of the data. The authors validate their approach through experiments on various tasks, showing that Vittle significantly improves MLLM performance under different distribution shifts.'}, 'zh': {'title': '提升多模态语言模型鲁棒性的创新方法', 'desc': '尽管多模态大型语言模型（MLLMs）被广泛应用，但在面对分布变化时，它们的性能会下降。现有的方法通常需要更多的指令数据或更大的模型架构，这会增加人力和计算成本。本文提出了一种新的方法，从表示学习的角度增强MLLM在分布变化下的鲁棒性。我们提出了视觉指令瓶颈调优（Vittle），并通过理论分析证明了其与信息理论鲁棒性指标的关系，实验证明Vittle在多个任务中有效提升了MLLM的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2505.12306', 'title': 'Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection', 'url': 'https://huggingface.co/papers/2505.12306', 'abstract': 'Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia\'s "Did You Know..." entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.', 'score': 3, 'issue_id': 3868, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': 'ccbad06f5ba35418', 'authors': ['Yuwei Zhang', 'Wenhao Yu', 'Shangbin Feng', 'Yifan Zhu', 'Letian Peng', 'Jayanth Srinivasa', 'Gaowen Liu', 'Jingbo Shang'], 'affiliations': ['Cisco', 'Tencent AI Lab', 'UC, San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.12306.jpg', 'data': {'categories': ['#transfer_learning', '#interpretability', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'WikiDYK: новый стандарт оценки памяти языковых моделей', 'desc': "Статья представляет новый масштабный бенчмарк WikiDYK для оценки способности языковых моделей запоминать знания. WikiDYK использует недавно добавленные факты из раздела Wikipedia 'Did You Know...', преобразуя их в разнообразные вопросно-ответные пары. Эксперименты показали, что двунаправленные языковые модели (BiLM) значительно лучше запоминают знания, чем однонаправленные причинные модели (CLM). Авторы предлагают модульную коллаборативную систему, использующую ансамбли BiLM в качестве внешних хранилищ знаний для интеграции с большими языковыми моделями."}, 'en': {'title': 'Enhancing Knowledge Memorization in Language Models with WikiDYK', 'desc': "This paper presents WikiDYK, a new benchmark for evaluating knowledge memorization in large language models (LLMs). It uses real-world facts from Wikipedia's 'Did You Know...' entries to create a diverse set of question-answer pairs. The study finds that Causal Language Models (CLMs) have weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), with a notable accuracy gap. To enhance BiLMs' performance, the authors propose a collaborative framework that combines multiple BiLMs as external knowledge sources, resulting in improved accuracy in knowledge retrieval tasks."}, 'zh': {'title': '知识记忆能力的新基准：WikiDYK', 'desc': '尽管大型语言模型（LLMs）取得了显著进展，但它们的知识记忆能力仍然未得到充分探索。本文提出了一种新颖的、真实世界的大规模知识注入基准，名为WikiDYK，能够随着时间的推移不断演变，而无需人工干预。WikiDYK利用维基百科“你知道吗...”条目中最近添加的、由人类撰写的事实，经过专家编辑的严格筛选，确保其可验证性和清晰性。实验结果表明，尽管因果语言模型（CLMs）在现代LLMs中普遍存在，但其知识记忆能力显著低于双向语言模型（BiLMs），准确性低23%。'}}}, {'id': 'https://huggingface.co/papers/2505.11966', 'title': 'Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier', 'url': 'https://huggingface.co/papers/2505.11966', 'abstract': 'Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.', 'score': 3, 'issue_id': 3869, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': 'cd659e075a3efafa', 'authors': ['Jianyuan Zhong', 'Zeju Li', 'Zhijian Xu', 'Xiangyu Wen', 'Kezhi Li', 'Qiang Xu'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.11966.jpg', 'data': {'categories': ['#inference', '#optimization', '#math', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Гибкая верификация для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет FlexiVe - новый генеративный верификатор для улучшения рассуждений больших языковых моделей (LLM). FlexiVe гибко распределяет вычислительные ресурсы между быстрым и медленным мышлением, используя стратегию гибкого распределения бюджета верификации. Авторы также предлагают конвейер Solve-Detect-Verify для эффективного масштабирования во время вывода. Эксперименты показывают, что FlexiVe превосходит базовые методы по точности рассуждений и эффективности вывода на сложных математических задачах.'}, 'en': {'title': 'Balancing Speed and Accuracy in LLM Reasoning with FlexiVe', 'desc': 'This paper discusses the challenges of using Large Language Models (LLMs) for complex tasks, particularly the balance between accuracy and computational efficiency. It introduces FlexiVe, a generative verifier that optimizes the use of computational resources by allowing for both quick and thorough reasoning processes. The authors propose a Solve-Detect-Verify pipeline that enhances the integration of FlexiVe, enabling targeted verification and improved feedback during inference. Experimental results demonstrate that FlexiVe significantly improves error detection and reasoning accuracy on various benchmarks compared to traditional methods.'}, 'zh': {'title': '灵活验证，提升推理效率与准确性', 'desc': '本文探讨了大型语言模型（LLM）在复杂任务推理中的准确性与计算效率之间的权衡。我们提出了一种新颖的生成验证器FlexiVe，它通过灵活分配验证预算，在快速可靠的思维与细致慢思维之间取得平衡。我们还提出了Solve-Detect-Verify管道，这是一种高效的推理时间扩展框架，能够智能整合FlexiVe，主动识别解决方案完成点以触发针对性的验证。实验结果表明，FlexiVe在ProcessBench上能够更准确地定位推理过程中的错误，并在多个数学推理基准测试中超越了现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.14178', 'title': 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits', 'url': 'https://huggingface.co/papers/2505.14178', 'abstract': 'Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.', 'score': 2, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'f4fdc7fb140f9273', 'authors': ['Xiang Zhang', 'Juntai Cao', 'Jiaqi Wei', 'Yiwei Xu', 'Chenyu You'], 'affiliations': ['Cisco', 'Stony Brook University', 'University of British Columbia', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14178.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#architecture', '#small_models', '#data', '#training'], 'emoji': '🧩', 'ru': {'title': 'Токенизация: скрытый ключ к символьным рассуждениям в ИИ', 'desc': "Статья исследует влияние токенизации на способность языковых моделей к символьным вычислениям. Авторы вводят понятие 'осведомленности о токенах' (Token Awareness) для формализации того, как неоптимальная гранулярность токенов нарушает логическое выравнивание и препятствует обобщению символьных процедур. Эмпирические эксперименты показывают, что структура токенов существенно влияет на производительность рассуждений, даже при использовании метода цепочки рассуждений (Chain-of-Thought). Исследование демонстрирует, что способность к символьным рассуждениям в больших языковых моделях (LLM) глубоко обусловлена токен-уровневыми представлениями."}, 'en': {'title': 'Tokenization Matters: Unlocking Reasoning in Language Models', 'desc': "This paper explores the importance of tokenization in language models, particularly how it affects reasoning capabilities. It highlights that traditional tokenization methods, like byte-pair encoding (BPE), can obscure essential reasoning units, limiting the model's ability to perform symbolic computation. The authors introduce the concept of Token Awareness, which emphasizes the need for better token granularity to enhance logical alignment and generalization in models. Through experiments on arithmetic and symbolic tasks, they show that models with well-structured token representations can significantly outperform larger models in reasoning tasks."}, 'zh': {'title': '分词结构决定推理能力', 'desc': '本文探讨了在语言模型中，分词（Tokenization）对推理能力的影响。我们发现，分词方案，特别是基于子词的方法（如字节对编码BPE），会合并或模糊基本的推理单元，从而妨碍符号计算。我们引入了“Token Awareness”的概念，强调了分词粒度不佳如何干扰逻辑对齐，阻碍模型的符号程序泛化。通过对算术和符号任务的系统评估，我们证明了分词结构显著影响推理性能，较小的模型在对齐格式下能够超越更大的系统。'}}}, {'id': 'https://huggingface.co/papers/2505.13010', 'title': 'To Bias or Not to Bias: Detecting bias in News with bias-detector', 'url': 'https://huggingface.co/papers/2505.13010', 'abstract': "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.", 'score': 2, 'issue_id': 3877, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '9f759ef4e436d2f0', 'authors': ['Himel Ghosh', 'Ahmed Mosharafa', 'Georg Groh'], 'affiliations': ['Sapienza University of Rome, Italy', 'Technical University of Munich (TUM), Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.13010.jpg', 'data': {'categories': ['#training', '#multimodal', '#interpretability', '#ethics', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Точное обнаружение предвзятости в СМИ с помощью глубокого обучения', 'desc': 'Исследователи разработали модель на основе RoBERTa для классификации предвзятости на уровне предложений, используя экспертно-аннотированный датасет BABE. Статистически значимые улучшения производительности были показаны по сравнению с базовой моделью DA-RoBERTa. Анализ внимания продемонстрировал, что модель избегает чрезмерной чувствительности к политически заряженным терминам. Представлен конвейер, объединяющий разработанную модель с существующим классификатором типов предвзятости для комплексного анализа медиа-предвзятости.'}, 'en': {'title': 'Enhancing Media Bias Detection with Context-Aware Models', 'desc': "This paper addresses the challenge of detecting media bias by fine-tuning a RoBERTa-based model on the BABE dataset, which is annotated by experts. The authors demonstrate significant performance improvements over a baseline model using statistical tests, indicating the effectiveness of their approach. They also analyze the model's attention mechanisms, showing it focuses on contextually relevant information rather than being overly sensitive to biased language. The study proposes a comprehensive pipeline for media bias detection and discusses future directions for enhancing bias classification and model interpretability."}, 'zh': {'title': '提升媒体偏见检测的智能化方法', 'desc': '本研究针对媒体偏见检测这一重要任务，提出了一种基于RoBERTa模型的句子级偏见分类方法。我们在专家标注的BABE数据集上进行了微调，并通过统计测试验证了模型性能的显著提升。模型的注意力分析表明，它能够有效避免对政治敏感词的过度敏感，而是更关注上下文相关的词汇。尽管受限于句子级分析和数据集规模，我们的方法在偏见检测中展现了良好的泛化能力和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2505.10176', 'title': 'Incorporating brain-inspired mechanisms for multimodal learning in\n  artificial intelligence', 'url': 'https://huggingface.co/papers/2505.10176', 'abstract': 'Multimodal learning enhances the perceptual capabilities of cognitive systems by integrating information from different sensory modalities. However, existing multimodal fusion research typically assumes static integration, not fully incorporating key dynamic mechanisms found in the brain. Specifically, the brain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal cues yield stronger multisensory integration benefits; conversely, when individual modal cues are stronger, the effect of fusion is diminished. This mechanism enables biological systems to achieve robust cognition even with scarce or noisy perceptual cues. Inspired by this biological mechanism, we explore the relationship between multimodal output and information from individual modalities, proposing an inverse effectiveness driven multimodal fusion (IEMF) strategy. By incorporating this strategy into neural networks, we achieve more efficient integration with improved model performance and computational efficiency, demonstrating up to 50% reduction in computational cost across diverse fusion methods. We conduct experiments on audio-visual classification, continual learning, and question answering tasks to validate our method. Results consistently demonstrate that our method performs excellently in these tasks. To verify universality and generalization, we also conduct experiments on Artificial Neural Networks (ANN) and Spiking Neural Networks (SNN), with results showing good adaptability to both network types. Our research emphasizes the potential of incorporating biologically inspired mechanisms into multimodal networks and provides promising directions for the future development of multimodal artificial intelligence. The code is available at https://github.com/Brain-Cog-Lab/IEMF.', 'score': 2, 'issue_id': 3878, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '6384b169333ad553', 'authors': ['Xiang He', 'Dongcheng Zhao', 'Yang Li', 'Qingqun Kong', 'Xin Yang', 'Yi Zeng'], 'affiliations': ['Brain-inspired Cognitive AI Lab, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Center for Long-term Al, Beijing, China', 'Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Chinese Academy of Sciences, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.10176.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#audio', '#agi'], 'emoji': '🧠', 'ru': {'title': 'Биологически вдохновленное мультимодальное слияние для эффективного ИИ', 'desc': 'Статья представляет новую стратегию мультимодального слияния, вдохновленную биологическим механизмом обратной эффективности в мозге. Авторы предлагают метод IEMF (Inverse Effectiveness driven Multimodal Fusion), который позволяет более эффективно интегрировать информацию из разных модальностей в нейронных сетях. Эксперименты показывают, что IEMF улучшает производительность модели и снижает вычислительные затраты до 50% в различных задачах, включая аудио-визуальную классификацию и ответы на вопросы. Метод демонстрирует хорошую адаптируемость как к искусственным нейронным сетям (ANN), так и к спайковым нейронным сетям (SNN).'}, 'en': {'title': 'Enhancing Multimodal Learning with Brain-Inspired Fusion', 'desc': "This paper discusses a new approach to multimodal learning that mimics how the human brain processes information from different senses. It introduces the Inverse Effectiveness Driven Multimodal Fusion (IEMF) strategy, which enhances the integration of sensory data by leveraging the brain's ability to combine weaker signals more effectively. The authors demonstrate that this method can significantly improve the performance and efficiency of neural networks, achieving up to a 50% reduction in computational costs. Experiments across various tasks show that IEMF is adaptable and effective in both Artificial Neural Networks and Spiking Neural Networks, highlighting the benefits of biologically inspired techniques in artificial intelligence."}, 'zh': {'title': '逆效应驱动的多模态融合策略', 'desc': '多模态学习通过整合来自不同感官的信息，增强了认知系统的感知能力。然而，现有的多模态融合研究通常假设静态整合，未能充分考虑大脑中的关键动态机制。具体来说，大脑表现出逆效应现象，即较弱的单模态线索会带来更强的多感官融合效益；相反，当单个模态线索较强时，融合效果会减弱。受这一生物机制的启发，我们提出了一种基于逆效应的多模态融合策略，显著提高了模型性能和计算效率。'}}}, {'id': 'https://huggingface.co/papers/2505.13778', 'title': 'CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM\n  APIs', 'url': 'https://huggingface.co/papers/2505.13778', 'abstract': 'As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.', 'score': 1, 'issue_id': 3884, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'e61730a128c76920', 'authors': ['Guoheng Sun', 'Ziyao Wang', 'Bowei Tian', 'Meng Liu', 'Zheyu Shen', 'Shwai He', 'Yexiao He', 'Wanghao Ye', 'Yiting Wang', 'Ang Li'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2505.13778.jpg', 'data': {'categories': ['#dataset', '#optimization', '#inference', '#reasoning', '#hallucinations', '#rl', '#security'], 'emoji': '🕵️', 'ru': {'title': 'CoIn: прозрачность и честность в скрытых рассуждениях LLM', 'desc': 'Статья представляет CoIn - фреймворк для аудита скрытых токенов в больших языковых моделях (LLM). CoIn решает проблему непрозрачности в использовании токенов при многоступенчатых рассуждениях LLM, которые часто скрываются провайдерами. Фреймворк использует хеш-дерево для проверки количества токенов и сопоставление релевантности на основе эмбеддингов для обнаружения сфабрикованного контента. Эксперименты показывают, что CoIn может эффективно обнаруживать искусственное увеличение количества токенов с точностью до 94.7%.'}, 'en': {'title': 'Ensuring Transparency in LLM Billing with CoIn', 'desc': 'This paper discusses the challenges of transparency in large language models (LLMs) that use complex reasoning processes, often hidden from users. These models, enhanced through reinforcement learning, can perform better on difficult tasks but may lead to inflated costs due to undisclosed reasoning tokens. The authors introduce CoIn, a framework designed to verify the authenticity and quantity of these hidden tokens, ensuring users are not overcharged. Through experiments, CoIn demonstrates a high success rate in detecting token inflation, promoting fairness and transparency in LLM billing practices.'}, 'zh': {'title': '提升LLM服务透明度的验证框架', 'desc': '随着后训练技术的发展，大型语言模型（LLMs）越来越多地增强了结构化的多步骤推理能力，这通常通过强化学习进行优化。这些增强推理的模型在复杂任务上表现优于标准LLMs，并且现在支撑着许多商业LLM API。然而，为了保护专有行为并减少冗长，提供者通常在返回最终答案时隐藏推理过程。这种不透明性导致了透明度缺口，用户为不可见的推理令牌付费，而这些令牌往往占据了大部分成本，用户却无法验证其真实性。'}}}, {'id': 'https://huggingface.co/papers/2505.13731', 'title': 'GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization', 'url': 'https://huggingface.co/papers/2505.13731', 'abstract': 'Worldwide image geolocalization-the task of predicting GPS coordinates from images taken anywhere on Earth-poses a fundamental challenge due to the vast diversity in visual content across regions. While recent approaches adopt a two-stage pipeline of retrieving candidates and selecting the best match, they typically rely on simplistic similarity heuristics and point-wise supervision, failing to model spatial relationships among candidates. In this paper, we propose GeoRanker, a distance-aware ranking framework that leverages large vision-language models to jointly encode query-candidate interactions and predict geographic proximity. In addition, we introduce a multi-order distance loss that ranks both absolute and relative distances, enabling the model to reason over structured spatial relationships. To support this, we curate GeoRanking, the first dataset explicitly designed for geographic ranking tasks with multimodal candidate information. GeoRanker achieves state-of-the-art results on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly outperforming current best methods.', 'score': 1, 'issue_id': 3885, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '0e5f5736d8ac2254', 'authors': ['Pengyue Jia', 'Seongheon Park', 'Song Gao', 'Xiangyu Zhao', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison', 'Department of Data Science, City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13731.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#benchmark', '#games'], 'emoji': '🌎', 'ru': {'title': 'GeoRanker: Точная геолокализация изображений с помощью ИИ', 'desc': 'Статья представляет GeoRanker - новую систему для геолокализации изображений по всему миру. Она использует модели компьютерного зрения и обработки естественного языка для совместного кодирования взаимодействий запроса и кандидатов, а также для прогнозирования географической близости. Авторы вводят функцию потерь, учитывающую многоуровневые расстояния, что позволяет модели рассуждать о структурированных пространственных отношениях. Также был создан набор данных GeoRanking специально для задач географического ранжирования с мультимодальной информацией о кандидатах.'}, 'en': {'title': 'GeoRanker: Revolutionizing Image Geolocalization with Distance-Aware Ranking', 'desc': 'This paper addresses the challenge of predicting GPS coordinates from images, known as image geolocalization, which is complicated by the diverse visual content found in different regions. The authors introduce GeoRanker, a novel ranking framework that utilizes large vision-language models to better understand the interactions between image queries and their potential geographic candidates. By implementing a multi-order distance loss, GeoRanker effectively ranks both absolute and relative distances, allowing it to capture complex spatial relationships among candidates. The paper also presents GeoRanking, a new dataset tailored for geographic ranking tasks, and demonstrates that GeoRanker achieves superior performance on established benchmarks compared to existing methods.'}, 'zh': {'title': 'GeoRanker：智能地理定位的新方法', 'desc': '本文提出了一种新的图像地理定位方法，称为GeoRanker，旨在从图像中预测GPS坐标。与传统方法不同，GeoRanker利用大型视觉-语言模型来共同编码查询和候选图像之间的交互，并预测地理接近度。我们还引入了一种多阶距离损失，能够对绝对和相对距离进行排名，从而更好地建模候选图像之间的空间关系。此外，我们创建了GeoRanking数据集，专门用于地理排名任务，支持多模态候选信息的处理。'}}}, {'id': 'https://huggingface.co/papers/2505.12154', 'title': 'Learning to Highlight Audio by Watching Movies', 'url': 'https://huggingface.co/papers/2505.12154', 'abstract': 'Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/.', 'score': 1, 'issue_id': 3882, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '8f03af3997e1149d', 'authors': ['Chao Huang', 'Ruohan Gao', 'J. M. F. Tsang', 'Jan Kurcius', 'Cagdas Bilen', 'Chenliang Xu', 'Anurag Kumar', 'Sanjeel Parekh'], 'affiliations': ['Meta Reality Labs Research', 'University of Maryland, College Park', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.12154.jpg', 'data': {'categories': ['#multimodal', '#video', '#audio', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Гармонизация аудио и видео с помощью машинного обучения', 'desc': 'Статья представляет новую задачу визуально-управляемого акустического выделения для улучшения согласованности аудио и видео контента. Авторы предлагают трансформерную мультимодальную модель для решения этой задачи. Для обучения модели создан новый датасет на основе фильмов с тщательно подобранным аудио и видео. Разработан процесс генерации псевдо-данных для имитации плохо смикшированного аудио в реальных сценариях.'}, 'en': {'title': 'Harmonizing Audio and Visuals for Engaging Video Content', 'desc': 'This paper addresses the imbalance in advancements between visual and audio elements in video content creation. It introduces a new task called visually-guided acoustic highlighting, which aims to enhance audio based on the visual cues from the video. The authors propose a transformer-based multimodal framework to achieve this, supported by a new dataset called the muddy mix dataset that simulates real-world audio mixing challenges. Their approach shows significant improvements over existing methods in both quantitative metrics and subjective assessments, highlighting the importance of integrating visual and audio elements for a cohesive viewing experience.'}, 'zh': {'title': '视觉引导音频高亮，提升视听体验！', 'desc': '近年来，视频内容的创作和消费显著增加。制作引人入胜的内容需要精心策划视觉和音频元素。为了弥补视觉和音频之间的差距，我们提出了一种新任务：视觉引导的音频高亮，旨在根据视频内容调整音频，以创造更和谐的视听体验。我们提出了一种基于变换器的多模态框架，并引入了新的数据集，以支持模型训练。'}}}, {'id': 'https://huggingface.co/papers/2505.11754', 'title': 'Masking in Multi-hop QA: An Analysis of How Language Models Perform with\n  Context Permutation', 'url': 'https://huggingface.co/papers/2505.11754', 'abstract': "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.", 'score': 1, 'issue_id': 3878, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'dcf59ca0d93ac6e2', 'authors': ['Wenyu Huang', 'Pavlos Vougiouklis', 'Mirella Lapata', 'Jeff Z. Pan'], 'affiliations': ['Huawei Edinburgh Research Centre, Poisson Lab, CSI, UK', 'School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.11754.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#optimization', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация языковых моделей для многоэтапного вопросно-ответного анализа', 'desc': 'Статья исследует многоэтапный вопросно-ответный анализ (MHQA) с использованием языковых моделей. Авторы обнаружили, что модели типа encoder-decoder превосходят декодер-модели в задачах MHQA, несмотря на меньший размер. Изменение порядка документов влияет на производительность моделей, причем оптимальные результаты достигаются при соответствии порядка документов цепочке рассуждений. Модификация причинной маски в декодер-моделях с помощью двунаправленного внимания улучшает их производительность в MHQA.'}, 'en': {'title': 'Unlocking Multi-hop Reasoning in Language Models', 'desc': 'This paper investigates the challenges of Multi-hop Question Answering (MHQA) using Language Models (LMs). It highlights that while LMs excel in standard question-answering, their performance can be limited by causal masking when reasoning across multiple information sources. The authors find that encoder-decoder models, like Flan-T5, outperform smaller causal decoder-only models in MHQA tasks, especially when the order of documents matches the reasoning chain. Additionally, they propose modifications to causal masks to enhance performance and analyze attention weights, discovering that higher attention values correlate with correct answers, which can be used to improve LM effectiveness.'}, 'zh': {'title': '多跳推理，提升问答能力！', 'desc': '多跳问题回答（MHQA）增加了问答的复杂性，使其更具挑战性。语言模型（LM）在处理多个搜索结果时，不仅需要检索相关信息，还需在信息源之间进行多跳推理。尽管LM在传统问答任务中表现良好，但因果掩码可能会妨碍其在复杂上下文中的推理能力。我们的研究发现，编码-解码模型在MHQA任务中通常优于仅使用因果解码器的模型，且通过调整因果掩码可以有效提升后者的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11730', 'title': 'Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling', 'url': 'https://huggingface.co/papers/2505.11730', 'abstract': 'Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.', 'score': 1, 'issue_id': 3883, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'bd83555174b293a3', 'authors': ['Hao Mark Chen', 'Guanxi Lu', 'Yasuyuki Okoshi', 'Zhiwen Mo', 'Masato Motomura', 'Hongxiang Fan'], 'affiliations': ['Imperial College London, UK', 'Institute of Science Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.11730.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#reasoning', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Гибкая проверка для эффективного масштабирования языковых моделей', 'desc': 'Статья представляет новый алгоритм Variable Granularity Search (VG-Search) для улучшения рассуждений больших языковых моделей (LLM). VG-Search обобщает методы beam search и Best-of-N sampling, позволяя настраивать частоту проверки генерируемого текста. Эксперименты показали, что динамический выбор гранулярности проверки может повысить вычислительную эффективность и масштабируемость модели. Адаптивные стратегии VG-Search достигают улучшения точности до 3.6% при снижении вычислительных затрат на 52%.'}, 'en': {'title': 'Dynamic Verification for Enhanced Language Model Efficiency', 'desc': 'This paper explores the concept of Test-time Scaling (TTS) in large language models (LLMs) and emphasizes the importance of verification in enhancing reasoning performance and computational efficiency. The authors introduce a novel approach called Variable Granularity Search (VG-Search), which allows for dynamic adjustment of verification frequency during the generation process. By systematically varying the granularity of verification, VG-Search improves both the accuracy and efficiency of LLMs compared to traditional methods like Beam Search and Best-of-N sampling. The results show significant gains in accuracy while drastically reducing computational costs, paving the way for more efficient LLM applications.'}, 'zh': {'title': '动态验证粒度提升推理效率', 'desc': '本文探讨了测试时缩放（TTS）在提升大型语言模型（LLMs）推理能力方面的有效性。验证在TTS中起着关键作用，影响推理性能和计算效率。我们首次系统性地研究了验证粒度的影响，即在生成过程中验证器的调用频率。通过引入可变粒度搜索（VG-Search）算法，我们的实验表明，动态选择粒度参数可以提高计算效率，并在准确性上超过传统的束搜索和最佳N采样。'}}}, {'id': 'https://huggingface.co/papers/2505.10588', 'title': 'Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation', 'url': 'https://huggingface.co/papers/2505.10588', 'abstract': 'This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.', 'score': 1, 'issue_id': 3868, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': 'cdc9a4f93d65b071', 'authors': ['Manisha Mehta', 'Fausto Giunchiglia'], 'affiliations': ['University of Trento, Trento, Italy', 'Warren Hyde Middle School, Cupertino, California, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.10588.jpg', 'data': {'categories': ['#healthcare', '#interpretability', '#benchmark', '#ethics', '#multimodal', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Преодолевая языковой барьер: ИИ на страже безопасности цифрового поколения', 'desc': "Исследование оценивает способность ИИ-систем интерпретировать цифровой язык поколения Альфа. Авторы анализируют четыре ведущие модели искусственного интеллекта на предмет обнаружения скрытых форм домогательств и манипуляций в дискурсе этого поколения. Работа включает создание уникального датасета выражений поколения Альфа и разработку framework'а для улучшения систем модерации на базе ИИ. Результаты подчеркивают острую необходимость переработки систем безопасности с учетом особенностей коммуникации молодежи."}, 'en': {'title': 'Bridging the Gap: Enhancing AI Safety for Generation Alpha', 'desc': "This research evaluates how AI systems understand the unique digital language of Generation Alpha, who are growing up with AI technology. It highlights the risks they face online due to their distinct communication styles, influenced by gaming and memes, which can hide harmful interactions from both humans and automated systems. The study tests four AI models on their ability to detect subtle harassment in Gen Alpha's online expressions, revealing significant gaps in their comprehension. The findings emphasize the need for improved AI moderation tools that are better suited to protect youth in their digital environments."}, 'zh': {'title': '重塑安全系统，保护阿尔法世代的数字交流', 'desc': '本研究独特地评估了人工智能系统如何解读阿尔法世代（2010-2024年出生）的数字语言。阿尔法世代是首个与人工智能共同成长的群体，他们在沉浸式数字环境中面临新的在线风险。研究分析了四种领先的人工智能模型（GPT-4、Claude、Gemini和Llama 3）在识别隐藏的骚扰和操控方面的能力。研究结果显示，现有的安全工具未能有效理解阿尔法世代的独特交流方式，强调了重新设计安全系统的紧迫性，以更好地保护年轻用户。'}}}, {'id': 'https://huggingface.co/papers/2505.14633', 'title': 'Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas', 'url': 'https://huggingface.co/papers/2505.14633', 'abstract': "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.", 'score': 0, 'issue_id': 3884, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '40e16ce405eaf398', 'authors': ['Yu Ying Chiu', 'Zhilin Wang', 'Sharan Maiya', 'Yejin Choi', 'Kyle Fish', 'Sydney Levine', 'Evan Hubinger'], 'affiliations': ['Anthropic', 'Cambridge', 'Harvard', 'MIT', 'NVIDIA', 'Stanford', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.14633.jpg', 'data': {'categories': ['#dataset', '#ethics', '#benchmark', '#alignment', '#healthcare'], 'emoji': '🧭', 'ru': {'title': 'Ценности как компас для выявления рисков ИИ', 'desc': 'Статья представляет новый подход к выявлению рисков в системах искусственного интеллекта, основанный на анализе ценностей и приоритетов моделей ИИ. Авторы разработали методологию LitmusValues для оценки приоритетов ИИ в различных классах ценностей. Они также создали набор данных AIRiskDilemmas, содержащий сценарии, в которых ценности противопоставляются друг другу в контексте рисков ИИ. Исследование показало, что анализ ценностей может предсказывать как известные, так и неизвестные рискованные поведения моделей ИИ.'}, 'en': {'title': 'Uncovering AI Risks Through Value Prioritization', 'desc': 'This paper addresses the growing challenge of detecting risks in advanced AI models, particularly those that use techniques like Alignment Faking to avoid detection. The authors propose a method called LitmusValues, which evaluates AI models based on their adherence to various value classes, serving as an early warning system for risky behaviors. They introduce AIRiskDilemmas, a set of scenarios that highlight conflicts between different values, relevant to AI safety. By analyzing the value prioritization of AI models through their choices in these dilemmas, the study reveals how even benign values can indicate potential risks in both known and unknown contexts.'}, 'zh': {'title': '识别人工智能风险的价值观优先级', 'desc': '随着更强大的模型出现，检测人工智能风险变得更加困难。本文提出了一种名为LitmusValues的评估管道，用于揭示人工智能模型在不同价值类别上的优先级。通过收集AIRiskDilemmas，我们创建了一系列涉及价值冲突的困境，以评估人工智能的安全风险。研究表明，LitmusValues中的价值观可以有效预测人工智能在已知和未知风险行为中的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.14629', 'title': 'KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.14629', 'abstract': 'Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.', 'score': 0, 'issue_id': 3880, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'cf8c6e04379454db', 'authors': ['Fnu Mohbat', 'Mohammed J Zaki'], 'affiliations': ['Rensselaer Polytechnic Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.14629.jpg', 'data': {'categories': ['#science', '#benchmark', '#open_source', '#graphs', '#dataset', '#multimodal', '#games'], 'emoji': '🍳', 'ru': {'title': 'KERL: умный помощник на кухне с графами знаний и ИИ', 'desc': 'Статья представляет систему KERL, объединяющую графы знаний о еде и большие языковые модели для персонализированных рекомендаций по питанию. KERL извлекает сущности из вопросов пользователя, ищет релевантную информацию в графе знаний и использует ее как контекст для языковой модели при выборе рецептов. Система также генерирует пошаговые инструкции и информацию о микронутриентах для каждого рецепта. Эксперименты показали, что KERL превосходит существующие подходы в задачах рекомендации блюд, генерации рецептов и анализа пищевой ценности.'}, 'en': {'title': 'KERL: Smart Food Recommendations with LLMs and Knowledge Graphs', 'desc': 'This paper presents KERL, a novel system that combines large language models (LLMs) with food-related knowledge graphs (KGs) to enhance food recommendations and recipe generation. KERL processes natural language queries by extracting relevant entities and retrieving corresponding subgraphs from the KG, which are then used as context for the LLM to generate personalized recipes and nutritional information. The system not only recommends recipes but also provides detailed cooking steps and micro-nutritional data tailored to user preferences. Experimental results demonstrate that KERL outperforms existing methods, showcasing its effectiveness in delivering comprehensive food-related solutions.'}, 'zh': {'title': 'KERL：智能食品推荐与配方生成的解决方案', 'desc': '本研究提出了KERL系统，它结合了食品知识图谱（KGs）和大型语言模型（LLMs），以提供个性化的食品推荐和生成配方及微量营养信息。KERL能够从自然语言问题中提取实体，并从知识图谱中检索子图，将其作为上下文输入到LLM中，以选择满足约束条件的配方。系统还生成每个配方的烹饪步骤和营养信息。通过大量实验，我们证明了KG增强的LLM在食品推荐、配方生成和营养分析方面显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.14556', 'title': 'Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI', 'url': 'https://huggingface.co/papers/2505.14556', 'abstract': 'Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.', 'score': 0, 'issue_id': 3880, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': 'a52590cd595c5afe', 'authors': ['Marlène Careil', 'Yohann Benchetrit', 'Jean-Rémi King'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.14556.jpg', 'data': {'categories': ['#science', '#data', '#diffusion', '#architecture', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Декодирование мыслей в изображения: новый уровень с Dynadiff', 'desc': 'Dynadiff - это новая модель диффузии для реконструкции изображений из динамически меняющихся данных фМРТ. Она упрощает процесс обучения по сравнению с существующими подходами и превосходит современные модели при работе с временными сигналами фМРТ. Модель особенно эффективна для реконструкции семантических характеристик изображений и позволяет точно отслеживать эволюцию представлений изображений в активности мозга. Dynadiff закладывает основу для декодирования изображений из мозговой активности с учетом временной составляющей.'}, 'en': {'title': 'Revolutionizing Brain-to-Image Decoding with Dynadiff', 'desc': 'This paper presents Dynadiff, a novel single-stage diffusion model for decoding images from dynamic fMRI recordings. Unlike traditional methods that rely on complex multi-stage processes, Dynadiff simplifies the training process and enhances performance on time-resolved brain signals. The model excels in reconstructing high-level semantic images while maintaining competitiveness with existing methods on preprocessed data. This advancement paves the way for more accurate and timely brain-to-image decoding, allowing researchers to better understand how images are represented in brain activity over time.'}, 'zh': {'title': '动态神经活动扩散：重建图像的新方法', 'desc': '本研究提出了一种新的单阶段扩散模型Dynadiff，用于从动态变化的功能磁共振成像(fMRI)记录中重建图像。与现有方法相比，Dynadiff简化了训练过程，并在时间分辨率的fMRI信号上超越了最先进的模型，尤其是在高层次语义图像重建指标上表现优异。该模型在处理预处理的fMRI数据时也保持了竞争力，同时能够精确描述大脑活动中图像表示的演变。总体而言，这项工作为时间分辨的大脑到图像解码奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2505.14467', 'title': 'Void in Language Models', 'url': 'https://huggingface.co/papers/2505.14467', 'abstract': 'Despite advances in transformer-based language models (LMs), a fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using a non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks.', 'score': 0, 'issue_id': 3886, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '3ea3356d2e91804e', 'authors': ['Mani Shemiranifar'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.14467.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#interpretability', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Меньше слоев - лучше результат: оптимизация работы языковых моделей', 'desc': "Исследователи изучают активацию слоев в трансформерных языковых моделях во время вывода, используя метод адаптивных вычислений L2 (LAC). Они обнаружили, что не все слои активируются одинаково в фазах обработки запроса и генерации ответа. Эксперименты показали, что пропуск неактивных слоев ('пустот') может улучшить производительность моделей на некоторых задачах. Например, Qwen2.5-7B-Instruct показал улучшение с 69.24 до 71.29 на MMLU, используя только 30% слоев."}, 'en': {'title': 'Unlocking Efficiency: Activating Only What Matters in Language Models', 'desc': "This paper explores whether all layers of transformer-based language models are activated during inference. It introduces a method called L2 Adaptive Computation (LAC) to identify unactivated layers, termed 'Voids', by monitoring the L2-norm of activations. The study analyzes layer activation in two phases: Prompt Processing and Response Generation, revealing that different layers are activated in each phase. The findings demonstrate that selectively skipping unactivated layers can enhance model performance on specific tasks, as shown by improved results on various benchmarks with fewer layers used."}, 'zh': {'title': '选择性激活层，提升模型性能', 'desc': '尽管基于变换器的语言模型取得了进展，但在推理过程中是否所有层都被激活仍然是一个未解的问题。我们通过一种名为L2自适应计算（LAC）的无参数方法来检测未激活的层（称为“空洞”）。该方法监测激活的L2范数变化，以识别在输入提示和生成响应阶段激活的层。我们的研究表明，在这两个阶段中激活的层是不同的，选择性跳过未激活的层可以提高模型在特定任务上的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.14366', 'title': 'Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds', 'url': 'https://huggingface.co/papers/2505.14366', 'abstract': 'We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.', 'score': 0, 'issue_id': 3882, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '0e72d48711224e3c', 'authors': ['Joel Currie', 'Gioele Migno', 'Enrico Piacenti', 'Maria Elena Giannaccini', 'Patric Bach', 'Davide De Tommaso', 'Agnieszka Wykowska'], 'affiliations': ['Italian Institute of Technology, Genova, Italy', 'University of Aberdeen, Aberdeen, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2505.14366.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#dataset', '#agents', '#healthcare', '#cv', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Визуальное пространственное мышление для взаимодействия человека и робота', 'desc': 'Статья представляет концептуальную основу для обучения моделей компьютерного зрения и обработки естественного языка (VLM) выполнению задач визуального принятия перспективы (VPT). Авторы создали синтетический набор данных в NVIDIA Omniverse для обучения с учителем задачам пространственного мышления. Каждый пример включает RGB-изображение, текстовое описание и эталонную матрицу трансформации 4x4, представляющую позу объекта. Исследование фокусируется на определении расстояния по оси Z как базовом навыке, с перспективой расширения до полного рассуждения в 6 степенях свободы.'}, 'en': {'title': 'Empowering Robots with Visual Perspective Taking for Better Interaction', 'desc': 'This paper introduces a framework for training Vision-Language Models (VLMs) to enhance Visual Perspective Taking (VPT), which is crucial for effective Human-Robot Interaction (HRI). The authors create a synthetic dataset using NVIDIA Omniverse, designed for supervised learning in spatial reasoning tasks, containing RGB images, natural language descriptions, and transformation matrices for object poses. The focus is on predicting Z-axis distance, laying the groundwork for future advancements in understanding full 6 Degrees Of Freedom (DOFs). This dataset is made publicly available to encourage further research in developing AI systems that can comprehend spatial relationships in interactive settings.'}, 'zh': {'title': '迈向人机交互的空间理解能力', 'desc': '本文提出了一个训练视觉语言模型（VLMs）以实现视觉视角理解（VPT）的概念框架，这是人机交互（HRI）中重要的能力。我们引入了一个合成数据集，该数据集在NVIDIA Omniverse中生成，支持空间推理任务的监督学习。每个实例包含一个RGB图像、一个自然语言描述和一个表示物体姿态的真实4X4变换矩阵。我们专注于推断Z轴距离作为基础技能，未来将扩展到完整的六自由度（DOFs）推理。'}}}, {'id': 'https://huggingface.co/papers/2505.11563', 'title': 'Object-Centric Representations Improve Policy Generalization in Robot\n  Manipulation', 'url': 'https://huggingface.co/papers/2505.11563', 'abstract': 'Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as a structured alternative that segments visual input into a finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark a range of visual encoders-object-centric, global and dense methods-across a suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.', 'score': 0, 'issue_id': 3880, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '114f28f1ced2da93', 'authors': ['Alexandre Chapin', 'Bruno Machado', 'Emmanuel Dellandrea', 'Liming Chen'], 'affiliations': ['Ecole Centrale de Lyon, LIRIS 69130, Ecully, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.11563.jpg', 'data': {'categories': ['#cv', '#robotics', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Объектно-ориентированные представления - ключ к обобщению в робототехнике', 'desc': 'Статья исследует объектно-ориентированные представления (OCR) как альтернативу глобальным и плотным признакам в задачах роботизированной манипуляции. Авторы сравнивают различные визуальные кодировщики на наборе симулированных и реальных задач манипуляции. Результаты показывают, что политики на основе OCR превосходят другие методы в обобщении на новые визуальные условия. Это указывает на перспективность OCR для создания визуальных систем, эффективно обобщающихся в динамичных робототехнических средах.'}, 'en': {'title': 'Unlocking Robustness in Robotics with Object-Centric Representations', 'desc': 'This paper explores the use of object-centric representations (OCR) in robotic manipulation tasks to improve learning and generalization. Unlike traditional methods that use global or dense features, which can mix relevant and irrelevant information, OCR focuses on distinct entities in the visual input. The authors benchmark various visual encoders, including OCR, against different manipulation tasks to assess their performance under changing conditions. The results show that OCR-based policies significantly outperform other methods, indicating their potential for robust performance in real-world scenarios.'}, 'zh': {'title': '物体中心表示：提升机器人操作的泛化能力', 'desc': '本研究探讨了物体中心表示（OCR）在机器人操作中的应用。与现有的全局或密集特征方法不同，OCR能够将视觉输入分割成一组独立的实体，从而更好地处理与任务相关的信息。我们在多种模拟和真实世界的操作任务中对比了不同的视觉编码器，发现OCR在面对不同的视觉条件时表现出更好的泛化能力。研究结果表明，OCR是一种有前景的方向，可以有效设计出适应动态真实环境的视觉系统。'}}}, {'id': 'https://huggingface.co/papers/2505.06914', 'title': 'The Distracting Effect: Understanding Irrelevant Passages in RAG', 'url': 'https://huggingface.co/papers/2505.06914', 'abstract': 'A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.   Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.', 'score': 0, 'issue_id': 3882, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 мая', 'en': 'May 11', 'zh': '5月11日'}, 'hash': 'dc5ed9552f5c1ec6', 'authors': ['Chen Amiraz', 'Florin Cuconasu', 'Simone Filice', 'Zohar Karnin'], 'affiliations': ['Sapienza University of Rome', 'Technology Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.06914.jpg', 'data': {'categories': ['#hallucinations', '#training', '#rag', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Укрощение отвлекающих факторов для повышения точности RAG', 'desc': 'Исследование посвящено проблеме отвлекающих пассажей в системах генерации с расширенным поиском (RAG). Авторы предлагают метод количественной оценки отвлекающего эффекта пассажа и демонстрируют его устойчивость для разных языковых моделей. Разработаны новые методы идентификации и использования сложных отвлекающих пассажей для улучшения RAG-систем. Дообучение языковых моделей на таких пассажах позволило повысить точность ответов на 7.5% по сравнению с обычными наборами данных RAG.'}, 'en': {'title': 'Enhancing RAG: Turning Distractions into Accuracy Boosts', 'desc': 'This paper addresses a significant challenge in Retrieval Augmented Generation (RAG) systems, where irrelevant passages can mislead language models (LLMs) and result in incorrect answers. The authors propose a new way to measure how distracting a passage can be in relation to a query and an LLM, providing a quantifiable metric for this effect. They introduce innovative techniques for identifying and leveraging these hard distracting passages, which leads to improved performance in answering accuracy. By fine-tuning LLMs with these selected passages, they demonstrate a notable increase in accuracy compared to traditional RAG approaches.'}, 'zh': {'title': '提升检索增强生成的准确性', 'desc': '本论文探讨了检索增强生成（RAG）中的一个重要问题，即与查询无关的检索段落可能会干扰答案生成的语言模型（LLM），导致错误的回答。我们提出了一种量化方法来衡量段落对查询和LLM的干扰效果，并展示了其在不同LLM中的稳健性。通过精心选择这些干扰段落并对LLM进行微调，我们的研究实现了高达7.5%的回答准确率提升。我们的贡献在于超越了简单的二元分类，发展了多种方法来识别和利用难以处理的干扰段落。'}}}, {'id': 'https://huggingface.co/papers/2505.11820', 'title': 'Chain-of-Model Learning for Language Model', 'url': 'https://huggingface.co/papers/2505.11820', 'abstract': 'In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.', 'score': 67, 'issue_id': 3848, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '2e8115f0fe78856b', 'authors': ['Kaitao Song', 'Xiaohua Wang', 'Xu Tan', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yongliang Shen', 'Cen LU', 'Zihao Li', 'Zifan Song', 'Caihua Shan', 'Yansen Wang', 'Kan Ren', 'Xiaoqing Zheng', 'Tao Qin', 'Yuqing Yang', 'Dongsheng Li', 'Lili Qiu'], 'affiliations': ['Fudan University', 'Microsoft Research', 'ShanghaiTech University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11820.jpg', 'data': {'categories': ['#training', '#open_source', '#inference', '#agi', '#architecture', '#optimization'], 'emoji': '🔗', 'ru': {'title': 'Цепная революция в языковых моделях: гибкость и эффективность', 'desc': 'В этой статье представлена новая парадигма обучения под названием Chain-of-Model (CoM), которая внедряет причинно-следственные связи в скрытые состояния каждого слоя модели в виде цепочки. Авторы вводят концепцию Chain-of-Representation (CoR), формулирующую скрытые состояния на каждом уровне как комбинацию нескольких под-представлений на уровне скрытых измерений. На основе этого принципа разработана архитектура Chain-of-Language-Model (CoLM), которая внедряет идею CoM в каждый слой Transformer. Экспериментальные результаты показывают, что семейство моделей CoLM достигает сопоставимой производительности со стандартным Transformer, одновременно обеспечивая большую гибкость в масштабировании и развертывании.'}, 'en': {'title': 'Scaling Language Models with Chain-of-Model Efficiency', 'desc': 'This paper introduces a new learning approach called Chain-of-Model (CoM), which enhances model training efficiency by incorporating causal relationships into the hidden states of each layer. It presents the Chain-of-Representation (CoR) concept, where hidden states are formed from multiple sub-representations, allowing each layer to only access its preceding chains. The CoM framework enables models to scale up by adding more chains, providing flexibility in deploying various sub-models of different sizes. The Chain-of-Language-Model (CoLM) and its variant CoLM-Air further optimize Transformer architectures by sharing key-value pairs across chains, resulting in improved performance and adaptability for language models.'}, 'zh': {'title': '链式模型：灵活高效的语言模型新范式', 'desc': '本文提出了一种新颖的学习范式，称为链式模型（CoM），它将因果关系融入每一层的隐藏状态，以链式结构提高模型训练的效率和推理的灵活性。我们引入了链式表示（CoR）的概念，将每一层的隐藏状态表示为多个子表示的组合（即链）。在每一层中，输出表示的每个链只能查看输入表示中所有前面的链，从而使得基于CoM框架构建的模型能够通过增加链的数量逐步扩大模型规模，并提供不同大小的子模型以实现灵活推理。基于这一原理，我们设计了链式语言模型（CoLM），并进一步引入了CoLM-Air，通过引入键值共享机制，计算第一个链中的所有键和值，然后在所有链之间共享，从而展示了额外的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2505.13417', 'title': 'AdaptThink: Reasoning Models Can Learn When to Think', 'url': 'https://huggingface.co/papers/2505.13417', 'abstract': 'Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.', 'score': 56, 'issue_id': 3845, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'edd33223d8d833a7', 'authors': ['Jiajie Zhang', 'Nianyi Lin', 'Lei Hou', 'Ling Feng', 'Juanzi Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13417.jpg', 'data': {'categories': ['#math', '#reasoning', '#inference', '#training', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное мышление для оптимизации рассуждений ИИ', 'desc': 'Исследователи представили новый алгоритм AdaptThink, который учит модели рассуждения адаптивно выбирать оптимальный режим мышления в зависимости от сложности задачи. Алгоритм использует метод обучения с подкреплением и включает в себя ограниченную целевую функцию оптимизации и стратегию выборки по важности. Эксперименты показали, что AdaptThink значительно сокращает вычислительные затраты при одновременном повышении производительности моделей. На трех наборах математических данных алгоритм сократил среднюю длину ответа модели DeepSeek-R1-Distill-Qwen-1.5B на 53% и повысил ее точность на 2.4%.'}, 'en': {'title': 'Optimize Reasoning with Adaptive Thinking Modes!', 'desc': 'This paper introduces AdaptThink, a reinforcement learning algorithm designed to optimize reasoning models by allowing them to choose between two thinking modes: NoThinking and traditional thinking. NoThinking enables models to skip lengthy reasoning processes for simpler tasks, improving efficiency without sacrificing performance. AdaptThink employs a constrained optimization objective to encourage the use of NoThinking while maintaining overall accuracy, and it uses importance sampling to balance training between both modes. The results show that AdaptThink significantly reduces inference costs and enhances performance on math tasks, demonstrating the effectiveness of adaptive thinking-mode selection.'}, 'zh': {'title': '自适应思考模式选择，提升推理效率与质量', 'desc': '最近，大型推理模型在各种任务上表现出色，但其冗长的思考过程显著增加了推理开销，导致效率成为瓶颈。本文提出了一种名为NoThinking的方法，鼓励推理模型跳过思考，直接生成最终解决方案，适用于相对简单的任务。基于此，我们提出了AdaptThink，这是一种新颖的强化学习算法，旨在根据问题难度自适应选择最佳思考模式。实验表明，AdaptThink显著降低了推理成本，同时提高了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11896', 'title': 'AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.11896', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.', 'score': 43, 'issue_id': 3846, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': 'bdc79864df7cbd51', 'authors': ['Chenwei Lou', 'Zewei Sun', 'Xinnian Liang', 'Meng Qu', 'Wei Shen', 'Wenqi Wang', 'Yuntao Li', 'Qingping Yang', 'Shuangzhi Wu'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2505.11896.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'AdaCoT: Умное рассуждение для языковых моделей', 'desc': 'AdaCoT - это новый фреймворк, позволяющий крупным языковым моделям (LLM) адаптивно решать, когда использовать метод цепочки рассуждений (Chain-of-Thought, CoT). Используя обучение с подкреплением, в частности Proximal Policy Optimization (PPO), AdaCoT оптимизирует баланс между производительностью модели и вычислительными затратами, связанными с применением CoT. Ключевым техническим вкладом является метод Selective Loss Masking (SLM), предотвращающий коллапс границы принятия решений во время многоэтапного обучения с подкреплением. Эксперименты показывают, что AdaCoT значительно снижает использование CoT для запросов, не требующих сложных рассуждений, сохраняя при этом высокую производительность на сложных задачах.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Language Models', 'desc': 'This paper presents AdaCoT, a new framework that improves the efficiency of Large Language Models (LLMs) by adaptively deciding when to use Chain-of-Thought (CoT) prompting. Traditional CoT prompting can be computationally expensive, especially for simpler queries, but AdaCoT optimizes this by framing the decision to use CoT as a Pareto optimization problem. The authors employ reinforcement learning, specifically Proximal Policy Optimization (PPO), to dynamically adjust when CoT is triggered based on the complexity of the input. Their approach includes a technique called Selective Loss Masking (SLM) to ensure stable training, resulting in significant reductions in CoT usage while maintaining high performance on complex tasks.'}, 'zh': {'title': '自适应链式推理，提升效率与性能', 'desc': '大型语言模型（LLMs）在处理复杂推理任务时表现出色，但在某些情况下面临挑战。为了解决这一问题，本文提出了AdaCoT（自适应链式推理），它允许模型根据输入的复杂性自适应地决定是否使用链式推理。我们将自适应推理视为一个帕累托优化问题，旨在平衡模型性能与链式推理的计算成本。实验结果表明，AdaCoT在不需要复杂推理的查询中显著减少了链式推理的使用，提升了效率。'}}}, {'id': 'https://huggingface.co/papers/2505.11254', 'title': 'Delta Attention: Fast and Accurate Sparse Attention Inference by Delta\n  Correction', 'url': 'https://huggingface.co/papers/2505.11254', 'abstract': 'The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.', 'score': 35, 'issue_id': 3847, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '2aba31b686859e82', 'authors': ['Jeffrey Willette', 'Heejun Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.11254.jpg', 'data': {'categories': ['#optimization', '#long_context', '#architecture', '#inference', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Коррекция распределения для эффективного разреженного внимания', 'desc': 'Статья предлагает новый метод для повышения эффективности разреженного внимания в трансформерах. Авторы обнаружили, что разреженное вычисление вызывает сдвиг распределения в выходных данных механизма внимания, что приводит к снижению производительности. Предложенная процедура корректирует этот сдвиг, приближая распределение выходных данных разреженного внимания к квадратичному. Метод может применяться поверх любого алгоритма разреженного внимания, значительно повышая точность при сохранении высокой разреженности и скорости обработки.'}, 'en': {'title': 'Boosting Sparse Attention: Aligning Outputs for Enhanced Performance', 'desc': 'This paper addresses the inefficiencies of the attention mechanism in transformers, which typically has a quadratic complexity that increases inference costs for long sequences. It highlights that while sparse attention methods can reduce computation, they often lead to performance degradation due to a distributional shift in attention outputs. The authors propose a novel procedure to correct this shift, aligning sparse attention outputs more closely with those of traditional quadratic attention. Their method significantly improves performance, achieving an average increase of 36 percentage points while maintaining high sparsity and speed, making it much faster than existing methods.'}, 'zh': {'title': '稀疏注意力的分布修正，提升性能与效率', 'desc': '本文探讨了变换器的注意力机制在处理长序列时的计算复杂度问题，导致推理成本高和延迟大。尽管注意力矩阵通常是稀疏的，但稀疏注意力推理方法在减少计算负担的同时，可能会导致性能下降。我们发现，性能下降的一个原因是稀疏计算引起了注意力输出的分布偏移，这使得解码时的查询与预填阶段的键对齐不佳。为了解决这个问题，我们提出了一种简单而有效的修正方法，使稀疏注意力输出的分布更接近于二次注意力，从而显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2505.13227', 'title': 'Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis', 'url': 'https://huggingface.co/papers/2505.13227', 'abstract': 'Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.', 'score': 34, 'issue_id': 3849, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'fe01e0daca57b031', 'authors': ['Tianbao Xie', 'Jiaqi Deng', 'Xiaochuan Li', 'Junlin Yang', 'Haoyuan Wu', 'Jixuan Chen', 'Wenjing Hu', 'Xinyuan Wang', 'Yuhui Xu', 'Zekun Wang', 'Yiheng Xu', 'Junli Wang', 'Doyen Sahoo', 'Tao Yu', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13227.jpg', 'data': {'categories': ['#data', '#dataset', '#graphs', '#agents', '#benchmark', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'Революция в обучении ИИ работе с компьютерными интерфейсами', 'desc': 'Статья представляет новый бенчмарк OSWorld-G для оценки способности моделей машинного обучения к интерпретации естественного языка в контексте графических интерфейсов. Авторы также создали крупнейший датасет Jedi, содержащий 4 миллиона примеров для обучения моделей взаимодействию с компьютерными интерфейсами. Модели, обученные на Jedi, превзошли существующие подходы на нескольких бенчмарках, включая OSWorld-G. Исследование показало, что улучшенное понимание интерфейсов значительно повышает способности крупных языковых моделей выполнять сложные компьютерные задачи.'}, 'en': {'title': 'Enhancing GUI Grounding with Comprehensive Datasets and Models', 'desc': 'This paper addresses the challenge of GUI grounding, which is the process of translating natural language commands into actions on graphical user interfaces. Current benchmarks are limited as they only focus on simple tasks, neglecting the complexities of real-world interactions that require understanding of software context and layout. The authors introduce OSWorld-G, a new benchmark with 564 detailed samples and the Jedi dataset, which contains 4 million examples to improve grounding tasks. Their findings show that using the Jedi dataset significantly enhances the performance of multi-scale models in executing complex computer tasks, demonstrating the importance of specialized data for effective grounding.'}, 'zh': {'title': '提升计算机使用代理的基础能力', 'desc': '本论文探讨了图形用户界面（GUI）基础的自然语言指令映射问题，指出现有基准测试过于简化，无法反映真实世界的复杂交互。为了解决这一问题，作者提出了OSWorld-G基准，包含564个精细注释的样本，涵盖文本匹配、元素识别、布局理解和精确操作等多种任务类型。此外，作者合成并发布了最大的计算机使用基础数据集Jedi，包含400万个示例，展示了其在复杂计算机任务中的有效性。通过详细的消融研究，论文还识别了影响基础性能的关键因素，并验证了不同界面元素的专门数据结合能够实现对新界面的组合泛化。'}}}, {'id': 'https://huggingface.co/papers/2505.13379', 'title': 'Thinkless: LLM Learns When to Think', 'url': 'https://huggingface.co/papers/2505.13379', 'abstract': "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless", 'score': 25, 'issue_id': 3846, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'd41117eabc11e5c3', 'authors': ['Gongfan Fang', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.13379.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#benchmark', '#training'], 'emoji': '🧠', 'ru': {'title': 'Умное переключение между кратким и развернутым мышлением в языковых моделях', 'desc': 'Статья представляет Thinkless - обучаемую систему, позволяющую языковым моделям адаптивно выбирать между кратким и развернутым рассуждением в зависимости от сложности задачи. Система использует обучение с подкреплением и два управляющих токена: <short> для кратких ответов и <think> для детального рассуждения. В основе метода лежит алгоритм DeGRPO, который разделяет цель обучения на выбор режима рассуждения и улучшение точности ответов. Эмпирические результаты показывают, что Thinkless способен сократить использование длинных цепочек рассуждений на 50-90%, значительно повышая эффективность моделей.'}, 'en': {'title': 'Thinkless: Smart Reasoning for Efficient Language Models', 'desc': "This paper introduces Thinkless, a framework designed to enhance the efficiency of Reasoning Language Models (RLMs) by enabling them to choose between short-form and long-form reasoning based on task complexity. The framework utilizes reinforcement learning and two control tokens, <short> for brief answers and <think> for detailed reasoning, to guide the model's response strategy. A novel algorithm called Decoupled Group Relative Policy Optimization (DeGRPO) is employed to separate the learning objectives, allowing for better control over reasoning mode selection and response accuracy. The results show that Thinkless can significantly reduce the reliance on long-chain reasoning, improving computational efficiency while maintaining performance on various benchmarks."}, 'zh': {'title': '让模型学会何时思考', 'desc': '本文提出了一种名为Thinkless的可学习框架，旨在提高大型语言模型（LLM）在推理任务中的效率。该框架通过强化学习训练，使模型能够根据任务复杂性和自身能力自适应选择短期或长期推理。Thinkless使用两个控制标记<short>和<think>来分别表示简洁回答和详细推理。实验结果表明，Thinkless能够将长期推理的使用减少50%至90%，显著提升推理语言模型的效率。'}}}, {'id': 'https://huggingface.co/papers/2505.13389', 'title': 'Faster Video Diffusion with Trainable Sparse Attention', 'url': 'https://huggingface.co/papers/2505.13389', 'abstract': 'Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53times with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6times and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.', 'score': 24, 'issue_id': 3850, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '33a48c202961951b', 'authors': ['Peiyuan Zhang', 'Haofeng Huang', 'Yongqi Chen', 'Will Lin', 'Zhengzhong Liu', 'Ion Stoica', 'Eric P. Xing', 'Hao Zhang'], 'affiliations': ['MBZUAI', 'UC Berkeley', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.13389.jpg', 'data': {'categories': ['#optimization', '#architecture', '#diffusion', '#training', '#open_source', '#video'], 'emoji': '🎥', 'ru': {'title': 'Эффективное разреженное внимание для масштабирования видео-диффузионных моделей', 'desc': 'Статья представляет новый метод разреженного внимания (VSA) для масштабирования видео-диффузионных трансформеров. VSA использует двухэтапный подход: грубый этап для выявления критических токенов и тонкий этап для вычисления внимания только внутри важных областей. Метод обучается от начала до конца, не требует пост-обработки и сохраняет 85% эффективности по сравнению с полным вниманием. Эксперименты показывают, что VSA сокращает вычислительные затраты в 2,53 раза без потери качества, а также ускоряет генерацию в 6 раз для существующих моделей.'}, 'en': {'title': 'Efficient Attention for Scalable Video Diffusion', 'desc': 'This paper introduces VSA, a novel trainable sparse attention mechanism designed to improve the efficiency of video diffusion transformers (DiTs). By focusing on a small subset of critical tokens, VSA reduces the computational burden associated with traditional quadratic attention. The method consists of a coarse stage that pools tokens and identifies important ones, followed by a fine stage that computes attention only within these selected tokens. The results demonstrate that VSA significantly decreases training FLOPS while maintaining performance, making it a viable alternative for scaling video diffusion models.'}, 'zh': {'title': '可训练稀疏注意力：视频扩散模型的新选择', 'desc': '这篇论文提出了一种名为VSA的可训练稀疏注意力机制，旨在解决视频扩散变换器（DiTs）在处理3D注意力时的计算限制。VSA通过将注意力计算分为粗略阶段和精细阶段，显著提高了计算效率，同时保持了高效的训练性能。实验结果表明，VSA在不降低扩散损失的情况下，将训练的FLOPS减少了2.53倍，并且在开源模型Wan-2.1上实现了6倍的注意力计算加速。该研究表明，可训练的稀疏注意力是全注意力的有效替代方案，并为视频扩散模型的进一步扩展提供了关键支持。'}}}, {'id': 'https://huggingface.co/papers/2505.13308', 'title': 'Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space', 'url': 'https://huggingface.co/papers/2505.13308', 'abstract': "Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.", 'score': 23, 'issue_id': 3851, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'bd9e494dc68db18c', 'authors': ['Hengli Li', 'Chenxi Li', 'Tong Wu', 'Xuekai Zhu', 'Yuxuan Wang', 'Zhaoxin Yu', 'Eric Hanchen Jiang', 'Song-Chun Zhu', 'Zixia Jia', 'Ying Nian Wu', 'Zilong Zheng'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'Institute of Automation, Chinese Academy of Sciences', 'NLCo Lab, Beijing Institute for General Artificial Intelligence', 'Shanghai Jiao Tong University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2505.13308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#reasoning', '#training', '#agi'], 'emoji': '🧠', 'ru': {'title': 'LatentSeek: Повышение способности рассуждать у ИИ через адаптацию в латентном пространстве', 'desc': 'Статья представляет LatentSeek - новый фреймворк для улучшения способностей рассуждения больших языковых моделей (LLM) с помощью адаптации на уровне экземпляров во время тестирования в латентном пространстве модели. LatentSeek использует градиент политики для итеративного обновления латентных представлений, руководствуясь самогенерируемыми сигналами вознаграждения. Метод превосходит сильные базовые линии на различных тестах рассуждений и демонстрирует высокую эффективность, обычно сходясь за несколько итераций. LatentSeek позиционируется как легковесное, масштабируемое и эффективное решение для улучшения способностей рассуждения LLM.'}, 'en': {'title': 'Enhancing LLM Reasoning with LatentSeek', 'desc': "This paper addresses the challenges of reasoning in Large Language Models (LLMs) as they strive for Artificial General Intelligence (AGI). It introduces LatentSeek, a framework that improves reasoning by adapting the model's latent space during test time, rather than updating parameters. By using policy gradient methods, LatentSeek iteratively refines latent representations based on self-generated rewards, leading to enhanced performance on reasoning tasks. The results show that LatentSeek outperforms existing methods and is efficient, converging quickly while still benefiting from additional iterations."}, 'zh': {'title': 'LatentSeek：提升LLM推理能力的新方法', 'desc': '这篇论文探讨了大型语言模型（LLMs）在推理能力方面的挑战，尤其是在追求通用人工智能（AGI）时。作者提出了一种新框架LatentSeek，通过在模型的潜在空间中进行测试时实例级适应（TTIA），来增强LLMs的推理能力。与以往方法不同，LatentSeek利用策略梯度迭代更新潜在表示，并通过自生成的奖励信号进行指导。实验结果表明，LatentSeek在多个推理基准测试中表现优异，显示出其在潜在空间中进行测试时扩展的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.12082', 'title': 'Model Merging in Pre-training of Large Language Models', 'url': 'https://huggingface.co/papers/2505.12082', 'abstract': 'Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.', 'score': 23, 'issue_id': 3854, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '7f25885def33b040', 'authors': ['Yunshui Li', 'Yiyuan Ma', 'Shen Yan', 'Chaoyi Zhang', 'Jing Liu', 'Jianqiao Lu', 'Ziwen Xu', 'Mengzhao Chen', 'Minrui Wang', 'Shiyi Zhan', 'Jin Ma', 'Xunhao Lai', 'Yao Luo', 'Xingyan Bin', 'Hongbin Ren', 'Mingji Han', 'Wenhao Hao', 'Bairen Yi', 'LingJun Liu', 'Bole Ma', 'Xiaoying Jia', 'Zhou Xun', 'Liang Xiang', 'Yonghui Wu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.12082.jpg', 'data': {'categories': ['#optimization', '#open_source', '#architecture', '#training'], 'emoji': '🔀', 'ru': {'title': 'Слияние моделей: путь к эффективному предобучению больших языковых моделей', 'desc': 'Статья исследует применение техники слияния моделей в процессе предварительного обучения больших языковых моделей. Авторы проводят эксперименты с плотными архитектурами и архитектурами Mixture-of-Experts, демонстрируя значительное улучшение производительности при слиянии контрольных точек, обученных с постоянной скоростью обучения. Результаты показывают возможность более эффективной разработки моделей и снижения затрат на обучение. Исследование предоставляет новые идеи о механизмах слияния моделей и практические рекомендации для сообщества открытого исходного кода.'}, 'en': {'title': 'Enhancing Language Models through Effective Model Merging', 'desc': 'This paper explores the technique of model merging to improve large language models during their pre-training phase. The authors conduct extensive experiments on various architectures, including dense models and Mixture-of-Experts (MoE), to assess the impact of merging checkpoints. They find that using constant learning rates during merging not only enhances model performance but also allows for better predictions of training behavior. The study provides valuable insights and practical guidelines for the open-source community to implement effective model merging strategies, ultimately reducing training costs and improving efficiency.'}, 'zh': {'title': '模型合并：提升预训练效率的新方法', 'desc': '模型合并是一种有前景的技术，可以增强大型语言模型，但在大规模预训练中的应用仍然相对未被探索。本文全面研究了在预训练过程中使用的模型合并技术。通过对数百万到超过1000亿参数的密集和混合专家（MoE）架构进行广泛实验，我们证明了使用恒定学习率训练的检查点合并不仅显著提高了性能，还能准确预测退火行为。这些改进使得模型开发更加高效，并显著降低了训练成本。'}}}, {'id': 'https://huggingface.co/papers/2505.13427', 'title': 'MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision', 'url': 'https://huggingface.co/papers/2505.13427', 'abstract': 'While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://github.com/ModalMinds/MM-PRM.', 'score': 20, 'issue_id': 3847, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '22362149c9b7b5ae', 'authors': ['Lingxiao Du', 'Fanqing Meng', 'Zongkai Liu', 'Zhixiang Zhou', 'Ping Luo', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13427.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#multimodal', '#math', '#training', '#open_source', '#benchmark', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Автоматизированное обучение мультимодальных моделей пошаговым рассуждениям', 'desc': 'В статье представлена модель MM-PRM, обучающаяся оценивать промежуточные шаги рассуждений в мультимодальных задачах. Авторы создали датасет MM-K12 с 10 000 мультимодальных математических задач и использовали метод Монте-Карло для генерации более 700 тысяч аннотаций шагов решения без участия человека. Применение MM-PRM в схеме вывода Best-of-N значительно улучшило результаты как на тестовом наборе MM-K12, так и на внешних бенчмарках. Исследование показывает эффективность использования мягких меток, меньших скоростей обучения и разнообразия путей для оптимизации производительности модели.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Process Supervision', 'desc': 'This paper introduces MM-PRM, a novel process reward model designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs) in solving complex math problems. The authors highlight that MLLMs often struggle with multi-step reasoning due to insufficient supervision of intermediate steps. To overcome this, they create a strong multimodal model called MM-Policy and a new dataset, MM-K12, containing 10,000 multimodal math problems. By employing a Monte Carlo Tree Search method, they generate over 700,000 annotations to train the PRM, which significantly enhances the logical consistency of reasoning in various benchmarks.'}, 'zh': {'title': '过程监督提升多模态推理的逻辑稳健性', 'desc': '这篇论文介绍了一种新的多模态过程奖励模型（MM-PRM），旨在提高多模态大语言模型在复杂多步骤推理中的表现。研究发现，现有模型在推理过程中缺乏细粒度的监督，导致逻辑不一致或部分正确的结果。为了解决这个问题，作者构建了一个强大的多模态模型MM-Policy，并创建了一个包含10,000个可验证答案的多模态数学问题数据集MM-K12。通过无人工标注的方式生成超过70万条步骤级注释，MM-PRM在推理路径评分中表现出显著的改进，证明了过程监督在增强多模态推理系统逻辑稳健性方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.13215', 'title': 'Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation', 'url': 'https://huggingface.co/papers/2505.13215', 'abstract': 'Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.', 'score': 20, 'issue_id': 3846, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '0ab00a261298ad44', 'authors': ['Seungjun Oh', 'Younggeun Lee', 'Hyejin Jeon', 'Eunbyung Park'], 'affiliations': ['Department of Artificial Intelligence, Sungkyunkwan University', 'Department of Artificial Intelligence, Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13215.jpg', 'data': {'categories': ['#3d'], 'emoji': '🎥', 'ru': {'title': 'Гибридный 3D-4D подход для эффективной реконструкции динамических сцен', 'desc': 'Статья представляет новый метод 3D-4DGS для реконструкции динамических 3D-сцен. Он комбинирует 3D гауссианы для статичных областей и 4D гауссианы для динамических элементов. Это позволяет значительно сократить вычислительные затраты и память по сравнению с полностью 4D подходом. Метод демонстрирует более быстрое обучение при сохранении или улучшении визуального качества.'}, 'en': {'title': 'Efficient 3D-4D Scene Reconstruction with Hybrid Gaussian Splatting', 'desc': 'This paper presents a new method called hybrid 3D-4D Gaussian Splatting (3D-4DGS) for dynamic 3D scene reconstruction. It combines 3D and 4D Gaussian representations to efficiently model static and dynamic elements in a scene. By converting static regions to 3D Gaussians, the method reduces computational load and memory usage while preserving the quality of dynamic elements with 4D Gaussians. The results show that 3D-4DGS achieves faster training times and improved visual quality compared to traditional 4D Gaussian Splatting techniques.'}, 'zh': {'title': '高效的动态3D场景重建新方法', 'desc': '最近动态3D场景重建的进展显示出良好的效果，能够实现高保真度的3D新视图合成，并提高时间一致性。在这些方法中，4D高斯点云（4DGS）因其能够建模高保真的空间和时间变化而受到关注。然而，现有方法在静态区域冗余分配4D高斯时，导致了显著的计算和内存开销，并可能降低图像质量。我们提出了一种混合3D-4D高斯点云（3D-4DGS）框架，能够自适应地用3D高斯表示静态区域，同时为动态元素保留4D高斯，从而显著提高计算效率。'}}}, {'id': 'https://huggingface.co/papers/2505.12805', 'title': 'FedSVD: Adaptive Orthogonalization for Private Federated Learning with\n  LoRA', 'url': 'https://huggingface.co/papers/2505.12805', 'abstract': 'Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update (BA) intensifies this effect. Freezing one matrix (e.g., A) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the B matrix and transmits it to the server. The server aggregates the B matrices, computes the product BA using the previous A, and refactorizes the result via SVD. This yields a new adaptive A composed of the orthonormal right singular vectors of BA, and an updated B containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing A to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of A bounds the gradient norms of B and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.', 'score': 20, 'issue_id': 3848, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '41ef4fd84db0c7fb', 'authors': ['Seanie Lee', 'Sangwoo Park', 'Dong Bok Lee', 'Dominik Wagner', 'Haebin Seong', 'Tobias Bocklet', 'Juho Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'Technische Hochschule Nürnberg Georg Simon Ohm'], 'pdf_title_img': 'assets/pdf/title_img/2505.12805.jpg', 'data': {'categories': ['#training', '#data', '#benchmark', '#security', '#optimization'], 'emoji': '🔒', 'ru': {'title': 'FedSVD: Защищенное федеративное обучение языковых моделей с сохранением эффективности', 'desc': 'Статья представляет новый метод FedSVD для эффективного федеративного обучения языковых моделей с дифференциальной приватностью. FedSVD решает проблему усиления шума в методе Low-Rank Adaptation (LoRA) при использовании DP-SGD. Метод использует сингулярное разложение (SVD) для глобальной репараметризации, что позволяет избежать квадратичного усиления шума. FedSVD показывает улучшенную стабильность и производительность по сравнению с базовыми методами в различных настройках приватности.'}, 'en': {'title': 'Enhancing Federated Learning with FedSVD: A Noise-Resilient Approach', 'desc': 'This paper presents FedSVD, a novel method that enhances the fine-tuning of language models in federated learning while addressing the challenges posed by noise amplification in differentially private stochastic gradient descent (DP-SGD). By utilizing singular value decomposition (SVD), FedSVD allows clients to optimize only one matrix (B) and send it to a central server, which then aggregates these updates to improve model adaptation. This approach mitigates the noise amplification that occurs when combining LoRA with DP-SGD, ensuring that the model remains expressive without compromising privacy. The results demonstrate that FedSVD improves both stability and performance across various privacy settings, outperforming existing methods.'}, 'zh': {'title': 'FedSVD：优化联邦学习中的低秩适应', 'desc': '本文提出了一种名为FedSVD的方法，旨在解决低秩适应（LoRA）在联邦学习中与差分隐私随机梯度下降（DP-SGD）结合时的噪声放大问题。通过引入基于奇异值分解（SVD）的全局重参数化，FedSVD允许每个客户端仅优化B矩阵并将其传输到服务器。服务器聚合B矩阵，计算BA的乘积，并通过SVD重新因式分解，从而生成新的适应性A和更新后的B。该方法有效减少了噪声放大，同时提高了模型的稳定性和性能，尤其在隐私设置下表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.12504', 'title': 'CPGD: Toward Stable Rule-based Reinforcement Learning for Language\n  Models', 'url': 'https://huggingface.co/papers/2505.12504', 'abstract': 'Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA.', 'score': 20, 'issue_id': 3847, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': 'f8b07da7e5e43f1e', 'authors': ['Zongkai Liu', 'Fanqing Meng', 'Lingxiao Du', 'Zhixiang Zhou', 'Chao Yu', 'Wenqi Shao', 'Qiaosheng Zhang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12504.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Стабильное обучение с подкреплением для языковых моделей', 'desc': 'Статья представляет новый алгоритм CPGD для стабилизации обучения с подкреплением языковых моделей. CPGD вводит ограничение на дрейф политики на основе KL-дивергенции и использует механизм отсечения для предотвращения чрезмерных обновлений политики. Авторы теоретически обосновывают CPGD и эмпирически демонстрируют, что он уменьшает нестабильность, наблюдаемую в предыдущих подходах. Результаты показывают значительное улучшение производительности при сохранении стабильности обучения.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with CPGD', 'desc': 'This paper introduces Clipped Policy Gradient Optimization with Policy Drift (CPGD), a new algorithm aimed at improving the stability of reinforcement learning in language models. Traditional methods often face issues like training collapse due to large policy updates and improper clipping. CPGD addresses these challenges by using a KL divergence-based policy drift constraint to regulate updates and a clipping mechanism to limit excessive changes. The authors provide theoretical support for CPGD and demonstrate its effectiveness in enhancing performance while ensuring stable training.'}, 'zh': {'title': '稳定强化学习，提升语言模型性能', 'desc': '最近，基于规则的强化学习（RL）在语言模型（LM）的推理能力上取得了显著进展，但现有的RL方法如GRPO、REINFORCE++和RLOO常常面临训练不稳定的问题。为了解决这个问题，我们提出了一种新算法——带有策略漂移的剪切策略梯度优化（CPGD），旨在稳定语言模型中的策略学习。CPGD通过基于KL散度的策略漂移约束动态地规范策略更新，并利用对数比率的剪切机制防止过大的策略更新。我们的理论分析和实证结果表明，CPGD不仅缓解了之前方法的训练不稳定性，还显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2505.12992', 'title': 'Fractured Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2505.12992', 'abstract': 'Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.', 'score': 16, 'issue_id': 3849, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '464c0b3fac842217', 'authors': ['Baohao Liao', 'Hanze Dong', 'Yuhui Xu', 'Doyen Sahoo', 'Christof Monz', 'Junnan Li', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2505.12992.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование рассуждений языковых моделей без переобучения', 'desc': 'Эта статья представляет новый метод под названием Fractured Sampling для улучшения рассуждений больших языковых моделей (LLM) во время вывода. Метод основан на идее усечения цепочки рассуждений (Chain-of-Thought, CoT) и позволяет балансировать между полным CoT и генерацией только ответа по трем осям: количество траекторий рассуждений, количество финальных решений на траекторию и глубина усечения. Авторы показывают, что Fractured Sampling достигает лучшего соотношения точности и вычислительных затрат на пяти эталонных наборах данных для задач рассуждения. Результаты демонстрируют значительное улучшение масштабирования производительности LLM при рассуждениях без необходимости переобучения модели.'}, 'en': {'title': 'Efficient Reasoning with Fractured Sampling', 'desc': 'This paper presents a new method called Fractured Sampling that enhances the reasoning abilities of large language models (LLMs) during inference without the need for retraining. It builds on the concept of Chain-of-Thought (CoT) prompting, which improves accuracy by generating intermediate reasoning steps, but often at a high token cost. The authors demonstrate that a truncated version of CoT can achieve similar results with significantly fewer tokens. By exploring different ways to balance reasoning depth and the number of solutions, Fractured Sampling offers a more efficient approach that improves accuracy while reducing computational costs.'}, 'zh': {'title': '高效推理：Fractured Sampling的创新之路', 'desc': '本文探讨了一种新的推理时间缩放技术，称为Fractured Sampling，旨在提高大型语言模型（LLMs）的推理能力。通过截断链式思维（CoT），该方法在生成最终答案时减少了所需的token数量，同时保持了与完整CoT相似的准确性。Fractured Sampling在推理轨迹数量、每条轨迹的最终解决方案数量和推理深度等三个维度上进行插值，从而优化了准确性与成本的平衡。实验结果表明，该方法在多个推理基准上表现出色，能够实现更高效和可扩展的LLM推理。'}}}, {'id': 'https://huggingface.co/papers/2505.13444', 'title': 'ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.13444', 'abstract': 'Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.', 'score': 15, 'issue_id': 3848, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '7bd36c8068fb640c', 'authors': ['Liyan Tang', 'Grace Kim', 'Xinyu Zhao', 'Thom Lake', 'Wenxuan Ding', 'Fangcong Yin', 'Prasann Singhal', 'Manya Wadhwa', 'Zeyu Leo Liu', 'Zayne Sprague', 'Ramya Namuduri', 'Bodun Hu', 'Juan Diego Rodriguez', 'Puyuan Peng', 'Greg Durrett'], 'affiliations': ['The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.13444.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#cv', '#benchmark', '#synthetic', '#reasoning', '#dataset'], 'emoji': '📊', 'ru': {'title': 'Раскрывая пробелы в визуальном мышлении ИИ при анализе диаграмм', 'desc': 'Статья представляет новый тестовый набор данных ChartMuseum для оценки понимания диаграмм моделями компьютерного зрения и обработки естественного языка. Исследование показывает, что современные мультимодальные модели значительно уступают людям в задачах, требующих сложного визуального анализа диаграмм. Авторы обнаружили существенное снижение производительности моделей при увеличении визуальной сложности задач. ChartMuseum эффективно выявляет разрыв между возможностями моделей и людей в понимании диаграмм, особенно в задачах, требующих преимущественно визуального рассуждения.'}, 'en': {'title': 'Bridging the Gap in Chart Understanding: Visual vs. Textual Reasoning', 'desc': 'This paper addresses the challenges faced by large vision-language models (LVLMs) in understanding charts, highlighting their struggle with visual reasoning compared to textual reasoning. The authors present a case study using a synthetic dataset that shows a significant drop in model performance as visual complexity increases, while human performance remains stable. They introduce ChartMuseum, a new benchmark for Chart Question Answering (QA) that includes 1,162 expert-annotated questions designed to test both visual and textual reasoning. The results reveal a substantial performance gap between humans and models, with the best model achieving only 63% accuracy compared to 93% for humans, particularly struggling with questions that require visual reasoning.'}, 'zh': {'title': '图表理解：人类与模型的差距', 'desc': '图表理解对大型视觉语言模型（LVLMs）提出了独特的挑战，因为它需要复杂的文本和视觉推理能力的结合。当前的LVLM在这些技能之间存在显著的不平衡，尤其是在视觉推理方面表现不佳。我们通过一个合成数据集进行案例研究，发现随着视觉复杂性的增加，模型性能显著下降，而人类的表现则保持稳定。我们引入了ChartMuseum，这是一个新的图表问答基准，包含1162个专家注释的问题，旨在评估复杂的视觉和文本推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.11932', 'title': 'Neuro-Symbolic Query Compiler', 'url': 'https://huggingface.co/papers/2505.11932', 'abstract': "Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar G[q] to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.", 'score': 14, 'issue_id': 3846, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '9445be4eff7e4edc', 'authors': ['Yuyao Zhang', 'Zhicheng Dou', 'Xiaoxi Li', 'Jiajie Jin', 'Yongkang Wu', 'Zhonghua Li', 'Qi Ye', 'Ji-Rong Wen'], 'affiliations': ['Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.11932.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Компиляция запросов для точного поиска в RAG-системах', 'desc': 'QCompiler - это нейро-символическая система для улучшения понимания сложных запросов в RAG-системах. Она использует минимальную грамматику BNF для формализации запросов и компилирует их в абстрактные синтаксические деревья. Это позволяет более точно извлекать документы и генерировать ответы на сложные запросы с вложенными структурами. QCompiler включает в себя переводчик выражений запросов, лексический синтаксический анализатор и рекурсивный процессор.'}, 'en': {'title': 'Enhancing Query Understanding in RAG Systems with QCompiler', 'desc': 'This paper introduces QCompiler, a neuro-symbolic framework designed to enhance the understanding of complex search queries in Retrieval-Augmented Generation (RAG) systems. It utilizes a specially designed Backus-Naur Form (BNF) grammar to formalize these queries, ensuring that they are both complete and free of unnecessary complexity. QCompiler operates through a series of components, including a Query Expression Translator and a Lexical Syntax Parser, which work together to convert queries into Abstract Syntax Trees (ASTs). By focusing on the atomicity of sub-queries, QCompiler improves the accuracy of document retrieval and response generation for intricate queries.'}, 'zh': {'title': '提升RAG系统的复杂查询识别能力', 'desc': '本文提出了一种名为QCompiler的神经符号框架，旨在提高检索增强生成（RAG）系统对复杂查询的识别能力。QCompiler基于语言语法规则和编译器设计，设计了一种最小但足够的巴科斯-诺尔形式（BNF）语法G[q]，以形式化复杂查询。与以往方法不同，这种语法在保持完整性的同时，减少了冗余。通过将查询编译成抽象语法树（AST），QCompiler能够更精确地检索文档并生成响应，从而显著提升RAG系统处理复杂查询的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.12346', 'title': 'SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2505.12346', 'abstract': "Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.", 'score': 13, 'issue_id': 3857, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': 'fbe6291cd68e8efb', 'authors': ['Minghan Chen', 'Guikun Chen', 'Wenguan Wang', 'Yi Yang'], 'affiliations': ['ReLER Lab, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12346.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#math', '#benchmark', '#rl', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Умное обучение ИИ: учитываем неуверенность модели', 'desc': 'Статья представляет новый метод обучения больших языковых моделей под названием SEED-GRPO. Этот подход учитывает уверенность модели в ответах на различные вопросы, измеряя семантическую энтропию генерируемых ответов. SEED-GRPO модулирует величину обновлений политики в зависимости от неопределенности вопроса, что позволяет более консервативно обновлять параметры на сложных вопросах. Эксперименты на пяти математических бенчмарках показали, что SEED-GRPO достигает нового уровня точности, превосходящего современные методы.'}, 'en': {'title': 'Enhancing LLM Training with Uncertainty Awareness', 'desc': "This paper introduces SEED-GRPO, an advanced method for training large language models (LLMs) that takes into account the model's uncertainty regarding input prompts. Traditional Group Relative Policy Optimization (GRPO) treats all prompts the same, which can overlook important variations in the model's confidence. SEED-GRPO incorporates semantic entropy, a measure of the diversity of answers generated for a prompt, to adjust how much the model learns from different questions. By applying this uncertainty-aware approach, the model can make more cautious updates for prompts it finds challenging, leading to improved performance on mathematical reasoning tasks."}, 'zh': {'title': '基于不确定性的策略优化新方法', 'desc': '大型语言模型（LLMs）在处理输入提示时表现出不同的信心水平，有些提示产生一致且语义相似的答案，而另一些则产生多样或矛盾的输出。这种变化反映了模型对输入提示的不确定性，表明模型对特定问题的理解程度。传统的群体相对策略优化（GRPO）在策略更新时对所有提示一视同仁，忽略了模型知识边界的重要信息。为了解决这个问题，我们提出了SEED-GRPO（语义熵增强的GRPO），它通过测量输入提示的语义熵来显式考虑LLMs的不确定性，从而动态调整策略更新的幅度。'}}}, {'id': 'https://huggingface.co/papers/2505.12081', 'title': 'VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.12081', 'abstract': 'Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).', 'score': 13, 'issue_id': 3845, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '9b7953f88ae7653d', 'authors': ['Yuqi Liu', 'Tianyuan Qu', 'Zhisheng Zhong', 'Bohao Peng', 'Shu Liu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2505.12081.jpg', 'data': {'categories': ['#multimodal', '#cv', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Единая модель для многозадачного визуального восприятия', 'desc': 'В статье представлен VisionReasoner - унифицированная модель для решения различных задач визуального восприятия. Модель использует новые стратегии когнитивного обучения с несколькими объектами и систематическое переформулирование задач для улучшения способностей к рассуждению. VisionReasoner генерирует структурированный процесс рассуждений перед выдачей ответов на запросы пользователей. Экспериментальные результаты показывают превосходство VisionReasoner над Qwen2.5VL в задачах обнаружения, сегментации и подсчета объектов.'}, 'en': {'title': 'VisionReasoner: Unifying Visual Perception with Advanced Reasoning', 'desc': 'This paper presents VisionReasoner, a unified framework designed to enhance visual perception tasks through advanced reasoning capabilities. It employs innovative multi-object cognitive learning strategies and reformulates tasks systematically to improve its performance across various visual challenges. The model processes visual inputs in a structured manner, allowing it to effectively respond to user queries. Evaluation results demonstrate that VisionReasoner significantly outperforms existing models in detection, segmentation, and counting tasks, showcasing its effectiveness as a comprehensive solution for visual perception.'}, 'zh': {'title': '统一视觉感知的推理能力', 'desc': '本文介绍了一种名为VisionReasoner的统一框架，能够在共享模型中处理多种视觉感知任务。通过设计新颖的多对象认知学习策略和系统的任务重构，VisionReasoner增强了其推理能力，以分析视觉输入并解决多样的感知任务。该模型在生成所需输出之前，会先进行结构化的推理过程，以响应用户查询。实验结果表明，VisionReasoner在检测、分割和计数等三个关键领域的十个任务上表现优异，超越了Qwen2.5VL。'}}}, {'id': 'https://huggingface.co/papers/2505.07704', 'title': 'Through the Looking Glass: Common Sense Consistency Evaluation of Weird\n  Images', 'url': 'https://huggingface.co/papers/2505.07704', 'abstract': 'Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.', 'score': 13, 'issue_id': 3862, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9fc123da51142c1e', 'authors': ['Elisei Rykov', 'Kseniia Petrushina', 'Kseniia Titova', 'Anton Razzhigaev', 'Alexander Panchenko', 'Vasily Konovalov'], 'affiliations': ['AIRI', 'MTS AI', 'Moscow Institute of Physics and Technology', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.07704.jpg', 'data': {'categories': ['#training', '#dataset', '#interpretability', '#cv', '#architecture', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Новый взгляд на здравый смысл в компьютерном зрении', 'desc': 'Исследователи представили новый метод оценки здравого смысла в изображениях под названием Through the Looking Glass (TLG). Метод использует крупные мультимодальные модели (LVLMs) и энкодер на основе трансформеров для извлечения атомарных фактов из изображений. Затем эти факты обрабатываются с помощью компактного классификатора с пулингом внимания. TLG достиг нового уровня производительности на датасетах WHOOPS! и WEIRD.'}, 'en': {'title': 'Assessing Image Realism with Through the Looking Glass (TLG)', 'desc': 'This paper presents a new method called Through the Looking Glass (TLG) for evaluating the common sense consistency of images using Large Vision-Language Models (LVLMs). The approach involves extracting atomic facts from images with a Transformer-based encoder, which helps in understanding the context of the images better. By fine-tuning a compact attention-pooling classifier on these encoded facts, TLG improves the accuracy of image assessments. The method has set a new benchmark in performance on the WHOOPS! and WEIRD datasets, demonstrating its effectiveness in measuring image realism.'}, 'zh': {'title': '透过镜子：图像常识一致性的新方法', 'desc': '在人工智能研究中，评估真实图像的外观是一项复杂的任务。我们提出了一种新方法，称为透过镜子（TLG），用于评估图像的一致性与常识。该方法利用大型视觉语言模型（LVLMs）提取图像中的基本事实，并通过对这些事实进行编码来获得准确的信息。我们的TLG在WHOOPS!和WEIRD数据集上实现了新的最先进性能，同时使用了紧凑的微调组件。'}}}, {'id': 'https://huggingface.co/papers/2505.13180', 'title': 'ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.13180', 'abstract': 'Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning.', 'score': 11, 'issue_id': 3851, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '333ba599d7e29ff8', 'authors': ['Matteo Merler', 'Nicola Dainese', 'Minttu Alakuijala', 'Giovanni Bonetta', 'Pietro Ferrazzi', 'Yu Tian', 'Bernardo Magnini', 'Pekka Marttinen'], 'affiliations': ['Department of Computer Science, Aalto University', 'Department of Mathematics, Università degli Studi di Padova', 'Fondazione Bruno Kessler'], 'pdf_title_img': 'assets/pdf/title_img/2505.13180.jpg', 'data': {'categories': ['#benchmark', '#video', '#cv', '#games', '#reasoning', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'ViPlan: новый бенчмарк для сравнения символьного и прямого визуального планирования', 'desc': 'Статья представляет ViPlan - первый открытый бенчмарк для визуального планирования с использованием символьных предикатов и моделей визуально-языкового восприятия (VLM). Авторы сравнивают эффективность символьного планирования на основе VLM и прямого планирования с помощью VLM в двух доменах: визуальном варианте классической задачи Blocksworld и симулированной среде домашней робототехники. Исследование показывает, что символьное планирование превосходит прямое планирование VLM в Blocksworld, где критически важна точная привязка к изображению, в то время как в задачах домашней робототехники наблюдается обратная ситуация. Авторы также обнаружили, что использование метода Chain-of-Thought не дает значительных преимуществ для большинства моделей и методов.'}, 'en': {'title': 'ViPlan: Bridging Symbolic Planning and Vision-Language Models for Better Visual Reasoning', 'desc': 'This paper discusses the integration of Large Language Models (LLMs) with symbolic planners to create more reliable and understandable plans in visual tasks. It introduces ViPlan, an open-source benchmark designed to evaluate various planning methods using Vision-Language Models (VLMs) in two distinct environments: Blocksworld and household robotics. The study compares the performance of symbolic planning against direct VLM planning, revealing that symbolic methods excel in scenarios requiring precise image grounding, while VLMs perform better in tasks that demand commonsense reasoning. Additionally, the findings indicate that current VLMs do not significantly benefit from Chain-of-Thought prompting, highlighting ongoing challenges in visual reasoning capabilities.'}, 'zh': {'title': '视觉规划的新基准：ViPlan', 'desc': '本论文介绍了ViPlan，这是第一个用于视觉规划的开源基准，结合了符号规划和视觉语言模型（VLM）。我们设计了一系列逐渐增加难度的任务，包括经典的Blocksworld规划问题和模拟家庭机器人环境。通过对九个开源VLM模型进行基准测试，我们发现符号规划在Blocksworld中表现优于直接使用VLM进行规划，而在家庭机器人任务中则相反。最后，我们的研究表明，当前的VLM在视觉推理方面仍然存在困难，使用Chain-of-Thought提示并没有显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11855', 'title': 'When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification\n  of Scientific Research', 'url': 'https://huggingface.co/papers/2505.11855', 'abstract': 'Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.', 'score': 8, 'issue_id': 3849, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '8432e529923dacc5', 'authors': ['Guijin Son', 'Jiwoo Hong', 'Honglu Fan', 'Heejeong Nam', 'Hyunwoo Ko', 'Seungwon Lim', 'Jinyeop Song', 'Jinha Choi', 'Gonçalo Paulo', 'Youngjae Yu', 'Stella Biderman'], 'affiliations': ['Boeing Korea', 'EleutherAI', 'KAIST', 'MIT', 'OneLineAI', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11855.jpg', 'data': {'categories': ['#data', '#science', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Большие языковые модели пока не готовы быть научными рецензентами', 'desc': 'Статья представляет SPOT - набор данных из 83 опубликованных научных работ с 91 значительной ошибкой, приведшей к опечаткам или отзыву. Авторы оценивают способность современных больших языковых моделей (LLM) обнаруживать эти ошибки в качестве автоматических верификаторов научных рукописей. Результаты показывают, что даже лучшие модели достигают лишь 21.1% полноты и 6.1% точности, демонстрируя ненадежность и непоследовательность. Качественный анализ выявляет, что ошибки моделей напоминают студенческие заблуждения, указывая на значительный разрыв между текущими возможностями LLM и требованиями к надежной ИИ-assisted верификации научных работ.'}, 'en': {'title': 'Bridging the Gap: LLMs as Verifiers in Scientific Discovery', 'desc': 'This paper investigates the use of large language models (LLMs) as tools for verifying scientific manuscripts, rather than just generating content. The authors introduce a dataset called SPOT, which includes published papers with significant errors that could lead to errata or retraction. They evaluate various state-of-the-art LLMs on this dataset and find that their performance is lacking, with low recall and precision rates. The study concludes that current LLMs are not yet reliable enough for academic verification, as they often make errors similar to those of novice students.'}, 'zh': {'title': '大型语言模型在学术验证中的挑战', 'desc': '这篇论文探讨了大型语言模型（LLMs）在学术验证中的应用，提出了一个名为SPOT的数据集，包含83篇已发表论文和91个显著错误。研究发现，当前最先进的LLMs在识别错误方面的表现不佳，最高召回率仅为21.1%，精确率为6.1%。此外，模型的置信度普遍较低，且在多次独立测试中，模型很少能重新发现相同的错误。通过与领域专家的定性分析，发现即使是最强的模型也会犯类似学生级别的误解错误，显示出当前LLMs在可靠的学术验证中存在显著差距。'}}}, {'id': 'https://huggingface.co/papers/2505.12849', 'title': 'Accelerate TarFlow Sampling with GS-Jacobi Iteration', 'url': 'https://huggingface.co/papers/2505.12849', 'abstract': 'Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow\'s sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is "simple" (converges in few iterations) or "tough" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow', 'score': 7, 'issue_id': 3846, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '191f5a409cc6b32e', 'authors': ['Ben Liu', 'Zhen Qin'], 'affiliations': ['TapTap, Shanghai, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.12849.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#cv', '#training'], 'emoji': '🚀', 'ru': {'title': 'Ускорение генерации изображений в TarFlow с помощью оптимизированных итераций', 'desc': 'Данная статья представляет метод ускорения процесса сэмплирования в модели TarFlow для генерации изображений. Авторы применяют итерационный метод Гаусса-Зейделя-Якоби и вводят две метрики: Convergence Ranking Metric (CRM) и Initial Guessing Metric (IGM). CRM используется для определения сложности блоков TarFlow, а IGM оценивает качество начальных значений для итераций. Эксперименты показали значительное ускорение сэмплирования (до 5.32 раз) без ухудшения качества генерируемых изображений.'}, 'en': {'title': 'Accelerating TarFlow: Faster Sampling without Quality Loss', 'desc': 'This paper presents an optimization strategy for the TarFlow model, which combines transformer architecture with Normalizing Flow for image generation. The authors introduce the Gauss-Seidel-Jacobi (GS-Jacobi) iteration method to accelerate the slow sampling process of TarFlow. They identify that certain blocks within the model are more critical for image generation and propose metrics to evaluate their performance: the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM). Experimental results show that the GS-Jacobi method significantly improves sampling speed while preserving image quality, achieving notable speed-ups across various benchmarks.'}, 'zh': {'title': '加速TarFlow采样，提升图像生成效率', 'desc': '图像生成模型在多个应用中取得了显著进展。TarFlow模型结合了变换器架构和归一化流模型，在多个基准测试中达到了最先进的结果。然而，由于因果注意力的顺序计算，TarFlow的采样过程非常缓慢。本文通过优化策略，利用高斯-赛德尔-雅可比迭代方法显著加速了TarFlow的采样过程，同时保持生成图像的质量。'}}}, {'id': 'https://huggingface.co/papers/2505.13388', 'title': 'R3: Robust Rubric-Agnostic Reward Models', 'url': 'https://huggingface.co/papers/2505.13388', 'abstract': 'Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3', 'score': 6, 'issue_id': 3860, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '2589c2eb838a62f9', 'authors': ['David Anugraha', 'Zilu Tang', 'Lester James V. Miranda', 'Hanyang Zhao', 'Mohammad Rifqi Farhansyah', 'Garry Kuwanto', 'Derry Wijaya', 'Genta Indra Winata'], 'affiliations': ['Allen Institute for AI', 'Boston University', 'Capital One', 'Columbia University', 'Institut Teknologi Bandung', 'Monash Indonesia', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13388.jpg', 'data': {'categories': ['#open_source', '#data', '#optimization', '#benchmark', '#alignment', '#interpretability', '#rlhf'], 'emoji': '🎯', 'ru': {'title': 'R3: Прозрачное и гибкое моделирование наград для языковых моделей', 'desc': 'R3 - это новая система моделирования наград для языковых моделей. Она обеспечивает лучшую интерпретируемость и контролируемость по сравнению с существующими подходами. R3 агностична к конкретным рубрикам и может обобщаться на различные аспекты оценки. Система предоставляет обоснованные оценки, что повышает прозрачность и гибкость при оценке языковых моделей.'}, 'en': {'title': 'R3: A New Era in Reward Modeling for Language Alignment', 'desc': 'This paper presents R3, a new framework for reward modeling that aims to improve the alignment of language model outputs with human preferences. Unlike traditional reward models that focus on narrow objectives, R3 is designed to be rubric-agnostic and generalizable, allowing it to adapt to various evaluation criteria. It also enhances interpretability by providing reasoned score assignments, making it easier to understand how scores are derived. Overall, R3 promotes a more transparent and flexible approach to evaluating language models, ensuring they align better with diverse human values.'}, 'zh': {'title': 'R3：提升语言模型对人类价值的对齐能力', 'desc': '奖励模型在将语言模型的输出与人类偏好对齐中至关重要，但现有方法往往缺乏可控性和可解释性。这些模型通常针对狭窄的目标进行优化，限制了它们在更广泛下游任务中的通用性。此外，它们的标量输出在没有上下文推理的情况下难以解释。为了解决这些限制，我们提出了R3，这是一种新颖的奖励建模框架，具有与评分标准无关、跨评估维度的可推广性，并提供可解释的推理评分分配。'}}}, {'id': 'https://huggingface.co/papers/2505.12058', 'title': 'Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset\n  Generation & Smoke-Tests for Continuous LLM Evaluation', 'url': 'https://huggingface.co/papers/2505.12058', 'abstract': 'Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.', 'score': 6, 'issue_id': 3853, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': 'db0c871b64bd6d89', 'authors': ['Vincent Koc'], 'affiliations': ['Comet ML, Inc. New York, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.12058.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#synthetic', '#open_source', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Молниеносное тестирование языковых моделей для всех', 'desc': 'TQB++ представляет собой легковесный многоязычный набор тестов для быстрой проверки языковых моделей. Он включает в себя золотой стандарт из 52 вопросов на английском языке и генератор синтетических данных для создания тестов на других языках. TQB++ позволяет быстро и эффективно тестировать модели, не требуя значительных вычислительных ресурсов. Этот инструмент особенно полезен для непрерывной интеграции и быстрой оценки качества моделей в процессе разработки.'}, 'en': {'title': 'Quick and Efficient Testing for Language Models', 'desc': 'Tiny QA Benchmark++ (TQB++) is a lightweight, multilingual testing suite designed for large-language-model (LLM) pipelines, providing a quick and cost-effective way to ensure model quality. It includes a small set of gold-standard tests and a synthetic data generator that allows users to create custom test packs in various languages and domains. TQB++ integrates easily with existing tools, enabling developers to incorporate micro-benchmarks into their workflows without significant resource expenditure. This framework aims to enhance continuous quality assurance in generative AI by quickly identifying issues like prompt errors and tokenizer drift before more extensive testing is conducted.'}, 'zh': {'title': '轻量级多语言测试，提升AI质量保障', 'desc': 'Tiny QA Benchmark++（TQB++）是一个超轻量级的多语言测试套件，旨在为大型语言模型（LLM）提供快速的单元测试数据集，运行时间短且成本低。它结合了一个52项的英语金标准集和一个基于LiteLLM的合成数据生成器，允许用户生成适合任何语言、领域或难度的小数据包。TQB++提供了十个现成的数据包，覆盖多种语言，并且每个数据集都附带了元数据和可直接使用的文件，方便集成到现有的开发流程中。这个框架的设计旨在加速生成式人工智能生态系统中的持续、资源高效的质量保证。'}}}, {'id': 'https://huggingface.co/papers/2505.13437', 'title': 'FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance', 'url': 'https://huggingface.co/papers/2505.13437', 'abstract': 'Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as "switch leap with 0.5 turn" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys\'s ability to generate more natural and plausible fine-grained human actions.', 'score': 4, 'issue_id': 3847, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '28a08dbfb09c6639', 'authors': ['Dian Shao', 'Mingfei Shi', 'Shengda Xu', 'Haodong Chen', 'Yongle Huang', 'Binglu Wang'], 'affiliations': ['School of Astronautics, Northwestern Polytechnical University, Xian, China', 'School of Automation, Northwestern Polytechnical University, Xian, China', 'School of Software, Northwestern Polytechnical University, Xian, China', 'Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.13437.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion', '#video'], 'emoji': '🤸', 'ru': {'title': 'Точная генерация движений человека с помощью физического моделирования', 'desc': 'Статья представляет FinePhys - фреймворк для генерации точных движений человека с использованием физических моделей. Система сначала оценивает 2D позы, затем преобразует их в 3D с помощью обучения в контексте. Далее применяется физическое моделирование на основе уравнений Эйлера-Лагранжа для улучшения 3D поз. Полученные физически корректные позы комбинируются с данными для создания многомасштабных 2D тепловых карт, используемых в процессе диффузии.'}, 'en': {'title': 'Bridging Physics and Data for Realistic Human Action Generation', 'desc': 'This paper presents FinePhys, a framework designed to improve the generation of fine-grained human actions in videos by integrating physics-based modeling. It addresses the challenges of synthesizing complex movements, such as gymnastics routines, by first estimating 2D poses and then converting them to 3D using in-context learning. To enhance the stability and interpretability of the generated poses, FinePhys employs a motion re-estimation module based on Euler-Lagrange equations, which calculates joint accelerations through bidirectional temporal updates. The combination of physics-based predictions with data-driven approaches results in more realistic and coherent human actions, as demonstrated by superior performance on benchmark datasets.'}, 'zh': {'title': 'FinePhys：物理驱动的细粒度人类动作生成框架', 'desc': '尽管视频生成技术取得了显著进展，但合成物理上合理的人类动作仍然是一个持续的挑战，尤其是在建模细粒度语义和复杂时间动态方面。本文提出了FinePhys，一个细粒度人类动作生成框架，结合物理学以获得有效的骨骼指导。FinePhys首先以在线方式估计2D姿势，然后通过上下文学习进行2D到3D的维度提升。通过引入基于物理的运动重新估计模块，FinePhys能够生成更自然和合理的细粒度人类动作。'}}}, {'id': 'https://huggingface.co/papers/2505.10238', 'title': 'MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation', 'url': 'https://huggingface.co/papers/2505.10238', 'abstract': 'Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.', 'score': 4, 'issue_id': 3851, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '5d8979fc79f9bd4e', 'authors': ['Yanbo Ding', 'Xirui Hu', 'Zhizhi Guo', 'Yali Wang'], 'affiliations': ['chinatelecom.cn', 'siat.ac.cn', 'xjtu.edu.cn'], 'pdf_title_img': 'assets/pdf/title_img/2505.10238.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': '🕺', 'ru': {'title': 'Революция в анимации человека: от 2D к 4D движению', 'desc': 'MTVCrafter - это новый подход к анимации изображений человека, использующий токенизацию 4D движения вместо 2D-изображений поз. Метод вводит 4DMoT для квантования 3D последовательностей движения в токены и MV-DiT для эффективного использования этих токенов в анимации. MTVCrafter превосходит существующие методы по метрике FID-VID на 65% и хорошо обобщается на разнообразных персонажей в открытом мире. Этот подход открывает новое направление в генерации видео с управлением позой человека.'}, 'en': {'title': 'Revolutionizing Human Animation with 4D Motion Tokens', 'desc': 'This paper presents MTVCrafter, a novel framework for human image animation that utilizes raw 3D motion sequences instead of traditional 2D-rendered pose images. By introducing 4DMoT, the framework quantizes 3D motion into 4D motion tokens, which provide enhanced spatio-temporal cues and greater flexibility in animation control. Additionally, MV-DiT employs motion-aware attention mechanisms to effectively utilize these motion tokens, allowing for more expressive human image generation in complex 3D environments. The results demonstrate that MTVCrafter achieves state-of-the-art performance, significantly improving generalization across various character types and styles.'}, 'zh': {'title': '开创4D运动标记的人像动画新方向', 'desc': '人像动画技术在数字人类应用中越来越受到关注，但现有方法主要依赖于2D渲染的姿态图像，限制了其泛化能力并丢失了重要的3D信息。为了解决这个问题，我们提出了MTVCrafter框架，它直接建模原始的3D运动序列（即4D运动），并引入了4DMoT（4D运动标记器）将3D运动序列量化为4D运动标记。与2D渲染的姿态图像相比，4D运动标记提供了更强的时空线索，避免了姿态图像与角色之间严格的像素级对齐，从而实现了更灵活和解耦的控制。我们的实验表明，MTVCrafter在多种风格和场景下对不同的开放世界角色具有良好的泛化能力，取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2505.12996', 'title': 'ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.12996', 'abstract': 'In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.', 'score': 3, 'issue_id': 3850, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '56ff8af5ab05144f', 'authors': ['Jiaan Wang', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.12996.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#rl', '#low_resource', '#machine_translation'], 'emoji': '🌐', 'ru': {'title': 'Революция в машинном переводе: от одноязычного к многоязычному совершенству', 'desc': 'Статья описывает новый метод моделирования вознаграждения для обучения с подкреплением в нейронном машинном переводе. Авторы используют сравнение результатов перевода с сильной моделью рассуждений (LRM) для формирования вознаграждений. Эксперименты показывают превосходство этого метода, достигая нового уровня качества в литературном переводе. Метод успешно расширен на многоязычный перевод для 11 языков, демонстрируя впечатляющие результаты в 90 направлениях перевода.'}, 'en': {'title': 'Revolutionizing Multilingual Translation with Advanced Reward Modeling', 'desc': 'This paper discusses advancements in large reasoning models (LRMs) for neural machine translation (MT), particularly focusing on improving translation capabilities through reinforcement learning (RL). The authors introduce a novel reward modeling method that evaluates translation quality by comparing outputs from a policy MT model against a powerful LRM, DeepSeek-R1-671B. Their approach not only enhances performance in literary translation but also extends to 11 languages, achieving state-of-the-art results and surpassing existing LRMs. The lightweight reward modeling allows for effective transfer of translation skills across multiple languages, demonstrating significant improvements in multilingual MT performance.'}, 'zh': {'title': '强化学习助力多语言翻译新突破', 'desc': '近年来，大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在复杂问题上展现了出色的能力，尤其是在数学和编程方面。一些开创性研究尝试将LRMs的成功应用于神经机器翻译（MT），并通过强化学习（RL）构建具有深度推理能力的MT模型。尽管取得了一些进展，但这些研究主要集中在高资源语言上，如英语和中文，其他语言的表现仍不明确。此外，我们设计了一种新的奖励建模方法，通过与强大的LRM（如DeepSeek-R1-671B）比较翻译结果，为MT模型提供奖励，从而充分发挥强化学习的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.12120', 'title': 'HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for\n  Computational Pathology', 'url': 'https://huggingface.co/papers/2505.12120', 'abstract': 'Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack sufficient scale, tissue diversity, and comprehensive clinical metadata, limiting the robustness and generalizability of AI models. In response, we introduce the HISTAI dataset, a large, multimodal, open-access WSI collection comprising over 60,000 slides from various tissue types. Each case in the HISTAI dataset is accompanied by extensive clinical metadata, including diagnosis, demographic information, detailed pathological annotations, and standardized diagnostic coding. The dataset aims to fill gaps identified in existing resources, promoting innovation, reproducibility, and the development of clinically relevant computational pathology solutions. The dataset can be accessed at https://github.com/HistAI/HISTAI.', 'score': 3, 'issue_id': 3855, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '4774ecdbd93d4a7b', 'authors': ['Dmitry Nechaev', 'Alexey Pchelnikov', 'Ekaterina Ivanova'], 'affiliations': ['HistAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.12120.jpg', 'data': {'categories': ['#multimodal', '#healthcare', '#dataset', '#data'], 'emoji': '🔬', 'ru': {'title': 'HISTAI: Крупномасштабный набор данных для прорыва в цифровой патологии', 'desc': 'Статья представляет новый набор данных HISTAI для цифровой патологии. Он содержит более 60 000 изображений срезов тканей различных типов с подробными клиническими метаданными. Каждый случай сопровождается диагнозом, демографической информацией, патологическими аннотациями и стандартизированным диагностическим кодированием. Набор данных HISTAI призван устранить пробелы в существующих ресурсах и способствовать разработке клинически значимых решений в области вычислительной патологии.'}, 'en': {'title': 'Empowering AI in Pathology with the HISTAI Dataset', 'desc': "This paper presents the HISTAI dataset, a significant advancement in Digital Pathology that addresses the limitations of existing Whole Slide Image (WSI) datasets. It comprises over 60,000 slides from diverse tissue types, ensuring a large-scale and multimodal resource for training AI models. Each slide is enriched with comprehensive clinical metadata, including diagnosis and detailed annotations, which enhances the dataset's utility for research and clinical applications. By providing this open-access resource, the authors aim to foster innovation and improve the robustness of AI solutions in computational pathology."}, 'zh': {'title': 'HISTAI数据集：推动数字病理学的创新与发展', 'desc': '本论文介绍了HISTAI数据集，这是一个大型的多模态开放获取的全幻灯片图像（WSI）集合，包含超过60,000张来自不同组织类型的幻灯片。该数据集配备了丰富的临床元数据，包括诊断、人口统计信息、详细的病理注释和标准化的诊断编码。HISTAI数据集旨在填补现有资源中的空白，促进创新、可重复性以及临床相关计算病理解决方案的发展。通过提供多样化和丰富注释的数据，HISTAI将增强人工智能模型的鲁棒性和普适性。'}}}, {'id': 'https://huggingface.co/papers/2505.11497', 'title': 'QVGen: Pushing the Limit of Quantized Video Generative Models', 'url': 'https://huggingface.co/papers/2505.11497', 'abstract': 'Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench.', 'score': 3, 'issue_id': 3851, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '48866ba1d6ad838b', 'authors': ['Yushi Huang', 'Ruihao Gong', 'Jing Liu', 'Yifu Ding', 'Chengtao Lv', 'Haotong Qin', 'Jun Zhang'], 'affiliations': ['Beihang University', 'ETH Zürich', 'Hong Kong University of Science and Technology', 'Monash University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11497.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#inference', '#video'], 'emoji': '🎬', 'ru': {'title': 'Революция в квантовании видео-диффузионных моделей', 'desc': 'QVGen - это новая система обучения с учетом квантования для высокопроизводительных и эффективных при выводе видео-диффузионных моделей при экстремально низкобитном квантовании. Система вводит вспомогательные модули для уменьшения ошибок квантования и улучшения сходимости. Используется стратегия уменьшения ранга для устранения накладных расходов при выводе. Эксперименты показывают, что QVGen достигает качества полной точности при 4-битных настройках и значительно превосходит существующие методы.'}, 'en': {'title': 'Efficient Video Synthesis with Low-Bit Quantization', 'desc': 'This paper introduces QVGen, a new framework for quantization-aware training (QAT) specifically designed for video diffusion models (DMs). It addresses the challenges of high computational and memory requirements by enabling efficient video synthesis at extremely low-bit quantization levels, such as 4-bit. The authors demonstrate that reducing the gradient norm is crucial for effective convergence in QAT and propose auxiliary modules to minimize quantization errors. Additionally, they implement a rank-decay strategy to eliminate inference overhead while maintaining performance, achieving significant improvements over existing methods in video quality metrics.'}, 'zh': {'title': '量化感知训练，提升视频合成效率！', 'desc': '视频扩散模型（DMs）在高质量视频合成方面取得了显著进展，但其计算和内存需求高，限制了实际应用。本文提出了一种新颖的量化感知训练（QAT）框架QVGen，旨在在极低位量化（如4位或更低）下实现高性能和高效推理。我们通过理论分析表明，降低梯度范数对QAT的收敛至关重要，并引入辅助模块（Phi）来减小量化误差，从而显著提高收敛性。通过逐步消除Phi的推理开销，我们的实验表明QVGen在4位设置下首次实现了与全精度相当的质量，并显著超越了现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.11484', 'title': 'SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2505.11484', 'abstract': "Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.", 'score': 3, 'issue_id': 3851, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '4a502c14d07f6e6c', 'authors': ['Yige Xu', 'Xu Guo', 'Zhiwei Zeng', 'Chunyan Miao'], 'affiliations': ['Alibaba-NTU Global e-Sustainability CorpLab (ANGEL)', 'College of Computing and Data Science, Nanyang Technological University, Singapore', 'Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly', 'KTH Royal Institute of Technology, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2505.11484.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений ИИ через разнообразные латентные мысли', 'desc': 'Статья представляет метод SoftCoT++, улучшающий рассуждения языковых моделей в непрерывном латентном пространстве. В отличие от дискретных методов, SoftCoT++ позволяет исследовать разнообразные пути рассуждений путем возмущения латентных представлений. Метод применяет контрастивное обучение для повышения разнообразия мягких представлений мыслей. Эксперименты показывают, что SoftCoT++ превосходит базовый SoftCoT и хорошо сочетается с другими методами масштабирования.'}, 'en': {'title': 'Enhancing Reasoning with Diverse Latent Thoughts', 'desc': 'This paper introduces SoftCoT++, an advanced method for Test-Time Scaling (TTS) that enhances reasoning performance in machine learning models. Unlike traditional TTS methods that work with discrete tokens, SoftCoT++ leverages continuous latent space to improve the quality of reasoning by allowing for diverse exploration of thought paths. By perturbing latent thoughts with specialized initial tokens and using contrastive learning, the method promotes diversity in soft thought representations. Experimental results show that SoftCoT++ significantly outperforms previous methods, demonstrating its effectiveness across various reasoning benchmarks and compatibility with existing scaling techniques.'}, 'zh': {'title': '多样化思维路径的探索新方法', 'desc': '测试时扩展（TTS）是一种在推理过程中通过分配额外计算来提高推理性能的方法，而不改变模型参数。最近的研究表明，在连续潜在空间中进行思考可以进一步提升推理性能。与离散解码不同，连续空间中的潜在表示是固定的，这限制了多样化的探索。为了解决这个问题，我们提出了SoftCoT++，通过扰动潜在思维并应用对比学习来促进思维路径的多样性，从而扩展了SoftCoT在测试时扩展范式中的应用。'}}}, {'id': 'https://huggingface.co/papers/2505.12872', 'title': 'From Grunts to Grammar: Emergent Language from Cooperative Foraging', 'url': 'https://huggingface.co/papers/2505.12872', 'abstract': 'Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains a challenge. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly.', 'score': 2, 'issue_id': 3853, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'cb62284e65b8c471', 'authors': ['Maytus Piriyajitakonkij', 'Rujikorn Charakorn', 'Weicheng Tao', 'Wei Pan', 'Mingfei Sun', 'Cheston Tan', 'Mengmi Zhang'], 'affiliations': ['Centre for Frontier AI Research (CFAR), ASTAR, Singapore', 'College of Computing and Data Science, Nanyang Technological University, Singapore', 'Department of Computer Science, The University of Manchester, United Kingdom', 'Institute for Infocomm Research (I2R), ASTAR, Singapore', 'Sakana AI, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.12872.jpg', 'data': {'categories': ['#rl', '#rlhf', '#agents', '#reasoning', '#open_source', '#multimodal', '#games'], 'emoji': '🗣️', 'ru': {'title': 'Эволюция языка через призму искусственного интеллекта', 'desc': 'Статья исследует эволюцию языка в контексте многоагентных игр по добыче ресурсов, используя глубокое обучение с подкреплением. Авторы моделируют среду, отражающую когнитивные и экологические ограничения, повлиявшие на развитие коммуникации у ранних людей. Результаты показывают, что агенты развивают протоколы общения с ключевыми свойствами естественного языка: произвольность, взаимозаменяемость, смещение, культурную передачу и композиционность. Исследование предлагает платформу для изучения эволюции языка в условиях частичной наблюдаемости, временных зависимостей и кооперативных целей в многоагентных средах.'}, 'en': {'title': 'Evolving Language Through Cooperative Learning in Multi-Agent Systems', 'desc': 'This paper explores how language can emerge in multi-agent systems through the lens of Foraging Games, which simulate the ecological and social conditions of early human cooperation. Using end-to-end deep reinforcement learning, agents learn to communicate and coordinate their actions in a shared environment with limited information. The study reveals that these agents develop communication protocols that exhibit characteristics similar to natural language, such as arbitrariness and compositionality. By analyzing how factors like population size and temporal dependencies influence language features, the research provides insights into the evolutionary processes of communication.'}, 'zh': {'title': '语言的演化：从合作到沟通的旅程', 'desc': '这篇论文探讨了语言如何在多智能体觅食游戏中出现。研究表明，智能体在共享的环境中，通过深度强化学习学习行动和沟通策略，逐渐发展出具有自然语言特征的沟通协议。论文量化了语言的不同特性，如任意性、可互换性和文化传递等，并分析了人口规模和时间依赖性等因素如何影响语言的演变。该框架为研究语言如何在合作目标和部分可观察性中演化提供了平台。'}}}, {'id': 'https://huggingface.co/papers/2505.11733', 'title': 'MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports', 'url': 'https://huggingface.co/papers/2505.11733', 'abstract': 'Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.', 'score': 2, 'issue_id': 3864, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '0eaea70de66597a0', 'authors': ['Kevin Wu', 'Eric Wu', 'Rahul Thapa', 'Kevin Wei', 'Angela Zhang', 'Arvind Suresh', 'Jacqueline J. Tao', 'Min Woo Sun', 'Alejandro Lozano', 'James Zou'], 'affiliations': ['Stanford University', 'University of California, San Francisco', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.11733.jpg', 'data': {'categories': ['#open_source', '#dataset', '#training', '#benchmark', '#alignment', '#data', '#healthcare', '#reasoning'], 'emoji': '🩺', 'ru': {'title': 'Новый стандарт оценки ИИ в медицинской диагностике', 'desc': 'Представлен новый датасет MedCaseReasoning для оценки способности языковых моделей (LLM) соответствовать клиническому мышлению врачей при постановке диагнозов. Датасет содержит более 14 тысяч диагностических случаев с подробными рассуждениями, основанными на медицинских отчетах. Тестирование современных LLM на этом датасете выявило значительные недостатки в их диагностике и рассуждениях. Однако дообучение моделей на данных MedCaseReasoning значительно улучшило точность диагностики и полноту клинических рассуждений.'}, 'en': {'title': 'Enhancing Medical Diagnosis with Reasoning Evaluation', 'desc': 'This paper addresses the limitations of current medical benchmarks that only evaluate the final accuracy of Large Language Models (LLMs) in clinical diagnosis. It introduces MedCaseReasoning, a new dataset designed to assess both the accuracy of diagnoses and the quality of reasoning behind them, featuring 14,489 cases with clinician-authored explanations. The authors evaluate existing LLMs and find that they struggle with both diagnostic accuracy and recalling reasoning statements. By fine-tuning these models on the new dataset, they achieve significant improvements in both diagnostic accuracy and reasoning recall, demonstrating the importance of evaluating the reasoning process in medical AI applications.'}, 'zh': {'title': '提升医学诊断的推理能力', 'desc': '本研究提出了MedCaseReasoning数据集，用于评估大型语言模型（LLMs）在医学诊断中的推理能力。与传统的医学基准不同，该数据集不仅关注最终答案的准确性，还重视临床推理过程的质量和可信度。我们发现当前最先进的LLMs在诊断和推理方面存在显著不足，尤其是在准确性和推理回忆率上。通过对LLMs进行微调，使用MedCaseReasoning的数据，诊断准确性和推理回忆率分别提高了29%和41%。'}}}, {'id': 'https://huggingface.co/papers/2505.11475', 'title': 'HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages', 'url': 'https://huggingface.co/papers/2505.11475', 'abstract': 'Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference', 'score': 2, 'issue_id': 3854, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '82282826ff02b787', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Hoo-Chang Shin', 'Felipe Soares', 'Alexander Bukharin', 'Ellie Evans', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.11475.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#multilingual', '#open_source', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'HelpSteer3-Preference: новый стандарт данных для RLHF', 'desc': 'Статья представляет HelpSteer3-Preference - высококачественный набор данных предпочтений для обучения языковых моделей с помощью обучения с подкреплением на основе обратной связи от человека (RLHF). Набор содержит более 40 000 образцов, охватывающих различные реальные применения больших языковых моделей (LLM), включая задачи в области STEM, программирования и многоязычные сценарии. Используя HelpSteer3-Preference, авторы обучили модели вознаграждения (Reward Models), достигшие наилучших результатов на бенчмарках RM-Bench и JudgeBench. Также показано применение набора данных для обучения генеративных моделей вознаграждения и настройки политик с помощью RLHF.'}, 'en': {'title': 'Enhancing Language Models with High-Quality Preference Data', 'desc': 'This paper presents HelpSteer3-Preference, a new dataset designed to improve the training of language models using Reinforcement Learning from Human Feedback (RLHF). It contains over 40,000 high-quality, human-annotated preference samples that cover a wide range of real-world applications, including STEM and coding tasks. The dataset has been shown to significantly enhance the performance of Reward Models (RMs), achieving top scores on benchmark tests. Additionally, the paper discusses how this dataset can be utilized to train Generative RMs and align policy models with RLHF techniques.'}, 'zh': {'title': '提升语言模型的偏好数据集', 'desc': '本论文介绍了HelpSteer3-Preference，这是一个高质量的人类标注偏好数据集，包含超过40,000个样本，旨在提升通用领域的语言模型训练。该数据集涵盖了多种真实世界应用，包括STEM、编程和多语言场景，具有多样性和高质量。通过使用HelpSteer3-Preference，我们训练的奖励模型在RM-Bench和JudgeBench上取得了优异的表现，显著提高了之前模型的性能。此外，我们还展示了如何利用该数据集训练生成性奖励模型，并将策略模型与人类反馈强化学习对齐。'}}}, {'id': 'https://huggingface.co/papers/2505.12781', 'title': 'A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation\n  through Low-Rank Clone', 'url': 'https://huggingface.co/papers/2505.12781', 'abstract': 'Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.', 'score': 1, 'issue_id': 3860, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '1ba09e10a601b724', 'authors': ['Jitai Hao', 'Qiang Huang', 'Hao Liu', 'Xinyan Xiao', 'Zhaochun Ren', 'Jun Yu'], 'affiliations': ['Baidu Inc.', 'Harbin Institute of Technology, Shenzhen', 'Leiden University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12781.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#training', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение малых языковых моделей с помощью низкоранговых проекций', 'desc': 'Статья представляет новый метод предобучения малых языковых моделей (SLM) под названием Low-Rank Clone (LRC). LRC использует низкоранговые проекционные матрицы для эффективного сжатия весов учителя и клонирования активаций, включая сигналы из полносвязных слоев. Этот подход позволяет достичь поведенческой эквивалентности с сильными моделями-учителями, максимизируя передачу знаний. Эксперименты показывают, что LRC достигает или превосходит современные модели, обученные на триллионах токенов, используя только 20 миллиардов токенов, что повышает эффективность обучения более чем в 1000 раз.'}, 'en': {'title': 'Efficient Training of Small Language Models with Low-Rank Clone', 'desc': 'This paper presents a new method called Low-Rank Clone (LRC) for training Small Language Models (SLMs) efficiently. LRC addresses three main challenges in model training: reducing information loss from hard pruning, improving the alignment of representations, and better utilizing activations from Feed-Forward Networks. By using low-rank projection matrices, LRC allows for soft pruning and aligns student activations with those of a teacher model, enhancing knowledge transfer. The results show that LRC achieves high performance comparable to larger models while significantly reducing the amount of training data needed.'}, 'zh': {'title': '低秩克隆：高效训练小型语言模型的创新方法', 'desc': '本文提出了一种名为低秩克隆（Low-Rank Clone, LRC）的高效预训练方法，旨在解决训练小型语言模型（SLMs）时面临的信息损失、表示对齐效率低和激活利用不足等挑战。LRC通过构建低秩投影矩阵，实现了教师模型权重的软修剪和学生模型激活的对齐，特别是前馈网络（FFN）的信号。该方法最大化了知识转移的效率，消除了对显式对齐模块的需求。实验结果表明，LRC在使用仅20B标记的情况下，能够匹配或超越在万亿标记上训练的最先进模型，达到了超过1000倍的训练效率。'}}}, {'id': 'https://huggingface.co/papers/2505.12257', 'title': 'LLM Context Conditioning and PWP Prompting for Multimodal Validation of\n  Chemical Formulas', 'url': 'https://huggingface.co/papers/2505.12257', 'abstract': "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.", 'score': 1, 'issue_id': 3849, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': '74901d316cc1d6cb', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2505.12257.jpg', 'data': {'categories': ['#science', '#multimodal', '#interpretability', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Повышение точности LLM в обнаружении ошибок через структурированную настройку контекста', 'desc': 'Это исследование изучает структурированный подход к настройке контекста больших языковых моделей (LLM) для улучшения их способности выявлять технические ошибки в сложных научных документах. Методология основана на принципах устойчивого рабочего процесса запросов (PWP) и направлена на повышение надежности общедоступных LLM для задач точной валидации. Эксперименты показали, что адаптированный PWP-подход улучшил идентификацию текстовых ошибок и даже позволил модели Gemini 2.5 Pro обнаружить скрытую ошибку в формуле на изображении. Результаты указывают на потенциал этого метода для разработки более надежных аналитических рабочих процессов на основе LLM, особенно для задач, требующих тщательного обнаружения ошибок в научно-технических документах.'}, 'en': {'title': 'Enhancing LLM Accuracy in Scientific Error Detection with PWP', 'desc': 'This paper explores how to improve Large Language Models (LLMs) in identifying subtle errors in complex scientific documents, especially those with images and formulas. It introduces a method called Persistent Workflow Prompting (PWP) to better condition LLMs during their analysis, enhancing their ability to detect inaccuracies. The study specifically tests this approach on Gemini 2.5 Pro and ChatGPT Plus o3, showing that PWP can significantly improve error detection compared to basic prompting strategies. The findings suggest that this method could lead to more reliable LLMs for validating technical content, although further research is needed to confirm its effectiveness across different contexts.'}, 'zh': {'title': '提升LLM在科学文档中的错误识别能力', 'desc': '本研究探讨了如何利用结构化的上下文条件来改善大型语言模型（LLMs）在复杂科学和技术文档中的错误识别能力。研究采用了持久工作提示（PWP）原则，旨在提高LLMs在推理时的表现，尤其是在验证化学公式时。通过对不同提示策略的评估，发现适应PWP结构的提示能够有效提高文本错误的识别率，并成功发现了之前未被手动审查识别的图像公式错误。该方法为开发更强大的LLM驱动分析工作流提供了有希望的技术，尤其是在需要细致错误检测的任务中。'}}}, {'id': 'https://huggingface.co/papers/2505.11988', 'title': 'TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text', 'url': 'https://huggingface.co/papers/2505.11988', 'abstract': 'Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.', 'score': 1, 'issue_id': 3850, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 мая', 'en': 'May 17', 'zh': '5月17日'}, 'hash': '95b404534e69c826', 'authors': ['Ahmed Lekssays', 'Utsav Shukla', 'Husrev Taha Sencar', 'Md Rizwan Parvez'], 'affiliations': ['Independent Researcher', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2505.11988.jpg', 'data': {'categories': ['#security', '#data', '#hallucinations', '#rag', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Точное распознавание техник злоумышленников с минимумом данных', 'desc': 'TechniqueRAG - это новая система для идентификации техник противников в текстах по кибербезопасности. Она использует извлечение информации и генеративные языковые модели, обученные на небольшом количестве примеров. TechniqueRAG применяет переранжирование с помощью LLM для улучшения релевантности извлеченной информации. Эксперименты показывают, что система достигает наилучших результатов без необходимости в больших размеченных датасетах.'}, 'en': {'title': 'Enhancing Cyber Defense with TechniqueRAG: Precision without Excessive Resources', 'desc': 'This paper introduces TechniqueRAG, a novel framework designed to improve the identification of adversarial techniques in cybersecurity texts. It combines retrieval-augmented generation (RAG) with instruction-tuned large language models (LLMs) to enhance domain specificity while minimizing the need for extensive labeled datasets. By fine-tuning only the generation component and employing zero-shot LLM re-ranking, TechniqueRAG improves the quality of retrieved candidates, ensuring they are more relevant to the specific domain of adversarial techniques. The results show that TechniqueRAG outperforms existing methods, achieving high accuracy without the heavy resource demands typically associated with task-specific optimizations.'}, 'zh': {'title': '提升安全文本对抗技术识别的创新框架', 'desc': '本文提出了一种名为TechniqueRAG的框架，旨在提高对安全文本中对抗性技术的识别能力。该框架结合了现成的检索器、经过指令调优的大型语言模型（LLM）和最小的文本-技术对，解决了数据稀缺的问题。通过仅对生成组件进行微调，TechniqueRAG避免了对资源密集型检索训练的依赖，同时通过零-shot LLM重排序提高了检索质量和领域特异性。实验结果表明，TechniqueRAG在多个安全基准上实现了最先进的性能，无需大量特定任务的优化或标记数据。'}}}, {'id': 'https://huggingface.co/papers/2505.10420', 'title': 'Learned Lightweight Smartphone ISP with Unpaired Data', 'url': 'https://huggingface.co/papers/2505.10420', 'abstract': 'The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .', 'score': 1, 'issue_id': 3855, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '0ba36884ee806c6f', 'authors': ['Andrei Arhire', 'Radu Timofte'], 'affiliations': ['Computer Vision Lab, CAIDAS & IFI University of Wurzburg', 'Faculty of Computer Science Alexandru Ioan Cuza University of Iasi'], 'pdf_title_img': 'assets/pdf/title_img/2505.10420.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': '📱', 'ru': {'title': 'Умная обработка фото без парных данных', 'desc': 'Статья описывает новый метод обучения Image Signal Processor (ISP) для смартфонов без использования парных данных. Авторы предлагают подход с несколькими дискриминаторами и предобученными сетями для сохранения структуры контента при обучении цвету и текстуре. Метод был протестирован на наборах данных Zurich RAW to RGB и Fujifilm UltraISP с использованием легких нейронных архитектур. Результаты показывают высокую эффективность предложенного подхода по сравнению с методами, требующими парных данных.'}, 'en': {'title': 'Revolutionizing Smartphone Imaging with Unpaired Learning!', 'desc': 'This paper presents a new method for training an Image Signal Processor (ISP) using unpaired data, which means it does not require exact matches between raw images and high-quality reference images. The authors utilize a multi-term loss function and adversarial training with multiple discriminators to effectively learn color and texture characteristics while preserving content structure. Their approach is designed for lightweight neural networks, making it suitable for mobile devices. The results demonstrate that this unpaired learning strategy outperforms traditional paired methods in terms of image quality and fidelity.'}, 'zh': {'title': '无配对数据的学习型图像信号处理器', 'desc': '本文提出了一种新颖的训练方法，用于学习图像信号处理器（ISP），旨在解决传统方法中需要成对的原始图像和高质量参考图像的问题。我们的方法采用无配对的学习策略，通过对抗训练和多重判别器来保持内容结构，同时学习目标RGB数据集的颜色和纹理特征。使用适合移动设备的轻量级神经网络架构，我们在多个数据集上评估了该方法，结果显示出与成对训练方法相比，具有更高的保真度和潜力。代码和预训练模型可在指定链接获取。'}}}, {'id': 'https://huggingface.co/papers/2505.03332', 'title': 'AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning', 'url': 'https://huggingface.co/papers/2505.03332', 'abstract': 'Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.', 'score': 1, 'issue_id': 3849, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '9c52936c2b9a7443', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2505.03332.jpg', 'data': {'categories': ['#reasoning', '#data', '#science', '#multimodal', '#interpretability', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'PWP: Новый подход к критическому анализу научных работ с помощью LLM', 'desc': 'Статья представляет новый метод инженерии промптов под названием Persistent Workflow Prompting (PWP) для улучшения способности больших языковых моделей (LLM) проводить критический анализ научных рукописей. PWP использует иерархическую модульную архитектуру для определения детальных рабочих процессов анализа, которые сохраняются в течение сессии. Метод был разработан с помощью итеративного применения мета-промптинга и мета-рассуждений для систематической кодификации экспертных процессов рецензирования. Демонстрации показывают, что PWP-управляемая LLM способна выявлять серьезные методологические недостатки и выполнять сложные задачи анализа.'}, 'en': {'title': 'Empowering LLMs for Expert Scientific Review with Persistent Workflow Prompting', 'desc': 'This paper addresses the challenges faced by Large Language Models (LLMs) in performing critical peer reviews of scientific manuscripts, particularly due to data limitations and the intricacies of expert reasoning. It introduces a new methodology called Persistent Workflow Prompting (PWP), which allows users to create structured prompts that guide LLMs through detailed analysis workflows without needing coding skills. The PWP framework is designed to help LLMs systematically evaluate scientific content by integrating various forms of data, such as text and images, to identify flaws and assess claims. The authors demonstrate the effectiveness of PWP in analyzing experimental chemistry manuscripts, showcasing its ability to enhance LLM performance in complex scientific evaluations.'}, 'zh': {'title': '持久工作流提示：提升科学评审的智能化', 'desc': '这篇论文介绍了一种新的提示工程方法，称为持久工作流提示（PWP），旨在帮助大型语言模型（LLMs）进行科学手稿的批判性同行评审。PWP通过标准的LLM聊天界面，使用分层模块化的架构，定义详细的分析工作流程，能够系统化地编码专家评审的工作流程。通过迭代的元提示技术和元推理，PWP能够引导LLM进行多模态评估，识别实验化学手稿中的主要方法论缺陷。该方法不仅提供了具体应用的示例，还展示了如何利用现有的LLM进行复杂科学任务的深入分析。'}}}, {'id': 'https://huggingface.co/papers/2505.12973', 'title': 'Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models', 'url': 'https://huggingface.co/papers/2505.12973', 'abstract': 'Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.', 'score': 0, 'issue_id': 3854, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'e144d477e8eb1cab', 'authors': ['Mahta Fetrat Qharabagh', 'Zahra Dehghanian', 'Hamid R. Rabiee'], 'affiliations': ['Dep. of Computer Engineering, Sharif University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.12973.jpg', 'data': {'categories': ['#data', '#healthcare', '#low_resource', '#dataset'], 'emoji': '🗣️', 'ru': {'title': 'Эффективное разрешение омографов для G2P конверсии в малоресурсных языках', 'desc': 'Статья посвящена проблеме разрешения омографов в задаче преобразования графем в фонемы (G2P) для малоресурсных языков. Авторы предлагают полуавтоматический метод создания датасетов с омографами и представляют датасет HomoRich. Они демонстрируют эффективность подхода, применяя его для улучшения современной системы G2P на основе глубокого обучения для персидского языка. Кроме того, авторы разрабатывают быструю версию известной системы eSpeak с поддержкой омографов - HomoFast eSpeak, показывая улучшение точности разрешения омографов примерно на 30%.'}, 'en': {'title': 'Enhancing G2P with Efficient Homograph Disambiguation', 'desc': 'This paper tackles the problem of homograph disambiguation in grapheme-to-phoneme (G2P) conversion, particularly for low-resource languages. It introduces a semi-automated method to create homograph datasets, exemplified by the new HomoRich dataset, which enhances a deep learning G2P system for Persian. Additionally, the authors propose a shift towards using comprehensive offline datasets to develop efficient, rule-based G2P methods that are suitable for real-time applications. The improved eSpeak system, named HomoFast eSpeak, demonstrates a significant increase in disambiguation accuracy, making it more effective for accessibility tools.'}, 'zh': {'title': '提升同形异义词消歧的智能解决方案', 'desc': '同形异义词消歧在图形到音素（G2P）转换中仍然是一个重要挑战，尤其是在资源匮乏的语言中。本文提出了一种半自动化的流程来构建同形异义词数据集，并生成了HomoRich数据集，以增强波斯语的深度学习G2P系统。我们还倡导利用丰富的离线数据集来开发适合实时应用的快速规则基础方法。通过改进著名的规则基础G2P系统eSpeak，我们的HomoFast eSpeak版本在同形异义词消歧准确性上提高了约30%。'}}}, {'id': 'https://huggingface.co/papers/2505.10831', 'title': 'Creating General User Models from Computer Use', 'url': 'https://huggingface.co/papers/2505.10831', 'abstract': "Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.", 'score': 0, 'issue_id': 3858, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'b70f4a83c98a9326', 'authors': ['Omar Shaikh', 'Shardul Sapkota', 'Shan Rizvi', 'Eric Horvitz', 'Joon Sung Park', 'Diyi Yang', 'Michael S. Bernstein'], 'affiliations': ['Independent', 'Microsoft Research', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10831.jpg', 'data': {'categories': ['#interpretability', '#agents', '#multimodal', '#agi', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'GUM: универсальная модель пользователя для создания интуитивных интерфейсов будущего', 'desc': 'Статья представляет архитектуру общей модели пользователя (GUM), которая учится понимать человека, наблюдая за его взаимодействием с компьютером. GUM анализирует неструктурированные данные и создает взвешенные предположения о знаниях и предпочтениях пользователя. Эта модель может делать выводы о контексте действий пользователя, например, о подготовке к свадьбе или трудностях при работе над текстом. GUM позволяет создавать более умные чат-ассистенты, управлять уведомлениями ОС и разрабатывать интерактивных агентов, адаптирующихся к предпочтениям пользователя в различных приложениях.'}, 'en': {'title': 'Empowering Technology to Anticipate User Needs', 'desc': 'This paper introduces a General User Model (GUM) that learns about users by observing their interactions with computers, using unstructured data like device screenshots. GUMs create confidence-weighted propositions to capture user preferences and behaviors, allowing for flexible reasoning about user needs. The architecture can infer context from multimodal observations, enabling proactive assistants that can suggest actions without explicit user requests. Overall, GUMs enhance human-computer interaction by providing a more comprehensive understanding of user behavior and preferences across different applications.'}, 'zh': {'title': '通用用户模型：智能人机交互的新未来', 'desc': '本文提出了一种通用用户模型（GUM），旨在通过观察用户与计算机的互动来学习用户的偏好和习惯。GUM能够处理非结构化的用户观察数据（如设备截图），并构建出反映用户知识和偏好的置信加权命题。通过多模态观察，GUM可以推断用户的需求，并根据上下文检索相关命题，持续修正已有命题。研究表明，基于GUM的助手能够主动识别用户需求并执行有用的建议，从而实现更智能的人机交互。'}}}, {'id': 'https://huggingface.co/papers/2505.04921', 'title': 'Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2505.04921', 'abstract': "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.", 'score': 102, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'a07d92b81581eea3', 'authors': ['Yunxin Li', 'Zhenyu Liu', 'Zitao Li', 'Xuanyu Zhang', 'Zhenran Xu', 'Xinyu Chen', 'Haoyuan Shi', 'Shenyuan Jiang', 'Xintong Wang', 'Jifang Wang', 'Shouzheng Huang', 'Xinping Zhao', 'Borui Jiang', 'Lanqing Hong', 'Longyue Wang', 'Zhuotao Tian', 'Baoxing Huai', 'Wenhan Luo', 'Weihua Luo', 'Zheng Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.04921.jpg', 'data': {'categories': ['#rl', '#multimodal', '#benchmark', '#survey', '#agents', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'От модульных систем к унифицированным мультимодальным моделям рассуждений', 'desc': 'Статья представляет обзор исследований в области мультимодальных моделей рассуждений (LMRMs). Авторы описывают эволюцию подходов от модульных систем к унифицированным языковым фреймворкам, обеспечивающим более согласованное межмодальное понимание. Рассматриваются достижения, такие как мультимодальная цепочка рассуждений (MCoT) и мультимодальное обучение с подкреплением. Статья завершается обсуждением концепции нативных больших мультимодальных моделей рассуждений (N-LMRMs) для масштабируемого и адаптивного рассуждения в сложных реальных средах.'}, 'en': {'title': 'Empowering AI with Multimodal Reasoning for Real-World Intelligence', 'desc': 'This paper discusses the importance of reasoning in artificial intelligence, especially in complex environments with multiple types of data like text, images, and audio. It introduces Large Multimodal Reasoning Models (LMRMs) that combine these different data types to enhance reasoning capabilities. The authors provide a structured survey of the evolution of multimodal reasoning, from early task-specific models to more integrated language-centric frameworks. They also highlight ongoing challenges in generalization and reasoning depth, while proposing a roadmap for future research in developing more adaptive and intelligent systems.'}, 'zh': {'title': '多模态推理：智能的未来', 'desc': '推理是智能的核心，影响决策、结论和跨领域的概括能力。在人工智能中，随着系统在开放、不确定和多模态环境中运行，推理变得至关重要，以实现稳健和适应性的行为。大型多模态推理模型（LMRMs）整合文本、图像、音频和视频等多种模态，支持复杂的推理能力，旨在实现全面感知、精确理解和深度推理。本文对多模态推理研究进行了全面的调查，提出了一个四阶段的发展路线图，反映了该领域设计理念的变化和新兴能力。'}}}, {'id': 'https://huggingface.co/papers/2505.04620', 'title': 'On Path to Multimodal Generalist: General-Level and General-Bench', 'url': 'https://huggingface.co/papers/2505.04620', 'abstract': 'The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/', 'score': 62, 'issue_id': 3671, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '57991e528141671e', 'authors': ['Hao Fei', 'Yuan Zhou', 'Juncheng Li', 'Xiangtai Li', 'Qingshan Xu', 'Bobo Li', 'Shengqiong Wu', 'Yaoting Wang', 'Junbao Zhou', 'Jiahao Meng', 'Qingyu Shi', 'Zhiyuan Zhou', 'Liangtao Shi', 'Minghe Gao', 'Daoan Zhang', 'Zhiqi Ge', 'Weiming Wu', 'Siliang Tang', 'Kaihang Pan', 'Yaobo Ye', 'Haobo Yuan', 'Tao Zhang', 'Tianjie Ju', 'Zixiang Meng', 'Shilin Xu', 'Liyu Jia', 'Wentao Hu', 'Meng Luo', 'Jiebo Luo', 'Tat-Seng Chua', 'Shuicheng Yan', 'Hanwang Zhang'], 'affiliations': ['HFUT', 'KAUST', 'NJU', 'NTU', 'NUS', 'PKU', 'SJTU', 'UR', 'WHU', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2505.04620.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agi'], 'emoji': '🧠', 'ru': {'title': 'Новый подход к оценке мультимодальных ИИ-систем на пути к AGI', 'desc': 'Статья описывает новую систему оценки мультимодальных больших языковых моделей (MLLM) под названием General-Level. Эта система определяет 5 уровней производительности и обобщаемости MLLM, предлагая методологию для сравнения моделей и оценки прогресса существующих систем. Ключевым понятием в этой системе является концепция Синергии, которая измеряет согласованность возможностей моделей в понимании и генерации контента, а также в работе с различными модальностями. Для поддержки этой системы оценки авторы представляют General-Bench - набор из более чем 700 задач и 325 800 примеров, охватывающий широкий спектр навыков, модальностей и форматов.'}, 'en': {'title': 'Towards Multimodal Generalists: Evaluating MLLM Progress', 'desc': 'The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI).'}, 'zh': {'title': '迈向真正的多模态通用人工智能', 'desc': '多模态大型语言模型（MLLM）正在快速发展，得益于大型语言模型（LLM）的先进能力。现有的MLLM正朝着多模态通用主义者的方向演变，不仅能够理解多种模态，还能在不同模态之间生成内容。本文提出了一种新的评估框架——General-Level，定义了MLLM性能和通用性的五个等级，以便比较不同模型的能力。通过General-Bench，我们提供了一个更广泛的技能和任务评估，揭示了当前多模态通用模型在实现真正人工智能方面的挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.05470', 'title': 'Flow-GRPO: Training Flow Matching Models via Online RL', 'url': 'https://huggingface.co/papers/2505.05470', 'abstract': "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.", 'score': 44, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '8db85b6df75e7479', 'authors': ['Jie Liu', 'Gongye Liu', 'Jiajun Liang', 'Yangguang Li', 'Jiaheng Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wanli Ouyang'], 'affiliations': ['CUHK MMLab', 'Kuaishou Technology', 'Nanjing University', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05470.jpg', 'data': {'categories': ['#rl', '#cv', '#rlhf', '#alignment', '#games', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Революция в генерации изображений: RL встречает потоковые модели', 'desc': 'Flow-GRPO – это новый метод, объединяющий онлайн-обучение с подкреплением (RL) и модели согласования потоков. Он использует преобразование ОДУ в СДУ для статистической выборки и стратегию уменьшения шума для повышения эффективности обучения. Метод показывает значительные улучшения в задачах генерации изображений по тексту, особенно в сложных композициях и визуальном рендеринге текста. Flow-GRPO также демонстрирует существенный прогресс в согласовании с предпочтениями человека без ухудшения качества или разнообразия изображений.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Flow-GRPO', 'desc': 'Flow-GRPO is a novel method that combines online reinforcement learning with flow matching models to enhance performance in text-to-image tasks. It employs an ODE-to-SDE conversion to facilitate statistical sampling, allowing for better exploration in reinforcement learning. Additionally, the Denoising Reduction strategy optimizes training efficiency by minimizing unnecessary denoising steps while maintaining inference quality. The results show significant improvements in accuracy and human preference alignment, with minimal risk of reward hacking, ensuring high-quality and diverse outputs.'}, 'zh': {'title': 'Flow-GRPO：强化学习与流匹配的完美结合', 'desc': '我们提出了Flow-GRPO，这是第一个将在线强化学习（RL）集成到流匹配模型中的方法。该方法采用了两个关键策略：首先，通过将确定性常微分方程（ODE）转换为等效的随机微分方程（SDE），实现了在所有时间步长上与原始模型的边际分布匹配，从而支持RL探索的统计采样；其次，采用去噪减少策略，在保持原始推理时间步数的同时减少训练去噪步骤，显著提高了采样效率而不降低性能。实验证明，Flow-GRPO在多个文本到图像的任务中表现出色，尤其在复杂组合中，RL调优的SD3.5几乎完美地生成了对象数量、空间关系和细粒度属性，显著提高了GenEval的准确率。'}}}, {'id': 'https://huggingface.co/papers/2505.02847', 'title': 'Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.02847', 'abstract': "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.", 'score': 22, 'issue_id': 3671, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '9204f0ca97eb8bc7', 'authors': ['Bang Zhang', 'Ruotian Ma', 'Qingxuan Jiang', 'Peisong Wang', 'Jiaqi Chen', 'Zheng Xie', 'Xingyu Chen', 'Yue Wang', 'Fanghua Ye', 'Jian Li', 'Yifan Yang', 'Zhaopeng Tu', 'Xiaolong Li'], 'affiliations': ['Hunyuan AI Digital Human, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.02847.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#alignment', '#agents', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'SAGE: Измерение эмпатии и социального интеллекта языковых моделей', 'desc': "Статья представляет SAGE - новую систему оценки способности больших языковых моделей (LLM) понимать человека, а не просто текст. SAGE использует 'Разумного Агента', который симулирует человеческие эмоции и мысли во время взаимодействия, обеспечивая более реалистичную оценку тестируемой модели в многоходовых диалогах. Эксперименты показали, что эмоциональные оценки SAGE коррелируют с психологическими метриками. Система также используется для создания публичного рейтинга LLM, выявляющего значительные различия между передовыми и базовыми моделями."}, 'en': {'title': 'Measuring Empathy in AI: The SAGE Framework', 'desc': 'The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems.'}, 'zh': {'title': '评估语言模型的情感理解能力', 'desc': '本文介绍了一种名为SAGE的自动评估框架，用于测量大型语言模型（LLM）对人类情感和社交认知的理解能力。SAGE通过模拟人类情感变化和内心想法，提供了更真实的多轮对话评估。实验结果表明，SAGE的情感评分与心理学评估工具的评分高度相关，验证了其心理学的真实性。该框架还建立了一个公开的Sentient排行榜，揭示了不同模型之间的显著差距，推动了对更具同理心和社交能力的语言代理的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.05315', 'title': 'Scalable Chain of Thoughts via Elastic Reasoning', 'url': 'https://huggingface.co/papers/2505.05315', 'abstract': 'Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.', 'score': 18, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '0ce3be6057da3ed2', 'authors': ['Yuhui Xu', 'Hanze Dong', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Caiming Xiong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05315.jpg', 'data': {'categories': ['#plp', '#training', '#reasoning', '#optimization', '#benchmark', '#math'], 'emoji': '🧠', 'ru': {'title': 'Эластичное рассуждение: эффективные цепочки мысли в условиях ограниченных ресурсов', 'desc': "Эта статья представляет новый подход под названием 'Эластичное рассуждение' для крупных моделей рассуждения (LRM). Метод разделяет процесс рассуждения на две фазы - мышление и решение - с независимыми бюджетами, что позволяет эффективно работать в условиях ограниченных ресурсов. Авторы предлагают стратегию обучения с ограниченным бюджетом, интегрированную в GRPO, которая учит модель адаптивно рассуждать при сокращении процесса мышления. Эксперименты на математических и программистских задачах показывают, что 'Эластичное рассуждение' работает надежно при строгих ограничениях бюджета и производит более краткие и эффективные рассуждения даже в неограниченных условиях."}, 'en': {'title': 'Elastic Reasoning: Scalable and Efficient Thought Processes for ML Models', 'desc': 'This paper introduces Elastic Reasoning, a framework designed to enhance the performance of large reasoning models (LRMs) under strict resource constraints. It separates the reasoning process into two distinct phases: thinking and solution, allowing for independent budget allocation for each phase. The framework employs a budget-constrained rollout strategy that helps models adaptively reason even when the thinking phase is limited, ensuring reliability in various scenarios. Empirical results show that Elastic Reasoning not only meets budget constraints effectively but also reduces training costs while improving the efficiency of reasoning outputs.'}, 'zh': {'title': '弹性推理：可控推理的新解决方案', 'desc': '大型推理模型（LRMs）在复杂任务上取得了显著进展，但其输出长度不受控制，给实际应用带来了挑战。我们提出了一种名为弹性推理的新框架，将推理过程分为思考和解决两个阶段，并为每个阶段分配独立的预算。在测试时，弹性推理优先考虑解决方案的完整性，从而在资源紧张的情况下显著提高可靠性。我们的实验证明，弹性推理在严格的预算限制下表现出色，同时训练成本显著低于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.05469', 'title': 'Generating Physically Stable and Buildable LEGO Designs from Text', 'url': 'https://huggingface.co/papers/2505.05469', 'abstract': 'We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.', 'score': 17, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '20752e033ce6b40a', 'authors': ['Ava Pun', 'Kangle Deng', 'Ruixuan Liu', 'Deva Ramanan', 'Changliu Liu', 'Jun-Yan Zhu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05469.jpg', 'data': {'categories': ['#robotics', '#dataset', '#multimodal', '#3d', '#open_source'], 'emoji': '🧱', 'ru': {'title': 'LegoGPT: от текста к физически стабильным моделям LEGO', 'desc': 'LegoGPT - это новый подход к генерации физически стабильных моделей из кубиков LEGO на основе текстовых запросов. Авторы создали большой набор данных физически стабильных конструкций LEGO с подписями и обучили авторегрессионную языковую модель предсказывать следующий кубик для добавления. Для улучшения стабильности конструкций применяется эффективная проверка валидности и откат с учетом физики во время авторегрессивного вывода. Эксперименты показывают, что LegoGPT создает стабильные, разнообразные и эстетичные конструкции LEGO, соответствующие текстовым запросам.'}, 'en': {'title': 'Building LEGO Dreams with AI!', 'desc': 'LegoGPT is a novel machine learning model designed to generate stable LEGO brick structures from textual descriptions. It utilizes a large dataset of LEGO designs paired with captions to train an autoregressive language model that predicts the next brick to add based on the input prompt. To ensure the generated designs are physically stable, the model incorporates a validity check and physics-aware rollback mechanism during the generation process. The results demonstrate that LegoGPT can create diverse and visually appealing LEGO models that can be assembled by both humans and robotic systems.'}, 'zh': {'title': '乐高设计的智能生成与稳定性保障', 'desc': '我们介绍了LegoGPT，这是第一个从文本提示生成物理稳定的乐高砖模型的方法。为了实现这一目标，我们构建了一个大规模的物理稳定乐高设计数据集，并训练了一个自回归的大型语言模型来预测下一个要添加的砖块。为了提高设计的稳定性，我们在自回归推理过程中采用了有效的有效性检查和物理感知回滚，利用物理法则和组装约束来修剪不可行的预测。我们的实验表明，LegoGPT生成的乐高设计稳定、多样且美观，与输入的文本提示紧密对齐。'}}}, {'id': 'https://huggingface.co/papers/2505.05071', 'title': 'FG-CLIP: Fine-Grained Visual and Textual Alignment', 'url': 'https://huggingface.co/papers/2505.05071', 'abstract': "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.", 'score': 15, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '4251cc9ddf64d2b8', 'authors': ['Chunyu Xie', 'Bin Wang', 'Fanjing Kong', 'Jincheng Li', 'Dawei Liang', 'Gengshen Zhang', 'Dawei Leng', 'Yuhui Yin'], 'affiliations': ['360 AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05071.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#multimodal', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'FG-CLIP: Точное понимание изображений на новом уровне', 'desc': 'Статья представляет Fine-Grained CLIP (FG-CLIP) - улучшенную версию модели CLIP для более детального понимания изображений. FG-CLIP использует 1,6 миллиарда пар изображение-текст с длинными подписями для захвата семантических деталей. Модель обучается на наборе данных из 12 миллионов изображений с 40 миллионами ограничивающих рамок и детальными подписями. FG-CLIP превосходит оригинальный CLIP и другие современные методы в различных задачах, включая детальное понимание изображений и мультимодальные бенчмарки.'}, 'en': {'title': 'Unlocking Fine-Grained Understanding with FG-CLIP', 'desc': 'The paper introduces Fine-Grained CLIP (FG-CLIP), an enhancement of the original CLIP model aimed at improving fine-grained understanding in multimodal tasks. FG-CLIP achieves this by generating a massive dataset of 1.6 billion long caption-image pairs, which helps capture detailed semantic information. Additionally, it constructs a high-quality dataset with 12 million images and 40 million bounding boxes, ensuring that the model learns from context-rich representations. By incorporating 10 million hard negative samples, FG-CLIP enhances its ability to differentiate subtle semantic differences, leading to superior performance in various tasks compared to the original CLIP and other leading models.'}, 'zh': {'title': '细粒度理解的新突破：FG-CLIP', 'desc': '对比语言-图像预训练（CLIP）在多模态任务中表现出色，但在细粒度理解方面存在困难。为了解决这个问题，我们提出了细粒度CLIP（FG-CLIP），通过三项关键创新来增强细粒度理解。首先，我们利用大型多模态模型生成16亿对长标题-图像对，以捕捉全局语义细节。其次，构建了一个高质量的数据集，包含1200万张图像和4000万个区域特定的边界框，确保精确且丰富的上下文表示。'}}}, {'id': 'https://huggingface.co/papers/2505.05467', 'title': 'StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant', 'url': 'https://huggingface.co/papers/2505.05467', 'abstract': 'We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.', 'score': 13, 'issue_id': 3674, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'c876dfdb1d290930', 'authors': ['Haibo Wang', 'Bo Feng', 'Zhengfeng Lai', 'Mingze Xu', 'Shiyu Li', 'Weifeng Ge', 'Afshin Dehghan', 'Meng Cao', 'Ping Huang'], 'affiliations': ['Apple', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05467.jpg', 'data': {'categories': ['#benchmark', '#video', '#dataset', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'StreamBridge: Революция в потоковом понимании видео', 'desc': 'StreamBridge - это фреймворк, который превращает офлайн-модели Video-LLM в потоковые. Он решает проблемы многоэтапного понимания в реальном времени и отсутствия механизмов проактивного реагирования. Фреймворк использует буфер памяти с компрессией и легковесную модель активации для интеграции с существующими Video-LLM. Для поддержки StreamBridge создан датасет Stream-IT, предназначенный для обучения потокового понимания видео.'}, 'en': {'title': 'Transforming Video-LLMs for Real-Time Streaming Success', 'desc': 'StreamBridge is a framework designed to enhance offline Video-LLMs for real-time streaming applications. It tackles two main issues: the need for effective multi-turn interactions and the ability to provide proactive responses. By using a memory buffer with round-decayed compression, it allows models to handle longer contexts in conversations. Additionally, StreamBridge introduces a lightweight activation model that integrates easily with existing Video-LLMs, and it is supported by the Stream-IT dataset, which is specifically created for streaming video understanding tasks.'}, 'zh': {'title': 'StreamBridge：流媒体视频理解的新突破', 'desc': 'StreamBridge是一个简单而有效的框架，可以将离线视频大语言模型（Video-LLMs）转变为支持流媒体的模型。它解决了在在线场景中适应现有模型的两个基本挑战：多轮实时理解能力有限和缺乏主动响应机制。具体来说，StreamBridge结合了内存缓冲区和逐轮衰减压缩策略，支持长上下文的多轮交互，并且采用了轻量级的解耦激活模型，能够轻松集成到现有的视频大语言模型中，实现持续的主动响应。此外，我们构建了Stream-IT，这是一个针对流媒体视频理解的大规模数据集，包含交错的视频-文本序列和多样的指令格式。'}}}, {'id': 'https://huggingface.co/papers/2505.05474', 'title': '3D Scene Generation: A Survey', 'url': 'https://huggingface.co/papers/2505.05474', 'abstract': '3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.', 'score': 12, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '06bda1a6228b8f26', 'authors': ['Beichen Wen', 'Haozhe Xie', 'Zhaoxi Chen', 'Fangzhou Hong', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore 637335'], 'pdf_title_img': 'assets/pdf/title_img/2505.05474.jpg', 'data': {'categories': ['#3d', '#robotics', '#multimodal', '#synthetic', '#survey'], 'emoji': '🌐', 'ru': {'title': 'Новые горизонты в генерации трехмерных сцен: от процедурных методов к нейронным сетям', 'desc': 'Эта статья представляет собой обзор современных методов генерации трехмерных сцен. Авторы систематизируют подходы по четырем парадигмам: процедурная генерация, нейронная генерация на основе 3D, генерация на основе изображений и генерация на основе видео. В работе анализируются технические основы, компромиссы и репрезентативные результаты каждого подхода, а также рассматриваются наборы данных, протоколы оценки и прикладные задачи. Статья завершается обсуждением ключевых проблем и перспективных направлений в области генерации 3D-сцен, включая повышение точности, физически корректную и интерактивную генерацию.'}, 'en': {'title': 'Advancing 3D Scene Generation with Deep Learning', 'desc': 'This paper reviews the latest techniques in 3D scene generation, which aims to create realistic and meaningful environments for various applications. It highlights the evolution from early procedural methods to modern deep generative models like GANs and diffusion models, which enhance the quality and diversity of generated scenes. The authors categorize these methods into four main paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation, analyzing their strengths and weaknesses. The paper also discusses ongoing challenges and future directions in the field, such as improving fidelity and integrating physics into scene generation.'}, 'zh': {'title': '3D场景生成的未来方向与挑战', 'desc': '3D场景生成旨在合成具有空间结构、语义意义和照片真实感的环境，广泛应用于沉浸式媒体、机器人、自动驾驶和具身人工智能等领域。早期基于程序规则的方法虽然具有可扩展性，但多样性有限。近年来，深度生成模型（如GAN和扩散模型）以及3D表示（如NeRF和3D高斯）取得了进展，使得能够学习真实世界场景的分布，从而提高了生成的真实感、多样性和视图一致性。本文综述了最新的3D场景生成方法，分析了其技术基础、权衡和代表性结果，并讨论了生成能力、3D表示、数据和评估等关键挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.04842', 'title': 'Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers', 'url': 'https://huggingface.co/papers/2505.04842', 'abstract': "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL^V that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL^V boosts MATH accuracy by over 20\\% with parallel sampling and enables 8-32times efficient test-time compute scaling compared to the base RL method. RL^V also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL^V achieves 1.2-1.6times higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.", 'score': 12, 'issue_id': 3688, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': 'ae650f18905f196e', 'authors': ['Kusha Sareen', 'Morgane M Moss', 'Alessandro Sordoni', 'Rishabh Agarwal', 'Arian Hosseini'], 'affiliations': ['Google DeepMind, Mila', 'Microsoft Research, Mila', 'Mila, McGill University', 'Mila, Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2505.04842.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#optimization', '#rl', '#math'], 'emoji': '🧠', 'ru': {'title': 'RL^V: Улучшение языковых моделей через совместное обучение рассуждению и проверке', 'desc': 'Эта статья представляет метод RL^V, который улучшает обучение с подкреплением для языковых моделей. RL^V обучает модель одновременно рассуждать и проверять результаты, используя данные, сгенерированные в процессе обучения с подкреплением. Метод значительно повышает точность на математических задачах и позволяет эффективно масштабировать вычисления во время тестирования. RL^V также демонстрирует хорошую обобщающую способность на различных типах задач.'}, 'en': {'title': 'Enhancing LLMs with Value-Driven Reinforcement Learning', 'desc': 'This paper introduces RL^V, a novel reinforcement learning method that enhances large language models (LLMs) by integrating value functions into the training process. Unlike traditional methods that discard learned value functions, RL^V allows LLMs to act as both reasoners and generative verifiers, improving their ability to verify outputs. The approach significantly increases accuracy in mathematical tasks and improves computational efficiency during testing, allowing for faster processing. Additionally, RL^V demonstrates strong performance across various task difficulties and domains, showcasing its versatility and effectiveness in real-world applications.'}, 'zh': {'title': 'RL^V：提升推理与验证的强化学习新方法', 'desc': '本文提出了一种新的强化学习方法RL^V，用于优化大型语言模型（LLM）的推理能力。与传统方法不同，RL^V同时训练LLM作为推理者和生成验证者，利用强化学习生成的数据来增强验证能力。实验结果表明，RL^V在MATH任务上的准确率提高了超过20%，并且在测试时计算效率上提升了8到32倍。该方法还展示了在不同难度和领域任务上的强泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.05327', 'title': 'ICon: In-Context Contribution for Automatic Data Selection', 'url': 'https://huggingface.co/papers/2505.05327', 'abstract': 'Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.', 'score': 11, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'b64bd2ecee9bb211', 'authors': ['Yixin Yang', 'Qingxiu Dong', 'Linli Yao', 'Fangwei Zhu', 'Zhifang Sui'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05327.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'ICon: Эффективный отбор данных для обучения языковых моделей без градиентов', 'desc': 'Статья представляет новый метод отбора данных для обучения больших языковых моделей (LLM) под названием ICon. Этот метод использует обучение в контексте (in-context learning) для измерения вклада образцов данных без вычисления градиентов или ручной разработки эвристик. ICon показал высокую эффективность в экспериментах на трех LLM и 12 бенчмарках, позволив моделям, обученным на 15% отобранных данных, превзойти модели, обученные на полных наборах данных. Анализ выбранных ICon образцов показал, что они представляют разнообразные задачи и имеют подходящий уровень сложности.'}, 'en': {'title': 'Efficient Data Selection for Better Language Model Training', 'desc': 'This paper introduces a new method called In-context Learning for Contribution Measurement (ICon) that helps select the best data for training Large Language Models (LLMs) without using complex gradient calculations. ICon leverages the concept of in-context learning to evaluate how much each data sample contributes to model performance, making it more efficient than traditional methods. The approach reduces reliance on human-designed heuristics, which can introduce bias, and instead focuses on the intrinsic qualities of the data. Experiments show that models trained with ICon-selected data perform better than those trained on full datasets or those selected by existing methods.'}, 'zh': {'title': '高效数据选择，提升模型性能！', 'desc': '本文提出了一种新的数据选择方法，称为ICon（In-context Learning for Contribution Measurement），用于提高大型语言模型（LLMs）的性能并降低训练成本。ICon是一种无梯度的方法，利用上下文学习的隐式微调特性来评估样本的贡献，而无需计算梯度或设计手动指标。通过在三种大型语言模型上进行广泛实验，ICon显示出其在选择高贡献数据方面的有效性，能够在减少计算成本的同时提高模型性能。实验结果表明，使用ICon选择的15%数据训练的模型在性能上超过了使用完整数据集的模型。'}}}, {'id': 'https://huggingface.co/papers/2505.03981', 'title': 'X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains', 'url': 'https://huggingface.co/papers/2505.03981', 'abstract': "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.", 'score': 11, 'issue_id': 3672, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '0e6c2f37e1536f9f', 'authors': ['Qianchu Liu', 'Sheng Zhang', 'Guanghui Qin', 'Timothy Ossowski', 'Yu Gu', 'Ying Jin', 'Sid Kiblawi', 'Sam Preston', 'Mu Wei', 'Paul Vozila', 'Tristan Naumann', 'Hoifung Poon'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.03981.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#transfer_learning', '#healthcare'], 'emoji': '🧠', 'ru': {'title': 'Обобщаемые рассуждения: от текста к мультимодальности и специализированным доменам', 'desc': 'Эта статья исследует возможность обобщения способностей к рассуждению на различные модальности и домены. Авторы представляют X-Reasoner - мультимодальную модель, обученную только на текстовых данных общего домена для обобщаемых рассуждений. Эксперименты показывают, что X-Reasoner успешно переносит навыки рассуждения на мультимодальные и узкоспециализированные задачи, превосходя существующие модели. Также представлена медицинская версия модели - X-Reasoner-Med, достигающая новых рекордных результатов на ряде текстовых и мультимодальных медицинских бенчмарков.'}, 'en': {'title': 'Unlocking Generalizable Reasoning Across Modalities with X-Reasoner', 'desc': 'This paper investigates whether reasoning abilities can be generalized across different types of data, specifically from text to other modalities like images. The authors present X-Reasoner, a vision-language model that is post-trained on general-domain text to enhance its reasoning capabilities. They employ a two-stage training process that includes supervised fine-tuning with detailed reasoning steps and reinforcement learning with measurable rewards. The results demonstrate that X-Reasoner not only excels in multimodal tasks but also improves in specialized fields, leading to the creation of X-Reasoner-Med, which sets new benchmarks in medical reasoning tasks.'}, 'zh': {'title': '推理能力的跨模态推广', 'desc': '最近的专有模型（如o3）展示了强大的多模态推理能力。然而，大多数现有的开源研究主要集中在训练仅基于文本的推理模型，评估也主要限于数学和一般领域任务。因此，如何有效地将推理能力扩展到文本输入和一般领域之外仍然不清楚。本文探讨了一个基本的研究问题：推理是否可以跨模态和领域进行推广？'}}}, {'id': 'https://huggingface.co/papers/2505.05408', 'title': 'Crosslingual Reasoning through Test-Time Scaling', 'url': 'https://huggingface.co/papers/2505.05408', 'abstract': "Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.", 'score': 8, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'a6c39be653cbaefe', 'authors': ['Zheng-Xin Yong', 'M. Farid Adilazuarda', 'Jonibek Mansurov', 'Ruochen Zhang', 'Niklas Muennighoff', 'Carsten Eickhoff', 'Genta Indra Winata', 'Julia Kreutzer', 'Stephen H. Bach', 'Alham Fikri Aji'], 'affiliations': ['Brown University', 'Capital One', 'Cohere Labs', 'MBZUAI', 'Stanford University', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.05408.jpg', 'data': {'categories': ['#math', '#reasoning', '#low_resource', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Английское обучение, глобальное рассуждение: потенциал и ограничения многоязычных LLM', 'desc': "Это исследование посвящено способности моделей больших языковых моделей (LLM) рассуждать на разных языках после обучения на английском языке. Авторы обнаружили, что увеличение вычислительных ресурсов при выводе улучшает многоязычное математическое рассуждение, даже для малоресурсных языков. Модели демонстрируют паттерн 'цитирование-и-размышление' при работе с неанглийскими входными данными. Исследователи также разработали стратегию контроля языка рассуждений и выявили ограничения в обобщении рассуждений вне предметной области."}, 'en': {'title': 'Unlocking Multilingual Reasoning with English-Centric Models', 'desc': 'This paper explores how reasoning abilities of large language models, which are often trained in English, can be applied to other languages. The authors find that increasing computational resources for English-based reasoning models enhances their performance in multilingual mathematical reasoning, even in languages with fewer resources. They also identify a pattern where these models can effectively reason about non-English inputs by quoting and thinking in English. However, the study highlights challenges in transferring reasoning skills from STEM topics to cultural knowledge, indicating that while there is potential for cross-lingual reasoning, improvements are needed for low-resource languages and diverse contexts.'}, 'zh': {'title': '提升多语言推理能力的潜力与挑战', 'desc': '本研究探讨了大型语言模型在多语言环境下的推理能力，尤其是英语推理的微调如何在其他语言中推广。我们发现，通过增加推理计算能力，英语中心的推理语言模型在多种语言（包括低资源语言）中的数学推理能力得到了显著提升。研究还表明，尽管英语中心的推理链主要是英语，但它们在处理非英语输入时仍能遵循特定的推理模式。最后，我们发现模型在高资源语言中的推理效果更好，但在跨领域推理方面存在局限性，尤其是在从STEM领域到文化常识的迁移上。'}}}, {'id': 'https://huggingface.co/papers/2505.05288', 'title': 'PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes', 'url': 'https://huggingface.co/papers/2505.05288', 'abstract': "We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.", 'score': 7, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '7ffec9bccc965d17', 'authors': ['Ahmed Abdelreheem', 'Filippo Aleotti', 'Jamie Watson', 'Zawar Qureshi', 'Abdelrahman Eldesokey', 'Peter Wonka', 'Gabriel Brostow', 'Sara Vicente', 'Guillermo Garcia-Hernando'], 'affiliations': ['KAUST', 'Niantic Spatial', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.05288.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#3d', '#survey', '#dataset'], 'emoji': '🧊', 'ru': {'title': 'Языковое управление размещением объектов в реальных 3D-сценах', 'desc': 'Статья представляет новую задачу размещения объектов в реальных 3D-сценах с помощью языковых инструкций. Модель получает облако точек 3D-сцены, 3D-модель объекта и текстовое описание желаемого размещения. Задача требует рассуждений о 3D-геометрических отношениях и свободном пространстве, что отличает ее от других задач локализации в 3D. Авторы предлагают новый бенчмарк, протокол оценки, датасет для обучения 3D языковых моделей и базовый метод решения.'}, 'en': {'title': 'Placing Objects with Words in 3D Spaces!', 'desc': 'This paper presents a new task called Language-Guided Object Placement in Real 3D Scenes, which involves placing a 3D object based on a textual description. The model must analyze a point cloud of a 3D scene and determine a suitable location for the object that aligns with the given prompt. This task is particularly challenging due to the ambiguity of multiple valid placements and the need for understanding 3D spatial relationships. The authors introduce a benchmark, evaluation protocol, and a dataset to train 3D language models, aiming to enhance the evaluation of generalist 3D models.'}, 'zh': {'title': '语言引导的3D物体放置新挑战', 'desc': '我们提出了一项新任务：在真实3D场景中进行语言引导的物体放置。我们的模型接收一个3D场景的点云、一个3D资产和一个文本提示，任务是找到一个符合提示的有效放置位置。与其他语言引导的3D场景定位任务相比，这项任务具有特定的挑战性，因为它存在多个有效解，并且需要推理3D几何关系和自由空间。我们通过提出新的基准和评估协议来开启这一任务，并引入了一个新的数据集用于训练3D大语言模型，以及第一个非平凡的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.04769', 'title': 'Vision-Language-Action Models: Concepts, Progress, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2505.04769', 'abstract': 'Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models', 'score': 7, 'issue_id': 3682, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '75960b9512a0a05b', 'authors': ['Ranjan Sapkota', 'Yang Cao', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Biological & Environmental Engineering, Ithaca, New York, USA', 'The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong', 'University of the Peloponnese, Department of Informatics and Telecommunications, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.04769.jpg', 'data': {'categories': ['#agi', '#robotics', '#ethics', '#architecture', '#agents', '#multimodal', '#training'], 'emoji': '🤖', 'ru': {'title': 'Объединение зрения, языка и действия: новая парадигма ИИ', 'desc': 'Статья представляет обзор моделей Vision-Language-Action (VLA), которые объединяют восприятие, понимание естественного языка и физические действия в единую вычислительную структуру. Авторы систематизируют последние достижения в области VLA по пяти тематическим направлениям. Рассматриваются архитектурные инновации, стратегии эффективного обучения и ускорения вывода в реальном времени. Обсуждаются применения в робототехнике, автономных транспортных средствах, медицине и других областях.'}, 'en': {'title': 'Unifying Vision, Language, and Action in AI', 'desc': 'Vision-Language-Action (VLA) models represent a significant step in AI by combining visual perception, language understanding, and physical actions into one system. This review organizes recent developments in VLA models into five key themes, highlighting their evolution from basic cross-modal learning to sophisticated generalist agents. It covers over 80 models from the last three years, focusing on innovations in architecture, training efficiency, and real-time performance. The paper also discusses applications in various fields and proposes solutions to challenges like real-time control and ethical deployment, aiming to guide future advancements in intelligent robotics and AI.'}, 'zh': {'title': '统一感知与行动的智能模型', 'desc': '视觉-语言-行动（VLA）模型是人工智能领域的一项重要进展，旨在将感知、自然语言理解和具体行动统一在一个计算框架内。本文综述了VLA模型的最新进展，系统地组织了五个主题支柱，描绘了这一快速发展的领域。我们回顾了VLA系统的概念基础，追溯其从跨模态学习架构到紧密集成视觉-语言模型、行动规划器和层次控制器的演变。文章还探讨了VLA模型在机器人、自动驾驶、医疗、农业和增强现实等多个应用领域的挑战与解决方案。'}}}, {'id': 'https://huggingface.co/papers/2505.03422', 'title': 'LiftFeat: 3D Geometry-Aware Local Feature Matching', 'url': 'https://huggingface.co/papers/2505.03422', 'abstract': 'Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.', 'score': 7, 'issue_id': 3678, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '5be3a3b002db41cc', 'authors': ['Yepeng Liu', 'Wenpeng Lai', 'Zhou Zhao', 'Yuxuan Xiong', 'Jinchi Zhu', 'Jun Cheng', 'Yongchao Xu'], 'affiliations': ['Institute for Infocomm Research, A*STAR, Singapore', 'SF Technology, Shenzhen, China', 'School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03422.jpg', 'data': {'categories': ['#cv', '#robotics', '#3d'], 'emoji': '🤖', 'ru': {'title': 'LiftFeat: Повышение надежности сопоставления признаков с помощью 3D геометрии', 'desc': 'Статья представляет новую легковесную нейронную сеть LiftFeat для надежного сопоставления локальных признаков изображений. LiftFeat повышает устойчивость дескрипторов путем агрегации 3D геометрических признаков, извлеченных с помощью предобученной модели оценки глубины. Сеть использует модуль, учитывающий 3D геометрию, для объединения признаков нормалей поверхности с исходными 2D дескрипторами. Экспериментальные результаты показывают превосходство LiftFeat над современными легковесными методами в задачах оценки относительной позы, оценки гомографии и визуальной локализации.'}, 'en': {'title': 'LiftFeat: Enhancing Feature Matching with 3D Geometry', 'desc': 'This paper introduces LiftFeat, a lightweight neural network designed to improve local feature matching in challenging visual conditions. It addresses issues like drastic lighting changes and low texture areas by incorporating 3D geometric features into the feature extraction process. The network uses a pre-trained monocular depth estimation model to create pseudo surface normal labels, which guide the extraction of 3D features. Experimental results show that LiftFeat significantly enhances the performance of visual localization and pose estimation tasks compared to existing lightweight methods.'}, 'zh': {'title': '提升视觉特征匹配的鲁棒性与效率', 'desc': '本论文提出了一种新的轻量级网络LiftFeat，旨在提高在极端条件下的视觉特征匹配能力。通过聚合三维几何特征，LiftFeat增强了原始描述符的鲁棒性。我们采用预训练的单目深度估计模型生成伪表面法线标签，以指导三维几何特征的提取。实验结果表明，LiftFeat在相对姿态估计、单应性估计和视觉定位任务中优于一些轻量级的最新方法。'}}}, {'id': 'https://huggingface.co/papers/2505.05064', 'title': 'WaterDrum: Watermarking for Data-centric Unlearning Metric', 'url': 'https://huggingface.co/papers/2505.05064', 'abstract': 'Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.', 'score': 6, 'issue_id': 3684, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '8d04facba381d919', 'authors': ['Xinyang Lu', 'Xinyuan Niu', 'Gregory Kang Ruey Lau', 'Bui Thi Cam Nhung', 'Rachael Hwee Ling Sim', 'Fanyu Wen', 'Chuan-Sheng Foo', 'See-Kiong Ng', 'Bryan Kian Hsiang Low'], 'affiliations': ['A*STAR, Way, Create', 'CNRS@CREATE, 1 Singapore #08Tower, Computer Singapore (CFAR), Create', 'Centre for Singapore Science, National University', 'Department of of Singapore', 'Frontier AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05064.jpg', 'data': {'categories': ['#leakage', '#dataset', '#benchmark', '#data', '#open_source'], 'emoji': '🧽', 'ru': {'title': 'WaterDrum: Точная оценка разобучения языковых моделей с помощью водяных знаков', 'desc': 'Статья представляет новую метрику для оценки разобучения больших языковых моделей (LLM), называемую WaterDrum. Эта метрика основана на использовании водяных знаков в тексте и преодолевает ограничения существующих подходов, ориентированных на полезность модели. Авторы также представляют новые наборы данных для тестирования алгоритмов разобучения LLM, содержащие различные уровни сходства между точками данных. WaterDrum позволяет более точно оценивать эффективность разобучения в реалистичных сценариях, где забываемые и сохраняемые данные могут быть семантически похожими.'}, 'en': {'title': 'WaterDrum: A New Era in LLM Unlearning Metrics', 'desc': 'This paper addresses the challenge of unlearning in large language models (LLMs), which is essential for removing sensitive or harmful data. Current metrics for evaluating unlearning focus on model performance but may not reflect true unlearning effectiveness, especially when data is semantically similar. The authors propose a new data-centric unlearning metric called WaterDrum, which utilizes robust text watermarking to better assess unlearning in practical scenarios. Additionally, they introduce benchmark datasets designed to test unlearning algorithms under various conditions, enhancing the evaluation process for LLM unlearning.'}, 'zh': {'title': '提升大语言模型的遗忘能力', 'desc': '大语言模型（LLM）的遗忘能力在实际应用中至关重要，尤其是在需要有效去除用户的私人、版权或有害数据影响时。现有的以效用为中心的遗忘度量可能无法准确评估在语义相似内容的情况下的遗忘程度。本文提出了一种新的数据中心遗忘度量方法，称为WaterDrum，利用稳健的文本水印技术来克服这些限制。我们还引入了新的基准数据集，用于评估LLM的遗忘算法，确保可以严格测试遗忘效果。'}}}, {'id': 'https://huggingface.co/papers/2505.02363', 'title': 'SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in\n  Language Model Preference Learning', 'url': 'https://huggingface.co/papers/2505.02363', 'abstract': 'Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.', 'score': 6, 'issue_id': 3687, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'da8e6360c88cd0ed', 'authors': ['Tianjian Li', 'Daniel Khashabi'], 'affiliations': ['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, US'], 'pdf_title_img': 'assets/pdf/title_img/2505.02363.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#dataset'], 'emoji': '🔀', 'ru': {'title': 'Объединение сильных сторон: новый подход к оптимизации языковых моделей', 'desc': 'Это исследование посвящено оптимизации языковых моделей с использованием данных о предпочтениях пользователей. Авторы обнаружили, что on-policy и off-policy данные имеют взаимодополняющие сильные стороны в разных типах задач. Они представили метод SIMPLEMIX, который объединяет преимущества обоих типов данных путем их простого смешивания. Результаты показывают, что SIMPLEMIX значительно улучшает согласование языковых моделей с предпочтениями пользователей, превосходя существующие подходы.'}, 'en': {'title': 'SIMPLEMIX: Uniting On-Policy and Off-Policy for Better Language Model Alignment', 'desc': 'This paper explores how to align language models with human preferences using two types of data: on-policy and off-policy. On-policy data is shown to excel in tasks requiring reasoning, like math and coding, while off-policy data is better for creative and open-ended tasks. The authors propose a new method called SIMPLEMIX, which effectively combines these two data sources to leverage their strengths. Their experiments demonstrate that SIMPLEMIX significantly enhances the performance of language models compared to existing methods, achieving notable improvements in alignment metrics.'}, 'zh': {'title': '结合在线与离线数据，提升语言模型对齐效果', 'desc': '本研究探讨了在偏好学习中，在线数据和离线数据的互补优势。在线数据在推理任务（如数学和编程）中表现优异，而离线数据在开放式任务（如创意写作和个人推荐）中更具优势。我们提出了一种名为SIMPLEMIX的方法，通过简单地混合这两种数据源，结合它们的优点。实验证明，SIMPLEMIX在多种任务和基准测试中显著提高了语言模型的对齐效果。'}}}, {'id': 'https://huggingface.co/papers/2505.04955', 'title': 'Chain-of-Thought Tokens are Computer Program Variables', 'url': 'https://huggingface.co/papers/2505.04955', 'abstract': 'Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically study the role of CoT tokens in LLMs on two compositional tasks: multi-digit multiplication and dynamic programming. While CoT is essential for solving these problems, we find that preserving only tokens that store intermediate results would achieve comparable performance. Furthermore, we observe that storing intermediate results in an alternative latent form will not affect model performance. We also randomly intervene some values in CoT, and notice that subsequent CoT tokens and the final answer would change correspondingly. These findings suggest that CoT tokens may function like variables in computer programs but with potential drawbacks like unintended shortcuts and computational complexity limits between tokens. The code and data are available at https://github.com/solitaryzero/CoTs_are_Variables.', 'score': 5, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'a8bb5e6b1e09e5dd', 'authors': ['Fangwei Zhu', 'Peiyi Wang', 'Zhifang Sui'], 'affiliations': ['School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04955.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Токены CoT в LLM: переменные для машинного мышления', 'desc': 'Это исследование посвящено изучению механизма рассуждений с использованием цепочки мыслей (Chain-of-Thought, CoT) в больших языковых моделях (LLM). Авторы проанализировали роль токенов CoT на примере задач многозначного умножения и динамического программирования. Результаты показывают, что токены CoT функционируют подобно переменным в компьютерных программах, сохраняя промежуточные результаты. Исследование также выявило потенциальные недостатки метода CoT, такие как нежелательные упрощения и ограничения вычислительной сложности между токенами.'}, 'en': {'title': 'Unlocking the Power of Chain-of-Thought Tokens in LLMs', 'desc': "This paper investigates the role of chain-of-thought (CoT) tokens in large language models (LLMs) when solving complex reasoning tasks. The authors find that while CoT is important for tasks like multi-digit multiplication and dynamic programming, retaining only tokens that represent intermediate results can yield similar performance. They also discover that storing these results in a different latent form does not impact the model's effectiveness. Additionally, the study reveals that altering CoT values affects subsequent tokens and final answers, indicating that CoT tokens may act like variables in programming, albeit with some limitations."}, 'zh': {'title': '链式推理：变量的力量与挑战', 'desc': '本文研究了链式推理（CoT）在大型语言模型（LLMs）中的作用，特别是在多位数乘法和动态规划这两个复杂任务中的表现。研究发现，虽然CoT对于解决这些问题至关重要，但仅保留存储中间结果的标记也能达到相似的效果。此外，将中间结果以另一种潜在形式存储不会影响模型的性能。最后，随机干预CoT中的某些值会导致后续的CoT标记和最终答案相应变化，这表明CoT标记可能像计算机程序中的变量，但也存在一些潜在的缺陷，如意外的捷径和标记之间的计算复杂性限制。'}}}, {'id': 'https://huggingface.co/papers/2504.19314', 'title': 'BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese', 'url': 'https://huggingface.co/papers/2504.19314', 'abstract': "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.", 'score': 4, 'issue_id': 3673, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': '06aff0f566bd3817', 'authors': ['Peilin Zhou', 'Bruce Leon', 'Xiang Ying', 'Can Zhang', 'Yifan Shao', 'Qichen Ye', 'Dading Chong', 'Zhiling Jin', 'Chenxuan Xie', 'Meng Cao', 'Yuxin Gu', 'Sixin Hong', 'Jing Ren', 'Jian Chen', 'Chao Liu', 'Yining Hua'], 'affiliations': ['Alibaba Group', 'HSBC', 'Harvard T.H. Chan School of Public Health', 'Hong Kong University of Science and Technology (Guangzhou)', 'MBZUAI', 'Mindverse AI', 'NIO', 'Peking University', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.19314.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#dataset', '#benchmark', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'BrowseComp-ZH: испытание языковых моделей в китайском интернете', 'desc': 'Статья представляет BrowseComp-ZH - новый бенчмарк для оценки способностей языковых моделей (ЯМ) работать с китайским веб-контентом. Бенчмарк состоит из 289 сложных многоэтапных вопросов в 11 различных областях. Несмотря на свои сильные разговорные и поисковые возможности, большинство современных ЯМ показывают низкую точность на этом тесте. Результаты демонстрируют, что для успеха в BrowseComp-ZH требуются не только эффективные стратегии поиска, но и сложные рассуждения и согласование информации.'}, 'en': {'title': 'Evaluating LLMs: The Challenge of Chinese Web Browsing', 'desc': 'This paper introduces BrowseComp-ZH, a benchmark designed to evaluate large language models (LLMs) on their ability to browse and retrieve information from the Chinese web. It consists of 289 challenging multi-hop questions across various domains, focusing on high difficulty and unique answers. The study reveals that most state-of-the-art models perform poorly, with accuracy rates often below 10%, highlighting the complexity of reasoning and retrieval in non-English contexts. The results indicate that current LLMs still face significant challenges in mastering the necessary skills for effective information retrieval and reasoning in diverse linguistic environments.'}, 'zh': {'title': '中文网络智能体评估新基准：BrowseComp-ZH', 'desc': '随着大型语言模型（LLMs）逐渐演变为使用工具的智能体，实时浏览网络的能力成为衡量其推理和检索能力的重要标准。现有的基准测试如BrowseComp主要集中在英语，忽视了其他主要信息生态系统（尤其是中文）在语言、基础设施和审查方面的复杂性。为了解决这一问题，我们推出了BrowseComp-ZH，这是一个专门为全面评估中文网络上的LLM智能体而设计的高难度基准测试。该基准测试包含289个跨越11个不同领域的多跳问题，旨在考察模型的检索策略、推理能力和信息整合能力。'}}}, {'id': 'https://huggingface.co/papers/2505.02550', 'title': 'Bielik v3 Small: Technical Report', 'url': 'https://huggingface.co/papers/2505.02550', 'abstract': 'We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.', 'score': 53, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'a7c9d183be6447dd', 'authors': ['Krzysztof Ociepa', 'Łukasz Flis', 'Remigiusz Kinas', 'Krzysztof Wróbel', 'Adrian Gwoździej'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02550.jpg', 'data': {'categories': ['#small_models', '#plp', '#multilingual', '#low_resource', '#dataset', '#benchmark'], 'emoji': '🇵🇱', 'ru': {'title': 'Эффективные языковые модели делают ИИ на польском языке доступнее', 'desc': 'Представлена серия Bielik v3 - эффективные генеративные текстовые модели для польского языка. Модели демонстрируют, что меньшие оптимизированные архитектуры могут достигать производительности сравнимой с гораздо более крупными аналогами. Ключевые инновации включают специальный польский токенизатор, взвешенную функцию потерь и адаптивную скорость обучения. Модели показывают отличные результаты в различных тестах, устанавливая новые стандарты для эффективного моделирования языка для менее распространенных языков.'}, 'en': {'title': 'Efficient Polish Language Models: Big Performance from Small Sizes', 'desc': 'Bielik v3 introduces efficient generative text models specifically designed for the Polish language, with sizes of 1.5B and 4.5B parameters. These models show that smaller architectures can perform as well as larger ones while using less computational power. Key innovations include a custom tokenizer for better token efficiency, a specialized loss function to balance learning, and an adaptive learning rate that adjusts during training. With training on a vast dataset, these models set new standards for language processing in Polish, making advanced AI more accessible for various applications.'}, 'zh': {'title': '高效波兰语生成模型的创新之路', 'desc': '我们介绍了Bielik v3，这是一系列针对波兰语处理的高效生成文本模型（1.5B和4.5B参数）。这些模型表明，较小且经过优化的架构可以在计算资源大幅减少的情况下，达到与更大模型相当的性能。我们的创新包括定制的波兰语分词器（APT4），显著提高了标记效率，以及加权指令交叉熵损失，平衡不同指令类型的学习。此外，动态调整的学习率根据训练进度进行调整，使得模型在多个基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.02410', 'title': 'Bielik 11B v2 Technical Report', 'url': 'https://huggingface.co/papers/2505.02410', 'abstract': "We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.", 'score': 44, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'e9cb82cbeaac24ed', 'authors': ['Krzysztof Ociepa', 'Łukasz Flis', 'Krzysztof Wróbel', 'Adrian Gwoździej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02410.jpg', 'data': {'categories': ['#inference', '#multilingual', '#low_resource', '#training', '#benchmark', '#architecture', '#optimization', '#reasoning'], 'emoji': '🇵🇱', 'ru': {'title': 'Революция в обработке польского языка: Bielik 11B v2 устанавливает новые стандарты эффективности', 'desc': 'Представлена модель Bielik 11B v2 - усовершенствованная языковая модель для обработки польских текстов. Основанная на архитектуре Mistral 7B v0.2 и масштабированная до 11 миллиардов параметров, она демонстрирует исключительную производительность на польских языковых бенчмарках. В модели применены две ключевые инновации: взвешенная функция потерь для инструкций и адаптивная скорость обучения. Bielik 11B v2 превосходит более крупные модели и устанавливает новые стандарты для эффективного моделирования языка с ограниченными ресурсами.'}, 'en': {'title': 'Revolutionizing Polish Language Processing with Bielik 11B v2', 'desc': 'Bielik 11B v2 is a cutting-edge language model specifically designed for processing Polish text. It utilizes the Mistral 7B v0.2 architecture and has been enhanced to 11 billion parameters, achieving remarkable results on Polish language tasks while also performing well in cross-lingual scenarios. The model introduces innovative techniques like Weighted Instruction Cross-Entropy Loss for better learning from diverse instructions and an Adaptive Learning Rate that adjusts based on the context length. Its efficiency and quantization options allow it to run on various hardware, making it a significant advancement in AI for Polish language applications.'}, 'zh': {'title': '波兰语处理的新标杆：Bielik 11B v2', 'desc': 'Bielik 11B v2 是一个针对波兰语文本处理的先进语言模型，基于 Mistral 7B v0.2 架构，参数规模达到 11B。该模型在波兰语基准测试中表现出色，同时具备强大的跨语言能力。我们引入了两项关键技术创新：加权指令交叉熵损失，通过为训练样本分配基于质量的权重来优化不同指令类型的学习，以及自适应学习率，根据上下文长度动态调整。综合评估显示，Bielik 11B v2 超越了许多更大模型的表现，尤其在语言理解和复杂推理等任务上显著优于其他专门的波兰语言模型。'}}}, {'id': 'https://huggingface.co/papers/2505.06111', 'title': 'UniVLA: Learning to Act Anywhere with Task-centric Latent Actions', 'url': 'https://huggingface.co/papers/2505.06111', 'abstract': "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.", 'score': 17, 'issue_id': 3704, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': 'bf19981dd100b8fb', 'authors': ['Qingwen Bu', 'Yanting Yang', 'Jisong Cai', 'Shenyuan Gao', 'Guanghui Ren', 'Maoqing Yao', 'Ping Luo', 'Hongyang Li'], 'affiliations': ['AgiBot', 'OpenDriveLab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.06111.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#benchmark', '#agents', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Универсальное обучение роботов через видео и язык', 'desc': 'UniVLA - это новый фреймворк для обучения универсальных политик взаимодействия робота с окружающей средой на основе зрения, языка и действий. Ключевая инновация заключается в использовании скрытой модели действий для извлечения представлений из видео, что позволяет использовать разнородные данные из различных воплощений и перспектив. Фреймворк демонстрирует превосходные результаты на нескольких бенчмарках по манипуляции и навигации, а также в реальных роботизированных системах. UniVLA достигает лучшей производительности по сравнению с OpenVLA, используя при этом значительно меньше вычислительных ресурсов и данных.'}, 'en': {'title': 'UniVLA: Empowering Robots with Cross-Embodiment Learning', 'desc': 'The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches.'}, 'zh': {'title': 'UniVLA：提升通用机器人学习效率的新框架', 'desc': '本文提出了一种新的框架UniVLA，用于学习跨体现的视觉-语言-动作（VLA）策略，以提高通用机器人在不同环境中的表现。我们通过视频中的潜在动作模型提取以任务为中心的动作表示，从而利用广泛的多样化数据。为了减少与任务无关的动态影响，我们结合了语言指令，并在DINO特征空间中建立了潜在动作模型。实验结果表明，UniVLA在多个操作和导航基准测试中表现优异，且在预训练计算和下游数据方面的需求显著低于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.05026', 'title': 'G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness', 'url': 'https://huggingface.co/papers/2505.05026', 'abstract': 'Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.', 'score': 11, 'issue_id': 3705, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '41e61eccd430ea55', 'authors': ['Jaehyun Jeon', 'Jang Han Yoon', 'Min Soo Kim', 'Sumin Shim', 'Yejin Choi', 'Hanbin Kim', 'Youngjae Yu'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05026.jpg', 'data': {'categories': ['#reasoning', '#cv', '#benchmark', '#optimization', '#inference'], 'emoji': '🖥️', 'ru': {'title': 'Оценка убедительности UI без A/B-тестов', 'desc': 'Авторы статьи представляют новый подход к оценке эффективности пользовательских интерфейсов с точки зрения их убедительности. Они вводят бенчмарк WiserUI-Bench для сравнительной оценки дизайна интерфейсов, содержащий 300 пар реальных UI-изображений с результатами A/B-тестов. Исследователи также предлагают стратегию G-FOCUS для улучшения оценки убедительности интерфейсов с помощью мультимодальных языковых моделей. Эксперименты показывают, что G-FOCUS превосходит существующие методы по согласованности и точности оценки пользовательских интерфейсов.'}, 'en': {'title': 'Revolutionizing UI Evaluation with G-FOCUS and WiserUI-Bench', 'desc': 'This paper discusses the importance of evaluating user interface (UI) design not just for its visual appeal but for its ability to influence user behavior, a concept known as Design Persuasiveness. The authors highlight the limitations of traditional A/B testing, which is often expensive and slow, and propose a new benchmark called WiserUI-Bench for assessing UI design effectiveness through pairwise comparisons. They introduce G-FOCUS, an innovative reasoning strategy that improves the accuracy of Vision-Language Models (VLMs) in evaluating UI persuasiveness by minimizing biases. The results demonstrate that G-FOCUS outperforms existing methods, paving the way for more efficient and scalable UI design optimization.'}, 'zh': {'title': '提升用户界面设计的说服力评估', 'desc': '本论文探讨了用户界面（UI）设计的有效性评估，强调设计的说服力对用户行为的影响。传统的A/B测试方法虽然常用，但成本高且耗时。我们提出了WiserUI-Bench，这是一个用于成对UI设计说服力评估的基准，包含300对真实的UI图像及其A/B测试结果和专家理由。此外，我们还提出了G-FOCUS，这是一种新颖的推理策略，能够提高基于视觉语言模型的说服力评估的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.02686', 'title': 'Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2505.02686', 'abstract': 'Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.', 'score': 11, 'issue_id': 3706, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '22b290e68229e62f', 'authors': ['Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02686.jpg', 'data': {'categories': ['#survey', '#training', '#rlhf', '#alignment', '#benchmark', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обучение с вознаграждением: ключ к совершенствованию больших языковых моделей', 'desc': 'Данная статья представляет обзор парадигмы обучения с вознаграждением в контексте больших языковых моделей (LLM). Авторы анализируют различные стратегии применения этой парадигмы на этапах обучения, вывода и пост-обработки. Обсуждаются методы, такие как обучение с подкреплением, декодирование с учетом вознаграждения и постобработка, которые позволяют LLM переходить от пассивного обучения на статических данных к активному обучению с динамической обратной связью. В статье также рассматриваются бенчмарки для моделей вознаграждения, основные приложения и будущие направления исследований в этой области.'}, 'en': {'title': 'Harnessing Rewards: The Future of Learning in LLMs', 'desc': 'This paper discusses the evolution of Large Language Models (LLMs) focusing on the shift from pre-training to learning from rewards. It highlights how reward signals guide LLM behavior through techniques like reinforcement learning, reward-guided decoding, and post-hoc correction. The authors categorize various strategies used in training, inference, and post-inference stages, emphasizing the importance of dynamic feedback for improving model alignment and reasoning. Additionally, the paper addresses benchmarks for reward models and outlines future challenges and directions in this area.'}, 'zh': {'title': '从奖励中学习，赋能大型语言模型', 'desc': '最近，大型语言模型（LLMs）的发展从预训练扩展到后训练和测试时扩展。一个关键的统一范式出现了：从奖励中学习，其中奖励信号作为指导星，引导LLM的行为。这个范式支持了许多流行的技术，如强化学习（在RLHF、DPO和GRPO中）、奖励引导解码和事后修正。通过这个范式，LLMs能够从静态数据的被动学习转向从动态反馈的主动学习，赋予它们对齐的偏好和深度推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.06046', 'title': 'Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information', 'url': 'https://huggingface.co/papers/2505.06046', 'abstract': "As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.", 'score': 10, 'issue_id': 3707, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': 'c671de3a9e8ff4de', 'authors': ['Joshua Harris', 'Fan Grayson', 'Felix Feldman', 'Timothy Laurence', 'Toby Nonnenmacher', 'Oliver Higgins', 'Leo Loman', 'Selina Patel', 'Thomas Finnie', 'Samuel Collins', 'Michael Borowitz'], 'affiliations': ['UK Health Security Agency (UKHSA)'], 'pdf_title_img': 'assets/pdf/title_img/2505.06046.jpg', 'data': {'categories': ['#alignment', '#science', '#dataset', '#healthcare', '#benchmark'], 'emoji': '🏥', 'ru': {'title': 'Оценка знаний LLM в сфере общественного здравоохранения: прогресс и ограничения', 'desc': 'Статья представляет новый бенчмарк PubHealthBench для оценки знаний больших языковых моделей (LLM) в области общественного здравоохранения Великобритании. Бенчмарк содержит более 8000 вопросов для оценки ответов моделей на вопросы с множественным выбором и свободной формой. Исследование показало, что новейшие частные LLM достигают более 90% точности в задачах с множественным выбором, превосходя людей с поверхностным использованием поисковых систем. Однако в задачах со свободной формой ответа производительность моделей ниже, что указывает на необходимость дополнительных мер безопасности при использовании LLM для предоставления информации о здравоохранении.'}, 'en': {'title': 'Evaluating LLMs for Public Health: Promising Yet Cautious', 'desc': "This paper investigates the knowledge of Large Language Models (LLMs) in the domain of UK public health information. It introduces a benchmark called PubHealthBench, which consists of over 8000 questions designed to evaluate LLMs' performance in Multiple Choice Question Answering (MCQA) and free form responses. The study assesses 24 different LLMs, revealing that the latest models perform well in MCQA tasks, achieving over 90% accuracy, but struggle with free form responses, none scoring above 75%. The findings suggest that while LLMs show promise as reliable sources of public health information, caution is needed when interpreting their free form outputs."}, 'zh': {'title': '提升公共卫生信息的准确性', 'desc': '随着大型语言模型（LLMs）的广泛应用，了解它们在特定领域的知识变得至关重要，尤其是在公共卫生领域。本文提出了一个新的基准测试PubHealthBench，包含超过8000个问题，用于评估LLMs在公共卫生查询中的多项选择问答（MCQA）和自由形式回答的能力。研究发现，最新的私有LLMs（如GPT-4.5和GPT-4.1）在MCQA测试中表现优异，准确率超过90%，甚至超越了人类的搜索引擎使用。然而，在自由形式回答中，模型的表现较低，没有一个模型得分超过75%。'}}}, {'id': 'https://huggingface.co/papers/2505.05621', 'title': 'A Preliminary Study for GPT-4o on Image Restoration', 'url': 'https://huggingface.co/papers/2505.05621', 'abstract': "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.", 'score': 4, 'issue_id': 3709, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '4fd37a23cd5db52f', 'authors': ['Hao Yang', 'Yan Yang', 'Ruikun Zhang', 'Liyuan Pan'], 'affiliations': ['Australian National University', 'Beijing Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.05621.jpg', 'data': {'categories': ['#optimization', '#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': '🖼️', 'ru': {'title': 'GPT-4o: Новый рубеж в восстановлении изображений', 'desc': 'Модель GPT-4o от OpenAI показала беспрецедентную производительность в генерации изображений. Исследователи провели систематическую оценку GPT-4o в различных задачах восстановления изображений. Хотя результаты визуально привлекательны, они часто страдают от структурной точности на уровне пикселей по сравнению с эталонными изображениями. Тем не менее, выходные данные GPT-4o могут служить мощными визуальными приорами, значительно улучшая производительность существующих сетей для удаления дымки, дождя и улучшения изображений при низкой освещенности.'}, 'en': {'title': 'Harnessing GPT-4o for Enhanced Image Restoration', 'desc': "The paper explores the capabilities of OpenAI's GPT-4o model in the field of image restoration, highlighting its ability to generate visually appealing images. Despite its impressive outputs, the model struggles with maintaining pixel-level accuracy, leading to issues like incorrect object positioning and altered image proportions. The authors demonstrate that GPT-4o can enhance existing image restoration techniques, particularly in tasks like dehazing and deraining, by providing valuable visual priors. This work aims to establish a foundation for integrating GPT-4o into future restoration workflows and encourages further research in image generation."}, 'zh': {'title': 'GPT-4o：图像修复的新动力', 'desc': 'OpenAI的GPT-4o模型结合了多模态输入和输出，展现了在图像生成方面的卓越性能。本文系统评估了GPT-4o在图像修复任务中的潜在影响，尽管其生成的修复图像在视觉上吸引人，但在像素级结构保真度上与真实图像相比存在问题。我们通过图像去雾、去雨和低光增强等案例研究，展示了GPT-4o的输出可以作为强大的视觉先验，显著提升现有去雾网络的性能。希望本研究能加速图像生成领域的创新，并将发布来自10个广泛使用的图像修复数据集的GPT-4o修复图像以支持进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2504.21467', 'title': 'Multiview Point Cloud Registration via Optimization in an Autoencoder\n  Latent Space', 'url': 'https://huggingface.co/papers/2504.21467', 'abstract': 'Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.', 'score': 0, 'issue_id': 3717, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '0bb5c596b88819cc', 'authors': ['Luc Vedrenne', 'Sylvain Faisan', 'Denis Fortun'], 'affiliations': ['ICube Laboratory, IMAGeS team, UMR 7357, CNRS, University of Strasbourg, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.21467.jpg', 'data': {'categories': ['#3d', '#cv', '#synthetic', '#optimization', '#open_source'], 'emoji': '🌀', 'ru': {'title': 'POLAR: Революция в многоракурсной регистрации облаков точек', 'desc': 'POLAR - это новый метод многоракурсной регистрации облаков точек, способный эффективно обрабатывать большое количество ракурсов. Он переносит задачу регистрации в латентное пространство предобученного автоэнкодера и использует специальную функцию потерь, учитывающую деградации данных. POLAR устойчив к высокому уровню деградаций и большим начальным углам между облаками точек. Метод значительно превосходит современные подходы на синтетических и реальных данных.'}, 'en': {'title': 'POLAR: Efficient and Robust Multiview Point Cloud Registration', 'desc': 'This paper presents POLAR, a new method for aligning multiple 3D point clouds, which is crucial in computer vision. Unlike traditional pairwise registration methods that struggle with scalability, POLAR efficiently handles many views and is robust against significant data degradation. The approach utilizes a pretrained autoencoder to transform the registration task into a latent space, allowing for better optimization. By incorporating a specialized loss function and a multistart optimization strategy, POLAR outperforms existing techniques on both synthetic and real datasets.'}, 'zh': {'title': 'POLAR：高效的多视角点云配准方法', 'desc': '点云刚性配准是3D计算机视觉中的一个基本问题。在多视角情况下，我们的目标是找到一组6D姿态来对齐一组物体。传统的基于成对配准的方法在视角数量增加时扩展性较差，而生成方法虽然克服了这一限制，但不适合处理大变换和高降级水平。本文提出了POLAR（点云潜在配准），它能够高效处理大量视角，同时对高降级和大初始角度具有鲁棒性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (23)', '#agents (52)', '#agi (16)', '#alignment (37)', '#architecture (76)', '#audio (14)', '#benchmark (180)', '#cv (65)', '#data (76)', '#dataset (153)', '#diffusion (41)', '#ethics (15)', '#games (34)', '#graphs (10)', '#hallucinations (24)', '#healthcare (18)', '#inference (50)', '#interpretability (46)', '#leakage (4)', '#long_context (14)', '#low_resource (20)', '#machine_translation (8)', '#math (32)', '#multilingual (23)', '#multimodal (127)', '#open_source (90)', '#optimization (183)', '#plp (5)', '#rag (16)', '#reasoning (153)', '#rl (78)', '#rlhf (37)', '#robotics (19)', '#science (19)', '#security (15)', '#small_models (14)', '#story_generation (2)', '#survey (19)', '#synthetic (28)', '#training (206)', '#transfer_learning (30)', '#video (32)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-26 00:56',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-26 00:56')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-26 00:56')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    