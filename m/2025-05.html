
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 159 papers. May 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Май 2025</span> | <span id="title-articles-count">159 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-04.html">⬅️ <span id="prev-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-06.html">➡️ <span id="next-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Май 2025', 'en': 'May 2025', 'zh': '5月2025年'};
        let feedDateNext = {'ru': '06.2025', 'en': '06/2025', 'zh': '6月2025年'};
        let feedDatePrev = {'ru': '04.2025', 'en': '04/2025', 'zh': '4月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.20438', 'title': 'PixelHacker: Image Inpainting with Structural and Semantic Consistency', 'url': 'https://huggingface.co/papers/2504.20438', 'abstract': 'Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/PixelHacker.', 'score': 23, 'issue_id': 3584, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '987ce511e3c86e06', 'authors': ['Ziyang Xu', 'Kangsheng Duan', 'Xiaolei Shen', 'Zhifeng Ding', 'Wenyu Liu', 'Xiaohu Ruan', 'Xiaoxin Chen', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'VIVO AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.20438.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#open_source', '#optimization', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'PixelHacker: Революционный подход к восстановлению изображений с помощью латентных категорий', 'desc': 'В статье представлен новый подход к задаче восстановления изображений под названием PixelHacker. Авторы разработали метод латентного категориального управления, используя большой набор данных из 14 миллионов пар изображение-маска с аннотациями переднего и заднего плана. PixelHacker применяет диффузионную модель с внедрением признаков через линейное внимание. Эксперименты показывают, что PixelHacker превосходит современные методы на различных наборах данных, демонстрируя высокую согласованность структуры и семантики изображений.'}, 'en': {'title': 'PixelHacker: Revolutionizing Image Inpainting with Latent Categories Guidance', 'desc': 'This paper presents a new approach to image inpainting called PixelHacker, which aims to improve the quality of generated images by addressing issues with complex structures and semantics. The authors introduce a large dataset of 14 million image-mask pairs to train their model, focusing on distinguishing between foreground and background categories. They utilize a diffusion-based model that incorporates linear attention to enhance the denoising process, ensuring better consistency in texture and color. Experimental results demonstrate that PixelHacker significantly outperforms existing state-of-the-art methods across various datasets, achieving superior image restoration results.'}, 'zh': {'title': 'PixelHacker：图像修复的新突破', 'desc': '图像修复是图像编辑与生成之间的一个重要研究领域。最近的最先进方法探索了新颖的注意力机制、轻量级架构和上下文感知建模，取得了显著的性能。然而，这些方法在处理复杂结构和语义时常常面临挑战，导致生成的图像出现伪影和不当生成。为了解决这个问题，我们设计了一种简单而有效的修复范式，称为潜在类别引导，并提出了一种基于扩散的模型PixelHacker。'}}}, {'id': 'https://huggingface.co/papers/2505.01079', 'title': 'Improving Editability in Image Generation with Layer-wise Memory', 'url': 'https://huggingface.co/papers/2505.01079', 'abstract': 'Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.', 'score': 18, 'issue_id': 3582, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'e1aa83ea7926943e', 'authors': ['Daneul Kim', 'Jaeah Lee', 'Jaesik Park'], 'affiliations': ['Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01079.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Умное последовательное редактирование изображений: сохраняем прошлое, добавляем новое', 'desc': 'Статья представляет новый подход к последовательному редактированию изображений с использованием нейронных сетей. Авторы предлагают метод, позволяющий сохранять предыдущие изменения и естественно интегрировать новые элементы в существующий контент. Ключевые инновации включают использование приблизительных масок, послойную память для хранения латентных представлений и применение техник Background Consistency Guidance и Multi-Query Disentanglement. Эффективность метода подтверждается экспериментами на новом наборе данных с метриками семантического выравнивания.'}, 'en': {'title': 'Seamless Sequential Image Editing with Context Preservation', 'desc': 'This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input.'}, 'zh': {'title': '实现自然连续的图像编辑', 'desc': '本论文探讨了图像编辑中的多次连续编辑问题，现有方法在处理多个对象的修改时存在困难，尤其是在保持之前编辑内容的同时自然地融入新对象。我们提出了两项关键方案：一是支持粗略的掩膜输入，以保留现有内容并自然整合新元素；二是支持多次修改的一致性编辑。我们的框架通过层级记忆存储先前编辑的潜在表示和提示嵌入，利用背景一致性引导保持场景的连贯性。实验结果表明，我们的方法在迭代图像编辑任务中表现优越，用户只需提供粗略掩膜即可实现高质量的编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2504.21117', 'title': 'Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts', 'url': 'https://huggingface.co/papers/2504.21117', 'abstract': 'Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.', 'score': 11, 'issue_id': 3587, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '2a43e27932acf80e', 'authors': ['Hanhua Hong', 'Chenghao Xiao', 'Yang Wang', 'Yiqi Liu', 'Wenge Rong', 'Chenghua Lin'], 'affiliations': ['Beihang University', 'Durham University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2504.21117.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#benchmark', '#interpretability'], 'emoji': '🔄', 'ru': {'title': 'Автоматическая генерация промптов для надежной оценки NLG систем', 'desc': 'Статья посвящена проблеме оценки систем генерации естественного языка (NLG) и предлагает новый метод на основе обучения инверсии. Этот подход позволяет автоматически создавать эффективные промпты для оценки, специфичные для конкретной модели, используя только один образец. Метод устраняет необходимость в трудоемкой ручной разработке промптов, повышая эффективность и надежность оценки. Работа открывает новое направление для более надежной и эффективной оценки с использованием языковых моделей (LLM).'}, 'en': {'title': 'Revolutionizing NLG Evaluation with Inversion Learning', 'desc': 'This paper addresses the difficulties in evaluating natural language generation (NLG) systems, particularly the inconsistencies and biases in human evaluations. It introduces an inversion learning method that creates effective prompts for evaluating models by learning from their outputs. This approach allows for automatic generation of tailored evaluation prompts, requiring only one sample, which enhances efficiency. The proposed method aims to improve the robustness of LLM-based evaluations, paving the way for more standardized assessment in NLG.'}, 'zh': {'title': '提升自然语言生成评估的效率与稳健性', 'desc': '评估自然语言生成（NLG）系统是一个具有挑战性的任务，因为有效输出的多样性使得标准化评估变得困难。虽然人工评估被认为是金标准，但其存在不一致性、缺乏标准化和人口偏见等问题，限制了可重复性。基于大型语言模型（LLM）的评估提供了一种可扩展的替代方案，但对提示设计非常敏感，微小的变化可能导致显著的差异。我们提出了一种反演学习方法，可以有效地从模型输出反向映射到输入指令，从而自动生成高效的、特定于模型的评估提示，提升了评估的效率和稳健性。'}}}, {'id': 'https://huggingface.co/papers/2505.00949', 'title': 'Llama-Nemotron: Efficient Reasoning Models', 'url': 'https://huggingface.co/papers/2505.00949', 'abstract': 'We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.', 'score': 8, 'issue_id': 3587, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'cbc28025b0c6bde3', 'authors': ['Akhiad Bercovich', 'Itay Levy', 'Izik Golan', 'Mohammad Dabbah', 'Ran El-Yaniv', 'Omri Puny', 'Ido Galil', 'Zach Moshe', 'Tomer Ronen', 'Najeeb Nabwani', 'Ido Shahaf', 'Oren Tropp', 'Ehud Karpas', 'Ran Zilberstein', 'Jiaqi Zeng', 'Soumye Singhal', 'Alexander Bukharin', 'Yian Zhang', 'Tugrul Konuk', 'Gerald Shen', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Yoshi Suhara', 'Olivier Delalleau', 'Zijia Chen', 'Zhilin Wang', 'David Mosallanezhad', 'Adi Renduchintala', 'Haifeng Qian', 'Dima Rekesh', 'Fei Jia', 'Somshubra Majumdar', 'Vahid Noroozi', 'Wasi Uddin Ahmad', 'Sean Narenthiran', 'Aleksander Ficek', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Igor Gitman', 'Ivan Moshkov', 'Wei Du', 'Shubham Toshniwal', 'George Armstrong', 'Branislav Kisacanin', 'Matvei Novikov', 'Daria Gitman', 'Evelina Bakhturina', 'Jane Polak Scowcroft', 'John Kamalu', 'Dan Su', 'Kezhi Kong', 'Markus Kliegl', 'Rabeeh Karimi', 'Ying Lin', 'Sanjeev Satheesh', 'Jupinder Parmar', 'Pritam Gundecha', 'Brandon Norick', 'Joseph Jennings', 'Shrimai Prabhumoye', 'Syeda Nahida Akter', 'Mostofa Patwary', 'Abhinav Khattar', 'Deepak Narayanan', 'Roger Waleffe', 'Jimmy Zhang', 'Bor-Yiing Su', 'Guyue Huang', 'Terry Kong', 'Parth Chadha', 'Sahil Jain', 'Christine Harvey', 'Elad Segal', 'Jining Huang', 'Sergey Kashirsky', 'Robert McQueen', 'Izzy Putterman', 'George Lam', 'Arun Venkatesan', 'Sherry Wu', 'Vinh Nguyen', 'Manoj Kilaru', 'Andrew Wang', 'Anna Warno', 'Abhilash Somasamudramath', 'Sandip Bhaskar', 'Maka Dong', 'Nave Assaf', 'Shahar Mor', 'Omer Ullman Argov', 'Scot Junkin', 'Oleksandr Romanenko', 'Pedro Larroy', 'Monika Katariya', 'Marco Rovinelli', 'Viji Balas', 'Nicholas Edelman', 'Anahita Bhiwandiwalla', 'Muthu Subramaniam', 'Smita Ithape', 'Karthik Ramamoorthy', 'Yuting Wu', 'Suguna Varshini Velury', 'Omri Almog', 'Joyjit Daw', 'Denys Fridman', 'Erick Galinkin', 'Michael Evans', 'Katherine Luna', 'Leon Derczynski', 'Nikki Pope', 'Eileen Long', 'Seth Schneider', 'Guillermo Siman', 'Tomasz Grzegorzek', 'Pablo Ribalta', 'Monika Katariya', 'Joey Conway', 'Trisha Saar', 'Ann Guan', 'Krzysztof Pawelec', 'Shyamala Prayaga', 'Oleksii Kuchaiev', 'Boris Ginsburg', 'Oluwatobi Olabiyi', 'Kari Briski', 'Jonathan Cohen', 'Bryan Catanzaro', 'Jonah Alben', 'Yonatan Geifman', 'Eric Chung'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00949.jpg', 'data': {'categories': ['#agi', '#training', '#rl', '#open_source', '#architecture', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Открытые модели рассуждений нового поколения', 'desc': 'Представлена серия моделей Llama-Nemotron - семейство гетерогенных моделей рассуждений с открытым исходным кодом. Модели доступны в трех размерах (8B, 49B, 253B) и обеспечивают высокую производительность и эффективность использования памяти. Обучение включает нейроархитектурный поиск, дистилляцию знаний и дообучение, а также этап постобработки с акцентом на рассуждения. Модели поддерживают динамическое переключение между режимами обычного чата и рассуждений.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Efficiency', 'desc': 'The Llama-Nemotron series introduces a new family of reasoning models designed for efficient inference and strong reasoning capabilities. These models come in three sizes, allowing flexibility for different applications while maintaining competitive performance against leading models. The training process involves advanced techniques like neural architecture search, knowledge distillation, and reinforcement learning to enhance reasoning abilities. Additionally, these models are open-source, providing resources for further research and development in the machine learning community.'}, 'zh': {'title': '开放推理模型，提升推理效率！', 'desc': 'Llama-Nemotron系列模型是一种开放的异构推理模型，具有卓越的推理能力和高效的推理性能。该系列包括三种不同规模的模型：Nano（8B）、Super（49B）和Ultra（253B），在推理速度和内存效率上优于现有的最先进模型。模型的训练过程采用了神经架构搜索、知识蒸馏和持续预训练，最后通过监督微调和大规模强化学习进行推理专注的后训练阶段。Llama-Nemotron模型是首个支持动态推理切换的开源模型，用户可以在推理过程中在标准聊天模式和推理模式之间切换。'}}}, {'id': 'https://huggingface.co/papers/2505.00174', 'title': 'Real-World Gaps in AI Governance Research', 'url': 'https://huggingface.co/papers/2505.00174', 'abstract': 'Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.', 'score': 8, 'issue_id': 3582, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '7618edbafcee6b13', 'authors': ['Ilan Strauss', 'Isobel Moure', "Tim O'Reilly", 'Sruly Rosenblat'], 'affiliations': ['AI Disclosures Project, Social Science Research Council', 'Institute for Innovation and Public Purpose, University College London', 'OReilly Media'], 'pdf_title_img': 'assets/pdf/title_img/2505.00174.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#alignment', '#healthcare', '#hallucinations', '#data'], 'emoji': '🔍', 'ru': {'title': 'Корпоративные исследования ИИ: пробелы в безопасности и необходимость прозрачности', 'desc': 'Статья анализирует 1178 работ по безопасности и надежности из 9439 статей по генеративному ИИ за период с января 2020 по март 2025 года. Исследователи сравнивают результаты ведущих компаний и университетов в области ИИ. Обнаружено, что корпоративные исследования ИИ все больше концентрируются на предварительном развертывании, включая выравнивание моделей и тестирование, в то время как внимание к проблемам этапа развертывания, таким как смещение модели, ослабевает. Авторы рекомендуют расширить доступ внешних исследователей к данным развертывания и систематическое наблюдение за поведением ИИ на рынке.'}, 'en': {'title': 'Bridging the Gap: Enhancing AI Safety in Deployment', 'desc': 'This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps.'}, 'zh': {'title': '关注人工智能部署阶段的研究缺口', 'desc': '本研究分析了1178篇安全性和可靠性论文与9439篇生成式人工智能论文，比较了主要人工智能公司和大学的研究成果。研究发现，企业的人工智能研究越来越集中在模型对齐和测试评估等预部署领域，而对部署阶段问题如模型偏见的关注有所减少。高风险部署领域（如医疗、金融、虚假信息等）存在显著的研究空白。为了改善对已部署人工智能的可观察性，建议扩大外部研究人员对部署数据的访问，并系统化市场中人工智能行为的可观察性。'}}}, {'id': 'https://huggingface.co/papers/2505.00023', 'title': 'CORG: Generating Answers from Complex, Interrelated Contexts', 'url': 'https://huggingface.co/papers/2505.00023', 'abstract': 'In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.', 'score': 5, 'issue_id': 3582, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '46da290a5c894311', 'authors': ['Hyunji Lee', 'Franck Dernoncourt', 'Trung Bui', 'Seunghyun Yoon'], 'affiliations': ['Adobe Research', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.00023.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#graphs', '#architecture', '#data'], 'emoji': '🧠', 'ru': {'title': 'CORG: Умная организация контекста для улучшения работы языковых моделей', 'desc': 'Статья представляет новый фреймворк под названием Context Organizer (CORG) для обработки сложных взаимосвязей между контекстами в корпусах реального мира. CORG организует множественные контексты в независимо обрабатываемые группы, что позволяет эффективно находить все релевантные ответы и обеспечивать устранение неоднозначности. Фреймворк состоит из трех ключевых компонентов: конструктора графов, ранжировщика и агрегатора. Результаты показывают, что CORG эффективно балансирует производительность и эффективность, превосходя существующие методы группировки.'}, 'en': {'title': 'Organizing Contexts for Better Language Understanding', 'desc': 'This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods.'}, 'zh': {'title': '上下文组织，提升模型效率与准确性', 'desc': '在现实世界的语料库中，知识经常在文档中重复出现，但由于命名模糊、信息过时或错误，导致上下文之间存在复杂的相互关系。以往的研究表明，语言模型在处理这些复杂性时通常只关注单一因素。我们将这些关系分为四种类型：干扰、模糊、反事实和重复。为了解决这些问题，我们提出了上下文组织器（CORG），它将多个上下文组织成独立处理的组，从而提高模型的效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.00562', 'title': 'TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching', 'url': 'https://huggingface.co/papers/2505.00562', 'abstract': "Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'bf5b246f5848fa6e', 'authors': ['Yue Meng', 'Chuchu Fan'], 'affiliations': ['Department of Aeronautics and Astronautics, MIT, Cambridge, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00562.jpg', 'data': {'categories': ['#dataset', '#inference', '#agents', '#robotics', '#graphs', '#optimization'], 'emoji': '⏱️', 'ru': {'title': 'Графовые нейросети для эффективного решения задач темпоральной логики', 'desc': 'Статья представляет TeLoGraF - новый метод для решения задач с темпоральной логикой сигналов (STL). Авторы используют графовые нейронные сети и технику flow-matching для обучения на разнообразном наборе STL-спецификаций. Эксперименты проводились в пяти симуляционных средах, от простых 2D-моделей до сложных роботов. Результаты показывают превосходство TeLoGraF над базовыми методами по скорости и универсальности применения.'}, 'en': {'title': 'TeLoGraF: Fast and Robust Solutions for Complex Temporal Logic Tasks', 'desc': 'This paper introduces TeLoGraF, a novel approach that leverages Graph Neural Networks (GNN) to effectively learn solutions for complex tasks defined by signal temporal logic (STL) specifications. The authors address the limitations of previous methods that relied on fixed STL templates by creating a diverse dataset of 200,000 STL specifications paired with demonstrations. Through extensive experiments across various simulation environments, TeLoGraF demonstrates superior performance in STL satisfaction rates and significantly faster inference times compared to traditional STL planning algorithms. Additionally, the graph-encoding technique shows robustness in handling complex and out-of-distribution STL specifications, making it a versatile tool for real-world applications.'}, 'zh': {'title': 'TeLoGraF：高效解决复杂时序逻辑任务的创新方法', 'desc': '本文提出了一种新的方法TeLoGraF，用于解决复杂任务的信号时序逻辑（STL）规范。我们利用图神经网络（GNN）编码器和流匹配技术，学习通用STL规范的解决方案。通过收集20万个配对示例，我们在多个仿真环境中进行了广泛实验，结果表明该方法在STL满足率上优于其他基线。与传统的STL规划算法相比，我们的方法在推理速度上快10到100倍，并且能够适应任何系统动态。'}}}, {'id': 'https://huggingface.co/papers/2504.20859', 'title': 'X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2504.20859', 'abstract': "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '2102f697cfc2375e', 'authors': ['Guy Hadad', 'Haggai Roitman', 'Yotam Eshel', 'Bracha Shapira', 'Lior Rokach'], 'affiliations': ['Ben-Gurion University of the Negev Beer Sheva, Israel', 'eBay Netanya, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.20859.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#low_resource', '#training', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'X-Cross: эффективные кросс-доменные рекомендации без обширного переобучения', 'desc': 'Статья представляет модель X-Cross для кросс-доменных последовательных рекомендаций, использующую несколько доменно-специфичных языковых моделей с низкоранговыми адаптерами (LoRA). X-Cross динамически улучшает представления каждой исходной языковой модели, интегрируя знания из всех других моделей. Эксперименты на данных Amazon показывают, что X-Cross достигает производительности, сравнимой с моделью, дообученной с LoRA, используя лишь 25% дополнительных параметров. Модель демонстрирует надежную производительность в кросс-доменных задачах, требуя на 50-75% меньше данных для эффективной донастройки.'}, 'en': {'title': 'X-Cross: Efficient Cross-Domain Recommendations with Minimal Data', 'desc': "The paper introduces 'X-Cross', a new model designed for cross-domain sequential recommendations that can quickly adapt to new product categories without extensive retraining. It utilizes multiple domain-specific language models, each fine-tuned with low-rank adapters (LoRA), to enhance the recommendation process. By refining the representations of these models layer by layer, X-Cross effectively integrates knowledge from different domains while maintaining their unique characteristics. The model shows strong performance on Amazon datasets, requiring significantly less fine-tuning data and parameters compared to traditional methods, making it efficient for data-limited scenarios."}, 'zh': {'title': 'X-Cross：高效的跨领域推荐解决方案', 'desc': '随着新产品的不断涌现，推荐系统需要快速适应新领域，而无需大量重新训练。本文提出了“X-Cross”模型，这是一种新颖的跨领域序列推荐模型，通过整合多个特定领域的语言模型来推荐新领域的产品。X-Cross通过逐层操作动态地优化每个源语言模型的表示，确保在跨领域适应时保留领域特有的细微差别。实验结果表明，X-Cross在跨领域任务中表现出色，且所需的微调数据量显著低于传统方法。'}}}, {'id': 'https://huggingface.co/papers/2505.03318', 'title': 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.03318', 'abstract': "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.", 'score': 63, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f0871f80f0b8fdd9', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.03318.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#multimodal', '#training', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение надежности мультимодальных моделей вознаграждения через цепочки рассуждений', 'desc': 'В статье представлен UnifiedReward-Think - первая унифицированная мультимодальная модель вознаграждения, основанная на цепочках рассуждений (CoT). Модель способна проводить многомерные пошаговые рассуждения для задач визуального понимания и генерации. Авторы используют подход обучения с подкреплением для выявления и стимулирования скрытых способностей модели к сложным рассуждениям. Эксперименты показывают превосходство предложенной модели в различных задачах визуального вознаграждения.'}, 'en': {'title': 'Empowering Vision Models with Long-Chain Reasoning', 'desc': 'This paper introduces UnifiedReward-Think, a novel multimodal reward model that enhances the alignment of vision models with human preferences through long-chain reasoning. By integrating explicit chains of thought (CoT) into the reward reasoning process, the model improves the accuracy and reliability of reward signals. The approach involves a two-step training process: first, distilling reasoning from a small dataset, and then fine-tuning with large-scale multimodal preference data. Experimental results show that this method significantly outperforms existing models in various vision tasks, demonstrating its effectiveness in complex reasoning scenarios.'}, 'zh': {'title': '长链思维提升多模态奖励模型的可靠性', 'desc': '最近在多模态奖励模型（RMs）方面的进展显示出将视觉模型与人类偏好对齐的潜力。然而，目前的RMs通常只能提供直接响应或进行浅层推理，导致奖励信号不准确。我们认为，将明确的长链思维（CoT）纳入奖励推理过程可以显著增强其可靠性和稳健性。本文提出了UnifiedReward-Think，这是第一个统一的基于CoT的多模态奖励模型，能够进行多维度、逐步的长链推理，适用于视觉理解和生成奖励任务。'}}}, {'id': 'https://huggingface.co/papers/2505.03335', 'title': 'Absolute Zero: Reinforced Self-play Reasoning with Zero Data', 'url': 'https://huggingface.co/papers/2505.03335', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.', 'score': 59, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'b53e736d1884218d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rlhf', '#training', '#rl', '#math', '#optimization', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Самообучающийся ИИ: революция в обучении с подкреплением', 'desc': 'Статья представляет новую парадигму обучения с подкреплением с проверяемыми наградами (RLVR) под названием Absolute Zero. В рамках этой парадигмы модель самостоятельно генерирует задачи для максимизации собственного прогресса обучения, не полагаясь на внешние данные. Авторы представляют систему Absolute Zero Reasoner (AZR), которая развивает свою учебную программу и способности к рассуждению, используя исполнитель кода для проверки предложенных задач и ответов. Несмотря на отсутствие внешних данных при обучении, AZR достигает лучших результатов в задачах кодирования и математических рассуждений по сравнению с существующими моделями.'}, 'en': {'title': 'Self-Learning AI: No Data, No Problem!', 'desc': 'This paper introduces a new approach in reinforcement learning called Absolute Zero, which allows a model to learn and improve its reasoning skills without needing external data or human supervision. The proposed Absolute Zero Reasoner (AZR) autonomously generates tasks that enhance its learning and validates its own reasoning through a code executor. This self-sufficient learning method leads to state-of-the-art performance in coding and mathematical reasoning tasks, surpassing models that rely on large datasets of human-created examples. The findings suggest that AZR can adapt to different model sizes and types, showcasing its versatility and potential for future AI development.'}, 'zh': {'title': '绝对零：自我进化的推理模型', 'desc': '强化学习与可验证奖励（RLVR）在提升大型语言模型的推理能力方面表现出色，能够直接从结果导向的奖励中学习。最近的RLVR研究在零设置下运行，避免了对推理过程的监督，但仍依赖于人工策划的问题和答案集合进行训练。由于高质量人类生成示例的稀缺性，依赖人类监督的长期可扩展性受到质疑。为了解决这些问题，我们提出了一种新的RLVR范式，称为绝对零（Absolute Zero），该范式下的模型能够自我提出任务以最大化学习进展，并通过解决这些任务来提升推理能力，而无需依赖任何外部数据。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2505.03005', 'title': 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale', 'url': 'https://huggingface.co/papers/2505.03005', 'abstract': "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper", 'score': 22, 'issue_id': 3625, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '0fe1c0b1575b6708', 'authors': ['Daniel Goldstein', 'Eric Alcaide', 'Janna Lu', 'Eugene Cheah'], 'affiliations': ['Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'EleutherAI', 'George Mason University', 'Recursal AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.03005.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#inference', '#benchmark', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Эффективное преобразование трансформеров в модели с линейным вниманием', 'desc': 'В статье представлен метод Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) для быстрого преобразования трансформеров с софтмакс-вниманием в модели декодеров с линейным вниманием. Авторы разработали две новые архитектуры на основе RWKV и конвертировали популярные модели Qwen2.5 размером 7B, 32B и 72B. Процесс конвертации требует всего 350-700 млн токенов, что составляет менее 0,005% от количества токенов, использованных для обучения исходных моделей. Полученные модели с линейным вниманием демонстрируют высокую производительность на стандартных бенчмарках.'}, 'en': {'title': 'Transforming Transformers: Efficient Linear Attention Models with RADLADS', 'desc': 'The paper introduces Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a method for transforming softmax attention transformers into efficient linear attention models. This conversion process is highly efficient, requiring only a small fraction of the original training data, specifically 350-700M tokens. Despite the reduced training cost, the resulting 72B linear attention model maintains performance levels comparable to its transformer counterparts. The authors also present new RWKV-variant architectures and make their models available on HuggingFace, demonstrating state-of-the-art results on standard benchmarks for linear attention models.'}, 'zh': {'title': '快速转换，线性注意力的未来', 'desc': '我们提出了一种快速注意力蒸馏到线性注意解码器的协议（RADLADS），可以迅速将软最大注意力变换器转换为线性注意解码模型。我们的转换过程只需350-700M个标记，远低于原始教师模型训练所需的0.005%的标记数量。转换为我们的72B线性注意模型的成本不到2000美元，但推理质量仍接近原始变换器。我们在标准基准测试中实现了同类最佳的下游性能，并将所有模型发布在HuggingFace上。'}}}, {'id': 'https://huggingface.co/papers/2505.03730', 'title': 'FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios', 'url': 'https://huggingface.co/papers/2505.03730', 'abstract': 'Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/', 'score': 19, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '2329fe6d2462c9c9', 'authors': ['Shiyi Zhang', 'Junhao Zhuang', 'Zhaoyang Zhang', 'Ying Shan', 'Yansong Tang'], 'affiliations': ['Tencent ARC Lab, China', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03730.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'Гибкий перенос действий между разными субъектами и сценариями', 'desc': 'Статья представляет FlexiAct - метод для переноса действий с референсного видео на произвольное целевое изображение. В отличие от существующих подходов, FlexiAct позволяет варьировать компоновку, ракурс и скелетную структуру между субъектом референсного видео и целевым изображением, сохраняя при этом идентичность. Для достижения этой цели авторы вводят RefAdapter - легковесный адаптер, обусловленный изображением, который превосходит существующие методы в балансировке согласованности внешнего вида и структурной гибкости. Также предлагается FAE (Frequency-aware Action Extraction) для извлечения действий непосредственно в процессе шумоподавления.'}, 'en': {'title': 'FlexiAct: Flexible Action Transfer for Diverse Video Customization', 'desc': 'The paper presents FlexiAct, a novel approach for customizing action videos by transferring actions from a reference video to a target image, regardless of differences in layout, viewpoint, and skeletal structure. This method addresses the limitations of existing techniques that require strict spatial consistency, allowing for greater adaptability across various subjects and scenarios. FlexiAct utilizes a lightweight image-conditioned adapter called RefAdapter to ensure identity consistency while adapting spatial structures. Additionally, it introduces Frequency-aware Action Extraction (FAE) to enhance action extraction during the denoising process, achieving superior results in maintaining both appearance and structural flexibility.'}, 'zh': {'title': '灵活的动作转移，打破空间限制', 'desc': '本文提出了一种名为FlexiAct的方法，用于根据输入控制信号生成视频，允许在不同布局、视角和骨架结构之间进行动作转移。与现有方法不同，FlexiAct能够在保持身份一致性的同时，适应目标图像的空间结构变化。为实现这一目标，文章引入了RefAdapter，一个轻量级的图像条件适配器，能够在外观一致性和结构灵活性之间取得良好平衡。此外，提出的FAE方法在去噪过程中直接提取动作，克服了传统方法的局限性。'}}}, {'id': 'https://huggingface.co/papers/2505.02922', 'title': 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference', 'url': 'https://huggingface.co/papers/2505.02922', 'abstract': 'The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.', 'score': 18, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'd7e3545dcade10b4', 'authors': ['Yaoqi Chen', 'Jinkai Zhang', 'Baotong Lu', 'Qianxi Zhang', 'Chengruidong Zhang', 'Jingjia Luo', 'Di Liu', 'Huiqiang Jiang', 'Qi Chen', 'Jing Liu', 'Bailu Ding', 'Xiao Yan', 'Jiawei Jiang', 'Chen Chen', 'Mingxing Zhang', 'Yuqing Yang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02922.jpg', 'data': {'categories': ['#long_context', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'RetroInfer: революция в эффективности вывода LLM с длинным контекстом', 'desc': 'RetroInfer - это новая система, которая переосмысливает кэш ключ-значение как систему хранения векторов для ускорения вывода LLM с длинным контекстом. В основе системы лежит волновой индекс, использующий разреженность внимания для эффективного извлечения критических токенов. RetroInfer также включает волновой буфер для координации размещения кэша и оптимизации вычислений между GPU и CPU. Эксперименты показывают ускорение до 4.5 раз по сравнению с полным вниманием в пределах памяти GPU и до 10.5 раз по сравнению с базовыми методами разреженного внимания при сохранении точности.'}, 'en': {'title': 'Accelerating Long-Context Inference with RetroInfer', 'desc': 'This paper introduces RetroInfer, a system designed to improve the efficiency of large language models (LLMs) during inference by addressing GPU memory and bandwidth limitations. It innovatively redefines the key-value (KV) cache as a vector storage system that leverages attention sparsity to speed up processing of long contexts. The core component, the wave index, utilizes advanced techniques for token retrieval, ensuring both efficiency and accuracy. Additionally, the wave buffer optimizes the coordination of KV cache and computation, achieving significant speed improvements while maintaining high model accuracy.'}, 'zh': {'title': '高效推理，突破上下文限制！', 'desc': '随着大型语言模型（LLMs）上下文长度的增加，推理效率面临显著挑战，主要是由于GPU内存和带宽的限制。我们提出了RetroInfer，这是一种新颖的系统，将关键值（KV）缓存重新概念化为向量存储系统，利用内在的注意力稀疏性来加速长上下文LLM推理。其核心是波动索引（wave index），一种注意力感知向量索引，能够通过三方注意力近似、精度受限的注意力估计和分段聚类等技术高效准确地检索关键标记。与以往在标记选择和硬件协调上存在困难的稀疏性方法不同，RetroInfer在不影响模型准确性的情况下提供了强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02872', 'title': 'Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading', 'url': 'https://huggingface.co/papers/2505.02872', 'abstract': "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.", 'score': 14, 'issue_id': 3629, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'aae5b0f63189eb32', 'authors': ['Cfir Avraham Hadar', 'Omer Shubi', 'Yoav Meiri', 'Yevgeni Berzak'], 'affiliations': ['Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.02872.jpg', 'data': {'categories': ['#multimodal', '#science', '#dataset', '#benchmark', '#interpretability', '#agi'], 'emoji': '👁️', 'ru': {'title': 'Расшифровка целей чтения по движениям глаз с помощью ИИ', 'desc': 'В этой статье исследуется возможность автоматического определения целей чтения на основе движений глаз. Авторы разработали задачи классификации и реконструкции целей, используя крупномасштабные данные отслеживания движений глаз при чтении на английском языке. Они создали и сравнили несколько дискриминативных и генеративных мультимодальных языковых моделей (LLM), объединяющих движения глаз и текст. Эксперименты показали значительный успех в обеих задачах, что указывает на способность LLM извлекать ценную информацию о целях чтения из движений глаз.'}, 'en': {'title': 'Decoding Reading Goals from Eye Movements with LLMs', 'desc': "This paper explores how eye movements during reading can reveal a reader's specific goals, such as information seeking. The authors introduce new tasks for classifying and reconstructing these goals using large-scale eye tracking data. They develop and evaluate various multimodal large language models (LLMs) that integrate eye movement data with text to improve goal understanding. The results indicate that these models can effectively decode readers' intentions, demonstrating the potential of LLMs in understanding reading behavior."}, 'zh': {'title': '从眼动数据解码阅读目标的创新研究', 'desc': '本研究首次探讨了如何从阅读时的眼动数据自动解码开放式阅读目标。我们引入了目标分类和目标重建任务，并建立了评估框架，使用了大规模的眼动追踪数据。通过结合眼动和文本信息，我们开发并比较了多种判别式和生成式的多模态大语言模型（LLMs）。实验结果表明，这些模型在提取读者的文本特定目标方面表现出显著的成功。'}}}, {'id': 'https://huggingface.co/papers/2505.02214', 'title': 'An Empirical Study of Qwen3 Quantization', 'url': 'https://huggingface.co/papers/2505.02214', 'abstract': "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.", 'score': 12, 'issue_id': 3627, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'be32ffc34f30d354', 'authors': ['Xingyu Zheng', 'Yuye Li', 'Haoran Chu', 'Yue Feng', 'Xudong Ma', 'Jie Luo', 'Jinyang Guo', 'Haotong Qin', 'Michele Magno', 'Xianglong Liu'], 'affiliations': ['Beihang University', 'ETH Zürich', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02214.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source', '#low_resource'], 'emoji': '🔬', 'ru': {'title': 'Квантование Qwen3: баланс между эффективностью и точностью', 'desc': 'Исследование посвящено оценке эффективности квантования модели Qwen3, одной из ведущих открытых больших языковых моделей (LLM). Авторы применили 5 классических методов пост-тренировочного квантования с различной битовой глубиной от 1 до 8 бит. Результаты показали, что Qwen3 сохраняет высокую производительность при умеренном квантовании, но значительно теряет в качестве при сверхнизкой точности. Это исследование подчеркивает необходимость дальнейших разработок для минимизации потерь производительности при экстремальном квантовании LLM.'}, 'en': {'title': "Unlocking Efficiency: Evaluating Qwen3's Performance Under Quantization", 'desc': "The Qwen series represents a significant advancement in open-source Large Language Models (LLMs), particularly with the introduction of Qwen3, which excels in natural language understanding tasks. This paper investigates the effects of low-bit quantization on Qwen3's performance, focusing on how different quantization techniques impact its robustness. By evaluating five classic post-training quantization methods across various bit-widths, the study reveals that while Qwen3 performs well at moderate bit-widths, it struggles with linguistic tasks at ultra-low precision. The findings highlight the challenges of compressing LLMs and suggest the need for further research to improve quantization strategies without sacrificing model accuracy."}, 'zh': {'title': '探索Qwen3的量化挑战与机遇', 'desc': 'Qwen系列是一个领先的开源大型语言模型（LLM），在自然语言理解任务中表现出色。最近发布的Qwen3在多个基准测试中表现优异，吸引了在资源受限环境中高效部署的关注。本文系统评估了Qwen3在不同量化设置下的鲁棒性，探讨了压缩这一先进模型的机遇与挑战。研究发现，尽管在中等位宽下Qwen3的性能仍具竞争力，但在超低精度下语言任务的表现显著下降，强调了LLM压缩中的持续难题。'}}}, {'id': 'https://huggingface.co/papers/2505.03735', 'title': 'Multi-Agent System for Comprehensive Soccer Understanding', 'url': 'https://huggingface.co/papers/2505.03735', 'abstract': 'Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.', 'score': 9, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f62fbd87d3f6f548', 'authors': ['Jiayuan Rao', 'Zifeng Li', 'Haoning Wu', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['SAI, Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03735.jpg', 'data': {'categories': ['#open_source', '#survey', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#agents'], 'emoji': '⚽', 'ru': {'title': 'Революция в ИИ-анализе футбола: от знаний к пониманию', 'desc': 'Статья представляет комплексный подход к пониманию футбола с помощью искусственного интеллекта. Авторы создали SoccerWiki - первую крупномасштабную мультимодальную базу знаний о футболе, и SoccerBench - обширный набор тестовых заданий для оценки понимания футбола ИИ-системами. Также они разработали SoccerAgent - мультиагентную систему, которая декомпозирует сложные вопросы о футболе путем совместных рассуждений. Исследование демонстрирует превосходство предложенного агентного подхода над современными мультимодальными языковыми моделями в задачах понимания футбола.'}, 'en': {'title': 'Revolutionizing Soccer Understanding with AI', 'desc': 'This paper presents a new framework for understanding soccer using AI, addressing the limitations of previous research that focused on narrow tasks. The authors introduce SoccerWiki, a large multimodal knowledge base that contains detailed information about various aspects of soccer, enabling better reasoning. They also create SoccerBench, a comprehensive benchmark with thousands of multimodal question-answer pairs to evaluate soccer understanding tasks. Finally, the paper introduces SoccerAgent, a multi-agent system that collaborates to answer complex soccer questions, demonstrating improved performance through extensive evaluations.'}, 'zh': {'title': '全面提升足球理解的智能框架', 'desc': '本论文提出了一个全面的足球理解框架，以填补现有研究的空白。我们构建了SoccerWiki，这是第一个大规模的多模态足球知识库，整合了关于球员、球队、裁判和场馆的丰富领域知识。我们还推出了SoccerBench，这是最大的足球特定基准，包含约10,000个标准化的多模态多选问答对，涵盖13个不同的理解任务。最后，我们介绍了SoccerAgent，一个新颖的多智能体系统，通过协作推理分解复杂的足球问题，展示了其卓越的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.03739', 'title': 'VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model', 'url': 'https://huggingface.co/papers/2505.03739', 'abstract': 'With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.', 'score': 6, 'issue_id': 3632, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'd1a6985b437b7562', 'authors': ['Zuwei Long', 'Yunhang Shen', 'Chaoyou Fu', 'Heting Gao', 'Lijiang Li', 'Peixian Chen', 'Mengdan Zhang', 'Hang Shao', 'Jian Li', 'Jinlong Peng', 'Haoyu Cao', 'Ke Li', 'Rongrong Ji', 'Xing Sun'], 'affiliations': ['Nanjing University', 'Tencent Youtu Lab', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03739.jpg', 'data': {'categories': ['#training', '#multimodal', '#audio', '#inference', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Революция в речевых технологиях: мгновенная генерация аудио с минимальной задержкой', 'desc': 'VITA-Audio - это инновационная крупномасштабная речевая модель, решающая проблему высокой задержки при генерации первого аудиотокена в потоковых системах. Модель использует легковесный модуль множественного кросс-модального предсказания токенов (MCTP), который эффективно генерирует несколько аудиотокенов за один проход модели. Применяется четырехэтапная стратегия прогрессивного обучения для ускорения модели при минимальной потере качества речи. VITA-Audio демонстрирует ускорение вывода в 3-5 раз и превосходит модели с открытым исходным кодом аналогичного размера по нескольким бенчмаркам для задач ASR, TTS и SQA.'}, 'en': {'title': 'VITA-Audio: Fast, Real-Time Speech Generation for Seamless Interaction', 'desc': 'The paper introduces VITA-Audio, a novel end-to-end speech model designed to enhance real-time human-computer interaction by reducing latency in audio generation. It features a Multiple Cross-modal Token Prediction (MCTP) module that allows for the simultaneous generation of multiple audio tokens, significantly speeding up the inference process. The model employs a four-stage progressive training strategy to maintain high speech quality while achieving faster performance. VITA-Audio stands out as the first multi-modal large language model capable of producing audio output during the initial forward pass, demonstrating a 3-5x speedup compared to existing models while excelling in various speech-related tasks.'}, 'zh': {'title': 'VITA-Audio：实时语音生成的新突破', 'desc': '随着人机交互需求的增加，基于语音的系统受到越来越多的关注。现有的语音模型在生成第一个音频标记时存在高延迟，这限制了其应用。为了解决这个问题，我们提出了VITA-Audio，这是一种端到端的大型语音模型，能够快速生成音频文本标记。我们的模型通过引入轻量级的多模态交叉标记预测模块，显著加快了推理速度，并在流媒体场景中减少了延迟。'}}}, {'id': 'https://huggingface.co/papers/2505.03368', 'title': 'Geospatial Mechanistic Interpretability of Large Language Models', 'url': 'https://huggingface.co/papers/2505.03368', 'abstract': 'Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs\' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.', 'score': 5, 'issue_id': 3629, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '1901718a6f967dbe', 'authors': ['Stef De Sabbata', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Leicester, UK', 'University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2505.03368.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#science', '#data', '#architecture', '#interpretability'], 'emoji': '🌍', 'ru': {'title': 'Заглядывая в мозг ИИ: как языковые модели понимают географию', 'desc': 'Статья исследует внутренние механизмы обработки географической информации в больших языковых моделях (LLM). Авторы предлагают новый подход к геопространственной интерпретируемости LLM, используя методы пространственного анализа. Они применяют пробинг и разреженные автоэнкодеры для выявления внутренних представлений географических данных в моделях. Исследование показывает, что извлеченные признаки для топонимов демонстрируют пространственные паттерны, связанные с их географическим положением.'}, 'en': {'title': 'Unveiling How LLMs Think Geographically', 'desc': 'This paper explores how Large Language Models (LLMs) understand and process geographical information. It introduces a framework for geospatial mechanistic interpretability, which aims to reverse engineer the internal workings of LLMs using spatial analysis. The authors utilize probing techniques and sparse autoencoders to uncover how LLMs represent geographic concepts, revealing patterns in their internal features. By demonstrating spatial autocorrelation in placenames, the study provides insights into the spatial reasoning capabilities of LLMs and their implications for geography.'}, 'zh': {'title': '揭示大型语言模型的地理信息处理机制', 'desc': '大型语言模型（LLMs）在自然语言处理任务中展现了前所未有的能力，尤其是在文本和代码的处理与生成方面。本文提出了一种新框架，旨在研究LLMs如何处理地理信息，特别是其内部机制的可解释性。我们通过空间分析和探测技术，揭示LLMs内部结构，并利用稀疏自编码器将多义性特征分解为更易解释的单义特征。实验结果表明，地名特征与其地理位置之间存在空间相关性，从而为理解LLMs处理地理信息的方式提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2505.03164', 'title': 'InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships', 'url': 'https://huggingface.co/papers/2505.03164', 'abstract': "Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.", 'score': 4, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '377a2c082764680a', 'authors': ['Ji Won Chung', 'Tongyu Zhou', 'Ivy Chen', 'Kevin Hsu', 'Ryan A. Rossi', 'Alexa Siu', 'Shunan Guo', 'Franck Dernoncourt', 'James Tompkin', 'Jeff Huang'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03164.jpg', 'data': {'categories': ['#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'InfoVids: Новый подход к визуализации данных через призму человека', 'desc': 'Это исследование представляет концепцию InfoVids - информационных видео, вдохновленных инфографикой. Они призваны создать более сбалансированные отношения между презентатором и визуализацией данных. Авторы изучают, как макет, форма и интерактивность влияют на восприятие зрителей. Эксперименты показали, что InfoVids снижают рассеивание внимания, переносят фокус на презентатора и создают более естественное и увлекательное взаимодействие с данными.'}, 'en': {'title': 'Revolutionizing Data Presentation with InfoVids', 'desc': 'This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualizations in a more cohesive manner. By using infographics-inspired videos, the authors aim to enhance viewer engagement and reduce distractions that typically arise from traditional 2D slides. The study evaluates InfoVids against standard presentation methods across various metrics, revealing that they foster a more interactive and natural experience for viewers. The findings suggest that this innovative approach can transform the dynamics of data presentation, making it more human-centric and effective.'}, 'zh': {'title': '重新定义演示者与可视化的关系', 'desc': '传统的数据展示通常将演示者和可视化分开，分别在3D世界和2D屏幕中进行，强调以可视化为中心的叙述。为了创造更以人为本的观看体验，我们通过InfoVids建立了可视化与演示者之间更平等的关系。这些受信息图启发的信息视频旨在重新定义演示者与可视化之间的关系。我们的研究表明，InfoVids减少了观众的注意力分散，使焦点从可视化转向演示者，并为观众提供了更互动、自然和引人入胜的数据表现。'}}}, {'id': 'https://huggingface.co/papers/2504.21650', 'title': 'HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation', 'url': 'https://huggingface.co/papers/2504.21650', 'abstract': "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.", 'score': 4, 'issue_id': 3632, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'a04fa605a99969e5', 'authors': ['Haiyang Zhou', 'Wangbo Yu', 'Jiawen Guan', 'Xinhua Cheng', 'Yonghong Tian', 'Li Yuan'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory', 'School of Electronic and Computer Engineering, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21650.jpg', 'data': {'categories': ['#3d', '#video', '#cv', '#dataset', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'Погружение в виртуальные миры: от панорамных видео к 4D-сценам', 'desc': 'HoloTime - это новая система для создания панорамных видео и 4D-сцен для VR и AR на основе диффузионных моделей. Она включает в себя генерацию панорамных видео по одному изображению или текстовому запросу с помощью модели Panoramic Animator. Затем производится реконструкция 4D-сцены методом Panoramic Space-Time Reconstruction. Для обучения моделей был создан набор данных 360World с панорамными видео.'}, 'en': {'title': 'Transforming VR and AR with HoloTime: Immersive 4D Experiences from Panoramic Videos', 'desc': 'This paper introduces HoloTime, a novel framework that enhances virtual reality (VR) and augmented reality (AR) experiences by generating immersive 4D assets from video diffusion models. It addresses the limitations of current models that focus on static 3D scenes by proposing a method to create panoramic videos from a single image, which are then transformed into 4D representations. The authors present the 360World dataset, a unique collection of panoramic videos that supports advanced 4D scene reconstruction tasks. Their approach, validated through comparative analysis, shows significant improvements in generating high-quality panoramic videos and reconstructing 4D scenes, ultimately leading to more engaging user experiences in immersive environments.'}, 'zh': {'title': 'HoloTime：提升VR和AR沉浸体验的全景视频生成框架', 'desc': '本论文提出了一种名为HoloTime的框架，旨在通过视频扩散模型生成全景视频，从而提升虚拟现实（VR）和增强现实（AR）技术的沉浸体验。我们引入了360World数据集，这是第一个专门用于4D场景重建任务的全景视频集合。通过Panoramic Animator模型，我们能够将全景图像转换为高质量的全景视频。最后，利用空间-时间深度估计方法，我们将生成的视频转化为4D点云，实现了更真实的4D场景重建。'}}}, {'id': 'https://huggingface.co/papers/2504.21798', 'title': 'SWE-smith: Scaling Data for Software Engineering Agents', 'url': 'https://huggingface.co/papers/2504.21798', 'abstract': 'Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.', 'score': 3, 'issue_id': 3638, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '35185d0e663ab134', 'authors': ['John Yang', 'Kilian Leret', 'Carlos E. Jimenez', 'Alexander Wettig', 'Kabir Khandpur', 'Yanzhe Zhang', 'Binyuan Hui', 'Ofir Press', 'Ludwig Schmidt', 'Diyi Yang'], 'affiliations': ['Alibaba Qwen', 'Indepedent', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21798.jpg', 'data': {'categories': ['#benchmark', '#data', '#synthetic', '#open_source', '#dataset', '#training'], 'emoji': '🛠️', 'ru': {'title': 'SWE-smith: революция в создании обучающих данных для ИИ в программировании', 'desc': 'SWE-smith - это новый конвейер для генерации обучающих данных в области программной инженерии в больших масштабах. Он создает среду выполнения для любой кодовой базы Python и автоматически синтезирует сотни или тысячи экземпляров задач, нарушающих существующие тесты. С помощью SWE-smith авторы создали набор данных из 50 тысяч экземпляров из 128 репозиториев GitHub, что на порядок больше, чем в предыдущих работах. Обученная на этих данных языковая модель SWE-agent-LM-32B достигла показателя Pass@1 в 40.2% на бенчмарке SWE-bench Verified, что является лучшим результатом среди моделей с открытым исходным кодом.'}, 'en': {'title': 'Scaling Software Engineering Data Generation with SWE-smith', 'desc': 'This paper presents SWE-smith, a new method for generating large-scale training data for language models in software engineering. Traditional datasets are limited in size and require extensive manual effort to curate, which hinders their effectiveness. SWE-smith automates the creation of execution environments and synthesizes numerous task instances from Python codebases, resulting in a dataset of 50,000 instances from 128 GitHub repositories. The authors demonstrate that their model, SWE-agent-LM-32B, achieves a state-of-the-art performance on the SWE-bench Verified benchmark, and they provide all resources to facilitate further research in this area.'}, 'zh': {'title': 'SWE-smith：大规模生成软件工程训练数据的解决方案', 'desc': '尽管语言模型在软件工程领域取得了进展，但收集训练数据仍然是一个重大挑战。现有的数据集规模较小，通常来自11个或更少的GitHub仓库，最多只有几千个训练实例。为了应对这一问题，我们提出了SWE-smith，这是一种用于大规模生成软件工程训练数据的新型管道。通过SWE-smith，我们创建了一个包含5万个实例的数据集，显著超过了以往的工作，并且开源了相关资源，以降低自动化软件工程研究的门槛。'}}}, {'id': 'https://huggingface.co/papers/2505.03052', 'title': 'Teaching Models to Understand (but not Generate) High-risk Data', 'url': 'https://huggingface.co/papers/2505.03052', 'abstract': "Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.", 'score': 2, 'issue_id': 3642, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'a988d97eaa0eb984', 'authors': ['Ryan Wang', 'Matthew Finlayson', 'Luca Soldaini', 'Swabha Swayamdipta', 'Robin Jia'], 'affiliations': ['Allen Institute for AI', 'Department of Computer Science, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.03052.jpg', 'data': {'categories': ['#training', '#hallucinations', '#rlhf', '#ethics'], 'emoji': '🛡️', 'ru': {'title': 'Понимать, но не генерировать: безопасное обучение языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием SLUNG. Этот метод позволяет моделям понимать потенциально опасный контент, не генерируя его. SLUNG избирательно применяет функцию потерь, избегая стимулирования генерации рискованных токенов, но сохраняя их в контексте модели. Эксперименты показывают, что SLUNG улучшает понимание моделями опасного контента без увеличения его генерации.'}, 'en': {'title': 'Learn to Understand, Not to Generate Toxicity', 'desc': "This paper presents a new training method called Selective Loss to Understand but Not Generate (SLUNG) for language models. SLUNG allows models to learn from high-risk content, like toxic text, without generating it in their outputs. By selectively applying loss functions, the model is trained to predict safe tokens that follow harmful ones, enhancing its understanding of sensitive topics. The results show that SLUNG improves the model's ability to recognize toxic content while preventing it from producing harmful responses."}, 'zh': {'title': '理解高风险内容，避免生成有害输出', 'desc': '本文提出了一种新的预训练方法，称为选择性损失以理解但不生成（SLUNG）。该方法允许模型在不生成高风险内容的情况下，理解这些内容。通过选择性地避免激励生成高风险标记，SLUNG确保这些内容仍然在模型的上下文窗口内。实验结果表明，SLUNG显著提高了模型对高风险数据的理解能力，同时没有增加生成的有害内容。'}}}, {'id': 'https://huggingface.co/papers/2505.02311', 'title': 'Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering', 'url': 'https://huggingface.co/papers/2505.02311', 'abstract': 'The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.', 'score': 2, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '2a0b4af232b71fc8', 'authors': ['Jihao Zhao', 'Chunlai Zhou', 'Biao Qin'], 'affiliations': ['School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02311.jpg', 'data': {'categories': ['#hallucinations', '#small_models', '#training', '#optimization', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Умное обнаружение галлюцинаций для эффективного сотрудничества языковых моделей', 'desc': 'Статья предлагает новый метод AttenHScore для оценки галлюцинаций в малых языковых моделях в реальном времени. Этот подход позволяет точнее определять момент для вызова большой языковой модели в коллаборативной парадигме. Авторы также используют реорганизацию знаний с учетом неопределенности, чтобы помочь малым моделям лучше обрабатывать ключевую информацию. Эксперименты показывают, что AttenHScore превосходит базовые методы в обнаружении галлюцинаций на нескольких наборах данных вопросно-ответных систем.'}, 'en': {'title': 'Enhancing Hallucination Detection in Language Models with AttenHScore', 'desc': 'This paper introduces a new metric called AttenHScore to improve the detection of hallucinations in small language models (LMs) during their generation process. Hallucinations refer to incorrect or nonsensical outputs produced by LMs, and the proposed metric helps identify when these errors occur in real-time. By adjusting the detection threshold dynamically, the method enhances the invocation of larger LMs to provide more accurate responses. Additionally, the paper discusses how uncertainty-aware knowledge reorganization can help small LMs better understand and utilize critical information from text, leading to improved performance without requiring extra training.'}, 'zh': {'title': '提升小型语言模型的幻觉检测能力', 'desc': '本文提出了一种新的评估指标AttenHScore，用于在小型语言模型生成过程中检测和传播幻觉。通过动态调整检测阈值，我们能够更准确地实时调用大型语言模型，从而提高幻觉检测的能力。我们还利用不确定性感知的知识重组，帮助小型语言模型更好地捕捉关键信息。实验结果表明，AttenHScore在多个问答数据集上优于大多数基线方法，尤其是在处理复杂查询时。'}}}, {'id': 'https://huggingface.co/papers/2504.18373', 'title': 'Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant', 'url': 'https://huggingface.co/papers/2504.18373', 'abstract': 'In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.', 'score': 2, 'issue_id': 3629, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '296dd197ef9ea331', 'authors': ['Lei Shen', 'Xiaoyu Shen'], 'affiliations': ['GEB Tech', 'Ningbo Institute of Digital Twin, EIT, Ningbo'], 'pdf_title_img': 'assets/pdf/title_img/2504.18373.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Auto-SLURP: новый стандарт для оценки многоагентных ИИ-ассистентов', 'desc': 'Авторы представляют Auto-SLURP - набор данных для оценки многоагентных систем на основе больших языковых моделей (LLM) в контексте интеллектуальных персональных ассистентов. Auto-SLURP расширяет исходный набор SLURP, добавляя симулированные серверы и внешние сервисы для комплексной оценки понимания языка, выполнения задач и генерации ответов. Эксперименты показывают, что Auto-SLURP представляет серьезную проблему для современных передовых систем. Результаты подчеркивают, что по-настоящему надежные и интеллектуальные многоагентные персональные ассистенты все еще находятся в стадии разработки.'}, 'en': {'title': 'Auto-SLURP: Benchmarking Multi-Agent LLMs for Intelligent Assistants', 'desc': 'This paper introduces Auto-SLURP, a new benchmark dataset designed to evaluate multi-agent frameworks that utilize large language models (LLMs) in the realm of intelligent personal assistants. It enhances the original SLURP dataset by relabeling data and incorporating simulated servers and external services, allowing for a more thorough assessment of language understanding, task execution, and response generation. The authors demonstrate that Auto-SLURP poses significant challenges to existing state-of-the-art frameworks, indicating that the development of reliable multi-agent personal assistants is still ongoing. The dataset and its associated code are made publicly available for further research and development.'}, 'zh': {'title': 'Auto-SLURP：评估智能个人助理的基准数据集', 'desc': '近年来，基于大型语言模型（LLMs）的多智能体框架发展迅速。然而，目前缺乏专门用于评估这些框架性能的基准数据集。为了解决这个问题，我们推出了Auto-SLURP，这是一个旨在评估基于LLM的多智能体框架的基准数据集，特别是在智能个人助理的背景下。Auto-SLURP通过重新标记数据并整合模拟服务器和外部服务，扩展了原始的SLURP数据集，从而实现了全面的端到端评估流程。'}}}, {'id': 'https://huggingface.co/papers/2505.00212', 'title': 'Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2505.00212', 'abstract': "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution", 'score': 1, 'issue_id': 3639, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '1b127d9b18e51c40', 'authors': ['Shaokun Zhang', 'Ming Yin', 'Jieyu Zhang', 'Jiale Liu', 'Zhiguang Han', 'Jingyang Zhang', 'Beibin Li', 'Chi Wang', 'Huazheng Wang', 'Yiran Chen', 'Qingyun Wu'], 'affiliations': ['Duke University', 'Google DeepMind', 'Meta', 'Nanyang Technological University', 'Oregon State University', 'Pennsylvania State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.00212.jpg', 'data': {'categories': ['#dataset', '#open_source', '#agents', '#reasoning'], 'emoji': '🕵️', 'ru': {'title': 'Кто виноват и когда: автоматическое расследование сбоев в многоагентных LLM-системах', 'desc': 'Статья представляет новую область исследований: автоматическое определение причин сбоев в многоагентных системах на основе больших языковых моделей (LLM). Авторы создали датасет Who&When, содержащий подробные логи сбоев из 127 многоагентных LLM-систем с аннотациями, связывающими сбои с конкретными агентами и этапами ошибок. На основе этого датасета разработаны и оценены три метода автоматического определения причин сбоев. Результаты показывают сложность задачи: лучший метод достигает точности 53.5% в идентификации агентов, ответственных за сбои, но только 14.2% в определении этапов сбоев.'}, 'en': {'title': 'Automating Failure Attribution in Multi-Agent Systems', 'desc': 'This paper addresses the challenge of identifying which agent and which step in a multi-agent system led to task failures, a process known as failure attribution. The authors introduce a new dataset called Who&When, which contains detailed failure logs from 127 LLM multi-agent systems, annotated to link failures to specific agents and error steps. They propose three automated methods for failure attribution and evaluate their performance, revealing that the best method only achieves 53.5% accuracy in identifying responsible agents and 14.2% in pinpointing failure steps. The findings indicate that current state-of-the-art models struggle with this task, emphasizing the complexity of automated failure attribution and the need for further research.'}, 'zh': {'title': '自动化失败归因：LLM多智能体系统的新挑战', 'desc': '本文探讨了在大型语言模型（LLM）多智能体系统中，如何自动识别导致任务失败的智能体和步骤。我们提出了一个新的研究领域：自动化失败归因，并引入了Who&When数据集，该数据集包含127个LLM多智能体系统的详细失败日志。通过使用Who&When数据集，我们开发并评估了三种自动化失败归因方法，并总结了它们的优缺点。尽管最佳方法在识别失败责任智能体方面达到了53.5%的准确率，但在确定失败步骤方面仅为14.2%，显示出该任务的复杂性和进一步研究的必要性。'}}}, {'id': 'https://huggingface.co/papers/2505.09568', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset', 'url': 'https://huggingface.co/papers/2505.09568', 'abstract': 'Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.', 'score': 6, 'issue_id': 3768, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '822f8dd79d39211b', 'authors': ['Jiuhai Chen', 'Zhiyang Xu', 'Xichen Pan', 'Yushi Hu', 'Can Qin', 'Tom Goldstein', 'Lifu Huang', 'Tianyi Zhou', 'Saining Xie', 'Silvio Savarese', 'Le Xue', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['New York University', 'Salesforce Research', 'UC Davis', 'University of Maryland', 'University of Washington', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.09568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#open_source', '#multimodal', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Объединение понимания и генерации изображений с помощью диффузионных трансформеров', 'desc': 'Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений.'}, 'en': {'title': 'Unifying Image Understanding and Generation with BLIP3-o', 'desc': 'This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks.'}, 'zh': {'title': '统一图像理解与生成的创新模型', 'desc': '本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。'}}}, {'id': 'https://huggingface.co/papers/2504.21635', 'title': 'Sadeed: Advancing Arabic Diacritization Through Small Language Model', 'url': 'https://huggingface.co/papers/2504.21635', 'abstract': "Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.", 'score': 44, 'issue_id': 3530, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'af5a1b038b3ccab3', 'authors': ['Zeina Aldallal', 'Sara Chrouf', 'Khalil Hennara', 'Mohamed Motaism Hamed', 'Muhammad Hreden', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21635.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#multilingual', '#benchmark', '#data', '#machine_translation'], 'emoji': '🔠', 'ru': {'title': 'Sadeed: Прорыв в диакритизации арабского текста с помощью компактной языковой модели', 'desc': 'Sadeed - это новый подход к диакритизации арабского текста, основанный на модели Kuwain 1.5B. Модель была дообучена на тщательно отобранных и обработанных наборах данных с диакритическими знаками. Несмотря на ограниченные вычислительные ресурсы, Sadeed показывает конкурентоспособные результаты по сравнению с проприетарными большими языковыми моделями. Авторы также представили новый бенчмарк SadeedDiac-25 для более справедливой оценки моделей диакритизации арабского текста.'}, 'en': {'title': 'Sadeed: Advancing Arabic Diacritization with Efficiency and Precision', 'desc': 'This paper presents Sadeed, a new method for Arabic text diacritization using a fine-tuned decoder-only language model based on Kuwain 1.5B. Sadeed is specifically trained on high-quality diacritized datasets, which were created through a thorough data-cleaning process. The model demonstrates competitive performance against larger proprietary models while requiring less computational power. Additionally, the authors introduce SadeedDiac-25, a new benchmark for evaluating Arabic diacritization, aiming to improve assessment practices in the field.'}, 'zh': {'title': 'Sadeed：阿拉伯语标记化的新突破', 'desc': '本文介绍了一种新的阿拉伯语文本标记化方法，名为Sadeed。该方法基于经过微调的解码器语言模型，专门针对阿拉伯语的丰富形态特征进行优化。Sadeed在高质量的标记化数据集上进行微调，尽管计算资源有限，但其性能与大型语言模型相当，且优于传统模型。此外，本文还提出了SadeedDiac-25基准，以促进对阿拉伯语标记化的公平评估。'}}}, {'id': 'https://huggingface.co/papers/2504.21776', 'title': 'WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability', 'url': 'https://huggingface.co/papers/2504.21776', 'abstract': 'Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.', 'score': 26, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '61ce82abe42f584a', 'authors': ['Xiaoxi Li', 'Jiajie Jin', 'Guanting Dong', 'Hongjin Qian', 'Yutao Zhu', 'Yongkang Wu', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21776.jpg', 'data': {'categories': ['#rl', '#science', '#agents', '#rlhf', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🕸️', 'ru': {'title': 'WebThinker: Расширение возможностей ИИ для глубоких веб-исследований', 'desc': 'WebThinker - это глубокая исследовательская система, которая расширяет возможности больших языковых моделей (LLM) для автономного поиска в интернете и составления исследовательских отчетов. Система включает модуль Deep Web Explorer для динамического поиска и извлечения информации из веб-страниц, а также стратегию автономного мышления, поиска и составления черновиков. WebThinker использует обучение с подкреплением на основе прямой оптимизации предпочтений (DPO) для улучшения использования исследовательских инструментов. Эксперименты показывают, что WebThinker значительно превосходит существующие методы на сложных задачах рассуждения и генерации научных отчетов.'}, 'en': {'title': 'Empowering LRMs with Real-Time Web Research Capabilities', 'desc': "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."}, 'zh': {'title': 'WebThinker：让推理模型更智能的研究助手', 'desc': '大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在长时间推理方面表现出色，但它们依赖静态内部知识，限制了在复杂知识密集型任务中的表现。为了解决这个问题，我们提出了WebThinker，一个深度研究代理，能够让LRMs自主搜索网络、浏览网页并在推理过程中撰写研究报告。WebThinker集成了深网探索模块，使LRMs在遇到知识空白时能够动态搜索和提取信息。通过引入基于强化学习的训练策略，我们的实验表明WebThinker在复杂推理基准和科学报告生成任务中显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.21850', 'title': 'COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning', 'url': 'https://huggingface.co/papers/2504.21850', 'abstract': 'Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.', 'score': 20, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '3db97f245360deb4', 'authors': ['Xindi Wu', 'Hee Seung Hwang', 'Polina Kirichenko', 'Olga Russakovsky'], 'affiliations': ['Meta AI', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21850.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#data', '#optimization', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'COMPACT: эффективное обучение MLLM сложным визуально-языковым задачам', 'desc': 'Статья представляет новый метод обучения мультимодальных больших языковых моделей (MLLM) под названием COMPACT. Этот подход генерирует набор данных для обучения, контролируя композиционную сложность примеров. COMPACT позволяет MLLM эффективнее обучаться сложным задачам, комбинируя атомарные возможности. Метод достигает сопоставимых результатов с LLaVA-665k, используя менее 10% данных, и превосходит его на сложных мультизадачных тестах.'}, 'en': {'title': 'Unlocking Complex Tasks with Efficient Compositional Training', 'desc': "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."}, 'zh': {'title': '提升复杂视觉语言任务的能力', 'desc': '多模态大型语言模型（MLLMs）在简单的视觉语言任务中表现出色，但在需要多种能力的复杂任务中却面临挑战。传统的视觉指令调优（VIT）主要关注数据量的扩大，而忽视了训练示例的组合复杂性。我们提出了COMPACT（组合原子到复杂视觉能力调优），它生成一个明确控制训练示例组合复杂性的训练数据集。COMPACT使得MLLMs能够更高效地学习复杂能力，并在多个基准测试中表现出色，尤其是在涉及复杂多能力任务时。'}}}, {'id': 'https://huggingface.co/papers/2504.21233', 'title': 'Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math', 'url': 'https://huggingface.co/papers/2504.21233', 'abstract': 'Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.', 'score': 19, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '0b800a9195884db4', 'authors': ['Haoran Xu', 'Baolin Peng', 'Hany Awadalla', 'Dongdong Chen', 'Yen-Chun Chen', 'Mei Gao', 'Young Jin Kim', 'Yunsheng Li', 'Liliang Ren', 'Yelong Shen', 'Shuohang Wang', 'Weijian Xu', 'Jianfeng Gao', 'Weizhu Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21233.jpg', 'data': {'categories': ['#training', '#rl', '#transfer_learning', '#small_models', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие потенциала малых языковых моделей в задачах рассуждения', 'desc': 'Статья представляет систематический подход к улучшению способностей рассуждения в малых языковых моделях (SLM). Авторы предлагают четырехэтапный рецепт обучения, включающий дистилляцию данных, дообучение на высококачественных данных с цепочками рассуждений, оптимизацию прямого предпочтения и обучение с подкреплением. Применение этого метода к модели Phi-4-Mini (3.8 млрд параметров) позволило превзойти более крупные модели в задачах математических рассуждений. Результаты подтверждают эффективность предложенного подхода для развития сильных способностей рассуждения даже в ресурсно-ограниченных малых моделях.'}, 'en': {'title': 'Unlocking Reasoning Power in Small Models', 'desc': 'This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning.'}, 'zh': {'title': '小模型也能强推理！', 'desc': '本论文探讨了如何通过链式思维（CoT）来提升小型语言模型（SLM）的推理能力。尽管大型语言模型（LLM）在生成中间推理步骤方面表现良好，但小型模型由于容量限制，提升推理能力仍然具有挑战性。研究表明，通过从LLM生成的合成数据进行蒸馏，可以显著改善SLM的推理能力。我们提出了一种系统的训练方案，包括四个步骤，最终在Phi-4-Mini模型上实现了超越更大模型的推理表现。'}}}, {'id': 'https://huggingface.co/papers/2504.20708', 'title': 'Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think', 'url': 'https://huggingface.co/papers/2504.20708', 'abstract': "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.", 'score': 17, 'issue_id': 3532, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'b26e58cf1cee464f', 'authors': ['Hasan Abed Al Kader Hammoud', 'Hani Itani', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2504.20708.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#interpretability', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Подмысли в LLM: путь к повышению точности рассуждений', 'desc': 'Исследование посвящено анализу промежуточных шагов рассуждения (подмыслей) в крупных языковых моделях (LLM) при решении сложных задач. Авторы предлагают метод сегментации цепочки рассуждений на подмысли и генерации продолжений из каждой промежуточной точки. Агрегирование ответов, полученных из разных подмыслей, часто дает более высокую точность по сравнению с использованием только финального ответа. Эксперименты на различных LLM и наборах данных по математическим рассуждениям показали значительное улучшение точности, достигающее 13%.'}, 'en': {'title': 'Unlocking Better Answers Through Subthoughts in LLMs', 'desc': 'This paper investigates the reasoning process of Large Language Models (LLMs) by focusing on intermediate reasoning steps, called subthoughts, rather than just the final answer. It questions whether the final answer is the best conclusion and explores if different reasoning paths can lead to varied results. The authors propose a method that segments reasoning into subthoughts and generates multiple potential answers from these segments, aggregating them to find the most frequent answer for improved accuracy. Their experiments demonstrate that this approach can enhance the accuracy of LLMs by up to 13% on challenging mathematical reasoning tasks.'}, 'zh': {'title': '优化推理路径，提升模型准确性', 'desc': '本文探讨了大型语言模型（LLMs）在解决复杂问题时的推理过程。我们提出了两个问题：最终答案是否可靠地代表模型的最佳结论？不同的推理路径是否会产生不同的结果？为了解答这些问题，我们分析了中间推理步骤，称为子思维，并提出了一种基于这些发现的方法。实验结果表明，通过选择最频繁的答案（众数）来聚合不同子思维的答案，准确性显著提高，最高可达13%。'}}}, {'id': 'https://huggingface.co/papers/2504.21318', 'title': 'Phi-4-reasoning Technical Report', 'url': 'https://huggingface.co/papers/2504.21318', 'abstract': 'We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.', 'score': 14, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '7004ae060f674e9a', 'authors': ['Marah Abdin', 'Sahaj Agarwal', 'Ahmed Awadallah', 'Vidhisha Balachandran', 'Harkirat Behl', 'Lingjiao Chen', 'Gustavo de Rosa', 'Suriya Gunasekar', 'Mojan Javaheripi', 'Neel Joshi', 'Piero Kauffmann', 'Yash Lara', 'Caio César Teodoro Mendes', 'Arindam Mitra', 'Besmira Nushi', 'Dimitris Papailiopoulos', 'Olli Saarikivi', 'Shital Shah', 'Vaishnavi Shrivastava', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi', 'Guoqing Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21318.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#math', '#transfer_learning', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Мощная модель рассуждений на основе тщательно отобранных данных', 'desc': 'Исследователи представили Phi-4-reasoning - модель с 14 миллиардами параметров, обученную для сложных задач рассуждения. Модель была обучена с помощью контролируемой тонкой настройки на тщательно отобранных обучающих примерах и демонстрациях рассуждений. Phi-4-reasoning генерирует подробные цепочки рассуждений, эффективно используя вычислительные ресурсы во время вывода. Модель превосходит по производительности значительно более крупные модели с открытыми весами на различных задачах рассуждения, включая математику, научное мышление, программирование и пространственное понимание.'}, 'en': {'title': 'Unlocking Complex Reasoning with Phi-4-Reasoning', 'desc': 'The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models.'}, 'zh': {'title': '推理模型的新突破：Phi-4-reasoning', 'desc': '本文介绍了Phi-4-reasoning，这是一个拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过对精心挑选的“可教”提示进行监督微调训练，生成详细的推理链，有效利用推理时的计算能力。我们还开发了Phi-4-reasoning-plus，通过基于结果的强化学习进一步增强，能够生成更长的推理轨迹，从而提高性能。综合评估显示，这两个模型在数学、科学推理、编码等多个任务上均优于更大的开放权重模型。'}}}, {'id': 'https://huggingface.co/papers/2504.20966', 'title': 'Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax', 'url': 'https://huggingface.co/papers/2504.20966', 'abstract': 'We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.', 'score': 14, 'issue_id': 3526, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'cb610c1427bdf307', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#architecture', '#inference', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Softpick: улучшенная активация для эффективных трансформеров', 'desc': "Статья представляет softpick - новый метод активации для механизмов внимания в трансформерах, заменяющий softmax. Softpick устраняет проблему 'attention sink' и чрезмерных активаций, сохраняя производительность на уровне softmax. Эксперименты показывают, что softpick создает более разреженные карты внимания и состояния с меньшим эксцессом. Модели с softpick превосходят softmax при квантовании, особенно при низкой битовой точности."}, 'en': {'title': 'Softpick: A Smarter Alternative to Softmax for Transformers', 'desc': 'This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability.'}, 'zh': {'title': 'softpick：提升Transformer注意力的新选择', 'desc': '本文介绍了一种名为softpick的新方法，它可以替代transformer注意力机制中的softmax。softpick不需要将权重归一化为1，能够消除注意力沉没和大规模激活。实验表明，使用softpick的模型在标准基准测试中与softmax表现相当，但注意力沉没率为0%，并且生成的隐藏状态具有更低的峰度。softpick在量化和低精度训练中表现优越，尤其在较低位数精度下具有明显优势，展示了其在稀疏性优化和可解释性方面的潜力。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.19720', 'title': 'Taming the Titans: A Survey of Efficient LLM Inference Serving', 'url': 'https://huggingface.co/papers/2504.19720', 'abstract': 'Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.', 'score': 9, 'issue_id': 3526, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': 'e74f8b7af65e09fd', 'authors': ['Ranran Zhen', 'Juntao Li', 'Yixin Ji', 'Zhenlin Yang', 'Tong Liu', 'Qingrong Xia', 'Xinyu Duan', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang'], 'affiliations': ['Huawei Cloud', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2504.19720.jpg', 'data': {'categories': ['#survey', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение инференса больших языковых моделей: от экземпляра до кластера', 'desc': 'Статья представляет собой обзор методов оптимизации инференса больших языковых моделей (LLM). Авторы рассматривают подходы на уровне отдельных экземпляров, включая размещение модели и управление запросами. Также описываются стратегии на уровне кластеров, такие как балансировка нагрузки и облачные решения. Особое внимание уделяется новым сценариям применения LLM и вспомогательным методам оптимизации.'}, 'en': {'title': 'Optimizing LLMs: Balancing Performance and Efficiency in Generative AI', 'desc': 'This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services.'}, 'zh': {'title': '推动大型语言模型推理服务的研究进展', 'desc': '大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其庞大的参数量和注意力机制的高计算需求导致了内存开销大，影响了推理服务的低延迟和高吞吐量。本文全面调查了应对这些挑战的方法，包括实例级和集群级的策略，以及新兴场景的方向。我们讨论了模型部署、请求调度、解码长度预测等实例级方法，以及GPU集群部署和多实例负载均衡等集群级策略。最后，我们提出了未来研究的潜在方向，以推动LLM推理服务的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.21855', 'title': 'ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction', 'url': 'https://huggingface.co/papers/2504.21855', 'abstract': 'In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '5d8989ce0c77aa23', 'authors': ['Qihao Liu', 'Ju He', 'Qihang Yu', 'Liang-Chieh Chen', 'Alan Yuille'], 'affiliations': ['Independent Researcher', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21855.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games', '#video', '#small_models'], 'emoji': '🎥', 'ru': {'title': 'ReVision: физика в помощь ИИ для создания реалистичных видео', 'desc': 'ReVision - это новый фреймворк для улучшения генерации видео с использованием трехмерных физических знаний. Он состоит из трех этапов: генерация грубого видео, извлечение 2D и 3D признаков для создания объектно-ориентированного представления, и уточнение движения с помощью параметризованной физической модели. ReVision значительно повышает качество генерируемых видео с точки зрения сложных движений и взаимодействий, превосходя даже более крупные модели.'}, 'en': {'title': 'ReVision: Enhancing Video Generation with 3D Physical Knowledge', 'desc': 'The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation.'}, 'zh': {'title': '通过3D物理知识提升视频生成的真实感与可控性', 'desc': '近年来，视频生成技术取得了显著进展，但在生成复杂动作和交互方面仍面临挑战。为了解决这些问题，我们提出了ReVision，这是一个可插拔的框架，能够将参数化的三维物理知识集成到预训练的条件视频生成模型中，从而显著提升生成高质量视频的能力。ReVision包括三个阶段：首先使用视频扩散模型生成粗略视频，然后提取2D和3D特征构建三维物体中心表示，最后通过参数化物理先验模型精炼运动序列，反馈到视频扩散模型中以生成一致的运动视频。我们的实验表明，ReVision在复杂视频生成上表现优异，甚至以较少的参数超越了大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.19056', 'title': 'Generative AI for Character Animation: A Comprehensive Survey of\n  Techniques, Applications, and Future Directions', 'url': 'https://huggingface.co/papers/2504.19056', 'abstract': 'Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.', 'score': 7, 'issue_id': 3535, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': 'deba947a8e69f6ee', 'authors': ['Mohammad Mahdi Abootorabi', 'Omid Ghahroodi', 'Pardis Sadat Zahraei', 'Hossein Behzadasl', 'Alireza Mirrokni', 'Mobina Salimipanah', 'Arash Rasouli', 'Bahar Behzadipour', 'Sara Azarnoush', 'Benyamin Maleki', 'Erfan Sadraiye', 'Kiarash Kiani Feriz', 'Mahdi Teymouri Nahad', 'Ali Moghadasi', 'Abolfazl Eshagh Abianeh', 'Nizi Nazar', 'Hamid R. Rabiee', 'Mahdieh Soleymani Baghshah', 'Meisam Ahmadi', 'Ehsaneddin Asgari'], 'affiliations': ['Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Iran University of Science and Technology', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2504.19056.jpg', 'data': {'categories': ['#survey', '#diffusion', '#cv', '#games', '#multimodal', '#video'], 'emoji': '🤖', 'ru': {'title': 'Генеративный ИИ революционизирует анимацию персонажей', 'desc': 'Это обзорная статья о применении генеративного искусственного интеллекта в анимации персонажей. Авторы рассматривают последние достижения в области лицевой анимации, синтеза движений, создания аватаров и других аспектов. В статье представлен комплексный обзор всех основных приложений генеративного ИИ для анимации персонажей, включая ведущие исследования, практические внедрения и используемые наборы данных. Также приводится справочная информация о фундаментальных моделях и метриках оценки для новичков в этой области.'}, 'en': {'title': 'Revolutionizing Character Animation with Generative AI', 'desc': 'This paper reviews the recent advancements in generative AI technologies specifically for character animation, which includes facial animation, gesture modeling, and motion synthesis. It highlights how foundation and diffusion models have significantly lowered the costs and time required for creating animated content. The survey provides a comprehensive overview of the state-of-the-art techniques, practical applications, and datasets used in the field, making it a valuable resource for newcomers. Additionally, it discusses ongoing challenges and suggests future research directions to enhance AI-driven character animation.'}, 'zh': {'title': '生成性AI：重塑角色动画的未来', 'desc': '这篇论文探讨了生成性人工智能在角色动画领域的应用，特别是在面部动画、表情渲染和动作合成等方面的最新进展。通过整合不同的生成模型和扩散模型，研究者们显著降低了动画内容的制作时间和成本。论文还提供了对当前研究、实际应用、常用数据集和新兴趋势的全面回顾，帮助新手了解基础模型和评估指标。最后，作者讨论了该领域面临的挑战，并为未来的研究方向提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2504.18904', 'title': 'RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning', 'url': 'https://huggingface.co/papers/2504.18904', 'abstract': 'Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': 'c724dcb5ceb5df7b', 'authors': ['Haoran Geng', 'Feishi Wang', 'Songlin Wei', 'Yuyang Li', 'Bangjun Wang', 'Boshi An', 'Charlie Tianyue Cheng', 'Haozhe Lou', 'Peihao Li', 'Yen-Jen Wang', 'Yutong Liang', 'Dylan Goetting', 'Chaoyi Xu', 'Haozhe Chen', 'Yuxi Qian', 'Yiran Geng', 'Jiageng Mao', 'Weikang Wan', 'Mingtong Zhang', 'Jiangran Lyu', 'Siheng Zhao', 'Jiazhao Zhang', 'Jialiang Zhang', 'Chengyang Zhao', 'Haoran Lu', 'Yufei Ding', 'Ran Gong', 'Yuran Wang', 'Yuxuan Kuang', 'Ruihai Wu', 'Baoxiong Jia', 'Carlo Sferrazza', 'Hao Dong', 'Siyuan Huang', 'Yue Wang', 'Jitendra Malik', 'Pieter Abbeel'], 'affiliations': ['BIGAI', 'CMU', 'PKU', 'Stanford', 'UC Berkeley', 'UCLA', 'UIUC', 'UMich', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2504.18904.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#synthetic', '#optimization', '#transfer_learning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboVerse: универсальная платформа для масштабирования и стандартизации обучения роботов', 'desc': 'RoboVerse - это комплексная платформа для робототехники, включающая симуляционную среду, синтетический набор данных и унифицированные бенчмарки. Платформа поддерживает множество симуляторов и роботизированных воплощений, обеспечивая плавный переход между различными средами. Синтетический набор данных отличается высокой точностью физики и фотореалистичным рендерингом. RoboVerse предлагает унифицированные бенчмарки для имитационного обучения и обучения с подкреплением, позволяющие оценивать различные уровни обобщения.'}, 'en': {'title': 'RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks', 'desc': 'This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field.'}, 'zh': {'title': 'RoboVerse：推动机器人学习的强大框架', 'desc': '本论文介绍了RoboVerse，这是一个综合框架，旨在解决机器人领域数据收集和评估的挑战。RoboVerse包括一个模拟平台、一个合成数据集和统一的基准测试，支持多种模拟器和机器人形态。通过高保真物理和逼真的渲染，合成数据集提供了高质量和多样性的数据。实验结果表明，RoboVerse显著提升了模仿学习、强化学习和从模拟到现实的转移性能，验证了其数据集和基准的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2504.21039', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report', 'url': 'https://huggingface.co/papers/2504.21039', 'abstract': 'As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.', 'score': 5, 'issue_id': 3528, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': 'a66fbdd0c4cc7250', 'authors': ['Paul Kassianik', 'Baturay Saglam', 'Alexander Chen', 'Blaine Nelson', 'Anu Vellore', 'Massimo Aufiero', 'Fraser Burch', 'Dhruv Kedia', 'Avi Zohary', 'Sajana Weerawardhena', 'Aman Priyanshu', 'Adam Swanda', 'Amy Chang', 'Hyrum Anderson', 'Kojin Oshiba', 'Omar Santos', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Foundation AI Cisco Systems Inc.', 'Security & Trust Organization Cisco Systems Inc.', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21039.jpg', 'data': {'categories': ['#architecture', '#data', '#open_source', '#training', '#dataset', '#benchmark', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Специализированная языковая модель открывает новые горизонты в кибербезопасности', 'desc': 'Статья представляет Foundation-Sec-8B - языковую модель, специализированную на кибербезопасности. Модель основана на архитектуре Llama 3.1 и дообучена на специально подобранном корпусе текстов по кибербезопасности. Foundation-Sec-8B показывает результаты, сравнимые с Llama 3.1-70B и GPT-4o-mini в некоторых задачах кибербезопасности. Авторы публикуют модель, чтобы ускорить развитие и внедрение ИИ-инструментов в сфере кибербезопасности.'}, 'en': {'title': 'Empowering Cybersecurity with Foundation-Sec-8B', 'desc': 'This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors.'}, 'zh': {'title': '推动网络安全的AI工具进步', 'desc': '随着基于变换器的大型语言模型（LLMs）在社会中的广泛应用，它们在软件工程、创意写作和数字艺术等领域带来了革命性的变化。然而，由于缺乏专业的训练数据和表示网络安全特定知识的复杂性，它们在网络安全领域的应用仍然有限。为了解决这些问题，我们提出了Foundation-Sec-8B，这是一个专注于网络安全的LLM，基于Llama 3.1架构，并通过在精心策划的网络安全语料库上进行持续预训练来增强。我们在多个网络安全基准测试中评估了Foundation-Sec-8B，结果显示它在某些网络安全特定任务上与Llama 3.1-70B和GPT-4o-mini相匹配。'}}}, {'id': 'https://huggingface.co/papers/2504.21336', 'title': 'UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation', 'url': 'https://huggingface.co/papers/2504.21336', 'abstract': 'Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.', 'score': 2, 'issue_id': 3532, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'b4a0872fb3eb8547', 'authors': ['Linshan Wu', 'Yuxiang Nie', 'Sunan He', 'Jiaxin Zhuang', 'Hao Chen'], 'affiliations': ['Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China', 'Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China', 'State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21336.jpg', 'data': {'categories': ['#optimization', '#cv', '#science', '#multimodal', '#dataset', '#healthcare', '#interpretability', '#agi'], 'emoji': '🧬', 'ru': {'title': 'UniBiomed: революция в анализе биомедицинских изображений с помощью ИИ', 'desc': 'UniBiomed - это универсальная модель для интерпретации биомедицинских изображений, объединяющая мультимодальную языковую модель и модель сегментации. Она обучена на 27 миллионах триплетов изображений, аннотаций и текстовых описаний из 10 различных модальностей медицинской визуализации. UniBiomed демонстрирует передовые результаты в задачах сегментации, распознавания заболеваний, диагностики с привязкой к регионам, визуальных вопросов-ответов и генерации отчетов. Модель обеспечивает автоматизированную интерпретацию биомедицинских изображений без необходимости в предварительной диагностике экспертами.'}, 'en': {'title': 'UniBiomed: Revolutionizing Biomedical Image Interpretation', 'desc': "This paper presents UniBiomed, a groundbreaking universal foundation model designed for interpreting biomedical images by integrating Multi-modal Large Language Models (MLLM) and segmentation techniques. Unlike traditional AI methods that treat text generation and image segmentation separately, UniBiomed combines these processes to provide a cohesive understanding of biomedical data. It utilizes a large-scale dataset of over 27 million image-text pairs across various imaging modalities, enabling it to perform multiple tasks such as segmentation, disease recognition, and report generation. The model's ability to automate grounded interpretation marks a significant advancement in clinical workflows, enhancing diagnostic efficiency and accuracy."}, 'zh': {'title': 'UniBiomed：生物医学图像分析的新突破', 'desc': '多模态生物医学图像的解释为生物医学图像分析开辟了新的机会。传统的人工智能方法通常依赖于分离的训练，导致在实际应用中缺乏灵活性，无法充分利用整体生物医学信息。为此，我们提出了UniBiomed，这是首个用于生物医学图像解释的通用基础模型，结合了多模态大语言模型和分割模型，能够统一生成临床文本和相应生物医学对象的分割。UniBiomed在多个生物医学任务中表现出色，显著提高了诊断效率。'}}}, {'id': 'https://huggingface.co/papers/2504.19043', 'title': 'Selecting Optimal Candidate Profiles in Adversarial Environments Using\n  Conjoint Analysis and Machine Learning', 'url': 'https://huggingface.co/papers/2504.19043', 'abstract': 'Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.', 'score': 2, 'issue_id': 3538, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': 'c7a7b2771c1e5c1f', 'authors': ['Connor T. Jerzak', 'Priyanshi Chandra', 'Rishi Hazra'], 'affiliations': ['Department of Government, University of Texas at Austin', 'Department of Statistics, Harvard College', 'Faculty of Informatics, Università della Svizzera Italiana'], 'pdf_title_img': 'assets/pdf/title_img/2504.19043.jpg', 'data': {'categories': [], 'emoji': '🗳️', 'ru': {'title': 'Оптимизация профилей политических кандидатов с помощью машинного обучения', 'desc': 'Статья рассматривает применение совместного анализа в политических исследованиях для оптимизации профилей кандидатов. Авторы предлагают метод стохастической интервенции для определения оптимального распределения атрибутов кандидата в условиях ограниченных данных. Исследование учитывает как одностороннюю, так и состязательную оптимизацию между двумя партиями. Результаты показывают, что состязательный подход дает более реалистичные прогнозы и лучше соответствует наблюдаемым кандидатам по сравнению с несостязательным методом.'}, 'en': {'title': 'Optimizing Political Candidate Selection through Adversarial Conjoint Analysis', 'desc': 'This paper explores the use of conjoint analysis in political candidate selection, focusing on how to identify optimal candidate profiles. It highlights the challenge of having too many possible candidate features compared to the limited number of observations in typical experiments. To solve this, the authors propose a stochastic intervention that generates a probability distribution of candidate attributes to maximize favorable outcomes. The study shows that considering adversarial dynamics between political parties leads to more accurate predictions of candidate success compared to traditional non-adversarial methods.'}, 'zh': {'title': '对抗性动态提升联合分析的洞察力', 'desc': '本文探讨了联合分析在政治候选人选择中的应用，特别是如何识别最佳候选人特征组合。由于特征组合的数量远超观察样本，无法精确确定最佳配置。为了解决这一问题，作者提出了一种最优随机干预方法，旨在通过概率分布实现最有利的平均结果。研究表明，在对抗性环境中，所建议的策略更可能与实际候选人匹配，揭示了对抗动态在社会科学实验数据分析中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.02567', 'title': 'Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities', 'url': 'https://huggingface.co/papers/2505.02567', 'abstract': "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).", 'score': 51, 'issue_id': 3655, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '0d49b4c41b7654a0', 'authors': ['Xinjie Zhang', 'Jintao Guo', 'Shanshan Zhao', 'Minghao Fu', 'Lunhao Duan', 'Guo-Hua Wang', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba Group', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02567.jpg', 'data': {'categories': ['#dataset', '#architecture', '#survey', '#multimodal', '#diffusion', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Объединение мультимодального понимания и генерации изображений: путь к универсальным моделям ИИ', 'desc': 'Эта статья представляет собой обзор современных подходов к объединению моделей мультимодального понимания и генерации изображений. Авторы анализируют три основных архитектурных парадигмы: основанные на диффузии, авторегрессивные и гибридные подходы. В работе также рассматриваются наборы данных и бенчмарки для унифицированных моделей. Обсуждаются ключевые проблемы в этой области, включая стратегию токенизации, кросс-модальное внимание и данные.'}, 'en': {'title': 'Bridging the Gap: Unifying Multimodal Understanding and Image Generation', 'desc': 'This paper surveys the integration of multimodal understanding and image generation models, which have traditionally developed separately. It highlights the architectural differences between autoregressive and diffusion-based models, emphasizing the challenges in unifying these approaches. The authors categorize existing unified models into three paradigms: diffusion-based, autoregressive-based, and hybrid methods. They also provide resources such as datasets and benchmarks to support future research in this emerging field.'}, 'zh': {'title': '统一多模态模型的未来探索', 'desc': '近年来，多模态理解模型和图像生成模型取得了显著进展，但这两个领域的发展相对独立，导致了不同的架构范式。自回归架构在多模态理解中占主导地位，而扩散模型则成为图像生成的基石。本文综述了当前统一框架的努力，介绍了多模态理解和文本到图像生成模型的基础概念及最新进展，并分析了三种主要的统一模型架构。我们还讨论了这一新兴领域面临的关键挑战，并提供了未来研究的参考资源。'}}}, {'id': 'https://huggingface.co/papers/2505.04588', 'title': 'ZeroSearch: Incentivize the Search Capability of LLMs without Searching', 'url': 'https://huggingface.co/papers/2505.04588', 'abstract': "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", 'score': 29, 'issue_id': 3647, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '24edc7c3c5e5e23d', 'authors': ['Hao Sun', 'Zile Qiao', 'Jiayan Guo', 'Xuanbo Fan', 'Yingyan Hou', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Yan Zhang'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.04588.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'ZeroSearch: обучение LLM эффективному поиску без реальных поисковых систем', 'desc': 'Статья представляет ZeroSearch - новую систему обучения с подкреплением для улучшения поисковых возможностей больших языковых моделей (LLM). В отличие от предыдущих подходов, ZeroSearch не требует взаимодействия с реальными поисковыми системами, что решает проблемы неконтролируемого качества документов и высоких затрат на API. Метод использует легковесную предобученную модель в качестве модуля поиска и стратегию постепенного ухудшения качества генерируемых документов во время обучения. Эксперименты показывают, что ZeroSearch эффективно улучшает поисковые способности LLM, причем модели с 14 миллиардами параметров даже превосходят реальные поисковые системы.'}, 'en': {'title': 'ZeroSearch: Enhancing LLM Search Without Real Engines', 'desc': 'This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines.'}, 'zh': {'title': '提升LLMs搜索能力的创新框架', 'desc': '有效的信息搜索对于提升大型语言模型（LLMs）的推理和生成能力至关重要。本文提出了一种名为ZeroSearch的强化学习框架，旨在提高LLMs的搜索能力，而无需与真实搜索引擎互动。该方法通过轻量级的监督微调，将LLM转变为一个检索模块，并在训练过程中逐步降低生成文档的质量，以激发模型的推理能力。实验结果表明，ZeroSearch能够有效提升LLMs的搜索能力，并在不同参数规模的模型中表现出良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.03821', 'title': 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models', 'url': 'https://huggingface.co/papers/2505.03821', 'abstract': "We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.", 'score': 18, 'issue_id': 3655, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 мая', 'en': 'May 3', 'zh': '5月3日'}, 'hash': 'abede452b390c7de', 'authors': ['Gracjan Góral', 'Alicja Ziarko', 'Piotr Miłoś', 'Michał Nauman', 'Maciej Wołczyk', 'Michał Kosiński'], 'affiliations': ['Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, S. Banacha 2, 02-097 Warsaw, PL', 'Graduate School of Business, Stanford University, Stanford, CA 94305, USA', 'IDEAS NCBR, Chmielna 69, 00-801 Warsaw, PL', 'Institute of Mathematics, Polish Academy of Sciences, J. & J. Sniadeckich 8, 00-656 Warsaw, PL', 'Robot Learning Lab, University of California, Berkeley, CA 94720, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.03821.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#training'], 'emoji': '👁️', 'ru': {'title': 'VLM модели: от распознавания объектов к пониманию перспективы', 'desc': 'Исследователи изучают способность моделей компьютерного зрения и обработки естественного языка (VLM) к визуальному восприятию перспективы. Они разработали набор из 144 визуальных задач, используя сцены с миниатюрной фигуркой человека и объектом в различных пространственных конфигурациях. Эксперименты показали, что современные модели, такие как GPT-4-Turbo и Claude Sonnet, хорошо справляются с пониманием сцен, но значительно хуже выполняют задачи пространственного мышления и восприятия перспективы. Результаты указывают на необходимость интеграции явных геометрических представлений и специализированных протоколов обучения в будущих разработках VLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Spatial Reasoning and Perspective Taking', 'desc': "This paper explores how well Vision Language Models (VLMs) can understand visual perspectives through a series of unique tasks. The tasks involve a humanoid figure and an object in various spatial arrangements, designed to test scene understanding, spatial reasoning, and visual perspective taking. The study evaluates several advanced models, finding that while they perform well in recognizing scenes, they struggle with more complex reasoning tasks. The results highlight a significant gap in the models' abilities, suggesting that future developments should focus on incorporating geometric representations and specialized training methods."}, 'zh': {'title': '提升视觉语言模型的空间推理能力', 'desc': '本文研究了视觉语言模型（VLMs）在视觉视角理解方面的能力，使用了一组新颖的视觉任务，这些任务灵感来源于人类的经典测试。我们设计了144个独特的视觉任务，通过系统地改变空间配置，如物体相对于人形小人偶的位置和方向，来评估模型的表现。每个视觉任务配有7个诊断问题，旨在评估场景理解、空间推理和视觉视角理解三个层次的视觉认知。评估结果显示，尽管这些先进模型在场景理解方面表现出色，但在空间推理和视角理解方面的表现显著下降，表明在复杂视觉任务中，表面物体识别与更深层次的空间和视角推理之间存在差距。'}}}, {'id': 'https://huggingface.co/papers/2505.00358', 'title': 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training', 'url': 'https://huggingface.co/papers/2505.00358', 'abstract': "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.", 'score': 14, 'issue_id': 3652, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '74b251baea8510bd', 'authors': ['Albert Ge', 'Tzu-Heng Huang', 'John Cooper', 'Avi Trost', 'Ziyi Chu', 'Satya Sai Srinath Namburi GNVV', 'Ziyang Cai', 'Kendall Park', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.00358.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#data', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'R&B: Умное смешивание данных для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый фреймворк R&B для оптимизации стратегий смешивания данных при обучении языковых моделей. R&B перегруппирует обучающие данные на основе семантического сходства и эффективно оптимизирует состав данных, используя матрицу Грама, полученную из градиентов доменов. Этот метод устраняет необходимость в дополнительных вычислениях для получения оценочной информации. Теоретический и эмпирический анализ показывает эффективность R&B по сравнению с неадаптивными подходами к смешиванию данных.'}, 'en': {'title': 'R&B: Smarter Data Mixing for Language Models', 'desc': 'This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods.'}, 'zh': {'title': 'R&B：高效的数据混合新策略', 'desc': '本文提出了一种新的数据混合策略R&B，旨在解决现有方法的两个主要缺陷。首先，R&B通过语义相似性重新划分训练数据，创建更细粒度的数据域，从而捕捉到重要的语义细节。其次，该框架通过利用训练过程中获得的领域梯度的Gram矩阵，优化数据组合，避免了额外的计算开销。实验结果表明，R&B在多种数据集上表现优异，能够以极小的计算成本超越现有的最先进数据混合策略。'}}}, {'id': 'https://huggingface.co/papers/2505.04622', 'title': 'PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer', 'url': 'https://huggingface.co/papers/2505.04622', 'abstract': 'Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '8205883cc18835a6', 'authors': ['Jingwen Ye', 'Yuze He', 'Yanning Zhou', 'Yiqin Zhu', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Wei Yang', 'Xiao Han'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04622.jpg', 'data': {'categories': ['#optimization', '#3d', '#cv', '#games'], 'emoji': '🧊', 'ru': {'title': 'Универсальная абстракция 3D-форм с помощью ИИ', 'desc': 'Статья представляет PrimitiveAnything - новый фреймворк для абстракции 3D-форм с помощью примитивов. Он использует трансформер, обученный на масштабных данных человеческих абстракций, для автоматической генерации сборок примитивов. PrimitiveAnything применяет унифицированную параметризацию для разных типов примитивов и генерирует высококачественные абстракции, соответствующие человеческому восприятию. Фреймворк демонстрирует хорошую обобщающую способность на разнообразных категориях форм и имеет потенциал для применения в играх и других 3D-приложениях.'}, 'en': {'title': 'Revolutionizing 3D Shape Understanding with PrimitiveAnything', 'desc': 'This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games.'}, 'zh': {'title': '形状抽象的新突破：PrimitiveAnything', 'desc': '形状原始抽象是将复杂的3D形状分解为简单几何元素的过程，这对人类视觉认知至关重要，并在计算机视觉和图形学中有广泛应用。现有的原始抽象方法通常依赖于几何优化，缺乏语义理解，或者仅从小规模、特定类别的数据集中学习，难以在多样的形状类别中进行泛化。我们提出了PrimitiveAnything，一个将形状原始抽象重新定义为原始组装生成任务的新框架。该框架通过大规模人类创作的抽象学习原始组装过程，从而能够更好地捕捉人类如何将复杂形状分解为原始元素。'}}}, {'id': 'https://huggingface.co/papers/2505.04512', 'title': 'HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation', 'url': 'https://huggingface.co/papers/2505.04512', 'abstract': 'Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '82e5839ef846d9d8', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Sen Liang', 'Yuan Zhou', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.04512.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Мультимодальная генерация персонализированных видео с сохранением идентичности', 'desc': 'HunyuanCustom - это мультимодальная система для генерации персонализированных видео, поддерживающая условия в виде изображений, аудио, видео и текста. Она использует модуль слияния текста и изображений на основе LLaVA для улучшенного мультимодального понимания, а также модуль усиления идентификации изображений для сохранения согласованности личности в кадрах. Система включает специальные механизмы для внедрения аудио- и видеоусловий, такие как AudioNet и сеть выравнивания признаков на основе патчей. Эксперименты показывают, что HunyuanCustom превосходит современные методы по согласованности идентичности, реалистичности и соответствию текста видео.'}, 'en': {'title': 'HunyuanCustom: Consistent and Multi-Modal Video Generation', 'desc': 'This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation.'}, 'zh': {'title': '多模态定制视频生成的创新之路', 'desc': '定制视频生成旨在根据用户定义的条件生成特定主题的视频，但现有方法在身份一致性和输入模态方面常常面临挑战。本文提出了HunyuanCustom，一个多模态定制视频生成框架，强调主题一致性，并支持图像、音频、视频和文本条件。我们的模型通过引入基于LLaVA的文本-图像融合模块和图像ID增强模块，解决了图像-文本条件生成任务，从而增强多模态理解。实验结果表明，HunyuanCustom在身份一致性、真实感和文本-视频对齐方面显著优于现有的最先进方法，验证了多模态条件和身份保持策略在可控视频生成中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.04364', 'title': "Benchmarking LLMs' Swarm intelligence", 'url': 'https://huggingface.co/papers/2505.04364', 'abstract': 'Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.', 'score': 12, 'issue_id': 3648, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '4b0575d2194aee20', 'authors': ['Kai Ruan', 'Mowen Huang', 'Ji-Rong Wen', 'Hao Sun'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04364.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#open_source', '#multimodal'], 'emoji': '🐝', 'ru': {'title': 'SwarmBench: Тестирование роевого интеллекта языковых моделей', 'desc': 'Статья представляет SwarmBench - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к роевому интеллекту в многоагентных системах. SwarmBench включает пять задач координации в 2D-сетке, где агенты ограничены локальным восприятием и коммуникацией. Результаты экспериментов показывают значительные различия в производительности LLM между задачами, выявляя сложности планирования в условиях неопределенности. Авторы предоставляют открытый инструментарий для воспроизводимых исследований координации на основе LLM в многоагентных системах.'}, 'en': {'title': 'Unlocking Swarm Intelligence in Language Models', 'desc': "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."}, 'zh': {'title': '探索大型语言模型的群体智能潜力', 'desc': '大型语言模型（LLMs）在复杂推理方面显示出潜力，但它们在多智能体系统（MAS）中在严格约束下的协调能力仍然未被充分探索，尤其是在群体智能的细微差别方面。现有基准测试往往无法完全捕捉到在不完整时空信息下，智能体进行去中心化协调所面临的独特挑战。为此，我们引入了SwarmBench，这是一个新颖的基准，旨在系统评估LLMs作为去中心化智能体的群体智能能力。通过评估多个领先的LLMs，我们发现它们在任务中的表现差异显著，突显了在局部信息限制下的协调困难。'}}}, {'id': 'https://huggingface.co/papers/2505.04528', 'title': 'Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving', 'url': 'https://huggingface.co/papers/2505.04528', 'abstract': 'As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.', 'score': 7, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '0e9e0d509e4b4624', 'authors': ['Qi Liu', 'Xinhao Zheng', 'Renqiu Xia', 'Xingzhi Qi', 'Qinxiang Cao', 'Junchi Yan'], 'affiliations': ['Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04528.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#alignment', '#interpretability', '#reasoning', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Формальная верификация процесса решения задач искусственным интеллектом', 'desc': 'Статья представляет новый подход к формализации решения задач как марковского процесса принятия решений. Авторы предлагают фреймворк FPS (Formal Problem-Solving), использующий среды формального доказательства теорем для верификации процесса решения задач. Также представлен D-FPS (Deductive FPS), разделяющий решение и проверку ответа для лучшего соответствия человеческому подходу. Созданы три новых набора данных для оценки систем решения задач, а также предложен метод RPE для формальной верификации корректности ответов.'}, 'en': {'title': 'Revolutionizing Problem-Solving with Formal Frameworks', 'desc': 'This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods.'}, 'zh': {'title': '形式化问题解决的新框架', 'desc': '这篇论文探讨了问题解决的形式化，提出了一种将问题解决视为确定性马尔可夫决策过程的框架。作者介绍了FPS（正式问题解决）框架，利用现有的正式定理证明环境进行过程验证的问题解决。为了提高人类对齐，论文还提出了D-FPS（演绎FPS），将求解与答案验证解耦。最后，作者构建了三个基准测试，并提出了一种符号方法RPE来评估答案的正确性。'}}}, {'id': 'https://huggingface.co/papers/2505.03912', 'title': 'OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2505.03912', 'abstract': 'Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.', 'score': 6, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f7347c1b093f9488', 'authors': ['Can Cui', 'Pengxiang Ding', 'Wenxuan Song', 'Shuanghao Bai', 'Xinyang Tong', 'Zirui Ge', 'Runze Suo', 'Wanqi Zhou', 'Yang Liu', 'Bofang Jia', 'Han Zhao', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['HKUST(GZ)', 'Westlake University', 'Xian Jiaotong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03912.jpg', 'data': {'categories': ['#architecture', '#optimization', '#agents', '#open_source', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Открытая платформа для исследования двухсистемных VLA архитектур', 'desc': 'Статья посвящена двухсистемным архитектурам VLA (Vision-Language-Action) в области воплощенного интеллекта. Авторы анализируют и сравнивают существующие архитектуры, проводя систематическую эмпирическую оценку их ключевых элементов. Целью работы является создание открытой модели с низкими вычислительными затратами для дальнейших исследований. Проект планирует регулярно обновляться новыми экспериментальными выводами и улучшенными открытыми моделями.'}, 'en': {'title': 'Empowering Embodied Intelligence with Open-Source VLA Models', 'desc': 'This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers.'}, 'zh': {'title': '推动双系统VLA架构的开源探索', 'desc': '本文探讨了双系统视觉-语言-行动（VLA）架构在具身智能研究中的重要性，并指出目前缺乏足够的开源工作来进行性能分析和优化。作者总结并比较了现有双系统架构的结构设计，并对其核心设计元素进行了系统的实证评估。最终，本文将提供一个低成本的开源模型，以便进一步探索和研究。该项目将持续更新，提供更多实验结论和性能改进的开源模型供大家选择。'}}}, {'id': 'https://huggingface.co/papers/2505.04606', 'title': 'OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution', 'url': 'https://huggingface.co/papers/2505.04606', 'abstract': "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", 'score': 5, 'issue_id': 3657, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '25e97f182730fc25', 'authors': ['Lianghong Guo', 'Wei Tao', 'Runhan Jiang', 'Yanlin Wang', 'Jiachi Chen', 'Xilin Liu', 'Yuchi Ma', 'Mingzhi Mao', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University, China', 'Huawei Cloud Computing Technologies Co., Ltd., China', 'Independent Researcher, China', 'Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04606.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset', '#long_context', '#multimodal'], 'emoji': '🐙', 'ru': {'title': 'OmniGIRL: Вызов для языковых моделей в решении задач GitHub', 'desc': 'Статья представляет OmniGIRL - новый многоязычный, мультимодальный и мультидоменный эталонный набор данных для автоматического разрешения проблем на GitHub. OmniGIRL включает 959 задач из репозиториев на четырех языках программирования и восьми различных доменах. Оценка показала, что современные языковые модели (LLM) демонстрируют ограниченную эффективность на OmniGIRL, особенно при работе с изображениями. Анализ причин неудач LLM на OmniGIRL предоставляет insights для будущих улучшений.'}, 'en': {'title': 'OmniGIRL: A Comprehensive Benchmark for GitHub Issue Resolution', 'desc': 'This paper introduces OmniGIRL, a new benchmark for automatically resolving GitHub issues using large language models (LLMs). Unlike existing benchmarks, OmniGIRL is designed to be multilingual, multimodal, and multi-domain, addressing the limitations of focusing on a single programming language and a narrow range of issues. The benchmark includes 959 instances from four programming languages and eight domains, highlighting the diversity of real-world problems. Evaluation results show that current LLMs perform poorly on this benchmark, particularly in resolving issues that require understanding images, indicating a need for further advancements in model capabilities.'}, 'zh': {'title': 'OmniGIRL：多语言多模态的GitHub问题解决基准', 'desc': '本文提出了OmniGIRL，一个多语言、多模态和多领域的GitHub问题解决基准。现有的基准存在三个主要限制：只关注单一编程语言、覆盖领域狭窄以及仅依赖文本信息。OmniGIRL包含来自四种编程语言和八个不同领域的959个任务实例，旨在更全面地评估大型语言模型的能力。我们的评估显示，当前的语言模型在OmniGIRL上的表现有限，尤其在处理需要理解图像的问题时表现更差。'}}}, {'id': 'https://huggingface.co/papers/2505.03570', 'title': 'OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents', 'url': 'https://huggingface.co/papers/2505.03570', 'abstract': 'In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.', 'score': 4, 'issue_id': 3654, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'e87199c8805bce4f', 'authors': ['Mariya Davydova', 'Daniel Jeffries', 'Patrick Barker', 'Arturo Márquez Flores', 'Sinéad Ryan'], 'affiliations': ['Kentauros AI Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.03570.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#optimization', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'OSUniverse: новый стандарт оценки ИИ-агентов в графическом интерфейсе', 'desc': 'В статье представлен OSUniverse - комплексный многомодальный бенчмарк для ИИ-агентов, навигирующих в графическом интерфейсе. Бенчмарк включает задачи разной сложности, от простых кликов до многошаговых тестов в нескольких приложениях. Современные агенты достигают не более 50% успеха, в то время как обычные офисные работники справляются со всеми задачами. Бенчмарк имеет автоматизированную систему валидации с погрешностью менее 2%.'}, 'en': {'title': 'OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks', 'desc': 'This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.'}, 'zh': {'title': 'OSUniverse：GUI导航AI的全新基准', 'desc': '本文介绍了OSUniverse，这是一个针对高级GUI导航AI代理的复杂多模态桌面任务基准，旨在易用性、可扩展性、全面覆盖测试案例和自动验证方面表现出色。我们将任务分为不同复杂度的级别，从基本的精确点击到需要灵活性、精确性和清晰思维的多步骤、多应用程序测试。在基准的第一版中，我们调整了测试案例的复杂性，以确保当时的最先进（SOTA）代理的结果不超过50%，而普通白领工人可以完美完成所有这些任务。该基准可以手动评分，同时我们还引入了一个平均错误率低于2%的自动验证机制，为全面自动化测量GUI导航AI代理的进展、能力和有效性提供了坚实基础。'}}}, {'id': 'https://huggingface.co/papers/2505.03418', 'title': 'Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey', 'url': 'https://huggingface.co/papers/2505.03418', 'abstract': 'Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.', 'score': 4, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '8417799a01a2ecc2', 'authors': ['Da Zheng', 'Lun Du', 'Junwei Su', 'Yuchen Tian', 'Yuqi Zhu', 'Jintian Zhang', 'Lanning Wei', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['Ant Group, China', 'The University of Hong Kong, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03418.jpg', 'data': {'categories': ['#rl', '#survey', '#math', '#training', '#reasoning', '#science', '#data'], 'emoji': '🧠', 'ru': {'title': 'LLM: Новый рубеж в решении сложных задач', 'desc': 'Эта статья исследует возможности и ограничения больших языковых моделей (LLM) в решении сложных задач. Авторы рассматривают такие техники, как рассуждения по цепочке мыслей (Chain-of-Thought), расширение знаний и различные методы верификации на основе LLM и инструментов. В статье обсуждаются проблемы применения LLM в различных областях, включая разработку программного обеспечения, математические рассуждения и доказательства, анализ данных и научные исследования. Также рассматриваются фундаментальные ограничения текущих решений на основе LLM и будущие направления развития в контексте многоступенчатых рассуждений, интеграции доменных знаний и верификации результатов.'}, 'en': {'title': 'Unlocking Complex Problem-Solving with Large Language Models', 'desc': 'This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving.'}, 'zh': {'title': '大型语言模型：复杂问题解决的新工具', 'desc': '本论文探讨了大型语言模型（LLMs）在复杂问题解决中的能力和局限性。与传统计算系统不同，LLMs结合了强大的计算能力和人类推理的近似，能够生成解决方案和进行推理。尽管LLMs在多步骤推理、领域知识整合和结果验证方面面临挑战，但它们在软件工程、数学推理、数据分析和科学研究等领域的应用潜力巨大。本文还讨论了当前LLM解决方案的基本局限性以及未来在复杂问题解决中的发展方向。'}}}, {'id': 'https://huggingface.co/papers/2505.04601', 'title': 'OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning', 'url': 'https://huggingface.co/papers/2505.04601', 'abstract': "OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.", 'score': 2, 'issue_id': 3666, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '0b9c03d9680cfe04', 'authors': ['Xianhang Li', 'Yanqing Liu', 'Haoqin Tu', 'Hongru Zhu', 'Cihang Xie'], 'affiliations': ['University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.04601.jpg', 'data': {'categories': ['#small_models', '#dataset', '#open_source', '#multimodal', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'OpenVision: открытые визуальные энкодеры для мультимодальных моделей', 'desc': 'OpenVision представляет собой семейство полностью открытых и экономически эффективных визуальных энкодеров, которые соответствуют или превосходят производительность CLIP от OpenAI при интеграции в мультимодальные модели. Энкодеры OpenVision основаны на существующих работах, таких как CLIPS и Recap-DataComp-1B, но предлагают новые идеи по улучшению качества кодирования. Разработчики предоставляют энкодеры различных размеров (от 5,9 до 632,1 миллионов параметров), что позволяет гибко балансировать между производительностью и эффективностью при создании мультимодальных моделей. OpenVision заполняет пробел в области открытых визуальных энкодеров, предоставляя полностью открытые данные и методы обучения.'}, 'en': {'title': 'OpenVision: Open and Efficient Vision Encoders for Multimodal Models', 'desc': "This paper introduces OpenVision, a new family of vision encoders that are fully open and cost-effective, designed to compete with OpenAI's CLIP. OpenVision not only matches but can also surpass CLIP's performance when used in multimodal frameworks like LLaVA. The authors leverage existing methodologies and datasets to improve encoder quality and provide insights into enhancing multimodal models. By offering a range of models with varying parameters, OpenVision allows users to choose between high performance and efficiency for their specific applications."}, 'zh': {'title': '开放视觉编码器，提升多模态模型性能', 'desc': 'OpenVision是一个完全开放的视觉编码器家族，旨在与OpenAI的CLIP相媲美或超越其性能。该论文展示了如何利用现有的训练框架和数据集，提升编码器的质量，并为多模态模型的发展提供实用的好处。OpenVision提供了从590万到6.32亿参数的多种选择，使得开发者可以在模型容量和效率之间灵活权衡。通过这些开放的资源，OpenVision为多模态模型的构建提供了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2505.04253', 'title': 'LLM-Independent Adaptive RAG: Let the Question Speak for Itself', 'url': 'https://huggingface.co/papers/2505.04253', 'abstract': 'Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '7ce9a465202a9c3c', 'authors': ['Maria Marina', 'Nikolay Ivanov', 'Sergey Pletenev', 'Mikhail Salnikov', 'Daria Galimzianova', 'Nikita Krayko', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.04253.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rag', '#benchmark', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Эффективный адаптивный поиск без использования языковых моделей', 'desc': 'Исследование предлагает новые методы адаптивного поиска для больших языковых моделей, не зависящие от самих моделей. Авторы изучили 27 признаков, организованных в 7 групп, и их гибридные комбинации для определения необходимости поиска дополнительной информации. Методы были оценены на 6 наборах данных вопросно-ответных систем. Результаты показывают, что предложенный подход соответствует производительности сложных методов, основанных на языковых моделях, при значительном повышении эффективности.'}, 'en': {'title': 'Efficient Adaptive Retrieval: Enhancing QA with External Information', 'desc': 'This paper addresses the issue of hallucinations in Large Language Models (LLMs) and presents a solution through Retrieval-Augmented Generation (RAG). The authors propose lightweight, LLM-independent adaptive retrieval methods that utilize external information, aiming to improve efficiency while maintaining performance. They analyze 27 features across 7 groups to create effective hybrid combinations for retrieval. The results indicate that their approach can achieve comparable performance to complex LLM-based methods but with significant efficiency improvements.'}, 'zh': {'title': '轻量级自适应检索，提升效率与准确性', 'desc': '大型语言模型（LLMs）容易出现幻觉，而增强检索生成（RAG）可以帮助减轻这一问题，但代价高昂且可能导致错误信息。自适应检索旨在仅在必要时进行检索，但现有方法依赖于基于LLM的不确定性估计，效率低下且不切实际。我们提出了一种基于外部信息的轻量级LLM独立自适应检索方法，研究了27个特征并将其组织成7个组及其混合组合。我们的评估结果表明，该方法在6个问答数据集上的表现与复杂的LLM方法相当，同时实现了显著的效率提升，展示了外部信息在自适应检索中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.03538', 'title': 'RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT', 'url': 'https://huggingface.co/papers/2505.03538', 'abstract': 'Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '9e7cb17dec2eda27', 'authors': ['Chuyu Zhao', 'Hao Huang', 'Jiashuo Guo', 'Ziyu Shen', 'Zhongwei Zhou', 'Jie Liu', 'Zekuan Yu'], 'affiliations': ['Academy for Engineering and Technology, Fudan University, Shanghai 200433, China', 'Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China', 'School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03538.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#transfer_learning', '#training'], 'emoji': '🦷', 'ru': {'title': 'Умное обучение для точной сегментации зубов', 'desc': 'Статья представляет новый метод полуконтролируемого обучения для сегментации зубов на КЛКТ-снимках, называемый Region-Aware Instructive Learning (RAIL). RAIL использует двухгрупповую двухстудентную архитектуру, где каждая группа содержит два студента, управляемых общим учителем. Метод вводит два инструктивных механизма: Disagreement-Focused Supervision (DFS) Controller для улучшения обучения с учителем и Confidence-Aware Learning (CAL) Modulator для повышения надежности псевдо-меток. Эксперименты показывают, что RAIL превосходит современные методы при ограниченном количестве размеченных данных.'}, 'en': {'title': 'Enhancing 3D Tooth Segmentation with RAIL: A Smart Learning Approach', 'desc': 'This paper presents a new method called Region-Aware Instructive Learning (RAIL) for improving 3D tooth segmentation from CBCT scans using semi-supervised learning. RAIL addresses challenges like limited supervision in ambiguous areas and the unreliability of pseudo-labels by employing a dual-group, dual-student framework with a shared teacher network. It introduces two key mechanisms: Disagreement-Focused Supervision (DFS) to enhance learning in uncertain regions, and Confidence-Aware Learning (CAL) to stabilize predictions by focusing on high-confidence areas. Experimental results demonstrate that RAIL outperforms existing methods, making it a significant advancement in the field of medical image segmentation.'}, 'zh': {'title': '区域感知指导学习：提升3D牙齿分割的半监督方法', 'desc': '半监督学习在CBCT扫描的3D牙齿分割中变得越来越重要，尤其是在标注数据稀缺的情况下。然而，现有方法面临两个主要挑战：在结构模糊或标注错误的区域，监督训练中的纠正监督有限；以及在未标注数据上，由于不可靠的伪标签导致的性能下降。为了解决这些问题，我们提出了区域感知指导学习（RAIL），这是一种双组双学生的半监督框架，通过交替训练促进组间知识转移和协作指导，同时减少对单一模型特征的过拟合。RAIL引入了两种指导机制，分别是关注分歧的监督控制器和基于置信度的学习调节器，以提高模型在不确定区域的学习效果。'}}}, {'id': 'https://huggingface.co/papers/2505.03105', 'title': 'Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation', 'url': 'https://huggingface.co/papers/2505.03105', 'abstract': 'Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI\'s evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.', 'score': 1, 'issue_id': 3657, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '24cdaf99b9b04dad', 'authors': ['Xule Lin'], 'affiliations': ['Department of Management and Entrepreneurship, Imperial College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.03105.jpg', 'data': {'categories': ['#agents', '#healthcare', '#science', '#ethics', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Cognitio Emergens: новая парадигма коэволюции человека и ИИ в науке', 'desc': 'Статья представляет концепцию Cognitio Emergens (CE) - новую модель сотрудничества человека и искусственного интеллекта в научных исследованиях. CE описывает, как распределяются роли между людьми и ИИ, какие эпистемические способности возникают в процессе взаимодействия, и какие силы влияют на эволюцию этих отношений. Модель подчеркивает динамичный и рекурсивный характер создания научных знаний в партнерстве человека и ИИ. CE предлагает сбалансированный взгляд на роль ИИ в науке, избегая как чрезмерного оптимизма, так и необоснованных страхов.'}, 'en': {'title': 'Transforming Scientific Collaboration: Humans and AI as Co-Evolutionary Partners', 'desc': 'This paper discusses how the relationship between humans and AI in scientific research is changing from a simple tool-user dynamic to a more collaborative partnership. It introduces a new framework called Cognitio Emergens (CE) that addresses the limitations of existing models by focusing on the evolving roles and interactions between humans and AI over time. CE includes three main components: Agency Configurations that describe how authority is shared, Epistemic Dimensions that outline capabilities developed through collaboration, and Partnership Dynamics that explore how these relationships change. By viewing human-AI collaboration as a co-evolutionary process, the framework aims to enhance scientific understanding while ensuring that human input remains significant in the face of advancing AI capabilities.'}, 'zh': {'title': '人类与AI的共同进化：知识创造的新视角', 'desc': '这篇论文探讨了人类与人工智能（AI）之间的合作关系如何从简单的工具使用者转变为共同进化的知识伙伴。文章介绍了Cognitio Emergens（CE）框架，旨在解决现有模型的局限性，强调科学理解是如何通过人类与AI的互动逐步形成的。CE框架包括三个主要组成部分：代理配置、认知维度和伙伴动态，帮助描述人类与AI之间的权力分配、合作能力和关系演变。通过重新定义人类与AI的科学合作，CE提供了促进有意义的人类参与和科学突破的概念工具。'}}}, {'id': 'https://huggingface.co/papers/2505.02820', 'title': 'AutoLibra: Agent Metric Induction from Open-Ended Feedback', 'url': 'https://huggingface.co/papers/2505.02820', 'abstract': 'Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don\'t click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent\'s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra\'s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.', 'score': 1, 'issue_id': 3661, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '52333e4c868e25f8', 'authors': ['Hao Zhu', 'Phil Cuvin', 'Xinkai Yu', 'Charlotte Ka Yee Yan', 'Jason Zhang', 'Diyi Yang'], 'affiliations': ['stanford.edu', 'upenn.edu', 'utoronto.ca'], 'pdf_title_img': 'assets/pdf/title_img/2505.02820.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rlhf', '#alignment', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'AutoLibra: Умная оценка ИИ-агентов на основе отзывов пользователей', 'desc': "AutoLibra - это новая система оценки агентов искусственного интеллекта, которая преобразует открытые отзывы пользователей в конкретные метрики для оценки поведения агентов. Система использует кластеризацию схожих положительных и отрицательных моделей поведения для создания четких метрик с конкретными примерами. AutoLibra также предлагает мета-метрики 'покрытие' и 'избыточность' для оценки согласованности набора метрик с открытыми отзывами. Экспериментально показано, что метрики AutoLibra эффективнее стандартных для улучшения производительности агентов в различных задачах."}, 'en': {'title': 'Transforming Feedback into Fine-Grained Agent Evaluation Metrics', 'desc': "The paper introduces AutoLibra, a novel framework for evaluating agents using fine-grained metrics derived from open-ended human feedback. It transforms qualitative feedback into quantitative metrics by clustering behaviors and defining clear examples, allowing for a more nuanced assessment of agent performance. AutoLibra also proposes two meta-metrics, 'coverage' and 'redundancy', to ensure that the evaluation metrics align well with the feedback provided. Experimental results show that AutoLibra improves agent performance significantly compared to traditional success metrics, making it a valuable tool for enhancing language agents."}, 'zh': {'title': 'AutoLibra：智能体评估的新标准', 'desc': '本文提出了一种名为AutoLibra的框架，用于评估智能体的表现。该框架通过将开放式人类反馈转化为细粒度的评估指标，来更好地衡量智能体的行为。AutoLibra通过对反馈进行归类和定义，生成具体的评估标准，并利用这些标准来优化智能体的表现。实验结果表明，AutoLibra能够生成比现有基准更有效的评估指标，并在多种任务中显著提升智能体的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02393', 'title': 'Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection', 'url': 'https://huggingface.co/papers/2505.02393', 'abstract': 'Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.', 'score': 1, 'issue_id': 3651, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'b5c708abbb25e1ce', 'authors': ['Sungheon Jeong', 'Jihong Park', 'Mohsen Imani'], 'affiliations': ['MOLOCO', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.02393.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#synthetic'], 'emoji': '🕵️', 'ru': {'title': 'Синтез событий из RGB для точного обнаружения видеоаномалий', 'desc': 'В этой статье представлен метод IEF-VAD для обнаружения аномалий в видео, который объединяет RGB-кадры с синтезированными событийными представлениями. Система моделирует шум датчика, применяет покадровые обновления в стиле фильтра Калмана и итеративно уточняет слитое латентное состояние. IEF-VAD достигает нового уровня производительности на нескольких реальных тестовых наборах данных для обнаружения аномалий. Метод подчеркивает важность синтетических событийных представлений для выделения ключевых признаков движения в задачах анализа видео.'}, 'en': {'title': 'Enhancing Video Anomaly Detection with Image-Event Fusion', 'desc': 'The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data.'}, 'zh': {'title': '图像与事件融合，提升视频异常检测的准确性', 'desc': '现有的视频异常检测器主要依赖RGB帧，但这些帧缺乏捕捉突发或瞬态运动线索的时间分辨率。为了解决这个问题，我们提出了一种图像-事件融合的视频异常检测框架（IEF-VAD），该框架直接从RGB视频合成事件表示，并通过一种基于不确定性的过程将其与图像特征融合。该系统通过拉普拉斯近似建模重尾传感器噪声，应用卡尔曼风格的逐帧更新来平衡时间上的模态，并迭代优化融合的潜在状态以消除残余的跨模态噪声。IEF-VAD在多个真实世界的异常检测基准上设定了新的最先进水平，展示了合成事件表示在强调RGB帧中常被低估的运动线索方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.01449', 'title': 'COSMOS: Predictable and Cost-Effective Adaptation of LLMs', 'url': 'https://huggingface.co/papers/2505.01449', 'abstract': 'Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.', 'score': 1, 'issue_id': 3665, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'cc6993ec1bb06272', 'authors': ['Jiayu Wang', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.01449.jpg', 'data': {'categories': ['#optimization', '#data', '#inference', '#training', '#benchmark'], 'emoji': '🔮', 'ru': {'title': 'COSMOS: Эффективное предсказание результатов адаптации LLM', 'desc': 'Статья представляет COSMOS - унифицированную систему предсказания для эффективного выбора стратегии адаптации больших языковых моделей (LLM). Авторы предлагают использовать легковесные прокси-модели и масштабируемые законы для прогнозирования производительности fine-tuning и обучения в контексте. Система COSMOS позволяет значительно сократить вычислительные затраты при сохранении высокой точности предсказаний. Эксперименты на восьми бенчмарках показали снижение вычислительных расходов в среднем на 92.72%.'}, 'en': {'title': 'COSMOS: Smart Predictions for Efficient LLM Adaptation', 'desc': 'This paper addresses the challenge of selecting the best model and adaptation strategy for large language models (LLMs) while managing resource constraints. It introduces COSMOS, a unified prediction framework designed to estimate the performance and cost of different adaptation strategies without the need for extensive trials. The framework utilizes lightweight proxy models and low-sample scaling laws to predict outcomes efficiently. The results indicate that COSMOS can significantly reduce computational costs while maintaining high prediction accuracy across various benchmarks.'}, 'zh': {'title': 'COSMOS：高效预测适应结果的框架', 'desc': '大型语言模型（LLMs）在多种任务中表现出色，但在资源有限的情况下选择最佳模型和适应策略非常具有挑战性。我们提出了COSMOS，一个统一的预测框架，可以在最低成本下高效估计适应结果。该框架利用轻量级代理模型和低样本扩展法则来预测微调性能和检索增强的上下文学习。我们的评估表明，COSMOS在八个基准测试中实现了高预测准确性，同时平均减少了92.72%的计算成本。'}}}, {'id': 'https://huggingface.co/papers/2505.07916', 'title': 'MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder', 'url': 'https://huggingface.co/papers/2505.07916', 'abstract': 'We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.', 'score': 80, 'issue_id': 3752, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '30175415a859995c', 'authors': ['Bowen Zhang', 'Congchao Guo', 'Geng Yang', 'Hang Yu', 'Haozhe Zhang', 'Heidi Lei', 'Jialong Mai', 'Junjie Yan', 'Kaiyue Yang', 'Mingqi Yang', 'Peikai Huang', 'Ruiyang Jin', 'Sitan Jiang', 'Weihua Cheng', 'Yawei Li', 'Yichen Xiao', 'Yiying Zhou', 'Yongmao Zhang', 'Yuan Lu', 'Yucen He'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07916.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#games', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон', 'desc': 'MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обучаемый кодировщик говорящего, который извлекает характеристики тембра из эталонного аудио без необходимости его транскрипции. Модель поддерживает 32 языка и демонстрирует отличные результаты по многим метрикам, достигая лучших показателей в клонировании голоса. MiniMax-Speech также обладает расширяемостью, позволяющей реализовать различные приложения, такие как управление эмоциями голоса и синтез тембра из текстового описания.'}, 'en': {'title': 'Revolutionizing Speech Synthesis with MiniMax-Speech', 'desc': 'MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.'}, 'zh': {'title': 'MiniMax-Speech：高质量语音生成的新突破', 'desc': 'MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2505.07591', 'title': 'A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models', 'url': 'https://huggingface.co/papers/2505.07591', 'abstract': "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.", 'score': 7, 'issue_id': 3750, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'ca7c47ccc0066e55', 'authors': ['Junjie Ye', 'Caishuang Huang', 'Zhuohan Chen', 'Wenjie Fu', 'Chenyuan Yang', 'Leyi Yang', 'Yilong Wu', 'Peng Wang', 'Meng Zhou', 'Xiaolong Yang', 'Tao Gui', 'Qi Zhang', 'Zhongchao Shi', 'Jianping Fan', 'Xuanjing Huang'], 'affiliations': ['Institute of Modern Languages and Linguistics, Fudan University', 'Lenovo Research', 'School of Computer Science, Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.07591.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#optimization', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Многомерная оценка следования инструкциям для больших языковых моделей', 'desc': 'Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV.'}, 'en': {'title': 'Enhancing LLMs with Multi-Dimensional Constraints', 'desc': 'This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters.'}, 'zh': {'title': '多维约束框架提升语言模型性能', 'desc': '本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.07215', 'title': 'Measuring General Intelligence with Generated Games', 'url': 'https://huggingface.co/papers/2505.07215', 'abstract': 'We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.', 'score': 5, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'ed99ec3875dd9a95', 'authors': ['Vivek Verma', 'David Huang', 'William Chen', 'Dan Klein', 'Nicholas Tomlin'], 'affiliations': ['Computer Science Division, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.07215.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#rl', '#games', '#synthetic'], 'emoji': '🎮', 'ru': {'title': 'gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды', 'desc': 'В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моделей к общему рассуждению. gg-bench автоматически генерирует новые игры с помощью большой языковой модели (LLM), которая создает описания и код игр. Для оценки языковые модели играют против обученных с помощью обучения с подкреплением агентов. Результаты показывают, что современные LLM достигают низких результатов (7-9% побед), в то время как специализированные модели рассуждений достигают 31-36% побед.'}, 'en': {'title': 'Dynamic Game Environments for Evaluating AI Reasoning', 'desc': 'The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI.'}, 'zh': {'title': 'gg-bench：评估语言模型推理能力的新基准', 'desc': '我们提出了gg-bench，这是一个用于评估语言模型一般推理能力的游戏环境集合。与大多数静态基准测试不同，gg-bench是一个数据生成过程，可以随时生成新的评估实例。具体来说，gg-bench通过使用大型语言模型生成新游戏的自然语言描述，并将每个游戏实现为Gym环境，最后通过自我对弈训练强化学习代理。我们通过模型在这些游戏中与强化学习代理的胜率来评估语言模型，发现当前最先进的语言模型在gg-bench上的胜率仅为7-9%。'}}}, {'id': 'https://huggingface.co/papers/2505.05464', 'title': 'Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging', 'url': 'https://huggingface.co/papers/2505.05464', 'abstract': 'Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.', 'score': 4, 'issue_id': 3760, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'd0d13229ec81018d', 'authors': ['Shiqi Chen', 'Jinghan Zhang', 'Tongyao Zhu', 'Wei Liu', 'Siyang Gao', 'Miao Xiong', 'Manling Li', 'Junxian He'], 'affiliations': ['City University of Hong Kong', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05464.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#architecture', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Объединение зрения и мышления: новый подход к мультимодальному ИИ', 'desc': 'Статья исследует возможность объединения моделей компьютерного зрения и языковых моделей для улучшения восприятия и рассуждения. Авторы предлагают метод слияния параметров моделей разных модальностей, что позволяет передавать способности к рассуждению от языковых моделей к визуально-языковым моделям без дополнительного обучения. Эксперименты показывают, что восприятие в основном кодируется в ранних слоях модели, а рассуждение - в средних и поздних. После слияния все слои начинают участвовать в рассуждении, в то время как распределение способностей восприятия остается неизменным.'}, 'en': {'title': 'Merging Models: Bridging Vision and Reasoning', 'desc': 'This paper investigates how to combine the visual understanding of Vision-Language Models (VLMs) with the reasoning skills of Large Language Models (LLMs) through a process called model merging. The authors propose a novel approach that merges models from different modalities, allowing VLMs to gain reasoning capabilities without the need for additional training. Their experiments reveal that while perception is mainly captured in the early layers of the model, reasoning is enhanced across all layers after merging. This study highlights the effectiveness of model merging for improving multimodal integration and understanding the interplay between perception and reasoning.'}, 'zh': {'title': '模型合并：跨模态的推理与感知结合', 'desc': '视觉-语言模型（VLMs）将视觉感知与大型语言模型（LLMs）的推理能力结合在一起。然而，这两种能力如何结合并相互贡献仍然不太清楚。我们通过模型合并的方法来探索感知与推理的组合，连接不同模型的参数。实验表明，模型合并能够成功地将LLMs的推理能力转移到VLMs，并且有助于理解感知与推理的内部机制。'}}}, {'id': 'https://huggingface.co/papers/2505.08638', 'title': 'TRAIL: Trace Reasoning and Agentic Issue Localization', 'url': 'https://huggingface.co/papers/2505.08638', 'abstract': 'The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '823a40ec2bb1f793', 'authors': ['Darshan Deshpande', 'Varun Gangal', 'Hersh Mehta', 'Jitin Krishnan', 'Anand Kannappan', 'Rebecca Qian'], 'affiliations': ['Patronus AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.08638.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#agents', '#open_source'], 'emoji': '🕵️', 'ru': {'title': 'Новый подход к отладке агентных систем: от таксономии ошибок к масштабируемой оценке', 'desc': 'Статья представляет новый подход к оценке сложных рабочих процессов агентных систем. Авторы разработали таксономию типов ошибок в агентных системах и создали набор данных TRAIL из 148 аннотированных трасс. Исследование показало, что современные языковые модели с длинным контекстом плохо справляются с отладкой трасс, даже лучшая модель Gemini-2.5-pro достигла всего 11% точности на TRAIL. Датасет и код открыты для дальнейших исследований в области масштабируемой оценки агентных рабочих процессов.'}, 'en': {'title': 'Revolutionizing Evaluation for Agentic Workflows', 'desc': 'This paper addresses the challenges of evaluating agentic workflows, which are increasingly used in various fields. Current methods rely on manual analysis of complex workflow traces, making them inefficient as the volume of data grows. The authors propose a new framework that includes a taxonomy of error types specific to agentic systems and introduce a dataset of 148 annotated traces for evaluation. Their findings indicate that existing large language models struggle with debugging these traces, highlighting the need for improved evaluation techniques in this area.'}, 'zh': {'title': '智能工作流评估的新方法', 'desc': '随着智能工作流在各个领域的广泛应用，系统地评估这些系统生成的复杂轨迹变得至关重要。目前的评估方法依赖于人工、特定领域的分析，这种方法无法适应日益复杂和庞大的智能输出。错误分析在这些环境中变得更加复杂，因为外部工具输出与语言模型推理之间的相互作用，使得调试变得更加困难。本文提出了一种针对智能工作流轨迹的动态评估方法，并引入了一种错误类型的正式分类法，同时构建了148个大型人类标注的轨迹数据集，以支持未来在智能工作流评估方面的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.08175', 'title': 'Fast Text-to-Audio Generation with Adversarial Post-Training', 'url': 'https://huggingface.co/papers/2505.08175', 'abstract': 'Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating approx12s of 44.1kHz stereo audio in approx75ms on an H100, and approx7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '845d8b64408c2d62', 'authors': ['Zachary Novack', 'Zach Evans', 'Zack Zukowski', 'Josiah Taylor', 'CJ Carr', 'Julian Parker', 'Adnan Al-Sinan', 'Gian Marco Iodice', 'Julian McAuley', 'Taylor Berg-Kirkpatrick', 'Jordi Pons'], 'affiliations': ['Arm', 'Stability AI', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.08175.jpg', 'data': {'categories': ['#audio', '#inference', '#optimization', '#diffusion', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Сверхбыстрая генерация аудио по тексту с помощью ARC пост-обучения', 'desc': 'Статья представляет новый метод ускорения моделей диффузии и потоков для генерации аудио по тексту - Adversarial Relativistic-Contrastive (ARC) пост-обучение. Это первый состязательный алгоритм ускорения, не основанный на дистилляции. ARC сочетает релятивистский состязательный подход с новой контрастивной целевой функцией дискриминатора для улучшения соответствия промпту. В сочетании с оптимизациями Stable Audio Open метод позволяет генерировать 12 секунд стерео аудио 44.1кГц за 75 мс на H100 и 7 секунд на мобильном устройстве.'}, 'en': {'title': 'Accelerating Text-to-Audio: The Fastest Model Yet!', 'desc': "This paper introduces Adversarial Relativistic-Contrastive (ARC) post-training, a novel method designed to speed up text-to-audio systems without relying on distillation. ARC post-training enhances diffusion and flow models by applying a relativistic adversarial approach combined with a contrastive discriminator objective, which improves the model's ability to follow prompts accurately. The authors demonstrate that their method significantly reduces inference time, achieving audio generation in approximately 75 milliseconds on high-performance hardware and 7 seconds on mobile devices. This advancement positions ARC post-training as the fastest text-to-audio model currently available, making it more practical for creative applications."}, 'zh': {'title': '加速文本到音频生成的革命性方法', 'desc': '本文介绍了一种新的后训练方法，称为对抗相对对比（ARC），旨在加速文本到音频系统的推理速度。ARC后训练算法不依赖于蒸馏，能够有效提升扩散/流模型的性能。通过结合相对对抗形式和对比判别器目标，ARC鼓励模型更好地遵循提示。经过优化后，该模型在H100上能在约75毫秒内生成12秒的44.1kHz立体声音频，成为目前已知的最快文本到音频模型。'}}}, {'id': 'https://huggingface.co/papers/2505.08727', 'title': 'Memorization-Compression Cycles Improve Generalization', 'url': 'https://huggingface.co/papers/2505.08727', 'abstract': 'We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.', 'score': 2, 'issue_id': 3760, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '07a8c687bc82dfeb', 'authors': ['Fangyuan Yu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08727.jpg', 'data': {'categories': ['#optimization', '#data', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Сжатие для обобщения: новый подход к обучению языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей, называемый Information Bottleneck Language Modeling (IBLM). Авторы доказывают, что улучшение обобщающей способности модели достигается не только за счет увеличения объема данных, но и путем сжатия внутренних представлений. В работе наблюдается циклическое чередование фаз запоминания и сжатия информации во время предобучения больших языковых моделей. На основе этих наблюдений предлагается алгоритм Gated Phase Transition (GAPT), который адаптивно переключается между фазами запоминания и сжатия, улучшая обобщающую способность модели и снижая эффект катастрофического забывания.'}, 'en': {'title': 'Compressing Knowledge for Better Generalization', 'desc': 'This paper demonstrates that improving generalization in machine learning models can be achieved not only by increasing the amount of data but also by compressing the internal representations of the model. The authors introduce the Information Bottleneck Language Modeling (IBLM) objective, which focuses on minimizing the complexity of representations while maintaining effective prediction capabilities. They observe a cycle of memorization and compression during the pretraining of large language models, which aligns with the principles of IBLM and reflects biological processes of learning and memory consolidation. To leverage this insight, they propose the Gated Phase Transition (GAPT) algorithm, which alternates between phases of memorization and compression, leading to significant improvements in model performance and generalization.'}, 'zh': {'title': '压缩与记忆的平衡提升模型泛化能力', 'desc': '本文理论上证明，模型的泛化能力不仅通过增加数据量来提高，还可以通过压缩内部表示来实现。我们提出了信息瓶颈语言建模（IBLM）目标，将语言建模重新构建为一个约束优化问题：在最优预测性能的条件下，最小化表示熵。通过实验证明，在大规模语言模型的预训练过程中，出现了记忆与压缩的循环现象，这种现象与IBLM所描述的预测-压缩权衡密切相关。基于这一观察，我们提出了门控相变（GAPT）训练算法，能够自适应地在记忆和压缩阶段之间切换，从而显著提高模型的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.08665', 'title': 'SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation', 'url': 'https://huggingface.co/papers/2505.08665', 'abstract': 'Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.', 'score': 2, 'issue_id': 3750, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '8cbf16dc2ec90273', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Free University of Bozen-Bolzano Bozen-Bolzano, IT'], 'pdf_title_img': 'assets/pdf/title_img/2505.08665.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training'], 'emoji': '🎥', 'ru': {'title': 'SkillFormer: эффективная оценка мастерства по мультиракурсному видео', 'desc': 'SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами.'}, 'en': {'title': 'SkillFormer: Efficient Multi-View Skill Assessment', 'desc': 'This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation.'}, 'zh': {'title': 'SkillFormer：高效的多视角技能评估架构', 'desc': '评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。'}}}, {'id': 'https://huggingface.co/papers/2505.08311', 'title': 'AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale', 'url': 'https://huggingface.co/papers/2505.08311', 'abstract': 'We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.', 'score': 2, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '492d10424bc0d97d', 'authors': ['Yunjie Ji', 'Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.08311.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#open_source', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Открытая 32B-модель устанавливает новую планку в рассуждениях', 'desc': 'AM-Thinking-v1 - это языковая модель с 32 миллиардами параметров, которая демонстрирует передовые способности к рассуждению. Модель превосходит DeepSeek-R1 и конкурирует с ведущими MoE-моделями, показывая впечатляющие результаты в математических и кодинговых тестах. AM-Thinking-v1 была создана на основе открытой модели Qwen2.5-32B с использованием тщательно разработанного процесса дообучения, включающего supervised fine-tuning и reinforcement learning. Авторы подчеркивают, что модель с 32 миллиардами параметров представляет собой практичный баланс между производительностью и удобством использования.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Innovation', 'desc': 'AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.'}, 'zh': {'title': '开源创新，推理新高度', 'desc': '我们介绍了AM-Thinking-v1，这是一个32B的密集语言模型，推动了推理的前沿，体现了开源创新的合作精神。它在AIME 2024、AIME 2025和LiveCodeBench等基准测试中表现出色，超越了DeepSeek-R1，并与领先的专家混合模型如Qwen3-235B-A22B和Seed1.5-Thinking相媲美。AM-Thinking-v1完全基于开源的Qwen2.5-32B基础模型，结合监督微调和强化学习，展现了卓越的推理能力。我们的工作证明了开源社区可以在32B规模上实现高性能，平衡顶级性能与实际可用性，激励更多合作努力。'}}}, {'id': 'https://huggingface.co/papers/2504.21475', 'title': 'Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines', 'url': 'https://huggingface.co/papers/2504.21475', 'abstract': 'This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.', 'score': 2, 'issue_id': 3756, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '86e4cc7ad4a84ece', 'authors': ['Serry Sibaee', 'Samar Ahmed', 'Abdullah Al Harbi', 'Omer Nacar', 'Adel Ammar', 'Yasser Habashi', 'Wadii Boulila'], 'affiliations': ['College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia', 'Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia', 'Independent Researcher, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21475.jpg', 'data': {'categories': ['#training', '#data', '#machine_translation', '#dataset', '#architecture', '#multilingual', '#low_resource'], 'emoji': '📚', 'ru': {'title': 'Революция в арабской лингвистике: трансформеры на службе обратного словаря', 'desc': 'Исследование посвящено разработке эффективной системы обратного словаря для арабского языка, позволяющей находить слова по их описаниям или значениям. Авторы представляют новый подход на основе трансформеров с полуэнкодерной нейросетевой архитектурой, достигающий наилучших результатов в задачах арабского обратного словаря. Методология включает создание комплексного датасета и установление формальных стандартов качества для арабских лексикографических определений. Эксперименты показали, что арабоязычные предобученные модели значительно превосходят многоязычные эмбеддинги, причем ARBERTv2 достигает наилучшего рейтингового показателя.'}, 'en': {'title': 'Bridging the Gap in Arabic Language Processing with a Reverse Dictionary', 'desc': 'This paper presents a new system for Arabic natural language processing called the Arabic Reverse Dictionary (RD), which helps users find words based on their meanings. The authors introduce a transformer-based model with a unique semi-encoder architecture that improves performance on Arabic RD tasks. They also create a detailed dataset and establish quality standards for Arabic definitions, showing that specialized Arabic models outperform general multilingual ones. Additionally, the study offers a theoretical framework for reverse dictionaries and introduces a Python library for easy implementation and training.'}, 'zh': {'title': '阿拉伯语反向词典：提升语言处理的利器', 'desc': '本研究解决了阿拉伯语自然语言处理中的关键问题，开发了一种有效的阿拉伯语反向词典系统，用户可以根据描述或含义找到单词。我们提出了一种新颖的基于变换器的半编码神经网络架构，具有几何递减层，达到了阿拉伯语反向词典任务的最先进结果。我们的研究方法包括全面的数据集构建过程，并建立了阿拉伯语词典定义的正式质量标准。实验表明，专门针对阿拉伯语的模型显著优于通用多语言嵌入，ARBERTv2模型获得了最佳排名分数。'}}}, {'id': 'https://huggingface.co/papers/2505.08751', 'title': 'Aya Vision: Advancing the Frontier of Multilingual Multimodality', 'url': 'https://huggingface.co/papers/2505.08751', 'abstract': 'Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.', 'score': 1, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'c063c125e504fa88', 'authors': ['Saurabh Dash', 'Yiyang Nan', 'John Dang', 'Arash Ahmadian', 'Shivalika Singh', 'Madeline Smith', 'Bharat Venkitesh', 'Vlad Shmyhlo', 'Viraat Aryabumi', 'Walter Beller-Morales', 'Jeremy Pekmez', 'Jason Ozuzu', 'Pierre Richemond', 'Acyr Locatelli', 'Nick Frosst', 'Phil Blunsom', 'Aidan Gomez', 'Ivan Zhang', 'Marzieh Fadaee', 'Manoj Govindassamy', 'Sudip Roy', 'Matthias Gallé', 'Beyza Ermis', 'Ahmet Üstün', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.08751.jpg', 'data': {'categories': ['#synthetic', '#data', '#low_resource', '#multimodal', '#training', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Прорыв в создании эффективных многоязычных мультимодальных ИИ-моделей', 'desc': 'Статья представляет новые методы для создания многоязычных мультимодальных языковых моделей. Авторы разработали систему синтетической аннотации для создания качественных многоязычных мультимодальных данных для обучения. Они также предложили технику объединения кросс-модальных моделей для предотвращения катастрофического забывания. Модели Aya-Vision, созданные с использованием этих методов, показывают лучшие результаты по сравнению с более крупными конкурентами в многоязычных мультимодальных задачах.'}, 'en': {'title': 'Advancing Multimodal Language Models for Multilingual Mastery', 'desc': 'This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.'}, 'zh': {'title': '多模态语言模型的突破性进展', 'desc': '构建多模态语言模型面临许多挑战，包括视觉和语言模态的对齐、高质量指令数据的整理，以及在引入视觉后避免文本能力的退化。在多语言环境中，这些困难更加突出，因为不同语言的多模态数据需求加剧了数据稀缺，机器翻译常常扭曲意义，并且灾难性遗忘现象更加明显。为了解决这些问题，我们提出了新的技术，包括一个合成注释框架，用于整理高质量、多样化的多语言多模态指令数据，使Aya Vision模型能够在多种语言中对多模态输入生成自然、符合人类偏好的响应。此外，我们还提出了一种跨模态模型合并技术，有效减轻灾难性遗忘，同时增强多模态生成性能。'}}}, {'id': 'https://huggingface.co/papers/2505.08712', 'title': 'NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance', 'url': 'https://huggingface.co/papers/2505.08712', 'abstract': "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.", 'score': 1, 'issue_id': 3752, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'a68cc40de86ece61', 'authors': ['Wenzhe Cai', 'Jiaqi Peng', 'Yuqiang Yang', 'Yujian Zhang', 'Meng Wei', 'Hanqing Wang', 'Yilun Chen', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.08712.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#diffusion', '#robotics', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'NavDP: Универсальная навигация роботов из симуляции в реальность', 'desc': 'Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключительно на симуляциях. NavDP использует комбинацию генерации траектории на основе диффузии и функции критика для выбора траектории, что позволяет ей успешно работать с разными типами роботов в реальном мире. Система обучается на большом наборе данных, сгенерированном в симуляции, что значительно эффективнее сбора реальных данных. Эксперименты показывают высокую производительность и способность к обобщению для различных типов роботов в разнообразных средах.'}, 'en': {'title': 'Efficient Robot Navigation with Simulation-Driven Learning', 'desc': 'This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.'}, 'zh': {'title': '机器人导航的新突破：导航扩散策略', 'desc': '本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.08445', 'title': 'Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency', 'url': 'https://huggingface.co/papers/2505.08445', 'abstract': 'Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.', 'score': 1, 'issue_id': 3756, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '7b4d8b3daa6efb6d', 'authors': ['Adel Ammar', 'Anis Koubaa', 'Omer Nacar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, P.O. Box 50927, Riyadh 11533, Saudi Arabia', 'Prince Sultan University, Rafha Street, P.O. Box 66833, Riyadh 11586, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.08445.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#healthcare', '#hallucinations', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация RAG: баланс между скоростью и точностью', 'desc': 'Статья анализирует влияние гиперпараметров на скорость и качество в системах генерации с дополнением извлечением (RAG). Исследуются векторные хранилища Chroma и Faiss, стратегии разбиения текста, переранжирование кросс-энкодером и температура. Оценивается шесть метрик, включая достоверность, корректность и релевантность ответов. Результаты показывают компромисс между скоростью и точностью, а также демонстрируют, что RAG-системы могут достичь очень высокой точности извлечения при правильном подборе гиперпараметров.'}, 'en': {'title': 'Optimizing RAG: Balancing Speed and Accuracy for Better AI Responses', 'desc': 'This paper explores how hyperparameters affect the performance of Retrieval-Augmented Generation (RAG) systems, which combine language generation with external information retrieval. It evaluates different vector stores, chunking strategies, and re-ranking methods to find the best balance between speed and accuracy. The findings indicate that while Chroma is faster, Faiss offers better retrieval precision, highlighting a trade-off between these two factors. The study also shows that with optimal hyperparameter tuning, RAG systems can achieve very high retrieval accuracy, which is crucial for applications like healthcare decision support.'}, 'zh': {'title': '优化RAG系统，实现高效准确的检索', 'desc': '大型语言模型在任务表现上表现优异，但常常会出现幻觉或依赖过时知识。检索增强生成（RAG）通过将生成与外部搜索结合，解决了这些问题。我们分析了超参数如何影响RAG系统的速度和质量，包括向量存储、分块策略、交叉编码重排序和温度等，并评估了六个指标。研究结果帮助从业者在调整RAG系统时平衡计算成本和准确性，以实现透明且最新的响应。'}}}, {'id': 'https://huggingface.co/papers/2505.07416', 'title': 'ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation', 'url': 'https://huggingface.co/papers/2505.07416', 'abstract': 'Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP', 'score': 1, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '4f126d39b5476454', 'authors': ['Truc Mai-Thanh Nguyen', 'Dat Minh Nguyen', 'Son T. Luu', 'Kiet Van Nguyen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07416.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#multilingual', '#low_resource', '#open_source'], 'emoji': '🇻🇳', 'ru': {'title': 'ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке', 'desc': 'Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзывов на вьетнамском языке. Авторы использовали искусственный интеллект для помощи аннотаторам, что значительно ускорило процесс разметки и снизило затраты. Датасет охватывает 4 домена, включая 2000 продуктов и 46000 отзывов. Исследователи провели эксперименты с базовыми моделями машинного обучения для оценки качества аннотаций, созданных человеком и ИИ.'}, 'en': {'title': 'Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction', 'desc': 'This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks.'}, 'zh': {'title': '提升越南语评论有用性的智能解决方案', 'desc': '多模态评论有用性预测（MRHP）是推荐系统中的一个重要任务，尤其是在电子商务平台上。本文介绍了ViMRHP（越南语多模态评论有用性预测），这是一个针对越南语的MRHP任务的大规模基准数据集，涵盖了四个领域，包括2000个产品和46000条评论。为了优化注释过程，我们利用人工智能辅助注释者构建ViMRHP数据集，从而显著减少了注释时间和成本，同时保持数据质量。尽管AI生成的注释在复杂任务中仍存在局限性，但我们通过详细的性能分析进一步探讨了这些问题。'}}}, {'id': 'https://huggingface.co/papers/2504.21853', 'title': 'A Survey of Interactive Generative Video', 'url': 'https://huggingface.co/papers/2504.21853', 'abstract': 'Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.', 'score': 40, 'issue_id': 3550, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '4e975f915f638955', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Hao Chen', 'Xihui Liu'], 'affiliations': ['Kuaishou Technology, Shenzhen, China', 'The Hong Kong University of Science and Technology (HKUST), Hong Kong', 'The University of Hong Kong, Pok Fu Lam, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.21853.jpg', 'data': {'categories': ['#robotics', '#interpretability', '#video', '#optimization', '#survey', '#agents', '#games', '#multimodal'], 'emoji': '🎮', 'ru': {'title': 'IGV: Будущее интерактивного видеоконтента', 'desc': 'Статья представляет обзор технологии Интерактивного Генеративного Видео (IGV), которая сочетает генеративные возможности с интерактивными функциями. Авторы рассматривают применение IGV в трех основных областях: игры, воплощенный ИИ и автономное вождение. Предложена комплексная структура, разбивающая идеальную систему IGV на пять модулей: Генерация, Контроль, Память, Динамика и Интеллект. В работе анализируются технические проблемы и будущие направления развития каждого компонента IGV.'}, 'en': {'title': 'Empowering Interactive Experiences with Generative Video', 'desc': 'Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.'}, 'zh': {'title': '推动互动生成视频技术的未来发展', 'desc': '互动生成视频（IGV）是一种新兴技术，旨在满足对高质量互动视频内容的需求。本文将IGV定义为一种结合生成能力和互动特性的技术，能够通过控制信号和反馈实现用户参与。我们调查了IGV在游戏、具身人工智能和自动驾驶等三个主要领域的应用，并提出了一个全面的框架，将理想的IGV系统分解为生成、控制、记忆、动态和智能五个模块。通过系统分析技术挑战和未来方向，本文旨在推动IGV领域的研究与发展。'}}}, {'id': 'https://huggingface.co/papers/2505.00662', 'title': 'DeepCritic: Deliberate Critique with Large Language Models', 'url': 'https://huggingface.co/papers/2505.00662', 'abstract': 'As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.', 'score': 37, 'issue_id': 3549, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '259dddc97137d27a', 'authors': ['Wenkai Yang', 'Jingwen Chen', 'Yankai Lin', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00662.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#reasoning', '#rlhf', '#math', '#training'], 'emoji': '🧮', 'ru': {'title': 'Усовершенствование LLM для глубокой критики математических решений', 'desc': 'Эта статья представляет новый двухэтапный подход к улучшению способностей больших языковых моделей (LLM) критиковать математические решения. На первом этапе используется Qwen2.5-72B-Instruct для генерации подробных пошаговых критик, которые служат обучающими данными. Затем применяется обучение с подкреплением для дальнейшего улучшения критических способностей модели. Разработанная модель-критик на базе Qwen2.5-7B-Instruct превосходит существующие LLM-критики в выявлении ошибок и предоставлении детальной обратной связи.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions', 'desc': "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."}, 'zh': {'title': '提升大型语言模型的数学批评能力', 'desc': '随着大型语言模型（LLMs）的快速发展，提供准确的反馈和可扩展的监督变得尤为重要。本文提出了一种新颖的两阶段框架，旨在增强LLMs在数学批评方面的能力。通过生成详细的逐步批评，模型能够更准确地识别错误并提供深入的反馈，帮助生成模型纠正错误。我们的实验表明，改进后的批评模型在错误识别基准测试中显著优于现有的LLM批评者。'}}}, {'id': 'https://huggingface.co/papers/2505.00703', 'title': 'T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT', 'url': 'https://huggingface.co/papers/2505.00703', 'abstract': 'Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1', 'score': 28, 'issue_id': 3548, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'ca564761ff71d15e', 'authors': ['Dongzhi Jiang', 'Ziyu Guo', 'Renrui Zhang', 'Zhuofan Zong', 'Hao Li', 'Le Zhuo', 'Shilin Yan', 'Pheng-Ann Heng', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'CUHK MiuLar Lab', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.00703.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🎨', 'ru': {'title': 'Рассуждения на двух уровнях улучшают генерацию изображений по тексту', 'desc': 'Данная статья представляет T2I-R1 - новую модель генерации изображений по тексту, улучшенную с помощью рассуждений и обучения с подкреплением. Модель использует двухуровневый процесс рассуждений: семантический уровень для планирования промпта и токенный уровень для попиксельной генерации. Авторы вводят метод BiCoT-GRPO для оптимизации обоих уровней рассуждений. Применение этих стратегий к базовой модели Janus-Pro позволило значительно улучшить результаты на бенчмарках T2I-CompBench и WISE.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Enhanced Reasoning', 'desc': 'This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests.'}, 'zh': {'title': '双层思维链提升文本到图像生成', 'desc': '本文介绍了一种新颖的文本到图像生成模型T2I-R1，该模型结合了强化学习和双层思维链推理过程。我们提出了两种思维链：语义层次的思维链用于高层次的提示规划，令生成过程更具逻辑性；而令牌层次的思维链则用于在逐块生成过程中进行低层次的像素处理。通过引入BiCoT-GRPO，我们能够在同一训练步骤中优化这两种思维链，从而提升生成效果。实验结果表明，T2I-R1在多个基准测试中表现优异，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2505.00497', 'title': 'KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution', 'url': 'https://huggingface.co/papers/2505.00497', 'abstract': 'Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.', 'score': 9, 'issue_id': 3554, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'dc645b5c7e476cea', 'authors': ['Antoni Bigata', 'Rodrigo Mira', 'Stella Bounareli', 'Michał Stypułkowski', 'Konstantinos Vougioukas', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London', 'University of Wrocław'], 'pdf_title_img': 'assets/pdf/title_img/2505.00497.jpg', 'data': {'categories': ['#cv', '#leakage', '#video', '#open_source', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'KeySync: Революция в синхронизации губ для видео', 'desc': 'KeySync - это новая двухэтапная система для синхронизации губ в видео с новым аудио. Она решает проблемы временной согласованности, утечки выражений и окклюзий лица с помощью специальной стратегии маскирования. KeySync достигает лучших результатов в реконструкции губ и кросс-синхронизации, улучшая визуальное качество и уменьшая утечку выражений согласно новой метрике LipLeak. Система эффективно справляется с окклюзиями и превосходит существующие методы в реальных приложениях, таких как автоматический дубляж.'}, 'en': {'title': 'KeySync: Mastering Lip Synchronization with Precision', 'desc': 'This paper introduces KeySync, a two-stage framework designed to improve lip synchronization in videos by aligning lip movements with new audio inputs. It addresses common challenges in talking head generation, such as maintaining temporal consistency and managing expression leakage and facial occlusions. KeySync employs a unique masking strategy to effectively tackle these issues, resulting in enhanced visual quality and reduced leakage as measured by a new metric called LipLeak. The framework demonstrates state-of-the-art performance in lip reconstruction and cross-synchronization, validated through various ablation studies.'}, 'zh': {'title': 'KeySync：提升唇部同步的创新框架', 'desc': '本论文介绍了一种名为KeySync的双阶段框架，用于解决唇部同步中的时间一致性问题。该方法还通过精心设计的遮罩策略，解决了输入视频中的表情泄漏和面部遮挡等新挑战。实验结果表明，KeySync在唇部重建和交叉同步方面达到了最先进的效果，显著提高了视觉质量并减少了表情泄漏。我们还通过多项消融研究验证了新遮罩方法在处理遮挡方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.00234', 'title': 'Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks', 'url': 'https://huggingface.co/papers/2505.00234', 'abstract': 'Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.', 'score': 9, 'issue_id': 3563, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '9e975cfc8ecf9dea', 'authors': ['Vishnu Sarukkai', 'Zhiqiang Xie', 'Kayvon Fatahalian'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00234.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#rl', '#agents', '#optimization', '#benchmark', '#games'], 'emoji': '🧠', 'ru': {'title': 'Самообучение LLM-агентов на собственном опыте', 'desc': 'Статья представляет метод улучшения агентов на основе больших языковых моделей (LLM) для задач последовательного принятия решений. Вместо специфической настройки под конкретные задачи, авторы предлагают автоматическое обучение на основе успешного опыта агента на схожих задачах. Метод включает создание и уточнение базы данных самостоятельно сгенерированных примеров. Результаты показывают значительное улучшение производительности на нескольких бенчмарках, сравнимое с более сложными подходами, использующими специфичные для задач компоненты.'}, 'en': {'title': 'Learning from Success: Automating Improvement in LLM Agents', 'desc': 'This paper explores a new approach for improving Large Language Model (LLM) agents in sequential decision-making tasks without relying on extensive task-specific knowledge engineering. Instead of manually tuning prompts or creating curated examples, the authors propose that LLM agents can learn from their own successful experiences by building a database of self-generated examples. The study shows that simply accumulating successful task trajectories can significantly enhance performance on various benchmarks. Additionally, the paper introduces methods for selecting high-performing examples from this database, leading to even better results that rival more complex, knowledge-intensive methods.'}, 'zh': {'title': '自动学习提升LLM代理性能的创新方法', 'desc': '本文探讨了如何通过自我生成的成功经验来自动提升大型语言模型（LLM）代理在顺序决策任务中的表现，而不是依赖于特定任务的知识工程。研究表明，简单地积累成功的轨迹可以显著提高在多个基准测试中的表现。我们还提出了两种扩展方法：通过基于种群的训练选择高效的示例集合，以及根据实证效用保留个别轨迹。这些方法进一步提升了性能，展示了自动轨迹数据库构建作为知识工程的有效替代方案。'}}}, {'id': 'https://huggingface.co/papers/2504.18715', 'title': 'Spatial Speech Translation: Translating Across Space With Binaural\n  Hearables', 'url': 'https://huggingface.co/papers/2504.18715', 'abstract': "Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.", 'score': 7, 'issue_id': 3566, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '979ba773cd3e90a0', 'authors': ['Tuochao Chen', 'Qirui Wang', 'Runlin He', 'Shyam Gollakota'], 'affiliations': ['Paul G. Allen School, University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.18715.jpg', 'data': {'categories': ['#audio', '#multilingual', '#machine_translation'], 'emoji': '🎧', 'ru': {'title': 'Пространственный перевод речи: слышать мир на своем языке', 'desc': 'Статья представляет концепцию пространственного перевода речи для слуховых устройств. Система позволяет переводить речь окружающих людей, сохраняя направление и характеристики голоса каждого говорящего в бинауральном выводе. Авторы решают технические задачи слепого разделения источников, локализации, перевода в реальном времени и бинаурального рендеринга. Прототип системы демонстрирует эффективность в условиях сильных помех и реверберации, достигая оценки BLEU до 22.01 при переводе между языками.'}, 'en': {'title': 'Transforming Speech Translation with Spatial Awareness', 'desc': "This paper presents a new approach to spatial speech translation using hearables, which are devices that can translate spoken language in real-time while preserving the spatial characteristics of the speakers. The authors address key challenges such as separating overlapping voices, accurately locating speakers, and ensuring that translations are expressive and timely, all while running efficiently on Apple M2 hardware. Their prototype demonstrates significant improvements over existing models, achieving a BLEU score of 22.01 in noisy environments, indicating high translation quality. User studies validate the system's ability to maintain the spatial cues of translated speech, marking a significant advancement in integrating spatial awareness into speech translation technology."}, 'zh': {'title': '空间语音翻译：让交流无障碍', 'desc': '本文介绍了一种新颖的空间语音翻译概念，旨在通过耳机将环境中说话者的语言实时翻译为佩戴者的母语，同时保留每位说话者的方向和独特声音特征。为实现这一目标，研究解决了多个技术挑战，包括盲源分离、定位、实时表达翻译和双耳渲染，以确保翻译音频中的说话者方向得以保留。通过原型双耳耳机的概念验证评估，研究表明该系统在强干扰环境中仍能实现高达22.01的BLEU分数，优于现有模型。用户研究进一步确认了该系统在真实环境中空间渲染翻译语音的有效性，标志着将空间感知整合到语音翻译中的第一步。'}}}, {'id': 'https://huggingface.co/papers/2504.21659', 'title': 'AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization', 'url': 'https://huggingface.co/papers/2504.21659', 'abstract': 'Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1', 'score': 6, 'issue_id': 3549, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '6487e5a67faf07a5', 'authors': ['Haotian Luo', 'Haiying He', 'Yibo Wang', 'Jinluan Yang', 'Rui Liu', 'Naiqiang Tan', 'Xiaochun Cao', 'Dacheng Tao', 'Li Shen'], 'affiliations': ['China Agricultural University', 'Didichuxing Co. Ltd', 'Nanyang Technological University', 'Sun Yat-sen University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21659.jpg', 'data': {'categories': ['#reasoning', '#inference', '#math', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное рассуждение: эффективность без потери качества', 'desc': 'Статья представляет новый подход к адаптивному и эффективному рассуждению в языковых моделях. Авторы предлагают двухэтапную структуру, включающую гибридную модель рассуждений и би-уровневое обучение предпочтениям. Эксперименты показывают, что метод значительно сокращает вычислительные затраты при сохранении производительности. На пяти математических наборах данных средняя длина рассуждений сократилась более чем на 50%.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Inference in Complex Tasks', 'desc': 'This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets.'}, 'zh': {'title': '自适应推理，提升效率！', 'desc': '最近，长推理模型在复杂推理任务中表现出色，但常常导致推理开销大，因此效率成为一个关键问题。我们的实证分析显示，使用长链推理（Long-CoT）的好处在不同问题上差异很大：有些问题需要复杂推理，而其他问题则没有改善，甚至准确率下降。这促使我们提出自适应推理策略，根据输入调整推理深度。我们提出了一种新颖的两阶段框架，通过混合长短链推理模型和双层偏好训练，显著降低推理成本，同时保持性能。'}}}, {'id': 'https://huggingface.co/papers/2504.19394', 'title': 'LLMs for Engineering: Teaching Models to Design High Powered Rockets', 'url': 'https://huggingface.co/papers/2504.19394', 'abstract': "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.", 'score': 6, 'issue_id': 3558, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': '5b65f8f309063f13', 'authors': ['Toby Simonds'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.19394.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#agi', '#benchmark', '#games', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'ИИ с обучением с подкреплением превосходит людей в проектировании ракет', 'desc': 'Это исследование оценивает возможности больших языковых моделей (LLM) в проектировании мощных ракет с помощью бенчмарка RocketBench. Модели тестируются на задачах оптимизации целевой высоты и точной посадки. Результаты показывают, что современные LLM демонстрируют хорошие базовые инженерные знания, но затрудняются с итерациями на основе результатов симуляций. Однако при усилении обучением с подкреплением (RL) модель с 7 миллиардами параметров превосходит как современные базовые модели, так и экспертов-людей.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Rocket Design Challenges', 'desc': 'This paper investigates the use of Large Language Models (LLMs) in the field of physical engineering, specifically in high-powered rocketry design. It introduces RocketBench, a benchmark that connects LLMs to advanced rocket simulations, and evaluates their performance on design tasks like optimizing target altitude and achieving precision landings. The results indicate that while LLMs possess a solid foundation in engineering knowledge, they struggle to improve their designs based on simulation feedback, often falling short of human capabilities. However, by incorporating reinforcement learning, a 7B parameter model surpasses both state-of-the-art models and human experts, highlighting the potential of RL-enhanced LLMs in complex engineering optimization tasks.'}, 'zh': {'title': '强化学习助力火箭设计，超越人类专家！', 'desc': '大型语言模型（LLMs）在软件工程中取得了显著进展，但在物理工程领域的应用仍然不够深入。本文通过RocketBench基准评估LLMs在高功率火箭设计中的能力，连接LLMs与高保真火箭模拟。我们测试了两个复杂的设计任务：目标高度优化和精确着陆挑战。研究发现，尽管最先进的LLMs展现出强大的工程知识，但在根据模拟结果迭代设计时表现不佳，最终未能超越人类专家的水平；然而，通过强化学习（RL）增强后，7B参数模型的表现超过了现有基础模型和人类专家。'}}}, {'id': 'https://huggingface.co/papers/2504.18983', 'title': 'MediAug: Exploring Visual Augmentation in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.18983', 'abstract': 'Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.', 'score': 5, 'issue_id': 3554, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 апреля', 'en': 'April 26', 'zh': '4月26日'}, 'hash': '13143b6fb9ae9216', 'authors': ['Xuyin Qi', 'Zeyu Zhang', 'Canxuan Gang', 'Hao Zhang', 'Lei Zhang', 'Zhiwei Zhang', 'Yang Zhao'], 'affiliations': ['AIML', 'ANU', 'La Trobe', 'PSU', 'UCAS', 'UNSW'], 'pdf_title_img': 'assets/pdf/title_img/2504.18983.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#healthcare', '#synthetic', '#training'], 'emoji': '🩺', 'ru': {'title': 'MediAug: Новый эталон аугментации данных для медицинской визуализации', 'desc': 'Эта статья представляет MediAug - комплексный фреймворк для оценки методов аугментации данных в медицинской визуализации. Авторы исследуют шесть методов аугментации на основе смешивания изображений, применяя их к задачам классификации опухолей мозга и глазных заболеваний. Эксперименты проводятся с использованием как сверточных нейронных сетей (ResNet-50), так и трансформеров (ViT-B). Результаты показывают, что разные методы аугментации дают наилучшие результаты для разных архитектур и задач, подчеркивая важность выбора подходящего метода для конкретной задачи.'}, 'en': {'title': 'Enhancing Medical Imaging with Advanced Data Augmentation', 'desc': 'This paper addresses the challenges of data augmentation in medical imaging, particularly the domain gap between natural images and medical scans. It introduces MediAug, a benchmark framework that evaluates six mix-based augmentation methods across different neural network architectures. The study finds that MixUp and SnapMix significantly enhance classification accuracy for brain tumors and eye diseases, respectively. By providing a systematic evaluation, the paper clarifies the effectiveness of various augmentation strategies in improving medical image analysis.'}, 'zh': {'title': '医学影像数据增强的新突破', 'desc': '数据增强在医学影像中对于提高分类准确性、病变检测和器官分割至关重要，尤其是在数据有限的情况下。然而，医学影像与自然照片之间的显著领域差距可能会扭曲关键的疾病特征。此外，现有的增强研究往往局限于单一任务或架构，导致先进的混合策略的优势不明确。为了解决这些问题，我们提出了一个统一的评估框架，整合了六种基于混合的数据增强方法，并在脑肿瘤MRI和眼病视网膜图像数据集上进行了评估。'}}}, {'id': 'https://huggingface.co/papers/2504.20605', 'title': 'TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models', 'url': 'https://huggingface.co/papers/2504.20605', 'abstract': 'Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.', 'score': 4, 'issue_id': 3554, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '9637fef0c8d474ed', 'authors': ['Mihai Nadas', 'Laura Diosan', 'Andrei Piscoran', 'Andreea Tomescu'], 'affiliations': ['Babes-Bolyai University', 'KlusAI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.20605.jpg', 'data': {'categories': ['#story_generation', '#ethics', '#multimodal', '#dataset', '#benchmark', '#alignment', '#open_source'], 'emoji': '📚', 'ru': {'title': 'Масштабное моральное повествование без гигантских моделей', 'desc': 'Исследователи создали TF1-EN-3M - первый открытый датасет из трех миллионов англоязычных басен, сгенерированных моделями до 8 миллиардов параметров. Каждая история следует шестиэлементной структуре, охватывая широкий тематический спектр. Оценка качества историй производится с помощью гибридного подхода, сочетающего критику на основе GPT и метрики разнообразия и читабельности. Датасет, код генерации и скрипты оценки выпущены под свободной лицензией, открывая возможности для исследований в области следования инструкциям, нарративного интеллекта и образовательного ИИ.'}, 'en': {'title': 'Empowering Moral Storytelling with TF1-EN-3M', 'desc': 'This paper introduces TF1-EN-3M, a novel dataset containing three million English fables designed to teach moral lessons, generated by smaller instruction-tuned models. The stories are structured using a six-slot framework that ensures each narrative includes essential elements like characters, traits, and morals. A unique evaluation system combines assessments from a GPT-based critic and various metrics to ensure quality and diversity in the generated fables. The dataset and associated tools are made publicly available, promoting further research in areas like narrative intelligence and ethical AI development.'}, 'zh': {'title': '道德故事生成的新纪元', 'desc': '这篇论文介绍了TF1-EN-3M，这是第一个包含三百万个英语寓言的开放数据集，专门由指令调优模型生成。每个故事遵循六个部分的结构，包括角色、特征、背景、冲突、解决方案和道德，确保了故事的连贯性和主题的多样性。论文还提出了一种混合评估方法，结合了基于GPT的评分和无参考的多样性与可读性指标。TF1-EN-3M为道德故事的生成和教育AI的研究提供了新的可能性，表明大规模的道德叙事不再需要专有的大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.20406', 'title': 'Skill Discovery for Software Scripting Automation via Offline\n  Simulations with LLMs', 'url': 'https://huggingface.co/papers/2504.20406', 'abstract': "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.", 'score': 3, 'issue_id': 3570, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '995b07a7273b51fd', 'authors': ['Paiheng Xu', 'Gang Wu', 'Xiang Chen', 'Tong Yu', 'Chang Xiao', 'Franck Dernoncourt', 'Tianyi Zhou', 'Wei Ai', 'Viswanathan Swaminathan'], 'affiliations': ['Adobe Research', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.20406.jpg', 'data': {'categories': ['#agents', '#dataset', '#alignment', '#graphs', '#games', '#architecture', '#data'], 'emoji': '🤖', 'ru': {'title': 'Автоматизация без кодирования: LLM создают скрипты для пользователей', 'desc': 'Эта статья представляет новый подход к созданию скриптов для автоматизации задач в программном обеспечении без необходимости программирования. Авторы предлагают офлайн-систему для создания набора проверенных скриптов с использованием больших языковых моделей (LLM) и общедоступных руководств по скриптингу. Система включает компоненты для создания задач и генерации навыков, а также использует модель на основе графовых нейронных сетей (GNN) для прогнозирования связей между API. Эксперименты с Adobe Illustrator показали значительное улучшение успешности автоматизации, сокращение времени отклика и экономию затрат на токены по сравнению с традиционной генерацией кода во время выполнения.'}, 'en': {'title': 'Empowering Users with Automated Scripting through AI', 'desc': 'This paper presents a framework that helps users automate tasks in software without needing deep programming skills. It uses Large Language Models (LLMs) to generate verified scripts by simulating tasks and refining them based on execution feedback. The framework includes a Graph Neural Network (GNN) to explore API synergies, which helps create a diverse set of useful scripts. Experiments show that this approach improves automation success rates and reduces costs compared to traditional methods.'}, 'zh': {'title': '利用LLM提升软件脚本自动化的创新框架', 'desc': '本文提出了一种离线模拟框架，旨在通过利用大型语言模型（LLMs）和公开的脚本指南，创建一个软件特定的技能集。该框架包括两个主要部分：任务创建和技能生成，通过执行反馈来优化和验证脚本。我们还引入了一种基于图神经网络（GNN）的链接预测模型，以捕捉API之间的协同作用，从而生成多样化的技能。实验结果表明，该框架在自动化成功率、响应时间和运行时成本方面显著优于传统的代码生成方法。'}}}, {'id': 'https://huggingface.co/papers/2505.00534', 'title': 'A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic', 'url': 'https://huggingface.co/papers/2505.00534', 'abstract': 'Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.', 'score': 2, 'issue_id': 3560, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': 'd411bc0f9d34adf4', 'authors': ['Muhammad Imran Zaman', 'Usama Ijaz Bajwa', 'Gulshan Saleem', 'Rana Hammad Raza'], 'affiliations': ['Department of Computer Science, COMSATS University Islamabad, Lahore Campus, Lahore, Pakistan', 'Pakistan Navy Engineering College, National University of Sciences and Technology (NUST), Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2505.00534.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#optimization', '#video', '#transfer_learning'], 'emoji': '🚗', 'ru': {'title': 'Умное отслеживание транспорта с помощью глубокого обучения', 'desc': 'Статья представляет фреймворк для отслеживания транспортных средств с использованием нескольких камер в городских условиях. Авторы применяют глубокое обучение, включая Mask R-CNN для обнаружения объектов и ResNet-152 для извлечения признаков. Система решает проблемы окклюзии, изменения освещения и теней с помощью специальных функций потерь и метрик расстояния. Фреймворк показал высокую эффективность на наборе данных AI City Challenge, достигнув IDF1 0.8289.'}, 'en': {'title': 'Revolutionizing Traffic Monitoring with Deep Learning Tracking', 'desc': 'This paper presents a deep learning framework for Multi-Object Multi-Camera Tracking (MO-MCT) aimed at improving traffic monitoring in Intelligent Transportation Systems. The framework utilizes Mask R-CNN for detecting vehicles and applies Non-Maximum Suppression (NMS) to manage overlapping detections. It incorporates transfer learning for vehicle re-identification, allowing for the tracking of vehicles across different cameras despite challenges like occlusions and varying lighting conditions. The system is evaluated on a large dataset and shows strong performance metrics, indicating its potential for real-world traffic applications.'}, 'zh': {'title': '智能交通中的高效多摄像头跟踪解决方案', 'desc': '随着网络摄像头数量的增加，视觉传感器在智能交通系统中的重要性日益增强。本文提出了一种基于深度学习的多目标多摄像头跟踪框架，旨在解决城市交通场景中手动目标跟踪和匹配的挑战。该框架利用Mask R-CNN进行目标检测，并通过非极大值抑制（NMS）选择重叠检测中的目标。通过迁移学习实现车辆的重新识别，结合适当的损失函数和距离度量，最终在AI City Challenge数据集上取得了竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02707', 'title': 'Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play', 'url': 'https://huggingface.co/papers/2505.02707', 'abstract': "A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.", 'score': 56, 'issue_id': 3604, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '1cf06df6011df348', 'authors': ['Yemin Shi', 'Yu Shu', 'Siwei Dong', 'Guangyi Liu', 'Jaward Sesay', 'Jingwen Li', 'Zhiting Hu'], 'affiliations': ['MBZUAI', 'Maitrix.org', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.02707.jpg', 'data': {'categories': ['#open_source', '#agi', '#multimodal', '#audio', '#architecture', '#agents', '#reasoning', '#multilingual', '#machine_translation'], 'emoji': '🗣️', 'ru': {'title': 'Voila: ИИ-собеседник с человеческим голосом', 'desc': 'Статья представляет Voila - семейство голосовых языковых моделей, способных вести естественный диалог с человеком в реальном времени. Модель использует сквозную архитектуру, объединяющую возможности больших языковых моделей с акустическим моделированием. Voila достигает задержки ответа всего 195 миллисекунд, что быстрее среднего времени реакции человека. Модель поддерживает более миллиона предварительно созданных голосов и может эффективно настраиваться на новые голоса по коротким аудиосэмплам.'}, 'en': {'title': 'Voila: Revolutionizing Voice AI for Natural Conversations', 'desc': 'This paper presents Voila, an advanced voice AI agent designed for seamless interaction in daily life. Voila utilizes a novel end-to-end architecture that allows for real-time, emotionally expressive conversations, achieving a response time of just 195 milliseconds. It combines large language models with sophisticated acoustic modeling to generate natural and persona-aware voice outputs, enabling users to customize voice characteristics easily. Additionally, Voila supports various voice-based applications, including automatic speech recognition and multilingual translation, and is fully open-sourced to promote further research in human-machine communication.'}, 'zh': {'title': 'Voila：实现自然情感互动的语音AI代理', 'desc': '这篇论文介绍了Voila，一个新型的语音AI代理，旨在实现与人类的自然互动。Voila采用端到端架构，支持全双工、低延迟的对话，能够捕捉语音中的情感和细微差别。它结合了大型语言模型的推理能力和强大的声学建模，允许用户通过简单的文本指令定义说话者的身份和语调。Voila还支持超过一百万种预构建的声音，并能从短至10秒的音频样本中高效定制新声音，推动人机交互的进步。'}}}, {'id': 'https://huggingface.co/papers/2505.02387', 'title': 'RM-R1: Reward Modeling as Reasoning', 'url': 'https://huggingface.co/papers/2505.02387', 'abstract': "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.", 'score': 43, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '6cac1dc82fc6bcdc', 'authors': ['Xiusi Chen', 'Gaotang Li', 'Ziqi Wang', 'Bowen Jin', 'Cheng Qian', 'Yu Wang', 'Hongru Wang', 'Yu Zhang', 'Denghui Zhang', 'Tong Zhang', 'Hanghang Tong', 'Heng Ji'], 'affiliations': ['Stevens Institute of Technology', 'Texas A&M University', 'University of California, San Diego', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02387.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#training', '#benchmark', '#rlhf', '#alignment', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Рассуждающие модели вознаграждения: новый подход к интерпретируемому обучению с подкреплением', 'desc': 'Эта статья представляет новый класс генеративных моделей вознаграждения - Reasoning Reward Models (ReasRMs), которые формулируют моделирование вознаграждения как задачу рассуждения. Авторы предлагают двухэтапный процесс обучения: дистилляция высококачественных цепочек рассуждений и обучение с подкреплением с проверяемыми вознаграждениями. Модели ReasRM улучшают результаты больших языковых моделей, генерируя цепочки рассуждений или специфические для чата рубрики. Эмпирически модели достигают высоких результатов в нескольких комплексных бенчмарках для моделей вознаграждения, превосходя более крупные модели на величину до 13.8%.'}, 'en': {'title': 'Enhancing Reward Models with Reasoning for Better Interpretability', 'desc': 'This paper discusses the importance of reward modeling in aligning large language models (LLMs) with human preferences using reinforcement learning from human feedback (RLHF). It highlights the limitations of existing reward models, which often lack interpretability and struggle to incorporate natural language critiques. The authors introduce Reasoning Reward Models (ReasRMs), which enhance interpretability by treating reward modeling as a reasoning task. Their approach includes a two-stage training process that improves performance and achieves state-of-the-art results on various benchmarks, outperforming larger models.'}, 'zh': {'title': '推理奖励模型：提升可解释性的关键', 'desc': '奖励建模在将大型语言模型（LLMs）与人类偏好对齐中至关重要，尤其是通过人类反馈的强化学习（RLHF）。现有的奖励模型（RM）往往生成不透明的标量分数或直接预测偏好的答案，缺乏可解释性。我们提出了一种新的生成奖励模型——推理奖励模型（ReasRMs），将奖励建模视为推理任务，从而显著提高了模型的可解释性和性能。通过高质量推理链的蒸馏和可验证奖励的强化学习，我们的模型在多个奖励模型基准测试中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.20752', 'title': 'Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers', 'url': 'https://huggingface.co/papers/2504.20752', 'abstract': 'Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio phi_r of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing phi_r drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.', 'score': 41, 'issue_id': 3604, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '46858ed83065e6ae', 'authors': ['Roman Abramov', 'Felix Steinbauer', 'Gjergji Kasneci'], 'affiliations': ['School of Computation, Information and Technology, Technical University of Munich, Munich, Germany', 'School of Social Sciences and Technology, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.20752.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#dataset', '#reasoning', '#synthetic', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Грокинг открывает дверь к надежному фактическому рассуждению в больших языковых моделях', 'desc': 'Статья представляет новый подход к улучшению многоступенчатого фактического рассуждения в трансформерах с помощью метода грокинга. Авторы расширяют применение грокинга на реальные данные, дополняя графы знаний синтетическими данными для достижения порога, необходимого для возникновения обобщения. Удивительно, но даже фактически неверные синтетические данные могут усилить схемы рассуждений, заставляя модель полагаться на реляционную структуру. Предложенный метод достигает точности 95-100% на бенчмарке 2WikiMultiHopQA, превосходя современные результаты.'}, 'en': {'title': 'Unlocking Reasoning in Transformers with Grokking and Data Augmentation', 'desc': "This paper explores how Transformers, a type of neural network, can improve their ability to reason through complex factual information. It introduces the concept of 'grokking' to real-world data by enhancing knowledge graphs with synthetic data, which helps the model learn logical patterns instead of just memorizing facts. Interestingly, the study finds that even incorrect synthetic data can aid in developing reasoning capabilities by emphasizing relational structures. The results show significant improvements in multi-hop reasoning tasks, achieving high accuracy and suggesting that this method can enhance the reasoning abilities of large language models."}, 'zh': {'title': '利用合成数据提升多步推理能力', 'desc': '本文探讨了变换器在多步事实推理中的不足，尤其是在真实世界知识稀缺的情况下。我们首次将grokking扩展到真实世界的事实数据，并通过增强知识图谱来解决数据集稀疏性的问题。研究发现，即使是事实不正确的合成数据也能增强模型的推理能力，而不是降低准确性。我们的实验表明，该方法在多跳推理基准测试中达到了95-100%的准确率，显著优于现有的强基线。'}}}, {'id': 'https://huggingface.co/papers/2505.02222', 'title': 'Practical Efficiency of Muon for Pretraining', 'url': 'https://huggingface.co/papers/2505.02222', 'abstract': 'We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.', 'score': 26, 'issue_id': 3608, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '3b2cca9779803131', 'authors': ['Essential AI', ':', 'Ishaan Shah', 'Anthony M. Polloreno', 'Karl Stratos', 'Philip Monk', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Ashish Tanwer', 'Darsh J Shah', 'Khoi Nguyen', 'Kurt Smith', 'Michael Callahan', 'Michael Pust', 'Mohit Parmar', 'Peter Rushton', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Somanshu Singla', 'Tim Romanski', 'Yash Vanjani', 'Ashish Vaswani'], 'affiliations': ['Essential AI, San Francisco, CA'], 'pdf_title_img': 'assets/pdf/title_img/2505.02222.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Muon: Оптимизатор второго порядка для эффективного обучения больших моделей', 'desc': 'Исследователи представляют оптимизатор Muon, который превосходит AdamW по соотношению вычислительной мощности и времени обучения. Muon эффективнее сохраняет эффективность данных при больших размерах батча, позволяя проводить более экономичное обучение. Авторы изучают комбинацию Muon и максимальной параметризации обновлений (muP) для эффективного переноса гиперпараметров. Результаты подтверждены обширными экспериментами с моделями до 4 миллиардов параметров.'}, 'en': {'title': 'Muon: A New Era of Efficient Optimization in Machine Learning', 'desc': 'This paper introduces Muon, a second-order optimizer that improves upon AdamW by expanding the Pareto frontier in terms of compute-time efficiency. Muon demonstrates superior data efficiency at large batch sizes, even exceeding the critical batch size, while maintaining computational efficiency for cost-effective training. The authors also explore the integration of Muon with the maximal update parameterization (muP) to enhance hyperparameter transfer efficiency. Their extensive experiments, involving models with up to four billion parameters, validate the effectiveness of Muon across various data distributions and architectures.'}, 'zh': {'title': 'Muon优化器：超越AdamW的高效训练', 'desc': '本文展示了Muon作为一种二阶优化器的简单实现，能够在计算时间的权衡上显著扩展Pareto前沿，相比于AdamW更具优势。研究发现，Muon在大批量训练中保持数据效率，超越了所谓的临界批量大小，同时仍然保持计算效率，从而实现更经济的训练。我们还研究了Muon与最大更新参数化（muP）的结合，以实现高效的超参数转移，并提出了一种简单的递归算法，能够考虑muP中的所有误差来源，同时仅引入适度的资源开销。通过对模型规模达到四十亿参数的广泛实验以及对数据分布和架构的消融研究，我们验证了我们的发现。'}}}, {'id': 'https://huggingface.co/papers/2505.02735', 'title': 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.02735', 'abstract': 'Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.', 'score': 20, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '1ffa5567eb2f4acc', 'authors': ['Zhouliang Yu', 'Ruotian Peng', 'Keyi Ding', 'Yizhe Li', 'Zhongyuan Peng', 'Minghao Liu', 'Yifan Zhang', 'Zheng Yuan', 'Huajian Xin', 'Wenhao Huang', 'Yandong Wen', 'Ge Zhang', 'Weiyang Liu'], 'affiliations': ['M-A-P', 'Max Planck Institute for Intelligent Systems, Tübingen', 'Numina', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02735.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'FormalMATH: Новый рубеж в формальном математическом рассуждении для ИИ', 'desc': 'FormalMATH - это масштабный бенчмарк на Lean4, содержащий 5560 формально верифицированных задач от олимпиадного до университетского уровня. Авторы представляют новый полуавтоматический конвейер для формализации, использующий специализированные языковые модели (LLM) и стратегии фильтрации. Оценка современных LLM-доказателей теорем показала их ограниченность, с максимальным успехом в 16.46% при практических ограничениях. Бенчмарк выявил обратную зависимость между наличием решения на естественном языке и успехом доказательства в сценариях рассуждений по цепочке мыслей.'}, 'en': {'title': 'FormalMATH: Advancing AI in Formal Mathematical Reasoning', 'desc': 'The paper introduces FormalMATH, a comprehensive benchmark designed to enhance formal mathematical reasoning in AI. It consists of 5,560 verified problems that cover a wide range of mathematical topics, from high school to undergraduate level. The authors propose a human-in-the-loop autoformalization pipeline that utilizes large language models (LLMs) for automating the formalization process, significantly reducing the need for expert input. The study also highlights the limitations of current LLM-based theorem provers, revealing their low success rates and biases across different mathematical domains.'}, 'zh': {'title': 'FormalMATH：推动正式数学推理的基准', 'desc': '本文提出了FormalMATH，这是一个大规模的Lean4基准，包含5560个经过正式验证的数学问题，涵盖从高中奥林匹克挑战到本科定理的多个领域。为了解决手动形式化的低效问题，我们引入了一种新的人机协作自动形式化流程，结合了专门的大型语言模型（LLMs）进行语句自动形式化、多LLM语义验证和基于否定的反驳过滤策略。我们的评估显示，当前最先进的LLM定理证明器在实际采样预算下的成功率仅为16.46%，并且在不同领域表现出明显的偏差。我们认为，FormalMATH为正式数学推理提供了一个强有力的基准。'}}}, {'id': 'https://huggingface.co/papers/2505.02819', 'title': 'ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations', 'url': 'https://huggingface.co/papers/2505.02819', 'abstract': "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.", 'score': 19, 'issue_id': 3608, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '7f169d4491ab6864', 'authors': ['Dmitriy Shopkhoev', 'Ammar Ali', 'Magauiya Zhussip', 'Valentin Malykh', 'Stamatios Lefkimmiatis', 'Nikos Komodakis', 'Sergey Zagoruyko'], 'affiliations': ['Archimedes Athena RC', 'IACM-Forth', 'IITU', 'ITMO University', 'MTS AI', 'Polynome', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2505.02819.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#inference', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Эффективная обрезка трансформеров без переобучения', 'desc': 'ReplaceMe - это метод обрезки глубины трансформеров без дополнительного обучения. Он заменяет блоки трансформера линейной операцией, сохраняя высокую производительность при низких коэффициентах сжатия. Метод требует только небольшой калибровочный набор данных для оценки линейного преобразования. ReplaceMe превосходит другие подходы без обучения и конкурирует с современными методами обрезки, требующими длительного дообучения.'}, 'en': {'title': 'Effortless Depth Pruning with ReplaceMe', 'desc': 'ReplaceMe is a novel method for depth pruning in transformer models that does not require additional training. Instead of retraining the model after pruning, it uses a small calibration dataset to create a linear transformation that approximates the pruned blocks. This allows for significant compression of the model while maintaining high performance, achieving up to 25% pruning with around 90% of the original performance. The method is efficient and can be easily integrated into existing architectures without adding extra parameters, making it a competitive alternative to traditional pruning techniques.'}, 'zh': {'title': 'ReplaceMe：高效的无训练深度剪枝方法', 'desc': '我们介绍了一种名为ReplaceMe的深度剪枝方法，它不需要训练就能有效地将变换器块替换为线性操作，同时在低压缩比下保持高性能。与传统的剪枝方法不同，ReplaceMe只需一个小的校准数据集来估计线性变换，从而近似剪枝后的块。这个估计的线性映射可以与剩余的变换器块无缝合并，避免了额外的网络参数需求。我们的实验表明，ReplaceMe在多个大型语言模型上表现优异，能够实现高达25%的剪枝，同时保留约90%的原始模型性能，且无需任何训练或调整步骤。'}}}, {'id': 'https://huggingface.co/papers/2505.02391', 'title': 'Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL', 'url': 'https://huggingface.co/papers/2505.02391', 'abstract': 'Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.', 'score': 19, 'issue_id': 3605, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '48b57ad8bd358e5d', 'authors': ['Jiarui Yao', 'Yifan Hao', 'Hanning Zhang', 'Hanze Dong', 'Wei Xiong', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02391.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Динамическое распределение ресурсов для эффективного обучения рассуждениям', 'desc': 'Эта статья представляет новый метод под названием GVM-RAFT для улучшения обучения с подкреплением в задачах рассуждений на основе цепочки мыслей (CoT) в больших языковых моделях. Авторы предлагают динамическую стратегию выделения вычислительных ресурсов, которая минимизирует дисперсию стохастического градиента. Теоретический анализ показывает, что этот подход обеспечивает ускоренную сходимость при определенных условиях. Эксперименты на задачах математических рассуждений демонстрируют 2-4-кратное ускорение и значительное повышение точности по сравнению с обычным методом RAFT.'}, 'en': {'title': 'Dynamic Resource Allocation for Enhanced Reasoning in LLMs', 'desc': 'This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains.'}, 'zh': {'title': '动态样本分配，提升推理效率！', 'desc': '本文探讨了大型语言模型中的链式推理（CoT）如何被形式化为潜变量问题，模型需要生成中间推理步骤。以往的方法如迭代奖励排名微调（RAFT）在处理不同难度的提示时，通常采用统一的推理预算，这导致了效率低下。我们提出了GVM-RAFT，这是一种针对特定提示的动态样本分配策略，旨在在计算预算限制下最小化随机梯度方差。实验结果表明，GVM-RAFT在数学推理任务中实现了2-4倍的加速和显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2505.01658', 'title': 'A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency', 'url': 'https://huggingface.co/papers/2505.01658', 'abstract': 'Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine', 'score': 19, 'issue_id': 3604, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 мая', 'en': 'May 3', 'zh': '5月3日'}, 'hash': '56f8dc116dbd737d', 'authors': ['Sihyeong Park', 'Sungryeol Jeon', 'Chaelyn Lee', 'Seokhun Jeon', 'Byung-Soo Kim', 'Jemin Lee'], 'affiliations': ['Electronics and Telecommunications Research Institute, South Korea', 'Korea Electronics Technology Institute, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01658.jpg', 'data': {'categories': ['#inference', '#open_source', '#optimization', '#survey'], 'emoji': '🚀', 'ru': {'title': 'Комплексный анализ движков вывода LLM: оптимизация, производительность и будущее', 'desc': 'Данная статья представляет собой комплексный анализ 25 движков вывода для больших языковых моделей (LLM), включая открытые и коммерческие решения. Авторы исследуют каждый движок по ряду критериев, таких как простота использования, масштабируемость и оптимизация для различных сценариев применения. В работе рассматриваются методы оптимизации, поддерживаемые каждым движком, и оценивается зрелость экосистемы открытых решений. Статья завершается обзором будущих направлений исследований в области движков вывода для LLM и предоставляет практические рекомендации для разработчиков и исследователей.'}, 'en': {'title': 'Optimizing Inference: A Deep Dive into LLM Engines', 'desc': 'This paper evaluates 25 open-source and commercial inference engines designed for large language models (LLMs). It focuses on key factors such as ease-of-use, deployment, scalability, and performance in terms of throughput and latency. The study also investigates the optimization techniques supported by each engine and assesses the maturity of the open-source ecosystem compared to commercial solutions. Finally, it outlines future research directions to enhance LLM services, hardware support, and security, providing valuable insights for researchers and developers.'}, 'zh': {'title': '优化大型语言模型推理引擎的研究与评估', 'desc': '大型语言模型（LLM）在聊天机器人、代码生成器和搜索引擎中得到了广泛应用。由于链式思维、复杂推理和代理服务等工作负载的增加，推理成本显著上升。虽然已经采用并行、压缩和缓存等优化方法来降低成本，但多样化的服务需求使得选择合适的方法变得困难。本文对25个开源和商业推理引擎进行了全面评估，探讨了它们的易用性、可部署性、通用支持、可扩展性以及适合吞吐量和延迟感知计算的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.02835', 'title': 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.02835', 'abstract': "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.", 'score': 17, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '5134d59b0389ade9', 'authors': ['Yi-Fan Zhang', 'Xingyu Lu', 'Xiao Hu', 'Chaoyou Fu', 'Bin Wen', 'Tianke Zhang', 'Changyi Liu', 'Kaiyu Jiang', 'Kaibing Chen', 'Kaiyu Tang', 'Haojie Ding', 'Jiankang Chen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Liang Wang'], 'affiliations': ['CASIA', 'KuaiShou', 'NJU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2505.02835.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#multimodal', '#rl', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Стабильное обучение с подкреплением для улучшения мультимодальных моделей вознаграждения', 'desc': 'Эта статья представляет новый подход к улучшению мультимодальных моделей вознаграждения (MRM) с использованием обучения с подкреплением (RL). Авторы предлагают алгоритм StableReinforce, который решает проблемы нестабильности, возникающие при применении существующих RL-алгоритмов к моделированию вознаграждений. Они собрали набор данных из 200 тысяч примеров предпочтений для обучения MRM. Их модель R1-Reward, обученная с помощью StableReinforce, значительно превосходит предыдущие модели на эталонных тестах мультимодального моделирования вознаграждений.'}, 'en': {'title': 'Stable Reinforcement for Enhanced Multimodal Reward Modeling', 'desc': 'This paper investigates the use of Reinforcement Learning (RL) to enhance Multimodal Reward Models (MRMs) for Multimodal Large Language Models (MLLMs). It identifies challenges in applying traditional RL algorithms to reward modeling, which can lead to instability during training. To overcome these issues, the authors introduce the StableReinforce algorithm, which improves training dynamics through better loss refinement and reward design. The results show that their reward model, R1-Reward, outperforms existing state-of-the-art models on multimodal benchmarks, demonstrating the effectiveness of their approach.'}, 'zh': {'title': '稳定强化学习提升多模态奖励模型性能', 'desc': '多模态奖励模型（MRMs）在提升多模态大型语言模型（MLLMs）的性能中起着重要作用。本文探讨了如何利用强化学习（RL）来改善奖励建模，提出将奖励建模问题重新表述为基于规则的RL任务。我们提出的StableReinforce算法通过优化训练损失、优势估计策略和奖励设计，解决了现有RL算法在奖励建模中导致的不稳定性问题。经过StableReinforce算法训练的奖励模型R1-Reward在多模态奖励建模基准上显著提高了性能，展示了RL算法在优化MRMs中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.01441', 'title': 'Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.01441', 'abstract': 'Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.', 'score': 17, 'issue_id': 3603, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': '4c0a590eccbb1960', 'authors': ['Joykirat Singh', 'Raghav Magazine', 'Yash Pandya', 'Akshay Nambi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01441.jpg', 'data': {'categories': ['#agents', '#rl', '#reasoning', '#training', '#benchmark', '#optimization', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'ARTIST: Агентный ИИ с инструментами для продвинутого решения задач', 'desc': 'Статья представляет ARTIST - фреймворк, объединяющий агентное рассуждение, обучение с подкреплением и интеграцию инструментов для больших языковых моделей (LLM). ARTIST позволяет моделям автономно решать, когда и как использовать инструменты в многоэтапных цепочках рассуждений. Эксперименты показывают, что ARTIST превосходит современные базовые модели в задачах математических рассуждений и многоэтапных вызовов функций. Результаты демонстрируют, что агентное обучение с подкреплением с интеграцией инструментов открывает новые возможности для надежного и интерпретируемого решения задач в LLM.'}, 'en': {'title': 'Empowering LLMs with Dynamic Reasoning and Tool Integration', 'desc': "This paper presents ARTIST, a new framework that enhances large language models (LLMs) by integrating agentic reasoning, reinforcement learning (RL), and tool usage. Unlike traditional LLMs that rely on static knowledge, ARTIST allows models to dynamically decide when and how to use external tools during complex reasoning tasks. The framework employs outcome-based RL to improve the model's strategies for interacting with tools and environments, enabling it to learn without needing detailed step-by-step guidance. Experimental results demonstrate that ARTIST significantly outperforms existing models, achieving better reasoning and tool utilization in challenging scenarios."}, 'zh': {'title': 'ARTIST：智能推理与工具集成的新前沿', 'desc': '大型语言模型（LLMs）在复杂推理任务中取得了显著进展，但仍然受到静态内部知识和仅基于文本推理的限制。现实世界的问题解决通常需要动态的多步骤推理、适应性决策以及与外部工具和环境的交互能力。我们提出了ARTIST（自我改进变换器中的代理推理和工具集成），这是一个将代理推理、强化学习和工具集成紧密结合的统一框架。实验结果表明，ARTIST在数学推理和多轮函数调用基准测试中表现优异，相较于基础模型有高达22%的绝对提升。'}}}, {'id': 'https://huggingface.co/papers/2505.02156', 'title': 'Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents', 'url': 'https://huggingface.co/papers/2505.02156', 'abstract': "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction rightarrow deep contemplation) based on real-time context. Our framework's core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach", 'score': 16, 'issue_id': 3602, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '13e5cf390c136aeb', 'authors': ['Minzheng Wang', 'Yongbin Li', 'Haobo Wang', 'Xinghua Zhang', 'Nan Xu', 'Bingli Wu', 'Fei Huang', 'Haiyang Yu', 'Wenji Mao'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.02156.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное обучение режимам мышления для улучшения социального интеллекта ИИ', 'desc': 'Статья представляет новый метод машинного обучения под названием Adaptive Mode Learning (AML) для улучшения социального интеллекта языковых агентов. AML использует алгоритм Adaptive Mode Policy Optimization (AMPO), который динамически выбирает один из четырех режимов мышления в зависимости от контекста. Этот подход позволяет агентам адаптивно регулировать глубину рассуждений, что приводит к более эффективному использованию токенов и улучшенной производительности на задачах социального интеллекта. Эксперименты показывают, что AML превосходит современные методы, достигая на 15.6% более высокой производительности при использовании на 32.8% более коротких цепочек рассуждений.'}, 'en': {'title': 'Dynamic Reasoning for Smarter Social Agents', 'desc': 'This paper introduces Adaptive Mode Learning (AML), a new approach for simulating social intelligence in language agents. Unlike existing methods that either lack dynamic reasoning or use a one-size-fits-all strategy, AML allows agents to switch between four different thinking modes based on the context of the interaction. The core of this framework is the Adaptive Mode Policy Optimization (AMPO) algorithm, which enhances reasoning efficiency by adapting the depth of thought to the situation. Experimental results show that AML significantly improves task performance and reduces the length of reasoning chains compared to current state-of-the-art methods.'}, 'zh': {'title': '自适应模式学习：提升社交智能的推理能力', 'desc': '本论文提出了一种新的方法，称为自适应模式学习（AML），旨在提高社交智能模拟的有效性。现有方法在推理深度上缺乏灵活性，导致在不同场景下的推理不够高效。AML通过实时上下文动态选择四种思维模式，从直觉反应到深度思考，优化了推理过程。实验结果表明，AML在社交智能任务中比现有最先进的方法提高了15.6%的性能，同时推理链长度减少了32.8%。'}}}, {'id': 'https://huggingface.co/papers/2505.02094', 'title': 'SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations', 'url': 'https://huggingface.co/papers/2505.02094', 'abstract': 'We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.', 'score': 14, 'issue_id': 3604, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '1ae5a69dab5de633', 'authors': ['Runyi Yu', 'Yinhuai Wang', 'Qihan Zhao', 'Hok Wai Tsui', 'Jingbo Wang', 'Ping Tan', 'Qifeng Chen'], 'affiliations': ['HKUST Hong Kong, China', 'Shanghai AI Laboratory Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02094.jpg', 'data': {'categories': ['#games', '#training', '#rl', '#optimization', '#data'], 'emoji': '🤖', 'ru': {'title': 'Улучшение обучения с подкреплением через аугментацию демонстраций', 'desc': 'Эта статья представляет новый подход к обучению с подкреплением на основе демонстраций взаимодействия (RLID). Авторы предлагают методы аугментации данных для преодоления проблем шума и ограниченности демонстраций: Stitched Trajectory Graph (STG) и State Transition Field (STF). Они также разрабатывают стратегию Adaptive Trajectory Sampling (ATS) для динамической генерации учебной программы и механизм исторического кодирования для обучения навыкам, зависящим от памяти. Эксперименты показывают значительное улучшение по сравнению с современными методами в плане стабильности сходимости, способности к обобщению и устойчивости восстановления.'}, 'en': {'title': 'Bridging Skills: Enhancing Reinforcement Learning with Augmented Trajectories', 'desc': 'This paper tackles the issue of noise and limited coverage in Reinforcement Learning from Interaction Demonstration (RLID). It highlights that even with noisy and sparse data, there are countless feasible trajectories that can connect different demonstrated skills. The authors introduce two innovative techniques: the Stitched Trajectory Graph (STG) for identifying transitions between skills, and the State Transition Field (STF) for linking arbitrary states. Their method, combined with Adaptive Trajectory Sampling (ATS) and a historical encoding mechanism, enhances skill learning and generalization beyond the initial demonstrations, showing significant improvements in various tasks.'}, 'zh': {'title': '克服演示噪声，实现技能的鲁棒学习', 'desc': '本文探讨了从交互演示中进行强化学习（RLID）时面临的挑战，特别是演示噪声和覆盖限制。现有的数据收集方法虽然提供了有价值的交互演示，但往往导致稀疏、断裂和噪声的轨迹，无法全面捕捉技能变化和过渡的全貌。我们提出的关键见解是，尽管演示存在噪声和稀疏性，但仍然存在无限的物理可行轨迹，可以自然地连接演示技能或从其邻近状态中产生。基于此，我们提出了两种数据增强技术，旨在提高技能获取的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.02471', 'title': 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction', 'url': 'https://huggingface.co/papers/2505.02471', 'abstract': 'We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'fa1cacd261a625e3', 'authors': ['Biao Gong', 'Cheng Zou', 'Dandan Zheng', 'Hu Yu', 'Jingdong Chen', 'Jianxin Sun', 'Junbo Zhao', 'Jun Zhou', 'Kaixiang Ji', 'Lixiang Ru', 'Libin Wang', 'Qingpei Guo', 'Rui Liu', 'Weilong Chai', 'Xinyu Xiao', 'Ziyuan Huang'], 'affiliations': ['Ant Group', 'Ming-Lite-Uni'], 'pdf_title_img': 'assets/pdf/title_img/2505.02471.jpg', 'data': {'categories': ['#agi', '#multimodal', '#open_source', '#cv'], 'emoji': '🤖', 'ru': {'title': 'Объединение зрения и языка в единой мультимодальной системе', 'desc': 'Ming-Lite-Uni - это открытая мультимодальная система, объединяющая генерацию изображений и обработку естественного языка. Она использует фиксированную мультимодальную языковую модель (MLLM) и обучаемую диффузионную модель для выполнения задач генерации изображений по тексту и редактирования изображений на основе инструкций. Система вводит новые многомасштабные обучаемые токены и стратегию выравнивания многомасштабных представлений. Экспериментальные результаты демонстрируют высокую производительность Ming-Lite-Uni и впечатляющую гибкость интерактивного процесса.'}, 'en': {'title': 'Unifying Vision and Language for Advanced AI', 'desc': "Ming-Lite-Uni is an innovative open-source framework designed to integrate vision and language through a unified visual generator and a multimodal autoregressive model. It introduces advanced techniques like multi-scale learnable tokens and representation alignment to enhance the interaction between text and images. By utilizing a fixed MLLM alongside a learnable diffusion model, it supports both text-to-image generation and image editing based on instructions. The framework's strong performance and interactive capabilities highlight its potential impact on the development of advanced AI systems, contributing to the journey towards artificial general intelligence (AGI)."}, 'zh': {'title': 'Ming-Lite-Uni：统一视觉与语言的多模态框架', 'desc': 'Ming-Lite-Uni是一个开源的多模态框架，旨在统一视觉和语言。它引入了新的视觉生成器和多模态自回归模型，支持文本到图像生成和图像编辑任务。该框架采用了多尺度可学习标记和多尺度表示对齐策略，提升了模型的表现。所有代码和模型权重都是开源的，鼓励社区进一步探索。'}}}, {'id': 'https://huggingface.co/papers/2505.02370', 'title': 'SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing', 'url': 'https://huggingface.co/papers/2505.02370', 'abstract': 'Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '5833f2e06b661a76', 'authors': ['Ming Li', 'Xin Gu', 'Fan Chen', 'Xiaoying Xing', 'Longyin Wen', 'Chen Chen', 'Sijie Zhu'], 'affiliations': ['ByteDance Intelligent Creation (USA)', 'Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2505.02370.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#training', '#cv'], 'emoji': '✏️', 'ru': {'title': 'Улучшение редактирования изображений через оптимизацию инструкций', 'desc': 'Статья представляет новый подход к улучшению моделей редактирования изображений на основе инструкций. Авторы предлагают метод создания более эффективных инструкций для редактирования, включая их исправление для лучшего соответствия парам оригинальных и отредактированных изображений. Они также вводят контрастные инструкции редактирования для повышения эффективности обучения. Результаты показывают значительное улучшение производительности по сравнению с существующими методами, достигая 9.19% улучшения на бенчмарке Real-Edit при использовании в 30 раз меньше данных для обучения и в 13 раз меньшем размере модели.'}, 'en': {'title': 'Enhancing Image Editing with Accurate Instructions and Contrastive Supervision', 'desc': 'This paper addresses the problem of noisy supervision in image editing tasks caused by mismatched editing instructions and image pairs. The authors propose a novel approach that involves creating more accurate editing instructions that align better with the original and edited images. They introduce contrastive editing instructions to enhance the effectiveness of these instructions and utilize a unified guide for vision-language models to rectify them. Their method improves supervision signals without relying on pre-training or VLM modules, achieving significant performance gains on benchmarks with less training data and smaller model sizes.'}, 'zh': {'title': '提升图像编辑效果的新方法', 'desc': '本论文提出了一种新方法，旨在改善图像编辑模型的监督信号。我们通过构建更有效的编辑指令，使其与原始和编辑后的图像对更好地对齐，并引入对比编辑指令来增强效果。研究发现，编辑模型在不同推理步骤中表现出特定的生成属性，这些属性与文本无关。我们的方法不依赖于之前的视觉语言模型或预训练任务，提供了一种更直接和高效的监督信号，显著提升了图像编辑的效果。'}}}, {'id': 'https://huggingface.co/papers/2505.01043', 'title': 'Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities', 'url': 'https://huggingface.co/papers/2505.01043', 'abstract': 'Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'b67e3ec75756e896', 'authors': ['Zhiwei Hao', 'Jianyuan Guo', 'Li Shen', 'Yong Luo', 'Han Hu', 'Guoxia Wang', 'Dianhai Yu', 'Yonggang Wen', 'Dacheng Tao'], 'affiliations': ['Baidu Inc., Beijing 100000, China', 'College of Computing and Data Science, Nanyang Technological University, 639798, Singapore', 'Computer Science, City University of Hong Kong, Hong Kong 999077, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China', 'School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China', 'School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01043.jpg', 'data': {'categories': ['#optimization', '#inference', '#survey', '#training'], 'emoji': '🔬', 'ru': {'title': 'Систематизация методов обучения нейросетей с низкой точностью для повышения эффективности', 'desc': 'Статья представляет собой обзор методов обучения нейронных сетей с низкой точностью вычислений. Авторы систематизируют существующие подходы, разделяя их на три основные категории: методы на основе фиксированной точки и целых чисел, методы на основе чисел с плавающей запятой и методы с использованием специализированных форматов. В работе также обсуждаются подходы к обучению с учетом квантования и намечаются перспективные направления исследований в этой области. Обзор направлен на предоставление исследователям единого представления о ландшафте методов обучения с низкой точностью.'}, 'en': {'title': 'Optimizing Large Language Models with Low-Precision Training', 'desc': 'This paper reviews low-precision training techniques for large language models (LLMs), which help improve training efficiency while reducing hardware resource demands. It categorizes these methods into three main groups based on numerical formats: fixed-point and integer-based, floating-point-based, and customized format-based methods. The survey also addresses quantization-aware training, which is closely related to low-precision training during model inference. By organizing the existing research, the paper aims to provide clarity and direction for future advancements in low-precision training.'}, 'zh': {'title': '低精度训练：提升效率的关键', 'desc': '大型语言模型（LLMs）在多个领域取得了显著的表现，但其训练所需的硬件资源极大，成为效率和可扩展性的障碍。为了应对这一挑战，低精度训练技术被广泛采用，显著提高了训练效率。尽管如此，低精度训练涉及多个组件，如权重、激活和梯度，每个组件可以用不同的数值格式表示，导致研究领域的碎片化。本文综述了现有的低精度训练方法，并根据数值格式将其系统地分类为三大类，以便于研究者理解和参考。'}}}, {'id': 'https://huggingface.co/papers/2505.02625', 'title': 'LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis', 'url': 'https://huggingface.co/papers/2505.02625', 'abstract': 'Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.', 'score': 7, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '2f8233e0d3083036', 'authors': ['Qingkai Fang', 'Yan Zhou', 'Shoutao Guo', 'Shaolei Zhang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02625.jpg', 'data': {'categories': ['#audio', '#small_models', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Прорыв в разговорном ИИ: компактная модель превосходит гигантов', 'desc': 'В статье представлена модель LLaMA-Omni 2 - серия речевых языковых моделей для естественного голосового взаимодействия. Модель основана на архитектуре Qwen2.5 и дополнена речевым энкодером и декодером. Несмотря на небольшой объем обучающих данных, LLaMA-Omni 2 показывает высокие результаты в задачах голосового вопросно-ответного взаимодействия. Модель превосходит предыдущие аналоги, обученные на гораздо большем объеме речевых данных.'}, 'en': {'title': 'Revolutionizing Speech Interaction with LLaMA-Omni 2', 'desc': 'This paper presents LLaMA-Omni 2, a new series of speech language models designed for real-time speech interaction. These models, ranging from 0.5B to 14B parameters, utilize a speech encoder and an autoregressive streaming speech decoder to facilitate intelligent spoken chatbots. Remarkably, LLaMA-Omni 2 achieves high performance despite being trained on only 200K multi-turn speech dialogue samples. It outperforms previous models like GLM-4-Voice, which were trained on significantly larger datasets, showcasing the efficiency of the new architecture.'}, 'zh': {'title': '实时智能语音交互的新突破', 'desc': '本论文介绍了LLaMA-Omni 2，这是一个系列的语音语言模型，参数范围从0.5亿到14亿，能够实现高质量的实时语音交互。该模型基于Qwen2.5系列，结合了语音编码器和自回归流式语音解码器。尽管仅在20万多轮语音对话样本上进行训练，LLaMA-Omni 2在多个语音问答和语音指令跟随基准测试中表现出色，超越了之前的最先进模型GLM-4-Voice。这表明，使用较少的数据也能训练出高效的智能语音聊天机器人。'}}}, {'id': 'https://huggingface.co/papers/2505.01583', 'title': 'TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action', 'url': 'https://huggingface.co/papers/2505.01583', 'abstract': 'Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.', 'score': 6, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'd29565337415d11f', 'authors': ['Jen-Hao Cheng', 'Vivian Wang', 'Huayu Wang', 'Huapeng Zhou', 'Yi-Hao Peng', 'Hou-I Liu', 'Hsiang-Wei Huang', 'Kuang-Ming Chen', 'Cheng-Yen Yang', 'Wenhao Chai', 'Yi-Ling Chen', 'Vibhav Vineet', 'Qin Cai', 'Jenq-Neng Hwang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.01583.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#video', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'TEMPURA: Улучшение понимания причинно-следственных связей в видео с помощью маскированного предсказания событий', 'desc': 'TEMPURA - это двухэтапная система обучения для улучшения понимания временных событий в видео. На первом этапе она использует маскированное предсказание событий для реконструкции пропущенных событий и генерации пошаговых причинно-следственных объяснений. Затем TEMPURA учится сегментировать видео и создавать подробные описания для каждого сегмента. Система обучается на большом наборе данных VER, содержащем 1 млн обучающих примеров и 500 тыс. видео с временными метками событий. Эксперименты показывают, что TEMPURA превосходит базовые модели в задачах временной локализации и обнаружения ключевых моментов.'}, 'en': {'title': 'TEMPURA: Enhancing Video Understanding through Causal Reasoning and Temporal Segmentation', 'desc': 'This paper introduces TEMPURA, a novel framework designed to improve how vision-language models understand the timing and relationships of events in videos. It addresses the limitations of existing methods that either lose important details by compressing video data or treat videos as continuous streams without clear boundaries. TEMPURA employs a two-stage training process that first predicts missing events to create causal explanations and then segments videos into distinct events with precise descriptions. The framework is trained on a large dataset, VER, and shows significant improvements in tasks like temporal grounding and highlight detection compared to existing models.'}, 'zh': {'title': 'TEMPURA：提升视频理解的因果推理与时间分割结合', 'desc': '本论文提出了一种名为TEMPURA的框架，用于提高视频的时间理解能力。TEMPURA通过掩蔽事件预测推理，重建缺失事件并生成逐步的因果解释，从而更好地理解视频中的事件关系。接着，它学习视频分割和密集标注，将视频分解为不重叠的事件，并提供详细的时间戳对齐描述。实验结果表明，TEMPURA在时间定位和高亮检测基准测试中优于现有的强基线模型，验证了因果推理与细粒度时间分割的结合能够提升视频理解。'}}}, {'id': 'https://huggingface.co/papers/2505.02823', 'title': 'MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing', 'url': 'https://huggingface.co/papers/2505.02823', 'abstract': 'Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.', 'score': 3, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'da9d48f7aea6c0bd', 'authors': ['Zinan Guo', 'Pengze Zhang', 'Yanze Wu', 'Chong Mou', 'Songtao Zhao', 'Qian He'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2505.02823.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#multimodal', '#data'], 'emoji': '🖼️', 'ru': {'title': 'MUSAR: Эффективная многосубъектная кастомизация на ограниченных данных', 'desc': 'MUSAR - это новая структура для многосубъектной кастомизации в машинном обучении. Она решает проблемы ограниченности данных и переплетения атрибутов между субъектами, используя обучение на диптихах и динамическую маршрутизацию внимания. MUSAR позволяет достичь высокого качества генерации изображений и согласованности субъектов, используя только данные по отдельным субъектам. Эксперименты показывают превосходство MUSAR над существующими методами по качеству изображений, согласованности субъектов и естественности взаимодействия.'}, 'en': {'title': 'MUSAR: Unlocking Multi-Subject Customization with Single-Subject Data', 'desc': 'The paper presents MUSAR, a novel framework designed to enhance multi-subject customization in machine learning. It addresses two main challenges: the scarcity of diverse multi-subject training data and the issue of attribute entanglement among different subjects. MUSAR employs debiased diptych learning to create training pairs from single-subject images, correcting biases through static attention routing and dual-branch LoRA. Additionally, it utilizes a dynamic attention routing mechanism to establish clear mappings between generated images and their corresponding subjects, leading to improved image quality and consistency across subjects.'}, 'zh': {'title': 'MUSAR：单一主题数据下的多主题定制新方法', 'desc': '当前的多主题定制方法面临两个主要挑战：获取多样化的多主题训练数据的困难，以及不同主题之间属性的纠缠。为了解决这些问题，我们提出了MUSAR框架，它能够在仅需单一主题训练数据的情况下，实现稳健的多主题定制。首先，我们引入了去偏差的双联学习，通过从单一主题图像构建双联训练对，促进多主题学习，并通过静态注意力路由和双分支LoRA主动纠正双联构建引入的分布偏差。其次，我们引入了动态注意力路由机制，适应性地建立生成图像与条件主题之间的双射映射，从而消除跨主题的纠缠。'}}}, {'id': 'https://huggingface.co/papers/2505.02005', 'title': 'Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields', 'url': 'https://huggingface.co/papers/2505.02005', 'abstract': 'Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.', 'score': 3, 'issue_id': 3611, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'b0fc91d25884f885', 'authors': ['Zhenxing Mi', 'Ping Yin', 'Xue Xiao', 'Dan Xu'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR', 'Inspur Cloud Information Technology Co, Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2505.02005.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset'], 'emoji': '🏙️', 'ru': {'title': 'Масштабируемый NeRF для эффективного моделирования больших сцен', 'desc': 'Switch-NeRF++ представляет собой новый подход к моделированию крупномасштабных сцен с использованием нейронных полей излучения (NeRF). Эта модель использует гетерогенную смесь хеш-экспертов (HMoHE) для эффективного разложения сцены и обучения специализированных NeRF-экспертов. Ключевыми элементами являются сеть гейтинга на основе хеширования для декомпозиции сцены и гетерогенные хеш-эксперты с различными диапазонами разрешения. Switch-NeRF++ демонстрирует значительное улучшение качества рендеринга и эффективности по сравнению с предыдущими методами для крупномасштабных сцен.'}, 'en': {'title': 'Efficient Scene Decomposition for Scalable NeRFs', 'desc': 'This paper presents Switch-NeRF++, a novel approach to improve the scalability and efficiency of Neural Radiance Fields (NeRF) for large-scale scenes. It introduces a Heterogeneous Mixture of Hash Experts (HMoHE) network that learns to decompose scenes into specialized components, allowing for better handling of scene heterogeneity. The framework utilizes a gating network that allocates 3D points to different NeRF experts, optimizing the learning process in an end-to-end manner. The results show significant improvements in both training and rendering speeds, achieving state-of-the-art accuracy on large-scale datasets.'}, 'zh': {'title': '高效大规模场景建模的新方法', 'desc': '本文介绍了一种名为Switch-NeRF++的网络，旨在解决大规模场景中的场景分解、建模异质性和建模效率等问题。该网络采用了异质混合哈希专家（HMoHE）架构，能够高效地学习异质分解和异质NeRF。通过一个门控网络，系统能够将3D点分配给专门的NeRF专家，从而实现端到端的学习。实验结果表明，该方法在大规模场景建模中具有优越的准确性和效率，训练速度提升8倍，渲染速度提升16倍。'}}}, {'id': 'https://huggingface.co/papers/2505.01456', 'title': 'Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation', 'url': 'https://huggingface.co/papers/2505.01456', 'abstract': 'LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.', 'score': 2, 'issue_id': 3608, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '0201cbbcc6e52005', 'authors': ['Vaidehi Patil', 'Yi-Lin Sung', 'Peter Hase', 'Jie Peng', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science University of North Carolina at Chapel Hill', 'School of Artificial Intelligence and Data Science University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01456.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#interpretability', '#security', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Забывание по требованию: новый рубеж в безопасности мультимодальных ИИ', 'desc': 'Эта статья посвящена проблеме целенаправленного забывания информации в мультимодальных языковых моделях (MLLM). Авторы представляют новый бенчмарк UnLOK-VQA для оценки методов удаления специфических знаний из MLLM. Они предлагают фреймворк атак и защиты, включающий шесть целей защиты и семь типов атак. Результаты показывают, что мультимодальные атаки эффективнее одномодальных, а наилучшая защита достигается удалением информации об ответе из внутренних состояний модели.'}, 'en': {'title': 'Enhancing Safety in Multimodal LLMs through Targeted Unlearning', 'desc': 'This paper discusses the risks associated with large language models (LLMs) that are trained on extensive datasets, particularly the unintentional acquisition of sensitive information. It highlights the challenges of multimodal LLMs, which combine text and images, making them more vulnerable to adversarial attacks that can extract private details. The authors introduce a new benchmark called UnLOK-VQA for evaluating targeted unlearning in multimodal contexts, along with a framework for assessing methods to remove specific knowledge from these models. Their findings indicate that multimodal attacks are more effective than those targeting only text or images, and that larger models tend to be more robust against such attacks, emphasizing the need for improved unlearning techniques in multimodal settings.'}, 'zh': {'title': '多模态大语言模型的敏感信息遗忘挑战', 'desc': '本论文探讨了多模态大语言模型（MLLMs）在处理敏感信息时的风险，尤其是在图像和文本结合的情况下。研究表明，攻击者可以利用多模态提示来提取这些敏感信息，因此需要有效的目标性遗忘机制。为此，作者提出了一个新的基准测试UnLOK-VQA，并建立了一个攻击与防御框架，以评估从MLLMs中删除特定多模态知识的方法。实验结果显示，多模态攻击的效果优于单一文本或图像攻击，而更大的模型在后期编辑时表现出更强的鲁棒性，表明模型规模对安全性有积极影响。'}}}, {'id': 'https://huggingface.co/papers/2505.02130', 'title': 'Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data', 'url': 'https://huggingface.co/papers/2505.02130', 'abstract': "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: https://github.com/millioniron/LLM_exploration{LLM4Exploration}", 'score': 1, 'issue_id': 3616, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'a07adea642e1877d', 'authors': ['Zhong Guan', 'Likang Wu', 'Hongke Zhao', 'Ming He', 'Jianpin Fan'], 'affiliations': ['AI Lab at Lenovo', 'College of Management and Economics, Tianjin University, Tianjin, China', 'Laboratory of Computation and Analytics of Complex Management Systems, Tianjin University, Tianjin, China', 'ai-deepcube'], 'pdf_title_img': 'assets/pdf/title_img/2505.02130.jpg', 'data': {'categories': ['#graphs', '#training', '#architecture', '#interpretability'], 'emoji': '🕸️', 'ru': {'title': 'Преодоление ограничений внимания в LLM для графовых данных', 'desc': 'Это исследование изучает, как модели большого языка (LLM) обрабатывают графовые данные с точки зрения механизмов внимания. Авторы обнаружили, что LLM могут распознавать графовые структуры, но испытывают трудности с моделированием отношений между узлами. Распределение внимания LLM по узлам графа не соответствует идеальным структурным паттернам. Исследователи предлагают использовать промежуточные окна внимания для улучшения обучения LLM на графовых данных.'}, 'en': {'title': 'Unlocking LLMs: Enhancing Graph Understanding with Attention', 'desc': 'This paper investigates how large language models (LLMs) utilize attention mechanisms when processing graph-structured data. It highlights that while LLMs can identify interactions between text and graph nodes, they struggle with understanding the relationships between nodes due to their architectural limitations. The study reveals that the attention distribution in LLMs does not effectively reflect the underlying graph structure, leading to suboptimal performance. To address these issues, the authors propose using intermediate-state attention windows to enhance training and improve the transition to fully connected attention during inference.'}, 'zh': {'title': '探索注意力机制在图数据中的表现', 'desc': '本文探讨了注意力机制在大型语言模型（LLMs）处理图结构数据时的表现。研究发现，尽管LLMs能够识别图数据并捕捉文本与节点之间的交互，但在建模节点间关系时存在困难。注意力分布未能与理想的结构模式对齐，显示出对图拓扑的适应性不足。通过引入中间状态的注意力窗口，研究表明可以提高LLMs的训练性能，并在推理时无缝过渡到完全连接的窗口。'}}}, {'id': 'https://huggingface.co/papers/2505.01548', 'title': 'Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional\n  Motion-enhanced Event Representation', 'url': 'https://huggingface.co/papers/2505.01548', 'abstract': 'Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.', 'score': 1, 'issue_id': 3619, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': '07a36a1d377e5cc7', 'authors': ['Zhen Yao', 'Xiaowen Ying', 'Mooi Choo Chuah'], 'affiliations': ['Lehigh University', 'Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01548.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': '🎥', 'ru': {'title': 'Улучшение слияния RGB и событийных данных для семантической сегментации', 'desc': 'Данная статья представляет новый подход к объединению данных с RGB-камер и событийных камер для задач компьютерного зрения. Авторы предлагают новое представление событий - Motion-enhanced Event Tensor (MET), которое преобразует разреженные воксели событий в плотную и темпорально согласованную форму. Также вводятся модули BFAM и TFM для устранения пространственно-временного и модального рассогласования между RGB и событийными данными. Эксперименты показывают значительное улучшение результатов семантической сегментации по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing RGB-Event Fusion with Motion-Enhanced Event Tensor', 'desc': 'This paper addresses the challenges of combining RGB images with event camera data in computer vision. It identifies three main types of misalignments: temporal, spatial, and modal, which hinder effective integration. To overcome these issues, the authors introduce the Motion-enhanced Event Tensor (MET), which creates a dense and coherent representation of events. Additionally, they propose two modules, BFAM and TFM, to further align the data and improve performance in semantic segmentation tasks, achieving superior results on benchmark datasets.'}, 'zh': {'title': '运动增强事件张量：解决RGB-事件融合挑战的创新方法', 'desc': '本文提出了一种新颖的事件表示方法，称为运动增强事件张量（MET），旨在解决RGB-事件融合中的三种内在不对齐问题：时间、空间和模态不对齐。MET通过利用密集光流和事件时间特征，将稀疏事件体素转化为密集且时间一致的形式，从而克服了现有方法的局限性。我们还引入了频率感知双向流聚合模块（BFAM）和时间融合模块（TFM），以解决模态和时空不对齐问题。实验结果表明，我们的框架在两个大规模数据集上显著优于现有的RGB-事件语义分割方法。'}}}, {'id': 'https://huggingface.co/papers/2505.07062', 'title': 'Seed1.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2505.07062', 'abstract': 'We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)', 'score': 83, 'issue_id': 3722, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 мая', 'en': 'May 11', 'zh': '5月11日'}, 'hash': 'c3406b40cc21820d', 'authors': ['Dong Guo', 'Faming Wu', 'Feida Zhu', 'Fuxing Leng', 'Guang Shi', 'Haobin Chen', 'Haoqi Fan', 'Jian Wang', 'Jianyu Jiang', 'Jiawei Wang', 'Jingji Chen', 'Jingjia Huang', 'Kang Lei', 'Liping Yuan', 'Lishu Luo', 'Pengfei Liu', 'Qinghao Ye', 'Rui Qian', 'Shen Yan', 'Shixiong Zhao', 'Shuai Peng', 'Shuangye Li', 'Sihang Yuan', 'Sijin Wu', 'Tianheng Cheng', 'Weiwei Liu', 'Wenqian Wang', 'Xianhan Zeng', 'Xiao Liu', 'Xiaobo Qin', 'Xiaohan Ding', 'Xiaojun Xiao', 'Xiaoying Zhang', 'Xuanwei Zhang', 'Xuehan Xiong', 'Yanghua Peng', 'Yangrui Chen', 'Yanwei Li', 'Yanxu Hu', 'Yi Lin', 'Yiyuan Hu', 'Yiyuan Zhang', 'Youbin Wu', 'Yu Li', 'Yudong Liu', 'Yue Ling', 'Yujia Qin', 'Zanbo Wang', 'Zhiwu He', 'Aoxue Zhang', 'Bairen Yi', 'Bencheng Liao', 'Can Huang', 'Can Zhang', 'Chaorui Deng', 'Chaoyi Deng', 'Cheng Lin', 'Cheng Yuan', 'Chenggang Li', 'Chenhui Gou', 'Chenwei Lou', 'Chengzhi Wei', 'Chundian Liu', 'Chunyuan Li', 'Deyao Zhu', 'Donghong Zhong', 'Feng Li', 'Feng Zhang', 'Gang Wu', 'Guodong Li', 'Guohong Xiao', 'Haibin Lin', 'Haihua Yang', 'Haoming Wang', 'Heng Ji', 'Hongxiang Hao', 'Hui Shen', 'Huixia Li', 'Jiahao Li', 'Jialong Wu', 'Jianhua Zhu', 'Jianpeng Jiao', 'Jiashi Feng', 'Jiaze Chen', 'Jianhui Duan', 'Jihao Liu', 'Jin Zeng', 'Jingqun Tang', 'Jingyu Sun', 'Joya Chen', 'Jun Long', 'Junda Feng', 'Junfeng Zhan', 'Junjie Fang', 'Junting Lu', 'Kai Hua', 'Kai Liu', 'Kai Shen', 'Kaiyuan Zhang', 'Ke Shen', 'Ke Wang', 'Keyu Pan', 'Kun Zhang', 'Kunchang Li', 'Lanxin Li', 'Lei Li', 'Lei Shi', 'Li Han', 'Liang Xiang', 'Liangqiang Chen', 'Lin Chen', 'Lin Li', 'Lin Yan', 'Liying Chi', 'Longxiang Liu', 'Mengfei Du', 'Mingxuan Wang', 'Ningxin Pan', 'Peibin Chen', 'Pengfei Chen', 'Pengfei Wu', 'Qingqing Yuan', 'Qingyao Shuai', 'Qiuyan Tao', 'Renjie Zheng', 'Renrui Zhang', 'Ru Zhang', 'Rui Wang', 'Rui Yang', 'Rui Zhao', 'Shaoqiang Xu', 'Shihao Liang', 'Shipeng Yan', 'Shu Zhong', 'Shuaishuai Cao', 'Shuangzhi Wu', 'Shufan Liu', 'Shuhan Chang', 'Songhua Cai', 'Tenglong Ao', 'Tianhao Yang', 'Tingting Zhang', 'Wanjun Zhong', 'Wei Jia', 'Wei Weng', 'Weihao Yu', 'Wenhao Huang', 'Wenjia Zhu', 'Wenli Yang', 'Wenzhi Wang', 'Xiang Long', 'XiangRui Yin', 'Xiao Li', 'Xiaolei Zhu', 'Xiaoying Jia', 'Xijin Zhang', 'Xin Liu', 'Xinchen Zhang', 'Xinyu Yang', 'Xiongcai Luo', 'Xiuli Chen', 'Xuantong Zhong', 'Xuefeng Xiao', 'Xujing Li', 'Yan Wu', 'Yawei Wen', 'Yifan Du', 'Yihao Zhang', 'Yining Ye', 'Yonghui Wu', 'Yu Liu', 'Yu Yue', 'Yufeng Zhou', 'Yufeng Yuan', 'Yuhang Xu', 'Yuhong Yang', 'Yun Zhang', 'Yunhao Fang', 'Yuntao Li', 'Yurui Ren', 'Yuwen Xiong', 'Zehua Hong', 'Zehua Wang', 'Zewei Sun', 'Zeyu Wang', 'Zhao Cai', 'Zhaoyue Zha', 'Zhecheng An', 'Zhehui Zhao', 'Zhengzhuo Xu', 'Zhipeng Chen', 'Zhiyong Wu', 'Zhuofan Zheng', 'Zihao Wang', 'Zilong Huang', 'Ziyu Zhu', 'Zuquan Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07062.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#architecture', '#training', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'Компактная мультимодальная модель с выдающимися способностями', 'desc': 'Seed1.5-VL - это мультимодальная модель, сочетающая зрение и язык для общего понимания и рассуждений. Она состоит из энкодера изображений на 532 млн параметров и языковой модели на основе смеси экспертов с 20 млрд активных параметров. Несмотря на компактную архитектуру, модель показывает высокие результаты в широком спектре задач, достигая state-of-the-art на 38 из 60 публичных бенчмарков. Seed1.5-VL особенно эффективна в задачах управления интерфейсами, игровом процессе и визуальных головоломках, превосходя ведущие мультимодальные системы.'}, 'en': {'title': 'Empowering Multimodal Understanding with Seed1.5-VL', 'desc': 'Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles.'}, 'zh': {'title': 'Seed1.5-VL：多模态理解与推理的新突破', 'desc': '我们介绍了Seed1.5-VL，这是一种旨在提升多模态理解和推理的视觉-语言基础模型。Seed1.5-VL由一个532M参数的视觉编码器和一个具有20B活跃参数的专家混合模型（MoE LLM）组成。尽管其架构相对紧凑，但在多个公共VLM基准测试中表现出色，在60个公共基准中有38个达到了最先进的性能。此外，在以代理为中心的任务中，如图形用户界面控制和游戏玩法，Seed1.5-VL超越了领先的多模态系统，包括OpenAI CUA和Claude 3.7。'}}}, {'id': 'https://huggingface.co/papers/2505.07608', 'title': 'MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining', 'url': 'https://huggingface.co/papers/2505.07608', 'abstract': "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.", 'score': 52, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9db5f7b72add3369', 'authors': ['Xiaomi LLM-Core Team', ':', 'Bingquan Xia', 'Bowen Shen', 'Cici', 'Dawei Zhu', 'Di Zhang', 'Gang Wang', 'Hailin Zhang', 'Huaqiu Liu', 'Jiebao Xiao', 'Jinhao Dong', 'Liang Zhao', 'Peidian Li', 'Peng Wang', 'Shihua Yu', 'Shimao Chen', 'Weikun Wang', 'Wenhan Ma', 'Xiangwei Deng', 'Yi Huang', 'Yifan Song', 'Zihan Jiang', 'Bowen Ye', 'Can Cai', 'Chenhong He', 'Dong Zhang', 'Duo Zhang', 'Guoan Wang', 'Hao Tian', 'Haochen Zhao', 'Heng Qu', 'Hongshen Xu', 'Jun Shi', 'Kainan Bao', 'QingKai Fang', 'Kang Zhou', 'Kangyang Zhou', 'Lei Li', 'Menghang Zhu', 'Nuo Chen', 'Qiantong Wang', 'Shaohui Liu', 'Shicheng Li', 'Shuhao Gu', 'Shuhuai Ren', 'Shuo Liu', 'Sirui Deng', 'Weiji Zhuang', 'Weiwei Lv', 'Wenyu Yang', 'Xin Zhang', 'Xing Yong', 'Xing Zhang', 'Xingchen Song', 'Xinzhe Xu', 'Xu Wang', 'Yihan Yan', 'Yu Tu', 'Yuanyuan Tian', 'Yudong Wang', 'Yue Yu', 'Zhenru Lin', 'Zhichao Song', 'Zihao Yue'], 'affiliations': ['Xiaomi LLM-Core Team'], 'pdf_title_img': 'assets/pdf/title_img/2505.07608.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#dataset', '#math', '#rl', '#data', '#training'], 'emoji': '🧠', 'ru': {'title': 'MiMo-7B: Мощная языковая модель для сложных рассуждений', 'desc': 'MiMo-7B - это большая языковая модель, оптимизированная для задач рассуждения. В процессе предварительного обучения использовалась улучшенная обработка данных и трехэтапная стратегия смешивания для усиления потенциала базовой модели. На этапе пост-обучения применялось обучение с подкреплением на наборе из 130 тысяч верифицируемых задач по математике и программированию. Итоговая модель MiMo-7B-RL превосходит более крупные модели в задачах рассуждения, математики и программирования.'}, 'en': {'title': 'MiMo-7B: Revolutionizing Reasoning with Advanced Training Techniques', 'desc': 'MiMo-7B is a large language model specifically designed for reasoning tasks, optimized through both pre-training and post-training processes. In the pre-training phase, it utilizes an advanced data preprocessing pipeline and a three-stage data mixing strategy to enhance its reasoning capabilities, training on a massive dataset of 25 trillion tokens. The post-training phase involves reinforcement learning with a curated dataset of 130,000 math and programming problems, addressing sparse-reward challenges with a code-reward scheme and strategic data resampling. Evaluations demonstrate that MiMo-7B-Base excels in reasoning tasks, outperforming larger models, while the final RL-tuned version, MiMo-7B-RL, achieves outstanding results in mathematics, coding, and general reasoning tasks.'}, 'zh': {'title': 'MiMo-7B：推理任务的强大语言模型', 'desc': '我们介绍了MiMo-7B，这是一个专为推理任务设计的大型语言模型，优化了预训练和后训练阶段。在预训练过程中，我们增强了数据预处理流程，并采用三阶段数据混合策略，以提升基础模型的推理能力。MiMo-7B-Base在25万亿个标记上进行预训练，并增加了多标记预测目标，以提高性能和加速推理速度。在后训练阶段，我们整理了130K个可验证的数学和编程问题数据集，结合测试难度驱动的代码奖励机制，解决稀疏奖励问题，并采用战略性数据重采样来稳定训练。'}}}, {'id': 'https://huggingface.co/papers/2505.07747', 'title': 'Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets', 'url': 'https://huggingface.co/papers/2505.07747', 'abstract': 'While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.', 'score': 46, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'd9ffe741ebae4acb', 'authors': ['Weiyu Li', 'Xuanyang Zhang', 'Zheng Sun', 'Di Qi', 'Hao Li', 'Wei Cheng', 'Weiwei Cai', 'Shihao Wu', 'Jiarui Liu', 'Zihao Wang', 'Xiao Chen', 'Feipeng Tian', 'Jianxiong Pan', 'Zeming Li', 'Gang Yu', 'Xiangyu Zhang', 'Daxin Jiang', 'Ping Tan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07747.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#data', '#diffusion', '#transfer_learning', '#3d', '#benchmark'], 'emoji': '🧊', 'ru': {'title': 'Открытая платформа для AI-генерации 3D-объектов нового поколения', 'desc': 'Статья представляет Step1X-3D - открытую систему для генерации 3D-объектов с использованием искусственного интеллекта. Авторы разработали процесс обработки большого набора данных, создав высококачественный датасет из 2 миллионов 3D-моделей. Архитектура системы включает гибридный VAE-DiT генератор геометрии и диффузионный модуль для синтеза текстур. Step1X-3D демонстрирует высокое качество генерации и позволяет применять методы контроля из 2D-генерации к 3D-синтезу.'}, 'en': {'title': 'Revolutionizing 3D Generation with Step1X-3D', 'desc': 'The paper introduces Step1X-3D, a framework designed to improve 3D generation in artificial intelligence. It tackles challenges like limited data and algorithmic issues by creating a high-quality dataset and employing a two-stage architecture that combines a geometry generator and a texture synthesis module. The framework allows for better detail preservation and consistency in 3D assets by integrating techniques from 2D generation. By providing open-source resources, it aims to enhance research and development in controllable 3D asset generation.'}, 'zh': {'title': 'Step1X-3D：开创可控3D生成的新标准', 'desc': '本论文介绍了Step1X-3D，这是一个开放框架，旨在解决3D生成中的数据稀缺、算法限制和生态系统碎片化等挑战。该框架通过严格的数据整理流程，处理超过500万资产，创建了一个200万高质量数据集，并采用标准化的几何和纹理属性。它结合了混合VAE-DiT几何生成器和基于扩散的纹理合成模块，能够生成高质量的3D模型。Step1X-3D还支持将2D控制技术直接转移到3D合成，推动了可控3D资产生成的新标准。'}}}, {'id': 'https://huggingface.co/papers/2505.07787', 'title': 'Learning from Peers in Reasoning Models', 'url': 'https://huggingface.co/papers/2505.07787', 'abstract': 'Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP\'s robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .', 'score': 34, 'issue_id': 3722, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '350f28f20ab516fc', 'authors': ['Tongxu Luo', 'Wenyu Du', 'Jiaxi Bi', 'Stephen Chung', 'Zhengyang Tang', 'Hao Yang', 'Min Zhang', 'Benyou Wang'], 'affiliations': ['DualityRL', 'Huawei', 'The Chinese University of Hong Kong, Shenzhen', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2505.07787.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#small_models', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Коллективный разум: как модели машинного обучения учатся друг у друга', 'desc': "Исследование показывает, что крупные модели рассуждений (LRM) могут попадать в 'ловушку доминирования префикса', когда плохое начало рассуждения мешает самокоррекции. Для решения этой проблемы предложен метод 'Обучение у сверстников' (LeaP), позволяющий моделям обмениваться промежуточными выводами во время вывода. Авторы также представили серию моделей LeaP-T, настроенных для эффективного следования инструкциям по обобщению и рефлексии. Эксперименты на математических бенчмарках показали значительное улучшение производительности моделей с использованием LeaP."}, 'en': {'title': 'Empowering LRMs through Peer Collaboration', 'desc': "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."}, 'zh': {'title': '同伴学习：提升推理模型的自我纠正能力', 'desc': '大型推理模型（LRMs）具有自我纠正的能力，但当推理过程以短而差的开头开始时，模型很难恢复。我们称这种现象为“前缀主导陷阱”。为了解决这个问题，我们提出了“从同伴学习”（LeaP），通过路由机制让每个推理路径总结其中间推理并与其他路径共享，从而在推理过程中融入同伴的见解。实验结果表明，LeaP显著提高了模型的表现，尤其是在处理不同任务难度时展现出强大的错误容忍能力。'}}}, {'id': 'https://huggingface.co/papers/2505.07447', 'title': 'Unified Continuous Generative Models', 'url': 'https://huggingface.co/papers/2505.07447', 'abstract': 'Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.', 'score': 31, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '8d18ef028e9905b1', 'authors': ['Peng Sun', 'Yi Jiang', 'Tao Lin'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07447.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Единый фреймворк для непрерывных генеративных моделей: от многошаговых до малошаговых', 'desc': 'Статья представляет унифицированный подход к обучению и сэмплированию непрерывных генеративных моделей. Авторы объединяют многошаговые методы, такие как диффузия и flow-matching, с малошаговыми подходами вроде consistency models. Их реализация, UCGM-{T,S}, достигает state-of-the-art результатов на ImageNet 256x256, используя диффузионный трансформер. Кроме того, UCGM-S улучшает производительность предобученной модели, снижая FID и количество шагов сэмплирования.'}, 'en': {'title': 'Unifying Generative Models for Superior Performance', 'desc': 'This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.'}, 'zh': {'title': '统一生成模型，提升生成性能！', 'desc': '本文介绍了一种统一的连续生成模型框架，旨在整合多步和少步生成方法的训练和采样。通过引入统一的训练和采样器（UCGM-{T,S}），我们实现了最先进的生成性能。实验结果表明，在ImageNet数据集上，UCGM-T能够在20步内将多步模型的FID降低到1.30，而少步模型在仅2步内达到1.42的FID。此外，使用UCGM-S对预训练模型进行改进，FID从250步的1.26降至仅40步的1.06。'}}}, {'id': 'https://huggingface.co/papers/2505.06548', 'title': 'REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback', 'url': 'https://huggingface.co/papers/2505.06548', 'abstract': 'Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.', 'score': 26, 'issue_id': 3722, 'pub_date': '2025-05-10', 'pub_date_card': {'ru': '10 мая', 'en': 'May 10', 'zh': '5月10日'}, 'hash': 'db28335cad79db53', 'authors': ['Aniruddha Roy', 'Pretam Ray', 'Abhilash Nandy', 'Somak Aditya', 'Pawan Goyal'], 'affiliations': ['Indian Institute of Technology, Kharagpur'], 'pdf_title_img': 'assets/pdf/title_img/2505.06548.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#small_models', '#rl', '#dataset', '#data'], 'emoji': '🤖', 'ru': {'title': 'Малые модели и RL улучшают генерацию инструкций для обучения больших ЯМ', 'desc': 'Статья исследует эффективность малых открытых языковых моделей (LLaMA 2-7B, LLama 2-13B, Mistral 7B) для полуавтоматической генерации инструкций для обучения больших языковых моделей. Авторы применяют фреймворк, уменьшающий необходимость ручной разметки и затраты на создание датасета инструкций. Они также интегрируют алгоритм обучения с подкреплением (RL) для улучшения результатов. Эксперименты показывают, что предложенный подход с RL превосходит предыдущие методы в 63-66% задач.'}, 'en': {'title': 'Empowering LLMs with Cost-Effective Instruction Generation', 'desc': 'This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods.'}, 'zh': {'title': '高效生成指令数据，提升LLMs性能', 'desc': '这篇论文探讨了指令驱动的大型语言模型（LLMs）在自然语言处理任务中的应用。作者提出了一种半自动化框架，利用开源的小型LLMs（如LLaMA 2-7B、LLaMA 2-13B和Mistral 7B）来生成指令数据集，从而减少人工干预和成本。通过引入基于强化学习的训练算法，研究表明这种方法在63-66%的任务中显著提高了性能。该研究为生成多样化的指令数据提供了一种更高效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2505.07293', 'title': 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection', 'url': 'https://huggingface.co/papers/2505.07293', 'abstract': "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.", 'score': 17, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '7403ae602b400fc4', 'authors': ['Kai Hua', 'Steven Wu', 'Ge Zhang', 'Ke Shen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.07293.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#data', '#optimization', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Улучшение способности ЯМ к рассуждениям через умный отбор данных', 'desc': 'Статья представляет метод AttentionInfluence для отбора данных, улучшающих способности языковых моделей к сложным рассуждениям. Этот метод использует небольшую предобученную модель для выбора данных путем маскирования головок внимания без необходимости в дополнительном обучении или разметке. Авторы применили AttentionInfluence к корпусу SmolLM для предобучения 7B-параметровой модели. Результаты показали значительное улучшение на нескольких бенчмарках, требующих знаний и рассуждений.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with AttentionInfluence', 'desc': 'This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.'}, 'zh': {'title': '无监督推理数据选择的新方法', 'desc': '最近，研究者们越来越关注收集推理密集型的预训练数据，以提高大型语言模型（LLMs）的复杂推理能力。以往的方法通常依赖于监督分类器来识别这些数据，这需要人类或LLMs进行标注，常常引入领域特定的偏见。我们提出了一种名为AttentionInfluence的方法，这是一种简单而有效的无监督训练方法。通过简单的注意力头屏蔽操作，我们的方法使得一个小型的预训练语言模型能够作为强大的数据选择器，从而在推理任务中取得显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2505.07818', 'title': 'DanceGRPO: Unleashing GRPO on Visual Generation', 'url': 'https://huggingface.co/papers/2505.07818', 'abstract': 'Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.', 'score': 16, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '023078250e0d651f', 'authors': ['Zeyue Xue', 'Jie Wu', 'Yu Gao', 'Fangyuan Kong', 'Lingting Zhu', 'Mengzhao Chen', 'Zhiheng Liu', 'Wei Liu', 'Qiushan Guo', 'Weilin Huang', 'Ping Luo'], 'affiliations': ['ByteDance Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.07818.jpg', 'data': {'categories': ['#alignment', '#optimization', '#video', '#multimodal', '#rl', '#diffusion', '#benchmark', '#rlhf'], 'emoji': '🎨', 'ru': {'title': 'DanceGRPO: Революция в обучении с подкреплением для генерации визуального контента', 'desc': 'Статья представляет DanceGRPO - унифицированный фреймворк для адаптации Group Relative Policy Optimization (GRPO) к задачам генерации визуального контента. DanceGRPO совместим с различными генеративными парадигмами, задачами, фундаментальными моделями и моделями вознаграждения. Фреймворк демонстрирует значительные улучшения по сравнению с базовыми методами на нескольких бенчмарках. DanceGRPO стабилизирует оптимизацию политики для сложной генерации видео и позволяет генеративной политике лучше захватывать траектории шумоподавления.'}, 'en': {'title': 'DanceGRPO: Unifying Reinforcement Learning for Visual Generation', 'desc': 'This paper presents DanceGRPO, a novel framework that enhances visual content generation by integrating Group Relative Policy Optimization (GRPO) with generative models like diffusion models and rectified flows. It addresses key challenges in reinforcement learning (RL) for visual generation, such as instability during training and compatibility with modern sampling methods. DanceGRPO is versatile, supporting multiple tasks including text-to-image and video generation, and utilizes various foundational and reward models to improve output quality. The framework shows significant performance improvements over existing methods, making it a promising solution for aligning generative models with human preferences in visual synthesis.'}, 'zh': {'title': 'DanceGRPO：视觉生成的统一强化学习框架', 'desc': '本论文介绍了DanceGRPO，这是第一个将群体相对策略优化（GRPO）应用于视觉生成的统一框架。它解决了现有基于强化学习（RL）的方法在现代常微分方程（ODE）采样、训练稳定性和视频生成验证方面的局限性。DanceGRPO能够在扩散模型和修正流等多种生成范式中无缝适应，并在文本到图像、文本到视频和图像到视频等任务中表现出显著的性能提升。该框架在多个基准测试中超越了基线，展示了在视觉生成任务中结合强化学习与人类反馈的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.07263', 'title': 'Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2505.07263', 'abstract': 'We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.', 'score': 15, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9735af402c0df35f', 'authors': ['Xiaokun Wang', 'Chris', 'Jiangbo Pei', 'Wei Shen', 'Yi Peng', 'Yunzhuo Hao', 'Weijie Qiu', 'Ai Jian', 'Tianyidan Xie', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07263.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#alignment', '#open_source', '#training', '#architecture', '#dataset'], 'emoji': '🌟', 'ru': {'title': 'Универсальная мультимодальная модель вознаграждения для улучшения ИИ-рассуждений', 'desc': 'Исследователи представили Skywork-VL Reward - мультимодальную модель вознаграждения для задач понимания и рассуждения. Они создали большой набор данных с предпочтениями для различных мультимодальных сценариев. Архитектура модели основана на Qwen2.5-VL-7B-Instruct с добавлением головы вознаграждения и многоэтапной тонкой настройкой. Skywork-VL Reward показывает высокие результаты на бенчмарках и эффективна для обучения моделей с помощью Mixed Preference Optimization.'}, 'en': {'title': 'Skywork-VL Reward: Advancing Multimodal Understanding and Reasoning', 'desc': 'The paper introduces Skywork-VL Reward, a novel multimodal reward model designed to enhance understanding and reasoning across different types of data, such as text and images. It utilizes a large-scale dataset that captures diverse tasks, with feedback gathered from both traditional vision-language models and more sophisticated reasoning models. The architecture is built on Qwen2.5-VL-7B-Instruct, featuring a reward head and employing multi-stage fine-tuning with pairwise ranking loss to optimize performance. Experimental results demonstrate that this model not only excels in multimodal tasks but also improves training for Mixed Preference Optimization, marking a significant step forward in creating effective reward models for multimodal alignment.'}, 'zh': {'title': 'Skywork-VL Reward：多模态对齐的突破性进展', 'desc': '我们提出了Skywork-VL Reward，这是一种多模态奖励模型，旨在为多模态理解和推理任务提供奖励信号。我们的技术方法包括两个关键组成部分：首先，我们构建了一个大规模的多模态偏好数据集，涵盖了广泛的任务和场景，数据来自标准视觉-语言模型和先进的视觉-语言推理模型。其次，我们设计了一种基于Qwen2.5-VL-7B-Instruct的奖励模型架构，集成了奖励头，并在成对偏好数据上应用了多阶段微调。实验评估表明，Skywork-VL Reward在多模态VL-RewardBench上达到了最先进的结果，并在文本奖励基准上表现出竞争力。'}}}, {'id': 'https://huggingface.co/papers/2505.03733', 'title': 'WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch', 'url': 'https://huggingface.co/papers/2505.03733', 'abstract': "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.", 'score': 15, 'issue_id': 3722, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '1d5e56d00ea8d485', 'authors': ['Zimu Lu', 'Yunqiao Yang', 'Houxing Ren', 'Haotian Hou', 'Han Xiao', 'Ke Wang', 'Weikang Shi', 'Aojun Zhou', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Multimedia Laboratory (MMLab), The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.03733.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#agents', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'WebGen-Bench: Новый рубеж в оценке LLM-агентов для веб-разработки', 'desc': 'WebGen-Bench - это новый бенчмарк для оценки способности LLM-агентов создавать многофайловые веб-сайты с нуля. Он включает разнообразные инструкции по генерации сайтов, охватывающие почти все важные типы веб-приложений. Качество сгенерированных сайтов оценивается с помощью автоматизированных тестовых случаев, созданных GPT-4o и проверенных вручную. Лучшая комбинация - Bolt.diy с DeepSeek-R1 - достигает только 27,8% точности, что подчеркивает сложность бенчмарка.'}, 'en': {'title': 'Benchmarking LLMs for Website Code Generation', 'desc': 'This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models.'}, 'zh': {'title': '评估LLM代理生成网站代码的挑战', 'desc': '本文介绍了一种新的基准测试WebGen-Bench，旨在评估基于大型语言模型（LLM）的代理在从零开始创建多文件网站代码库的能力。该基准包含多样化的网站生成指令，涵盖了三大类和十三小类，几乎包括所有重要类型的Web应用程序。为了评估生成网站的质量，使用GPT-4o生成针对每个功能的测试用例，并手动过滤和调整，最终形成647个测试用例。通过强大的网页导航代理自动执行测试，评估生成网站的响应是否符合预期结果，结果显示最佳模型组合的准确率仅为27.8%，显示出基准的挑战性。'}}}, {'id': 'https://huggingface.co/papers/2505.07796', 'title': 'Learning Dynamics in Continual Pre-Training for Large Language Models', 'url': 'https://huggingface.co/papers/2505.07796', 'abstract': 'Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.', 'score': 12, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'c63d617be0b4d13a', 'authors': ['Xingjin Wang', 'Howe Tissue', 'Lu Wang', 'Linjing Li', 'Daniel Dajun Zeng'], 'affiliations': ['Ritzz-AI', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07796.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training'], 'emoji': '📈', 'ru': {'title': 'Раскрытие секретов непрерывного предобучения языковых моделей', 'desc': 'Статья исследует динамику обучения при непрерывном предобучении (CPT) больших языковых моделей. Авторы наблюдают, как меняется производительность модели на общих и специфических задачах на каждом шаге обучения. Они предлагают масштабируемый закон CPT, объединяющий эффекты смещения распределения и снижения скорости обучения. Этот подход позволяет прогнозировать потери на любом этапе обучения и настраивать гиперпараметры для различных целей CPT.'}, 'en': {'title': 'Unlocking the Dynamics of Continual Pre-Training', 'desc': "This paper investigates the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on how performance in general and specific domains changes during training. The authors analyze the CPT loss curve, revealing that it represents a transition between different performance states influenced by distribution shifts and learning rate adjustments. They propose a scaling law that predicts loss across various training steps and learning rate schedules, providing insights into key factors like peak learning rate and replay ratio. The findings are validated through extensive experiments, showing the law's applicability across different datasets and training configurations."}, 'zh': {'title': '持续预训练的动态与优化法则', 'desc': '持续预训练（CPT）是一种将强大的基础模型应用于特定下游任务的有效方法。本文探讨了大型语言模型在CPT过程中的学习动态，特别关注在每个训练步骤中，通用性能和下游领域性能的演变。我们观察到CPT损失曲线本质上描述了从一个曲线到另一个隐藏曲线的过渡，并通过解耦分布变化和学习率退火的影响来进行描述。我们推导出了一种CPT缩放法则，结合了这两个因素，使得能够预测在任何（持续）训练步骤和学习率调度下的损失。'}}}, {'id': 'https://huggingface.co/papers/2505.07596', 'title': 'Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent', 'url': 'https://huggingface.co/papers/2505.07596', 'abstract': 'Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.', 'score': 10, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '10063104a79da512', 'authors': ['Ziyang Huang', 'Xiaowei Yuan', 'Yiming Ju', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.07596.jpg', 'data': {'categories': ['#reasoning', '#rag', '#agents', '#optimization', '#hallucinations', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Умный поиск: когда искать, а когда довериться себе', 'desc': 'Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с использованием метода retrieval-augmented generation (RAG). Авторы предлагают агента IKEA, который способен эффективно определять границы собственных знаний и принимать решение о необходимости внешнего поиска информации. Для обучения агента используется функция вознаграждения, учитывающая границы знаний, и специально подготовленный набор данных. Эксперименты показывают, что IKEA превосходит базовые методы, значительно сокращает частоту обращений к внешним источникам и демонстрирует хорошую обобщающую способность.'}, 'en': {'title': 'Optimizing Knowledge Use in Language Models with IKEA', 'desc': 'This paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), a novel approach to enhance the performance of Large Language Models (LLMs) by optimizing their retrieval processes. IKEA intelligently determines when to use its internal knowledge versus when to perform external searches, reducing unnecessary retrievals and improving inference speed. The model employs a unique reward function that encourages effective use of internal knowledge while still allowing for external retrieval when needed. Evaluations show that IKEA not only outperforms existing methods but also generalizes well across various knowledge reasoning tasks.'}, 'zh': {'title': '智能检索，优化知识利用', 'desc': '本文介绍了一种新的强化学习模型，名为IKEA，旨在提高大型语言模型（LLMs）的检索能力。IKEA能够识别自身知识的边界，并优先使用内部知识，只有在内部知识不足时才会进行外部检索。通过引入一种新的奖励函数和训练数据集，IKEA能够有效减少冗余检索，提高回答的准确性。实验结果表明，IKEA在多个知识推理任务中表现优异，显著降低了检索频率，并展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.06176', 'title': "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", 'url': 'https://huggingface.co/papers/2505.06176', 'abstract': 'Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.', 'score': 7, 'issue_id': 3733, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': '884e34691df4b88b', 'authors': ['Niladri Shekhar Dutt', 'Duygu Ceylan', 'Niloy J. Mitra'], 'affiliations': ['Adobe Research, UK', 'University College London, Adobe Research, UK', 'University College London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.06176.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#dataset', '#data', '#multimodal', '#interpretability', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Интеллектуальная ретушь фотографий: MLLM на страже профессионального качества', 'desc': 'Статья представляет новый подход к ретуши фотографий с использованием мультимодальной большой языковой модели (MLLM). Модель обучается анализировать сырые фотографии, предлагать подходящие корректировки и реализовывать их с помощью заранее заданных процедурных операций обработки изображений. MLLM сначала обучается на специально разработанных визуальных головоломках для понимания операций обработки изображений, а затем на синтезированном наборе данных с рассуждениями экспертов. Предложенный метод сохраняет детали объектов и разрешение, обеспечивая понятные пользователю операции редактирования.'}, 'en': {'title': 'Empowering Photo Retouching with Intelligent Editing Guidance', 'desc': 'This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.'}, 'zh': {'title': '利用多模态模型提升照片修饰质量', 'desc': '本论文探讨了如何利用多模态大型语言模型（MLLM）来改进原始照片的修饰过程。我们训练MLLM理解图像处理操作，并通过解决视觉难题来增强其操作意识。该模型能够规划和建议编辑序列，确保修饰操作对用户可理解，并保留对象细节和分辨率。我们的实验结果表明，与现有的生成和程序化方法相比，该方法在可解释性和身份保留方面具有明显优势。'}}}, {'id': 'https://huggingface.co/papers/2505.07819', 'title': 'H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning', 'url': 'https://huggingface.co/papers/2505.07819', 'abstract': 'Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '9285a87dc24d7d07', 'authors': ['Yiyang Lu', 'Yufeng Tian', 'Zhecheng Yuan', 'Xianbang Wang', 'Pu Hua', 'Zhengrong Xue', 'Huazhe Xu'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute', 'Tsinghua University IIIS'], 'pdf_title_img': 'assets/pdf/title_img/2505.07819.jpg', 'data': {'categories': ['#diffusion', '#agents', '#robotics', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Иерархическое обучение для улучшения визуомоторного контроля роботов', 'desc': 'Статья представляет новый подход к обучению визуомоторной политики для робототехнической манипуляции - Триединую Иерархическую Диффузионную Политику (H^3DP). Этот метод использует трехуровневую иерархию: послойную организацию входных данных с учетом глубины, многомасштабное представление визуальных признаков и иерархически обусловленный процесс диффузии для генерации действий. H^3DP усиливает интеграцию между визуальным восприятием и предсказанием действий. Эксперименты показывают значительное улучшение производительности как в симуляции, так и в реальных задачах манипуляции.'}, 'en': {'title': 'Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration', 'desc': 'This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.'}, 'zh': {'title': '增强视觉与动作生成的三层次学习框架', 'desc': '本文提出了一种新的视觉运动策略学习框架，称为三层次扩散策略（H^{\textbf{3}DP）。该框架通过引入层次结构，增强了视觉特征与动作生成之间的结合。H^{3}DP包含三个层次：基于深度信息的输入分层、多尺度视觉表示和层次条件扩散过程。实验结果表明，H^{3}DP在44个仿真任务中相较于基线方法平均提高了27.5%的性能，并在四个复杂的双手真实世界操作任务中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.07260', 'title': 'UMoE: Unifying Attention and FFN with Shared Experts', 'url': 'https://huggingface.co/papers/2505.07260', 'abstract': 'Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '6a820b0cd1c07ac4', 'authors': ['Yuanhang Yang', 'Chaozheng Wang', 'Jing Li'], 'affiliations': ['Hong Kong Polytechnic University, Hong Kong, China', 'Institute of Science Tokyo, Tokyo, Japan', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07260.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Унификация MoE в Transformer: повышение эффективности через переосмысление внимания', 'desc': 'Статья представляет новый подход к архитектуре Разреженной смеси экспертов (Sparse Mixture of Experts, MoE) для масштабирования моделей Transformer. Авторы предлагают унифицированный дизайн MoE для слоев внимания и полносвязных слоев, переформулируя механизм внимания. Новая архитектура, названная UMoE, демонстрирует улучшенную производительность за счет MoE в слоях внимания. UMoE также позволяет эффективно разделять параметры между компонентами внимания и полносвязными слоями.'}, 'en': {'title': 'Unifying MoE for Enhanced Transformer Performance', 'desc': 'This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.'}, 'zh': {'title': '统一注意力与前馈网络的稀疏专家架构', 'desc': '稀疏专家混合（MoE）架构是一种有前景的方法，用于扩展Transformer模型。虽然早期的研究主要将MoE应用于前馈网络（FFN）层，但最近的研究开始探索将MoE扩展到注意力层，以提高模型性能。现有的基于注意力的MoE层需要专门的实现，并且与基于FFN的层相比，性能不尽如人意。本文提出了一种新颖的注意力机制重构，统一了注意力层和FFN层中的MoE设计，提出的UMoE架构通过基于注意力的MoE层实现了更优的性能，同时实现了FFN和注意力组件之间的高效参数共享。'}}}, {'id': 'https://huggingface.co/papers/2505.00612', 'title': 'Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation', 'url': 'https://huggingface.co/papers/2505.00612', 'abstract': 'In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\\em leakage} and {\\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.', 'score': 5, 'issue_id': 3733, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '98b0a79b86c5cd15', 'authors': ['D. Sculley', 'Will Cukierski', 'Phil Culliton', 'Sohier Dane', 'Maggie Demkin', 'Ryan Holbrook', 'Addison Howard', 'Paul Mooney', 'Walter Reade', 'Megan Risdal', 'Nate Keating'], 'affiliations': ['Kaggle, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.00612.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#evaluation'], 'emoji': '🏆', 'ru': {'title': 'Соревнования по ИИ - ключ к надежной оценке генеративных моделей', 'desc': 'Эта статья рассматривает кризис в эмпирической оценке генеративного ИИ, указывая на недостаточность традиционных методов оценки машинного обучения. Авторы отмечают проблемы, связанные с неограниченными входными и выходными пространствами, отсутствием четко определенной эталонной истины и сильными обратными связями в генеративных моделях. Особое внимание уделяется проблемам утечки и загрязнения данных как наиболее критичным для оценки генеративного ИИ. Статья предлагает рассматривать соревнования по ИИ как золотой стандарт для эмпирической строгости в оценке генеративного ИИ.'}, 'en': {'title': 'Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI', 'desc': 'This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.'}, 'zh': {'title': '生成性AI评估的新标准：人工智能竞赛的价值', 'desc': '在这篇论文中，我们观察到生成性人工智能的实证评估正面临危机，因为传统的机器学习评估和基准策略无法满足现代生成性AI模型和系统的评估需求。这些模型通常具有几乎无限的输入和输出空间，缺乏明确的真实目标，并且在预测时强烈依赖于之前模型输出的上下文。此外，论文指出，泄漏和污染问题是生成性AI评估中最重要且最难解决的挑战。我们认为，人工智能竞赛领域已经发展出有效的措施来应对泄漏问题，因此应将其视为生成性AI评估的黄金标准。'}}}, {'id': 'https://huggingface.co/papers/2505.07812', 'title': 'Continuous Visual Autoregressive Generation via Score Maximization', 'url': 'https://huggingface.co/papers/2505.07812', 'abstract': 'Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.', 'score': 3, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '5635f18df39cf275', 'authors': ['Chenze Shao', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07812.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#training', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Непрерывное визуальное авторегрессионное моделирование без квантования', 'desc': 'Статья представляет новый подход к визуальному авторегрессионному моделированию (VAR) для непрерывных данных без использования векторного квантования. Авторы предлагают фреймворк Continuous VAR, основанный на строго корректных правилах оценки (strictly proper scoring rules). Основное внимание уделяется целевым функциям на основе энергетической оценки (energy score), которая не требует вычисления правдоподобия. Этот метод позволяет преодолеть ограничения традиционных подходов VAR, связанные с потерей информации при дискретизации.'}, 'en': {'title': 'Revolutionizing Visual Data Generation with Continuous VAR', 'desc': 'This paper presents a new approach called Continuous Visual AutoRegressive (VAR) modeling, which allows for the generation of visual data without converting it into discrete formats. Traditional methods often lose important information during quantization, but this framework uses strictly proper scoring rules to evaluate and optimize generative models directly in continuous space. By focusing on energy scores as training objectives, the method avoids the challenges of probabilistic predictions in continuous domains. Additionally, it shows that previous continuous autoregressive methods can be derived from this new framework, enhancing their applicability and performance.'}, 'zh': {'title': '无量化的连续自回归生成新框架', 'desc': '传统观点认为自回归模型主要用于处理离散数据。在处理连续数据（如视觉数据）时，视觉自回归建模（VAR）通常需要通过量化方法将数据转换为离散空间，这可能导致信息损失。为了解决这个问题，我们提出了一种连续VAR框架，能够直接进行视觉自回归生成，而无需向量量化。该框架的理论基础是严格适当的评分规则，这为评估生成模型如何逼近真实分布提供了强大的统计工具。'}}}, {'id': 'https://huggingface.co/papers/2505.07793', 'title': 'Overflow Prevention Enhances Long-Context Recurrent LLMs', 'url': 'https://huggingface.co/papers/2505.07793', 'abstract': 'A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.', 'score': 3, 'issue_id': 3728, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'f4bfefd5343cbf0c', 'authors': ['Assaf Ben-Kish', 'Itamar Zimerman', 'M. Jehanzeb Mirza', 'James Glass', 'Leonid Karlinsky', 'Raja Giryes'], 'affiliations': ['IBM Research', 'MIT CSAIL', 'Tel Aviv University', 'Xero'], 'pdf_title_img': 'assets/pdf/title_img/2505.07793.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Чанки побеждают рекуррентность в обработке длинных контекстов', 'desc': 'Исследование фокусируется на рекуррентных суб-квадратичных моделях для обработки длинных контекстов в больших языковых моделях. Эксперименты показывают, что даже при обучении на длинных контекстах, модели недостаточно эффективно их используют. Предложенный метод обработки по чанкам, выбирающий наиболее релевантные части входных данных, значительно улучшает производительность ряда моделей на бенчмарке LongBench. Результаты ставят под сомнение способность рекуррентных моделей эффективно использовать зависимости в длинных контекстах.'}, 'en': {'title': 'Unlocking Long-Context Potential with Chunk-Based Inference', 'desc': 'This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.'}, 'zh': {'title': '提升长上下文处理效率的新方法', 'desc': '最近，长文本模型（LLMs）发展出了一种新的子二次模型，旨在提高长上下文处理的效率。我们研究了主要的长上下文模型，重点关注它们固定大小的递归记忆如何影响性能。实验表明，即使这些模型经过长上下文训练，它们对长上下文的利用仍然不足。我们提出的基于块的推理方法能够识别并处理输入中最相关的部分，从而有效缓解递归记忆的不足，并在多个长上下文任务中取得显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.06324', 'title': 'Document Attribution: Examining Citation Relationships using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.06324', 'abstract': "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.   To tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.", 'score': 3, 'issue_id': 3724, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': 'd1b4a407c1a67da8', 'authors': ['Vipula Rawte', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka'], 'affiliations': ['Adobe Inc.', 'Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.06324.jpg', 'data': {'categories': ['#training', '#interpretability', '#multimodal', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Повышение надежности атрибуции в LLM: от текстового включения до механизма внимания', 'desc': 'Эта статья посвящена проблеме атрибуции в больших языковых моделях (LLM) при работе с документами. Авторы предлагают два метода для повышения надежности цитирования: подход с нулевым обучением, основанный на текстовом включении, и использование механизма внимания. Первый метод, использующий flan-ul2, показал улучшение на 0,27% и 2,4% по сравнению с базовой линией на наборах данных AttributionBench. Второй метод, применяющий меньшую модель flan-t5-small, превзошел базовую линию по показателю F1 почти во всех слоях, кроме нескольких.'}, 'en': {'title': 'Enhancing Trust in LLMs through Improved Attribution Techniques', 'desc': 'This paper addresses the challenges of trustworthiness and interpretability in Large Language Models (LLMs) when used for document-based tasks. It introduces two techniques for improving attribution, which is the process of linking model outputs back to their source documents. The first technique is a zero-shot approach that treats attribution as a textual entailment task, showing measurable improvements in performance. The second technique investigates how the attention mechanism in LLMs can enhance attribution accuracy, achieving better F1 scores in most layers of a smaller model.'}, 'zh': {'title': '提升大型语言模型的可信性与可解释性', 'desc': '随着大型语言模型（LLMs）在文档摘要、问答和信息提取等任务中的应用日益增多，确保这些系统的可信性和可解释性变得至关重要。本文提出了一种归因方法，通过追踪生成的输出回到其源文档来解决这一挑战。我们提出了两种技术：一种是零样本方法，将归因视为简单的文本蕴含任务，另一种是探索注意力机制在增强归因过程中的作用。实验结果表明，我们的方法在多个基准测试中均有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.07291', 'title': 'INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.07291', 'abstract': 'We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training.', 'score': 2, 'issue_id': 3742, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'bf4c666ff6a739cf', 'authors': ['Prime Intellect Team', 'Sami Jaghouar', 'Justus Mattern', 'Jack Min Ong', 'Jannik Straube', 'Manveer Basra', 'Aaron Pazdera', 'Kushal Thaman', 'Matthew Di Ferrante', 'Felix Gabriel', 'Fares Obeid', 'Kemal Erdem', 'Michael Keiblinger', 'Johannes Hagemann'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07291.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#rl', '#dataset', '#reasoning'], 'emoji': '🌐', 'ru': {'title': 'Глобальное распределенное обучение с подкреплением для крупномасштабных языковых моделей', 'desc': 'INTELLECT-2 представляет собой первую глобально распределенную систему обучения с подкреплением для языковой модели с 32 миллиардами параметров. В отличие от традиционного централизованного подхода, INTELLECT-2 использует полностью асинхронное обучение с подкреплением на динамичном, гетерогенном рое вычислительных узлов. Для реализации этой уникальной инфраструктуры были разработаны специальные компоненты, включая фреймворк PRIME-RL, систему верификации TOPLOC и механизм эффективного распространения весов модели SHARDCAST. Авторы также предложили модификации стандартного алгоритма GRPO и методы фильтрации данных для достижения стабильности обучения.'}, 'en': {'title': 'Revolutionizing Reinforcement Learning with Distributed Training', 'desc': 'INTELLECT-2 is a groundbreaking reinforcement learning model that utilizes a globally distributed training approach for a 32 billion parameter language model. It employs asynchronous training across a diverse group of independent compute contributors, which is a shift from traditional centralized methods. The paper introduces innovative components like PRIME-RL for managing distributed training, TOPLOC for verifying data from untrusted sources, and SHARDCAST for efficient communication of model updates. By refining the GRPO training method and implementing effective data filtering, INTELLECT-2 achieves enhanced stability and performance, surpassing previous models in its category.'}, 'zh': {'title': 'INTELLECT-2：全球分布式强化学习的创新之路', 'desc': '我们介绍了INTELLECT-2，这是第一个全球分布式的强化学习训练，使用了320亿参数的语言模型。与传统的集中式训练不同，INTELLECT-2通过一个动态的、异构的计算贡献者群体，采用完全异步的强化学习来训练推理模型。为了支持这种独特的基础设施，我们从头开始构建了多个组件，包括专为分布式异步强化学习设计的PRIME-RL训练框架，以及验证不可信推理工作者的TOPLOC和高效广播策略权重的SHARDCAST。我们还对标准的GRPO训练方法和数据过滤技术进行了修改，以确保训练的稳定性，并成功实现模型的训练目标，从而在320亿参数范围内超越了现有的QwQ-32B推理模型。'}}}, {'id': 'https://huggingface.co/papers/2505.07233', 'title': 'DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2505.07233', 'abstract': 'Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG', 'score': 2, 'issue_id': 3734, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'b6c55b4c738d5230', 'authors': ['Jiashuo Sun', 'Xianrui Zhong', 'Sizhe Zhou', 'Jiawei Han'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07233.jpg', 'data': {'categories': ['#rag', '#optimization', '#interpretability', '#rl'], 'emoji': '🔄', 'ru': {'title': 'Динамическая оптимизация извлечения знаний для генеративных ИИ-систем', 'desc': 'DynamicRAG - это новая система генерации с извлечением информации (RAG), использующая динамический ранжировщик документов. Ранжировщик оптимизируется с помощью обучения с подкреплением, используя качество ответов языковой модели в качестве сигнала обратной связи. Система автоматически определяет оптимальное количество и порядок документов для каждого запроса. DynamicRAG показала наилучшие результаты на семи наборах данных, требующих обширных знаний.'}, 'en': {'title': 'DynamicRAG: Optimizing Document Retrieval for Better Generation', 'desc': "This paper introduces DynamicRAG, a new framework for retrieval-augmented generation (RAG) systems that enhances the quality of generated responses by optimizing the reranking of retrieved documents. The reranker in DynamicRAG is designed to dynamically adjust both the order and the number of documents based on the specific query, addressing the challenge of selecting the optimal number of documents. By employing reinforcement learning, the reranker uses feedback from the quality of the language model's output to improve its decisions. The results show that DynamicRAG outperforms existing methods across multiple knowledge-intensive datasets, achieving state-of-the-art performance."}, 'zh': {'title': '动态调整，提升生成质量的RAG框架', 'desc': '检索增强生成（RAG）系统结合了大型语言模型（LLM）和外部知识检索，适用于知识密集型任务。本文提出了一种新的RAG框架DynamicRAG，其中的重排序器能够根据查询动态调整检索文档的顺序和数量。我们将重排序器建模为一个通过强化学习（RL）优化的智能体，利用LLM输出质量作为奖励来优化重排序决策。实验结果表明，DynamicRAG在七个知识密集型数据集上表现优异，达到了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2505.04918', 'title': 'Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction', 'url': 'https://huggingface.co/papers/2505.04918', 'abstract': "Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.", 'score': 2, 'issue_id': 3728, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '73efdd4a4328c88d', 'authors': ['Jiaqi Zheng', 'Qing Ling', 'Yerong Feng'], 'affiliations': ['Shenzhen Institute of Meteorological Innovation', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04918.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#graphs', '#architecture', '#training'], 'emoji': '🌎', 'ru': {'title': 'PASSAT: Физически обоснованный прогноз погоды с учетом топологии Земли', 'desc': 'PASSAT - это новая модель глубокого обучения для прогнозирования погоды, учитывающая физику атмосферных процессов и топологию поверхности Земли. Модель решает уравнения адвекции и Навье-Стокса на сферическом многообразии, используя сферическую графовую нейронную сеть для моделирования взаимодействия Земли и атмосферы. PASSAT генерирует начальные поля скоростей, критически важные для решения уравнения адвекции. В экспериментах на данных ERA5 с разрешением 5.625° PASSAT превзошла как современные модели глубокого обучения, так и операционную модель численного прогноза погоды IFS T42.'}, 'en': {'title': 'PASSAT: Bridging Physics and Topology for Superior Weather Prediction', 'desc': "The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth's surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns."}, 'zh': {'title': 'PASSAT：结合物理与拓扑的天气预测新模型', 'desc': '本文提出了一种新型的天气预测模型PASSAT，该模型结合了物理学和地形信息。PASSAT通过对流过程和地球-大气相互作用来描述天气演变，并考虑了地球表面的拓扑结构。该模型在球面流形上数值求解对流方程和纳维-斯托克斯方程，并利用球面图神经网络捕捉地球-大气的相互作用。实验结果表明，PASSAT在5.625度分辨率的ERA5数据集上优于现有的深度学习天气预测模型和操作性数值天气预测模型IFS T42。'}}}, {'id': 'https://huggingface.co/papers/2505.04066', 'title': 'LLAMAPIE: Proactive In-Ear Conversation Assistants', 'url': 'https://huggingface.co/papers/2505.04066', 'abstract': 'We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.', 'score': 1, 'issue_id': 3739, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': 'f8bf204612751793', 'authors': ['Tuochao Chen', 'Nicholas Batchelder', 'Alisa Liu', 'Noah Smith', 'Shyamnath Gollakota'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.04066.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#dataset', '#data'], 'emoji': '🎧', 'ru': {'title': 'LlamaPIE: Незаметный помощник для живых разговоров', 'desc': 'LlamaPIE - это первый в реальном времени проактивный ассистент, разработанный для улучшения человеческих разговоров через незаметные, краткие подсказки, передаваемые через слуховые устройства. В отличие от традиционных языковых моделей, этот ассистент работает в фоновом режиме, предугадывая потребности пользователя без прерывания разговоров. Авторы решают несколько задач, включая определение момента для ответа, создание кратких ответов и обработку в реальном времени на устройстве. Оценка подхода на реальных данных и пользовательские исследования показывают эффективность LlamaPIE в предоставлении полезной, ненавязчивой помощи.'}, 'en': {'title': 'LlamaPIE: Enhancing Conversations with Proactive Assistance', 'desc': 'LlamaPIE is a novel real-time proactive assistant that enhances human conversations by providing discreet guidance through hearable devices. Unlike traditional models that wait for user prompts, LlamaPIE anticipates user needs and offers assistance without interrupting the flow of conversation. The system tackles challenges such as timing for responses, generating concise and relevant replies, and utilizing user context for personalized support, all while processing information on-device. Evaluations show that users prefer LlamaPIE over reactive models, indicating its effectiveness in improving live interactions.'}, 'zh': {'title': 'LlamaPIE：提升对话的主动助手', 'desc': 'LlamaPIE 是首个实时主动助手，旨在通过可穿戴设备在对话中提供简洁的指导。与传统语言模型不同，它在后台运行，能够预测用户需求而不打断对话。我们解决了多个挑战，包括何时响应、如何生成简洁的回应以及如何利用用户知识提供上下文感知的帮助。通过构建半合成对话数据集和提出双模型管道，我们在真实数据集上评估了该方法的有效性，用户研究显示出对主动助手的强烈偏好。'}}}, {'id': 'https://huggingface.co/papers/2505.07086', 'title': 'Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design', 'url': 'https://huggingface.co/papers/2505.07086', 'abstract': "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.", 'score': 0, 'issue_id': 3725, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 мая', 'en': 'May 11', 'zh': '5月11日'}, 'hash': 'a2fce171208a1e7a', 'authors': ['Tong Chen', 'Yinuo Zhang', 'Sophia Tang', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer Science, Fudan University', 'Management and Technology Program, University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2505.07086.jpg', 'data': {'categories': ['#training', '#dataset', '#science', '#data', '#optimization'], 'emoji': '🧬', 'ru': {'title': 'Многоцелевая оптимизация биологических последовательностей с помощью дискретного сопоставления потоков', 'desc': 'Статья представляет новый метод машинного обучения под названием Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) для проектирования биологических последовательностей с множественными целевыми функциями. MOG-DFM использует предобученные генеративные модели на основе дискретного сопоставления потоков и направляет их к Парето-эффективным компромиссам между несколькими скалярными целями. Авторы демонстрируют эффективность метода на примерах генерации пептидов с оптимизированными свойствами и проектирования последовательностей ДНК с заданными характеристиками. MOG-DFM показывает себя мощным инструментом для многоцелевого дизайна биомолекулярных последовательностей.'}, 'en': {'title': 'Optimizing Biomolecule Design with MOG-DFM', 'desc': "This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering."}, 'zh': {'title': '多目标优化，助力生物分子设计', 'desc': '在生物分子工程中，设计满足多种功能和生物物理标准的生物序列仍然是一个重要挑战。本文提出了一种名为多目标引导离散流匹配（MOG-DFM）的框架，能够在多个标量目标之间实现帕累托有效的权衡。MOG-DFM通过计算混合排名方向分数和应用自适应超锥过滤器，来引导预训练的离散时间流匹配生成器进行多目标优化。我们展示了MOG-DFM在生成优化的肽结合物和特定增强子类DNA序列方面的有效性，证明了其在多属性引导的生物分子序列设计中的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2505.04921', 'title': 'Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2505.04921', 'abstract': "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.", 'score': 102, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'a07d92b81581eea3', 'authors': ['Yunxin Li', 'Zhenyu Liu', 'Zitao Li', 'Xuanyu Zhang', 'Zhenran Xu', 'Xinyu Chen', 'Haoyuan Shi', 'Shenyuan Jiang', 'Xintong Wang', 'Jifang Wang', 'Shouzheng Huang', 'Xinping Zhao', 'Borui Jiang', 'Lanqing Hong', 'Longyue Wang', 'Zhuotao Tian', 'Baoxing Huai', 'Wenhan Luo', 'Weihua Luo', 'Zheng Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.04921.jpg', 'data': {'categories': ['#rl', '#multimodal', '#benchmark', '#survey', '#agents', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'От модульных систем к унифицированным мультимодальным моделям рассуждений', 'desc': 'Статья представляет обзор исследований в области мультимодальных моделей рассуждений (LMRMs). Авторы описывают эволюцию подходов от модульных систем к унифицированным языковым фреймворкам, обеспечивающим более согласованное межмодальное понимание. Рассматриваются достижения, такие как мультимодальная цепочка рассуждений (MCoT) и мультимодальное обучение с подкреплением. Статья завершается обсуждением концепции нативных больших мультимодальных моделей рассуждений (N-LMRMs) для масштабируемого и адаптивного рассуждения в сложных реальных средах.'}, 'en': {'title': 'Empowering AI with Multimodal Reasoning for Real-World Intelligence', 'desc': 'This paper discusses the importance of reasoning in artificial intelligence, especially in complex environments with multiple types of data like text, images, and audio. It introduces Large Multimodal Reasoning Models (LMRMs) that combine these different data types to enhance reasoning capabilities. The authors provide a structured survey of the evolution of multimodal reasoning, from early task-specific models to more integrated language-centric frameworks. They also highlight ongoing challenges in generalization and reasoning depth, while proposing a roadmap for future research in developing more adaptive and intelligent systems.'}, 'zh': {'title': '多模态推理：智能的未来', 'desc': '推理是智能的核心，影响决策、结论和跨领域的概括能力。在人工智能中，随着系统在开放、不确定和多模态环境中运行，推理变得至关重要，以实现稳健和适应性的行为。大型多模态推理模型（LMRMs）整合文本、图像、音频和视频等多种模态，支持复杂的推理能力，旨在实现全面感知、精确理解和深度推理。本文对多模态推理研究进行了全面的调查，提出了一个四阶段的发展路线图，反映了该领域设计理念的变化和新兴能力。'}}}, {'id': 'https://huggingface.co/papers/2505.04620', 'title': 'On Path to Multimodal Generalist: General-Level and General-Bench', 'url': 'https://huggingface.co/papers/2505.04620', 'abstract': 'The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/', 'score': 62, 'issue_id': 3671, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '57991e528141671e', 'authors': ['Hao Fei', 'Yuan Zhou', 'Juncheng Li', 'Xiangtai Li', 'Qingshan Xu', 'Bobo Li', 'Shengqiong Wu', 'Yaoting Wang', 'Junbao Zhou', 'Jiahao Meng', 'Qingyu Shi', 'Zhiyuan Zhou', 'Liangtao Shi', 'Minghe Gao', 'Daoan Zhang', 'Zhiqi Ge', 'Weiming Wu', 'Siliang Tang', 'Kaihang Pan', 'Yaobo Ye', 'Haobo Yuan', 'Tao Zhang', 'Tianjie Ju', 'Zixiang Meng', 'Shilin Xu', 'Liyu Jia', 'Wentao Hu', 'Meng Luo', 'Jiebo Luo', 'Tat-Seng Chua', 'Shuicheng Yan', 'Hanwang Zhang'], 'affiliations': ['HFUT', 'KAUST', 'NJU', 'NTU', 'NUS', 'PKU', 'SJTU', 'UR', 'WHU', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2505.04620.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agi'], 'emoji': '🧠', 'ru': {'title': 'Новый подход к оценке мультимодальных ИИ-систем на пути к AGI', 'desc': 'Статья описывает новую систему оценки мультимодальных больших языковых моделей (MLLM) под названием General-Level. Эта система определяет 5 уровней производительности и обобщаемости MLLM, предлагая методологию для сравнения моделей и оценки прогресса существующих систем. Ключевым понятием в этой системе является концепция Синергии, которая измеряет согласованность возможностей моделей в понимании и генерации контента, а также в работе с различными модальностями. Для поддержки этой системы оценки авторы представляют General-Bench - набор из более чем 700 задач и 325 800 примеров, охватывающий широкий спектр навыков, модальностей и форматов.'}, 'en': {'title': 'Towards Multimodal Generalists: Evaluating MLLM Progress', 'desc': 'The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI).'}, 'zh': {'title': '迈向真正的多模态通用人工智能', 'desc': '多模态大型语言模型（MLLM）正在快速发展，得益于大型语言模型（LLM）的先进能力。现有的MLLM正朝着多模态通用主义者的方向演变，不仅能够理解多种模态，还能在不同模态之间生成内容。本文提出了一种新的评估框架——General-Level，定义了MLLM性能和通用性的五个等级，以便比较不同模型的能力。通过General-Bench，我们提供了一个更广泛的技能和任务评估，揭示了当前多模态通用模型在实现真正人工智能方面的挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.05470', 'title': 'Flow-GRPO: Training Flow Matching Models via Online RL', 'url': 'https://huggingface.co/papers/2505.05470', 'abstract': "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.", 'score': 44, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '8db85b6df75e7479', 'authors': ['Jie Liu', 'Gongye Liu', 'Jiajun Liang', 'Yangguang Li', 'Jiaheng Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wanli Ouyang'], 'affiliations': ['CUHK MMLab', 'Kuaishou Technology', 'Nanjing University', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05470.jpg', 'data': {'categories': ['#rl', '#cv', '#rlhf', '#alignment', '#games', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Революция в генерации изображений: RL встречает потоковые модели', 'desc': 'Flow-GRPO – это новый метод, объединяющий онлайн-обучение с подкреплением (RL) и модели согласования потоков. Он использует преобразование ОДУ в СДУ для статистической выборки и стратегию уменьшения шума для повышения эффективности обучения. Метод показывает значительные улучшения в задачах генерации изображений по тексту, особенно в сложных композициях и визуальном рендеринге текста. Flow-GRPO также демонстрирует существенный прогресс в согласовании с предпочтениями человека без ухудшения качества или разнообразия изображений.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Flow-GRPO', 'desc': 'Flow-GRPO is a novel method that combines online reinforcement learning with flow matching models to enhance performance in text-to-image tasks. It employs an ODE-to-SDE conversion to facilitate statistical sampling, allowing for better exploration in reinforcement learning. Additionally, the Denoising Reduction strategy optimizes training efficiency by minimizing unnecessary denoising steps while maintaining inference quality. The results show significant improvements in accuracy and human preference alignment, with minimal risk of reward hacking, ensuring high-quality and diverse outputs.'}, 'zh': {'title': 'Flow-GRPO：强化学习与流匹配的完美结合', 'desc': '我们提出了Flow-GRPO，这是第一个将在线强化学习（RL）集成到流匹配模型中的方法。该方法采用了两个关键策略：首先，通过将确定性常微分方程（ODE）转换为等效的随机微分方程（SDE），实现了在所有时间步长上与原始模型的边际分布匹配，从而支持RL探索的统计采样；其次，采用去噪减少策略，在保持原始推理时间步数的同时减少训练去噪步骤，显著提高了采样效率而不降低性能。实验证明，Flow-GRPO在多个文本到图像的任务中表现出色，尤其在复杂组合中，RL调优的SD3.5几乎完美地生成了对象数量、空间关系和细粒度属性，显著提高了GenEval的准确率。'}}}, {'id': 'https://huggingface.co/papers/2505.02847', 'title': 'Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.02847', 'abstract': "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.", 'score': 22, 'issue_id': 3671, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 мая', 'en': 'May 1', 'zh': '5月1日'}, 'hash': '9204f0ca97eb8bc7', 'authors': ['Bang Zhang', 'Ruotian Ma', 'Qingxuan Jiang', 'Peisong Wang', 'Jiaqi Chen', 'Zheng Xie', 'Xingyu Chen', 'Yue Wang', 'Fanghua Ye', 'Jian Li', 'Yifan Yang', 'Zhaopeng Tu', 'Xiaolong Li'], 'affiliations': ['Hunyuan AI Digital Human, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.02847.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#alignment', '#agents', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'SAGE: Измерение эмпатии и социального интеллекта языковых моделей', 'desc': "Статья представляет SAGE - новую систему оценки способности больших языковых моделей (LLM) понимать человека, а не просто текст. SAGE использует 'Разумного Агента', который симулирует человеческие эмоции и мысли во время взаимодействия, обеспечивая более реалистичную оценку тестируемой модели в многоходовых диалогах. Эксперименты показали, что эмоциональные оценки SAGE коррелируют с психологическими метриками. Система также используется для создания публичного рейтинга LLM, выявляющего значительные различия между передовыми и базовыми моделями."}, 'en': {'title': 'Measuring Empathy in AI: The SAGE Framework', 'desc': 'The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems.'}, 'zh': {'title': '评估语言模型的情感理解能力', 'desc': '本文介绍了一种名为SAGE的自动评估框架，用于测量大型语言模型（LLM）对人类情感和社交认知的理解能力。SAGE通过模拟人类情感变化和内心想法，提供了更真实的多轮对话评估。实验结果表明，SAGE的情感评分与心理学评估工具的评分高度相关，验证了其心理学的真实性。该框架还建立了一个公开的Sentient排行榜，揭示了不同模型之间的显著差距，推动了对更具同理心和社交能力的语言代理的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.05315', 'title': 'Scalable Chain of Thoughts via Elastic Reasoning', 'url': 'https://huggingface.co/papers/2505.05315', 'abstract': 'Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.', 'score': 18, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '0ce3be6057da3ed2', 'authors': ['Yuhui Xu', 'Hanze Dong', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Caiming Xiong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05315.jpg', 'data': {'categories': ['#plp', '#training', '#reasoning', '#optimization', '#benchmark', '#math'], 'emoji': '🧠', 'ru': {'title': 'Эластичное рассуждение: эффективные цепочки мысли в условиях ограниченных ресурсов', 'desc': "Эта статья представляет новый подход под названием 'Эластичное рассуждение' для крупных моделей рассуждения (LRM). Метод разделяет процесс рассуждения на две фазы - мышление и решение - с независимыми бюджетами, что позволяет эффективно работать в условиях ограниченных ресурсов. Авторы предлагают стратегию обучения с ограниченным бюджетом, интегрированную в GRPO, которая учит модель адаптивно рассуждать при сокращении процесса мышления. Эксперименты на математических и программистских задачах показывают, что 'Эластичное рассуждение' работает надежно при строгих ограничениях бюджета и производит более краткие и эффективные рассуждения даже в неограниченных условиях."}, 'en': {'title': 'Elastic Reasoning: Scalable and Efficient Thought Processes for ML Models', 'desc': 'This paper introduces Elastic Reasoning, a framework designed to enhance the performance of large reasoning models (LRMs) under strict resource constraints. It separates the reasoning process into two distinct phases: thinking and solution, allowing for independent budget allocation for each phase. The framework employs a budget-constrained rollout strategy that helps models adaptively reason even when the thinking phase is limited, ensuring reliability in various scenarios. Empirical results show that Elastic Reasoning not only meets budget constraints effectively but also reduces training costs while improving the efficiency of reasoning outputs.'}, 'zh': {'title': '弹性推理：可控推理的新解决方案', 'desc': '大型推理模型（LRMs）在复杂任务上取得了显著进展，但其输出长度不受控制，给实际应用带来了挑战。我们提出了一种名为弹性推理的新框架，将推理过程分为思考和解决两个阶段，并为每个阶段分配独立的预算。在测试时，弹性推理优先考虑解决方案的完整性，从而在资源紧张的情况下显著提高可靠性。我们的实验证明，弹性推理在严格的预算限制下表现出色，同时训练成本显著低于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.05469', 'title': 'Generating Physically Stable and Buildable LEGO Designs from Text', 'url': 'https://huggingface.co/papers/2505.05469', 'abstract': 'We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.', 'score': 17, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '20752e033ce6b40a', 'authors': ['Ava Pun', 'Kangle Deng', 'Ruixuan Liu', 'Deva Ramanan', 'Changliu Liu', 'Jun-Yan Zhu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05469.jpg', 'data': {'categories': ['#robotics', '#dataset', '#multimodal', '#3d', '#open_source'], 'emoji': '🧱', 'ru': {'title': 'LegoGPT: от текста к физически стабильным моделям LEGO', 'desc': 'LegoGPT - это новый подход к генерации физически стабильных моделей из кубиков LEGO на основе текстовых запросов. Авторы создали большой набор данных физически стабильных конструкций LEGO с подписями и обучили авторегрессионную языковую модель предсказывать следующий кубик для добавления. Для улучшения стабильности конструкций применяется эффективная проверка валидности и откат с учетом физики во время авторегрессивного вывода. Эксперименты показывают, что LegoGPT создает стабильные, разнообразные и эстетичные конструкции LEGO, соответствующие текстовым запросам.'}, 'en': {'title': 'Building LEGO Dreams with AI!', 'desc': 'LegoGPT is a novel machine learning model designed to generate stable LEGO brick structures from textual descriptions. It utilizes a large dataset of LEGO designs paired with captions to train an autoregressive language model that predicts the next brick to add based on the input prompt. To ensure the generated designs are physically stable, the model incorporates a validity check and physics-aware rollback mechanism during the generation process. The results demonstrate that LegoGPT can create diverse and visually appealing LEGO models that can be assembled by both humans and robotic systems.'}, 'zh': {'title': '乐高设计的智能生成与稳定性保障', 'desc': '我们介绍了LegoGPT，这是第一个从文本提示生成物理稳定的乐高砖模型的方法。为了实现这一目标，我们构建了一个大规模的物理稳定乐高设计数据集，并训练了一个自回归的大型语言模型来预测下一个要添加的砖块。为了提高设计的稳定性，我们在自回归推理过程中采用了有效的有效性检查和物理感知回滚，利用物理法则和组装约束来修剪不可行的预测。我们的实验表明，LegoGPT生成的乐高设计稳定、多样且美观，与输入的文本提示紧密对齐。'}}}, {'id': 'https://huggingface.co/papers/2505.05071', 'title': 'FG-CLIP: Fine-Grained Visual and Textual Alignment', 'url': 'https://huggingface.co/papers/2505.05071', 'abstract': "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.", 'score': 15, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '4251cc9ddf64d2b8', 'authors': ['Chunyu Xie', 'Bin Wang', 'Fanjing Kong', 'Jincheng Li', 'Dawei Liang', 'Gengshen Zhang', 'Dawei Leng', 'Yuhui Yin'], 'affiliations': ['360 AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05071.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#multimodal', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'FG-CLIP: Точное понимание изображений на новом уровне', 'desc': 'Статья представляет Fine-Grained CLIP (FG-CLIP) - улучшенную версию модели CLIP для более детального понимания изображений. FG-CLIP использует 1,6 миллиарда пар изображение-текст с длинными подписями для захвата семантических деталей. Модель обучается на наборе данных из 12 миллионов изображений с 40 миллионами ограничивающих рамок и детальными подписями. FG-CLIP превосходит оригинальный CLIP и другие современные методы в различных задачах, включая детальное понимание изображений и мультимодальные бенчмарки.'}, 'en': {'title': 'Unlocking Fine-Grained Understanding with FG-CLIP', 'desc': 'The paper introduces Fine-Grained CLIP (FG-CLIP), an enhancement of the original CLIP model aimed at improving fine-grained understanding in multimodal tasks. FG-CLIP achieves this by generating a massive dataset of 1.6 billion long caption-image pairs, which helps capture detailed semantic information. Additionally, it constructs a high-quality dataset with 12 million images and 40 million bounding boxes, ensuring that the model learns from context-rich representations. By incorporating 10 million hard negative samples, FG-CLIP enhances its ability to differentiate subtle semantic differences, leading to superior performance in various tasks compared to the original CLIP and other leading models.'}, 'zh': {'title': '细粒度理解的新突破：FG-CLIP', 'desc': '对比语言-图像预训练（CLIP）在多模态任务中表现出色，但在细粒度理解方面存在困难。为了解决这个问题，我们提出了细粒度CLIP（FG-CLIP），通过三项关键创新来增强细粒度理解。首先，我们利用大型多模态模型生成16亿对长标题-图像对，以捕捉全局语义细节。其次，构建了一个高质量的数据集，包含1200万张图像和4000万个区域特定的边界框，确保精确且丰富的上下文表示。'}}}, {'id': 'https://huggingface.co/papers/2505.05467', 'title': 'StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant', 'url': 'https://huggingface.co/papers/2505.05467', 'abstract': 'We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.', 'score': 13, 'issue_id': 3674, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'c876dfdb1d290930', 'authors': ['Haibo Wang', 'Bo Feng', 'Zhengfeng Lai', 'Mingze Xu', 'Shiyu Li', 'Weifeng Ge', 'Afshin Dehghan', 'Meng Cao', 'Ping Huang'], 'affiliations': ['Apple', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05467.jpg', 'data': {'categories': ['#benchmark', '#video', '#dataset', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'StreamBridge: Революция в потоковом понимании видео', 'desc': 'StreamBridge - это фреймворк, который превращает офлайн-модели Video-LLM в потоковые. Он решает проблемы многоэтапного понимания в реальном времени и отсутствия механизмов проактивного реагирования. Фреймворк использует буфер памяти с компрессией и легковесную модель активации для интеграции с существующими Video-LLM. Для поддержки StreamBridge создан датасет Stream-IT, предназначенный для обучения потокового понимания видео.'}, 'en': {'title': 'Transforming Video-LLMs for Real-Time Streaming Success', 'desc': 'StreamBridge is a framework designed to enhance offline Video-LLMs for real-time streaming applications. It tackles two main issues: the need for effective multi-turn interactions and the ability to provide proactive responses. By using a memory buffer with round-decayed compression, it allows models to handle longer contexts in conversations. Additionally, StreamBridge introduces a lightweight activation model that integrates easily with existing Video-LLMs, and it is supported by the Stream-IT dataset, which is specifically created for streaming video understanding tasks.'}, 'zh': {'title': 'StreamBridge：流媒体视频理解的新突破', 'desc': 'StreamBridge是一个简单而有效的框架，可以将离线视频大语言模型（Video-LLMs）转变为支持流媒体的模型。它解决了在在线场景中适应现有模型的两个基本挑战：多轮实时理解能力有限和缺乏主动响应机制。具体来说，StreamBridge结合了内存缓冲区和逐轮衰减压缩策略，支持长上下文的多轮交互，并且采用了轻量级的解耦激活模型，能够轻松集成到现有的视频大语言模型中，实现持续的主动响应。此外，我们构建了Stream-IT，这是一个针对流媒体视频理解的大规模数据集，包含交错的视频-文本序列和多样的指令格式。'}}}, {'id': 'https://huggingface.co/papers/2505.05474', 'title': '3D Scene Generation: A Survey', 'url': 'https://huggingface.co/papers/2505.05474', 'abstract': '3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.', 'score': 12, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '06bda1a6228b8f26', 'authors': ['Beichen Wen', 'Haozhe Xie', 'Zhaoxi Chen', 'Fangzhou Hong', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore 637335'], 'pdf_title_img': 'assets/pdf/title_img/2505.05474.jpg', 'data': {'categories': ['#3d', '#robotics', '#multimodal', '#synthetic', '#survey'], 'emoji': '🌐', 'ru': {'title': 'Новые горизонты в генерации трехмерных сцен: от процедурных методов к нейронным сетям', 'desc': 'Эта статья представляет собой обзор современных методов генерации трехмерных сцен. Авторы систематизируют подходы по четырем парадигмам: процедурная генерация, нейронная генерация на основе 3D, генерация на основе изображений и генерация на основе видео. В работе анализируются технические основы, компромиссы и репрезентативные результаты каждого подхода, а также рассматриваются наборы данных, протоколы оценки и прикладные задачи. Статья завершается обсуждением ключевых проблем и перспективных направлений в области генерации 3D-сцен, включая повышение точности, физически корректную и интерактивную генерацию.'}, 'en': {'title': 'Advancing 3D Scene Generation with Deep Learning', 'desc': 'This paper reviews the latest techniques in 3D scene generation, which aims to create realistic and meaningful environments for various applications. It highlights the evolution from early procedural methods to modern deep generative models like GANs and diffusion models, which enhance the quality and diversity of generated scenes. The authors categorize these methods into four main paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation, analyzing their strengths and weaknesses. The paper also discusses ongoing challenges and future directions in the field, such as improving fidelity and integrating physics into scene generation.'}, 'zh': {'title': '3D场景生成的未来方向与挑战', 'desc': '3D场景生成旨在合成具有空间结构、语义意义和照片真实感的环境，广泛应用于沉浸式媒体、机器人、自动驾驶和具身人工智能等领域。早期基于程序规则的方法虽然具有可扩展性，但多样性有限。近年来，深度生成模型（如GAN和扩散模型）以及3D表示（如NeRF和3D高斯）取得了进展，使得能够学习真实世界场景的分布，从而提高了生成的真实感、多样性和视图一致性。本文综述了最新的3D场景生成方法，分析了其技术基础、权衡和代表性结果，并讨论了生成能力、3D表示、数据和评估等关键挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.04842', 'title': 'Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers', 'url': 'https://huggingface.co/papers/2505.04842', 'abstract': "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL^V that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL^V boosts MATH accuracy by over 20\\% with parallel sampling and enables 8-32times efficient test-time compute scaling compared to the base RL method. RL^V also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL^V achieves 1.2-1.6times higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.", 'score': 12, 'issue_id': 3688, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': 'ae650f18905f196e', 'authors': ['Kusha Sareen', 'Morgane M Moss', 'Alessandro Sordoni', 'Rishabh Agarwal', 'Arian Hosseini'], 'affiliations': ['Google DeepMind, Mila', 'Microsoft Research, Mila', 'Mila, McGill University', 'Mila, Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2505.04842.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#optimization', '#rl', '#math'], 'emoji': '🧠', 'ru': {'title': 'RL^V: Улучшение языковых моделей через совместное обучение рассуждению и проверке', 'desc': 'Эта статья представляет метод RL^V, который улучшает обучение с подкреплением для языковых моделей. RL^V обучает модель одновременно рассуждать и проверять результаты, используя данные, сгенерированные в процессе обучения с подкреплением. Метод значительно повышает точность на математических задачах и позволяет эффективно масштабировать вычисления во время тестирования. RL^V также демонстрирует хорошую обобщающую способность на различных типах задач.'}, 'en': {'title': 'Enhancing LLMs with Value-Driven Reinforcement Learning', 'desc': 'This paper introduces RL^V, a novel reinforcement learning method that enhances large language models (LLMs) by integrating value functions into the training process. Unlike traditional methods that discard learned value functions, RL^V allows LLMs to act as both reasoners and generative verifiers, improving their ability to verify outputs. The approach significantly increases accuracy in mathematical tasks and improves computational efficiency during testing, allowing for faster processing. Additionally, RL^V demonstrates strong performance across various task difficulties and domains, showcasing its versatility and effectiveness in real-world applications.'}, 'zh': {'title': 'RL^V：提升推理与验证的强化学习新方法', 'desc': '本文提出了一种新的强化学习方法RL^V，用于优化大型语言模型（LLM）的推理能力。与传统方法不同，RL^V同时训练LLM作为推理者和生成验证者，利用强化学习生成的数据来增强验证能力。实验结果表明，RL^V在MATH任务上的准确率提高了超过20%，并且在测试时计算效率上提升了8到32倍。该方法还展示了在不同难度和领域任务上的强泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.05327', 'title': 'ICon: In-Context Contribution for Automatic Data Selection', 'url': 'https://huggingface.co/papers/2505.05327', 'abstract': 'Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.', 'score': 11, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'b64bd2ecee9bb211', 'authors': ['Yixin Yang', 'Qingxiu Dong', 'Linli Yao', 'Fangwei Zhu', 'Zhifang Sui'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05327.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'ICon: Эффективный отбор данных для обучения языковых моделей без градиентов', 'desc': 'Статья представляет новый метод отбора данных для обучения больших языковых моделей (LLM) под названием ICon. Этот метод использует обучение в контексте (in-context learning) для измерения вклада образцов данных без вычисления градиентов или ручной разработки эвристик. ICon показал высокую эффективность в экспериментах на трех LLM и 12 бенчмарках, позволив моделям, обученным на 15% отобранных данных, превзойти модели, обученные на полных наборах данных. Анализ выбранных ICon образцов показал, что они представляют разнообразные задачи и имеют подходящий уровень сложности.'}, 'en': {'title': 'Efficient Data Selection for Better Language Model Training', 'desc': 'This paper introduces a new method called In-context Learning for Contribution Measurement (ICon) that helps select the best data for training Large Language Models (LLMs) without using complex gradient calculations. ICon leverages the concept of in-context learning to evaluate how much each data sample contributes to model performance, making it more efficient than traditional methods. The approach reduces reliance on human-designed heuristics, which can introduce bias, and instead focuses on the intrinsic qualities of the data. Experiments show that models trained with ICon-selected data perform better than those trained on full datasets or those selected by existing methods.'}, 'zh': {'title': '高效数据选择，提升模型性能！', 'desc': '本文提出了一种新的数据选择方法，称为ICon（In-context Learning for Contribution Measurement），用于提高大型语言模型（LLMs）的性能并降低训练成本。ICon是一种无梯度的方法，利用上下文学习的隐式微调特性来评估样本的贡献，而无需计算梯度或设计手动指标。通过在三种大型语言模型上进行广泛实验，ICon显示出其在选择高贡献数据方面的有效性，能够在减少计算成本的同时提高模型性能。实验结果表明，使用ICon选择的15%数据训练的模型在性能上超过了使用完整数据集的模型。'}}}, {'id': 'https://huggingface.co/papers/2505.03981', 'title': 'X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains', 'url': 'https://huggingface.co/papers/2505.03981', 'abstract': "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.", 'score': 11, 'issue_id': 3672, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '0e6c2f37e1536f9f', 'authors': ['Qianchu Liu', 'Sheng Zhang', 'Guanghui Qin', 'Timothy Ossowski', 'Yu Gu', 'Ying Jin', 'Sid Kiblawi', 'Sam Preston', 'Mu Wei', 'Paul Vozila', 'Tristan Naumann', 'Hoifung Poon'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.03981.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#transfer_learning', '#healthcare'], 'emoji': '🧠', 'ru': {'title': 'Обобщаемые рассуждения: от текста к мультимодальности и специализированным доменам', 'desc': 'Эта статья исследует возможность обобщения способностей к рассуждению на различные модальности и домены. Авторы представляют X-Reasoner - мультимодальную модель, обученную только на текстовых данных общего домена для обобщаемых рассуждений. Эксперименты показывают, что X-Reasoner успешно переносит навыки рассуждения на мультимодальные и узкоспециализированные задачи, превосходя существующие модели. Также представлена медицинская версия модели - X-Reasoner-Med, достигающая новых рекордных результатов на ряде текстовых и мультимодальных медицинских бенчмарков.'}, 'en': {'title': 'Unlocking Generalizable Reasoning Across Modalities with X-Reasoner', 'desc': 'This paper investigates whether reasoning abilities can be generalized across different types of data, specifically from text to other modalities like images. The authors present X-Reasoner, a vision-language model that is post-trained on general-domain text to enhance its reasoning capabilities. They employ a two-stage training process that includes supervised fine-tuning with detailed reasoning steps and reinforcement learning with measurable rewards. The results demonstrate that X-Reasoner not only excels in multimodal tasks but also improves in specialized fields, leading to the creation of X-Reasoner-Med, which sets new benchmarks in medical reasoning tasks.'}, 'zh': {'title': '推理能力的跨模态推广', 'desc': '最近的专有模型（如o3）展示了强大的多模态推理能力。然而，大多数现有的开源研究主要集中在训练仅基于文本的推理模型，评估也主要限于数学和一般领域任务。因此，如何有效地将推理能力扩展到文本输入和一般领域之外仍然不清楚。本文探讨了一个基本的研究问题：推理是否可以跨模态和领域进行推广？'}}}, {'id': 'https://huggingface.co/papers/2505.05408', 'title': 'Crosslingual Reasoning through Test-Time Scaling', 'url': 'https://huggingface.co/papers/2505.05408', 'abstract': "Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.", 'score': 8, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'a6c39be653cbaefe', 'authors': ['Zheng-Xin Yong', 'M. Farid Adilazuarda', 'Jonibek Mansurov', 'Ruochen Zhang', 'Niklas Muennighoff', 'Carsten Eickhoff', 'Genta Indra Winata', 'Julia Kreutzer', 'Stephen H. Bach', 'Alham Fikri Aji'], 'affiliations': ['Brown University', 'Capital One', 'Cohere Labs', 'MBZUAI', 'Stanford University', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.05408.jpg', 'data': {'categories': ['#math', '#reasoning', '#low_resource', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Английское обучение, глобальное рассуждение: потенциал и ограничения многоязычных LLM', 'desc': "Это исследование посвящено способности моделей больших языковых моделей (LLM) рассуждать на разных языках после обучения на английском языке. Авторы обнаружили, что увеличение вычислительных ресурсов при выводе улучшает многоязычное математическое рассуждение, даже для малоресурсных языков. Модели демонстрируют паттерн 'цитирование-и-размышление' при работе с неанглийскими входными данными. Исследователи также разработали стратегию контроля языка рассуждений и выявили ограничения в обобщении рассуждений вне предметной области."}, 'en': {'title': 'Unlocking Multilingual Reasoning with English-Centric Models', 'desc': 'This paper explores how reasoning abilities of large language models, which are often trained in English, can be applied to other languages. The authors find that increasing computational resources for English-based reasoning models enhances their performance in multilingual mathematical reasoning, even in languages with fewer resources. They also identify a pattern where these models can effectively reason about non-English inputs by quoting and thinking in English. However, the study highlights challenges in transferring reasoning skills from STEM topics to cultural knowledge, indicating that while there is potential for cross-lingual reasoning, improvements are needed for low-resource languages and diverse contexts.'}, 'zh': {'title': '提升多语言推理能力的潜力与挑战', 'desc': '本研究探讨了大型语言模型在多语言环境下的推理能力，尤其是英语推理的微调如何在其他语言中推广。我们发现，通过增加推理计算能力，英语中心的推理语言模型在多种语言（包括低资源语言）中的数学推理能力得到了显著提升。研究还表明，尽管英语中心的推理链主要是英语，但它们在处理非英语输入时仍能遵循特定的推理模式。最后，我们发现模型在高资源语言中的推理效果更好，但在跨领域推理方面存在局限性，尤其是在从STEM领域到文化常识的迁移上。'}}}, {'id': 'https://huggingface.co/papers/2505.05288', 'title': 'PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes', 'url': 'https://huggingface.co/papers/2505.05288', 'abstract': "We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.", 'score': 7, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '7ffec9bccc965d17', 'authors': ['Ahmed Abdelreheem', 'Filippo Aleotti', 'Jamie Watson', 'Zawar Qureshi', 'Abdelrahman Eldesokey', 'Peter Wonka', 'Gabriel Brostow', 'Sara Vicente', 'Guillermo Garcia-Hernando'], 'affiliations': ['KAUST', 'Niantic Spatial', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.05288.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#3d', '#survey', '#dataset'], 'emoji': '🧊', 'ru': {'title': 'Языковое управление размещением объектов в реальных 3D-сценах', 'desc': 'Статья представляет новую задачу размещения объектов в реальных 3D-сценах с помощью языковых инструкций. Модель получает облако точек 3D-сцены, 3D-модель объекта и текстовое описание желаемого размещения. Задача требует рассуждений о 3D-геометрических отношениях и свободном пространстве, что отличает ее от других задач локализации в 3D. Авторы предлагают новый бенчмарк, протокол оценки, датасет для обучения 3D языковых моделей и базовый метод решения.'}, 'en': {'title': 'Placing Objects with Words in 3D Spaces!', 'desc': 'This paper presents a new task called Language-Guided Object Placement in Real 3D Scenes, which involves placing a 3D object based on a textual description. The model must analyze a point cloud of a 3D scene and determine a suitable location for the object that aligns with the given prompt. This task is particularly challenging due to the ambiguity of multiple valid placements and the need for understanding 3D spatial relationships. The authors introduce a benchmark, evaluation protocol, and a dataset to train 3D language models, aiming to enhance the evaluation of generalist 3D models.'}, 'zh': {'title': '语言引导的3D物体放置新挑战', 'desc': '我们提出了一项新任务：在真实3D场景中进行语言引导的物体放置。我们的模型接收一个3D场景的点云、一个3D资产和一个文本提示，任务是找到一个符合提示的有效放置位置。与其他语言引导的3D场景定位任务相比，这项任务具有特定的挑战性，因为它存在多个有效解，并且需要推理3D几何关系和自由空间。我们通过提出新的基准和评估协议来开启这一任务，并引入了一个新的数据集用于训练3D大语言模型，以及第一个非平凡的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.04769', 'title': 'Vision-Language-Action Models: Concepts, Progress, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2505.04769', 'abstract': 'Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models', 'score': 7, 'issue_id': 3682, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '75960b9512a0a05b', 'authors': ['Ranjan Sapkota', 'Yang Cao', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Biological & Environmental Engineering, Ithaca, New York, USA', 'The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong', 'University of the Peloponnese, Department of Informatics and Telecommunications, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.04769.jpg', 'data': {'categories': ['#agi', '#robotics', '#ethics', '#architecture', '#agents', '#multimodal', '#training'], 'emoji': '🤖', 'ru': {'title': 'Объединение зрения, языка и действия: новая парадигма ИИ', 'desc': 'Статья представляет обзор моделей Vision-Language-Action (VLA), которые объединяют восприятие, понимание естественного языка и физические действия в единую вычислительную структуру. Авторы систематизируют последние достижения в области VLA по пяти тематическим направлениям. Рассматриваются архитектурные инновации, стратегии эффективного обучения и ускорения вывода в реальном времени. Обсуждаются применения в робототехнике, автономных транспортных средствах, медицине и других областях.'}, 'en': {'title': 'Unifying Vision, Language, and Action in AI', 'desc': 'Vision-Language-Action (VLA) models represent a significant step in AI by combining visual perception, language understanding, and physical actions into one system. This review organizes recent developments in VLA models into five key themes, highlighting their evolution from basic cross-modal learning to sophisticated generalist agents. It covers over 80 models from the last three years, focusing on innovations in architecture, training efficiency, and real-time performance. The paper also discusses applications in various fields and proposes solutions to challenges like real-time control and ethical deployment, aiming to guide future advancements in intelligent robotics and AI.'}, 'zh': {'title': '统一感知与行动的智能模型', 'desc': '视觉-语言-行动（VLA）模型是人工智能领域的一项重要进展，旨在将感知、自然语言理解和具体行动统一在一个计算框架内。本文综述了VLA模型的最新进展，系统地组织了五个主题支柱，描绘了这一快速发展的领域。我们回顾了VLA系统的概念基础，追溯其从跨模态学习架构到紧密集成视觉-语言模型、行动规划器和层次控制器的演变。文章还探讨了VLA模型在机器人、自动驾驶、医疗、农业和增强现实等多个应用领域的挑战与解决方案。'}}}, {'id': 'https://huggingface.co/papers/2505.03422', 'title': 'LiftFeat: 3D Geometry-Aware Local Feature Matching', 'url': 'https://huggingface.co/papers/2505.03422', 'abstract': 'Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.', 'score': 7, 'issue_id': 3678, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '5be3a3b002db41cc', 'authors': ['Yepeng Liu', 'Wenpeng Lai', 'Zhou Zhao', 'Yuxuan Xiong', 'Jinchi Zhu', 'Jun Cheng', 'Yongchao Xu'], 'affiliations': ['Institute for Infocomm Research, A*STAR, Singapore', 'SF Technology, Shenzhen, China', 'School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03422.jpg', 'data': {'categories': ['#cv', '#robotics', '#3d'], 'emoji': '🤖', 'ru': {'title': 'LiftFeat: Повышение надежности сопоставления признаков с помощью 3D геометрии', 'desc': 'Статья представляет новую легковесную нейронную сеть LiftFeat для надежного сопоставления локальных признаков изображений. LiftFeat повышает устойчивость дескрипторов путем агрегации 3D геометрических признаков, извлеченных с помощью предобученной модели оценки глубины. Сеть использует модуль, учитывающий 3D геометрию, для объединения признаков нормалей поверхности с исходными 2D дескрипторами. Экспериментальные результаты показывают превосходство LiftFeat над современными легковесными методами в задачах оценки относительной позы, оценки гомографии и визуальной локализации.'}, 'en': {'title': 'LiftFeat: Enhancing Feature Matching with 3D Geometry', 'desc': 'This paper introduces LiftFeat, a lightweight neural network designed to improve local feature matching in challenging visual conditions. It addresses issues like drastic lighting changes and low texture areas by incorporating 3D geometric features into the feature extraction process. The network uses a pre-trained monocular depth estimation model to create pseudo surface normal labels, which guide the extraction of 3D features. Experimental results show that LiftFeat significantly enhances the performance of visual localization and pose estimation tasks compared to existing lightweight methods.'}, 'zh': {'title': '提升视觉特征匹配的鲁棒性与效率', 'desc': '本论文提出了一种新的轻量级网络LiftFeat，旨在提高在极端条件下的视觉特征匹配能力。通过聚合三维几何特征，LiftFeat增强了原始描述符的鲁棒性。我们采用预训练的单目深度估计模型生成伪表面法线标签，以指导三维几何特征的提取。实验结果表明，LiftFeat在相对姿态估计、单应性估计和视觉定位任务中优于一些轻量级的最新方法。'}}}, {'id': 'https://huggingface.co/papers/2505.05064', 'title': 'WaterDrum: Watermarking for Data-centric Unlearning Metric', 'url': 'https://huggingface.co/papers/2505.05064', 'abstract': 'Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.', 'score': 6, 'issue_id': 3684, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '8d04facba381d919', 'authors': ['Xinyang Lu', 'Xinyuan Niu', 'Gregory Kang Ruey Lau', 'Bui Thi Cam Nhung', 'Rachael Hwee Ling Sim', 'Fanyu Wen', 'Chuan-Sheng Foo', 'See-Kiong Ng', 'Bryan Kian Hsiang Low'], 'affiliations': ['A*STAR, Way, Create', 'CNRS@CREATE, 1 Singapore #08Tower, Computer Singapore (CFAR), Create', 'Centre for Singapore Science, National University', 'Department of of Singapore', 'Frontier AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05064.jpg', 'data': {'categories': ['#leakage', '#dataset', '#benchmark', '#data', '#open_source'], 'emoji': '🧽', 'ru': {'title': 'WaterDrum: Точная оценка разобучения языковых моделей с помощью водяных знаков', 'desc': 'Статья представляет новую метрику для оценки разобучения больших языковых моделей (LLM), называемую WaterDrum. Эта метрика основана на использовании водяных знаков в тексте и преодолевает ограничения существующих подходов, ориентированных на полезность модели. Авторы также представляют новые наборы данных для тестирования алгоритмов разобучения LLM, содержащие различные уровни сходства между точками данных. WaterDrum позволяет более точно оценивать эффективность разобучения в реалистичных сценариях, где забываемые и сохраняемые данные могут быть семантически похожими.'}, 'en': {'title': 'WaterDrum: A New Era in LLM Unlearning Metrics', 'desc': 'This paper addresses the challenge of unlearning in large language models (LLMs), which is essential for removing sensitive or harmful data. Current metrics for evaluating unlearning focus on model performance but may not reflect true unlearning effectiveness, especially when data is semantically similar. The authors propose a new data-centric unlearning metric called WaterDrum, which utilizes robust text watermarking to better assess unlearning in practical scenarios. Additionally, they introduce benchmark datasets designed to test unlearning algorithms under various conditions, enhancing the evaluation process for LLM unlearning.'}, 'zh': {'title': '提升大语言模型的遗忘能力', 'desc': '大语言模型（LLM）的遗忘能力在实际应用中至关重要，尤其是在需要有效去除用户的私人、版权或有害数据影响时。现有的以效用为中心的遗忘度量可能无法准确评估在语义相似内容的情况下的遗忘程度。本文提出了一种新的数据中心遗忘度量方法，称为WaterDrum，利用稳健的文本水印技术来克服这些限制。我们还引入了新的基准数据集，用于评估LLM的遗忘算法，确保可以严格测试遗忘效果。'}}}, {'id': 'https://huggingface.co/papers/2505.02363', 'title': 'SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in\n  Language Model Preference Learning', 'url': 'https://huggingface.co/papers/2505.02363', 'abstract': 'Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.', 'score': 6, 'issue_id': 3687, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'da8e6360c88cd0ed', 'authors': ['Tianjian Li', 'Daniel Khashabi'], 'affiliations': ['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, US'], 'pdf_title_img': 'assets/pdf/title_img/2505.02363.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#dataset'], 'emoji': '🔀', 'ru': {'title': 'Объединение сильных сторон: новый подход к оптимизации языковых моделей', 'desc': 'Это исследование посвящено оптимизации языковых моделей с использованием данных о предпочтениях пользователей. Авторы обнаружили, что on-policy и off-policy данные имеют взаимодополняющие сильные стороны в разных типах задач. Они представили метод SIMPLEMIX, который объединяет преимущества обоих типов данных путем их простого смешивания. Результаты показывают, что SIMPLEMIX значительно улучшает согласование языковых моделей с предпочтениями пользователей, превосходя существующие подходы.'}, 'en': {'title': 'SIMPLEMIX: Uniting On-Policy and Off-Policy for Better Language Model Alignment', 'desc': 'This paper explores how to align language models with human preferences using two types of data: on-policy and off-policy. On-policy data is shown to excel in tasks requiring reasoning, like math and coding, while off-policy data is better for creative and open-ended tasks. The authors propose a new method called SIMPLEMIX, which effectively combines these two data sources to leverage their strengths. Their experiments demonstrate that SIMPLEMIX significantly enhances the performance of language models compared to existing methods, achieving notable improvements in alignment metrics.'}, 'zh': {'title': '结合在线与离线数据，提升语言模型对齐效果', 'desc': '本研究探讨了在偏好学习中，在线数据和离线数据的互补优势。在线数据在推理任务（如数学和编程）中表现优异，而离线数据在开放式任务（如创意写作和个人推荐）中更具优势。我们提出了一种名为SIMPLEMIX的方法，通过简单地混合这两种数据源，结合它们的优点。实验证明，SIMPLEMIX在多种任务和基准测试中显著提高了语言模型的对齐效果。'}}}, {'id': 'https://huggingface.co/papers/2505.04955', 'title': 'Chain-of-Thought Tokens are Computer Program Variables', 'url': 'https://huggingface.co/papers/2505.04955', 'abstract': 'Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically study the role of CoT tokens in LLMs on two compositional tasks: multi-digit multiplication and dynamic programming. While CoT is essential for solving these problems, we find that preserving only tokens that store intermediate results would achieve comparable performance. Furthermore, we observe that storing intermediate results in an alternative latent form will not affect model performance. We also randomly intervene some values in CoT, and notice that subsequent CoT tokens and the final answer would change correspondingly. These findings suggest that CoT tokens may function like variables in computer programs but with potential drawbacks like unintended shortcuts and computational complexity limits between tokens. The code and data are available at https://github.com/solitaryzero/CoTs_are_Variables.', 'score': 5, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': 'a8bb5e6b1e09e5dd', 'authors': ['Fangwei Zhu', 'Peiyi Wang', 'Zhifang Sui'], 'affiliations': ['School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04955.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Токены CoT в LLM: переменные для машинного мышления', 'desc': 'Это исследование посвящено изучению механизма рассуждений с использованием цепочки мыслей (Chain-of-Thought, CoT) в больших языковых моделях (LLM). Авторы проанализировали роль токенов CoT на примере задач многозначного умножения и динамического программирования. Результаты показывают, что токены CoT функционируют подобно переменным в компьютерных программах, сохраняя промежуточные результаты. Исследование также выявило потенциальные недостатки метода CoT, такие как нежелательные упрощения и ограничения вычислительной сложности между токенами.'}, 'en': {'title': 'Unlocking the Power of Chain-of-Thought Tokens in LLMs', 'desc': "This paper investigates the role of chain-of-thought (CoT) tokens in large language models (LLMs) when solving complex reasoning tasks. The authors find that while CoT is important for tasks like multi-digit multiplication and dynamic programming, retaining only tokens that represent intermediate results can yield similar performance. They also discover that storing these results in a different latent form does not impact the model's effectiveness. Additionally, the study reveals that altering CoT values affects subsequent tokens and final answers, indicating that CoT tokens may act like variables in programming, albeit with some limitations."}, 'zh': {'title': '链式推理：变量的力量与挑战', 'desc': '本文研究了链式推理（CoT）在大型语言模型（LLMs）中的作用，特别是在多位数乘法和动态规划这两个复杂任务中的表现。研究发现，虽然CoT对于解决这些问题至关重要，但仅保留存储中间结果的标记也能达到相似的效果。此外，将中间结果以另一种潜在形式存储不会影响模型的性能。最后，随机干预CoT中的某些值会导致后续的CoT标记和最终答案相应变化，这表明CoT标记可能像计算机程序中的变量，但也存在一些潜在的缺陷，如意外的捷径和标记之间的计算复杂性限制。'}}}, {'id': 'https://huggingface.co/papers/2504.19314', 'title': 'BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese', 'url': 'https://huggingface.co/papers/2504.19314', 'abstract': "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.", 'score': 4, 'issue_id': 3673, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 апреля', 'en': 'April 27', 'zh': '4月27日'}, 'hash': '06aff0f566bd3817', 'authors': ['Peilin Zhou', 'Bruce Leon', 'Xiang Ying', 'Can Zhang', 'Yifan Shao', 'Qichen Ye', 'Dading Chong', 'Zhiling Jin', 'Chenxuan Xie', 'Meng Cao', 'Yuxin Gu', 'Sixin Hong', 'Jing Ren', 'Jian Chen', 'Chao Liu', 'Yining Hua'], 'affiliations': ['Alibaba Group', 'HSBC', 'Harvard T.H. Chan School of Public Health', 'Hong Kong University of Science and Technology (Guangzhou)', 'MBZUAI', 'Mindverse AI', 'NIO', 'Peking University', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.19314.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#dataset', '#benchmark', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'BrowseComp-ZH: испытание языковых моделей в китайском интернете', 'desc': 'Статья представляет BrowseComp-ZH - новый бенчмарк для оценки способностей языковых моделей (ЯМ) работать с китайским веб-контентом. Бенчмарк состоит из 289 сложных многоэтапных вопросов в 11 различных областях. Несмотря на свои сильные разговорные и поисковые возможности, большинство современных ЯМ показывают низкую точность на этом тесте. Результаты демонстрируют, что для успеха в BrowseComp-ZH требуются не только эффективные стратегии поиска, но и сложные рассуждения и согласование информации.'}, 'en': {'title': 'Evaluating LLMs: The Challenge of Chinese Web Browsing', 'desc': 'This paper introduces BrowseComp-ZH, a benchmark designed to evaluate large language models (LLMs) on their ability to browse and retrieve information from the Chinese web. It consists of 289 challenging multi-hop questions across various domains, focusing on high difficulty and unique answers. The study reveals that most state-of-the-art models perform poorly, with accuracy rates often below 10%, highlighting the complexity of reasoning and retrieval in non-English contexts. The results indicate that current LLMs still face significant challenges in mastering the necessary skills for effective information retrieval and reasoning in diverse linguistic environments.'}, 'zh': {'title': '中文网络智能体评估新基准：BrowseComp-ZH', 'desc': '随着大型语言模型（LLMs）逐渐演变为使用工具的智能体，实时浏览网络的能力成为衡量其推理和检索能力的重要标准。现有的基准测试如BrowseComp主要集中在英语，忽视了其他主要信息生态系统（尤其是中文）在语言、基础设施和审查方面的复杂性。为了解决这一问题，我们推出了BrowseComp-ZH，这是一个专门为全面评估中文网络上的LLM智能体而设计的高难度基准测试。该基准测试包含289个跨越11个不同领域的多跳问题，旨在考察模型的检索策略、推理能力和信息整合能力。'}}}, {'id': 'https://huggingface.co/papers/2505.02550', 'title': 'Bielik v3 Small: Technical Report', 'url': 'https://huggingface.co/papers/2505.02550', 'abstract': 'We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.', 'score': 53, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'a7c9d183be6447dd', 'authors': ['Krzysztof Ociepa', 'Łukasz Flis', 'Remigiusz Kinas', 'Krzysztof Wróbel', 'Adrian Gwoździej'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02550.jpg', 'data': {'categories': ['#small_models', '#plp', '#multilingual', '#low_resource', '#dataset', '#benchmark'], 'emoji': '🇵🇱', 'ru': {'title': 'Эффективные языковые модели делают ИИ на польском языке доступнее', 'desc': 'Представлена серия Bielik v3 - эффективные генеративные текстовые модели для польского языка. Модели демонстрируют, что меньшие оптимизированные архитектуры могут достигать производительности сравнимой с гораздо более крупными аналогами. Ключевые инновации включают специальный польский токенизатор, взвешенную функцию потерь и адаптивную скорость обучения. Модели показывают отличные результаты в различных тестах, устанавливая новые стандарты для эффективного моделирования языка для менее распространенных языков.'}, 'en': {'title': 'Efficient Polish Language Models: Big Performance from Small Sizes', 'desc': 'Bielik v3 introduces efficient generative text models specifically designed for the Polish language, with sizes of 1.5B and 4.5B parameters. These models show that smaller architectures can perform as well as larger ones while using less computational power. Key innovations include a custom tokenizer for better token efficiency, a specialized loss function to balance learning, and an adaptive learning rate that adjusts during training. With training on a vast dataset, these models set new standards for language processing in Polish, making advanced AI more accessible for various applications.'}, 'zh': {'title': '高效波兰语生成模型的创新之路', 'desc': '我们介绍了Bielik v3，这是一系列针对波兰语处理的高效生成文本模型（1.5B和4.5B参数）。这些模型表明，较小且经过优化的架构可以在计算资源大幅减少的情况下，达到与更大模型相当的性能。我们的创新包括定制的波兰语分词器（APT4），显著提高了标记效率，以及加权指令交叉熵损失，平衡不同指令类型的学习。此外，动态调整的学习率根据训练进度进行调整，使得模型在多个基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.02410', 'title': 'Bielik 11B v2 Technical Report', 'url': 'https://huggingface.co/papers/2505.02410', 'abstract': "We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.", 'score': 44, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'e9cb82cbeaac24ed', 'authors': ['Krzysztof Ociepa', 'Łukasz Flis', 'Krzysztof Wróbel', 'Adrian Gwoździej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02410.jpg', 'data': {'categories': ['#inference', '#multilingual', '#low_resource', '#training', '#benchmark', '#architecture', '#optimization', '#reasoning'], 'emoji': '🇵🇱', 'ru': {'title': 'Революция в обработке польского языка: Bielik 11B v2 устанавливает новые стандарты эффективности', 'desc': 'Представлена модель Bielik 11B v2 - усовершенствованная языковая модель для обработки польских текстов. Основанная на архитектуре Mistral 7B v0.2 и масштабированная до 11 миллиардов параметров, она демонстрирует исключительную производительность на польских языковых бенчмарках. В модели применены две ключевые инновации: взвешенная функция потерь для инструкций и адаптивная скорость обучения. Bielik 11B v2 превосходит более крупные модели и устанавливает новые стандарты для эффективного моделирования языка с ограниченными ресурсами.'}, 'en': {'title': 'Revolutionizing Polish Language Processing with Bielik 11B v2', 'desc': 'Bielik 11B v2 is a cutting-edge language model specifically designed for processing Polish text. It utilizes the Mistral 7B v0.2 architecture and has been enhanced to 11 billion parameters, achieving remarkable results on Polish language tasks while also performing well in cross-lingual scenarios. The model introduces innovative techniques like Weighted Instruction Cross-Entropy Loss for better learning from diverse instructions and an Adaptive Learning Rate that adjusts based on the context length. Its efficiency and quantization options allow it to run on various hardware, making it a significant advancement in AI for Polish language applications.'}, 'zh': {'title': '波兰语处理的新标杆：Bielik 11B v2', 'desc': 'Bielik 11B v2 是一个针对波兰语文本处理的先进语言模型，基于 Mistral 7B v0.2 架构，参数规模达到 11B。该模型在波兰语基准测试中表现出色，同时具备强大的跨语言能力。我们引入了两项关键技术创新：加权指令交叉熵损失，通过为训练样本分配基于质量的权重来优化不同指令类型的学习，以及自适应学习率，根据上下文长度动态调整。综合评估显示，Bielik 11B v2 超越了许多更大模型的表现，尤其在语言理解和复杂推理等任务上显著优于其他专门的波兰语言模型。'}}}, {'id': 'https://huggingface.co/papers/2505.06111', 'title': 'UniVLA: Learning to Act Anywhere with Task-centric Latent Actions', 'url': 'https://huggingface.co/papers/2505.06111', 'abstract': "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.", 'score': 17, 'issue_id': 3704, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': 'bf19981dd100b8fb', 'authors': ['Qingwen Bu', 'Yanting Yang', 'Jisong Cai', 'Shenyuan Gao', 'Guanghui Ren', 'Maoqing Yao', 'Ping Luo', 'Hongyang Li'], 'affiliations': ['AgiBot', 'OpenDriveLab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.06111.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#benchmark', '#agents', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Универсальное обучение роботов через видео и язык', 'desc': 'UniVLA - это новый фреймворк для обучения универсальных политик взаимодействия робота с окружающей средой на основе зрения, языка и действий. Ключевая инновация заключается в использовании скрытой модели действий для извлечения представлений из видео, что позволяет использовать разнородные данные из различных воплощений и перспектив. Фреймворк демонстрирует превосходные результаты на нескольких бенчмарках по манипуляции и навигации, а также в реальных роботизированных системах. UniVLA достигает лучшей производительности по сравнению с OpenVLA, используя при этом значительно меньше вычислительных ресурсов и данных.'}, 'en': {'title': 'UniVLA: Empowering Robots with Cross-Embodiment Learning', 'desc': 'The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches.'}, 'zh': {'title': 'UniVLA：提升通用机器人学习效率的新框架', 'desc': '本文提出了一种新的框架UniVLA，用于学习跨体现的视觉-语言-动作（VLA）策略，以提高通用机器人在不同环境中的表现。我们通过视频中的潜在动作模型提取以任务为中心的动作表示，从而利用广泛的多样化数据。为了减少与任务无关的动态影响，我们结合了语言指令，并在DINO特征空间中建立了潜在动作模型。实验结果表明，UniVLA在多个操作和导航基准测试中表现优异，且在预训练计算和下游数据方面的需求显著低于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.05026', 'title': 'G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness', 'url': 'https://huggingface.co/papers/2505.05026', 'abstract': 'Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.', 'score': 11, 'issue_id': 3705, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '41e61eccd430ea55', 'authors': ['Jaehyun Jeon', 'Jang Han Yoon', 'Min Soo Kim', 'Sumin Shim', 'Yejin Choi', 'Hanbin Kim', 'Youngjae Yu'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05026.jpg', 'data': {'categories': ['#reasoning', '#cv', '#benchmark', '#optimization', '#inference'], 'emoji': '🖥️', 'ru': {'title': 'Оценка убедительности UI без A/B-тестов', 'desc': 'Авторы статьи представляют новый подход к оценке эффективности пользовательских интерфейсов с точки зрения их убедительности. Они вводят бенчмарк WiserUI-Bench для сравнительной оценки дизайна интерфейсов, содержащий 300 пар реальных UI-изображений с результатами A/B-тестов. Исследователи также предлагают стратегию G-FOCUS для улучшения оценки убедительности интерфейсов с помощью мультимодальных языковых моделей. Эксперименты показывают, что G-FOCUS превосходит существующие методы по согласованности и точности оценки пользовательских интерфейсов.'}, 'en': {'title': 'Revolutionizing UI Evaluation with G-FOCUS and WiserUI-Bench', 'desc': 'This paper discusses the importance of evaluating user interface (UI) design not just for its visual appeal but for its ability to influence user behavior, a concept known as Design Persuasiveness. The authors highlight the limitations of traditional A/B testing, which is often expensive and slow, and propose a new benchmark called WiserUI-Bench for assessing UI design effectiveness through pairwise comparisons. They introduce G-FOCUS, an innovative reasoning strategy that improves the accuracy of Vision-Language Models (VLMs) in evaluating UI persuasiveness by minimizing biases. The results demonstrate that G-FOCUS outperforms existing methods, paving the way for more efficient and scalable UI design optimization.'}, 'zh': {'title': '提升用户界面设计的说服力评估', 'desc': '本论文探讨了用户界面（UI）设计的有效性评估，强调设计的说服力对用户行为的影响。传统的A/B测试方法虽然常用，但成本高且耗时。我们提出了WiserUI-Bench，这是一个用于成对UI设计说服力评估的基准，包含300对真实的UI图像及其A/B测试结果和专家理由。此外，我们还提出了G-FOCUS，这是一种新颖的推理策略，能够提高基于视觉语言模型的说服力评估的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.02686', 'title': 'Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2505.02686', 'abstract': 'Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.', 'score': 11, 'issue_id': 3706, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '22b290e68229e62f', 'authors': ['Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02686.jpg', 'data': {'categories': ['#survey', '#training', '#rlhf', '#alignment', '#benchmark', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обучение с вознаграждением: ключ к совершенствованию больших языковых моделей', 'desc': 'Данная статья представляет обзор парадигмы обучения с вознаграждением в контексте больших языковых моделей (LLM). Авторы анализируют различные стратегии применения этой парадигмы на этапах обучения, вывода и пост-обработки. Обсуждаются методы, такие как обучение с подкреплением, декодирование с учетом вознаграждения и постобработка, которые позволяют LLM переходить от пассивного обучения на статических данных к активному обучению с динамической обратной связью. В статье также рассматриваются бенчмарки для моделей вознаграждения, основные приложения и будущие направления исследований в этой области.'}, 'en': {'title': 'Harnessing Rewards: The Future of Learning in LLMs', 'desc': 'This paper discusses the evolution of Large Language Models (LLMs) focusing on the shift from pre-training to learning from rewards. It highlights how reward signals guide LLM behavior through techniques like reinforcement learning, reward-guided decoding, and post-hoc correction. The authors categorize various strategies used in training, inference, and post-inference stages, emphasizing the importance of dynamic feedback for improving model alignment and reasoning. Additionally, the paper addresses benchmarks for reward models and outlines future challenges and directions in this area.'}, 'zh': {'title': '从奖励中学习，赋能大型语言模型', 'desc': '最近，大型语言模型（LLMs）的发展从预训练扩展到后训练和测试时扩展。一个关键的统一范式出现了：从奖励中学习，其中奖励信号作为指导星，引导LLM的行为。这个范式支持了许多流行的技术，如强化学习（在RLHF、DPO和GRPO中）、奖励引导解码和事后修正。通过这个范式，LLMs能够从静态数据的被动学习转向从动态反馈的主动学习，赋予它们对齐的偏好和深度推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.06046', 'title': 'Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information', 'url': 'https://huggingface.co/papers/2505.06046', 'abstract': "As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.", 'score': 10, 'issue_id': 3707, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 мая', 'en': 'May 9', 'zh': '5月9日'}, 'hash': 'c671de3a9e8ff4de', 'authors': ['Joshua Harris', 'Fan Grayson', 'Felix Feldman', 'Timothy Laurence', 'Toby Nonnenmacher', 'Oliver Higgins', 'Leo Loman', 'Selina Patel', 'Thomas Finnie', 'Samuel Collins', 'Michael Borowitz'], 'affiliations': ['UK Health Security Agency (UKHSA)'], 'pdf_title_img': 'assets/pdf/title_img/2505.06046.jpg', 'data': {'categories': ['#alignment', '#science', '#dataset', '#healthcare', '#benchmark'], 'emoji': '🏥', 'ru': {'title': 'Оценка знаний LLM в сфере общественного здравоохранения: прогресс и ограничения', 'desc': 'Статья представляет новый бенчмарк PubHealthBench для оценки знаний больших языковых моделей (LLM) в области общественного здравоохранения Великобритании. Бенчмарк содержит более 8000 вопросов для оценки ответов моделей на вопросы с множественным выбором и свободной формой. Исследование показало, что новейшие частные LLM достигают более 90% точности в задачах с множественным выбором, превосходя людей с поверхностным использованием поисковых систем. Однако в задачах со свободной формой ответа производительность моделей ниже, что указывает на необходимость дополнительных мер безопасности при использовании LLM для предоставления информации о здравоохранении.'}, 'en': {'title': 'Evaluating LLMs for Public Health: Promising Yet Cautious', 'desc': "This paper investigates the knowledge of Large Language Models (LLMs) in the domain of UK public health information. It introduces a benchmark called PubHealthBench, which consists of over 8000 questions designed to evaluate LLMs' performance in Multiple Choice Question Answering (MCQA) and free form responses. The study assesses 24 different LLMs, revealing that the latest models perform well in MCQA tasks, achieving over 90% accuracy, but struggle with free form responses, none scoring above 75%. The findings suggest that while LLMs show promise as reliable sources of public health information, caution is needed when interpreting their free form outputs."}, 'zh': {'title': '提升公共卫生信息的准确性', 'desc': '随着大型语言模型（LLMs）的广泛应用，了解它们在特定领域的知识变得至关重要，尤其是在公共卫生领域。本文提出了一个新的基准测试PubHealthBench，包含超过8000个问题，用于评估LLMs在公共卫生查询中的多项选择问答（MCQA）和自由形式回答的能力。研究发现，最新的私有LLMs（如GPT-4.5和GPT-4.1）在MCQA测试中表现优异，准确率超过90%，甚至超越了人类的搜索引擎使用。然而，在自由形式回答中，模型的表现较低，没有一个模型得分超过75%。'}}}, {'id': 'https://huggingface.co/papers/2505.05621', 'title': 'A Preliminary Study for GPT-4o on Image Restoration', 'url': 'https://huggingface.co/papers/2505.05621', 'abstract': "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.", 'score': 4, 'issue_id': 3709, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 мая', 'en': 'May 8', 'zh': '5月8日'}, 'hash': '4fd37a23cd5db52f', 'authors': ['Hao Yang', 'Yan Yang', 'Ruikun Zhang', 'Liyuan Pan'], 'affiliations': ['Australian National University', 'Beijing Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.05621.jpg', 'data': {'categories': ['#optimization', '#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': '🖼️', 'ru': {'title': 'GPT-4o: Новый рубеж в восстановлении изображений', 'desc': 'Модель GPT-4o от OpenAI показала беспрецедентную производительность в генерации изображений. Исследователи провели систематическую оценку GPT-4o в различных задачах восстановления изображений. Хотя результаты визуально привлекательны, они часто страдают от структурной точности на уровне пикселей по сравнению с эталонными изображениями. Тем не менее, выходные данные GPT-4o могут служить мощными визуальными приорами, значительно улучшая производительность существующих сетей для удаления дымки, дождя и улучшения изображений при низкой освещенности.'}, 'en': {'title': 'Harnessing GPT-4o for Enhanced Image Restoration', 'desc': "The paper explores the capabilities of OpenAI's GPT-4o model in the field of image restoration, highlighting its ability to generate visually appealing images. Despite its impressive outputs, the model struggles with maintaining pixel-level accuracy, leading to issues like incorrect object positioning and altered image proportions. The authors demonstrate that GPT-4o can enhance existing image restoration techniques, particularly in tasks like dehazing and deraining, by providing valuable visual priors. This work aims to establish a foundation for integrating GPT-4o into future restoration workflows and encourages further research in image generation."}, 'zh': {'title': 'GPT-4o：图像修复的新动力', 'desc': 'OpenAI的GPT-4o模型结合了多模态输入和输出，展现了在图像生成方面的卓越性能。本文系统评估了GPT-4o在图像修复任务中的潜在影响，尽管其生成的修复图像在视觉上吸引人，但在像素级结构保真度上与真实图像相比存在问题。我们通过图像去雾、去雨和低光增强等案例研究，展示了GPT-4o的输出可以作为强大的视觉先验，显著提升现有去雾网络的性能。希望本研究能加速图像生成领域的创新，并将发布来自10个广泛使用的图像修复数据集的GPT-4o修复图像以支持进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2504.21467', 'title': 'Multiview Point Cloud Registration via Optimization in an Autoencoder\n  Latent Space', 'url': 'https://huggingface.co/papers/2504.21467', 'abstract': 'Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.', 'score': 0, 'issue_id': 3717, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '0bb5c596b88819cc', 'authors': ['Luc Vedrenne', 'Sylvain Faisan', 'Denis Fortun'], 'affiliations': ['ICube Laboratory, IMAGeS team, UMR 7357, CNRS, University of Strasbourg, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.21467.jpg', 'data': {'categories': ['#3d', '#cv', '#synthetic', '#optimization', '#open_source'], 'emoji': '🌀', 'ru': {'title': 'POLAR: Революция в многоракурсной регистрации облаков точек', 'desc': 'POLAR - это новый метод многоракурсной регистрации облаков точек, способный эффективно обрабатывать большое количество ракурсов. Он переносит задачу регистрации в латентное пространство предобученного автоэнкодера и использует специальную функцию потерь, учитывающую деградации данных. POLAR устойчив к высокому уровню деградаций и большим начальным углам между облаками точек. Метод значительно превосходит современные подходы на синтетических и реальных данных.'}, 'en': {'title': 'POLAR: Efficient and Robust Multiview Point Cloud Registration', 'desc': 'This paper presents POLAR, a new method for aligning multiple 3D point clouds, which is crucial in computer vision. Unlike traditional pairwise registration methods that struggle with scalability, POLAR efficiently handles many views and is robust against significant data degradation. The approach utilizes a pretrained autoencoder to transform the registration task into a latent space, allowing for better optimization. By incorporating a specialized loss function and a multistart optimization strategy, POLAR outperforms existing techniques on both synthetic and real datasets.'}, 'zh': {'title': 'POLAR：高效的多视角点云配准方法', 'desc': '点云刚性配准是3D计算机视觉中的一个基本问题。在多视角情况下，我们的目标是找到一组6D姿态来对齐一组物体。传统的基于成对配准的方法在视角数量增加时扩展性较差，而生成方法虽然克服了这一限制，但不适合处理大变换和高降级水平。本文提出了POLAR（点云潜在配准），它能够高效处理大量视角，同时对高降级和大初始角度具有鲁棒性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (11)', '#agents (26)', '#agi (9)', '#alignment (15)', '#architecture (31)', '#audio (6)', '#benchmark (68)', '#cv (20)', '#data (33)', '#dataset (64)', '#diffusion (13)', '#ethics (5)', '#games (14)', '#graphs (5)', '#hallucinations (8)', '#healthcare (7)', '#inference (15)', '#interpretability (19)', '#leakage (3)', '#long_context (5)', '#low_resource (10)', '#machine_translation (4)', '#math (15)', '#multilingual (12)', '#multimodal (51)', '#open_source (36)', '#optimization (75)', '#plp (3)', '#rag (4)', '#reasoning (50)', '#rl (27)', '#rlhf (15)', '#robotics (10)', '#science (8)', '#security (2)', '#small_models (10)', '#story_generation (1)', '#survey (13)', '#synthetic (10)', '#training (76)', '#transfer_learning (15)', '#video (13)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-15 07:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-15 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-15 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    