
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 581 papers. May 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">–ú–∞–π 2025</span> | <span id="title-articles-count">581 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-04.html">‚¨ÖÔ∏è <span id="prev-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-06.html">‚û°Ô∏è <span id="next-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">üìà <span id='top-day-label'>–î–µ–Ω—å</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '–ú–∞–π 2025', 'en': 'May 2025', 'zh': '5Êúà2025Âπ¥'};
        let feedDateNext = {'ru': '06.2025', 'en': '06/2025', 'zh': '6Êúà2025Âπ¥'};
        let feedDatePrev = {'ru': '04.2025', 'en': '04/2025', 'zh': '4Êúà2025Âπ¥'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.07062', 'title': 'Seed1.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2505.07062', 'abstract': 'We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)', 'score': 83, 'issue_id': 3722, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 –º–∞—è', 'en': 'May 11', 'zh': '5Êúà11Êó•'}, 'hash': 'c3406b40cc21820d', 'authors': ['Dong Guo', 'Faming Wu', 'Feida Zhu', 'Fuxing Leng', 'Guang Shi', 'Haobin Chen', 'Haoqi Fan', 'Jian Wang', 'Jianyu Jiang', 'Jiawei Wang', 'Jingji Chen', 'Jingjia Huang', 'Kang Lei', 'Liping Yuan', 'Lishu Luo', 'Pengfei Liu', 'Qinghao Ye', 'Rui Qian', 'Shen Yan', 'Shixiong Zhao', 'Shuai Peng', 'Shuangye Li', 'Sihang Yuan', 'Sijin Wu', 'Tianheng Cheng', 'Weiwei Liu', 'Wenqian Wang', 'Xianhan Zeng', 'Xiao Liu', 'Xiaobo Qin', 'Xiaohan Ding', 'Xiaojun Xiao', 'Xiaoying Zhang', 'Xuanwei Zhang', 'Xuehan Xiong', 'Yanghua Peng', 'Yangrui Chen', 'Yanwei Li', 'Yanxu Hu', 'Yi Lin', 'Yiyuan Hu', 'Yiyuan Zhang', 'Youbin Wu', 'Yu Li', 'Yudong Liu', 'Yue Ling', 'Yujia Qin', 'Zanbo Wang', 'Zhiwu He', 'Aoxue Zhang', 'Bairen Yi', 'Bencheng Liao', 'Can Huang', 'Can Zhang', 'Chaorui Deng', 'Chaoyi Deng', 'Cheng Lin', 'Cheng Yuan', 'Chenggang Li', 'Chenhui Gou', 'Chenwei Lou', 'Chengzhi Wei', 'Chundian Liu', 'Chunyuan Li', 'Deyao Zhu', 'Donghong Zhong', 'Feng Li', 'Feng Zhang', 'Gang Wu', 'Guodong Li', 'Guohong Xiao', 'Haibin Lin', 'Haihua Yang', 'Haoming Wang', 'Heng Ji', 'Hongxiang Hao', 'Hui Shen', 'Huixia Li', 'Jiahao Li', 'Jialong Wu', 'Jianhua Zhu', 'Jianpeng Jiao', 'Jiashi Feng', 'Jiaze Chen', 'Jianhui Duan', 'Jihao Liu', 'Jin Zeng', 'Jingqun Tang', 'Jingyu Sun', 'Joya Chen', 'Jun Long', 'Junda Feng', 'Junfeng Zhan', 'Junjie Fang', 'Junting Lu', 'Kai Hua', 'Kai Liu', 'Kai Shen', 'Kaiyuan Zhang', 'Ke Shen', 'Ke Wang', 'Keyu Pan', 'Kun Zhang', 'Kunchang Li', 'Lanxin Li', 'Lei Li', 'Lei Shi', 'Li Han', 'Liang Xiang', 'Liangqiang Chen', 'Lin Chen', 'Lin Li', 'Lin Yan', 'Liying Chi', 'Longxiang Liu', 'Mengfei Du', 'Mingxuan Wang', 'Ningxin Pan', 'Peibin Chen', 'Pengfei Chen', 'Pengfei Wu', 'Qingqing Yuan', 'Qingyao Shuai', 'Qiuyan Tao', 'Renjie Zheng', 'Renrui Zhang', 'Ru Zhang', 'Rui Wang', 'Rui Yang', 'Rui Zhao', 'Shaoqiang Xu', 'Shihao Liang', 'Shipeng Yan', 'Shu Zhong', 'Shuaishuai Cao', 'Shuangzhi Wu', 'Shufan Liu', 'Shuhan Chang', 'Songhua Cai', 'Tenglong Ao', 'Tianhao Yang', 'Tingting Zhang', 'Wanjun Zhong', 'Wei Jia', 'Wei Weng', 'Weihao Yu', 'Wenhao Huang', 'Wenjia Zhu', 'Wenli Yang', 'Wenzhi Wang', 'Xiang Long', 'XiangRui Yin', 'Xiao Li', 'Xiaolei Zhu', 'Xiaoying Jia', 'Xijin Zhang', 'Xin Liu', 'Xinchen Zhang', 'Xinyu Yang', 'Xiongcai Luo', 'Xiuli Chen', 'Xuantong Zhong', 'Xuefeng Xiao', 'Xujing Li', 'Yan Wu', 'Yawei Wen', 'Yifan Du', 'Yihao Zhang', 'Yining Ye', 'Yonghui Wu', 'Yu Liu', 'Yu Yue', 'Yufeng Zhou', 'Yufeng Yuan', 'Yuhang Xu', 'Yuhong Yang', 'Yun Zhang', 'Yunhao Fang', 'Yuntao Li', 'Yurui Ren', 'Yuwen Xiong', 'Zehua Hong', 'Zehua Wang', 'Zewei Sun', 'Zeyu Wang', 'Zhao Cai', 'Zhaoyue Zha', 'Zhecheng An', 'Zhehui Zhao', 'Zhengzhuo Xu', 'Zhipeng Chen', 'Zhiyong Wu', 'Zhuofan Zheng', 'Zihao Wang', 'Zilong Huang', 'Ziyu Zhu', 'Zuquan Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07062.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#architecture', '#training', '#reasoning', '#data'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤—ã–¥–∞—é—â–∏–º–∏—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏', 'desc': 'Seed1.5-VL - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–æ—á–µ—Ç–∞—é—â–∞—è –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ 532 –º–ª–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å 20 –º–ª—Ä–¥ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è state-of-the-art –Ω–∞ 38 –∏–∑ 60 –ø—É–±–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. Seed1.5-VL –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏, –∏–≥—Ä–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –≤–µ–¥—É—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Empowering Multimodal Understanding with Seed1.5-VL', 'desc': 'Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles.'}, 'zh': {'title': 'Seed1.5-VLÔºöÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜSeed1.5-VLÔºåËøôÊòØ‰∏ÄÁßçÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÊé®ÁêÜÁöÑËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°Âûã„ÄÇSeed1.5-VLÁî±‰∏Ä‰∏™532MÂèÇÊï∞ÁöÑËßÜËßâÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™ÂÖ∑Êúâ20BÊ¥ªË∑ÉÂèÇÊï∞ÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàMoE LLMÔºâÁªÑÊàê„ÄÇÂ∞ΩÁÆ°ÂÖ∂Êû∂ÊûÑÁõ∏ÂØπÁ¥ßÂáëÔºå‰ΩÜÂú®Â§ö‰∏™ÂÖ¨ÂÖ±VLMÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂú®60‰∏™ÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏≠Êúâ38‰∏™ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂú®‰ª•‰ª£ÁêÜ‰∏∫‰∏≠ÂøÉÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÊéßÂà∂ÂíåÊ∏∏ÊàèÁé©Ê≥ïÔºåSeed1.5-VLË∂ÖË∂ä‰∫ÜÈ¢ÜÂÖàÁöÑÂ§öÊ®°ÊÄÅÁ≥ªÁªüÔºåÂåÖÊã¨OpenAI CUAÂíåClaude 3.7„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07608', 'title': 'MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining', 'url': 'https://huggingface.co/papers/2505.07608', 'abstract': "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.", 'score': 52, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9db5f7b72add3369', 'authors': ['Xiaomi LLM-Core Team', ':', 'Bingquan Xia', 'Bowen Shen', 'Cici', 'Dawei Zhu', 'Di Zhang', 'Gang Wang', 'Hailin Zhang', 'Huaqiu Liu', 'Jiebao Xiao', 'Jinhao Dong', 'Liang Zhao', 'Peidian Li', 'Peng Wang', 'Shihua Yu', 'Shimao Chen', 'Weikun Wang', 'Wenhan Ma', 'Xiangwei Deng', 'Yi Huang', 'Yifan Song', 'Zihan Jiang', 'Bowen Ye', 'Can Cai', 'Chenhong He', 'Dong Zhang', 'Duo Zhang', 'Guoan Wang', 'Hao Tian', 'Haochen Zhao', 'Heng Qu', 'Hongshen Xu', 'Jun Shi', 'Kainan Bao', 'QingKai Fang', 'Kang Zhou', 'Kangyang Zhou', 'Lei Li', 'Menghang Zhu', 'Nuo Chen', 'Qiantong Wang', 'Shaohui Liu', 'Shicheng Li', 'Shuhao Gu', 'Shuhuai Ren', 'Shuo Liu', 'Sirui Deng', 'Weiji Zhuang', 'Weiwei Lv', 'Wenyu Yang', 'Xin Zhang', 'Xing Yong', 'Xing Zhang', 'Xingchen Song', 'Xinzhe Xu', 'Xu Wang', 'Yihan Yan', 'Yu Tu', 'Yuanyuan Tian', 'Yudong Wang', 'Yue Yu', 'Zhenru Lin', 'Zhichao Song', 'Zihao Yue'], 'affiliations': ['Xiaomi LLM-Core Team'], 'pdf_title_img': 'assets/pdf/title_img/2505.07608.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#dataset', '#math', '#rl', '#data', '#training'], 'emoji': 'üß†', 'ru': {'title': 'MiMo-7B: –ú–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': 'MiMo-7B - —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ù–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –Ω–∞–±–æ—Ä–µ –∏–∑ 130 —Ç—ã—Å—è—á –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –ò—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å MiMo-7B-RL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'MiMo-7B: Revolutionizing Reasoning with Advanced Training Techniques', 'desc': 'MiMo-7B is a large language model specifically designed for reasoning tasks, optimized through both pre-training and post-training processes. In the pre-training phase, it utilizes an advanced data preprocessing pipeline and a three-stage data mixing strategy to enhance its reasoning capabilities, training on a massive dataset of 25 trillion tokens. The post-training phase involves reinforcement learning with a curated dataset of 130,000 math and programming problems, addressing sparse-reward challenges with a code-reward scheme and strategic data resampling. Evaluations demonstrate that MiMo-7B-Base excels in reasoning tasks, outperforming larger models, while the final RL-tuned version, MiMo-7B-RL, achieves outstanding results in mathematics, coding, and general reasoning tasks.'}, 'zh': {'title': 'MiMo-7BÔºöÊé®ÁêÜ‰ªªÂä°ÁöÑÂº∫Â§ßËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜMiMo-7BÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫Êé®ÁêÜ‰ªªÂä°ËÆæËÆ°ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ºòÂåñ‰∫ÜÈ¢ÑËÆ≠ÁªÉÂíåÂêéËÆ≠ÁªÉÈò∂ÊÆµ„ÄÇÂú®È¢ÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨Â¢ûÂº∫‰∫ÜÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊµÅÁ®ãÔºåÂπ∂ÈááÁî®‰∏âÈò∂ÊÆµÊï∞ÊçÆÊ∑∑ÂêàÁ≠ñÁï•Ôºå‰ª•ÊèêÂçáÂü∫Á°ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇMiMo-7B-BaseÂú®25‰∏á‰∫ø‰∏™Ê†áËÆ∞‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Â¢ûÂä†‰∫ÜÂ§öÊ†áËÆ∞È¢ÑÊµãÁõÆÊ†áÔºå‰ª•ÊèêÈ´òÊÄßËÉΩÂíåÂä†ÈÄüÊé®ÁêÜÈÄüÂ∫¶„ÄÇÂú®ÂêéËÆ≠ÁªÉÈò∂ÊÆµÔºåÊàë‰ª¨Êï¥ÁêÜ‰∫Ü130K‰∏™ÂèØÈ™åËØÅÁöÑÊï∞Â≠¶ÂíåÁºñÁ®ãÈóÆÈ¢òÊï∞ÊçÆÈõÜÔºåÁªìÂêàÊµãËØïÈöæÂ∫¶È©±Âä®ÁöÑ‰ª£Á†ÅÂ•ñÂä±Êú∫Âà∂ÔºåËß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±ÈóÆÈ¢òÔºåÂπ∂ÈááÁî®ÊàòÁï•ÊÄßÊï∞ÊçÆÈáçÈááÊ†∑Êù•Á®≥ÂÆöËÆ≠ÁªÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07747', 'title': 'Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets', 'url': 'https://huggingface.co/papers/2505.07747', 'abstract': 'While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.', 'score': 46, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'd9ffe741ebae4acb', 'authors': ['Weiyu Li', 'Xuanyang Zhang', 'Zheng Sun', 'Di Qi', 'Hao Li', 'Wei Cheng', 'Weiwei Cai', 'Shihao Wu', 'Jiarui Liu', 'Zihao Wang', 'Xiao Chen', 'Feipeng Tian', 'Jianxiong Pan', 'Zeming Li', 'Gang Yu', 'Xiangyu Zhang', 'Daxin Jiang', 'Ping Tan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07747.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#data', '#diffusion', '#transfer_learning', '#3d', '#benchmark'], 'emoji': 'üßä', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Step1X-3D - –æ—Ç–∫—Ä—ã—Ç—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ 3D-–º–æ–¥–µ–ª–µ–π. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π VAE-DiT –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ç–µ–∫—Å—Ç—É—Ä. Step1X-3D –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∏–∑ 2D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫ 3D-—Å–∏–Ω—Ç–µ–∑—É.'}, 'en': {'title': 'Revolutionizing 3D Generation with Step1X-3D', 'desc': 'The paper introduces Step1X-3D, a framework designed to improve 3D generation in artificial intelligence. It tackles challenges like limited data and algorithmic issues by creating a high-quality dataset and employing a two-stage architecture that combines a geometry generator and a texture synthesis module. The framework allows for better detail preservation and consistency in 3D assets by integrating techniques from 2D generation. By providing open-source resources, it aims to enhance research and development in controllable 3D asset generation.'}, 'zh': {'title': 'Step1X-3DÔºöÂºÄÂàõÂèØÊéß3DÁîüÊàêÁöÑÊñ∞Ê†áÂáÜ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜStep1X-3DÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊîæÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥3DÁîüÊàê‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫„ÄÅÁÆóÊ≥ïÈôêÂà∂ÂíåÁîüÊÄÅÁ≥ªÁªüÁ¢éÁâáÂåñÁ≠âÊåëÊàò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰∏•Ê†ºÁöÑÊï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ãÔºåÂ§ÑÁêÜË∂ÖËøá500‰∏áËµÑ‰∫ßÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™200‰∏áÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºåÂπ∂ÈááÁî®Ê†áÂáÜÂåñÁöÑÂá†‰ΩïÂíåÁ∫πÁêÜÂ±ûÊÄß„ÄÇÂÆÉÁªìÂêà‰∫ÜÊ∑∑ÂêàVAE-DiTÂá†‰ΩïÁîüÊàêÂô®ÂíåÂü∫‰∫éÊâ©Êï£ÁöÑÁ∫πÁêÜÂêàÊàêÊ®°ÂùóÔºåËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑ3DÊ®°Âûã„ÄÇStep1X-3DËøòÊîØÊåÅÂ∞Ü2DÊéßÂà∂ÊäÄÊúØÁõ¥Êé•ËΩ¨ÁßªÂà∞3DÂêàÊàêÔºåÊé®Âä®‰∫ÜÂèØÊéß3DËµÑ‰∫ßÁîüÊàêÁöÑÊñ∞Ê†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07787', 'title': 'Learning from Peers in Reasoning Models', 'url': 'https://huggingface.co/papers/2505.07787', 'abstract': 'Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP\'s robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .', 'score': 34, 'issue_id': 3722, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '350f28f20ab516fc', 'authors': ['Tongxu Luo', 'Wenyu Du', 'Jiaxi Bi', 'Stephen Chung', 'Zhengyang Tang', 'Hao Yang', 'Min Zhang', 'Benyou Wang'], 'affiliations': ['DualityRL', 'Huawei', 'The Chinese University of Hong Kong, Shenzhen', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2505.07787.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#small_models', '#reasoning', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º: –∫–∞–∫ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –º–æ–≥—É—Ç –ø–æ–ø–∞–¥–∞—Ç—å –≤ '–ª–æ–≤—É—à–∫—É –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–∞', –∫–æ–≥–¥–∞ –ø–ª–æ—Ö–æ–µ –Ω–∞—á–∞–ª–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–µ—à–∞–µ—Ç —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ '–û–±—É—á–µ–Ω–∏–µ —É —Å–≤–µ—Ä—Å—Ç–Ω–∏–∫–æ–≤' (LeaP), –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–æ–¥–µ–ª—è–º –æ–±–º–µ–Ω–∏–≤–∞—Ç—å—Å—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π LeaP-T, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ –æ–±–æ–±—â–µ–Ω–∏—é –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LeaP."}, 'en': {'title': 'Empowering LRMs through Peer Collaboration', 'desc': "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."}, 'zh': {'title': 'Âêå‰º¥Â≠¶‰π†ÔºöÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑËá™ÊàëÁ∫†Ê≠£ËÉΩÂäõ', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂÖ∑ÊúâËá™ÊàëÁ∫†Ê≠£ÁöÑËÉΩÂäõÔºå‰ΩÜÂΩìÊé®ÁêÜËøáÁ®ã‰ª•Áü≠ËÄåÂ∑ÆÁöÑÂºÄÂ§¥ÂºÄÂßãÊó∂ÔºåÊ®°ÂûãÂæàÈöæÊÅ¢Â§ç„ÄÇÊàë‰ª¨Áß∞ËøôÁßçÁé∞Ë±°‰∏∫‚ÄúÂâçÁºÄ‰∏ªÂØºÈô∑Èò±‚Äù„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‚Äú‰ªéÂêå‰º¥Â≠¶‰π†‚ÄùÔºàLeaPÔºâÔºåÈÄöËøáË∑ØÁî±Êú∫Âà∂ËÆ©ÊØè‰∏™Êé®ÁêÜË∑ØÂæÑÊÄªÁªìÂÖ∂‰∏≠Èó¥Êé®ÁêÜÂπ∂‰∏éÂÖ∂‰ªñË∑ØÂæÑÂÖ±‰∫´Ôºå‰ªéËÄåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËûçÂÖ•Âêå‰º¥ÁöÑËßÅËß£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLeaPÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜ‰∏çÂêå‰ªªÂä°ÈöæÂ∫¶Êó∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈîôËØØÂÆπÂøçËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07447', 'title': 'Unified Continuous Generative Models', 'url': 'https://huggingface.co/papers/2505.07447', 'abstract': 'Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.', 'score': 31, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '8d18ef028e9905b1', 'authors': ['Peng Sun', 'Yi Jiang', 'Tao Lin'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07447.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': 'üîÑ', 'ru': {'title': '–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–æ –º–∞–ª–æ—à–∞–≥–æ–≤—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –¥–∏—Ñ—Ñ—É–∑–∏—è –∏ flow-matching, —Å –º–∞–ª–æ—à–∞–≥–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –≤—Ä–æ–¥–µ consistency models. –ò—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, UCGM-{T,S}, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ ImageNet 256x256, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, UCGM-S —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, —Å–Ω–∏–∂–∞—è FID –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Unifying Generative Models for Superior Performance', 'desc': 'This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.'}, 'zh': {'title': 'Áªü‰∏ÄÁîüÊàêÊ®°ÂûãÔºåÊèêÂçáÁîüÊàêÊÄßËÉΩÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑËøûÁª≠ÁîüÊàêÊ®°ÂûãÊ°ÜÊû∂ÔºåÊó®Âú®Êï¥ÂêàÂ§öÊ≠•ÂíåÂ∞ëÊ≠•ÁîüÊàêÊñπÊ≥ïÁöÑËÆ≠ÁªÉÂíåÈááÊ†∑„ÄÇÈÄöËøáÂºïÂÖ•Áªü‰∏ÄÁöÑËÆ≠ÁªÉÂíåÈááÊ†∑Âô®ÔºàUCGM-{T,S}ÔºâÔºåÊàë‰ª¨ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁîüÊàêÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏äÔºåUCGM-TËÉΩÂ§üÂú®20Ê≠•ÂÜÖÂ∞ÜÂ§öÊ≠•Ê®°ÂûãÁöÑFIDÈôç‰ΩéÂà∞1.30ÔºåËÄåÂ∞ëÊ≠•Ê®°ÂûãÂú®‰ªÖ2Ê≠•ÂÜÖËææÂà∞1.42ÁöÑFID„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®UCGM-SÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÊîπËøõÔºåFID‰ªé250Ê≠•ÁöÑ1.26ÈôçËá≥‰ªÖ40Ê≠•ÁöÑ1.06„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06548', 'title': 'REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback', 'url': 'https://huggingface.co/papers/2505.06548', 'abstract': 'Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.', 'score': 26, 'issue_id': 3722, 'pub_date': '2025-05-10', 'pub_date_card': {'ru': '10 –º–∞—è', 'en': 'May 10', 'zh': '5Êúà10Êó•'}, 'hash': 'db28335cad79db53', 'authors': ['Aniruddha Roy', 'Pretam Ray', 'Abhilash Nandy', 'Somak Aditya', 'Pawan Goyal'], 'affiliations': ['Indian Institute of Technology, Kharagpur'], 'pdf_title_img': 'assets/pdf/title_img/2505.06548.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#small_models', '#rl', '#dataset', '#data'], 'emoji': 'ü§ñ', 'ru': {'title': '–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –∏ RL —É–ª—É—á—à–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ø–ú', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLaMA 2-7B, LLama 2-13B, Mistral 7B) –¥–ª—è –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–º–µ–Ω—å—à–∞—é—â–∏–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω–∏ —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å RL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ 63-66% –∑–∞–¥–∞—á.'}, 'en': {'title': 'Empowering LLMs with Cost-Effective Instruction Generation', 'desc': 'This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods.'}, 'zh': {'title': 'È´òÊïàÁîüÊàêÊåá‰ª§Êï∞ÊçÆÔºåÊèêÂçáLLMsÊÄßËÉΩ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊåá‰ª§È©±Âä®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂçäËá™Âä®ÂåñÊ°ÜÊû∂ÔºåÂà©Áî®ÂºÄÊ∫êÁöÑÂ∞èÂûãLLMsÔºàÂ¶ÇLLaMA 2-7B„ÄÅLLaMA 2-13BÂíåMistral 7BÔºâÊù•ÁîüÊàêÊåá‰ª§Êï∞ÊçÆÈõÜÔºå‰ªéËÄåÂáèÂ∞ë‰∫∫Â∑•Âπ≤È¢ÑÂíåÊàêÊú¨„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÁÆóÊ≥ïÔºåÁ†îÁ©∂Ë°®ÊòéËøôÁßçÊñπÊ≥ïÂú®63-66%ÁöÑ‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇËØ•Á†îÁ©∂‰∏∫ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊåá‰ª§Êï∞ÊçÆÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥È´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07293', 'title': 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection', 'url': 'https://huggingface.co/papers/2505.07293', 'abstract': "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.", 'score': 17, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '7403ae602b400fc4', 'authors': ['Kai Hua', 'Steven Wu', 'Ge Zhang', 'Ke Shen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.07293.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#data', '#optimization', '#small_models'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ø–ú –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AttentionInfluence –¥–ª—è –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—é—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–±–æ–ª—å—à—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ–≤–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏–ª–∏ —Ä–∞–∑–º–µ—Ç–∫–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ AttentionInfluence –∫ –∫–æ—Ä–ø—É—Å—É SmolLM –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è 7B-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with AttentionInfluence', 'desc': 'This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.'}, 'zh': {'title': 'Êó†ÁõëÁù£Êé®ÁêÜÊï∞ÊçÆÈÄâÊã©ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøëÔºåÁ†îÁ©∂ËÄÖ‰ª¨Ë∂äÊù•Ë∂äÂÖ≥Ê≥®Êî∂ÈõÜÊé®ÁêÜÂØÜÈõÜÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ‰ª•ÂæÄÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÁõëÁù£ÂàÜÁ±ªÂô®Êù•ËØÜÂà´Ëøô‰∫õÊï∞ÊçÆÔºåËøôÈúÄË¶Å‰∫∫Á±ªÊàñLLMsËøõË°åÊ†áÊ≥®ÔºåÂ∏∏Â∏∏ÂºïÂÖ•È¢ÜÂüüÁâπÂÆöÁöÑÂÅèËßÅ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AttentionInfluenceÁöÑÊñπÊ≥ïÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊó†ÁõëÁù£ËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÈÄöËøáÁÆÄÂçïÁöÑÊ≥®ÊÑèÂäõÂ§¥Â±èËîΩÊìç‰ΩúÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÂæó‰∏Ä‰∏™Â∞èÂûãÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§ü‰Ωú‰∏∫Âº∫Â§ßÁöÑÊï∞ÊçÆÈÄâÊã©Âô®Ôºå‰ªéËÄåÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÂèñÂæóÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07818', 'title': 'DanceGRPO: Unleashing GRPO on Visual Generation', 'url': 'https://huggingface.co/papers/2505.07818', 'abstract': 'Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.', 'score': 16, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '023078250e0d651f', 'authors': ['Zeyue Xue', 'Jie Wu', 'Yu Gao', 'Fangyuan Kong', 'Lingting Zhu', 'Mengzhao Chen', 'Zhiheng Liu', 'Wei Liu', 'Qiushan Guo', 'Weilin Huang', 'Ping Luo'], 'affiliations': ['ByteDance Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.07818.jpg', 'data': {'categories': ['#alignment', '#optimization', '#video', '#multimodal', '#rl', '#diffusion', '#benchmark', '#rlhf'], 'emoji': 'üé®', 'ru': {'title': 'DanceGRPO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DanceGRPO - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ Group Relative Policy Optimization (GRPO) –∫ –∑–∞–¥–∞—á–∞–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. DanceGRPO —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–¥–∏–≥–º–∞–º–∏, –∑–∞–¥–∞—á–∞–º–∏, —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. DanceGRPO —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –ª—É—á—à–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'DanceGRPO: Unifying Reinforcement Learning for Visual Generation', 'desc': 'This paper presents DanceGRPO, a novel framework that enhances visual content generation by integrating Group Relative Policy Optimization (GRPO) with generative models like diffusion models and rectified flows. It addresses key challenges in reinforcement learning (RL) for visual generation, such as instability during training and compatibility with modern sampling methods. DanceGRPO is versatile, supporting multiple tasks including text-to-image and video generation, and utilizes various foundational and reward models to improve output quality. The framework shows significant performance improvements over existing methods, making it a promising solution for aligning generative models with human preferences in visual synthesis.'}, 'zh': {'title': 'DanceGRPOÔºöËßÜËßâÁîüÊàêÁöÑÁªü‰∏ÄÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜDanceGRPOÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Â∞ÜÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÂ∫îÁî®‰∫éËßÜËßâÁîüÊàêÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇÂÆÉËß£ÂÜ≥‰∫ÜÁé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑÊñπÊ≥ïÂú®Áé∞‰ª£Â∏∏ÂæÆÂàÜÊñπÁ®ãÔºàODEÔºâÈááÊ†∑„ÄÅËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåËßÜÈ¢ëÁîüÊàêÈ™åËØÅÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇDanceGRPOËÉΩÂ§üÂú®Êâ©Êï£Ê®°ÂûãÂíå‰øÆÊ≠£ÊµÅÁ≠âÂ§öÁßçÁîüÊàêËåÉÂºè‰∏≠Êó†ÁºùÈÄÇÂ∫îÔºåÂπ∂Âú®ÊñáÊú¨Âà∞ÂõæÂÉè„ÄÅÊñáÊú¨Âà∞ËßÜÈ¢ëÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂü∫Á∫øÔºåÂ±ïÁ§∫‰∫ÜÂú®ËßÜËßâÁîüÊàê‰ªªÂä°‰∏≠ÁªìÂêàÂº∫ÂåñÂ≠¶‰π†‰∏é‰∫∫Á±ªÂèçÈ¶àÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07263', 'title': 'Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2505.07263', 'abstract': 'We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.', 'score': 15, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9735af402c0df35f', 'authors': ['Xiaokun Wang', 'Chris', 'Jiangbo Pei', 'Wei Shen', 'Yi Peng', 'Yunzhuo Hao', 'Weijie Qiu', 'Ai Jian', 'Tianyidan Xie', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07263.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#alignment', '#open_source', '#training', '#architecture', '#dataset'], 'emoji': 'üåü', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Skywork-VL Reward - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ Qwen2.5-VL-7B-Instruct —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≥–æ–ª–æ–≤—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π. Skywork-VL Reward –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é Mixed Preference Optimization.'}, 'en': {'title': 'Skywork-VL Reward: Advancing Multimodal Understanding and Reasoning', 'desc': 'The paper introduces Skywork-VL Reward, a novel multimodal reward model designed to enhance understanding and reasoning across different types of data, such as text and images. It utilizes a large-scale dataset that captures diverse tasks, with feedback gathered from both traditional vision-language models and more sophisticated reasoning models. The architecture is built on Qwen2.5-VL-7B-Instruct, featuring a reward head and employing multi-stage fine-tuning with pairwise ranking loss to optimize performance. Experimental results demonstrate that this model not only excels in multimodal tasks but also improves training for Mixed Preference Optimization, marking a significant step forward in creating effective reward models for multimodal alignment.'}, 'zh': {'title': 'Skywork-VL RewardÔºöÂ§öÊ®°ÊÄÅÂØπÈΩêÁöÑÁ™ÅÁ†¥ÊÄßËøõÂ±ï', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜSkywork-VL RewardÔºåËøôÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºåÊó®Âú®‰∏∫Â§öÊ®°ÊÄÅÁêÜËß£ÂíåÊé®ÁêÜ‰ªªÂä°Êèê‰æõÂ•ñÂä±‰ø°Âè∑„ÄÇÊàë‰ª¨ÁöÑÊäÄÊúØÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™ÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜÔºöÈ¶ñÂÖàÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÂπøÊ≥õÁöÑ‰ªªÂä°ÂíåÂú∫ÊôØÔºåÊï∞ÊçÆÊù•Ëá™Ê†áÂáÜËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíåÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜÊ®°Âûã„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éQwen2.5-VL-7B-InstructÁöÑÂ•ñÂä±Ê®°ÂûãÊû∂ÊûÑÔºåÈõÜÊàê‰∫ÜÂ•ñÂä±Â§¥ÔºåÂπ∂Âú®ÊàêÂØπÂÅèÂ•ΩÊï∞ÊçÆ‰∏äÂ∫îÁî®‰∫ÜÂ§öÈò∂ÊÆµÂæÆË∞É„ÄÇÂÆûÈ™åËØÑ‰º∞Ë°®ÊòéÔºåSkywork-VL RewardÂú®Â§öÊ®°ÊÄÅVL-RewardBench‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂Âú®ÊñáÊú¨Â•ñÂä±Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03733', 'title': 'WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch', 'url': 'https://huggingface.co/papers/2505.03733', 'abstract': "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.", 'score': 15, 'issue_id': 3722, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '1d5e56d00ea8d485', 'authors': ['Zimu Lu', 'Yunqiao Yang', 'Houxing Ren', 'Haotian Hou', 'Han Xiao', 'Ke Wang', 'Weikang Shi', 'Aojun Zhou', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Multimedia Laboratory (MMLab), The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.03733.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#agents', '#dataset'], 'emoji': 'üåê', 'ru': {'title': 'WebGen-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏', 'desc': 'WebGen-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–µ –≤–µ–±-—Å–∞–π—Ç—ã —Å –Ω—É–ª—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ –ø–æ—á—Ç–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Ç–∏–ø—ã –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π. –ö–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö GPT-4o –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é. –õ—É—á—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è - Bolt.diy —Å DeepSeek-R1 - –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–ª—å–∫–æ 27,8% —Ç–æ—á–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–∞.'}, 'en': {'title': 'Benchmarking LLMs for Website Code Generation', 'desc': 'This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models.'}, 'zh': {'title': 'ËØÑ‰º∞LLM‰ª£ÁêÜÁîüÊàêÁΩëÁ´ô‰ª£Á†ÅÁöÑÊåëÊàò', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïWebGen-BenchÔºåÊó®Âú®ËØÑ‰º∞Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑ‰ª£ÁêÜÂú®‰ªéÈõ∂ÂºÄÂßãÂàõÂª∫Â§öÊñá‰ª∂ÁΩëÁ´ô‰ª£Á†ÅÂ∫ìÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´Â§öÊ†∑ÂåñÁöÑÁΩëÁ´ôÁîüÊàêÊåá‰ª§ÔºåÊ∂µÁõñ‰∫Ü‰∏âÂ§ßÁ±ªÂíåÂçÅ‰∏âÂ∞èÁ±ªÔºåÂá†‰πéÂåÖÊã¨ÊâÄÊúâÈáçË¶ÅÁ±ªÂûãÁöÑWebÂ∫îÁî®Á®ãÂ∫è„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞ÁîüÊàêÁΩëÁ´ôÁöÑË¥®ÈáèÔºå‰ΩøÁî®GPT-4oÁîüÊàêÈíàÂØπÊØè‰∏™ÂäüËÉΩÁöÑÊµãËØïÁî®‰æãÔºåÂπ∂ÊâãÂä®ËøáÊª§ÂíåË∞ÉÊï¥ÔºåÊúÄÁªàÂΩ¢Êàê647‰∏™ÊµãËØïÁî®‰æã„ÄÇÈÄöËøáÂº∫Â§ßÁöÑÁΩëÈ°µÂØºËà™‰ª£ÁêÜËá™Âä®ÊâßË°åÊµãËØïÔºåËØÑ‰º∞ÁîüÊàêÁΩëÁ´ôÁöÑÂìçÂ∫îÊòØÂê¶Á¨¶ÂêàÈ¢ÑÊúüÁªìÊûúÔºåÁªìÊûúÊòæÁ§∫ÊúÄ‰Ω≥Ê®°ÂûãÁªÑÂêàÁöÑÂáÜÁ°ÆÁéá‰ªÖ‰∏∫27.8%ÔºåÊòæÁ§∫Âá∫Âü∫ÂáÜÁöÑÊåëÊàòÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07796', 'title': 'Learning Dynamics in Continual Pre-Training for Large Language Models', 'url': 'https://huggingface.co/papers/2505.07796', 'abstract': 'Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.', 'score': 12, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'c63d617be0b4d13a', 'authors': ['Xingjin Wang', 'Howe Tissue', 'Lu Wang', 'Linjing Li', 'Daniel Dajun Zeng'], 'affiliations': ['Ritzz-AI', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07796.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training'], 'emoji': 'üìà', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ (CPT) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –Ω–∞–±–ª—é–¥–∞—é—Ç, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—â–∏—Ö –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∑–∞–∫–æ–Ω CPT, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç—ã —Å–º–µ—â–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –ª—é–±–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–µ–ª–µ–π CPT.'}, 'en': {'title': 'Unlocking the Dynamics of Continual Pre-Training', 'desc': "This paper investigates the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on how performance in general and specific domains changes during training. The authors analyze the CPT loss curve, revealing that it represents a transition between different performance states influenced by distribution shifts and learning rate adjustments. They propose a scaling law that predicts loss across various training steps and learning rate schedules, providing insights into key factors like peak learning rate and replay ratio. The findings are validated through extensive experiments, showing the law's applicability across different datasets and training configurations."}, 'zh': {'title': 'ÊåÅÁª≠È¢ÑËÆ≠ÁªÉÁöÑÂä®ÊÄÅ‰∏é‰ºòÂåñÊ≥ïÂàô', 'desc': 'ÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºàCPTÔºâÊòØ‰∏ÄÁßçÂ∞ÜÂº∫Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÂ∫îÁî®‰∫éÁâπÂÆö‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊúâÊïàÊñπÊ≥ï„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®CPTËøáÁ®ã‰∏≠ÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºåÁâπÂà´ÂÖ≥Ê≥®Âú®ÊØè‰∏™ËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÈÄöÁî®ÊÄßËÉΩÂíå‰∏ãÊ∏∏È¢ÜÂüüÊÄßËÉΩÁöÑÊºîÂèò„ÄÇÊàë‰ª¨ËßÇÂØüÂà∞CPTÊçüÂ§±Êõ≤Á∫øÊú¨Ë¥®‰∏äÊèèËø∞‰∫Ü‰ªé‰∏Ä‰∏™Êõ≤Á∫øÂà∞Âè¶‰∏Ä‰∏™ÈöêËóèÊõ≤Á∫øÁöÑËøáÊ∏°ÔºåÂπ∂ÈÄöËøáËß£ËÄ¶ÂàÜÂ∏ÉÂèòÂåñÂíåÂ≠¶‰π†ÁéáÈÄÄÁÅ´ÁöÑÂΩ±ÂìçÊù•ËøõË°åÊèèËø∞„ÄÇÊàë‰ª¨Êé®ÂØºÂá∫‰∫Ü‰∏ÄÁßçCPTÁº©ÊîæÊ≥ïÂàôÔºåÁªìÂêà‰∫ÜËøô‰∏§‰∏™Âõ†Á¥†Ôºå‰ΩøÂæóËÉΩÂ§üÈ¢ÑÊµãÂú®‰ªª‰ΩïÔºàÊåÅÁª≠ÔºâËÆ≠ÁªÉÊ≠•È™§ÂíåÂ≠¶‰π†ÁéáË∞ÉÂ∫¶‰∏ãÁöÑÊçüÂ§±„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07596', 'title': 'Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent', 'url': 'https://huggingface.co/papers/2505.07596', 'abstract': 'Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.', 'score': 10, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '10063104a79da512', 'authors': ['Ziyang Huang', 'Xiaowei Yuan', 'Yiming Ju', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.07596.jpg', 'data': {'categories': ['#reasoning', '#rag', '#agents', '#optimization', '#hallucinations', '#rl', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω—ã–π –ø–æ–∏—Å–∫: –∫–æ–≥–¥–∞ –∏—Å–∫–∞—Ç—å, –∞ –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä–∏—Ç—å—Å—è —Å–µ–±–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ retrieval-augmented generation (RAG). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–≥–µ–Ω—Ç–∞ IKEA, –∫–æ—Ç–æ—Ä—ã–π —Å–ø–æ—Å–æ–±–µ–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω–µ—à–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è –≥—Ä–∞–Ω–∏—Ü—ã –∑–Ω–∞–Ω–∏–π, –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ IKEA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –≤–Ω–µ—à–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Optimizing Knowledge Use in Language Models with IKEA', 'desc': 'This paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), a novel approach to enhance the performance of Large Language Models (LLMs) by optimizing their retrieval processes. IKEA intelligently determines when to use its internal knowledge versus when to perform external searches, reducing unnecessary retrievals and improving inference speed. The model employs a unique reward function that encourages effective use of internal knowledge while still allowing for external retrieval when needed. Evaluations show that IKEA not only outperforms existing methods but also generalizes well across various knowledge reasoning tasks.'}, 'zh': {'title': 'Êô∫ËÉΩÊ£ÄÁ¥¢Ôºå‰ºòÂåñÁü•ËØÜÂà©Áî®', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê®°ÂûãÔºåÂêç‰∏∫IKEAÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊ£ÄÁ¥¢ËÉΩÂäõ„ÄÇIKEAËÉΩÂ§üËØÜÂà´Ëá™Ë∫´Áü•ËØÜÁöÑËæπÁïåÔºåÂπ∂‰ºòÂÖà‰ΩøÁî®ÂÜÖÈÉ®Áü•ËØÜÔºåÂè™ÊúâÂú®ÂÜÖÈÉ®Áü•ËØÜ‰∏çË∂≥Êó∂Êâç‰ºöËøõË°åÂ§ñÈÉ®Ê£ÄÁ¥¢„ÄÇÈÄöËøáÂºïÂÖ•‰∏ÄÁßçÊñ∞ÁöÑÂ•ñÂä±ÂáΩÊï∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåIKEAËÉΩÂ§üÊúâÊïàÂáèÂ∞ëÂÜó‰ΩôÊ£ÄÁ¥¢ÔºåÊèêÈ´òÂõûÁ≠îÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIKEAÂú®Â§ö‰∏™Áü•ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÈôç‰Ωé‰∫ÜÊ£ÄÁ¥¢È¢ëÁéáÔºåÂπ∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06176', 'title': "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", 'url': 'https://huggingface.co/papers/2505.06176', 'abstract': 'Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.', 'score': 7, 'issue_id': 3733, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': '884e34691df4b88b', 'authors': ['Niladri Shekhar Dutt', 'Duygu Ceylan', 'Niloy J. Mitra'], 'affiliations': ['Adobe Research, UK', 'University College London, Adobe Research, UK', 'University College London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.06176.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#dataset', '#data', '#multimodal', '#interpretability', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ—Ç—É—à—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: MLLM –Ω–∞ —Å—Ç—Ä–∞–∂–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—Ç—É—à–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MLLM). –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—ã—Ä—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –∏ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏—Ö —Å –ø–æ–º–æ—â—å—é –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. MLLM —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø–æ–Ω—è—Ç–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Empowering Photo Retouching with Intelligent Editing Guidance', 'desc': 'This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.'}, 'zh': {'title': 'Âà©Áî®Â§öÊ®°ÊÄÅÊ®°ÂûãÊèêÂçáÁÖßÁâá‰øÆÈ•∞Ë¥®Èáè', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÊù•ÊîπËøõÂéüÂßãÁÖßÁâáÁöÑ‰øÆÈ•∞ËøáÁ®ã„ÄÇÊàë‰ª¨ËÆ≠ÁªÉMLLMÁêÜËß£ÂõæÂÉèÂ§ÑÁêÜÊìç‰ΩúÔºåÂπ∂ÈÄöËøáËß£ÂÜ≥ËßÜËßâÈöæÈ¢òÊù•Â¢ûÂº∫ÂÖ∂Êìç‰ΩúÊÑèËØÜ„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üËßÑÂàíÂíåÂª∫ËÆÆÁºñËæëÂ∫èÂàóÔºåÁ°Æ‰øù‰øÆÈ•∞Êìç‰ΩúÂØπÁî®Êà∑ÂèØÁêÜËß£ÔºåÂπ∂‰øùÁïôÂØπË±°ÁªÜËäÇÂíåÂàÜËæ®Áéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÁîüÊàêÂíåÁ®ãÂ∫èÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄßÂíåË∫´‰ªΩ‰øùÁïôÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07819', 'title': 'H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning', 'url': 'https://huggingface.co/papers/2505.07819', 'abstract': 'Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9285a87dc24d7d07', 'authors': ['Yiyang Lu', 'Yufeng Tian', 'Zhecheng Yuan', 'Xianbang Wang', 'Pu Hua', 'Zhengrong Xue', 'Huazhe Xu'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute', 'Tsinghua University IIIS'], 'pdf_title_img': 'assets/pdf/title_img/2505.07819.jpg', 'data': {'categories': ['#diffusion', '#agents', '#robotics', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–æ–º–æ—Ç–æ—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–æ–±–æ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–æ–º–æ—Ç–æ—Ä–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ - –¢—Ä–∏–µ–¥–∏–Ω—É—é –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –ü–æ–ª–∏—Ç–∏–∫—É (H^3DP). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –∏–µ—Ä–∞—Ä—Ö–∏—é: –ø–æ—Å–ª–æ–π–Ω—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –≥–ª—É–±–∏–Ω—ã, –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. H^3DP —É—Å–∏–ª–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –¥–µ–π—Å—Ç–≤–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –≤ —Å–∏–º—É–ª—è—Ü–∏–∏, —Ç–∞–∫ –∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏.'}, 'en': {'title': 'Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration', 'desc': 'This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.'}, 'zh': {'title': 'Â¢ûÂº∫ËßÜËßâ‰∏éÂä®‰ΩúÁîüÊàêÁöÑ‰∏âÂ±ÇÊ¨°Â≠¶‰π†Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâËøêÂä®Á≠ñÁï•Â≠¶‰π†Ê°ÜÊû∂ÔºåÁß∞‰∏∫‰∏âÂ±ÇÊ¨°Êâ©Êï£Á≠ñÁï•ÔºàH^{\textbf{3}DPÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•Â±ÇÊ¨°ÁªìÊûÑÔºåÂ¢ûÂº∫‰∫ÜËßÜËßâÁâπÂæÅ‰∏éÂä®‰ΩúÁîüÊàê‰πãÈó¥ÁöÑÁªìÂêà„ÄÇH^{3}DPÂåÖÂê´‰∏â‰∏™Â±ÇÊ¨°ÔºöÂü∫‰∫éÊ∑±Â∫¶‰ø°ÊÅØÁöÑËæìÂÖ•ÂàÜÂ±Ç„ÄÅÂ§öÂ∞∫Â∫¶ËßÜËßâË°®Á§∫ÂíåÂ±ÇÊ¨°Êù°‰ª∂Êâ©Êï£ËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåH^{3}DPÂú®44‰∏™‰ªøÁúü‰ªªÂä°‰∏≠Áõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÂπ≥ÂùáÊèêÈ´ò‰∫Ü27.5%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®Âõõ‰∏™Â§çÊùÇÁöÑÂèåÊâãÁúüÂÆû‰∏ñÁïåÊìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07260', 'title': 'UMoE: Unifying Attention and FFN with Shared Experts', 'url': 'https://huggingface.co/papers/2505.07260', 'abstract': 'Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '6a820b0cd1c07ac4', 'authors': ['Yuanhang Yang', 'Chaozheng Wang', 'Jing Li'], 'affiliations': ['Hong Kong Polytechnic University, Hong Kong, China', 'Institute of Science Tokyo, Tokyo, Japan', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07260.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è MoE –≤ Transformer: –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (Sparse Mixture of Experts, MoE) –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π Transformer. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∑–∞–π–Ω MoE –¥–ª—è —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—è –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è. –ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –Ω–∞–∑–≤–∞–Ω–Ω–∞—è UMoE, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç MoE –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è. UMoE —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º–∏ —Å–ª–æ—è–º–∏.'}, 'en': {'title': 'Unifying MoE for Enhanced Transformer Performance', 'desc': 'This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.'}, 'zh': {'title': 'Áªü‰∏ÄÊ≥®ÊÑèÂäõ‰∏éÂâçÈ¶àÁΩëÁªúÁöÑÁ®ÄÁñè‰∏ìÂÆ∂Êû∂ÊûÑ', 'desc': 'Á®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊû∂ÊûÑÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊâ©Â±ïTransformerÊ®°Âûã„ÄÇËôΩÁÑ∂Êó©ÊúüÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂ∞ÜMoEÂ∫îÁî®‰∫éÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÂ±ÇÔºå‰ΩÜÊúÄËøëÁöÑÁ†îÁ©∂ÂºÄÂßãÊé¢Á¥¢Â∞ÜMoEÊâ©Â±ïÂà∞Ê≥®ÊÑèÂäõÂ±ÇÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑMoEÂ±ÇÈúÄË¶Å‰∏ìÈó®ÁöÑÂÆûÁé∞ÔºåÂπ∂‰∏î‰∏éÂü∫‰∫éFFNÁöÑÂ±ÇÁõ∏ÊØîÔºåÊÄßËÉΩ‰∏çÂ∞ΩÂ¶Ç‰∫∫ÊÑè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÈáçÊûÑÔºåÁªü‰∏Ä‰∫ÜÊ≥®ÊÑèÂäõÂ±ÇÂíåFFNÂ±Ç‰∏≠ÁöÑMoEËÆæËÆ°ÔºåÊèêÂá∫ÁöÑUMoEÊû∂ÊûÑÈÄöËøáÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑMoEÂ±ÇÂÆûÁé∞‰∫ÜÊõ¥‰ºòÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜFFNÂíåÊ≥®ÊÑèÂäõÁªÑ‰ª∂‰πãÈó¥ÁöÑÈ´òÊïàÂèÇÊï∞ÂÖ±‰∫´„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00612', 'title': 'Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation', 'url': 'https://huggingface.co/papers/2505.00612', 'abstract': 'In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\\em leakage} and {\\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.', 'score': 5, 'issue_id': 3733, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '98b0a79b86c5cd15', 'authors': ['D. Sculley', 'Will Cukierski', 'Phil Culliton', 'Sohier Dane', 'Maggie Demkin', 'Ryan Holbrook', 'Addison Howard', 'Paul Mooney', 'Walter Reade', 'Megan Risdal', 'Nate Keating'], 'affiliations': ['Kaggle, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.00612.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#evaluation'], 'emoji': 'üèÜ', 'ru': {'title': '–°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –ò–ò - –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫—Ä–∏–∑–∏—Å –≤ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –∏ –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏—Å—Ç–∏–Ω—ã –∏ —Å–∏–ª—å–Ω—ã–º–∏ –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞–º —É—Ç–µ—á–∫–∏ –∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò. –°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –ò–ò –∫–∞–∫ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏ –≤ –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò.'}, 'en': {'title': 'Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI', 'desc': 'This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.'}, 'zh': {'title': 'ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜÔºö‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÁöÑ‰ª∑ÂÄº', 'desc': 'Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨ËßÇÂØüÂà∞ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆûËØÅËØÑ‰º∞Ê≠£Èù¢‰∏¥Âç±Êú∫ÔºåÂõ†‰∏∫‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ËØÑ‰º∞ÂíåÂü∫ÂáÜÁ≠ñÁï•Êó†Ê≥ïÊª°Ë∂≥Áé∞‰ª£ÁîüÊàêÊÄßAIÊ®°ÂûãÂíåÁ≥ªÁªüÁöÑËØÑ‰º∞ÈúÄÊ±Ç„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÂÖ∑ÊúâÂá†‰πéÊó†ÈôêÁöÑËæìÂÖ•ÂíåËæìÂá∫Á©∫Èó¥ÔºåÁº∫‰πèÊòéÁ°ÆÁöÑÁúüÂÆûÁõÆÊ†áÔºåÂπ∂‰∏îÂú®È¢ÑÊµãÊó∂Âº∫ÁÉà‰æùËµñ‰∫é‰πãÂâçÊ®°ÂûãËæìÂá∫ÁöÑ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÊåáÂá∫ÔºåÊ≥ÑÊºèÂíåÊ±°ÊüìÈóÆÈ¢òÊòØÁîüÊàêÊÄßAIËØÑ‰º∞‰∏≠ÊúÄÈáçË¶Å‰∏îÊúÄÈöæËß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ËÆ§‰∏∫Ôºå‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÈ¢ÜÂüüÂ∑≤ÁªèÂèëÂ±ïÂá∫ÊúâÊïàÁöÑÊé™ÊñΩÊù•Â∫îÂØπÊ≥ÑÊºèÈóÆÈ¢òÔºåÂõ†Ê≠§Â∫îÂ∞ÜÂÖ∂ËßÜ‰∏∫ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÈªÑÈáëÊ†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07812', 'title': 'Continuous Visual Autoregressive Generation via Score Maximization', 'url': 'https://huggingface.co/papers/2505.07812', 'abstract': 'Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.', 'score': 3, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '5635f18df39cf275', 'authors': ['Chenze Shao', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07812.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#training', '#architecture'], 'emoji': 'üîÑ', 'ru': {'title': '–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é (VAR) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Continuous VAR, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ç—Ä–æ–≥–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª–∞—Ö –æ—Ü–µ–Ω–∫–∏ (strictly proper scoring rules). –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ (energy score), –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ VAR, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø–æ—Ç–µ—Ä–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Revolutionizing Visual Data Generation with Continuous VAR', 'desc': 'This paper presents a new approach called Continuous Visual AutoRegressive (VAR) modeling, which allows for the generation of visual data without converting it into discrete formats. Traditional methods often lose important information during quantization, but this framework uses strictly proper scoring rules to evaluate and optimize generative models directly in continuous space. By focusing on energy scores as training objectives, the method avoids the challenges of probabilistic predictions in continuous domains. Additionally, it shows that previous continuous autoregressive methods can be derived from this new framework, enhancing their applicability and performance.'}, 'zh': {'title': 'Êó†ÈáèÂåñÁöÑËøûÁª≠Ëá™ÂõûÂΩíÁîüÊàêÊñ∞Ê°ÜÊû∂', 'desc': '‰º†ÁªüËßÇÁÇπËÆ§‰∏∫Ëá™ÂõûÂΩíÊ®°Âûã‰∏ªË¶ÅÁî®‰∫éÂ§ÑÁêÜÁ¶ªÊï£Êï∞ÊçÆ„ÄÇÂú®Â§ÑÁêÜËøûÁª≠Êï∞ÊçÆÔºàÂ¶ÇËßÜËßâÊï∞ÊçÆÔºâÊó∂ÔºåËßÜËßâËá™ÂõûÂΩíÂª∫Ê®°ÔºàVARÔºâÈÄöÂ∏∏ÈúÄË¶ÅÈÄöËøáÈáèÂåñÊñπÊ≥ïÂ∞ÜÊï∞ÊçÆËΩ¨Êç¢‰∏∫Á¶ªÊï£Á©∫Èó¥ÔºåËøôÂèØËÉΩÂØºËá¥‰ø°ÊÅØÊçüÂ§±„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËøûÁª≠VARÊ°ÜÊû∂ÔºåËÉΩÂ§üÁõ¥Êé•ËøõË°åËßÜËßâËá™ÂõûÂΩíÁîüÊàêÔºåËÄåÊó†ÈúÄÂêëÈáèÈáèÂåñ„ÄÇËØ•Ê°ÜÊû∂ÁöÑÁêÜËÆ∫Âü∫Á°ÄÊòØ‰∏•Ê†ºÈÄÇÂΩìÁöÑËØÑÂàÜËßÑÂàôÔºåËøô‰∏∫ËØÑ‰º∞ÁîüÊàêÊ®°ÂûãÂ¶Ç‰ΩïÈÄºËøëÁúüÂÆûÂàÜÂ∏ÉÊèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÁªüËÆ°Â∑•ÂÖ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07793', 'title': 'Overflow Prevention Enhances Long-Context Recurrent LLMs', 'url': 'https://huggingface.co/papers/2505.07793', 'abstract': 'A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.', 'score': 3, 'issue_id': 3728, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'f4bfefd5343cbf0c', 'authors': ['Assaf Ben-Kish', 'Itamar Zimerman', 'M. Jehanzeb Mirza', 'James Glass', 'Leonid Karlinsky', 'Raja Giryes'], 'affiliations': ['IBM Research', 'MIT CSAIL', 'Tel Aviv University', 'Xero'], 'pdf_title_img': 'assets/pdf/title_img/2505.07793.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture', '#long_context'], 'emoji': 'üß†', 'ru': {'title': '–ß–∞–Ω–∫–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å—É–±-–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ —á–∞–Ω–∫–∞–º, –≤—ã–±–∏—Ä–∞—é—â–∏–π –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä—è–¥–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ LongBench. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞–≤—è—Ç –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'Unlocking Long-Context Potential with Chunk-Based Inference', 'desc': 'This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.'}, 'zh': {'title': 'ÊèêÂçáÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøëÔºåÈïøÊñáÊú¨Ê®°ÂûãÔºàLLMsÔºâÂèëÂ±ïÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ≠ê‰∫åÊ¨°Ê®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÁöÑÊïàÁéá„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü‰∏ªË¶ÅÁöÑÈïø‰∏ä‰∏ãÊñáÊ®°ÂûãÔºåÈáçÁÇπÂÖ≥Ê≥®ÂÆÉ‰ª¨Âõ∫ÂÆöÂ§ßÂ∞èÁöÑÈÄíÂΩíËÆ∞ÂøÜÂ¶Ç‰ΩïÂΩ±ÂìçÊÄßËÉΩ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøËøô‰∫õÊ®°ÂûãÁªèËøáÈïø‰∏ä‰∏ãÊñáËÆ≠ÁªÉÔºåÂÆÉ‰ª¨ÂØπÈïø‰∏ä‰∏ãÊñáÁöÑÂà©Áî®‰ªçÁÑ∂‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÂü∫‰∫éÂùóÁöÑÊé®ÁêÜÊñπÊ≥ïËÉΩÂ§üËØÜÂà´Âπ∂Â§ÑÁêÜËæìÂÖ•‰∏≠ÊúÄÁõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºå‰ªéËÄåÊúâÊïàÁºìËß£ÈÄíÂΩíËÆ∞ÂøÜÁöÑ‰∏çË∂≥ÔºåÂπ∂Âú®Â§ö‰∏™Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠ÂèñÂæóÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06324', 'title': 'Document Attribution: Examining Citation Relationships using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.06324', 'abstract': "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.   To tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.", 'score': 3, 'issue_id': 3724, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': 'd1b4a407c1a67da8', 'authors': ['Vipula Rawte', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka'], 'affiliations': ['Adobe Inc.', 'Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.06324.jpg', 'data': {'categories': ['#training', '#interpretability', '#multimodal', '#hallucinations'], 'emoji': 'üîç', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ LLM: –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ø–æ–¥—Ö–æ–¥ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∫–ª—é—á–µ–Ω–∏–∏, –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è. –ü–µ—Ä–≤—ã–π –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π flan-ul2, –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 0,27% –∏ 2,4% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö AttributionBench. –í—Ç–æ—Ä–æ–π –º–µ—Ç–æ–¥, –ø—Ä–∏–º–µ–Ω—è—é—â–∏–π –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å flan-t5-small, –ø—Ä–µ–≤–∑–æ—à–µ–ª –±–∞–∑–æ–≤—É—é –ª–∏–Ω–∏—é –ø–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é F1 –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö —Å–ª–æ—è—Ö, –∫—Ä–æ–º–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö.'}, 'en': {'title': 'Enhancing Trust in LLMs through Improved Attribution Techniques', 'desc': 'This paper addresses the challenges of trustworthiness and interpretability in Large Language Models (LLMs) when used for document-based tasks. It introduces two techniques for improving attribution, which is the process of linking model outputs back to their source documents. The first technique is a zero-shot approach that treats attribution as a textual entailment task, showing measurable improvements in performance. The second technique investigates how the attention mechanism in LLMs can enhance attribution accuracy, achieving better F1 scores in most layers of a smaller model.'}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØ‰ø°ÊÄß‰∏éÂèØËß£ÈáäÊÄß', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊ°£ÊëòË¶Å„ÄÅÈóÆÁ≠îÂíå‰ø°ÊÅØÊèêÂèñÁ≠â‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®Êó•ÁõäÂ¢ûÂ§öÔºåÁ°Æ‰øùËøô‰∫õÁ≥ªÁªüÁöÑÂèØ‰ø°ÊÄßÂíåÂèØËß£ÈáäÊÄßÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂΩíÂõ†ÊñπÊ≥ïÔºåÈÄöËøáËøΩË∏™ÁîüÊàêÁöÑËæìÂá∫ÂõûÂà∞ÂÖ∂Ê∫êÊñáÊ°£Êù•Ëß£ÂÜ≥Ëøô‰∏ÄÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊäÄÊúØÔºö‰∏ÄÁßçÊòØÈõ∂Ê†∑Êú¨ÊñπÊ≥ïÔºåÂ∞ÜÂΩíÂõ†ËßÜ‰∏∫ÁÆÄÂçïÁöÑÊñáÊú¨Ëï¥Âê´‰ªªÂä°ÔºåÂè¶‰∏ÄÁßçÊòØÊé¢Á¥¢Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®Â¢ûÂº∫ÂΩíÂõ†ËøáÁ®ã‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07291', 'title': 'INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.07291', 'abstract': 'We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training.', 'score': 2, 'issue_id': 3742, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'bf4c666ff6a739cf', 'authors': ['Prime Intellect Team', 'Sami Jaghouar', 'Justus Mattern', 'Jack Min Ong', 'Jannik Straube', 'Manveer Basra', 'Aaron Pazdera', 'Kushal Thaman', 'Matthew Di Ferrante', 'Felix Gabriel', 'Fares Obeid', 'Kemal Erdem', 'Michael Keiblinger', 'Johannes Hagemann'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07291.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#rl', '#dataset', '#reasoning'], 'emoji': 'üåê', 'ru': {'title': '–ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'INTELLECT-2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—É—é –≥–ª–æ–±–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 32 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, INTELLECT-2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –¥–∏–Ω–∞–º–∏—á–Ω–æ–º, –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–º —Ä–æ–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —É–∑–ª–æ–≤. –î–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ PRIME-RL, —Å–∏—Å—Ç–µ–º—É –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ TOPLOC –∏ –º–µ—Ö–∞–Ω–∏–∑–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ SHARDCAST. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ GRPO –∏ –º–µ—Ç–æ–¥—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Reinforcement Learning with Distributed Training', 'desc': 'INTELLECT-2 is a groundbreaking reinforcement learning model that utilizes a globally distributed training approach for a 32 billion parameter language model. It employs asynchronous training across a diverse group of independent compute contributors, which is a shift from traditional centralized methods. The paper introduces innovative components like PRIME-RL for managing distributed training, TOPLOC for verifying data from untrusted sources, and SHARDCAST for efficient communication of model updates. By refining the GRPO training method and implementing effective data filtering, INTELLECT-2 achieves enhanced stability and performance, surpassing previous models in its category.'}, 'zh': {'title': 'INTELLECT-2ÔºöÂÖ®ÁêÉÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜINTELLECT-2ÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®ÁêÉÂàÜÂ∏ÉÂºèÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºå‰ΩøÁî®‰∫Ü320‰∫øÂèÇÊï∞ÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰∏é‰º†ÁªüÁöÑÈõÜ‰∏≠ÂºèËÆ≠ÁªÉ‰∏çÂêåÔºåINTELLECT-2ÈÄöËøá‰∏Ä‰∏™Âä®ÊÄÅÁöÑ„ÄÅÂºÇÊûÑÁöÑËÆ°ÁÆóË¥°ÁåÆËÄÖÁæ§‰ΩìÔºåÈááÁî®ÂÆåÂÖ®ÂºÇÊ≠•ÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•ËÆ≠ÁªÉÊé®ÁêÜÊ®°Âûã„ÄÇ‰∏∫‰∫ÜÊîØÊåÅËøôÁßçÁã¨ÁâπÁöÑÂü∫Á°ÄËÆæÊñΩÔºåÊàë‰ª¨‰ªéÂ§¥ÂºÄÂßãÊûÑÂª∫‰∫ÜÂ§ö‰∏™ÁªÑ‰ª∂ÔºåÂåÖÊã¨‰∏ì‰∏∫ÂàÜÂ∏ÉÂºèÂºÇÊ≠•Âº∫ÂåñÂ≠¶‰π†ËÆæËÆ°ÁöÑPRIME-RLËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰ª•ÂèäÈ™åËØÅ‰∏çÂèØ‰ø°Êé®ÁêÜÂ∑•‰ΩúËÄÖÁöÑTOPLOCÂíåÈ´òÊïàÂπøÊí≠Á≠ñÁï•ÊùÉÈáçÁöÑSHARDCAST„ÄÇÊàë‰ª¨ËøòÂØπÊ†áÂáÜÁöÑGRPOËÆ≠ÁªÉÊñπÊ≥ïÂíåÊï∞ÊçÆËøáÊª§ÊäÄÊúØËøõË°å‰∫Ü‰øÆÊîπÔºå‰ª•Á°Æ‰øùËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÔºåÂπ∂ÊàêÂäüÂÆûÁé∞Ê®°ÂûãÁöÑËÆ≠ÁªÉÁõÆÊ†áÔºå‰ªéËÄåÂú®320‰∫øÂèÇÊï∞ËåÉÂõ¥ÂÜÖË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑQwQ-32BÊé®ÁêÜÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07233', 'title': 'DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2505.07233', 'abstract': 'Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG', 'score': 2, 'issue_id': 3734, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'b6c55b4c738d5230', 'authors': ['Jiashuo Sun', 'Xianrui Zhong', 'Sizhe Zhou', 'Jiawei Han'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07233.jpg', 'data': {'categories': ['#rag', '#optimization', '#interpretability', '#rl'], 'emoji': 'üîÑ', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º', 'desc': 'DynamicRAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG), –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –ø–æ—Ä—è–¥–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. DynamicRAG –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å–µ–º–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—à–∏—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'DynamicRAG: Optimizing Document Retrieval for Better Generation', 'desc': "This paper introduces DynamicRAG, a new framework for retrieval-augmented generation (RAG) systems that enhances the quality of generated responses by optimizing the reranking of retrieved documents. The reranker in DynamicRAG is designed to dynamically adjust both the order and the number of documents based on the specific query, addressing the challenge of selecting the optimal number of documents. By employing reinforcement learning, the reranker uses feedback from the quality of the language model's output to improve its decisions. The results show that DynamicRAG outperforms existing methods across multiple knowledge-intensive datasets, achieving state-of-the-art performance."}, 'zh': {'title': 'Âä®ÊÄÅË∞ÉÊï¥ÔºåÊèêÂçáÁîüÊàêË¥®ÈáèÁöÑRAGÊ°ÜÊû∂', 'desc': 'Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÁªìÂêà‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåÂ§ñÈÉ®Áü•ËØÜÊ£ÄÁ¥¢ÔºåÈÄÇÁî®‰∫éÁü•ËØÜÂØÜÈõÜÂûã‰ªªÂä°„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑRAGÊ°ÜÊû∂DynamicRAGÔºåÂÖ∂‰∏≠ÁöÑÈáçÊéíÂ∫èÂô®ËÉΩÂ§üÊ†πÊçÆÊü•ËØ¢Âä®ÊÄÅË∞ÉÊï¥Ê£ÄÁ¥¢ÊñáÊ°£ÁöÑÈ°∫Â∫èÂíåÊï∞Èáè„ÄÇÊàë‰ª¨Â∞ÜÈáçÊéíÂ∫èÂô®Âª∫Ê®°‰∏∫‰∏Ä‰∏™ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰ºòÂåñÁöÑÊô∫ËÉΩ‰ΩìÔºåÂà©Áî®LLMËæìÂá∫Ë¥®Èáè‰Ωú‰∏∫Â•ñÂä±Êù•‰ºòÂåñÈáçÊéíÂ∫èÂÜ≥Á≠ñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDynamicRAGÂú®‰∏É‰∏™Áü•ËØÜÂØÜÈõÜÂûãÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04918', 'title': 'Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction', 'url': 'https://huggingface.co/papers/2505.04918', 'abstract': "Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.", 'score': 2, 'issue_id': 3728, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '73efdd4a4328c88d', 'authors': ['Jiaqi Zheng', 'Qing Ling', 'Yerong Feng'], 'affiliations': ['Shenzhen Institute of Meteorological Innovation', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04918.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#graphs', '#architecture', '#training'], 'emoji': 'üåé', 'ru': {'title': 'PASSAT: –§–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã —Å —É—á–µ—Ç–æ–º —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –ó–µ–º–ª–∏', 'desc': 'PASSAT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≥–æ–¥—ã, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è —Ñ–∏–∑–∏–∫—É –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ —Ç–æ–ø–æ–ª–æ–≥–∏—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ó–µ–º–ª–∏. –ú–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–¥–≤–µ–∫—Ü–∏–∏ –∏ –ù–∞–≤—å–µ-–°—Ç–æ–∫—Å–∞ –Ω–∞ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ñ–µ—Ä–∏—á–µ—Å–∫—É—é –≥—Ä–∞—Ñ–æ–≤—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ó–µ–º–ª–∏ –∏ –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã. PASSAT –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–æ–ª—è —Å–∫–æ—Ä–æ—Å—Ç–µ–π, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–¥–≤–µ–∫—Ü–∏–∏. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö ERA5 —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 5.625¬∞ PASSAT –ø—Ä–µ–≤–∑–æ—à–ª–∞ –∫–∞–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ–≥–æ–¥—ã IFS T42.'}, 'en': {'title': 'PASSAT: Bridging Physics and Topology for Superior Weather Prediction', 'desc': "The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth's surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns."}, 'zh': {'title': 'PASSATÔºöÁªìÂêàÁâ©ÁêÜ‰∏éÊãìÊâëÁöÑÂ§©Ê∞îÈ¢ÑÊµãÊñ∞Ê®°Âûã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§©Ê∞îÈ¢ÑÊµãÊ®°ÂûãPASSATÔºåËØ•Ê®°ÂûãÁªìÂêà‰∫ÜÁâ©ÁêÜÂ≠¶ÂíåÂú∞ÂΩ¢‰ø°ÊÅØ„ÄÇPASSATÈÄöËøáÂØπÊµÅËøáÁ®ãÂíåÂú∞ÁêÉ-Â§ßÊ∞îÁõ∏‰∫í‰ΩúÁî®Êù•ÊèèËø∞Â§©Ê∞îÊºîÂèòÔºåÂπ∂ËÄÉËôë‰∫ÜÂú∞ÁêÉË°®Èù¢ÁöÑÊãìÊâëÁªìÊûÑ„ÄÇËØ•Ê®°ÂûãÂú®ÁêÉÈù¢ÊµÅÂΩ¢‰∏äÊï∞ÂÄºÊ±ÇËß£ÂØπÊµÅÊñπÁ®ãÂíåÁ∫≥Áª¥-ÊñØÊâòÂÖãÊñØÊñπÁ®ãÔºåÂπ∂Âà©Áî®ÁêÉÈù¢ÂõæÁ•ûÁªèÁΩëÁªúÊçïÊçâÂú∞ÁêÉ-Â§ßÊ∞îÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPASSATÂú®5.625Â∫¶ÂàÜËæ®ÁéáÁöÑERA5Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊ∑±Â∫¶Â≠¶‰π†Â§©Ê∞îÈ¢ÑÊµãÊ®°ÂûãÂíåÊìç‰ΩúÊÄßÊï∞ÂÄºÂ§©Ê∞îÈ¢ÑÊµãÊ®°ÂûãIFS T42„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04066', 'title': 'LLAMAPIE: Proactive In-Ear Conversation Assistants', 'url': 'https://huggingface.co/papers/2505.04066', 'abstract': 'We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.', 'score': 1, 'issue_id': 3739, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': 'f8bf204612751793', 'authors': ['Tuochao Chen', 'Nicholas Batchelder', 'Alisa Liu', 'Noah Smith', 'Shyamnath Gollakota'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.04066.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#dataset', '#data'], 'emoji': 'üéß', 'ru': {'title': 'LlamaPIE: –ù–µ–∑–∞–º–µ—Ç–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∂–∏–≤—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤', 'desc': 'LlamaPIE - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –Ω–µ–∑–∞–º–µ—Ç–Ω—ã–µ, –∫—Ä–∞—Ç–∫–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–µ —á–µ—Ä–µ–∑ —Å–ª—É—Ö–æ–≤—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —ç—Ç–æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ, –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±–µ–∑ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ. –û—Ü–µ–Ω–∫–∞ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å LlamaPIE –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–ª–µ–∑–Ω–æ–π, –Ω–µ–Ω–∞–≤—è–∑—á–∏–≤–æ–π –ø–æ–º–æ—â–∏.'}, 'en': {'title': 'LlamaPIE: Enhancing Conversations with Proactive Assistance', 'desc': 'LlamaPIE is a novel real-time proactive assistant that enhances human conversations by providing discreet guidance through hearable devices. Unlike traditional models that wait for user prompts, LlamaPIE anticipates user needs and offers assistance without interrupting the flow of conversation. The system tackles challenges such as timing for responses, generating concise and relevant replies, and utilizing user context for personalized support, all while processing information on-device. Evaluations show that users prefer LlamaPIE over reactive models, indicating its effectiveness in improving live interactions.'}, 'zh': {'title': 'LlamaPIEÔºöÊèêÂçáÂØπËØùÁöÑ‰∏ªÂä®Âä©Êâã', 'desc': 'LlamaPIE ÊòØÈ¶ñ‰∏™ÂÆûÊó∂‰∏ªÂä®Âä©ÊâãÔºåÊó®Âú®ÈÄöËøáÂèØÁ©øÊà¥ËÆæÂ§áÂú®ÂØπËØù‰∏≠Êèê‰æõÁÆÄÊ¥ÅÁöÑÊåáÂØº„ÄÇ‰∏é‰º†ÁªüËØ≠Ë®ÄÊ®°Âûã‰∏çÂêåÔºåÂÆÉÂú®ÂêéÂè∞ËøêË°åÔºåËÉΩÂ§üÈ¢ÑÊµãÁî®Êà∑ÈúÄÊ±ÇËÄå‰∏çÊâìÊñ≠ÂØπËØù„ÄÇÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜÂ§ö‰∏™ÊåëÊàòÔºåÂåÖÊã¨‰ΩïÊó∂ÂìçÂ∫î„ÄÅÂ¶Ç‰ΩïÁîüÊàêÁÆÄÊ¥ÅÁöÑÂõûÂ∫î‰ª•ÂèäÂ¶Ç‰ΩïÂà©Áî®Áî®Êà∑Áü•ËØÜÊèê‰æõ‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂ∏ÆÂä©„ÄÇÈÄöËøáÊûÑÂª∫ÂçäÂêàÊàêÂØπËØùÊï∞ÊçÆÈõÜÂíåÊèêÂá∫ÂèåÊ®°ÂûãÁÆ°ÈÅìÔºåÊàë‰ª¨Âú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÁî®Êà∑Á†îÁ©∂ÊòæÁ§∫Âá∫ÂØπ‰∏ªÂä®Âä©ÊâãÁöÑÂº∫ÁÉàÂÅèÂ•Ω„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07086', 'title': 'Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design', 'url': 'https://huggingface.co/papers/2505.07086', 'abstract': "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.", 'score': 0, 'issue_id': 3725, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 –º–∞—è', 'en': 'May 11', 'zh': '5Êúà11Êó•'}, 'hash': 'a2fce171208a1e7a', 'authors': ['Tong Chen', 'Yinuo Zhang', 'Sophia Tang', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer Science, Fudan University', 'Management and Technology Program, University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2505.07086.jpg', 'data': {'categories': ['#training', '#dataset', '#science', '#data', '#optimization'], 'emoji': 'üß¨', 'ru': {'title': '–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏. MOG-DFM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –∏ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –∫ –ü–∞—Ä–µ—Ç–æ-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞–º –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–∫–∞–ª—è—Ä–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ–ø—Ç–∏–¥–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –î–ù–ö —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏. MOG-DFM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–µ–±—è –º–æ—â–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞ –±–∏–æ–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.'}, 'en': {'title': 'Optimizing Biomolecule Design with MOG-DFM', 'desc': "This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering."}, 'zh': {'title': 'Â§öÁõÆÊ†á‰ºòÂåñÔºåÂä©ÂäõÁîüÁâ©ÂàÜÂ≠êËÆæËÆ°', 'desc': 'Âú®ÁîüÁâ©ÂàÜÂ≠êÂ∑•Á®ã‰∏≠ÔºåËÆæËÆ°Êª°Ë∂≥Â§öÁßçÂäüËÉΩÂíåÁîüÁâ©Áâ©ÁêÜÊ†áÂáÜÁöÑÁîüÁâ©Â∫èÂàó‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§öÁõÆÊ†áÂºïÂØºÁ¶ªÊï£ÊµÅÂåπÈÖçÔºàMOG-DFMÔºâÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®Â§ö‰∏™Ê†áÈáèÁõÆÊ†á‰πãÈó¥ÂÆûÁé∞Â∏ïÁ¥ØÊâòÊúâÊïàÁöÑÊùÉË°°„ÄÇMOG-DFMÈÄöËøáËÆ°ÁÆóÊ∑∑ÂêàÊéíÂêçÊñπÂêëÂàÜÊï∞ÂíåÂ∫îÁî®Ëá™ÈÄÇÂ∫îË∂ÖÈî•ËøáÊª§Âô®ÔºåÊù•ÂºïÂØºÈ¢ÑËÆ≠ÁªÉÁöÑÁ¶ªÊï£Êó∂Èó¥ÊµÅÂåπÈÖçÁîüÊàêÂô®ËøõË°åÂ§öÁõÆÊ†á‰ºòÂåñ„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜMOG-DFMÂú®ÁîüÊàê‰ºòÂåñÁöÑËÇΩÁªìÂêàÁâ©ÂíåÁâπÂÆöÂ¢ûÂº∫Â≠êÁ±ªDNAÂ∫èÂàóÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§öÂ±ûÊÄßÂºïÂØºÁöÑÁîüÁâ©ÂàÜÂ≠êÂ∫èÂàóËÆæËÆ°‰∏≠ÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19897', 'title': 'ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows', 'url': 'https://huggingface.co/papers/2505.19897', 'abstract': "ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.", 'score': 75, 'issue_id': 4002, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '7903b97e8a51b382', 'authors': ['Qiushi Sun', 'Zhoumianze Liu', 'Chang Ma', 'Zichen Ding', 'Fangzhi Xu', 'Zhangyue Yin', 'Haiteng Zhao', 'Zhenyu Wu', 'Kanzhi Cheng', 'Zhaoyang Liu', 'Jianing Wang', 'Qintong Li', 'Xiangru Tang', 'Tianbao Xie', 'Xiachong Feng', 'Xiang Li', 'Ben Kao', 'Wenhai Wang', 'Biqing Qi', 'Lingpeng Kong', 'Zhiyong Wu'], 'affiliations': ['East China Normal University', 'Fudan University', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19897.jpg', 'data': {'categories': ['#science', '#multimodal', '#agents', '#benchmark'], 'emoji': 'üß™', 'ru': {'title': 'ScienceBoard: —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö', 'desc': 'ScienceBoard –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é —Å—Ä–µ–¥—É –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –¥–∏–Ω–∞–º–∏—á–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –±–∏–æ—Ö–∏–º–∏—è, –∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è –∏ –≥–µ–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 169 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –æ–Ω–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª–∏—à—å 15% –æ–±—â–µ–≥–æ —É—Ä–æ–≤–Ω—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ç–µ–∫—É—â–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Unlocking the Potential of LLMs in Scientific Workflows', 'desc': 'The paper introduces ScienceBoard, a platform designed to evaluate the performance of Large Language Model (LLM)-based agents in scientific workflows. It features a realistic environment with dynamic tasks across various scientific domains, allowing agents to interact with professional software. Despite the advancements in LLMs, evaluations reveal that these agents struggle with complex tasks, achieving only a 15% success rate. The study highlights the need for improved design principles to enhance the capabilities of these agents in aiding scientific discovery.'}, 'zh': {'title': 'ÁßëÂ≠¶ÂèëÁé∞ÁöÑÊñ∞Âä©ÊâãÔºöScienceBoard', 'desc': 'ScienceBoardÊòØ‰∏Ä‰∏™Áé∞ÂÆûÁöÑÁßëÂ≠¶Â∑•‰ΩúÊµÅÁ®ãÁéØÂ¢ÉÂíåÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫ÂÆÉ‰ª¨Âú®Â§çÊùÇÁßëÂ≠¶‰ªªÂä°‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Âπ≥Âè∞Êèê‰æõ‰∫Ü‰∏Ä‰∏™Â§öÈ¢ÜÂüüÁöÑÂä®ÊÄÅÁéØÂ¢ÉÔºåÂÖÅËÆ∏‰ª£ÁêÜÈÄöËøá‰∏çÂêåÁöÑÊé•Âè£Ëá™‰∏ª‰∫íÂä®Ôºå‰ª•Âä†ÈÄüÂ§çÊùÇÁöÑÁ†îÁ©∂‰ªªÂä°ÂíåÂÆûÈ™å„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õ‰ª£ÁêÜÂú®ËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫‰∏ÄÂÆöÁöÑÊΩúÂäõÔºå‰ΩÜÂÆÉ‰ª¨Âú®Â§çÊùÇÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÊàêÂäüÁéá‰ªÖ‰∏∫15%ÔºåÊòæÁ§∫Âá∫‰ªçÈúÄÊîπËøõ„ÄÇÈÄöËøáÊ∑±ÂÖ•ÂàÜÊûêÔºåÊú¨Êñá‰∏∫Ëß£ÂÜ≥ÂΩìÂâç‰ª£ÁêÜÁöÑÂ±ÄÈôêÊÄßÂíåËÆæËÆ°Êõ¥ÊúâÊïàÁöÑ‰ª£ÁêÜÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21327', 'title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2505.21327', 'abstract': "MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  \t\t\t\t\tAI-generated summary \t\t\t\t Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.", 'score': 67, 'issue_id': 3991, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'a022fb524c5969bf', 'authors': ['Jiakang Yuan', 'Tianshuo Peng', 'Yilei Jiang', 'Yiting Lu', 'Renrui Zhang', 'Kaituo Feng', 'Chaoyou Fu', 'Tao Chen', 'Lei Bai', 'Bo Zhang', 'Xiangyu Yue'], 'affiliations': ['Fudan University', 'MMLab, The Chinese University of Hong Kong', 'Nanjing University', 'Shanghai AI Laboratory', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21327.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –ª–æ–≥–∏–∫–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': "MME-Reasoning - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–µ, –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–µ –∏ –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –∏–º–µ–Ω–Ω–æ –Ω–∞ –ª–æ–≥–∏–∫–µ, –∞ –Ω–µ –Ω–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –∏–ª–∏ –∑–Ω–∞–Ω–∏—è—Ö. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å –∑–∞–º–µ—Ç–Ω—ã–º–∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–Ω–∞–ª–∏–∑ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ '—Ä–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è' –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª, –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é."}, 'en': {'title': 'Unveiling the Reasoning Gaps in Multimodal AI', 'desc': 'The paper introduces MME-Reasoning, a benchmark designed to assess the logical reasoning abilities of multimodal large language models (MLLMs). It categorizes reasoning into three types: inductive, deductive, and abductive, addressing gaps in existing evaluations that often overlook these distinctions. The study reveals that even advanced MLLMs struggle with logical reasoning tasks, showing significant performance imbalances across the different reasoning types. Additionally, the paper analyzes common methods aimed at improving reasoning, highlighting the persistent limitations of current MLLMs in effectively handling diverse logical reasoning challenges.'}, 'zh': {'title': 'ËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ', 'desc': 'MME-Reasoning ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÈÄªËæëÊé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜÔºåÊè≠Á§∫‰∫ÜÂú®ÂΩíÁ∫≥„ÄÅÊºîÁªéÂíåÊ∫ØÂõ†Êé®ÁêÜÁ±ªÂûã‰∏äÁöÑÊòæËëóÂ±ÄÈôêÊÄßÂíåÊÄßËÉΩ‰∏çÂπ≥Ë°°„ÄÇÂ∞ΩÁÆ°Â§öÊ®°ÊÄÅÊé®ÁêÜÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁé∞ÊúâÂü∫ÂáÜÊú™ËÉΩÂÖ®Èù¢ËØÑ‰º∞ÂÖ∂Êé®ÁêÜËÉΩÂäõÔºåÁº∫‰πèÂØπÈÄªËæëÊé®ÁêÜÁ±ªÂûãÁöÑÊòéÁ°ÆÂàÜÁ±ª„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü MME-ReasoningÔºåÊ∂µÁõñÊâÄÊúâ‰∏âÁßçÊé®ÁêÜÁ±ªÂûãÁöÑÈóÆÈ¢òÔºåÁ°Æ‰øùÊØè‰∏™ÈóÆÈ¢òÊúâÊïàËØÑ‰º∞Êé®ÁêÜËÉΩÂäõÔºåËÄåÈùûÊÑüÁü•ÊäÄËÉΩÊàñÁü•ËØÜÂπøÂ∫¶„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑ MLLMs Âú®ÂÖ®Èù¢ÁöÑÈÄªËæëÊé®ÁêÜËØÑ‰º∞‰∏≠Ë°®Áé∞ÊúâÈôêÔºå‰∏îÂú®‰∏çÂêåÊé®ÁêÜÁ±ªÂûã‰πãÈó¥Â≠òÂú®ÊòéÊòæÁöÑÊÄßËÉΩÂ∑ÆÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21497', 'title': 'Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers', 'url': 'https://huggingface.co/papers/2505.21497', 'abstract': "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.", 'score': 58, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '7f740f76be754bce', 'authors': ['Wei Pang', 'Kevin Qinghong Lin', 'Xiangru Jian', 'Xi He', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.21497.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#agents', '#science'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤: –æ—Ç —Å—Ç–∞—Ç—å–∏ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∏ –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—è –Ω–µ–¥–∞–≤–Ω–∏–µ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Å—Ç–µ—Ä–∞–º–∏, —Å–æ–∑–¥–∞–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç PosterAgent - –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è Parser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, Planner –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ—Å—Ç–µ—Ä–∞ –∏ Painter-Commenter –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Qwen-2.5 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –º–µ—Ç—Ä–∏–∫, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞ 87% –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –ø–æ–∫–æ–ª–µ–Ω–∏—é –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å—Ç–µ—Ä–æ–≤.'}, 'en': {'title': 'Revolutionizing Academic Poster Generation with PosterAgent', 'desc': 'This paper addresses the challenge of generating academic posters from lengthy scientific documents by introducing a benchmark and metric suite for evaluation. It presents PosterAgent, a multi-agent pipeline that includes a Parser for structuring content, a Planner for layout design, and a Painter-Commenter loop for refining visuals based on feedback. The study evaluates the effectiveness of generated posters using metrics like visual quality, textual coherence, and the ability to convey core content through quizzes. The results show that their open-source approach significantly outperforms existing models while being more efficient in token usage, paving the way for future advancements in automated poster generation.'}, 'zh': {'title': 'Ëá™Âä®ÂåñÂ≠¶ÊúØÊµ∑Êä•ÁîüÊàêÁöÑÊñ∞Á∫™ÂÖÉ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ≠¶ÊúØÊµ∑Êä•ÁîüÊàêÂü∫ÂáÜÂíåËØÑ‰º∞ÊåáÊ†áÔºåÊó®Âú®Â∞ÜÈïøÁØáÊñáÊ°£ÂéãÁº©‰∏∫ËßÜËßâ‰∏äËøûË¥ØÁöÑÂçïÈ°µÊµ∑Êä•„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPosterAgentÔºå‰∏Ä‰∏™Â§ö‰ª£ÁêÜÁÆ°ÈÅìÔºåËÉΩÂ§üÊúâÊïàÂú∞Ëß£Êûê„ÄÅËßÑÂàíÂíåÁªòÂà∂Êµ∑Êä•ÂÜÖÂÆπ„ÄÇÈÄöËøáÂØπÊØî‰∏çÂêåÊ®°ÂûãÁöÑËæìÂá∫ÔºåÊàë‰ª¨ÂèëÁé∞‰∫∫Á±ªËÆæËÆ°ÁöÑÊµ∑Êä•Âú®ËßÜËßâËØ≠‰πâ‰∏äÊõ¥ÂÖ∑Âê∏ÂºïÂäõÔºåËÄåGPT-4oÊ®°ÂûãËôΩÁÑ∂Â§ñËßÇÁæéËßÇÔºå‰ΩÜÊñáÊú¨Ë¥®ÈáèÂíå‰ø°ÊÅØ‰º†ËææËÉΩÂäõËæÉÂ∑Æ„ÄÇÊàë‰ª¨ÁöÑÂºÄÊ∫êÂèò‰ΩìÂú®Â§ö‰∏™ÊåáÊ†á‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁ≥ªÁªüÔºåÂπ∂‰∏îÊòæËëóÂáèÂ∞ë‰∫ÜÊâÄÈúÄÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18445', 'title': 'OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data', 'url': 'https://huggingface.co/papers/2505.18445', 'abstract': "OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.", 'score': 57, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '0a5e56835e542da2', 'authors': ['Yiren Song', 'Cheng Liu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.18445.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#diffusion', '#cv', '#training'], 'emoji': 'üé®', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ç–∏–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'OmniConsistency - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã (DiTs) –∏ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä–∞—Ö –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏. –ü–ª–∞–≥–∏–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–∑–¥–µ–ª—è—é—â—É—é –∏–∑—É—á–µ–Ω–∏–µ —Å—Ç–∏–ª—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. OmniConsistency —Å–æ–≤–º–µ—Å—Ç–∏–º —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ —Å—Ç–∏–ª–µ–≤—ã–º–∏ LoRA –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Achieving Consistent and High-Quality Image Stylization with OmniConsistency', 'desc': 'OmniConsistency is a novel approach that improves the consistency and generalization of image stylization using large-scale Diffusion Transformers. It addresses two main challenges in image-to-image pipelines: ensuring consistent stylization across complex scenes and preventing degradation of style when using style LoRAs. The method introduces a learning framework that focuses on maintaining consistency while allowing for flexible style application. Experimental results demonstrate that OmniConsistency achieves visual quality and coherence comparable to leading commercial models.'}, 'zh': {'title': 'OmniConsistencyÔºöÊèêÂçáÂõæÂÉèÈ£éÊ†º‰∏ÄËá¥ÊÄßÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'OmniConsistency ÊòØ‰∏ÄÁßçÂà©Áî®Â§ßËßÑÊ®°Êâ©Êï£ÂèòÊç¢Âô®ÔºàDiffusion TransformersÔºâÊù•Â¢ûÂº∫ÂõæÂÉèÂà∞ÂõæÂÉèÁÆ°ÈÅì‰∏≠ÁöÑÈ£éÊ†º‰∏ÄËá¥ÊÄßÂíåÊ≥õÂåñËÉΩÂäõÁöÑÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠‰øùÊåÅ‰∏ÄËá¥È£éÊ†ºÂíåÈò≤Ê≠¢È£éÊ†ºÈÄÄÂåñÁöÑ‰∏§‰∏™‰∏ªË¶ÅÊåëÊàò„ÄÇOmniConsistency Êèê‰æõ‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂØπÈΩêÂõæÂÉèÂØπÁöÑ‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂπ∂ÈááÁî®‰∏§Èò∂ÊÆµÁöÑÊ∏êËøõÂ≠¶‰π†Á≠ñÁï•Êù•ÂàÜÁ¶ªÈ£éÊ†ºÂ≠¶‰π†‰∏é‰∏ÄËá¥ÊÄß‰øùÊåÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniConsistency ÊòæËëóÊèêÈ´ò‰∫ÜËßÜËßâËøûË¥ØÊÄßÂíåÁæéÂ≠¶Ë¥®ÈáèÔºåÊÄßËÉΩÂèØ‰∏éÂïÜ‰∏öÊúÄÂÖàËøõÊ®°Âûã GPT-4o Áõ∏Â™≤Áæé„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20292', 'title': 'OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for\n  Subject-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20292', 'abstract': "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 16 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.", 'score': 49, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'e2a8d12789199cde', 'authors': ['Shenghai Yuan', 'Xianyi He', 'Yufan Deng', 'Yang Ye', 'Jinfa Huang', 'Bin Lin', 'Chongyang Ma', 'Jiebo Luo', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20292.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#synthetic', '#video'], 'emoji': 'üé¨', 'ru': {'title': 'OpenS2V-Nexus: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenS2V-Nexus - –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (Subject-to-Video, S2V). –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è OpenS2V-Eval - –¥–µ—Ç–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ, –∏ OpenS2V-5M - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ —Å—É–±—ä–µ–∫—Ç-—Ç–µ–∫—Å—Ç-–≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—É–±—ä–µ–∫—Ç–∞, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ 16 —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö S2V –º–æ–¥–µ–ª–µ–π, –≤—ã—è–≤–ª—è—é—â–∞—è –∏—Ö —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã.'}, 'en': {'title': 'Revolutionizing Video Generation with Subject Fidelity', 'desc': 'The paper introduces Subject-to-Video (S2V) generation, which focuses on creating videos that accurately reflect reference content. It presents OpenS2V-Nexus, a framework that includes OpenS2V-Eval, a detailed benchmark for evaluating video generation, and OpenS2V-5M, a large dataset of five million subject-text-video pairs. Unlike previous benchmarks, OpenS2V-Eval emphasizes the generation of videos that maintain subject consistency and natural appearance. The authors also propose three new metrics to assess generated videos based on subject fidelity, naturalness, and relevance to the input text, facilitating a comprehensive evaluation of various S2V models.'}, 'zh': {'title': 'ÊûÑÂª∫ËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Âü∫ÂáÜ‰∏éÊï∞ÊçÆÈõÜ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫ÜSubject-to-Video (S2V) ÁîüÊàêÁöÑÂü∫Á°ÄËÆæÊñΩOpenS2V-NexusÔºåÊó®Âú®ÂàõÂª∫Âø†ÂÆû‰∫éÂèÇËÄÉÂÜÖÂÆπÁöÑËßÜÈ¢ë„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜOpenS2V-EvalÔºå‰∏Ä‰∏™ÁªÜÁ≤íÂ∫¶ÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éÁîüÊàêÂÖ∑ÊúâËá™ÁÑ∂Â§ñËßÇÂíåË∫´‰ªΩ‰øùÁúüÂ∫¶ÁöÑ‰∏ÄËá¥ËßÜÈ¢ë„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞ÁîüÊàêËßÜÈ¢ëÁöÑË¥®ÈáèÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏âÁßçËá™Âä®ÂåñÊåáÊ†áÔºåÂàÜÂà´ÈáèÂåñ‰∏ªÈ¢ò‰∏ÄËá¥ÊÄß„ÄÅËá™ÁÑ∂ÊÄßÂíåÊñáÊú¨Áõ∏ÂÖ≥ÊÄß„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´‰∫îÁôæ‰∏á‰∏™È´òË¥®Èáè720P‰∏ªÈ¢ò-ÊñáÊú¨-ËßÜÈ¢ë‰∏âÂÖÉÁªÑÁöÑÂºÄÊîæÊ∫ê‰ª£Á†ÅÊï∞ÊçÆÈõÜOpenS2V-5MÔºå‰ª•ÊîØÊåÅÊú™Êù•ÁöÑS2VÁîüÊàêÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19641', 'title': 'SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond', 'url': 'https://huggingface.co/papers/2505.19641', 'abstract': 'SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.', 'score': 39, 'issue_id': 3996, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '4c75ba0f126de4b9', 'authors': ['Junteng Liu', 'Yuanxiang Fan', 'Zhuo Jiang', 'Han Ding', 'Yongyi Hu', 'Chi Zhang', 'Yiqi Shi', 'Shitong Weng', 'Aili Chen', 'Shiqi Chen', 'Yunan Huang', 'Mozhi Zhang', 'Pengyu Zhao', 'Junjie Yan', 'Junxian He'], 'affiliations': ['MiniMax', 'The City University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.19641.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#reasoning', '#open_source'], 'emoji': 'üß†', 'ru': {'title': 'SynLogic: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é', 'desc': 'SynLogic - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ª–µ–≥–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –∏–¥–µ–∞–ª—å–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö SynLogic, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö SynLogic —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é —É–ª—É—á—à–∞–µ—Ç –æ–±—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'Enhancing AI Reasoning with SynLogic Framework', 'desc': 'SynLogic is a framework designed to improve the logical reasoning skills of Large Language Models (LLMs) using Reinforcement Learning (RL). It generates a diverse set of logical reasoning tasks, allowing for controlled data synthesis that can be adjusted in difficulty and quantity. The framework not only enhances the reasoning capabilities of LLMs but also improves their generalization across different domains by mixing logical reasoning data with mathematical and coding tasks. The results show that models trained with SynLogic achieve state-of-the-art performance, demonstrating its potential as a valuable resource for advancing AI reasoning.'}, 'zh': {'title': 'SynLogicÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'SynLogicÊòØ‰∏Ä‰∏™Êï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÈÄªËæëÊé®ÁêÜÊï∞ÊçÆÔºåÊ∂µÁõñ35‰∏™‰∏çÂêåÁöÑÊé®ÁêÜ‰ªªÂä°ÔºåÂπ∂ÂÖÅËÆ∏Ê†πÊçÆÈöæÂ∫¶ÂíåÊï∞ÈáèËøõË°åÊéßÂà∂ÂêàÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®SynLogicÊï∞ÊçÆËøõË°åRLËÆ≠ÁªÉÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÊé®ÁêÜÊÄßËÉΩÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äÁé∞ÊúâÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂ∞ÜSynLogicÊï∞ÊçÆ‰∏éÊï∞Â≠¶ÂíåÁºñÁ†Å‰ªªÂä°Ê∑∑ÂêàËÆ≠ÁªÉÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜËøô‰∫õÈ¢ÜÂüüÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊé®ÁêÜÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21189', 'title': 'Exploring the Latent Capacity of LLMs for One-Step Text Generation', 'url': 'https://huggingface.co/papers/2505.21189', 'abstract': 'LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  \t\t\t\t\tAI-generated summary \t\t\t\t A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.', 'score': 35, 'issue_id': 3996, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '993d2720f6fed612', 'authors': ['Gleb Mezentsev', 'Ivan Oseledets'], 'affiliations': ['AIRI Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.21189.jpg', 'data': {'categories': ['#data', '#long_context', '#architecture', '#multimodal'], 'emoji': 'üöÄ', 'ru': {'title': '–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Å–ø–æ—Å–æ–±–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –¥–≤–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å LLM –∫ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç —Å–≤–æ–π—Å—Ç–≤–∞ —ç—Ç–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä—É—é –æ–Ω–∏ –∫–æ–¥–∏—Ä—É—é—Ç. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ —ç—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∏—Ä—É—é—Ç —Å–≤—è–∑–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.'}, 'en': {'title': 'Unlocking Multi-Token Generation in LLMs Without Autoregression', 'desc': 'This paper investigates the ability of large language models (LLMs) to generate long text segments in a single forward pass using learned embeddings. It demonstrates that LLMs can produce hundreds of accurate tokens without relying on autoregressive methods, which typically require iterative decoding. The study reveals that by using just two learned embeddings, LLMs can reconstruct extensive text, showcasing a previously underexplored capability. Additionally, the authors analyze the properties of these embeddings, suggesting that they encode meaningful information and could lead to the development of a dedicated encoder for improved text generation.'}, 'zh': {'title': 'Êé¢Á¥¢LLMsÁöÑÂ§öÊ†áËÆ∞ÁîüÊàêËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ê≤°ÊúâËá™ÂõûÂΩíÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÈïøÊñáÊú¨ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÜªÁªìÁöÑLLMsÂèØ‰ª•‰ªÖÈÄöËøá‰∏§‰∏™Â≠¶‰π†Âà∞ÁöÑÂµåÂÖ•ÔºåÂú®‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠‰∏≠ÁîüÊàêÊï∞Áôæ‰∏™ÂáÜÁ°ÆÁöÑÊ†áËÆ∞„ÄÇËøôÊè≠Á§∫‰∫ÜLLMsÂú®Â§öÊ†áËÆ∞ÁîüÊàêÊñπÈù¢ÁöÑÊΩúÂäõÔºåË∂ÖÂá∫‰∫Ü‰º†ÁªüÁöÑËø≠‰ª£Ëß£Á†ÅÊñπÊ≥ï„ÄÇÊàë‰ª¨ËøòÂàÜÊûê‰∫ÜËøô‰∫õÂµåÂÖ•ÁöÑË°å‰∏∫ÔºåÂπ∂Êèê‰æõ‰∫ÜÂÆÉ‰ª¨ÊâÄÁºñÁ†Å‰ø°ÊÅØÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17813', 'title': "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning", 'url': 'https://huggingface.co/papers/2505.17813', 'abstract': 'Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.', 'score': 35, 'issue_id': 3992, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'd8d9d938b84c5ea6', 'authors': ['Michael Hassid', 'Gabriel Synnaeve', 'Yossi Adi', 'Roy Schwartz'], 'affiliations': ['FAIR Team, Meta', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2505.17813.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–ö–æ—Ä–æ—á–µ –º—ã—Å–ª—å - –±—ã—Å—Ç—Ä–µ–µ –≤—ã–≤–æ–¥: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ short-m@k, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ—Ç–≤–µ—Ç –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º —Å—Ä–µ–¥–∏ –ø–µ—Ä–≤—ã—Ö m –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ü–µ–ø–æ—á–∫–∏ —á–∞—Å—Ç–æ –¥–∞—é—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º –¥–ª–∏–Ω–Ω—ã–µ, –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –∞–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Shorter Chains, Smarter Reasoning!', 'desc': 'This paper investigates the effectiveness of reasoning in large language models (LLMs) by comparing long and short reasoning chains. The authors find that shorter reasoning chains can yield significantly more accurate answers, with improvements of up to 34.5% compared to longer chains. They introduce a new method called short-m@k, which allows for parallel processing of multiple reasoning chains and selects the final answer based on majority voting. Their results indicate that shorter reasoning processes not only enhance performance but also reduce computational costs and inference time, challenging the traditional belief that longer reasoning leads to better outcomes.'}, 'zh': {'title': 'Áü≠ÊÄùÁª¥ÈìæÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠‰æùËµñÈïøÊÄùÁª¥ÈìæÁöÑÂÅáËÆæ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËæÉÁü≠ÁöÑÊé®ÁêÜÈìæÂú®ÂõûÁ≠îÈóÆÈ¢òÊó∂Êõ¥ÂèØËÉΩ‰∫ßÁîüÊ≠£Á°ÆÁ≠îÊ°àÔºåÂáÜÁ°ÆÁéáÊØîÊúÄÈïøÈìæÈ´òÂá∫34.5%„ÄÇÂü∫‰∫éÊ≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊñπÊ≥ïshort-m@kÔºåÈÄöËøáÂπ∂Ë°åÁîüÊàêk‰∏™Áã¨Á´ãÁöÑÊÄùÁª¥ËøáÁ®ãÔºåÂπ∂Âú®Á¨¨‰∏Ä‰∏™m‰∏™ÂÆåÊàêÂêéÂÅúÊ≠¢ËÆ°ÁÆóÔºåÊúÄÁªàÁ≠îÊ°àÈÄöËøáÂ§öÊï∞ÊäïÁ•®ÈÄâÂá∫„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÁü≠ÊÄùÁª¥ÈìæÁöÑËÆ≠ÁªÉÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩÔºåÊåëÊàò‰∫ÜÈïøÊÄùÁª¥ÈìæÂøÖÁÑ∂Â∏¶Êù•Êõ¥Â•ΩÊé®ÁêÜËÉΩÂäõÁöÑ‰º†ÁªüËßÇÂøµ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19000', 'title': 'VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied\n  Iterative Policy Optimization', 'url': 'https://huggingface.co/papers/2505.19000', 'abstract': "A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability.", 'score': 34, 'issue_id': 3991, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '058fcf46b0f20cc6', 'authors': ['Yunxin Li', 'Xinyu Chen', 'Zitao Li', 'Zhenyu Liu', 'Longyue Wang', 'Wenhan Luo', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Group', 'Division of AMC and Department of ECE, HKUST', 'Harbin Institute of Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.19000.jpg', 'data': {'categories': ['#reasoning', '#training', '#video', '#optimization', '#rl', '#rlhf'], 'emoji': 'üé•', 'ru': {'title': 'VerIPO: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∏–¥–µ–æ-LLM —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ VerIPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ-LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Verifier –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏ GRPO –∏ DPO –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. VerIPO –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ 7 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å GRPO. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ VerIPO –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–¥–µ–æ-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Video Reasoning with Verifier-guided Optimization', 'desc': 'This paper introduces VerIPO, a new method for improving Video Large Language Models (Video-LLMs) using a Verifier-guided Iterative Policy Optimization approach. It addresses the limitations of existing Reinforcement Fine-Tuning methods by incorporating a Rollout-Aware Verifier that enhances the quality of reasoning chains during training. By creating high-quality contrastive data, this method allows for faster and more effective optimization, achieving results that are significantly better than traditional methods. Experimental findings show that VerIPO not only speeds up the training process but also improves the contextual consistency and length of reasoning outputs in video tasks.'}, 'zh': {'title': 'È™åËØÅËÄÖÂºïÂØºÁöÑËø≠‰ª£‰ºòÂåñÔºåÊèêÂçáËßÜÈ¢ëÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫VerIPOÁöÑÈ™åËØÅËÄÖÂºïÂØºËø≠‰ª£Á≠ñÁï•‰ºòÂåñÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáËßÜÈ¢ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàVideo-LLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®GRPOÂíåDPOÈò∂ÊÆµ‰πãÈó¥ÂºïÂÖ•‰∏Ä‰∏™ÂõûÊªöÊÑüÁü•È™åËØÅÂô®ÔºåÂΩ¢ÊàêGRPO-È™åËØÅÂô®-DPOËÆ≠ÁªÉÂæ™ÁéØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Âø´‰∏îÊõ¥ÊúâÊïàÁöÑ‰ºòÂåñ„ÄÇÈ™åËØÅÂô®Âà©Áî®Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞Êé®ÁêÜÈÄªËæëÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑÂØπÊØîÊï∞ÊçÆÔºå‰øÉËøõ‰∫ÜÈïøÈìæÊé®ÁêÜÁöÑÁîüÊàê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVerIPOÂú®‰ºòÂåñÈÄüÂ∫¶ÂíåÊé®ÁêÜË¥®Èáè‰∏äÂùáÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑGRPOÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16459', 'title': 'MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.16459', 'abstract': 'The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.', 'score': 34, 'issue_id': 3991, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'd18f80036a817d7c', 'authors': ['Guiyao Tie', 'Xueyang Zhou', 'Tianhe Gu', 'Ruihang Zhang', 'Chaoran Hu', 'Sizhe Zhang', 'Mengqu Sun', 'Yan Zhang', 'Pan Zhou', 'Lichao Sun'], 'affiliations': ['Huazhong University of Science and Technology', 'Lehigh University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16459.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'MMMR: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMMR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). MMMR –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 1083 —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —à–µ—Å—Ç—å —Ç–∏–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∏ –∫–æ–Ω–≤–µ–π–µ—Ä –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (RTEP). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MLLM —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏ –º—ã—à–ª–µ–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏ –±–µ–∑ –Ω–∏—Ö, –Ω–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –ë–µ–Ω—á–º–∞—Ä–∫ –≤—ã—è–≤–ª—è–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –æ—Å–Ω–æ–≤—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Evaluating Multi-Modal Reasoning: The MMMR Benchmark', 'desc': 'The MMMR benchmark is designed to evaluate the reasoning abilities of Multi-Modal Large Language Models (MLLMs) by focusing on their thinking quality across various reasoning types. It includes a challenging dataset with 1,083 questions that require complex reasoning, and a modular evaluation pipeline to assess reasoning quality beyond just accuracy. The study finds that while MLLMs with intermediate thinking traces perform better than those without, they still exhibit issues like inconsistency and overthinking. This benchmark aims to bridge the gap between accuracy and reasoning quality, providing a structured approach for future advancements in multi-modal reasoning systems.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÊñ∞Âü∫ÂáÜÔºöMMMR', 'desc': 'MMMRÂü∫ÂáÜÊµãËØïËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈáçÁÇπÂú®‰∫éÈÄöËøáÂ§öÊ†∑ÁöÑÊé®ÁêÜÁ±ªÂûãÂíåÊ®°ÂùóÂåñËØÑ‰º∞ÊµÅÁ®ãÊù•ËØÑ‰º∞ÊÄùÁª¥Ë¥®Èáè„ÄÇÂ∞ΩÁÆ°MLLMsÂú®ËØ≠Ë®Ä„ÄÅËßÜËßâÂíåÁªìÊûÑÂåñËæìÂÖ•ÁöÑÁªü‰∏ÄÂ§ÑÁêÜ‰∏äÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂÖ∂Êé®ÁêÜËÉΩÂäõ‰ªçÁÑ∂‰∏çÂ§üÊ∏ÖÊô∞ÔºåÁº∫‰πèÊ†áÂáÜÂåñÁöÑËØÑ‰º∞Âü∫ÂáÜ„ÄÇMMMRÂåÖÂê´‰∏Ä‰∏™È´òÈöæÂ∫¶ÁöÑÊï∞ÊçÆÈõÜÂíå‰∏Ä‰∏™Êé®ÁêÜËøΩË∏™ËØÑ‰º∞ÁÆ°ÈÅìÔºåÊó®Âú®Ë∂ÖË∂äÂáÜÁ°ÆÊÄßËØÑ‰º∞ÔºåÂÖ≥Ê≥®Êé®ÁêÜÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÅ‰∏ÄËá¥ÊÄßÂíåÁªìÊûÑÂåñÈîôËØØÊ≥®Èáä„ÄÇÈÄöËøáÂÆûËØÅÁªìÊûúÔºåMMMRÊè≠Á§∫‰∫ÜÂáÜÁ°ÆÊÄß‰∏éÊé®ÁêÜË¥®Èáè‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰∏∫Êú™Êù•Ê®°ÂûãÁöÑÂèëÂ±ïÊèê‰æõ‰∫ÜÂèØÊìç‰ΩúÁöÑËØÑ‰º∞Ê°ÜÊû∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21496', 'title': 'UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents', 'url': 'https://huggingface.co/papers/2505.21496', 'abstract': 'In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.', 'score': 31, 'issue_id': 3993, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'db6e0d226a2c500e', 'authors': ['Han Xiao', 'Guozhi Wang', 'Yuxiang Chai', 'Zimu Lu', 'Weifeng Lin', 'Hao He', 'Lue Fan', 'Liuyang Bian', 'Rui Hu', 'Liang Liu', 'Shuai Ren', 'Yafei Wen', 'Xiaoxin Chen', 'Aojun Zhou', 'Hongsheng Li'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.21496.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#synthetic', '#open_source', '#dataset', '#agents', '#training', '#data'], 'emoji': 'üßû', 'ru': {'title': 'UI-Genie: —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤', 'desc': 'UI-Genie - —ç—Ç–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—ã –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è UI-Genie-RM —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç, –∏ –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö UI-Genie-RM-517k –∏ UI-Genie-Agent-16k –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UI-Genie –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.'}, 'en': {'title': 'UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models', 'desc': 'This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach.'}, 'zh': {'title': 'UI-GenieÔºöËá™ÊàëÊîπËøõÁöÑGUI‰ª£ÁêÜÊ°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜUI-GenieÔºåËøôÊòØ‰∏Ä‰∏™Ëá™ÊàëÊîπËøõÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥GUI‰ª£ÁêÜ‰∏≠ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöËΩ®ËøπÁªìÊûúÁöÑÈ™åËØÅÂõ∞ÈöæÂíåÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÂ•ñÂä±Ê®°ÂûãÂíåËá™ÊàëÊîπËøõÁÆ°ÈÅìÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÂ•ñÂä±Ê®°ÂûãUI-Genie-RMÈááÁî®ÂõæÂÉè-ÊñáÊú¨‰∫§ÈîôÊû∂ÊûÑÔºåÊúâÊïàÂ§ÑÁêÜÂéÜÂè≤‰∏ä‰∏ãÊñáÔºåÂπ∂Áªü‰∏Ä‰∫ÜÂä®‰ΩúÁ∫ßÂíå‰ªªÂä°Á∫ßÂ•ñÂä±„ÄÇËá™ÊàëÊîπËøõÁÆ°ÈÅìÈÄöËøáÂ•ñÂä±ÂºïÂØºÊé¢Á¥¢ÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÁªìÊûúÈ™åËØÅÔºåÈÄêÊ≠•Êâ©Â±ïÂèØËß£ÂÜ≥ÁöÑÂ§çÊùÇGUI‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçá‰ª£ÁêÜÂíåÂ•ñÂä±Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18875', 'title': 'Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation', 'url': 'https://huggingface.co/papers/2505.18875', 'abstract': 'SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.', 'score': 30, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': 'bc68e232d8897ad4', 'authors': ['Shuo Yang', 'Haocheng Xi', 'Yilong Zhao', 'Muyang Li', 'Jintao Zhang', 'Han Cai', 'Yujun Lin', 'Xiuyu Li', 'Chenfeng Xu', 'Kelly Peng', 'Jianfei Chen', 'Song Han', 'Kurt Keutzer', 'Ion Stoica'], 'affiliations': ['MIT', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.18875.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#optimization'], 'emoji': 'üéûÔ∏è', 'ru': {'title': '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': 'SVG2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫—É –¥–ª—è —Ç–æ—á–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤. SVG2 –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é k-means –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –±—é–¥–∂–µ—Ç–∞ top-p –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–¥—Ä–∞, –¥–æ—Å—Ç–∏–≥–∞—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 2.30x –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Maximizing Video Generation Efficiency with SVG2', 'desc': 'SVG2 is a novel framework designed to improve the efficiency and quality of video generation without the need for extensive training. It focuses on accurately identifying critical tokens through semantic-aware permutation, which groups tokens based on their meanings rather than just their positions. This method reduces computational waste by ensuring that critical tokens are processed together, optimizing GPU usage. By implementing dynamic budget control, SVG2 achieves significant speed improvements while maintaining high video quality, demonstrating a balance between performance and resource efficiency.'}, 'zh': {'title': 'SVG2ÔºöÊèêÂçáËßÜÈ¢ëÁîüÊàêÊïàÁéá‰∏éË¥®ÈáèÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'SVG2ÊòØ‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂáÜÁ°ÆËØÜÂà´ÂíåÂ§ÑÁêÜÂÖ≥ÈîÆÊ†áËÆ∞ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÊïàÁéáÂíåË¥®Èáè„ÄÇÂÆÉÈááÁî®ËØ≠‰πâÊÑüÁü•ÁöÑÊéíÂàóÂíåÂä®ÊÄÅÈ¢ÑÁÆóÊéßÂà∂ÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®ËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÁîüÊàêË¥®Èáè‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇSVG2ÈÄöËøák-meansËÅöÁ±ªÂíåÈáçÊñ∞ÊéíÂàóÊ†áËÆ∞ÔºåÁ°Æ‰øù‰∫ÜÁ≤æÁ°ÆÁöÑËÅöÁ±ªË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜËØÜÂà´ÂáÜÁ°ÆÊÄßÔºåÂπ∂ÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÊµ™Ë¥π„ÄÇËØ•Ê°ÜÊû∂Âú®‰øùÊåÅÁîüÊàêË¥®ÈáèÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÈ´òËææ2.30ÂÄçÁöÑÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21333', 'title': 'MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios', 'url': 'https://huggingface.co/papers/2505.21333', 'abstract': 'MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.', 'score': 29, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '25639980b7f7add5', 'authors': ['Yang Shi', 'Huanqian Wang', 'Wulin Xie', 'Huanyao Zhang', 'Lijie Zhao', 'Yi-Fan Zhang', 'Xinfeng Li', 'Chaoyou Fu', 'Zhuoer Wen', 'Wenting Liu', 'Zhuoran Zhang', 'Xinlong Chen', 'Bohan Zeng', 'Sihan Yang', 'Yuanxing Zhang', 'Pengfei Wan', 'Haotian Wang', 'Wenjing Yang'], 'affiliations': ['CASIA', 'CUHKSZ', 'Kuaishou', 'NTU', 'PKU', 'THU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.21333.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#reasoning', '#video'], 'emoji': 'üé•', 'ru': {'title': '–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ OCR –Ω–∞ –≤–∏–¥–µ–æ', 'desc': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ (OCR) –Ω–∞ –≤–∏–¥–µ–æ –∏–∑-–∑–∞ —Ä–∞–∑–º—ã—Ç–∏—è –ø—Ä–∏ –¥–≤–∏–∂–µ–Ω–∏–∏, –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –±–µ–Ω—á–º–∞—Ä–∫ MME-VideoOCR, –≤–∫–ª—é—á–∞—é—â–∏–π 10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–¥–∞—á –∏ 25 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π MLLM –≤ –≤–∏–¥–µ–æ OCR. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 18 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM –≤—ã—è–≤–∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –∏ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ OCR –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'Enhancing Video OCR: Bridging the Gap in MLLM Performance', 'desc': 'This paper discusses the challenges faced by Multimodal Large Language Models (MLLMs) in performing Optical Character Recognition (OCR) on videos. It highlights that factors like motion blur and temporal variations significantly reduce their accuracy compared to static images. To address these issues, the authors introduce the MME-VideoOCR benchmark, which includes a variety of tasks designed to test spatio-temporal reasoning and language understanding in video contexts. The evaluation of 18 MLLMs reveals that even the best models struggle with comprehensive video comprehension, particularly in scenarios requiring integration of information across multiple frames.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëOCRÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÊåëÊàò', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÈùôÊÄÅÂõæÂÉèÁöÑÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºàOCRÔºâ‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ËßÜÈ¢ëOCR‰∏≠ÊïàÊûúÊòæËëó‰∏ãÈôç„ÄÇËøôÊòØÁî±‰∫éËßÜÈ¢ëÂÜÖÂÆπ‰∏≠ÁöÑËøêÂä®Ê®°Á≥ä„ÄÅÊó∂Èó¥ÂèòÂåñÂíåËßÜËßâÊïàÊûúÁ≠âÂõ†Á¥†ÂΩ±Âìç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMME-VideoOCRÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçËßÜÈ¢ëOCRÂ∫îÁî®Âú∫ÊôØÔºåÂåÖÂê´10‰∏™‰ªªÂä°Á±ªÂà´Âíå25‰∏™ÂÖ∑‰Ωì‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑMLLMsÂú®Â§ÑÁêÜÈúÄË¶ÅÊï¥‰ΩìËßÜÈ¢ëÁêÜËß£ÁöÑ‰ªªÂä°Êó∂ËÉΩÂäõÊúâÈôêÔºåÂ∞§ÂÖ∂ÊòØÂú®Êó∂Á©∫Êé®ÁêÜÂíåË∑®Â∏ß‰ø°ÊÅØÊï¥ÂêàÊñπÈù¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20355', 'title': 'GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.20355', 'abstract': "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git", 'score': 27, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'd4035428ea14ce6b', 'authors': ['Yeonjoon Jung', 'Daehyun Ahn', 'Hyungjun Kim', 'Taesu Kim', 'Eunhyeok Park'], 'affiliations': ['POSTECH', 'SqueezeBits'], 'pdf_title_img': 'assets/pdf/title_img/2505.20355.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#optimization'], 'emoji': 'üß©', 'ru': {'title': 'GraLoRA: –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Granular Low-Rank Adaptation (GraLoRA). GraLoRA –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ Low-Rank Adaptation (LoRA), —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–Ω–≥–∞. –ú–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –ø–æ–¥–±–ª–æ–∫–∏, –∫–∞–∂–¥—ã–π —Å–æ —Å–≤–æ–∏–º –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–º –∞–¥–∞–ø—Ç–µ—Ä–æ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GraLoRA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LoRA –∏ –¥—Ä—É–≥–∏–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞, –¥–æ—Å—Ç–∏–≥–∞—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ 8.5% –≤ –º–µ—Ç—Ä–∏–∫–µ Pass@1 –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ HumanEval+.'}, 'en': {'title': 'GraLoRA: Unlocking the Power of Fine-Tuning with Granular Adaptation', 'desc': 'This paper introduces Granular Low-Rank Adaptation (GraLoRA), a new method designed to improve the performance of Low-Rank Adaptation (LoRA) in fine-tuning generative models. LoRA is effective but struggles with overfitting when the rank is increased, leading to poor accuracy compared to full fine-tuning. GraLoRA addresses this issue by dividing weight matrices into smaller sub-blocks, allowing each to have its own low-rank adapter, which enhances gradient propagation and reduces entanglement. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other methods, achieving notable improvements in various benchmarks without increasing computational costs.'}, 'zh': {'title': 'È¢óÁ≤í‰ΩéÁß©ÈÄÇÂ∫îÔºöË∂ÖË∂äLoRAÁöÑÈ´òÊïàÂæÆË∞É', 'desc': '‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÊòØ‰∏ÄÁßçÊµÅË°åÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÊñπÊ≥ïÔºåÂõ†ÂÖ∂ÁÆÄÂçïÊúâÊïàËÄåÂèóÂà∞ÈáçËßÜ„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÊúâÊâÄÊîπËøõÔºåLoRA‰ªçÁÑ∂Èù¢‰∏¥‰∏Ä‰∏™Ê†πÊú¨ÊÄßÈôêÂà∂ÔºöÂΩìÁì∂È¢àÂä†ÂÆΩÊó∂ÂÆπÊòìËøáÊãüÂêà„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁªìÊûÑÔºåÁß∞‰∏∫È¢óÁ≤í‰ΩéÁß©ÈÄÇÂ∫îÔºàGraLoRAÔºâÔºåÂÆÉÂ∞ÜÊùÉÈáçÁü©ÈòµÂàíÂàÜ‰∏∫Â≠êÂùóÔºåÊØè‰∏™Â≠êÂùóÈÉΩÊúâËá™Â∑±ÁöÑ‰ΩéÁß©ÈÄÇÈÖçÂô®Ôºå‰ªéËÄåÂÖãÊúç‰∫ÜLoRAÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåGraLoRAÂú®‰ª£Á†ÅÁîüÊàêÂíåÂ∏∏ËØÜÊé®ÁêÜÂü∫ÂáÜ‰∏äË°®Áé∞‰ºò‰∫éLoRAÔºåÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21374', 'title': 'Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?', 'url': 'https://huggingface.co/papers/2505.21374', 'abstract': 'Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.', 'score': 26, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '0683137770e562ab', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Teng Wang', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.21374.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#video'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ', 'desc': 'Video-Holmes - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–æ–º–µ—Ç—Ä–∞–∂–Ω—ã–µ —Ñ–∏–ª—å–º—ã-—Å–∞—Å–ø–µ–Ω—Å –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 1837 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ 270 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–∏–¥–µ–æ, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 7 —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –ë–µ–Ω—á–º–∞—Ä–∫ –≤—ã—è–≤–∏–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏-–ª—é–¥—å–º–∏. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å Gemini-2.5-Pro –¥–æ—Å—Ç–∏–≥–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ 45%, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Video-Holmes: A New Benchmark for Complex Video Reasoning', 'desc': 'The Video-Holmes benchmark assesses the complex video reasoning abilities of Multimodal Language Models (MLLMs) using suspense short films. It highlights the challenges these models face in integrating information compared to human experts, particularly in real-world reasoning scenarios. The benchmark includes 1,837 questions based on 270 annotated films, requiring models to connect multiple visual clues across different segments. Despite advancements in reasoning techniques, the evaluation shows that even the best models struggle with accuracy, achieving only 45%, indicating significant room for improvement in multimodal reasoning.'}, 'zh': {'title': 'Video-HolmesÔºöÊøÄÂä±Ê®°ÂûãÊõ¥ÂÉè‰∫∫Á±ªÊé®ÁêÜÁöÑÂü∫ÂáÜÊµãËØï', 'desc': 'Video-HolmesÂü∫ÂáÜÊµãËØïËØÑ‰º∞‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§çÊùÇËßÜÈ¢ëÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºåÁâπÂà´ÊòØÈÄöËøáÊÇ¨ÁñëÁü≠ÁâáÊù•Êè≠Á§∫‰∏é‰∫∫Á±ª‰∏ìÂÆ∂Áõ∏ÊØîÁöÑ‰ø°ÊÅØÊï¥ÂêàÊåëÊàò„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´Êù•Ëá™270ÈÉ®ÊâãÂä®Ê≥®ÈáäÁöÑÊÇ¨ÁñëÁü≠ÁâáÁöÑ1837‰∏™ÈóÆÈ¢òÔºåËÆæËÆ°‰∫Ü‰∏É‰∏™‰ªªÂä°ÔºåË¶ÅÊ±ÇÊ®°Âûã‰∏ªÂä®ÂØªÊâæÂíåËøûÊé•ÂàÜÊï£Âú®‰∏çÂêåËßÜÈ¢ëÁâáÊÆµ‰∏≠ÁöÑÂ§ö‰∏™Áõ∏ÂÖ≥ËßÜËßâÁ∫øÁ¥¢„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÁöÑMLLMsÂú®ËßÜËßâÊÑüÁü•ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®‰ø°ÊÅØÊï¥Âêà‰∏äÂç¥Èù¢‰∏¥ÈáçÂ§ßÂõ∞ÈöæÔºåËÆ∏Â§öÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá‰Ωé‰∫é40%„ÄÇÊàë‰ª¨Â∏åÊúõVideo-HolmesËÉΩÂ§üÊøÄÂä±Ê®°ÂûãÊõ¥ÂÉè‰∫∫Á±ªËøõË°åÊé®ÁêÜÔºåÂπ∂Âº∫Ë∞ÉËøô‰∏ÄÈ¢ÜÂüüÁöÑÊåÅÁª≠ÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21297', 'title': 'rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset', 'url': 'https://huggingface.co/papers/2505.21297', 'abstract': 'A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.', 'score': 21, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '1fdb1a9edd20c73b', 'authors': ['Yifei Liu', 'Li Lyna Zhang', 'Yi Zhu', 'Bingcheng Dong', 'Xudong Zhou', 'Ning Shang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Dalian University of Technology', 'Microsoft Research Asia', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21297.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#reasoning', '#data'], 'emoji': 'üß†', 'ru': {'title': 'rStar-Coder: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –æ –∫–æ–¥–µ', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ rStar-Coder - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ –∫–æ–¥–µ. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 418 —Ç—ã—Å—è—á –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è, 580 —Ç—ã—Å—è—á –ø–æ–¥—Ä–æ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ rStar-Coder –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π Qwen –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø–æ–∑–≤–æ–ª–∏–≤ –∏–º –¥–æ—Å—Ç–∏—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å—Ä–∞–≤–Ω–∏–º—ã—Ö —Å –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ LLM –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ –∫–æ–¥–µ, –Ω–æ –ø—Ä–∏ –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ LiveCodeBench –º–æ–¥–µ–ª—å Qwen2.5-14B —É–ª—É—á—à–∏–ª–∞ —Å–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å 23.3% –¥–æ 62.5% –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ rStar-Coder.'}, 'en': {'title': 'Unlocking Code Reasoning with rStar-Coder', 'desc': 'The paper presents rStar-Coder, a large-scale dataset designed to enhance code reasoning capabilities in large language models (LLMs). It addresses the challenge of limited high-difficulty datasets by providing 418,000 verified code problems and 580,000 long-reasoning solutions, complete with diverse test cases. The dataset is created through a three-step process that includes curating competitive programming problems, synthesizing input-output test cases, and verifying solutions. Experiments show that models trained on rStar-Coder significantly outperform existing benchmarks, demonstrating its effectiveness in improving code reasoning tasks.'}, 'zh': {'title': 'ÊèêÂçá‰ª£Á†ÅÊé®ÁêÜËÉΩÂäõÁöÑrStar-CoderÊï∞ÊçÆÈõÜ', 'desc': 'rStar-CoderÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜÔºåÊó®Âú®ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´418,000‰∏™Á´û‰∫âÁ∫ßÂà´ÁöÑ‰ª£Á†ÅÈóÆÈ¢òÂíå580,000‰∏™ÈïøÊé®ÁêÜËß£ÂÜ≥ÊñπÊ°àÔºåÈÖçÂ§á‰∏∞ÂØåÁöÑÊµãËØïÁî®‰æãÔºåÊ∂µÁõñ‰∏çÂêåÈöæÂ∫¶„ÄÇÈÄöËøá‰∏âÈ°πÊ†∏ÂøÉË¥°ÁåÆÔºårStar-CoderÊèê‰æõ‰∫ÜÂèØÈ™åËØÅÁöÑËæìÂÖ•ËæìÂá∫ÊµãËØïÊ°à‰æãÔºåÁ°Æ‰øù‰∫ÜËß£ÂÜ≥ÊñπÊ°àÁöÑÊúâÊïàÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®rStar-CoderÁöÑÊï∞ÊçÆÈõÜÔºåQwenÊ®°ÂûãÂú®Â§ö‰∏™‰ª£Á†ÅÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18943', 'title': 'MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems', 'url': 'https://huggingface.co/papers/2505.18943', 'abstract': "MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.", 'score': 18, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '718f1062d34a47a7', 'authors': ['Xuanming Zhang', 'Yuxuan Chen', 'Min-Hsuan Yeh', 'Yixuan Li'], 'affiliations': ['Tsinghua University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.18943.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agents', '#reasoning', '#alignment'], 'emoji': 'üß†', 'ru': {'title': 'MetaMind: –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º', 'desc': 'MetaMind - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ —Ç–µ–æ—Ä–∏–∏ —Å–æ–∑–Ω–∞–Ω–∏—è. –û–Ω–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≥–∏–ø–æ—Ç–µ–∑, –∏—Ö —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤. MetaMind –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Ä–æ–≤–Ω—è —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ç–µ–æ—Ä–∏–∏ —Å–æ–∑–Ω–∞–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—è —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 35.7% –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∞–≥–µ–Ω—Ç–∞: –∞–≥–µ–Ω—Ç —Ç–µ–æ—Ä–∏–∏ —Å–æ–∑–Ω–∞–Ω–∏—è, –¥–æ–º–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –∏ –∞–≥–µ–Ω—Ç –æ—Ç–≤–µ—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—É—é –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç—å, —Å–æ—Ü–∏–∞–ª—å–Ω—É—é —É–º–µ—Å—Ç–Ω–æ—Å—Ç—å –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.'}, 'en': {'title': 'Empowering AI with Human-like Social Intelligence', 'desc': 'MetaMind is a multi-agent framework that enhances large language models (LLMs) by improving their ability to understand human social interactions through Theory of Mind (ToM) tasks. It breaks down social understanding into three key stages: generating hypotheses about mental states, refining these hypotheses with cultural and ethical considerations, and producing contextually appropriate responses. This approach allows LLMs to achieve human-like performance in social reasoning, showing significant improvements in real-world scenarios and ToM reasoning tasks. The framework demonstrates the importance of each component in achieving a balance between contextual relevance and social appropriateness, paving the way for more empathetic AI interactions.'}, 'zh': {'title': 'MetaMindÔºöÊèêÂçáAIÁöÑÁ§æ‰ºöÊô∫ËÉΩ', 'desc': 'MetaMindÊòØ‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÁÅµÊÑüÊù•Ê∫ê‰∫éÂÖÉËÆ§Áü•ÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂøÉÊô∫ÁêÜËÆ∫‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÁ§æ‰ºöÁêÜËß£ÂàÜËß£‰∏∫‰∏â‰∏™Âçè‰ΩúÈò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÂøÉÊô∫ÁêÜËÆ∫‰ª£ÁêÜÁîüÊàêÁî®Êà∑ÂøÉÁêÜÁä∂ÊÄÅÁöÑÂÅáËÆæÔºõÂÖ∂Ê¨°ÔºåÈ¢ÜÂüü‰ª£ÁêÜÂà©Áî®ÊñáÂåñËßÑËåÉÂíå‰º¶ÁêÜÁ∫¶ÊùüÊù•ÁªÜÂåñËøô‰∫õÂÅáËÆæÔºõÊúÄÂêéÔºåÂìçÂ∫î‰ª£ÁêÜÁîüÊàêÁ¨¶Âêà‰∏ä‰∏ãÊñáÁöÑÈÄÇÂΩìÂõûÂ∫î„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåMetaMindÂú®‰∏â‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÈ¶ñÊ¨°‰ΩøLLMsÂú®ÂÖ≥ÈîÆÁöÑÂøÉÊô∫ÁêÜËÆ∫‰ªªÂä°‰∏äËææÂà∞‰∫∫Á±ªÊ∞¥Âπ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21334', 'title': 'HoliTom: Holistic Token Merging for Fast Video Large Language Models', 'url': 'https://huggingface.co/papers/2505.21334', 'abstract': "HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.", 'score': 17, 'issue_id': 3994, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'b07c0c110f38f89c', 'authors': ['Kele Shao', 'Keda Tao', 'Can Qin', 'Haoxuan You', 'Yang Sui', 'Huan Wang'], 'affiliations': ['Columbia University', 'Rice University', 'Salesforce AI Research', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21334.jpg', 'data': {'categories': ['#inference', '#video', '#optimization'], 'emoji': 'üé¨', 'ru': {'title': 'HoliTom: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-LLM', 'desc': 'HoliTom - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∏–¥–µ–æ-LLM –º–æ–¥–µ–ª—è—Ö. –û–Ω —Å–æ—á–µ—Ç–∞–µ—Ç –≤–Ω–µ—à–Ω—é—é –æ–±—Ä–µ–∑–∫—É LLM —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —É–¥–∞–µ—Ç—Å—è —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –¥–æ 6,9% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ, —Å–æ—Ö—Ä–∞–Ω–∏–≤ 99,1% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'HoliTom: Efficient Video LLMs through Smart Token Pruning', 'desc': 'HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference.'}, 'zh': {'title': 'HoliTomÔºöÈ´òÊïàËßÜÈ¢ëLLMÁöÑÂÖ®Êñ∞Ââ™ÊûùÁ≠ñÁï•', 'desc': 'HoliTomÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉÊó†ÂÖ≥ÁöÑÊï¥‰Ωì‰ª§ÁâåÂêàÂπ∂Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂÖ®ÁêÉÊó∂Èó¥ÂàÜÂâ≤ËøõË°åÂ§ñÈÉ®LLMÂâ™ÊûùÔºåÊòæËëóÂáèÂ∞ëËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàËßÜÈ¢ëLLMÔºâÁöÑËÆ°ÁÆóÊïàÁéáÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂ§ñÈÉ®LLMÂâ™ÊûùÂíåÂÜÖÈÉ®LLMÂü∫‰∫é‰ª§ÁâåÁõ∏‰ººÊÄßÁöÑÂêàÂπ∂ÔºåËÉΩÂ§üÂú®‰∏çÁâ∫Áâ≤ÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÂáèÂ∞ëËßÜËßâ‰ª§ÁâåË∂ÖËøá90%„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåHoliTomÊúâÊïàÁºìËß£‰∫ÜLLMÁöÑËÆ°ÁÆóË¥üÊãÖÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü99.1%ÁöÑÂéüÂßãÊÄßËÉΩ„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®ËÆ°ÁÆóÊàêÊú¨ÂíåËß£Á†ÅÈÄüÂ∫¶‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®È´òÊïàËßÜÈ¢ëLLMÊé®ÁêÜ‰∏≠ÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17952', 'title': 'Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with\n  Minimalist Rule-Based RL', 'url': 'https://huggingface.co/papers/2505.17952', 'abstract': 'Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.', 'score': 16, 'issue_id': 3999, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '6dbd0f852449229e', 'authors': ['Che Liu', 'Haozhe Wang', 'Jiazhen Pan', 'Zhongwei Wan', 'Yong Dai', 'Fangzhen Lin', 'Wenjia Bai', 'Daniel Rueckert', 'Rossella Arcucci'], 'affiliations': ['Fudan University', 'HKUST', 'Imperial College London', 'Ohio State University', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.17952.jpg', 'data': {'categories': ['#healthcare', '#reasoning', '#benchmark', '#rl', '#dataset', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —è–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AlphaMed - –ø–µ—Ä–≤—É—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (LLM), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Ä–∞–∑–≤–∏—Ç—É—é –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —à–µ—Å—Ç–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –æ—Ç–º–µ—á–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö LLM.'}, 'en': {'title': 'Reinforcement Learning Powers Medical Reasoning Without Supervision', 'desc': 'This paper introduces AlphaMed, a medical large language model (LLM) that enhances reasoning capabilities through reinforcement learning (RL) without the need for supervised fine-tuning (SFT) or costly chain-of-thought (CoT) data. AlphaMed utilizes minimalist rule-based rewards on publicly available multiple-choice question-answering datasets, achieving state-of-the-art performance on various medical QA benchmarks. The study reveals that the quality and diversity of the dataset significantly influence reasoning performance, demonstrating that informative datasets can effectively promote reasoning skills. Additionally, the authors highlight the limitations of current evaluation methods and advocate for the development of more challenging benchmarks to better assess reasoning in medical contexts.'}, 'zh': {'title': 'ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂåªÂ≠¶Êé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜAlphaMedÔºåËøôÊòØÈ¶ñ‰∏™ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂÆûÁé∞Êé®ÁêÜËÉΩÂäõÁöÑÂåªÂ≠¶Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºåÊó†ÈúÄÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊàñÈó≠Ê∫êÊ®°ÂûãÁöÑÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊï∞ÊçÆ„ÄÇAlphaMedÂú®ÂÖ≠‰∏™ÂåªÂ≠¶ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑSFT+RLËÆ≠ÁªÉÊ®°Âûã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊï∞ÊçÆÈõÜÁöÑ‰ø°ÊÅØÈáèÊòØÊé®ÁêÜÊÄßËÉΩÁöÑÂÖ≥ÈîÆÈ©±Âä®Âõ†Á¥†Ôºå‰ΩøÁî®‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂ§öÈ°πÈÄâÊã©ÈóÆÁ≠îÊï∞ÊçÆËøõË°åÁÆÄÁ∫¶ÁöÑRLËÉΩÂ§üÊúâÊïàËØ±ÂØºÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÂèëÁé∞ÔºåÂΩìÂâçËØÑ‰º∞ÊñπÊ≥ïÂ≠òÂú®Â±ÄÈôêÊÄßÔºåÈúÄË¶ÅÊõ¥ÂÖ∑ÊåëÊàòÊÄßÂíåÊé®ÁêÜÂØºÂêëÁöÑÂåªÂ≠¶ÈóÆÁ≠îÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14064', 'title': 'NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in\n  Brain MRI', 'url': 'https://huggingface.co/papers/2505.14064', 'abstract': 'NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously unknown categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.   We therefore present NOVA, a challenging, real-life evaluation-only benchmark of sim900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an extreme stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.', 'score': 16, 'issue_id': 3999, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '168ea71152c0b54f', 'authors': ['Cosmin I. Bercea', 'Jun Li', 'Philipp Raffler', 'Evamaria O. Riedel', 'Lena Schmitzer', 'Angela Kurz', 'Felix Bitzer', 'Paula Ro√üm√ºller', 'Julian Canisius', 'Mirjam L. Beyrle', 'Che Liu', 'Wenjia Bai', 'Bernhard Kainz', 'Julia A. Schnabel', 'Benedikt Wiestler'], 'affiliations': ['FAU Erlangen-N√ºrnberg', 'Helmholtz Center Munich', 'Imperial College London', 'Kings College London', 'Klinikum Rechts der Isar', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.14064.jpg', 'data': {'categories': ['#healthcare', '#cv', '#reasoning', '#benchmark', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': 'NOVA: –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –Ω–∞ –æ–±–æ–±—â–µ–Ω–∏–µ –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏', 'desc': 'NOVA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–¥–∫–∏—Ö –ø–∞—Ç–æ–ª–æ–≥–∏—è—Ö –ú–†–¢ –≥–æ–ª–æ–≤–Ω–æ–≥–æ –º–æ–∑–≥–∞. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 900 —Å–Ω–∏–º–∫–æ–≤ —Å 281 —Ä–µ–¥–∫–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–µ–π –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∞–Ω–æ–º–∞–ª–∏–π, –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. NOVA —Å–ª—É–∂–∏—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–æ–º –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Ä–∞–∑—Ä—ã–≤ –∫–∞–∫ –≤–æ –≤–Ω–µ—à–Ω–µ–º –≤–∏–¥–µ –æ–±—Ä–∞–∑—Ü–æ–≤, —Ç–∞–∫ –∏ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–¥—É—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º.'}, 'en': {'title': 'NOVA: Stress-Testing AI for Rare MRI Pathologies', 'desc': 'NOVA is a benchmark designed to evaluate vision-language models specifically on rare MRI pathologies that are clinically relevant. It focuses on out-of-distribution detection and open-world recognition, which are crucial for models to handle unseen data effectively. By using a dataset of 900 brain MRI scans with 281 rare conditions, NOVA tests models on their ability to generalize beyond their training data. The results from leading models show significant performance drops, highlighting the need for improved capabilities in detecting and reasoning about unknown anomalies in medical imaging.'}, 'zh': {'title': 'NOVAÔºöÊåëÊàòËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ®ÄÊúâÁóÖÁêÜËØÜÂà´', 'desc': 'NOVAÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Á®ÄÊúâ‰∏¥Â∫äÁõ∏ÂÖ≥MRIÁóÖÁêÜÊñπÈù¢ÁöÑÂü∫ÂáÜÔºåÊåëÊàòÊ®°ÂûãÂú®ÂàÜÂ∏ÉÂ§ñÂíåÂºÄÊîæ‰∏ñÁïåËØÜÂà´ËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´900‰∏™ËÑëÈÉ®MRIÊâ´ÊèèÔºåÊ∂µÁõñ281ÁßçÁ®ÄÊúâÁóÖÁêÜÂíå‰∏çÂêåÁöÑËé∑ÂèñÂçèËÆÆ„ÄÇÊØè‰∏™Ê°à‰æãÈÉΩÂåÖÊã¨‰∏∞ÂØåÁöÑ‰∏¥Â∫äÂèôËø∞ÂíåÂèåÁõ≤‰∏ìÂÆ∂ÁöÑËæπÁïåÊ°ÜÊ≥®ÈáäÔºåÊîØÊåÅÂºÇÂ∏∏ÂÆö‰Ωç„ÄÅËßÜËßâÊèèËø∞ÂíåËØäÊñ≠Êé®ÁêÜÁöÑËÅîÂêàËØÑ‰º∞„ÄÇNOVA‰ªéÊú™Áî®‰∫éËÆ≠ÁªÉÔºåÂõ†Ê≠§ÂÆÉÊòØÂØπÊ®°ÂûãÂú®ÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËÉΩÂäõÁöÑÊûÅÈôêÂéãÂäõÊµãËØïÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÂú®Â§ÑÁêÜÊú™Áü•ÂºÇÂ∏∏Êó∂ÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21505', 'title': "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective", 'url': 'https://huggingface.co/papers/2505.21505', 'abstract': "The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.", 'score': 14, 'issue_id': 3994, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'b77701cf90420b4c', 'authors': ['Shimao Zhang', 'Zhejian Lai', 'Xiang Liu', 'Shuaijie She', 'Xiao Liu', 'Yeyun Gong', 'Shujian Huang', 'Jiajun Chen'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21505.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#alignment'], 'emoji': 'üåê', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–π—Ä–æ–Ω–∞—Ö LLM', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–ª–∏—è–Ω–∏–µ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ–±—â–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–≤–æ–¥ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–ª–æ–≤–∞—Ä—è. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ü–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM.'}, 'en': {'title': 'Enhancing Multilingual Capabilities in LLMs through Neuron Identification', 'desc': "This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model's ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities."}, 'zh': {'title': 'ÁªÜÁ≤íÂ∫¶Á•ûÁªèÂÖÉËØÜÂà´ÔºåÊèêÂçáÂ§öËØ≠Ë®ÄËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÁ•ûÁªèÂÖÉËØÜÂà´ÁÆóÊ≥ïÔºåÁî®‰∫éÊ£ÄÊµãÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑËØ≠Ë®ÄÁâπÂÆöÁ•ûÁªèÂÖÉÂíåËØ≠Ë®ÄÊó†ÂÖ≥Á•ûÁªèÂÖÉ„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÂ§öËØ≠Ë®ÄÁêÜËß£„ÄÅÂÖ±‰∫´ËØ≠‰πâÊé®ÁêÜ„ÄÅÂ§öËØ≠Ë®ÄËæìÂá∫ËΩ¨Êç¢ÂíåËØçÊ±áÁ©∫Èó¥ËæìÂá∫Á≠âÊñπÈù¢ÂØπÂ§öËØ≠Ë®ÄÂØπÈΩêÂíåËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ≠òÂú®Âú®Â§ÑÁêÜ‰∏çÂêåËØ≠Ë®ÄÊó∂ÈÄâÊã©ÊÄßÊøÄÊ¥ªÁöÑËØ≠Ë®ÄÁâπÂÆöÁ•ûÁªèÂÖÉÔºåËøô‰∏∫Ê∑±ÂÖ•ÁêÜËß£LLMsÂú®Â§öËØ≠Ë®ÄÂú∫ÊôØ‰∏≠ÁöÑÊú∫Âà∂Êèê‰æõ‰∫ÜÊñ∞ËßÜËßí„ÄÇÈÄöËøáÂØπ‰∏çÂêåÁ±ªÂûãÁ•ûÁªèÂÖÉÁöÑÁ≥ªÁªüÂàÜÊûêÔºåÊàë‰ª¨‰∏∫Êõ¥Â•ΩÂú∞ÁêÜËß£LLMsÁöÑÂ§öËØ≠Ë®ÄÂØπÈΩêÂíåËÉΩÂäõÊèê‰æõ‰∫ÜÂÆûËØÅÁªìÊûúÂíåÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20275', 'title': 'ImgEdit: A Unified Image Editing Dataset and Benchmark', 'url': 'https://huggingface.co/papers/2505.20275', 'abstract': 'Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs, which contain both novel and complex single-turn edits, as well as challenging multi-turn tasks. To ensure the data quality, we employ a multi-stage pipeline that integrates a cutting-edge vision-language model, a detection model, a segmentation model, alongside task-specific in-painting procedures and strict post-processing. ImgEdit surpasses existing datasets in both task novelty and data quality. Using ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to process the reference image and editing prompt, which outperforms existing open-source models on multiple tasks, highlighting the value of ImgEdit and model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a benchmark designed to evaluate image editing performance in terms of instruction adherence, editing quality, and detail preservation. It includes a basic testsuite, a challenging single-turn suite, and a dedicated multi-turn suite. We evaluate both open-source and proprietary models, as well as ImgEdit-E1, providing deep analysis and actionable insights into the current behavior of image-editing models. The source data are publicly available on https://github.com/PKU-YuanGroup/ImgEdit.', 'score': 14, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'ab1a7940b7d4c31c', 'authors': ['Yang Ye', 'Xianyi He', 'Zongjian Li', 'Bin Lin', 'Shenghai Yuan', 'Zhiyuan Yan', 'Bohan Hou', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20275.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#optimization', '#cv', '#data'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'ImgEdit: –ø—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ImgEdit - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 1,2 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ –∏ –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å ImgEdit-E1, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ ImgEdit-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ImgEdit-E1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–¥–∞—á–∞–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Empowering Open-Source Image Editing with ImgEdit Dataset', 'desc': 'This paper presents ImgEdit, a new dataset designed to improve open-source image-editing models by providing 1.2 million high-quality image edit pairs. The dataset includes both simple and complex editing tasks, ensuring a wide range of challenges for model training. To maintain high data quality, a multi-stage pipeline is used, incorporating advanced models for vision-language processing, detection, and segmentation. The authors also introduce ImgEdit-E1, an editing model that outperforms existing open-source models, and ImgEdit-Bench, a benchmark for evaluating image editing performance across various tasks.'}, 'zh': {'title': 'ImgEditÔºöÈ´òË¥®ÈáèÂõæÂÉèÁºñËæëÁöÑÁ™ÅÁ†¥', 'desc': 'ÊúÄËøëÁîüÊàêÊ®°ÂûãÁöÑËøõÂ±ï‰ΩøÂæóÈ´ò‰øùÁúüÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÁîüÊàêÊàê‰∏∫ÂèØËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂºÄÊ∫êÂõæÂÉèÁºñËæëÊ®°Âûã‰ªçÁÑ∂ËêΩÂêé‰∫é‰∏ìÊúâÊ®°ÂûãÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈ´òË¥®ÈáèÊï∞ÊçÆÁöÑÁº∫‰πèÂíåÂü∫ÂáÜÊµãËØï‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜImgEditÔºåËøôÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÈ´òË¥®ÈáèÂõæÂÉèÁºñËæëÊï∞ÊçÆÈõÜÔºåÂåÖÂê´120‰∏á‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑÁºñËæëÂØπÔºåÊ∂µÁõñÊñ∞È¢ñÂíåÂ§çÊùÇÁöÑÂçïËΩÆÁºñËæë‰ª•ÂèäÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂ§öËΩÆ‰ªªÂä°„ÄÇÊàë‰ª¨‰ΩøÁî®Â§öÈò∂ÊÆµÊµÅÁ®ãÁ°Æ‰øùÊï∞ÊçÆË¥®ÈáèÔºåÊï¥Âêà‰∫ÜÂÖàËøõÁöÑËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÅÊ£ÄÊµãÊ®°Âûã„ÄÅÂàÜÂâ≤Ê®°Âûã‰ª•ÂèäÁâπÂÆö‰ªªÂä°ÁöÑ‰øÆÂ§çÁ®ãÂ∫èÂíå‰∏•Ê†ºÁöÑÂêéÂ§ÑÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21457', 'title': 'Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO', 'url': 'https://huggingface.co/papers/2505.21457', 'abstract': "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.", 'score': 11, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '6d338aff50466f43', 'authors': ['Muzhi Zhu', 'Hao Zhong', 'Canyu Zhao', 'Zongze Du', 'Zheng Huang', 'Mingyu Liu', 'Hao Chen', 'Cheng Zou', 'Jingdong Chen', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21457.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': 'ACTIVE-O3: –ù–∞–¥–µ–ª–µ–Ω–∏–µ MLLM –∞–∫—Ç–∏–≤–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ACTIVE-O3 - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥–µ–ª–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∑–∞–¥–∞—á–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è MLLM –∏ —Å–æ–∑–¥–∞—é—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ ACTIVE-O3 –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. ACTIVE-O3 —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–º —Ç–µ—Å—Ç–µ V*.'}, 'en': {'title': 'Empowering MLLMs with Active Perception for Smarter Decision-Making', 'desc': 'This paper introduces ACTIVE-O3, a reinforcement learning framework designed to enhance Multimodal Large Language Models (MLLMs) with active perception capabilities. Active perception involves strategically selecting where to focus attention to gather relevant information, which is crucial for effective decision-making in robotics. The authors highlight the limitations of the existing GPT-o3 model in terms of search efficiency and region selection accuracy. By establishing a benchmark suite for evaluating ACTIVE-O3, the paper aims to advance research in active perception for MLLMs, demonstrating strong performance in various tasks without needing explicit reasoning data.'}, 'zh': {'title': 'ÊèêÂçáÊú∫Âô®‰∫∫‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': '‰∏ªÂä®ËßÜËßâÔºå‰πüÁß∞‰∏∫‰∏ªÂä®ÊÑüÁü•ÔºåÊòØÊåá‰∏ªÂä®ÈÄâÊã©ËßÇÂØüÁöÑÊñπÂºèÂíå‰ΩçÁΩÆÔºå‰ª•Ëé∑Âèñ‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Êú∫Âô®‰∫∫Á≥ªÁªü‰∏≠ÁöÑ‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ACTIVE-O3„ÄÇÊàë‰ª¨ÂÆö‰πâ‰∫ÜMLLMsÁöÑ‰∏ªÂä®ÊÑüÁü•‰ªªÂä°ÔºåÂπ∂ÊåáÂá∫Áé∞ÊúâÊ®°ÂûãÂú®ÊêúÁ¥¢ÊïàÁéáÂíåÂå∫ÂüüÈÄâÊã©‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇÈÄöËøáÂª∫Á´ãÁªºÂêàÂü∫ÂáÜÊµãËØïÂ•ó‰ª∂ÔºåACTIVE-O3Âú®Â§ö‰∏™‰ªªÂä°‰∏≠Â±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÈõ∂-shotÊé®ÁêÜËÉΩÂäõÔºåÊé®Âä®‰∫Ü‰∏ªÂä®ÊÑüÁü•ÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21500', 'title': 'ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.21500', 'abstract': "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.", 'score': 10, 'issue_id': 3993, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'c8cec2407a7def0e', 'authors': ['Dingming Li', 'Hongxing Li', 'Zixuan Wang', 'Yuchen Yan', 'Hang Zhang', 'Siqi Chen', 'Guiyang Hou', 'Shengpei Jiang', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21500.jpg', 'data': {'categories': ['#benchmark', '#cv', '#reasoning', '#3d', '#games'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ViewSpatial-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ vision-language –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –∞–ª–ª–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –ø—è—Ç—å —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π 3D-–∫–æ–Ω–≤–µ–π–µ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ–∑–≤–æ–ª–∏–ª–∞ —É–ª—É—á—à–∏—Ç—å –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ 46.24%.'}, 'en': {'title': 'Enhancing VLMs with Multi-Viewpoint Spatial Reasoning', 'desc': "This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity's viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs' spatial reasoning capabilities."}, 'zh': {'title': 'ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ', 'desc': 'ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÁêÜËß£ÂíåÊé®ÁêÜËßÜËßâÂÜÖÂÆπÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶ÅË∑®ËßÜËßíÁêÜËß£ÂíåÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠‰ªçÈù¢‰∏¥ÈáçÂ§ßÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞ÂΩìÂâçÁöÑVLMs‰∏ªË¶ÅÂú®Ëá™Êàë‰∏≠ÂøÉÁöÑÁ©∫Èó¥Êé®ÁêÜ‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÈúÄË¶ÅÈááÁî®ÂÖ∂‰ªñÂÆû‰ΩìÁöÑÁ©∫Èó¥ÂèÇËÄÉÊ°ÜÊû∂Êó∂ÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞Êé®ÂπøÂà∞‰ªñÂøÉ‰∏≠ÂøÉËßÜËßí„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜViewSpatial-BenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®‰∏∫Â§öËßÜËßíÁ©∫Èó¥ÂÆö‰ΩçËØÜÂà´ËØÑ‰º∞ËÆæËÆ°ÁöÑÁªºÂêàÂü∫ÂáÜÔºåÊîØÊåÅËá™Âä®ÂåñÁöÑ3DÊ†áÊ≥®ÊµÅÁ®ãÁîüÊàêÁ≤æÁ°ÆÁöÑÊñπÂêëÊ†áÁ≠æ„ÄÇÈÄöËøáÂú®Êàë‰ª¨ÁöÑÂ§öËßÜËßíÁ©∫Èó¥Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉVLMsÔºåÊàë‰ª¨Âú®ÂêÑÈ°π‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü46.24%ÁöÑÊï¥‰ΩìÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21491', 'title': 'Frame In-N-Out: Unbounded Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.21491', 'abstract': 'Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.', 'score': 10, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'cf3b062e968f421f', 'authors': ['Boyang Wang', 'Xuweiyi Chen', 'Matheus Gadelha', 'Zezhou Cheng'], 'affiliations': ['Adobe Research', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.21491.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#diffusion', '#architecture', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ –∫–∞–¥—Ä–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏, –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ—Ö–Ω–∏–∫–µ Frame In –∏ Frame Out, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —É–ø—Ä–∞–≤–ª—è—Ç—å –æ–±—ä–µ–∫—Ç–∞–º–∏, –≤—Ö–æ–¥—è—â–∏–º–∏ –≤ –∫–∞–¥—Ä –∏–ª–∏ –≤—ã—Ö–æ–¥—è—â–∏–º–∏ –∏–∑ –Ω–µ–≥–æ. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Mastering Video Generation with Motion Control', 'desc': 'This paper addresses key challenges in video generation, specifically focusing on controllability, temporal coherence, and detail synthesis. It introduces a novel technique called Frame In and Frame Out, allowing users to manipulate objects in a video scene based on specified motion trajectories. The authors present a new dataset and evaluation protocol tailored for this task, along with a Diffusion Transformer architecture that preserves identity while enabling motion control. Results demonstrate that their method significantly improves upon existing video generation models.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÂèØÊéßÊÄß‰∏é‰∏ÄËá¥ÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÂÖ≥Ê≥®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÂèØÊéßÊÄß„ÄÅÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÁªÜËäÇÂêàÊàêÁ≠âÂÖ≥ÈîÆÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÂ∏ßËøõÂ∏ßÂá∫‚ÄùÁöÑÁîµÂΩ±ÊäÄÊúØÔºåÂÖÅËÆ∏Áî®Êà∑ÊéßÂà∂ÂõæÂÉè‰∏≠ÁöÑÂØπË±°Ëá™ÁÑ∂Âú∞Á¶ªÂºÄÊàñËøõÂÖ•Âú∫ÊôØ„ÄÇ‰∏∫ÊîØÊåÅËøô‰∏Ä‰ªªÂä°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂçäËá™Âä®Á≠ñÂàíÁöÑÊñ∞Êï∞ÊçÆÈõÜÔºåÂπ∂Âà∂ÂÆö‰∫ÜÈíàÂØπËØ•ËÆæÁΩÆÁöÑÁªºÂêàËØÑ‰º∞ÂçèËÆÆ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20322', 'title': 'Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms', 'url': 'https://huggingface.co/papers/2505.20322', 'abstract': 'Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.', 'score': 10, 'issue_id': 3990, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '63cd6df71ddaefa4', 'authors': ['Mengru Wang', 'Ziwen Xu', 'Shengyu Mao', 'Shumin Deng', 'Zhaopeng Tu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['NUS-NCS Joint Lab, Singapore', 'National University of Singapore', 'Tencent AI Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20322.jpg', 'data': {'categories': ['#rl', '#reasoning', '#security', '#training', '#alignment'], 'emoji': 'üéØ', 'ru': {'title': '–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Steering Target Atoms (STA) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å STA –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Å–æ–±—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –≥–∏–±–∫–æ—Å—Ç—å –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing Control in Language Models with Steering Target Atoms', 'desc': 'This paper introduces a new method called Steering Target Atoms (STA) to improve the control over language model generation. It addresses the challenge of intertwined internal representations in large models, which can hinder precise steering and lead to unintended consequences. By isolating and manipulating specific knowledge components, STA enhances the safety and reliability of model outputs. The experiments show that this approach is effective, especially in adversarial situations, and it also improves reasoning control in large models.'}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ÂºïÂØºÁõÆÊ†áÂéüÂ≠êÔºàSTAÔºâÔºåÊó®Âú®ÊèêÈ´òËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÈÄöËøá‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂàÜÁ¶ªÂíåÊìçÊéßÈ´òÁª¥Á©∫Èó¥‰∏≠ÁöÑÁü•ËØÜÁªÑ‰ª∂Ôºå‰ªéËÄåÂ¢ûÂº∫ÂØπÊ®°ÂûãË°å‰∏∫ÁöÑÊéßÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSTAÂú®ÂØπÊäóÊÄßÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂ∞ÜËøô‰∏ÄÂºïÂØºÁ≠ñÁï•Â∫îÁî®‰∫éÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Á≤æÁ°ÆÊé®ÁêÜÊéßÂà∂‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16901', 'title': 'Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks', 'url': 'https://huggingface.co/papers/2505.16901', 'abstract': "Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", 'score': 10, 'issue_id': 3994, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'a91f0c74f1c3c191', 'authors': ['Hongyuan Tao', 'Ying Zhang', 'Zhenhao Tang', 'Hongen Peng', 'Xukun Zhu', 'Bingchang Liu', 'Yingguang Yang', 'Ziyin Zhang', 'Zhaogui Xu', 'Haipeng Zhang', 'Linchao Zhu', 'Rui Wang', 'Hang Yu', 'Jianguo Li', 'Peng Di'], 'affiliations': ['Ant Group, Hangzhou, China', 'Shanghai Jiaotong University, Shanghai, China', 'ShanghaiTech University, Shanghai, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16901.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#games', '#architecture', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–ì—Ä–∞—Ñ–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–¥–∞: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Code Graph Models (CGM) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. CGM –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–æ–≤ –∫–æ–¥–∞ –≤ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —É–ª—É—á—à–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∏ —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ. –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –í —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –±–µ–∑–∞–≥–µ–Ω—Ç–Ω—ã–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–º graph RAG, –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ SWE-bench Lite, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Empowering Code Generation with Open-Source Graph Models', 'desc': 'This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark.'}, 'zh': {'title': 'ÂºÄÊ∫ê‰ª£Á†ÅÂõæÊ®°ÂûãÊèêÂçá‰ª£Á†ÅÁîüÊàêÊÄßËÉΩ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂºÄÊ∫ê‰ª£Á†ÅÂõæÊ®°ÂûãÔºàCode Graph Models, CGMsÔºâÔºåÊó®Âú®ÊèêÂçá‰ª£Á†ÅÁîüÊàê‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂ∞Ü‰ª£Á†ÅÂõæÁªìÊûÑÊï¥ÂêàÂà∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÔºåCGMsËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£‰ª£Á†ÅÂ∫ì‰∏≠ÁöÑÂáΩÊï∞ÂíåÊñá‰ª∂„ÄÇ‰∏é‰æùËµñ‰ª£ÁêÜÁöÑ‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏çÈúÄË¶Å‰ª£ÁêÜÔºåÁ°Æ‰øù‰∫ÜÊï∞ÊçÆÈöêÁßÅÂíåÊ®°ÂûãÂÆöÂà∂ÁöÑÁÅµÊ¥ªÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ÂºÄÊ∫êQwen2.5-72BÊ®°ÂûãÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®SWE-bench LiteÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫Ü43.00%ÁöÑËß£ÂÜ≥ÁéáÔºåË°®Áé∞‰ºò‰∫éÂÖ∂‰ªñÂºÄÊ∫êÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21493', 'title': 'Reinforcing General Reasoning without Verifiers', 'url': 'https://huggingface.co/papers/2505.21493', 'abstract': 'The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree.', 'score': 9, 'issue_id': 4003, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'f16e69476bc70236', 'authors': ['Xiangxin Zhou', 'Zichen Liu', 'Anya Sims', 'Haonan Wang', 'Tianyu Pang', 'Chongxuan Li', 'Liang Wang', 'Min Lin', 'Chao Du'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'National University of Singapore', 'Renmin University of China', 'Sea AI Lab, Singapore', 'University of Chinese Academy of Sciences', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2505.21493.jpg', 'data': {'categories': ['#optimization', '#math', '#rl', '#benchmark', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'VeriFree: –æ–±—É—á–µ–Ω–∏–µ LLM –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VeriFree. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∏–∑–±–µ–≥–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏-–≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–µ. VeriFree –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã–µ –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ –º–µ—Ç–æ–¥—ã —Å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMLU-Pro –∏ GPQA. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –≤ —Å—Ç–∏–ª–µ DeepSeek-R1-Zero –Ω–∞ –æ–±—â–∏–µ –æ–±–ª–∞—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ö–∏–º–∏—è, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ —ç–∫–æ–Ω–æ–º–∏–∫–∞.'}, 'en': {'title': 'VeriFree: Reinventing Reinforcement Learning for Language Models Without Verifiers', 'desc': 'This paper introduces a new method called VeriFree for training large language models (LLMs) without the need for a separate verifier. Traditional reinforcement learning (RL) approaches rely on rule-based verification, which limits their application in complex real-world domains. VeriFree simplifies the process by directly maximizing the likelihood of generating correct answers, eliminating the need for an additional verifier model. The results show that VeriFree not only reduces computational demands but also performs as well or better than existing verifier-based methods across various benchmarks.'}, 'zh': {'title': 'Êó†È™åËØÅÂô®ÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøëÔºå‰ΩøÁî®DeepSeek-R1-ZeroÈ£éÊ†ºÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÂíåÊï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ï‰ªÖÈôê‰∫éÂèØ‰ª•ËøõË°åÂü∫‰∫éËßÑÂàôÁöÑÁ≠îÊ°àÈ™åËØÅÁöÑ‰ªªÂä°ÔºåÊó†Ê≥ïËá™ÁÑ∂Êâ©Â±ïÂà∞ÂåñÂ≠¶„ÄÅÂåªÁñó„ÄÅÂ∑•Á®ã„ÄÅÊ≥ïÂæã„ÄÅÁîüÁâ©„ÄÅÂïÜ‰∏öÂíåÁªèÊµéÁ≠âÁé∞ÂÆû‰∏ñÁïåÈ¢ÜÂüü„ÄÇÁõÆÂâçÁöÑËß£ÂÜ≥ÊñπÊ°àÊòØ‰ΩøÁî®È¢ùÂ§ñÁöÑLLM‰Ωú‰∏∫Ê®°ÂûãÈ™åËØÅÂô®Ôºå‰ΩÜËøô‰ºöÂºïÂÖ•ÂØπÂº∫È™åËØÅÂô®LLMÁöÑ‰æùËµñ„ÄÅÂ•ñÂä±ÈªëÂÆ¢ÊîªÂáªÁöÑÈ£éÈô©‰ª•ÂèäÂú®ËÆ≠ÁªÉÊúüÈó¥Áª¥Êä§È™åËØÅÂô®Ê®°ÂûãÁöÑÂÆûÈôÖË¥üÊãÖ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÂπ∂Â∞ÜDeepSeek-R1-ZeroÈ£éÊ†ºÁöÑËÆ≠ÁªÉÊâ©Â±ïÂà∞‰∏ÄËà¨Êé®ÁêÜÈ¢ÜÂüüÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†È™åËØÅÂô®ÁöÑÊñπÊ≥ïÔºàVeriFreeÔºâÔºåËØ•ÊñπÊ≥ïÁªïËøáÁ≠îÊ°àÈ™åËØÅÔºåÁõ¥Êé•‰ΩøÁî®RLÊúÄÂ§ßÂåñÁîüÊàêÂèÇËÄÉÁ≠îÊ°àÁöÑÊ¶ÇÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21473', 'title': 'DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction', 'url': 'https://huggingface.co/papers/2505.21473', 'abstract': "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.", 'score': 8, 'issue_id': 3992, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'd4df44fe3325795e', 'authors': ['Yiheng Liu', 'Liao Qu', 'Huichao Zhang', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Xian Li', 'Shuai Wang', 'Daniel K. Du', 'Shu Cheng', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.21473.jpg', 'data': {'categories': ['#benchmark', '#cv', '#training', '#architecture', '#diffusion'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É', 'desc': 'DetailFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º –¥–µ—Ç–∞–ª–µ–π. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —É—Ö—É–¥—à–∞—é—â–∏–º—Å—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –¥–µ—Ç–∞–ª–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Å —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 8 —Ä–∞–∑.'}, 'en': {'title': 'DetailFlow: Efficient 1D Image Generation with Coarse-to-Fine Refinement', 'desc': 'This paper introduces DetailFlow, a new method for generating images using a 1D autoregressive approach. It employs a next-detail prediction strategy that allows the model to start with a broad image structure and gradually add finer details. By using a resolution-aware token sequence and a parallel inference mechanism, DetailFlow significantly improves generation speed and reduces the number of tokens needed for high-quality image synthesis. The results show that DetailFlow outperforms existing models in both image quality and efficiency, achieving better performance with fewer resources.'}, 'zh': {'title': 'DetailFlowÔºöÈ´òÊïàÁöÑËá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DetailFlowÁöÑÁ≤óÂà∞ÁªÜÁöÑ1DËá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÊñπÊ≥ïÔºåÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏ã‰∏Ä‰∏™ÁªÜËäÇÈ¢ÑÊµãÁ≠ñÁï•Êù•Âª∫Ê®°ÂõæÂÉè„ÄÇÈÄöËøáÂ≠¶‰π†‰∏Ä‰∏™ÂàÜËæ®ÁéáÊÑüÁü•ÁöÑÊ†áËÆ∞Â∫èÂàóÔºåÂπ∂‰ΩøÁî®ÈÄêÊ≠•ÈôçÁ∫ßÁöÑÂõæÂÉèËøõË°åÁõëÁù£ÔºåDetailFlow‰ΩøÁîüÊàêËøáÁ®ãËÉΩÂ§ü‰ªéÂÖ®Â±ÄÁªìÊûÑÂºÄÂßãÔºåÈÄêÊ≠•ÁªÜÂåñÁªÜËäÇ„ÄÇËØ•Á≤óÂà∞ÁªÜÁöÑ1DÊ†áËÆ∞Â∫èÂàó‰∏éËá™ÂõûÂΩíÊé®ÁêÜÊú∫Âà∂ÂæàÂ•ΩÂú∞ÂØπÈΩêÔºå‰∏∫Ëá™ÂõûÂΩíÊ®°ÂûãÁîüÊàêÂ§çÊùÇËßÜËßâÂÜÖÂÆπÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥Ëá™ÁÑ∂ÂíåÈ´òÊïàÁöÑÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDetailFlowÂú®ÂõæÂÉèÂêàÊàêË¥®ÈáèÂíåÊïàÁéá‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20287', 'title': 'MotionPro: A Precise Motion Controller for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20287', 'abstract': 'MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  \t\t\t\t\tAI-generated summary \t\t\t\t Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/.', 'score': 7, 'issue_id': 4004, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '032f4425f4fd7152', 'authors': ['Zhongwei Zhang', 'Fuchen Long', 'Zhaofan Qiu', 'Yingwei Pan', 'Wu Liu', 'Ting Yao', 'Tao Mei'], 'affiliations': ['HiDream.ai Inc.', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20287.jpg', 'data': {'categories': ['#benchmark', '#video', '#games', '#optimization'], 'emoji': 'üé¨', 'ru': {'title': '–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'MotionPro - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∏ –º–∞—Å–∫–∏ –¥–≤–∏–∂–µ–Ω–∏—è. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ –∏ —Ä–∞–∑–¥–µ–ª—è—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∫–∞–º–µ—Ä—ã. MotionPro –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç—ã –ø–æ—Ç–æ–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –≤–∏–¥–µ–æ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –≤–º–µ—Å—Ç–æ –±–æ–ª—å—à–∏—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö —è–¥–µ—Ä. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –º–∞—Å–∫—É –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –æ–±—â–µ–π –¥–∏–Ω–∞–º–∏–∫–∏ –¥–≤–∏–∂—É—â–∏—Ö—Å—è –æ–±–ª–∞—Å—Ç–µ–π.'}, 'en': {'title': 'Precision in Motion Control for Image-to-Video Generation', 'desc': 'MotionPro is a novel approach for image-to-video generation that enhances motion control by using region-wise trajectories and motion masks. This method allows for precise regulation of motion synthesis, effectively distinguishing between object and camera movements. Instead of relying on large Gaussian kernels, MotionPro utilizes localized trajectories to achieve fine-grained control over motion dynamics. Additionally, it incorporates a motion mask to improve video denoising and has been evaluated using a newly created benchmark dataset, MC-Bench, demonstrating its effectiveness in motion control tasks.'}, 'zh': {'title': 'Á≤æÁ°ÆËøêÂä®ÊéßÂà∂ÔºåÊèêÂçáÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàê', 'desc': 'MotionPro ÊòØ‰∏ÄÁßçÁ≤æÁ°ÆÁöÑËøêÂä®ÊéßÂà∂Âô®ÔºåÂà©Áî®Âå∫ÂüüËΩ®ËøπÂíåËøêÂä®Êé©Á†ÅÊù•ÊîπÂñÑÂõæÂÉèÂà∞ËßÜÈ¢ëÁöÑÁîüÊàê„ÄÇÂÆÉÈÄöËøá‰º∞ËÆ°ÊØè‰∏™ËÆ≠ÁªÉËßÜÈ¢ëÁöÑÊµÅÂä®ÂõæÔºåÂπ∂‰ªé‰∏≠ÈááÊ†∑Âå∫ÂüüËΩ®ËøπÔºåÊù•ÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑËøêÂä®ÂêàÊàê„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåMotionPro Áõ¥Êé•‰ΩøÁî®Â±ÄÈÉ®Âå∫ÂüüÂÜÖÁöÑËΩ®ËøπÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑËøêÂä®ÊéßÂà∂„ÄÇËØ•ÊñπÊ≥ïËøòÈÄöËøáÁâπÂæÅË∞ÉÂà∂Â¢ûÂº∫ËßÜÈ¢ëÂéªÂô™ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ MC-BenchÔºåÁî®‰∫éËØÑ‰º∞ÁªÜÁ≤íÂ∫¶ÂíåÂØπË±°Á∫ßÁöÑËøêÂä®ÊéßÂà∂ÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19099', 'title': 'SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.19099', 'abstract': "SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75\\%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.", 'score': 7, 'issue_id': 3991, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': 'e7a5589d299cea0e', 'authors': ['Kun Xiang', 'Heng Li', 'Terry Jingchen Zhang', 'Yinya Huang', 'Zirong Liu', 'Peixin Qu', 'Jixi He', 'Jiaqi Chen', 'Yu-Jie Yuan', 'Jianhua Han', 'Hang Xu', 'Hanhui Li', 'Mrinmaya Sachan', 'Xiaodan Liang'], 'affiliations': ['ETH Zurich', 'Huawei Noahs Ark Lab', 'Sun Yat-sen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.19099.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#benchmark', '#cv', '#multimodal'], 'emoji': 'üî¨', 'ru': {'title': 'SeePhys: –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM –≤ —Ñ–∏–∑–∏–∫–µ', 'desc': 'SeePhys - —ç—Ç–æ –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ —Ñ–∏–∑–∏–∫–µ. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 7 —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ —Ñ–∏–∑–∏–∫–∏ –∏ –≤–∫–ª—é—á–∞–µ—Ç 21 –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 75% –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –¥–∞–∂–µ —Å–∞–º—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ–Ω–µ–µ 60% –Ω–∞ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ.'}, 'en': {'title': 'SeePhys: Bridging Visual Reasoning and Physics in LLMs', 'desc': 'SeePhys is a new benchmark designed to test how well large language models (LLMs) can solve physics problems that involve visual reasoning. It includes a wide range of questions, from middle school to PhD level, and features many different types of diagrams that are crucial for finding the right answers. Unlike previous benchmarks, SeePhys requires models to extract visual information for 75% of the problems, making it essential for them to interpret diagrams accurately. The results show that even the best models struggle with these tasks, highlighting significant gaps in their ability to connect visual data with physics reasoning without relying heavily on text.'}, 'zh': {'title': 'SeePhysÔºöÊåëÊàòËßÜËßâÊé®ÁêÜ‰∏éÁâ©ÁêÜÈóÆÈ¢òËß£ÂÜ≥ÁöÑÂü∫ÂáÜ', 'desc': 'SeePhysÊòØ‰∏Ä‰∏™Â§ßÂûãÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Áâ©ÁêÜÈóÆÈ¢ò‰∏äÁöÑÊé®ÁêÜËÉΩÂäõÔºåÊ∂µÁõñ‰ªé‰∏≠Â≠¶Âà∞ÂçöÂ£´ËµÑÊ†ºËÄÉËØïÁöÑËåÉÂõ¥„ÄÇËØ•Âü∫ÂáÜÊ∂âÂèäÁâ©ÁêÜÂ≠¶ÁöÑ7‰∏™Âü∫Êú¨È¢ÜÂüüÔºåÂπ∂ÂåÖÂê´21Á±ªÈ´òÂ∫¶ÂºÇË¥®ÁöÑÂõæË°®„ÄÇ‰∏é‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏çÂêåÔºåSeePhys‰∏≠75%ÁöÑÈóÆÈ¢òÈúÄË¶ÅÊèêÂèñËßÜËßâ‰ø°ÊÅØÊâçËÉΩÂæóÂá∫Ê≠£Á°ÆÁ≠îÊ°àÔºåÊòæÁ§∫Âá∫ËßÜËßâ‰ø°ÊÅØÂú®Ëß£ÂÜ≥ÈóÆÈ¢ò‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑËßÜËßâÊé®ÁêÜÊ®°ÂûãÔºåÂÖ∂ÂáÜÁ°ÆÁéá‰πüÊú™ËÉΩË∂ÖËøá60%ÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÁêÜËß£ÊñπÈù¢ÁöÑÊ†πÊú¨ÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21494', 'title': 'Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment', 'url': 'https://huggingface.co/papers/2505.21494', 'abstract': "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.", 'score': 6, 'issue_id': 3992, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '9308405d203b4ba9', 'authors': ['Xiaojun Jia', 'Sensen Gao', 'Simeng Qin', 'Tianyu Pang', 'Chao Du', 'Yihao Huang', 'Xinfeng Li', 'Yiming Li', 'Bo Li', 'Yang Liu'], 'affiliations': ['MBZUAI, United Arab Emirates', 'Nanyang Technological University, Singapore', 'Sea AI Lab, Singapore', 'University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.21494.jpg', 'data': {'categories': ['#multimodal', '#security', '#training'], 'emoji': 'üéØ', 'ru': {'title': '–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π FOA-Attack. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–º –∏ –ª–æ–∫–∞–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω—è—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç–∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –≤–ª–∏—è–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.'}, 'en': {'title': 'Enhancing Adversarial Transferability with FOA-Attack', 'desc': 'This paper addresses the vulnerability of multimodal large language models (MLLMs) to transferable adversarial examples. The authors introduce a new attack method called FOA-Attack, which focuses on aligning both global and local features to enhance the transferability of adversarial samples. By utilizing cosine similarity for global feature alignment and optimal transport for local feature alignment, the method improves the effectiveness of attacks on closed-source models. The proposed approach outperforms existing methods in various experiments, demonstrating its robustness in generating transferable adversarial examples.'}, 'zh': {'title': 'ÊèêÂçáÂØπÊäóÊ†∑Êú¨ËΩ¨ÁßªËÉΩÂäõÁöÑFOA-AttackÊñπÊ≥ï', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂÆπÊòìÂèóÂà∞ÂèØËΩ¨ÁßªÁöÑÂØπÊäóÊ†∑Êú¨ÊîªÂáª„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂØπÈΩêÂÖ®Â±ÄÁâπÂæÅÊù•ÂÆûÁé∞ÁõÆÊ†áÊîªÂáªÔºå‰ΩÜÂøΩËßÜ‰∫ÜÂ±ÄÈÉ®‰ø°ÊÅØÁöÑ‰∏∞ÂØåÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁâπÂæÅÊúÄ‰ºòÂØπÈΩêÁöÑÈíàÂØπÊÄßÂèØËΩ¨ÁßªÂØπÊäóÊîªÂáªÊñπÊ≥ïÔºåÁß∞‰∏∫FOA-Attack„ÄÇÈÄöËøáÂºïÂÖ•ÂÖ®Â±ÄÁâπÂæÅÊçüÂ§±ÂíåÂ±ÄÈÉ®ËÅöÁ±ªÊúÄ‰ºò‰º†ËæìÊçüÂ§±ÔºåÊàë‰ª¨ÊòæËëóÊèêÈ´ò‰∫ÜÂØπÊäóÊ†∑Êú¨ÁöÑËΩ¨ÁßªËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20561', 'title': 'Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.20561', 'abstract': 'BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.', 'score': 4, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '4b7b0470932e5c49', 'authors': ['Shenao Zhang', 'Yaqing Wang', 'Yinxiao Liu', 'Tianqi Liu', 'Peter Grabowski', 'Eugene Ie', 'Zhaoran Wang', 'Yunxuan Li'], 'affiliations': ['Google', 'Google DeepMind', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20561.jpg', 'data': {'categories': ['#rl', '#training', '#rlhf', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'BARL: –£–º–Ω–µ–µ –∏—Å—Å–ª–µ–¥—É–µ–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞—Å—Å—É–∂–¥–∞–µ–º', 'desc': 'BARL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª—É—á—à–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. BARL –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä—É–µ—Ç —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ BARL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–∞—Ä–∫–æ–≤—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏.'}, 'en': {'title': 'Enhancing LLMs with Reflective Exploration through BARL', 'desc': 'The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods.'}, 'zh': {'title': 'BARLÔºöÊèêÂçáLLMÊÄßËÉΩÁöÑË¥ùÂè∂ÊñØËá™ÈÄÇÂ∫îÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂', 'desc': 'BARLÊòØ‰∏ÄÁßçË¥ùÂè∂ÊñØËá™ÈÄÇÂ∫îÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÈÄöËøáÊï¥ÂêàÂèçÊÄùÊé®ÁêÜÂíåÈ´òÊïàÊé¢Á¥¢ÔºåÊèêÂçá‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÈ©¨Â∞îÂèØÂ§´Âº∫ÂåñÂ≠¶‰π†ÈôêÂà∂‰∫ÜÊé¢Á¥¢ËøáÁ®ãÔºå‰ªÖ‰æùËµñÂΩìÂâçÁä∂ÊÄÅÁöÑÂéÜÂè≤‰∏ä‰∏ãÊñáÊù•Â≠¶‰π†ÊúÄ‰ºòÁ≠ñÁï•ÔºåÂØºËá¥ÂèçÊÄùÊé®ÁêÜÁöÑÂá∫Áé∞ÂíåÂÖ∂Âú®ÊµãËØïÊó∂ÁöÑÂ•ΩÂ§Ñ‰∏çÊòéÁ°Æ„ÄÇBARLÊ°ÜÊû∂ÈÄöËøá‰ºòÂåñÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºåÈºìÂä±Ê®°ÂûãÂú®Â•ñÂä±ÊúÄÂ§ßÂåñÂíå‰ø°ÊÅØÊî∂ÈõÜ‰πãÈó¥ËøõË°åÂπ≥Ë°°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBARLÂú®ÂêàÊàêÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠‰ºò‰∫é‰º†ÁªüÁöÑÈ©¨Â∞îÂèØÂ§´Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂ±ïÁé∞Âá∫Êõ¥È´òÁöÑ‰ª§ÁâåÊïàÁéáÂíåÊõ¥ÊúâÊïàÁöÑÊé¢Á¥¢ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20426', 'title': 'MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness', 'url': 'https://huggingface.co/papers/2505.20426', 'abstract': "Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/", 'score': 4, 'issue_id': 4002, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '29aacf61c54d1cda', 'authors': ['Yunlong Tang', 'Pinxin Liu', 'Mingqian Feng', 'Zhangyun Tan', 'Rui Mao', 'Chao Huang', 'Jing Bi', 'Yunzhong Xiao', 'Susan Liang', 'Hang Hua', 'Ali Vosoughi', 'Luchuan Song', 'Zeliang Zhang', 'Chenliang Xu'], 'affiliations': ['Carnegie Mellon University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20426.jpg', 'data': {'categories': ['#3d', '#cv', '#interpretability', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'üî¨', 'ru': {'title': 'MMPerspective: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MMPerspective - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (MLLM). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 10 –∑–∞–¥–∞—á –≤ —Ç—Ä–µ—Ö –∏–∑–º–µ—Ä–µ–Ω–∏—è—Ö: –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 2,711 —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û—Ü–µ–Ω–∫–∞ 43 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM –≤—ã—è–≤–∏–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–æ —Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏, –µ–µ –º–∞—Å—à—Ç–∞–±–æ–º –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã.'}, 'en': {'title': 'Evaluating Perspective Understanding in Multimodal Models', 'desc': 'This paper introduces MMPerspective, a benchmark designed to evaluate how well multimodal large language models (MLLMs) understand perspective geometry. It includes 10 tasks that assess three key areas: how models perceive perspective, their reasoning abilities, and their robustness to changes. The benchmark features over 2,700 images and more than 5,000 question-answer pairs that test various skills like recognizing vanishing points and understanding 3D spatial relationships. The study finds that while MLLMs perform well on basic tasks, they struggle with complex reasoning and maintaining spatial accuracy when faced with transformations, revealing important insights into their limitations and potential improvements.'}, 'zh': {'title': 'MMPerspectiveÔºöËØÑ‰º∞ËßÜËßíÁêÜËß£ÁöÑÂü∫ÂáÜ', 'desc': 'ÁêÜËß£ËßÜËßíÂØπ‰∫∫Á±ªËßÜËßâÊÑüÁü•Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßíÂá†‰ΩïÊñπÈù¢ÁöÑÁêÜËß£Á®ãÂ∫¶Â∞ö‰∏çÊòéÁ°Æ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMMPerspectiveÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°ÁöÑÂü∫ÂáÜÔºåÊó®Âú®ÈÄöËøá10‰∏™Á≤æÂøÉËÆæËÆ°ÁöÑ‰ªªÂä°Á≥ªÁªüÂú∞ËØÑ‰º∞MLLMsÂØπËßÜËßíÁöÑÁêÜËß£„ÄÇÂü∫ÂáÜÂåÖÂê´2711‰∏™ÁúüÂÆûÂíåÂêàÊàêÂõæÂÉèÂÆû‰æãÔºå‰ª•Âèä5083‰∏™ÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºåÊé¢ËÆ®ÂÖ≥ÈîÆËÉΩÂäõÔºåÂ¶ÇÊ∂àÂ§±ÁÇπÊÑüÁü•„ÄÅËßÜËßíÁ±ªÂûãÊé®ÁêÜÂíå3DÁ©∫Èó¥‰∏≠ÁöÑÁ∫øÂÖ≥Á≥ªÁêÜËß£Á≠â„ÄÇÈÄöËøáÂØπ43‰∏™ÊúÄÂÖàËøõÁöÑMLLMsËøõË°åÂÖ®Èù¢ËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Ê®°ÂûãÂú®Ë°®Èù¢ÊÑüÁü•‰ªªÂä°‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÁªÑÂêàÊé®ÁêÜÂíå‰øùÊåÅÁ©∫Èó¥‰∏ÄËá¥ÊÄßÊñπÈù¢Â≠òÂú®ÊòæËëóÂ±ÄÈôê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20289', 'title': 'VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual\n  Tool Selection', 'url': 'https://huggingface.co/papers/2505.20289', 'abstract': "VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems.", 'score': 4, 'issue_id': 3991, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '06184525a85012f4', 'authors': ['Zeyi Huang', 'Yuyang Ji', 'Anirudh Sundara Rajan', 'Zefan Cai', 'Wen Xiao', 'Junjie Hu', 'Yong Jae Lee'], 'affiliations': ['Microsoft', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.20289.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#optimization', '#cv', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'VisTA: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'VisTA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, VisTA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫–≤–æ–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤—ã–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VisTA –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.'}, 'en': {'title': 'Empowering Visual Agents with Autonomous Tool Selection', 'desc': 'VisTA is a reinforcement learning framework designed to improve visual reasoning by allowing agents to autonomously select and combine tools from a diverse library. Unlike traditional methods that either require extensive human supervision or lack active exploration, VisTA uses end-to-end reinforcement learning to refine tool selection strategies based on task outcomes. The framework employs Group Relative Policy Optimization (GRPO) to enable agents to discover effective pathways for tool selection without explicit reasoning guidance. Experiments show that VisTA significantly outperforms existing methods, particularly in challenging scenarios, demonstrating its potential for enhancing generalization and adaptability in visual reasoning tasks.'}, 'zh': {'title': 'VisTAÔºöËá™‰∏ªÈÄâÊã©Â∑•ÂÖ∑ÁöÑËßÜËßâÊé®ÁêÜÊñ∞Ê°ÜÊû∂', 'desc': 'VisTAÊòØ‰∏Ä‰∏™Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËá™‰∏ªÈÄâÊã©ÂíåÁªÑÂêàÂ§öÊ†∑ÂåñÂ∑•ÂÖ∑Êù•Â¢ûÂº∫ËßÜËßâÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåVisTA‰∏ç‰æùËµñ‰∫éËÆ≠ÁªÉÂâçÊèêÁ§∫ÊàñÂ§ßËßÑÊ®°ÂæÆË∞ÉÔºåËÄåÊòØÈÄöËøáÁ´ØÂà∞Á´ØÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•‰ºòÂåñÂ∑•ÂÖ∑ÈÄâÊã©Á≠ñÁï•„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®‰ªªÂä°ÁªìÊûú‰Ωú‰∏∫ÂèçÈ¶à‰ø°Âè∑ÔºåÂÖÅËÆ∏Êô∫ËÉΩ‰ΩìËá™‰∏ªÂèëÁé∞ÊúâÊïàÁöÑÂ∑•ÂÖ∑ÈÄâÊã©Ë∑ØÂæÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVisTAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂàÜÂ∏ÉÂ§ñÁ§∫‰æãÊó∂ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â¢ûÂº∫Ê≥õÂåñËÉΩÂäõÂíåÁÅµÊ¥ªÂà©Áî®Â§öÊ†∑ÂåñÂ∑•ÂÖ∑ÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21205', 'title': 'Sci-Fi: Symmetric Constraint for Frame Inbetweening', 'url': 'https://huggingface.co/papers/2505.21205', 'abstract': "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.", 'score': 3, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'a20a9eb43b42208f', 'authors': ['Liuhan Chen', 'Xiaodong Cun', 'Xiaoyu Li', 'Xianyi He', 'Shenghai Yuan', 'Jie Chen', 'Ying Shan', 'Li Yuan'], 'affiliations': ['ARC Lab, Tencent PCG', 'GVC Lab, Great Bay University', 'Rabbitpre Intelligence', 'Shenzhen Graduate School, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21205.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#video', '#training'], 'emoji': 'üéûÔ∏è', 'ru': {'title': '–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ –º–µ–∂–¥—É –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ –∫–æ–Ω–µ—á–Ω—ã–º –∫–∞–¥—Ä–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Sci-Fi, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ (I2V-DM). Sci-Fi –≤–≤–æ–¥–∏—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å EF-Net –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–µ—á–Ω–æ–º –∫–∞–¥—Ä–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–ª–∞–≤–Ω—ã—Ö –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏.'}, 'en': {'title': 'Achieving Harmony in Video Frame Synthesis with Sci-Fi', 'desc': 'This paper presents a new approach called Sci-Fi for generating intermediate video sequences from given start and end frames. The authors identify a limitation in existing methods that treat the start and end frame constraints equally, which can lead to poor video quality. Sci-Fi introduces a novel mechanism using a lightweight module, EF-Net, to enhance the influence of the end frame, ensuring it has a similar impact as the start frame. The results show that Sci-Fi produces smoother and more consistent transitions in generated videos compared to current state-of-the-art techniques.'}, 'zh': {'title': 'ÂÆûÁé∞Ëµ∑ÂßãÂ∏ß‰∏éÁªìÊùüÂ∏ßÁöÑÂØπÁß∞Á∫¶Êùü', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫Sci-FiÔºåÁî®‰∫éÂú®ÁªôÂÆöÁöÑËµ∑ÂßãÂ∏ßÂíåÁªìÊùüÂ∏ß‰πãÈó¥ÂêàÊàê‰∏≠Èó¥ËßÜÈ¢ëÂ∫èÂàó„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂ§ßÂûãÈ¢ÑËÆ≠ÁªÉÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºàI2V-DMsÔºâÔºå‰ΩÜÂú®ÂºïÂÖ•ÁªìÊùüÂ∏ßÁ∫¶ÊùüÊó∂Â≠òÂú®ËÆæËÆ°Áº∫Èô∑„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁÆÄÂçïÂú∞‰ΩøÁî®‰∏éËµ∑ÂßãÂ∏ßÁõ∏ÂêåÁöÑÊú∫Âà∂Êù•ÂºïÂÖ•ÁªìÊùüÂ∏ßÁ∫¶ÊùüÔºåÂèØËÉΩÊó†Ê≥ïÊúâÊïàÂΩ±Âìç‰∏≠Èó¥ÂÜÖÂÆπÔºå‰ªéËÄåÂØºËá¥ÁîüÊàêÂ∏ßÁöÑËøêÂä®‰∏ç‰∏ÄËá¥ÊàñÂ§ñËßÇÂ¥©Ê∫É„ÄÇSci-FiÈÄöËøáÂºïÂÖ•‰∏ÄÁßçÊîπËøõÁöÑÊú∫Âà∂ÂíåËΩªÈáèÁ∫ßÊ®°ÂùóEF-NetÔºå‰ΩøÁªìÊùüÂ∏ßÁ∫¶ÊùüÁöÑÂΩ±ÂìçÂäõ‰∏éËµ∑ÂßãÂ∏ßÁõ∏ÂΩìÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂíåË∞êÁöÑËøáÊ∏°ÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21178', 'title': 'Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.21178', 'abstract': 'As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model\'s reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the "walk before you run" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.', 'score': 3, 'issue_id': 3995, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'e8fe10bbb758cc6a', 'authors': ['Mingyang Song', 'Mao Zheng'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.21178.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#rl', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ö—Ä–∞—Ç–∫–æ—Å—Ç—å - —Å–µ—Å—Ç—Ä–∞ —Ç–∞–ª–∞–Ω—Ç–∞: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ú–µ—Ç–æ–¥ ConciseR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª–∞–∫–æ–Ω–∏—á–Ω—ã—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø —É—Å–∏–ª–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∞ –≤—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ ConciseR –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'ConciseR: Streamlining Reasoning in Large Language Models', 'desc': 'This paper introduces ConciseR, a two-stage reinforcement learning framework designed to improve the reasoning capabilities of Large Language Models (LLMs) by generating more concise Chain-of-Thought (CoT) responses. The first stage enhances reasoning through Group Relative Policy Optimization with dynamic sampling, while the second stage focuses on enforcing conciseness using Length-aware Group Relative Policy Optimization. The approach addresses the issue of overthinking in LLMs, which often leads to redundant responses in long CoT outputs. Experimental results show that ConciseR outperforms existing state-of-the-art models across various reasoning benchmarks, demonstrating its effectiveness in producing efficient and concise reasoning.'}, 'zh': {'title': 'ÁÆÄÊ¥ÅÊé®ÁêÜÔºåÊèêÂçáLLMsÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ConciseRÁöÑ‰∏§Èò∂ÊÆµÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõÂπ∂ÂÆûÁé∞ÁÆÄÊ¥ÅÁöÑÊé®ÁêÜ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøá‰ΩøÁî®Êõ¥Â§öÁöÑËÆ≠ÁªÉÊ≠•È™§ÔºåÈááÁî®ÊîπËøõÁöÑÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPO++ÔºâÊù•ÊøÄÂä±Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂàôÈÄöËøáÈïøÂ∫¶ÊÑüÁü•ÁöÑÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàL-GRPOÔºâÊù•Âº∫Âà∂ÊâßË°åÁÆÄÊ¥ÅÊÄßÔºåÊèêÈ´òÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåConciseRÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÁîüÊàêÁöÑÊé®ÁêÜÂìçÂ∫îÊõ¥‰∏∫ÁÆÄÊ¥ÅÔºåË∂ÖË∂ä‰∫ÜÊúÄÊñ∞ÁöÑÊé®ÁêÜÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21070', 'title': 'Minute-Long Videos with Dual Parallelisms', 'url': 'https://huggingface.co/papers/2505.21070', 'abstract': 'Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54times lower latency and 1.48times lower memory cost on 8timesRTX 4090 GPUs.', 'score': 3, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'c777be11d4844462', 'authors': ['Zeqing Wang', 'Bowen Zheng', 'Xingyi Yang', 'Yuecong Xu', 'Xinchao Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'National University of Singapore', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21070.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': 'üéûÔ∏è', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer (DiT), –Ω–∞–∑—ã–≤–∞–µ–º—É—é DualParal. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤, —Ç–∞–∫ –∏ —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ –º–µ–∂–¥—É GPU –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–ª–æ—á–Ω—É—é —Å—Ö–µ–º—É —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –¥–ª–∏–Ω–æ–π 1025 –∫–∞–¥—Ä–æ–≤ —Å –¥–æ 6,54 —Ä–∞–∑ –º–µ–Ω—å—à–µ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ 1,48 —Ä–∞–∑ –º–µ–Ω—å—à–∏–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏ –Ω–∞ 8 GPU RTX 4090.'}, 'en': {'title': 'Revolutionizing Video Generation with DualPar Efficiency!', 'desc': 'The paper presents a new method called DualPar for improving the efficiency of video generation using Diffusion Transformer (DiT) models. It addresses the high latency and memory costs associated with generating long videos by distributing the workload across multiple GPUs. The method uses a block-wise denoising approach to maintain synchronized noise levels while allowing for parallel processing of frames and model layers. Additionally, it introduces a feature cache and coordinated noise initialization to optimize performance, resulting in faster and more efficient video generation without artifacts.'}, 'zh': {'title': 'È´òÊïàÁîüÊàêÊó†ÈôêÈïøËßÜÈ¢ëÁöÑÂàõÊñ∞Á≠ñÁï•', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTÔºâÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁîüÊàêÊó∂ÁöÑÂ§ÑÁêÜÂª∂ËøüÂíåÂÜÖÂ≠òÊàêÊú¨ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂàÜÂ∏ÉÂºèÊé®ÁêÜÁ≠ñÁï•ÔºåÁß∞‰∏∫DualParalÔºåÈÄöËøáÂú®Â§ö‰∏™GPU‰∏äÂπ∂Ë°åÂ§ÑÁêÜÊó∂Èó¥Â∏ßÂíåÊ®°ÂûãÂ±ÇÊù•ÊèêÈ´òÊïàÁéá„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÊâ©Êï£Ê®°ÂûãÂØπÂô™Â£∞Ê∞¥Âπ≥ÂêåÊ≠•ÁöÑË¶ÅÊ±ÇÔºåÊàë‰ª¨ÈááÁî®‰∫ÜÂùóÁ∫ßÂéªÂô™ÊñπÊ°àÔºå‰ΩøÂæóÊØè‰∏™GPUÂ§ÑÁêÜÁâπÂÆöÁöÑÂ∏ßÂùóÂíåÂ±ÇÔºåÂêåÊó∂ÂÆûÁé∞ÂºÇÊ≠•ËÆ°ÁÆóÂíåÈÄö‰ø°„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÊó∂ÊòæËëóÈôç‰Ωé‰∫ÜÂª∂ËøüÂíåÂÜÖÂ≠òÊ∂àËÄóÔºåËÉΩÂ§üÈ´òÊïàÁîüÊàêÊó†ÈôêÈïøÁöÑËßÜÈ¢ë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19433', 'title': 'Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic\n  Capabilities in LLM Compression', 'url': 'https://huggingface.co/papers/2505.19433', 'abstract': "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.", 'score': 3, 'issue_id': 3995, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '1fb4272abd1ee9c8', 'authors': ['Peijie Dong', 'Zhenheng Tang', 'Xiang Liu', 'Lujun Li', 'Xiaowen Chu', 'Bo Li'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (GuangZhou)'], 'pdf_title_img': 'assets/pdf/title_img/2505.19433.jpg', 'data': {'categories': ['#agents', '#optimization', '#inference', '#benchmark'], 'emoji': 'üî¨', 'ru': {'title': 'ACBench: –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Å–∂–∞—Ç—ã—Ö LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ACBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è —Å–∂–∞—Ç–∏—è –Ω–∞ –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). ACBench –≤–∫–ª—é—á–∞–µ—Ç 12 –∑–∞–¥–∞—á –ø–æ 4 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º, —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä—É–Ω–∏–Ω–≥–∞, –∞ —Ç–∞–∫–∂–µ 15 –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ 4-–±–∏—Ç–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∞ 10-15%. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ERank, Top-k Ranking Correlation –∏ Energy –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞.'}, 'en': {'title': 'Optimizing LLM Compression for Real-World Agentic Tasks', 'desc': 'This paper presents the Agent Compression Benchmark (ACBench), which evaluates how post-training compression affects the agentic capabilities of large language models (LLMs). Unlike existing benchmarks that focus solely on language modeling and understanding, ACBench assesses 12 tasks across four key capabilities, including workflow generation and long-context retrieval. The study explores various compression techniques, such as quantization and pruning, across multiple LLMs of different sizes. Results indicate that while some compression methods maintain performance in certain tasks, they can significantly reduce accuracy in real-world applications, highlighting the need for careful optimization in LLM deployment.'}, 'zh': {'title': '‰ª£ÁêÜÂéãÁº©Âü∫ÂáÜÔºö‰ºòÂåñLLMÁöÑËÉΩÂäõ‰∏éÊïàÁéá', 'desc': 'ÂêéËÆ≠ÁªÉÂéãÁº©ÊäÄÊúØÂèØ‰ª•ÂáèÂ∞ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÊàêÊú¨Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑËµÑÊ∫êÈÉ®ÁΩ≤„ÄÇÁé∞ÊúâÁöÑÂéãÁº©Âü∫ÂáÜ‰∏ªË¶ÅÂÖ≥Ê≥®ËØ≠Ë®ÄÂª∫Ê®°ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£‰ªªÂä°ÔºåÂøΩËßÜ‰∫ÜÊ®°ÂûãÂú®Â∑•‰ΩúÊµÅ„ÄÅÂ∑•ÂÖ∑‰ΩøÁî®„ÄÅÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÂÆûÈôÖÂ∫îÁî®Á≠âÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰ª£ÁêÜÂéãÁº©Âü∫ÂáÜÔºàACBenchÔºâÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢ËØÑ‰º∞ÂéãÁº©ÂØπLLMs‰ª£ÁêÜËÉΩÂäõÂΩ±ÂìçÁöÑÂü∫ÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå4‰ΩçÈáèÂåñÂú®Â∑•‰ΩúÊµÅÁîüÊàêÂíåÂ∑•ÂÖ∑‰ΩøÁî®ÊñπÈù¢‰øùÊåÅËâØÂ•ΩÊÄßËÉΩÔºå‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®ÂáÜÁ°ÆÊÄß‰∏ä‰∏ãÈôç‰∫Ü10%-15%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19314', 'title': 'SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline', 'url': 'https://huggingface.co/papers/2505.19314', 'abstract': "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.", 'score': 3, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': 'd58bb66dfe2fc291', 'authors': ['Helin Wang', 'Jiarui Hai', 'Dongchao Yang', 'Chen Chen', 'Kai Li', 'Junyi Peng', 'Thomas Thebaud', 'Laureano Moro Velazquez', 'Jesus Villalba', 'Najim Dehak'], 'affiliations': ['Brno University of Technology', 'Johns Hopkins University', 'Nanyang Technological University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19314.jpg', 'data': {'categories': ['#audio'], 'emoji': 'üéôÔ∏è', 'ru': {'title': 'SoloSpeech: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': 'SoloSpeech - —ç—Ç–æ –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –∏–∑ —Å–º–µ—Å–∏ –≥–æ–ª–æ—Å–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–∂–∞—Ç–∏–µ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏—é. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å - —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞—É–¥–∏–æ. SoloSpeech –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Ä–∞–∑–±–æ—Ä—á–∏–≤–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤—É –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Libri2Mix, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –æ—Ç–ª–∏—á–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'SoloSpeech: Revolutionizing Target Speech Extraction with Generative Techniques', 'desc': "This paper introduces SoloSpeech, a new approach for Target Speech Extraction (TSE) that effectively isolates a target speaker's voice from a mix of multiple speakers. Unlike traditional discriminative models that can produce artifacts and lack naturalness, SoloSpeech employs a cascaded generative pipeline that includes processes for compression, extraction, reconstruction, and correction. It utilizes a speaker-embedding-free target extractor that aligns the latent spaces of cue audio and mixture audio, enhancing performance and reducing discrepancies. Evaluated on the Libri2Mix dataset, SoloSpeech sets a new benchmark for intelligibility and quality in TSE, showing strong generalization capabilities in diverse real-world scenarios."}, 'zh': {'title': 'SoloSpeechÔºöÊèêÂçáÁõÆÊ†áËØ≠Èü≥ÊèêÂèñÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÁõÆÊ†áËØ≠Èü≥ÊèêÂèñÔºàTSEÔºâÊó®Âú®‰ªéÂ§ö‰∏™ËØ¥ËØùËÄÖÁöÑÊ∑∑ÂêàÈü≥È¢ë‰∏≠ÂàÜÁ¶ªÂá∫ÁõÆÊ†áËØ¥ËØùËÄÖÁöÑÂ£∞Èü≥ÔºåÈÄöÂ∏∏Âà©Áî®‰Ωú‰∏∫ËæÖÂä©Èü≥È¢ëÁöÑËØ¥ËØùËÄÖÁâπÂæÅÁ∫øÁ¥¢„ÄÇÂ∞ΩÁÆ°ËøëÊúüÁöÑTSEËøõÂ±ï‰∏ªË¶ÅÈááÁî®‰∫ÜÈ´òÊÑüÁü•Ë¥®ÈáèÁöÑÂà§Âà´Ê®°ÂûãÔºå‰ΩÜËøô‰∫õÊ®°ÂûãÂ∏∏Â∏∏ÂºïÂÖ•‰∏çÂøÖË¶ÅÁöÑ‰º™ÂΩ±ÔºåÈôç‰ΩéËá™ÁÑ∂ÊÄßÔºåÂπ∂ÂØπËÆ≠ÁªÉÂíåÊµãËØïÁéØÂ¢É‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊïèÊÑü„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÁîüÊàêÊ®°ÂûãÂú®ÊÑüÁü•Ë¥®ÈáèÂíåÂèØÊáÇÊÄßÊñπÈù¢ÊªûÂêé„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSoloSpeechÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ∫ßËÅîÁîüÊàêÁÆ°ÈÅìÔºåÈõÜÊàê‰∫ÜÂéãÁº©„ÄÅÊèêÂèñ„ÄÅÈáçÂª∫Âíå‰øÆÊ≠£ËøáÁ®ãÔºåËÉΩÂ§üÂú®ÁõÆÊ†áËØ≠Èü≥ÊèêÂèñÂíåËØ≠Èü≥ÂàÜÁ¶ª‰ªªÂä°‰∏≠ÂÆûÁé∞Êñ∞ÁöÑÊúÄÂÖàËøõÁöÑÂèØÊáÇÊÄßÂíåË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20321', 'title': 'BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases', 'url': 'https://huggingface.co/papers/2505.20321', 'abstract': 'BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.', 'score': 3, 'issue_id': 4001, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '3f6bd45ed0456607', 'authors': ['Mathew J. Koretsky', 'Maya Willey', 'Adi Asija', 'Owen Bianchi', 'Chelsea X. Alvarado', 'Tanay Nayak', 'Nicole Kuznetsov', 'Sungwon Kim', 'Mike A. Nalls', 'Daniel Khashabi', 'Faraz Faghri'], 'affiliations': ['Center for Alzheimers Disease and Related Dementias, NIA, NIH', 'DataTecnica LLC', 'Johns Hopkins University', 'Laboratory of Neurogenetics, NIA, NIH'], 'pdf_title_img': 'assets/pdf/title_img/2505.20321.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multimodal', '#benchmark', '#reasoning'], 'emoji': 'üß¨', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ-SQL –∑–∞–¥–∞—á–∞—Ö –¥–ª—è –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—ã', 'desc': 'BiomedSQL - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ SQL, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –±–æ–ª—å—à—É—é –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 68 000 —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å/SQL-–∑–∞–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π BigQuery. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π –≤—ã–≤–æ–¥–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º —É—Ä–æ–≤–Ω–µ–º.'}, 'en': {'title': 'Bridging the Gap in Biomedical Text-to-SQL Reasoning', 'desc': 'BiomedSQL is a benchmark designed to assess scientific reasoning in text-to-SQL tasks specifically within the biomedical domain. It highlights the challenges faced by existing models in translating qualitative scientific questions into executable SQL queries, especially when implicit reasoning is required. The benchmark includes 68,000 question/SQL query/answer triples based on a comprehensive biomedical knowledge base, which necessitates understanding complex domain-specific criteria. Evaluation of various large language models (LLMs) shows significant performance gaps, indicating the need for improved systems that can effectively support scientific discovery through advanced reasoning capabilities.'}, 'zh': {'title': 'BiomedSQLÔºöÊé®Âä®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÁöÑÁßëÂ≠¶Êé®ÁêÜ', 'desc': 'BiomedSQLÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞ÊñáÊú¨Âà∞SQLÁîüÊàê‰∏≠ÁöÑÁßëÂ≠¶Êé®ÁêÜËÉΩÂäõÔºåÁâπÂà´ÊòØÂú®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüü„ÄÇÂÆÉÂåÖÂê´68,000‰∏™ÈóÆÈ¢ò„ÄÅSQLÊü•ËØ¢ÂíåÁ≠îÊ°àÁöÑ‰∏âÂÖÉÁªÑÔºåÂü∫‰∫é‰∏Ä‰∏™Êï¥Âêà‰∫ÜÂü∫Âõ†-ÁñæÁóÖÂÖ≥ËÅî„ÄÅÁªÑÂ≠¶Êï∞ÊçÆÂõ†ÊûúÊé®Êñ≠ÂíåËçØÁâ©ÊâπÂáÜËÆ∞ÂΩïÁöÑÁü•ËØÜÂ∫ì„ÄÇÁé∞ÊúâÁöÑÊñáÊú¨Âà∞SQLÁ≥ªÁªüÂú®Â∞ÜÁßëÂ≠¶ÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÂèØÊâßË°åÁöÑSQLÊó∂ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈöêÂê´È¢ÜÂüüÊé®ÁêÜÊó∂ÔºåË°®Áé∞‰∏ç‰Ω≥„ÄÇÈÄöËøáÂØπÂ§öÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËØÑ‰º∞ÔºåBiomedSQLÊè≠Á§∫‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ùÔºå‰∏∫ÊîØÊåÅÁßëÂ≠¶ÂèëÁé∞ÁöÑÊñáÊú¨Âà∞SQLÁ≥ªÁªüÁöÑËøõÊ≠•Â•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17005', 'title': 'R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17005', 'abstract': "R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus.", 'score': 3, 'issue_id': 3996, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'c185506804508a79', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Wenqing Tian', 'Zhipeng Chen', 'Yuhuan Wu', 'Jiahao Zhao', 'Yingqian Min', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['Beijing Institute of Technology', 'DataCanvas Alaya NeW', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17005.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#reasoning', '#rag'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è', 'desc': 'R1-Searcher++ - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—É—Ç–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –∑–∞—Ç–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π. R1-Searcher++ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ—á–µ—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –≤–Ω–µ—à–Ω–∏–º –ø–æ–∏—Å–∫–æ–º, –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É–ª—É—á—à–∞—è —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing LLMs with Adaptive Knowledge Integration', 'desc': "R1-Searcher++ is a new framework that enhances Large Language Models (LLMs) by combining internal knowledge with external information through a two-stage training process. The first stage, called SFT Cold-start, helps the model learn basic formats, while the second stage uses Reinforcement Learning (RL) to improve knowledge acquisition dynamically. This RL phase encourages the model to explore and rewards it for effectively using its internal knowledge, while also allowing it to memorize and integrate new information from external sources. As a result, R1-Searcher++ significantly boosts the model's reasoning efficiency and performance compared to existing methods."}, 'zh': {'title': 'R1-Searcher++ÔºöÊô∫ËÉΩÊï¥ÂêàÂÜÖÂ§ñÈÉ®Áü•ËØÜÁöÑÊ°ÜÊû∂', 'desc': 'R1-Searcher++ÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÔºåÂ¢ûÂº∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËÉΩÂäõÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Êï¥ÂêàÂÜÖÈÉ®ÂíåÂ§ñÈÉ®Áü•ËØÜ„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàËøõË°åÂÜ∑ÂêØÂä®ÁöÑÂàùÂßãÊ†ºÂºèÂ≠¶‰π†ÔºåÁÑ∂ÂêéÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõË°åÂä®ÊÄÅÁü•ËØÜËé∑Âèñ„ÄÇÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÈááÁî®ÁªìÊûúÁõëÁù£ÔºåÈºìÂä±Ê®°ÂûãÊé¢Á¥¢ÔºåÂêåÊó∂ÂºïÂÖ•Â•ñÂä±Êú∫Âà∂‰ª•‰øÉËøõÂÜÖÈÉ®Áü•ËØÜÁöÑÂà©Áî®ÔºåÂπ∂ÁªìÂêàËÆ∞ÂøÜÊú∫Âà∂‰∏çÊñ≠Âê∏Êî∂Ê£ÄÁ¥¢Âà∞ÁöÑ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåR1-Searcher++Âú®Ê£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÊïàÁéáÂíåÊÄßËÉΩ‰∏ä‰ºò‰∫é‰πãÂâçÁöÑRAGÂíåÊé®ÁêÜÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11277', 'title': 'Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs', 'url': 'https://huggingface.co/papers/2505.11277', 'abstract': "AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.", 'score': 3, 'issue_id': 3995, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'ec7d9a17f0e241f5', 'authors': ['Yaorui Shi', 'Shihan Li', 'Chang Wu', 'Zhiyuan Liu', 'Junfeng Fang', 'Hengxing Cai', 'An Zhang', 'Xiang Wang'], 'affiliations': ['DP Technology', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.11277.jpg', 'data': {'categories': ['#optimization', '#rag', '#rl', '#reasoning', '#benchmark'], 'emoji': 'üîç', 'ru': {'title': 'AutoRefine: –£–º–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': 'AutoRefine - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—è–µ—Ç –∑–Ω–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. AutoRefine –≤–≤–æ–¥–∏—Ç —ç—Ç–∞–ø—ã —É—Ç–æ—á–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏ –ø–æ–∏—Å–∫–∞, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å, –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤—ã–≤–∞—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ AutoRefine –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Refine Your Search, Enhance Your Answers!', 'desc': "AutoRefine is a reinforcement learning framework designed to improve the performance of large language models (LLMs) in complex question-answering tasks. It enhances retrieval-augmented reasoning by allowing LLMs to iteratively refine their knowledge and optimize their search processes. The framework introduces a 'search-and-refine-during-think' approach, where the model filters and organizes information before generating answers. Experiments show that AutoRefine outperforms existing methods, especially in scenarios requiring multi-hop reasoning, by producing higher-quality searches and synthesizing evidence more effectively."}, 'zh': {'title': 'AutoRefineÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂', 'desc': 'AutoRefineÊòØ‰∏ÄÁßçÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËø≠‰ª£‰ºòÂåñÁü•ËØÜÂíåÊêúÁ¥¢ËøáÁ®ãÊù•Â¢ûÂº∫Ê£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜ„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∫Ü‚ÄúÊÄùËÄÉÊó∂ÊêúÁ¥¢‰∏éÁ≤æÁÇº‚ÄùÁöÑÊñ∞ËåÉÂºèÔºåÂú®ËøûÁª≠ÁöÑÊêúÁ¥¢Ë∞ÉÁî®‰πãÈó¥ÂºïÂÖ•‰∫ÜÊòéÁ°ÆÁöÑÁü•ËØÜÁ≤æÁÇºÊ≠•È™§Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®ÁîüÊàêÁ≠îÊ°à‰πãÂâçËøáÊª§„ÄÅÊèêÁÇºÂíåÁªÑÁªáËØÅÊçÆ„ÄÇÈÄöËøáÁªìÂêàÁâπÂÆöÁöÑÊ£ÄÁ¥¢Â•ñÂä±ÂíåÁ≠îÊ°àÊ≠£Á°ÆÊÄßÂ•ñÂä±ÔºåAutoRefineÂú®Â§çÊùÇÁöÑÂ§öË∑≥ÈóÆÁ≠î‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAutoRefineËÉΩÂ§üËøõË°åÊõ¥È¢ëÁπÅ‰∏îÈ´òË¥®ÈáèÁöÑÊêúÁ¥¢ÔºåÊúâÊïàÂêàÊàêËØÅÊçÆÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19973', 'title': 'DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response', 'url': 'https://huggingface.co/papers/2505.19973', 'abstract': 'DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.', 'score': 2, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'c4a0e825312e2a47', 'authors': ['Bilel Cherif', 'Tamas Bisztray', 'Richard A. Dubniczky', 'Aaesha Aldahmani', 'Saeed Alshehhi', 'Norbert Tihanyi'], 'affiliations': ['E√∂tv√∂s Lor√°nd University, Budapest, Hungary', 'Technology Innovation Institute, Abu Dhabi, UAE', 'University of Oslo, Oslo, Norway'], 'pdf_title_img': 'assets/pdf/title_img/2505.19973.jpg', 'data': {'categories': ['#science', '#hallucinations', '#benchmark', '#dataset', '#optimization', '#reasoning'], 'emoji': 'üîç', 'ru': {'title': '–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–π –∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏–∫–∏', 'desc': 'DFIR-Metric - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π –∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏–∫–∏. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ç–µ—Å—Ç—ã –Ω–∞ –∑–Ω–∞–Ω–∏—è, —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–ª—É—á–∞–∏ –∞–Ω–∞–ª–∏–∑–∞. Benchmark –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç 14 LLM –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É - Task Understanding Score (TUS), –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –ø–æ—á—Ç–∏ –Ω—É–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.'}, 'en': {'title': 'Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark', 'desc': 'The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics.'}, 'zh': {'title': 'DFIR-MetricÔºöÊï∞Â≠óÂèñËØÅ‰∏≠ÁöÑËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞Êñ∞Ê†áÂáÜ', 'desc': 'DFIR-Metric ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠óÂèñËØÅÈ¢ÜÂüüË°®Áé∞ÁöÑÂü∫ÂáÜÂ∑•ÂÖ∑„ÄÇÂÆÉÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÁü•ËØÜËØÑ‰º∞„ÄÅÁé∞ÂÆûÂèñËØÅÊåëÊàòÂíåÂÆûÈôÖÂàÜÊûêÊ°à‰æãÔºåÊó®Âú®ÂÖ®Èù¢ÊµãËØïÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂØπ 14 ‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËØÑ‰º∞ÔºåÁ†îÁ©∂ËÄÖÂàÜÊûê‰∫ÜÂÆÉ‰ª¨Âú®ÂáÜÁ°ÆÊÄßÂíå‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÊñ∞ÂºïÂÖ•ÁöÑ‰ªªÂä°ÁêÜËß£ÂàÜÊï∞ÔºàTUSÔºâÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞ËØÑ‰º∞Ê®°ÂûãÂú®Êé•ËøëÈõ∂ÂáÜÁ°ÆÁéáÁöÑÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18657', 'title': 'MLLMs are Deeply Affected by Modality Bias', 'url': 'https://huggingface.co/papers/2505.18657', 'abstract': 'MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose a systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: 1. Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. 3. Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides a fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systems-advancing progress toward Artificial General Intelligence.', 'score': 2, 'issue_id': 3998, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '530ba33aeb74da52', 'authors': ['Xu Zheng', 'Chenfei Liao', 'Yuqian Fu', 'Kaiyu Lei', 'Yuanhuiyi Lyu', 'Lutao Jiang', 'Bin Ren', 'Jialei Chen', 'Jiawen Wang', 'Chengxin Li', 'Linfeng Zhang', 'Danda Pani Paudel', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Nicu Sebe', 'Dacheng Tao', 'Luc Van Gool', 'Xuming Hu'], 'affiliations': ['CSE, HKUST', 'China University of Mining & Technology, Beijing', 'College of Computing & Data Science, Nanyang Technological University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nagoya University', 'SPIC Energy Science and Technology Research Institute', 'Shanghai Jiao Tong University', 'Tongji University', 'University of Pisa, IT', 'University of Trento, IT', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18657.jpg', 'data': {'categories': ['#interpretability', '#training', '#agi', '#multimodal', '#architecture'], 'emoji': 'üî¨', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–¥–∞—é—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞–¥ –≤–∏–∑—É–∞–ª—å–Ω–æ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã —ç—Ç–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏: —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ MLLM. –°—Ç–∞—Ç—å—è –ø—Ä–∏–∑—ã–≤–∞–µ—Ç –∫ –º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–º —É—Å–∏–ª–∏—è–º –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –æ–±–æ–±—â–∞–µ–º—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Balancing Modalities: Overcoming Bias in MLLMs', 'desc': 'This paper discusses the issue of modality bias in Multimodal Large Language Models (MLLMs), where these models tend to favor language inputs over visual ones. It highlights how this bias affects the integration of different modalities, leading to imbalanced learning and performance across tasks. The authors diagnose the current state of modality bias, propose a research roadmap to address it, and identify key factors contributing to this bias, such as data characteristics and training objectives. They emphasize the need for balanced training strategies and architectures to improve multimodal integration and advance towards more generalizable AI systems.'}, 'zh': {'title': 'ÂÖãÊúçÊ®°ÊÄÅÂÅèËßÅÔºåÂÆûÁé∞Â§öÊ®°ÊÄÅÂπ≥Ë°°Êï¥Âêà', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Êï¥ÂêàÊñáÊú¨ÂíåÂõæÂÉèÁ≠â‰∏çÂêåÊ®°ÊÄÅÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÆÉ‰ª¨Â≠òÂú®Ê®°ÊÄÅÂÅèËßÅÔºåÈÄöÂ∏∏Êõ¥‰æùËµñËØ≠Ë®ÄËÄåÂøΩËßÜËßÜËßâËæìÂÖ•„ÄÇËøôÁßçÂÅèËßÅÂΩ±Âìç‰∫ÜÂ§öÊ®°ÊÄÅÁöÑÂπ≥Ë°°Êï¥ÂêàÔºåÂØºËá¥Â≠¶‰π†Âä®ÊÄÅÁöÑ‰∏çÂπ≥Ë°°„ÄÇÊú¨ÊñáËØäÊñ≠‰∫ÜÊ®°ÊÄÅÂÅèËßÅÁöÑÁé∞Áä∂ÔºåÊèêÂá∫‰∫ÜÁ≥ªÁªüÁöÑÁ†îÁ©∂Ë∑ØÁ∫øÂõæÔºåÂπ∂ËØÜÂà´‰∫ÜÂΩ±ÂìçÊ®°ÊÄÅÂÅèËßÅÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇÊàë‰ª¨Âª∫ËÆÆÊú™Êù•ÁöÑÁ†îÁ©∂Â∫îÂÖ≥Ê≥®Âπ≥Ë°°ÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÊ®°ÂûãÊû∂ÊûÑÔºå‰ª•‰øÉËøõÂ§öÊ®°ÊÄÅÁöÑÊúâÊïàÊï¥Âêà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18134', 'title': 'VideoGameBench: Can Vision-Language Models complete popular video games?', 'url': 'https://huggingface.co/papers/2505.18134', 'abstract': "VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.", 'score': 2, 'issue_id': 4002, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '8dbd2cb973419464', 'authors': ['Alex L. Zhang', 'Thomas L. Griffiths', 'Karthik R. Narasimhan', 'Ofir Press'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.18134.jpg', 'data': {'categories': ['#games', '#cv', '#multimodal', '#benchmark', '#video'], 'emoji': 'üéÆ', 'ru': {'title': '–í–∏–¥–µ–æ–∏–≥—Ä—ã –∫–∞–∫ –≤—ã–∑–æ–≤ –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': 'VideoGameBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–∏–¥–µ–æ–∏–≥—Ä–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 10 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏–≥—Ä 1990-—Ö –≥–æ–¥–æ–≤ –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–ª–µ–π –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –¥–∞–∂–µ –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –∏–≥—Ä. –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–∏–∑–≤–∞–Ω —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.'}, 'en': {'title': 'Evaluating Human-Like Skills in AI through Video Games', 'desc': "VideoGameBench is a new benchmark designed to assess the capabilities of vision-language models (VLMs) in real-time video game interactions using only visual inputs and high-level objectives. It highlights the gap in VLMs' performance on tasks that require human-like skills such as perception and spatial navigation, which are essential for mastering video games. The benchmark includes 10 classic video games from the 1990s, challenging models to complete them without additional game-specific information. Results show that even advanced models struggle significantly, indicating the need for further research to enhance VLMs' abilities in these areas."}, 'zh': {'title': 'ËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ∏∏ÊàèËÉΩÂäõ', 'desc': 'VideoGameBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®ÂÆûÊó∂ËßÜÈ¢ëÊ∏∏Êàè‰∫íÂä®‰∏≠ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®‰ªÖÊúâÁöÑËßÜËßâËæìÂÖ•ÂíåÈ´òÂ±ÇÊ¨°ÁõÆÊ†áÔºåÂº∫Ë∞É‰∫Ü‰∫∫Á±ªÊäÄËÉΩÁöÑÊåëÊàò„ÄÇÂ∞ΩÁÆ° VLM Âú®ÁºñÁ†ÅÂíåÊï∞Â≠¶Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÊÑüÁü•„ÄÅÁ©∫Èó¥ÂØºËà™ÂíåËÆ∞ÂøÜÁÆ°ÁêÜÁ≠â‰∫∫Á±ªËá™ÁÑ∂‰ªªÂä°‰∏äÁöÑËÉΩÂäõ‰ªçÁÑ∂Áº∫‰πèÁ†îÁ©∂„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂâçÊ≤øÁöÑ VLM Âú®Ê∏∏ÊàèÁöÑÂºÄÂßãÈò∂ÊÆµÂ∞±ÈÅáÂà∞‰∫ÜÂõ∞ÈöæÔºåÊé®ÁêÜÂª∂ËøüÊòØÂÖ∂Âú®ÂÆûÊó∂ÁéØÂ¢É‰∏≠ÁöÑ‰∏ªË¶ÅÈôêÂà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17908', 'title': 'ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback', 'url': 'https://huggingface.co/papers/2505.17908', 'abstract': 'ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves a promising path for the development of open-source general-purpose generative AI systems. Project page: https://github.com/LitaoGuo/ComfyMind', 'score': 2, 'issue_id': 3995, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '002e547984f03ef8', 'authors': ['Litao Guo', 'Xinli Xu', 'Luozhou Wang', 'Jiantao Lin', 'Jinsong Zhou', 'Zixin Zhang', 'Bolan Su', 'Ying-Cong Chen'], 'affiliations': ['HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2505.17908.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#training', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'ComfyMind: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'ComfyMind - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ ComfyUI. –û–Ω–∞ –≤–≤–æ–¥–∏—Ç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –†–∞–±–æ—á–µ–≥–æ –ü—Ä–æ—Ü–µ—Å—Å–∞ (SWI) –¥–ª—è –∞–±—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –≥—Ä–∞—Ñ–æ–≤ —É–∑–ª–æ–≤ –≤ –≤—ã–∑—ã–≤–∞–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏. –°–∏—Å—Ç–µ–º–∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –î–µ—Ä–µ–≤–∞ –ü–æ–∏—Å–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–∫ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. ComfyMind –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Generative Workflows with ComfyMind', 'desc': 'ComfyMind is a collaborative AI system that enhances generative workflows by introducing a Semantic Workflow Interface and a Search Tree Planning mechanism. The Semantic Workflow Interface simplifies complex tasks by allowing users to describe workflows in natural language, which reduces errors and improves usability. The Search Tree Planning mechanism organizes the generation process into a hierarchical structure, enabling adaptive corrections and localized feedback during execution. Overall, ComfyMind demonstrates superior performance in generation, editing, and reasoning tasks compared to existing open-source systems, making it a significant advancement in general-purpose generative AI.'}, 'zh': {'title': 'ComfyMindÔºöÊèêÂçáÁîüÊàêÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂçè‰ΩúAIÁ≥ªÁªü', 'desc': 'ComfyMindÊòØ‰∏Ä‰∏™Âü∫‰∫éComfyUIÁöÑÂçè‰ΩúAIÁ≥ªÁªüÔºåÊó®Âú®Â¢ûÂº∫ÁîüÊàêÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÂÆÉÂºïÂÖ•‰∫ÜËØ≠‰πâÂ∑•‰ΩúÊµÅÊé•Âè£ÂíåÊêúÁ¥¢Ê†ëËßÑÂàíÊú∫Âà∂Ôºå‰ΩøÂæóÁîüÊàê„ÄÅÁºñËæëÂíåÊé®ÁêÜ‰ªªÂä°ÁöÑÊÄßËÉΩË∂ÖË∂äÁé∞ÊúâÁöÑÂºÄÊ∫êÁ≥ªÁªü„ÄÇÈÄöËøáÂ∞Ü‰ΩéÁ∫ßËäÇÁÇπÂõæÊäΩË±°‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÁöÑÂèØË∞ÉÁî®ÂäüËÉΩÊ®°ÂùóÔºåComfyMindÂáèÂ∞ë‰∫ÜÁªìÊûÑÈîôËØØÂπ∂ÊèêÈ´ò‰∫ÜÈ´òÂ±ÇÊ¨°ÁöÑÁªÑÂêàËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊêúÁ¥¢Ê†ëËßÑÂàíÊú∫Âà∂ÂÖÅËÆ∏Âú®ÊØè‰∏™Èò∂ÊÆµËøõË°åËá™ÈÄÇÂ∫î‰øÆÊ≠£Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂ§çÊùÇÁîüÊàêÂ∑•‰ΩúÊµÅÁ®ãÁöÑÁ®≥ÂÆöÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16673', 'title': 'R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO', 'url': 'https://huggingface.co/papers/2505.16673', 'abstract': 'Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.', 'score': 2, 'issue_id': 3994, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'e90d190149bd97ed', 'authors': ['Huanjin Yao', 'Qixiang Yin', 'Jingyi Zhang', 'Min Yang', 'Yibo Wang', 'Wenhao Wu', 'Fei Su', 'Li Shen', 'Minghui Qiu', 'Dacheng Tao', 'Jiaxing Huang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'ByteDance', 'Nanyang Technological University', 'The University of Sydney', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16673.jpg', 'data': {'categories': ['#rl', '#training', '#multimodal', '#benchmark', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π MLLM —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª—è–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'Share-GRPO - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM). –ú–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤, –ø–æ–∑–≤–æ–ª—è—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. Share-GRPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã–≥–æ–¥. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —à–µ—Å—Ç–∏ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞.'}, 'en': {'title': 'Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning', 'desc': 'This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks.'}, 'zh': {'title': 'Share-GRPOÔºöÊèêÂçáÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïShare-GRPOÔºåÊó®Âú®Â¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÊâ©Â±ïÈóÆÈ¢òÁ©∫Èó¥ÂíåÂÖ±‰∫´Â§öÊ†∑ÁöÑÊé®ÁêÜËΩ®ËøπÔºåShare-GRPOÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÁ®ÄÁñèÂ•ñÂä±Âíå‰ºòÂäøÊ∂àÂ§±ÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÈÄöËøáÊï∞ÊçÆËΩ¨Êç¢ÊäÄÊúØÊâ©Â±ïÁªôÂÆöÈóÆÈ¢òÁöÑÁ©∫Èó¥ÔºåÁÑ∂ÂêéÈºìÂä±MLLMÂú®Êâ©Â±ïÁöÑÈóÆÈ¢òÁ©∫Èó¥‰∏≠Êé¢Á¥¢Â§öÊ†∑ÁöÑÊé®ÁêÜËΩ®ËøπÔºåÂπ∂Âú®Âº∫ÂåñÂ≠¶‰π†ËøáÁ®ã‰∏≠ÂÖ±‰∫´Ëøô‰∫õËΩ®Ëøπ„ÄÇÊ≠§Â§ñÔºåShare-GRPOÂú®‰ºòÂäøËÆ°ÁÆó‰∏≠ÂÖ±‰∫´Â•ñÂä±‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÁõ∏ÂØπ‰ºòÂäøÁöÑ‰º∞ËÆ°ÂáÜÁ°ÆÊÄßÔºåÂ¢ûÂº∫‰∫ÜÁ≠ñÁï•ËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21499', 'title': 'AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery', 'url': 'https://huggingface.co/papers/2505.21499', 'abstract': "AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.", 'score': 1, 'issue_id': 3997, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'd5affb6081e51a29', 'authors': ['Haowei Wang', 'Junjie Wang', 'Xiaojun Jia', 'Rupeng Zhang', 'Mingyang Li', 'Zhe Liu', 'Yang Liu', 'Qing Wang'], 'affiliations': ['Institute of Software, Chinese Academy of Sciences, Beijing, China', 'Nanyang Technological University, Singapore', 'State Key Laboratory of Intelligent Game, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21499.jpg', 'data': {'categories': ['#cv', '#agents', '#security'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–†–µ–∫–ª–∞–º–∞ –∫–∞–∫ –æ—Ä—É–∂–∏–µ: –Ω–æ–≤–∞—è —É–≥—Ä–æ–∑–∞ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–µ—Ç–∏', 'desc': 'AdInject - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ–∫–ª–∞–º—ã –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞, –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞ –∏–ª–∏ –∑–Ω–∞–Ω–∏—è –æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. AdInject –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—é—â—É—é –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö 100% —É—Å–ø–µ—Ö–∞, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ—Ä—å–µ–∑–Ω—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'AdInject: Harnessing Ads for Real-World Attacks on Web Agents', 'desc': "AdInject is a new method that shows how internet advertising can be used to attack vision-language model-based web agents. This method highlights the security weaknesses of these agents when they operate in uncontrolled online environments. Unlike previous attacks that required detailed knowledge of the agent's workings, AdInject works in a black-box setting, making it more applicable to real-world scenarios. The research demonstrates that by cleverly designing malicious ads, attackers can significantly mislead web agents, achieving high success rates in their attacks."}, 'zh': {'title': 'Âà©Áî®ÂπøÂëäÊ≥®ÂÖ•ÊîªÂáªÁΩëÁªú‰ª£ÁêÜÁöÑÂÆâÂÖ®ÊºèÊ¥û', 'desc': 'AdInjectÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈªëÁÆ±ÊîªÂáªÊñπÊ≥ïÔºåÂà©Áî®‰∫íËÅîÁΩëÂπøÂëäÂêëÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁΩëÁªú‰ª£ÁêÜÊ≥®ÂÖ•ÊÅ∂ÊÑèÂÜÖÂÆπÔºåÊòæÁ§∫Âá∫ÁΩëÁªú‰ª£ÁêÜÂÆâÂÖ®ÊÄßÁöÑÈáçË¶ÅÊºèÊ¥û„ÄÇËØ•ÊñπÊ≥ïÂú®Êõ¥Áé∞ÂÆûÁöÑÂ®ÅËÉÅÊ®°Âûã‰∏ãËøêË°åÔºå‰∏çÈúÄË¶ÅÂØπÁî®Êà∑ÊÑèÂõæÁöÑÂÖ∑‰Ωì‰∫ÜËß£Ôºå‰∏îÂÅáËÆæ‰ª£ÁêÜ‰∏∫ÈªëÁÆ±„ÄÇAdInjectÈÄöËøáËÆæËÆ°ËØØÂØºÊÄßÁöÑÂπøÂëäÂÜÖÂÆπÔºåËØ±‰Ωø‰ª£ÁêÜÁÇπÂáªÔºåÂπ∂‰ΩøÁî®Âü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπøÂëäÂÜÖÂÆπ‰ºòÂåñÊäÄÊúØÔºå‰ΩøÂπøÂëäÂÜÖÂÆπ‰∏éÁõÆÊ†áÁΩëÁ´ôÁöÑ‰∏ä‰∏ãÊñáÊõ¥Áõ∏ÂÖ≥Ôºå‰ªéËÄåÊèêÈ´òÊîªÂáªÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAdInjectÂú®Â§ßÂ§öÊï∞Âú∫ÊôØ‰∏≠ÁöÑÊîªÂáªÊàêÂäüÁéáË∂ÖËøá60%ÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊé•Ëøë100%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20286', 'title': 'Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal\n  Predefinition and Maximal Self-Evolution', 'url': 'https://huggingface.co/papers/2505.20286', 'abstract': 'Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce Alita--a generalist agent designed with the principle of "Simplicity is the ultimate sophistication," enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at https://github.com/CharlesQ9/Alita{https://github.com/CharlesQ9/Alita}.', 'score': 1, 'issue_id': 4001, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'cbcb892a82c3101a', 'authors': ['Jiahao Qiu', 'Xuan Qi', 'Tongcheng Zhang', 'Xinzhe Juan', 'Jiacheng Guo', 'Yifu Lu', 'Yimin Wang', 'Zixin Yao', 'Qihan Ren', 'Xun Jiang', 'Xing Zhou', 'Dongrui Liu', 'Ling Yang', 'Yue Wu', 'Kaixuan Huang', 'Shilong Liu', 'Hongru Wang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'IIIS, Tsinghua University', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tianqiao and Chrissy Chen Institute', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2505.20286.jpg', 'data': {'categories': ['#open_source', '#agi', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–ü—Ä–æ—Å—Ç–æ—Ç–∞ - –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –ò–ò-–∞–≥–µ–Ω—Ç—É', 'desc': '–ê–ª–∏—Ç–∞ - —ç—Ç–æ –∞–≥–µ–Ω—Ç —à–∏—Ä–æ–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏–π –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –∏ —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—é. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–¥–∞—á–µ–π, –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–ª–∏—Ç–∞ –æ—Å–Ω–∞—â–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –¥–ª—è –ø—Ä—è–º–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –Ω–∞–º–Ω–æ–≥–æ –ø—Ä–æ—â–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –ê–≥–µ–Ω—Ç –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö GAIA, Mathvista –∏ PathVQA, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –º–Ω–æ–≥–∏–µ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': "Simplicity Fuels Alita's Generalist Intelligence", 'desc': 'Alita is a generalist agent that excels in various tasks by focusing on simplicity and self-evolution. It minimizes the need for predefined tools, allowing it to adapt and generalize better across different domains. By using task-related model context protocols, Alita can autonomously develop and refine its capabilities, enhancing its problem-solving skills. Its performance on benchmarks like GAIA demonstrates its effectiveness, achieving high accuracy rates compared to more complex systems.'}, 'zh': {'title': 'ÁÆÄÁ∫¶ËÆæËÆ°ÔºåÂº∫Â§ßÊô∫ËÉΩÔºÅ', 'desc': 'AlitaÊòØ‰∏ÄÁßç‰ª•ÁÆÄÁ∫¶‰∏∫È©±Âä®ÁöÑÈÄöÁî®Êô∫ËÉΩ‰ΩìÔºåÈÄöËøáÊúÄÂ∞èÁöÑÈ¢ÑÂÆö‰πâÂíåËá™ÊàëËøõÂåñÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òÊÄßËÉΩ„ÄÇ‰∏é‰æùËµñÊâãÂä®È¢ÑÂÆö‰πâÂ∑•ÂÖ∑ÁöÑÁé∞ÊúâÊ°ÜÊû∂‰∏çÂêåÔºåAlita‰ªÖÈÖçÂ§á‰∏Ä‰∏™Áõ¥Êé•Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑÁªÑ‰ª∂Ôºå‰ΩøÂÖ∂ËÆæËÆ°Êõ¥Âä†ÁÆÄÊ¥Å„ÄÇÂÆÉÈÄöËøáÁîüÊàê‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊ®°Âûã‰∏ä‰∏ãÊñáÂçèËÆÆÔºàMCPsÔºâÊù•Â¢ûÂº∫Ëá™ÊàëËøõÂåñËÉΩÂäõÔºå‰ªéËÄåÂÆûÁé∞ÂèØÊâ©Â±ïÁöÑÊô∫ËÉΩÊé®ÁêÜ„ÄÇAlitaÂú®GAIAÂü∫ÂáÜÈ™åËØÅÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü75.15%ÁöÑpass@1Âíå87.27%ÁöÑpass@3ÂáÜÁ°ÆÁéáÔºåË°®Áé∞‰ºò‰∫éËÆ∏Â§öÂ§çÊùÇÁöÑÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19650', 'title': 'Modality Curation: Building Universal Embeddings for Advanced Multimodal\n  Information Retrieval', 'url': 'https://huggingface.co/papers/2505.19650', 'abstract': 'UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE.', 'score': 1, 'issue_id': 3997, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'ef6a1b7516769657', 'authors': ['Fanheng Kong', 'Jingyuan Zhang', 'Yahui Liu', 'Hongzhi Zhang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yu Tian', 'Victoria W.', 'Fuzheng Zhang', 'Guorui Zhou'], 'affiliations': ['Kuaishou Technology', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19650.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#training', '#multimodal'], 'emoji': 'üîç', 'ru': {'title': 'UNITE: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UNITE - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–∞–ª—å–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Modal-Aware Masked Contrastive Learning (MAMCL) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–º—è–≥—á–∏—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. UNITE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.'}, 'en': {'title': 'UNITE: Bridging Modalities for Superior Information Retrieval', 'desc': 'UNITE is a framework designed to improve multimodal information retrieval (MIR) by focusing on data curation and modality-aware training. It addresses the challenges of aligning different types of data, which often have varying characteristics. The framework introduces Modal-Aware Masked Contrastive Learning (MAMCL) to enhance the learning process by reducing conflicts between different modalities. By conducting extensive experiments, UNITE demonstrates significant improvements in retrieval performance across various benchmarks, setting a new standard in the field.'}, 'zh': {'title': 'UNITEÔºöÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'UNITEÊòØ‰∏Ä‰∏™ÈíàÂØπÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÔºàMIRÔºâÊåëÊàòÁöÑÈÄöÁî®Ê°ÜÊû∂Ôºå‰∏ªË¶ÅÈÄöËøáÊï∞ÊçÆÊï¥ÁêÜÂíåÊ®°ÊÄÅÊÑüÁü•ËÆ≠ÁªÉÊù•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇËØ•Á†îÁ©∂È¶ñÊ¨°Á≥ªÁªüÂàÜÊûê‰∫ÜÊ®°ÊÄÅÁâπÂÆöÊï∞ÊçÆÂ±ûÊÄßÂ¶Ç‰ΩïÂΩ±Âìç‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞ÔºåÂπ∂ÊèêÂá∫‰∫ÜÊ®°ÊÄÅÊÑüÁü•Êé©ËîΩÂØπÊØîÂ≠¶‰π†ÔºàMAMCLÔºâÔºå‰ª•ÂáèËΩª‰∏çÂêåÊ®°ÊÄÅÂÆû‰æã‰πãÈó¥ÁöÑÁ´û‰∫âÂÖ≥Á≥ª„ÄÇÈÄöËøáÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåUNITEÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇËØ•Â∑•‰Ωú‰∏∫Êú™Êù•Â§öÊ®°ÊÄÅÁ≥ªÁªüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂü∫Á°ÄËìùÂõæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19377', 'title': 'Absolute Coordinates Make Motion Generation Easy', 'url': 'https://huggingface.co/papers/2505.19377', 'abstract': 'Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.', 'score': 1, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'd9415ccd47a86548', 'authors': ['Zichong Meng', 'Zeyu Han', 'Xiaogang Peng', 'Yiming Xie', 'Huaizu Jiang'], 'affiliations': ['Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19377.jpg', 'data': {'categories': ['#training', '#multimodal', '#games', '#optimization', '#cv'], 'emoji': 'ü§ñ', 'ru': {'title': '–ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å—É—Å—Ç–∞–≤–æ–≤ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏–π, —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç—É –∏ —Ö–æ—Ä–æ—à—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –¥–∞–∂–µ —Å –ø—Ä–æ—Å—Ç–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Transformer. –ü–æ–¥—Ö–æ–¥ —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä—è–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ—Ä—à–∏–Ω –º–µ—à–∞ SMPL-H –≤ –¥–≤–∏–∂–µ–Ω–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Revolutionizing Text-to-Motion with Global Coordinates', 'desc': 'This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications.'}, 'zh': {'title': 'ÂÖ®Â±ÄÁªùÂØπÂùêÊ†áÊèêÂçáÊñáÊú¨Âà∞ËøêÂä®ÁîüÊàêÁöÑÊïàÊûú', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñáÊú¨Âà∞ËøêÂä®ÁîüÊàêÊñπÊ≥ïÔºå‰ΩøÁî®ÂÖ®Â±ÄÁ©∫Èó¥‰∏≠ÁöÑÁªùÂØπÂÖ≥ËäÇÂùêÊ†áÊù•ÊèêÈ´òËøêÂä®ÁöÑÁúüÂÆûÊÑü„ÄÅÊñáÊú¨ÂØπÈΩêÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ‰º†ÁªüÁöÑËøêÂä®Ë°®Á§∫ÊñπÊ≥ï‰æùËµñ‰∫éÁõ∏ÂØπËøêÂä®ÔºåËôΩÁÑ∂ÁÆÄÂåñ‰∫ÜËÆ≠ÁªÉËøáÁ®ãÔºå‰ΩÜÂØπÊâ©Êï£Ê®°ÂûãÁöÑÂ∫îÁî®ÈÄ†Êàê‰∫ÜÈôêÂà∂„ÄÇÈÄöËøáÁ≥ªÁªüÂàÜÊûêÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜËøôÁßçÊñ∞ÁöÑË°®Á§∫ÊñπÊ≥ïÂú®ËøêÂä®ÁúüÂÆûÊÑüÂíåÊñáÊú¨ÂØπÈΩêÊñπÈù¢ÊòæËëóÊèêÂçáÔºå‰∏îËÉΩÂ§üÊîØÊåÅ‰∏ãÊ∏∏‰ªªÂä°„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜËØ•ÊñπÊ≥ïÂú®‰ªéÊñáÊú¨Áõ¥Êé•ÁîüÊàêËøêÂä®ÁöÑSMPL-HÁΩëÊ†ºÈ°∂ÁÇπÊñπÈù¢ÁöÑËâØÂ•ΩÊ≥õÂåñËÉΩÂäõÔºå‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÂíåËøêÂä®Áõ∏ÂÖ≥Â∫îÁî®Â•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19094', 'title': 'SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.19094', 'abstract': 'SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.', 'score': 1, 'issue_id': 4005, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': 'af44c678d01cadc9', 'authors': ['Chuming Shen', 'Wei Wei', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['School of Computer Science & Technology, Huazhong University of Science and Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.19094.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#optimization', '#dataset', '#rl'], 'emoji': 'üîç', 'ru': {'title': 'SATORI: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ VQA —á–µ—Ä–µ–∑ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'desc': 'SATORI - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (VQA), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∞–µ—Ç VQA –Ω–∞ —Ç—Ä–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö —ç—Ç–∞–ø–∞: –æ–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞. SATORI —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞—Å—Å–µ–∏–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 15.7% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –Ω–∞ —Å–µ–º–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö VQA.'}, 'en': {'title': 'SATORI: Enhancing VQA with Structured Stages and Focused Rewards', 'desc': 'The paper introduces SATORI, a method that improves Visual Question Answering (VQA) by breaking it down into three clear stages: global image captioning, region localization, and answer prediction. This structured approach allows for explicit rewards at each stage, which helps the model focus on important areas of the image, thus enhancing accuracy. By addressing the issues of visual focus and policy-gradient variance, SATORI achieves significant performance gains, with improvements of up to 15.7% in accuracy over previous methods. Additionally, the authors provide a new dataset, VQA-Verify, to support the training of their model with annotated captions and bounding boxes.'}, 'zh': {'title': 'SATORIÔºöÊèêÂçáËßÜËßâÈóÆÁ≠îÁöÑÂÖ≥ÈîÆÂå∫ÂüüÂÖ≥Ê≥®‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SATORIÁöÑÊñπÊ≥ïÔºåÂ∞ÜËßÜËßâÈóÆÁ≠îÔºàVQAÔºâ‰ªªÂä°ÂàÜËß£‰∏∫‰∏â‰∏™ÂèØÈ™åËØÅÁöÑÈò∂ÊÆµÔºöÂÖ®Â±ÄÂõæÂÉèÊèèËø∞„ÄÅÂå∫ÂüüÂÆö‰ΩçÂíåÁ≠îÊ°àÈ¢ÑÊµã„ÄÇÊØè‰∏™Èò∂ÊÆµÈÉΩÊèê‰æõÊòéÁ°ÆÁöÑÂ•ñÂä±‰ø°Âè∑Ôºå‰ª•Â¢ûÂº∫ÂØπÂÖ≥ÈîÆÂå∫ÂüüÁöÑÂÖ≥Ê≥®Âπ∂ÂáèÂ∞ëÁ≠ñÁï•Ê¢ØÂ∫¶ÁöÑÊñπÂ∑Æ„ÄÇÈÄöËøáÂºïÂÖ•VQA-VerifyÊï∞ÊçÆÈõÜÔºåÊú¨Êñá‰∏∫ËÆ≠ÁªÉÊèê‰æõ‰∫ÜÂ∏¶ÊúâÁ≠îÊ°àÂØπÈΩêÁöÑÊèèËø∞ÂíåËæπÁïåÊ°ÜÁöÑ12kÊ≥®ÈáäÊï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®‰∏É‰∏™VQAÂü∫ÂáÜÊµãËØï‰∏≠ÔºåSATORIÊñπÊ≥ïÁöÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜÊúÄÂ§ö15.7%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17190', 'title': 'Tropical Attention: Neural Algorithmic Reasoning for Combinatorial\n  Algorithms', 'url': 'https://huggingface.co/papers/2505.17190', 'abstract': 'Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.', 'score': 1, 'issue_id': 3997, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'e7c2b885aa2f16a0', 'authors': ['Baran Hashemi', 'Kurt Pasque', 'Chris Teska', 'Ruriko Yoshida'], 'affiliations': ['Naval Postgraduate School, Monterey, California', 'Origins Data Science Lab, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.17190.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#security', '#math', '#reasoning'], 'emoji': 'üå¥', 'ru': {'title': '–¢—Ä–æ–ø–∏—á–µ—Å–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –æ—Å—Ç—Ä–æ–µ –º–∞—Å—à—Ç–∞–±–Ω–æ-–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –Ω–µ–π—Ä–æ–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á', 'desc': "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–¢—Ä–æ–ø–∏—á–µ—Å–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ', –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ç—Ä–æ–ø–∏—á–µ—Å–∫–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–æ–ø–∏—á–µ—Å–∫–∏–µ —Å—Ö–µ–º—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å —Ç—Ä–æ–ø–∏—á–µ—Å–∫–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å softmax –ø–æ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º."}, 'en': {'title': 'Tropical Attention: Enhancing Algorithmic Reasoning with Sharp Structures', 'desc': 'This paper introduces Tropical attention, a new attention mechanism designed for combinatorial optimization problems that traditionally use dynamic programming. Unlike existing models that use softmax-normalized attention, which can blur important structures in data, Tropical attention operates within the max-plus semiring, preserving the sharp characteristics of value functions. The authors demonstrate that Tropical attention can effectively approximate tropical circuits used in dynamic programming algorithms, leading to improved performance on out-of-distribution tasks. Additionally, they highlight the robustness of Tropical transformers against adversarial attacks, making them a strong alternative to softmax-based models in algorithmic reasoning.'}, 'zh': {'title': 'ÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÔºöÊèêÂçáÁÆóÊ≥ïÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄß‰∏éÊÄßËÉΩ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÔºåÊó®Âú®Ëß£ÂÜ≥ÁªÑÂêà‰ºòÂåñÈóÆÈ¢ò‰∏≠ÁöÑÂä®ÊÄÅËßÑÂàíÁÆóÊ≥ï„ÄÇ‰º†ÁªüÁöÑÁ•ûÁªèÁÆóÊ≥ïÊé®ÁêÜÊ®°Âûã‰ΩøÁî®softmaxÂΩí‰∏ÄÂåñÁöÑÁÇπÁßØÊ≥®ÊÑèÂäõÔºåËøô‰ºöÊ®°Á≥äÊéâÈáçË¶ÅÁöÑÂá†‰ΩïÁªìÊûÑ„ÄÇÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÂú®ÁÉ≠Â∏¶Âá†‰ΩïÁöÑÊúÄÂ§ßÂä†Ê≥ïÂçäÁéØ‰∏≠ÂéüÁîüÊìç‰ΩúÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Ëøë‰ººÂä®ÊÄÅËßÑÂàíÁ±ªÂûãÁöÑÁîµË∑Ø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁÉ≠Â∏¶ÂèòÊç¢Âô®Âú®ÁÆóÊ≥ïÊé®ÁêÜ‰ªªÂä°‰∏≠Âú®ÈïøÂ∫¶Ê≥õÂåñÂíå‰ª∑ÂÄºÊ≥õÂåñÊñπÈù¢ÁöÑË°®Áé∞‰ºò‰∫ésoftmaxÂü∫Á∫øÔºåÂπ∂‰∏îÂú®ÂØπÊäóÊîªÂáª‰∏ã‰øùÊåÅÁ®≥ÂÆö„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16340', 'title': 'Improving Chemical Understanding of LLMs via SMILES Parsing', 'url': 'https://huggingface.co/papers/2505.16340', 'abstract': 'Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.', 'score': 1, 'issue_id': 3993, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '22065ebe729018b8', 'authors': ['Yunhui Jang', 'Jaehyung Kim', 'Sungsoo Ahn'], 'affiliations': ['KAIST', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16340.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#science', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': 'üß™', 'ru': {'title': 'CLEANMOL: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLEANMOL - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). CLEANMOL —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–∑–±–æ—Ä SMILES-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–ª–µ–∫—É–ª –≤ –≤–∏–¥–µ –Ω–∞–±–æ—Ä–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–ª–µ–∫—É–ª. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–æ–±—É—á–∏–ª–∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ LLM –Ω–∞ —ç—Ç–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CLEANMOL —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–ª–µ–∫—É–ª –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö –∏–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Mol-Instructions.'}, 'en': {'title': 'CLEANMOL: Enhancing LLMs for Molecular Understanding', 'desc': 'This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks.'}, 'zh': {'title': 'CLEANMOLÔºöÊèêÂçáÂàÜÂ≠êÁªìÊûÑÁêÜËß£ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂàÜÂ≠êÁßëÂ≠¶ÁöÑÁßëÂ≠¶ÂèëÁé∞‰∏≠Ë¢´Ë∂äÊù•Ë∂äÂ§öÂú∞ËÆ§ÂèØ‰∏∫Âº∫Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇ‰∏∫‰∫Ü‰ΩøËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂáÜÁ°ÆÁêÜËß£ÂàÜÂ≠êÁªìÊûÑÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜCLEANMOLÊ°ÜÊû∂ÔºåÂ∞ÜSMILESËß£ÊûêËΩ¨Âåñ‰∏∫‰∏ÄÁ≥ªÂàóÊ∏ÖÊô∞‰∏îÁ°ÆÂÆöÁöÑ‰ªªÂä°Ôºå‰ª•‰øÉËøõÂõæÁ∫ßÂàÜÂ≠êÁêÜËß£„ÄÇËøô‰∫õ‰ªªÂä°ÂåÖÊã¨Â≠êÂõæÂåπÈÖçÂíåÂÖ®Â±ÄÂõæÂåπÈÖçÔºåÊèê‰æõ‰∏éÂàÜÂ≠êÁªìÊûÑÁâπÊÄßÁõ∏‰∏ÄËá¥ÁöÑÁªìÊûÑÂåñÁõëÁù£„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåCLEANMOL‰∏ç‰ªÖÂ¢ûÂº∫‰∫ÜÁªìÊûÑÁêÜËß£ËÉΩÂäõÔºåËøòÂú®Mol-InstructionsÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21501', 'title': 'Vision Transformers with Self-Distilled Registers', 'url': 'https://huggingface.co/papers/2505.21501', 'abstract': 'Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher\'s inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.', 'score': 0, 'issue_id': 4005, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'e71640ad11c559f5', 'authors': ['Yinjie Chen', 'Zipeng Yan', 'Chong Zhou', 'Bo Dai', 'Andrew F. Luo'], 'affiliations': ['Nanyang Technological University', 'University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21501.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#training'], 'emoji': 'üî¨', 'ru': {'title': '–°–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Vision Transformers –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Post Hoc Registers (PH-Reg) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Vision Transformers (ViT). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ViT —Å –ø–æ–º–æ—â—å—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –Ω–µ —Ç—Ä–µ–±—É—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. PH-Reg –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—á–∏—Ç–µ–ª—å—Å–∫—É—é –∏ —É—á–µ–Ω–∏—á–µ—Å–∫—É—é —Å–µ—Ç–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ ViT, –æ—Å—Ç–∞–≤–ª—è—è —É—á–∏—Ç–µ–ª—è –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º, –∞ —É—á–µ–Ω–∏–∫–∞ –¥–æ–ø–æ–ª–Ω—è—è —Å–ª—É—á–∞–π–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º —É—á–∏—Ç–µ–ª—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—á–∏—â–µ–Ω–Ω—ã–µ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–ª–æ—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ —É—á–µ–Ω–∏–∫–∞.'}, 'en': {'title': 'Enhancing Vision Transformers with Post Hoc Registers', 'desc': "This paper introduces Post Hoc Registers (PH-Reg), a self-distillation technique designed to enhance Vision Transformers (ViTs) by integrating register tokens. These register tokens help to mitigate the issue of artifact tokens that can negatively impact the model's performance in tasks requiring precise localization. The method allows for the incorporation of these tokens into pre-trained ViTs without the need for extensive retraining or additional labeled data. By leveraging a frozen teacher network and optimizing a small subset of the student network's weights, PH-Reg effectively improves segmentation and depth prediction capabilities."}, 'zh': {'title': 'Ëá™Ëí∏È¶èÔºöÊèêÂçáËßÜËßâÂèòÊç¢Âô®ÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Post Hoc RegistersÔºàPH-RegÔºâÁöÑËá™Ëí∏È¶èÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂ∞ÜÊ≥®ÂÜå‰ª§ÁâåÈõÜÊàêÂà∞È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâ‰∏≠ÔºåÂáèÂ∞ë‰º™ÂΩ±‰ª§ÁâåÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÊèêÂçáÂàÜÂâ≤ÂíåÊ∑±Â∫¶È¢ÑÊµãÁöÑÊÄßËÉΩ„ÄÇViTÂú®ËßÜËßâÂ§ÑÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜ‰º™ÂΩ±‰ª§Áâå‰ºöÂπ≤Êâ∞Â±ÄÈÉ®ËØ≠‰πâÔºåÈôç‰ΩéÊ®°ÂûãÂú®ÁªÜÁ≤íÂ∫¶ÂÆö‰ΩçÂíåÁªìÊûÑ‰∏ÄËá¥ÊÄß‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇPH-RegÊñπÊ≥ï‰∏çÈúÄË¶Å‰ªéÂ§¥ÂºÄÂßãÈáçÊñ∞ËÆ≠ÁªÉÊ®°ÂûãÔºåËÄåÊòØÂà©Áî®Áé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉViTÔºåÈÄöËøáÊ∑ªÂä†ÈöèÊú∫ÂàùÂßãÂåñÁöÑÊ≥®ÂÜå‰ª§ÁâåÊù•‰ºòÂåñÂ≠¶ÁîüÁΩëÁªú„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊàë‰ª¨ËÉΩÂ§üÊúâÊïàÂáèÂ∞ë‰º™ÂΩ±‰ª§ÁâåÁöÑÊï∞ÈáèÔºåÊèêÂçáÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨ÂíåÁ∫øÊÄßÊé¢Êµã‰∏ãÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21471', 'title': 'Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2505.21471', 'abstract': 'The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, ExtAgents, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $boldsymbol{inftyBench+}, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls within or exceeds the context window$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications.', 'score': 0, 'issue_id': 4001, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '3daf58c369674282', 'authors': ['Zijun Liu', 'Zhennan Wan', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.21471.jpg', 'data': {'categories': ['#long_context', '#inference', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': 'ExtAgents: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É ExtAgents, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –≠—Ç–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ LLM. ExtAgents –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª–∏–Ω–Ω—ã—Ö –æ–±–∑–æ—Ä–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Scalable Knowledge Integration with ExtAgents Framework', 'desc': 'The ExtAgents framework improves how large language models (LLMs) integrate external knowledge during inference without needing to extend their context window. It addresses key challenges in knowledge synchronization and reasoning that limit the effectiveness of existing methods. By using a multi-agent approach, ExtAgents allows for better scalability and performance in tasks that require significant external information. This method not only enhances results in multi-hop question answering but also maintains efficiency through parallel processing.'}, 'zh': {'title': 'ExtAgentsÔºöÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁü•ËØÜÊï¥ÂêàÁöÑÂèØÊâ©Â±ïÊÄß', 'desc': 'ExtAgentsÊòØ‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÊó∂Êï¥ÂêàÂ§ñÈÉ®Áü•ËØÜÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâÁü•ËØÜÂêåÊ≠•ÂíåÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑ‰∏§‰∏™Ê†∏ÂøÉÁì∂È¢àÔºå‰ªéËÄåÂú®‰∏çÂ¢ûÂä†‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇÈÄöËøáÂú®Â§öË∑≥ÈóÆÁ≠îÊµãËØïÂíåÂÖ∂‰ªñÂÖ¨ÂÖ±ÊµãËØïÈõÜ‰∏äÁöÑÂü∫ÂáÜÊµãËØïÔºåExtAgentsÊòæËëóÊèêÈ´ò‰∫ÜÂú®Áõ∏ÂêåÂ§ñÈÉ®Áü•ËØÜËæìÂÖ•‰∏ãÁöÑË°®Áé∞„ÄÇËØ•ÊñπÊ≥ïËøòÂõ†ÂÖ∂È´òÂπ∂Ë°åÊÄßËÄå‰øùÊåÅ‰∫ÜÈ´òÊïàÁéáÔºåÊú™Êù•Âú®ÂçèË∞ÉLLMÊô∫ËÉΩ‰Ωì‰ª•Â¢ûÂä†Â§ñÈÉ®Áü•ËØÜËæìÂÖ•ÊñπÈù¢ÁöÑÁ†îÁ©∂Â∞ÜÊúâÂä©‰∫éÂÆûÈôÖÂ∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.21097', 'title': 'Thinker: Learning to Think Fast and Slow', 'url': 'https://huggingface.co/papers/2505.21097', 'abstract': 'A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.', 'score': 0, 'issue_id': 4005, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': '90232cf610bff733', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.21097.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç—É–∏—Ü–∏–∏ –∏ –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –∑–∞–¥–∞—á–∏ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–µ–æ—Ä–∏–∏ –¥–≤–æ–π–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –∏ –≤–∫–ª—é—á–∞–µ—Ç —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞: –±—ã—Å—Ç—Ä–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –ø—Ä–æ–≤–µ—Ä–∫—É, –º–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ –æ–±–æ–±—â–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å 24.9% –¥–æ 27.9% –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen2.5-1.5B –∏ —Å 45.9% –¥–æ 49.8% –¥–ª—è DeepSeek-R1-Qwen-1.5B. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç, —á—Ç–æ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –∏ deliberative —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - —ç—Ç–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ, –Ω–æ –¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞ —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–∏–≥—Ä—ã–≤–∞—é—Ç –æ—Ç —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing LLM Accuracy through Structured Reasoning', 'desc': 'This paper presents a novel four-stage modification to question-answering tasks for Large Language Models (LLMs), inspired by Dual Process Theory. The stages include Fast Thinking, Verification, Slow Thinking, and Summarization, which help separate intuitive responses from more deliberate reasoning. By implementing this structured approach, the authors demonstrate significant improvements in accuracy for math and coding tasks, with notable efficiency in token usage. The results indicate that enhancing both intuitive and deliberative reasoning can lead to better performance in LLMs.'}, 'zh': {'title': '‰ºòÂåñÈóÆÁ≠î‰ªªÂä°ÔºåÊèêÂçáÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂèåÈáçËøáÁ®ãÁêÜËÆ∫ÁöÑÂõõÈò∂ÊÆµÈóÆÁ≠î‰ªªÂä°‰øÆÊîπÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶ÂíåÁºñÁ®ãÈ¢ÜÂüüÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÊÄùÁª¥ËøáÁ®ãÂàÜ‰∏∫Âø´ÈÄüÊÄùËÄÉ„ÄÅÈ™åËØÅ„ÄÅÊÖ¢ÈÄüÊÄùËÄÉÂíåÊÄªÁªìÂõõ‰∏™Èò∂ÊÆµÔºå‰ª•ÊîπÂñÑÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ËØÑ‰º∞ÂíåÁ≤æÁÇºÂÖ∂ÂàùÂßãÂõûÁ≠îÔºå‰ªéËÄåÂáèÂ∞ëÂÜó‰ΩôÂíå‰∏çÁ°ÆÂÆöÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéáÔºåËØÅÊòé‰∫ÜÁõ¥ËßâÂíåÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜÊòØ‰∫íË°•ÁöÑÁ≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20793', 'title': 'Rendering-Aware Reinforcement Learning for Vector Graphics Generation', 'url': 'https://huggingface.co/papers/2505.20793', 'abstract': 'RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  \t\t\t\t\tAI-generated summary \t\t\t\t Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.', 'score': 0, 'issue_id': 4004, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 –º–∞—è', 'en': 'May 27', 'zh': '5Êúà27Êó•'}, 'hash': 'bba0f264ad66c8c3', 'authors': ['Juan A. Rodriguez', 'Haotian Zhang', 'Abhay Puri', 'Aarash Feizi', 'Rishav Pramanik', 'Pascal Wichmann', 'Arnab Mondal', 'Mohammad Reza Samsami', 'Rabiul Awal', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar', 'David Vazquez', 'Christopher Pal', 'Marco Pedersoli'], 'affiliations': ['Apple', 'Canada CIFAR AI Chair', 'Columbia University', 'Google Research', 'Independent Scholar', 'McGill University', 'Mila', 'Polytechnique Montr√©al', 'ServiceNow Research', 'Stony Brook University', '√âTS Montr√©al'], 'pdf_title_img': 'assets/pdf/title_img/2505.20793.jpg', 'data': {'categories': ['#cv', '#rag', '#transfer_learning', '#rl', '#games'], 'emoji': 'üé®', 'ru': {'title': 'RLRF: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SVG —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞', 'desc': '–ú–µ—Ç–æ–¥ RLRF –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SVG –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (VLM). –û–Ω —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç—Ä–µ–Ω–¥–µ—Ä–µ–Ω–Ω—ã–µ SVG —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ SVG. RLRF –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º, —Ä–µ—à–∞—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é SVG.'}, 'en': {'title': 'Enhancing SVG Generation with Reinforcement Learning from Rendering Feedback', 'desc': "The paper presents RLRF, a novel reinforcement learning method that improves the generation of Scalable Vector Graphics (SVG) using rendering feedback. By comparing rendered SVG outputs to original images, RLRF provides evaluative feedback that enhances the model's ability to produce accurate and efficient SVGs. This approach leverages the strengths of vision-language models (VLMs) to capture both global semantics and detailed visual patterns, addressing limitations of previous methods that lacked direct rendering feedback. Ultimately, RLRF demonstrates superior performance over traditional supervised fine-tuning, leading to high-quality SVG generation with better structural understanding and generalization capabilities."}, 'zh': {'title': 'Âà©Áî®Ê∏≤ÊüìÂèçÈ¶àÊèêÂçáSVGÁîüÊàêÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï', 'desc': 'RLRFÊòØ‰∏ÄÁßçÂà©Áî®Ê∏≤ÊüìÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁîüÊàêÂèØÁº©ÊîæÁü¢ÈáèÂõæÂΩ¢ÔºàSVGÔºâÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂ∞ÜÁîüÊàêÁöÑSVG‰∏éÂéüÂßãÂõæÂÉèËøõË°åÊØîËæÉÔºåRLRFËÉΩÂ§üÊèê‰æõËßÜËßâ‰øùÁúüÂ∫¶ÂèçÈ¶àÔºå‰ªéËÄåÊåáÂØºÊ®°ÂûãÁîüÊàêÊõ¥ÂáÜÁ°ÆÂíåÈ´òÊïàÁöÑSVG„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁé∞ÊúâVLMÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Êú™ËßÇÂØüÊ∏≤ÊüìÂõæÂÉèÁöÑÈóÆÈ¢òÔºåÊòæËëóÊèêÂçá‰∫ÜSVGÁîüÊàêÁöÑË¥®ÈáèÂíåÁªìÊûÑÁêÜËß£ËÉΩÂäõ„ÄÇRLRFÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÔºåÂ±ïÁé∞Âá∫Êõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20162', 'title': 'Capability-Based Scaling Laws for LLM Red-Teaming', 'url': 'https://huggingface.co/papers/2505.20162', 'abstract': "Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.", 'score': 0, 'issue_id': 4003, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'f1f248fb099b6803', 'authors': ['Alexander Panfilov', 'Paul Kassianik', 'Maksym Andriushchenko', 'Jonas Geiping'], 'affiliations': ['Cisco Systems Inc.', 'ELLIS Institute T√ºbingen', 'EPFL', 'Foundation AI', 'Max Planck Institute for Intelligent Systems', 'T√ºbingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2505.20162.jpg', 'data': {'categories': ['#rlhf', '#benchmark', '#security', '#alignment'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ü—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö - –∫–ª—é—á –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Ä–µ–∑–∫–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫—É—é—â–µ–≥–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –±–æ–ª–µ–µ —á–µ–º 500 –ø–∞—Ä–∞–º–∏ –∞—Ç–∞–∫—É—é—â–∏–π-—Ü–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –∞—Ç–∞–∫–∏ —Ç–∏–ø–∞ jailbreak, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –í—ã—è–≤–ª–µ–Ω—ã —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏: –±–æ–ª–µ–µ —Å–ø–æ—Å–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ —è–≤–ª—è—é—Ç—Å—è –ª—É—á—à–∏–º–∏ –∞—Ç–∞–∫—É—é—â–∏–º–∏, —É—Å–ø–µ—Ö –∞—Ç–∞–∫–∏ —Ä–µ–∑–∫–æ –ø–∞–¥–∞–µ—Ç –ø—Ä–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–µ —Ü–µ–ª–∏, –∏ —É—Å–ø–µ—Ö –∞—Ç–∞–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –Ω–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ MMLU-Pro. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ—Ü–µ–Ω–∫–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤ –¥–ª—è –±—É–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.'}, 'en': {'title': 'Bridging the Capability Gap: Rethinking Red-Teaming for Advanced AI', 'desc': "This paper explores the effectiveness of red-teaming, which is a method used to identify vulnerabilities in large language models (LLMs). It finds that as LLMs become more capable, traditional red-teaming strategies may fail, especially when the target model is stronger than the attacker. The study evaluates over 500 pairs of attackers and targets, revealing that attack success rates drop significantly when the target model's capabilities exceed those of the attacker. The authors propose a scaling law to predict attack success based on the capability gap, emphasizing the need for new strategies to ensure the safe deployment of advanced AI models."}, 'zh': {'title': 'ËÉΩÂäõÂ∑ÆË∑ùÂÜ≥ÂÆöÊîªÂáªÊàêÂäüÁéá', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Á∫¢ÈòüÊµãËØï‰∏≠ÁöÑÂ∫îÁî®ÔºåÂº∫Ë∞É‰∫ÜÊîªÂáªËÄÖ‰∏éÁõÆÊ†áÊ®°ÂûãËÉΩÂäõÂ∑ÆË∑ùÁöÑÈáçË¶ÅÊÄß„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩìÁõÆÊ†áÊ®°ÂûãÁöÑËÉΩÂäõË∂ÖËøáÊîªÂáªËÄÖÊó∂ÔºåÊîªÂáªÊàêÂäüÁéá‰ºöÊòæËëó‰∏ãÈôç„ÄÇÈÄöËøáÂàÜÊûê500Â§ö‰∏™ÊîªÂáªËÄÖ‰∏éÁõÆÊ†áÂØπÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁõëÊµãÂíåËØÑ‰º∞Á≠ñÁï•Ôºå‰ª•Â∫îÂØπÊú™Êù•ÁöÑÈ£éÈô©„ÄÇÁªìÊûúË°®ÊòéÔºåËÉΩÂäõÊõ¥Âº∫ÁöÑÊ®°ÂûãÂú®ÊîªÂáª‰∏≠Ë°®Áé∞Êõ¥Â•ΩÔºåËÄåÂõ∫ÂÆöËÉΩÂäõÁöÑÊîªÂáªËÄÖÔºàÂ¶Ç‰∫∫Á±ªÔºâÂèØËÉΩÂú®Èù¢ÂØπÊú™Êù•Ê®°ÂûãÊó∂ÂèòÂæóÊó†Êïà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20052', 'title': 'Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion\n  Enhances Protein Representations', 'url': 'https://huggingface.co/papers/2505.20052', 'abstract': 'A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  \t\t\t\t\tAI-generated summary \t\t\t\t Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions.', 'score': 0, 'issue_id': 4000, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '74f830b28865c630', 'authors': ['Hazem Alsamkary', 'Mohamed Elshaffei', 'Mohamed Elkerdawy', 'Ahmed Elnaggar'], 'affiliations': ['Proteinea Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.20052.jpg', 'data': {'categories': ['#training', '#optimization', '#healthcare', '#dataset', '#science'], 'emoji': 'üß¨', 'ru': {'title': '–ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –±–µ–ª–∫–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–ª–∫–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (PLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å Ankh3 –±—ã–ª–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–µ–ª–∫–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª–∏–ª PLM –∏–∑—É—á–∏—Ç—å –±–æ–ª–µ–µ –±–æ–≥–∞—Ç—ã–µ –∏ –æ–±–æ–±—â–∞–µ–º—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–µ–ª–∫–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ç–æ—Ä–∏—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —Ñ–ª—É–æ—Ä–µ—Å—Ü–µ–Ω—Ü–∏–∏, —Ñ–∏—Ç–Ω–µ—Å–∞ GB1 –∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤.'}, 'en': {'title': 'Unlocking Protein Insights with Multi-Task Learning', 'desc': 'This paper presents a multi-task pre-training strategy for protein language models (PLMs) to enhance their performance on various protein prediction tasks. The proposed model, Ankh3, is optimized on two objectives: masked language modeling with varying masking probabilities and protein sequence completion, both using only protein sequences. By leveraging multiple tasks during pre-training, the model learns richer and more generalizable representations of protein sequences. The results show significant improvements in downstream tasks, indicating that this approach provides a deeper understanding of protein properties, leading to better prediction accuracy.'}, 'zh': {'title': 'Â§ö‰ªªÂä°È¢ÑËÆ≠ÁªÉÔºåÊèêÂçáËõãÁôΩË¥®È¢ÑÊµãËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§ö‰ªªÂä°È¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁî®‰∫éÊèêÈ´òËõãÁôΩË¥®ËØ≠Ë®ÄÊ®°ÂûãÔºàPLMsÔºâÂú®‰∏ãÊ∏∏ËõãÁôΩË¥®È¢ÑÊµã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂêåÊó∂‰ºòÂåñÂ§ö‰∏™ÁõÆÊ†áÔºåÊ®°ÂûãËÉΩÂ§ü‰ªéËõãÁôΩË¥®Â∫èÂàó‰∏≠Â≠¶‰π†Êõ¥‰∏∞ÂØåÁöÑË°®Á§∫„ÄÇÊàë‰ª¨ÂºÄÂèëÁöÑAnkh3Ê®°ÂûãÁªìÂêà‰∫ÜÂ§öÁßçÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÂíåËõãÁôΩË¥®Â∫èÂàóË°•ÂÖ®‰ªªÂä°ÔºåÂ±ïÁ§∫‰∫ÜPLMsÂú®‰ªÖ‰æùËµñËõãÁôΩË¥®Â∫èÂàóÊó∂ÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰∫åÁ∫ßÁªìÊûÑÈ¢ÑÊµã„ÄÅËçßÂÖâ„ÄÅGB1ÈÄÇÂ∫îÊÄßÂíåÊé•Ëß¶È¢ÑÊµãÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20036', 'title': 'Beyond Simple Concatenation: Fairly Assessing PLM Architectures for\n  Multi-Chain Protein-Protein Interactions Prediction', 'url': 'https://huggingface.co/papers/2505.20036', 'abstract': 'The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatenation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.', 'score': 0, 'issue_id': 4000, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '778086e609851ecb', 'authors': ['Hazem Alsamkary', 'Mohamed Elshaffei', 'Mohamed Soudy', 'Sara Ossman', 'Abdallah Amr', 'Nehal Adel Abdelsalam', 'Mohamed Elkerdawy', 'Ahmed Elnaggar'], 'affiliations': ['Proteinea Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.20036.jpg', 'data': {'categories': ['#training', '#optimization', '#leakage', '#architecture', '#dataset', '#science'], 'emoji': 'üß¨', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞—Ñ—Ñ–∏–Ω–Ω–æ—Å—Ç–∏ –±–µ–ª–æ–∫-–±–µ–ª–∫–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PPB-Affinity –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–ª–∫–æ–≤ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –∞—Ñ—Ñ–∏–Ω–Ω–æ—Å—Ç–∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è –±–µ–ª–æ–∫-–±–µ–ª–æ–∫. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º –ø—É–ª–∏–Ω–≥–æ–º –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –ø—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–µ—Ç–æ–¥—ã –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç 8207 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –±–µ–ª–æ–∫-–±–µ–ª–æ–∫ —Å –ø–æ—Ä–æ–≥–æ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π ‚â§30% –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–µ–¥—É—â–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–ª–∫–æ–≤, –≤–∫–ª—é—á–∞—è ProtT5, ESM2, Ankh, Ankh2 –∏ ESM3.'}, 'en': {'title': 'Unlocking Protein Interactions: Advanced Architectures for Better Predictions', 'desc': 'This study presents the PPB-Affinity dataset, which contains 8,207 unique entries of protein-protein interactions, refined to eliminate inconsistencies and duplicates. The authors evaluate four different architectural designs for adapting protein language models (PLMs) to predict binding affinity, focusing on hierarchical pooling and pooled attention addition methods. Their experiments show that these advanced architectures significantly outperform traditional concatenation methods, achieving up to a 12% improvement in Spearman correlation. This work emphasizes the importance of using sophisticated model designs to enhance the predictive power of PLMs in understanding protein interactions.'}, 'zh': {'title': 'ÊèêÂçáËõãÁôΩË¥®Áõ∏‰∫í‰ΩúÁî®È¢ÑÊµãÁöÑÊû∂ÊûÑËÆæËÆ°', 'desc': 'Êú¨Á†îÁ©∂‰ªãÁªç‰∫Ü‰∏Ä‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑPPB-AffinityÊï∞ÊçÆÈõÜÔºåÂπ∂ËØÑ‰º∞‰∫ÜÂõõÁßçÊû∂ÊûÑËÆæËÆ°Ôºå‰ª•ÈÄÇÂ∫îËõãÁôΩË¥®ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµãËõãÁôΩË¥®-ËõãÁôΩË¥®Áõ∏‰∫í‰ΩúÁî®ÁªìÂêà‰∫≤ÂíåÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ±ÇÊ¨°Ê±†ÂåñÂíåÊ±†ÂåñÊ≥®ÊÑèÂäõÂä†Ê≥ïÊû∂ÊûÑÁöÑË°®Áé∞‰ºò‰∫éÁÆÄÂçïÁöÑËøûÊé•ÊñπÊ≥ï„ÄÇÈÄöËøáÂºïÂÖ•‰∏•Ê†ºÁöÑÊï∞ÊçÆÈõÜÊ†áÂáÜÔºåÁ°Æ‰øù‰∫ÜËÆ≠ÁªÉ„ÄÅÈ™åËØÅÂíåÊµãËØïÈõÜÁöÑÊúâÊïàÂàÜÂâ≤ÔºåÂáèÂ∞ë‰∫ÜÊï∞ÊçÆÊ≥ÑÊºèÁöÑÈ£éÈô©„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ§çÊùÇÁöÑÊû∂ÊûÑËÆæËÆ°ÂØπ‰∫éÂÖÖÂàÜÂà©Áî®ËõãÁôΩË¥®ËØ≠Ë®ÄÊ®°ÂûãÂú®PPIÁªìÂêà‰∫≤ÂíåÂäõÈ¢ÑÊµã‰∏≠ÁöÑËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19954', 'title': 'An Explainable Diagnostic Framework for Neurodegenerative Dementias via\n  Reinforcement-Optimized LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.19954', 'abstract': "A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions.", 'score': 0, 'issue_id': 4001, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '3af67df3af862fe0', 'authors': ['Andrew Zamai', 'Nathanael Fijalkow', 'Boris Mansencal', 'Laurent Simon', 'Eloi Navet', 'Pierrick Coupe'], 'affiliations': ['Univ. Bordeaux, CNRS, Bordeaux INP, LaBRI, UMR 5800, F-33400 Talence, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.19954.jpg', 'data': {'categories': ['#3d', '#interpretability', '#cv', '#healthcare', '#multimodal', '#rl', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ–∑—Ä–∞—á–Ω–∞—è –Ω–µ–π—Ä–æ–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ò–ò —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π —è—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –Ω–µ–π—Ä–æ–¥–µ–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–µ–º–µ–Ω—Ü–∏—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è 3D –ú–†–¢ –º–æ–∑–≥–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—Ç—á–µ—Ç—ã. –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –æ—Ç—á–µ—Ç–æ–≤. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–π—Ä–æ–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Enhancing Diagnostic Clarity with Causally Grounded Explanations', 'desc': "This paper presents a new framework that improves the clarity of deep learning models used for diagnosing neurodegenerative dementias. It combines modular pipelines that convert brain MRIs into textual reports with reinforcement learning to enhance the reasoning capabilities of Large Language Models (LLMs). By generating explanations that are causally linked to neuroimaging data, the framework provides insights into the model's decision-making process. This approach not only maintains high diagnostic accuracy but also offers transparent rationales that assist clinicians in differentiating between various dementia types."}, 'zh': {'title': 'ÊèêÂçáÁ•ûÁªèÈÄÄË°åÊÄßÁó¥ÂëÜËØäÊñ≠ÈÄèÊòéÂ∫¶ÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÁªìÂêàÊ®°ÂùóÂåñÁÆ°ÈÅìÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÊó®Âú®ÊèêÈ´òÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂú®Á•ûÁªèÈÄÄË°åÊÄßÁó¥ÂëÜËØäÊñ≠‰∏≠ÁöÑÈÄèÊòéÂ∫¶„ÄÇÊàë‰ª¨È¶ñÂÖàÂ∞Ü3D T1Âä†ÊùÉËÑëMRIËΩ¨Êç¢‰∏∫ÊñáÊú¨ÊîæÂ∞ÑÂ≠¶Êä•ÂëäÔºåÁÑ∂ÂêéÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ∏ÆÂä©‰∏¥Â∫äÂåªÁîüËøõË°åÂâçÈ¢ùÂè∂Áó¥ÂëÜ‰∫öÂûã„ÄÅÈòøÂ∞îËå®Êµ∑ÈªòÁóÖÂíåÊ≠£Â∏∏Ë°∞ËÄÅÁöÑÈâ¥Âà´ËØäÊñ≠„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊàë‰ª¨ÈºìÂä±ËØ≠Ë®ÄÊ®°ÂûãËøõË°åËØäÊñ≠Êé®ÁêÜÔºå‰ªéËÄåÁîüÊàêÂü∫‰∫éÁ•ûÁªèÂΩ±ÂÉèÂ≠¶ÂèëÁé∞ÁöÑÁªìÊûÑÂåñËØäÊñ≠ÁêÜÁî±„ÄÇ‰∏é‰º†ÁªüÁöÑÂêéÈ™åÂèØËß£ÈáäÊÄßÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ÁîüÊàêÂõ†ÊûúËß£ÈáäÔºåÊó¢‰øùÊåÅ‰∫ÜÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑÈ¢ÑÊµãÊÄßËÉΩÔºåÂèàÊèê‰æõ‰∫ÜÊîØÊåÅËØäÊñ≠ÁªìËÆ∫ÁöÑÂêàÁêÜÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19235', 'title': 'CoreMatching: A Co-adaptive Sparse Inference Framework with Token and\n  Neuron Pruning for Comprehensive Acceleration of Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.19235', 'abstract': 'A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.', 'score': 0, 'issue_id': 4005, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '0d7373d1050c8170', 'authors': ['Qinsi Wang', 'Hancheng Ye', 'Ming-Yu Chung', 'Yudong Liu', 'Yueqian Lin', 'Martin Kuo', 'Mingyuan Ma', 'Jianyi Zhang', 'Yiran Chen'], 'affiliations': ['Department of Electrical and Computer Engineering, Duke University, North Carolina, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.19235.jpg', 'data': {'categories': ['#architecture', '#inference', '#cv', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'CoreMatching: —Å–∏–Ω–µ—Ä–≥–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CoreMatching –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é —Ç–æ–∫–µ–Ω–æ–≤ –∏ –Ω–µ–π—Ä–æ–Ω–æ–≤, –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—è –∏—Ö –≤–∑–∞–∏–º–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–µ—Å—è—Ç–∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç—Ä–µ—Ö –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ù–∞ NVIDIA Titan Xp –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ 5-–∫—Ä–∞—Ç–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ FLOPS –∏ 10-–∫—Ä–∞—Ç–Ω–æ–µ –æ–±—â–µ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ.'}, 'en': {'title': 'Unlocking Efficiency: Synergizing Token and Neuron Sparsity in VLMs', 'desc': 'This paper introduces a new framework called CoreMatching that improves the efficiency of vision-language models (VLMs) by combining two types of sparsity: token sparsity and neuron sparsity. Token sparsity helps reduce the number of tokens used, while neuron sparsity minimizes the computations needed for high-dimensional data. The authors explore the interaction between key neurons and tokens, revealing that they influence each other, which leads to better performance. By leveraging this synergy, CoreMatching significantly enhances inference efficiency, achieving remarkable speedups and reductions in computational load across various tasks and devices.'}, 'zh': {'title': 'Ê†∏ÂøÉÂåπÈÖçÔºöÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéáÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ†∏ÂøÉÂåπÈÖçÊ°ÜÊû∂ÔºåÈÄöËøáÂà©Áî®Ê†áËÆ∞ÂíåÁ•ûÁªèÂÖÉÁ®ÄÁñèÊÄß‰πãÈó¥ÁöÑÂçèÂêå‰ΩúÁî®ÔºåÊèêÈ´ò‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéá„ÄÇ‰º†ÁªüÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂú®Êé®ÁêÜÊó∂Ê∂àËÄóÂ§ßÈáèÊó∂Èó¥ÂíåÂÜÖÂ≠ò„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂÖ≥ÈîÆÁöÑÁ•ûÁªèÂÖÉÂíåÊ†áËÆ∞Âú®Êé®ÁêÜËøáÁ®ã‰∏≠Áõ∏‰∫íÂΩ±ÂìçÔºåÂ¢ûÂº∫‰∫ÜÂΩºÊ≠§ÁöÑÊïàÊûú„ÄÇÂü∫‰∫éËøô‰∏ÄÂèëÁé∞ÔºåCoreMatchingÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÁªìÂêàËøô‰∏§ÁßçÁ®ÄÁñèÊÄßÔºå‰ªéËÄåÂú®Â§ö‰∏™‰ªªÂä°ÂíåËÆæÂ§á‰∏äË∂ÖË∂äÁé∞ÊúâÁöÑÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17855', 'title': 'Explaining Sources of Uncertainty in Automated Fact-Checking', 'url': 'https://huggingface.co/papers/2505.17855', 'abstract': 'CLUE generates natural language explanations for a language model\'s uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding sources of a model\'s uncertainty regarding its predictions is crucial for effective human-AI collaboration. Prior work proposes using numerical uncertainty or hedges ("I\'m not sure, but ..."), which do not explain uncertainty that arises from conflicting evidence, leaving users unable to resolve disagreements or rely on the output. We introduce CLUE (Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the first framework to generate natural language explanations of model uncertainty by (i) identifying relationships between spans of text that expose claim-evidence or inter-evidence conflicts and agreements that drive the model\'s predictive uncertainty in an unsupervised way, and (ii) generating explanations via prompting and attention steering that verbalize these critical interactions. Across three language models and two fact-checking datasets, we show that CLUE produces explanations that are more faithful to the model\'s uncertainty and more consistent with fact-checking decisions than prompting for uncertainty explanations without span-interaction guidance. Human evaluators judge our explanations to be more helpful, more informative, less redundant, and more logically consistent with the input than this baseline. CLUE requires no fine-tuning or architectural changes, making it plug-and-play for any white-box language model. By explicitly linking uncertainty to evidence conflicts, it offers practical support for fact-checking and generalises readily to other tasks that require reasoning over complex information.', 'score': 0, 'issue_id': 4001, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '331b47ce3c56f5bf', 'authors': ['Jingyi Sun', 'Greta Warren', 'Irina Shklovski', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17855.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#training', '#reasoning', '#data'], 'emoji': 'üîç', 'ru': {'title': 'CLUE: –ü—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'CLUE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –û–Ω –≤—ã—è–≤–ª—è–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ —Ç–µ–∫—Å—Ç–∞, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö. CLUE —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏–µ–º, –≤–µ—Ä–±–∞–ª–∏–∑—É—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏ —Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è CLUE –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'CLUE: Clear Explanations for Model Uncertainty', 'desc': "CLUE is a framework that generates natural language explanations for a language model's uncertainty by analyzing conflicts and agreements in text spans. It identifies how different pieces of evidence relate to each other, revealing the reasons behind the model's uncertainty in its predictions. This approach enhances the clarity of explanations, making them more useful for tasks like fact-checking. CLUE operates without needing any modifications to the model, allowing it to be easily integrated into existing systems."}, 'zh': {'title': 'CLUEÔºöÊè≠Á§∫ËØ≠Ë®ÄÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄßÁöÑÊô∫ËÉΩËß£ÈáäÂ∑•ÂÖ∑', 'desc': 'CLUEÊòØ‰∏Ä‰∏™ÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄËß£ÈáäÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Êè≠Á§∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÂÆÉÈÄöËøáËØÜÂà´ÊñáÊú¨ÁâáÊÆµ‰πãÈó¥ÁöÑÂÜ≤Á™ÅÂíå‰∏ÄËá¥ÊÄßÔºåÂ∏ÆÂä©Áî®Êà∑ÁêÜËß£Ê®°ÂûãÁöÑÈ¢ÑÊµã‰∏çÁ°ÆÂÆöÊÄß„ÄÇ‰∏é‰ª•ÂæÄÁöÑÊï∞ÂÄº‰∏çÁ°ÆÂÆöÊÄßÊñπÊ≥ï‰∏çÂêåÔºåCLUEËÉΩÂ§üÊèê‰æõÊõ¥Ê∏ÖÊô∞ÁöÑËß£ÈáäÔºåÁâπÂà´ÊòØÂú®‰∫ãÂÆûÊ†∏Êü•Á≠â‰ªªÂä°‰∏≠„ÄÇËØ•Ê°ÜÊû∂Êó†ÈúÄÂæÆË∞ÉÊàñÊû∂ÊûÑÊõ¥ÊîπÔºåÈÄÇÁî®‰∫é‰ªª‰ΩïÁôΩÁõíËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÊúâÊïàÊîØÊåÅÂ§çÊùÇ‰ø°ÊÅØÁöÑÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17639', 'title': 'PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and\n  Retrieval', 'url': 'https://huggingface.co/papers/2505.17639', 'abstract': 'Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\% expert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning (87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and 81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.', 'score': 0, 'issue_id': 4004, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '42ac16b897bccbaa', 'authors': ['Zehua Pei', 'Ying Zhang', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17639.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#open_source', '#training'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PreMoe - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE) –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏. PreMoe –≤–∫–ª—é—á–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—É—é –æ–±—Ä–µ–∑–∫—É —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (PEP) –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ–¥ –∑–∞–¥–∞—á—É (TAER). PEP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É TCESS –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –∞ TAER –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –∏ —Ö—Ä–∞–Ω–∏—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PreMoe –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Efficiently Scaling MoE Models with PreMoe', 'desc': 'This paper presents PreMoe, a framework designed to efficiently deploy large Mixture-of-Experts (MoE) models in environments with limited memory. It introduces two key components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER), which work together to optimize expert selection based on task-specific needs. PEP uses a new metric to determine the importance of experts for particular tasks, allowing for a significant reduction in the number of experts needed. TAER enhances inference speed by pre-computing expert patterns, ensuring that only the most relevant experts are loaded for each user query, thus minimizing memory usage while maintaining high accuracy.'}, 'zh': {'title': 'È´òÊïàÈÉ®ÁΩ≤Â§ßËßÑÊ®°MoEÊ®°ÂûãÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÂèØ‰ª•Âú®‰∏çÊòæËëóÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâ©Â±ïÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂèÇÊï∞Êï∞Èáè„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂûãMoEÊ®°ÂûãÁöÑÂÜÖÂ≠òÈúÄÊ±Ç‰ΩøÂÖ∂Âú®ÂêÑÁßçËÆ°ÁÆóÁéØÂ¢É‰∏≠ÁöÑÈÉ®ÁΩ≤ÂèóÂà∞ÈôêÂà∂„ÄÇÊú¨ÊñáÈ¶ñÂÖàÂ±ïÁ§∫‰∫ÜMoEÂ±Ç‰∏≠‰∏ìÂÆ∂ÊøÄÊ¥ªÊ®°ÂºèÁöÑ‰ªªÂä°ÁâπÂÆö‰∏ì‰∏öÂåñ„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜPreMoeÊ°ÜÊû∂ÔºåÈÄöËøáÊ¶ÇÁéá‰∏ìÂÆ∂‰øÆÂâ™ÔºàPEPÔºâÂíå‰ªªÂä°Ëá™ÈÄÇÂ∫î‰∏ìÂÆ∂Ê£ÄÁ¥¢ÔºàTAERÔºâÊù•ÂÆûÁé∞Â§ßËßÑÊ®°MoEÊ®°ÂûãÂú®ÂÜÖÂ≠òÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÈ´òÊïàÈÉ®ÁΩ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15561', 'title': 'Do RAG Systems Suffer From Positional Bias?', 'url': 'https://huggingface.co/papers/2505.15561', 'abstract': "Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.", 'score': 0, 'issue_id': 3996, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'ca5f3f3552697daa', 'authors': ['Florin Cuconasu', 'Simone Filice', 'Guy Horowitz', 'Yoelle Maarek', 'Fabrizio Silvestri'], 'affiliations': ['Sapienza University of Rome', 'Technology Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15561.jpg', 'data': {'categories': ['#interpretability', '#rag', '#benchmark', '#hallucinations'], 'emoji': 'üîç', 'ru': {'title': '–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ RAG: –Ω–µ —Ç–∞–∫ —Å—Ç—Ä–∞—à–Ω–æ, –∫–∞–∫ –∫–∞–∂–µ—Ç—Å—è', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ Retrieval Augmented Generation. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–∞—Å—Ç–æ –ø–æ–º–µ—â–∞—é—Ç –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–µ –æ—Ç—Ä—ã–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±–æ–ª–µ–µ 60% –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω —Å–∏–ª—å–Ω–æ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–π –æ—Ç—Ä—ã–≤–æ–∫ —Å—Ä–µ–¥–∏ —Ç–æ–ø-10 –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Å—Å–∞–∂–µ–π. –í –∏—Ç–æ–≥–µ, —Å–ª–æ–∂–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è –æ—Ç—Ä—ã–≤–∫–æ–≤ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –ø–µ—Ä–µ–¥ —Å–ª—É—á–∞–π–Ω—ã–º –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ–º.'}, 'en': {'title': 'Navigating Positional Bias in Retrieval Augmented Generation', 'desc': "This paper explores how Retrieval Augmented Generation (RAG) can improve the accuracy of large language models (LLMs) by incorporating relevant passages from an external corpus. It specifically examines the effect of positional bias, which is how LLMs prioritize information based on its location in the input prompt. The study finds that while retrieval systems aim to provide relevant information, they often inadvertently highlight distracting passages, affecting the LLM's performance. Ultimately, the research shows that attempts to optimize passage arrangement based on positional bias do not yield better results than random ordering, indicating a need for improved retrieval strategies."}, 'zh': {'title': '‰ΩçÁΩÆÂÅèÂ∑ÆÂΩ±ÂìçÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁöÑÊïàÊûú', 'desc': 'Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÂ¶Ç‰ΩïÈÄöËøáÂ∞ÜÂ§ñÈÉ®ËØ≠ÊñôÂ∫ì‰∏≠ÁöÑÊÆµËêΩÊ∑ªÂä†Âà∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊèêÁ§∫‰∏≠Êù•ÊèêÈ´òÂÖ∂ÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫Ü‰ΩçÁΩÆÂÅèÂ∑ÆÔºåÂç≥LLMÊ†πÊçÆ‰ø°ÊÅØÂú®ÊèêÁ§∫‰∏≠ÁöÑ‰ΩçÁΩÆ‰∏çÂêåËÄåÂä†ÊùÉÁöÑÂÄæÂêëÔºåÂ¶Ç‰ΩïÂΩ±ÂìçÂÖ∂Âà©Áî®Áõ∏ÂÖ≥ÊÆµËêΩÁöÑËÉΩÂäõ‰ª•ÂèäÂØπÂπ≤Êâ∞ÊÆµËêΩÁöÑÊïèÊÑüÊÄß„ÄÇÈÄöËøáÂú®‰∏â‰∏™Âü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Â∞ΩÁÆ°Ê£ÄÁ¥¢ÁÆ°ÈÅìÊó®Âú®Ëé∑ÂèñÁõ∏ÂÖ≥ÊÆµËêΩÔºå‰ΩÜÂç¥Á≥ªÁªüÊÄßÂú∞Â∞ÜÈ´òÂ∫¶Âπ≤Êâ∞ÁöÑÊÆµËêΩÊéíÂú®ÂâçÂàóÔºåË∂ÖËøá60%ÁöÑÊü•ËØ¢Âú®Ââç10‰∏™Ê£ÄÁ¥¢ÊÆµËêΩ‰∏≠ÂåÖÂê´Ëá≥Â∞ë‰∏Ä‰∏™È´òÂ∫¶Âπ≤Êâ∞ÁöÑÊÆµËêΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËØïÂõæÊ†πÊçÆLLM‰ΩçÁΩÆÂÅèÂ•ΩÈáçÊñ∞ÊéíÂàóÊÆµËêΩÁöÑÂ§çÊùÇÁ≠ñÁï•Âπ∂Ê≤°ÊúâÊØîÈöèÊú∫Êâì‰π±Ë°®Áé∞Êõ¥Â•Ω„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03318', 'title': 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.03318', 'abstract': "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.", 'score': 63, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': 'f0871f80f0b8fdd9', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.03318.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#multimodal', '#training', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω UnifiedReward-Think - –ø–µ—Ä–≤–∞—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT). –ú–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Vision Models with Long-Chain Reasoning', 'desc': 'This paper introduces UnifiedReward-Think, a novel multimodal reward model that enhances the alignment of vision models with human preferences through long-chain reasoning. By integrating explicit chains of thought (CoT) into the reward reasoning process, the model improves the accuracy and reliability of reward signals. The approach involves a two-step training process: first, distilling reasoning from a small dataset, and then fine-tuning with large-scale multimodal preference data. Experimental results show that this method significantly outperforms existing models in various vision tasks, demonstrating its effectiveness in complex reasoning scenarios.'}, 'zh': {'title': 'ÈïøÈìæÊÄùÁª¥ÊèêÂçáÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÁöÑÂèØÈù†ÊÄß', 'desc': 'ÊúÄËøëÂú®Â§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºàRMsÔºâÊñπÈù¢ÁöÑËøõÂ±ïÊòæÁ§∫Âá∫Â∞ÜËßÜËßâÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑRMsÈÄöÂ∏∏Âè™ËÉΩÊèê‰æõÁõ¥Êé•ÂìçÂ∫îÊàñËøõË°åÊµÖÂ±ÇÊé®ÁêÜÔºåÂØºËá¥Â•ñÂä±‰ø°Âè∑‰∏çÂáÜÁ°Æ„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÂ∞ÜÊòéÁ°ÆÁöÑÈïøÈìæÊÄùÁª¥ÔºàCoTÔºâÁ∫≥ÂÖ•Â•ñÂä±Êé®ÁêÜËøáÁ®ãÂèØ‰ª•ÊòæËëóÂ¢ûÂº∫ÂÖ∂ÂèØÈù†ÊÄßÂíåÁ®≥ÂÅ•ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜUnifiedReward-ThinkÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂü∫‰∫éCoTÁöÑÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§üËøõË°åÂ§öÁª¥Â∫¶„ÄÅÈÄêÊ≠•ÁöÑÈïøÈìæÊé®ÁêÜÔºåÈÄÇÁî®‰∫éËßÜËßâÁêÜËß£ÂíåÁîüÊàêÂ•ñÂä±‰ªªÂä°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03335', 'title': 'Absolute Zero: Reinforced Self-play Reasoning with Zero Data', 'url': 'https://huggingface.co/papers/2505.03335', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.', 'score': 59, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': 'b53e736d1884218d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rlhf', '#training', '#rl', '#math', '#optimization', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Absolute Zero. –í —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã –º–æ–¥–µ–ª—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è, –Ω–µ –ø–æ–ª–∞–≥–∞—è—Å—å –Ω–∞ –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–∏—Å—Ç–µ–º—É Absolute Zero Reasoner (AZR), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–≤–∏–≤–∞–µ—Ç —Å–≤–æ—é —É—á–µ–±–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –∫–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –∏ –æ—Ç–≤–µ—Ç–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, AZR –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Self-Learning AI: No Data, No Problem!', 'desc': 'This paper introduces a new approach in reinforcement learning called Absolute Zero, which allows a model to learn and improve its reasoning skills without needing external data or human supervision. The proposed Absolute Zero Reasoner (AZR) autonomously generates tasks that enhance its learning and validates its own reasoning through a code executor. This self-sufficient learning method leads to state-of-the-art performance in coding and mathematical reasoning tasks, surpassing models that rely on large datasets of human-created examples. The findings suggest that AZR can adapt to different model sizes and types, showcasing its versatility and potential for future AI development.'}, 'zh': {'title': 'ÁªùÂØπÈõ∂ÔºöËá™ÊàëËøõÂåñÁöÑÊé®ÁêÜÊ®°Âûã', 'desc': 'Âº∫ÂåñÂ≠¶‰π†‰∏éÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÁªìÊûúÂØºÂêëÁöÑÂ•ñÂä±‰∏≠Â≠¶‰π†„ÄÇÊúÄËøëÁöÑRLVRÁ†îÁ©∂Âú®Èõ∂ËÆæÁΩÆ‰∏ãËøêË°åÔºåÈÅøÂÖç‰∫ÜÂØπÊé®ÁêÜËøáÁ®ãÁöÑÁõëÁù£Ôºå‰ΩÜ‰ªç‰æùËµñ‰∫é‰∫∫Â∑•Á≠ñÂàíÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°àÈõÜÂêàËøõË°åËÆ≠ÁªÉ„ÄÇÁî±‰∫éÈ´òË¥®Èáè‰∫∫Á±ªÁîüÊàêÁ§∫‰æãÁöÑÁ®ÄÁº∫ÊÄßÔºå‰æùËµñ‰∫∫Á±ªÁõëÁù£ÁöÑÈïøÊúüÂèØÊâ©Â±ïÊÄßÂèóÂà∞Ë¥®Áñë„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑRLVRËåÉÂºèÔºåÁß∞‰∏∫ÁªùÂØπÈõ∂ÔºàAbsolute ZeroÔºâÔºåËØ•ËåÉÂºè‰∏ãÁöÑÊ®°ÂûãËÉΩÂ§üËá™ÊàëÊèêÂá∫‰ªªÂä°‰ª•ÊúÄÂ§ßÂåñÂ≠¶‰π†ËøõÂ±ïÔºåÂπ∂ÈÄöËøáËß£ÂÜ≥Ëøô‰∫õ‰ªªÂä°Êù•ÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºåËÄåÊó†ÈúÄ‰æùËµñ‰ªª‰ΩïÂ§ñÈÉ®Êï∞ÊçÆ„ÄÇ'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2505.03005', 'title': 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale', 'url': 'https://huggingface.co/papers/2505.03005', 'abstract': "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper", 'score': 22, 'issue_id': 3625, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '0fe1c0b1575b6708', 'authors': ['Daniel Goldstein', 'Eric Alcaide', 'Janna Lu', 'Eugene Cheah'], 'affiliations': ['Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'EleutherAI', 'George Mason University', 'Recursal AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.03005.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#inference', '#benchmark', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å —Å–æ—Ñ—Ç–º–∞–∫—Å-–≤–Ω–∏–º–∞–Ω–∏–µ–º –≤ –º–æ–¥–µ–ª–∏ –¥–µ–∫–æ–¥–µ—Ä–æ–≤ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–≤–µ –Ω–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ RWKV –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ Qwen2.5 —Ä–∞–∑–º–µ—Ä–æ–º 7B, 32B –∏ 72B. –ü—Ä–æ—Ü–µ—Å—Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ 350-700 –º–ª–Ω —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω–µ–µ 0,005% –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Transforming Transformers: Efficient Linear Attention Models with RADLADS', 'desc': 'The paper introduces Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a method for transforming softmax attention transformers into efficient linear attention models. This conversion process is highly efficient, requiring only a small fraction of the original training data, specifically 350-700M tokens. Despite the reduced training cost, the resulting 72B linear attention model maintains performance levels comparable to its transformer counterparts. The authors also present new RWKV-variant architectures and make their models available on HuggingFace, demonstrating state-of-the-art results on standard benchmarks for linear attention models.'}, 'zh': {'title': 'Âø´ÈÄüËΩ¨Êç¢ÔºåÁ∫øÊÄßÊ≥®ÊÑèÂäõÁöÑÊú™Êù•', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂø´ÈÄüÊ≥®ÊÑèÂäõËí∏È¶èÂà∞Á∫øÊÄßÊ≥®ÊÑèËß£Á†ÅÂô®ÁöÑÂçèËÆÆÔºàRADLADSÔºâÔºåÂèØ‰ª•ËøÖÈÄüÂ∞ÜËΩØÊúÄÂ§ßÊ≥®ÊÑèÂäõÂèòÊç¢Âô®ËΩ¨Êç¢‰∏∫Á∫øÊÄßÊ≥®ÊÑèËß£Á†ÅÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑËΩ¨Êç¢ËøáÁ®ãÂè™ÈúÄ350-700M‰∏™Ê†áËÆ∞ÔºåËøú‰Ωé‰∫éÂéüÂßãÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉÊâÄÈúÄÁöÑ0.005%ÁöÑÊ†áËÆ∞Êï∞Èáè„ÄÇËΩ¨Êç¢‰∏∫Êàë‰ª¨ÁöÑ72BÁ∫øÊÄßÊ≥®ÊÑèÊ®°ÂûãÁöÑÊàêÊú¨‰∏çÂà∞2000ÁæéÂÖÉÔºå‰ΩÜÊé®ÁêÜË¥®Èáè‰ªçÊé•ËøëÂéüÂßãÂèòÊç¢Âô®„ÄÇÊàë‰ª¨Âú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÂêåÁ±ªÊúÄ‰Ω≥ÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩÔºåÂπ∂Â∞ÜÊâÄÊúâÊ®°ÂûãÂèëÂ∏ÉÂú®HuggingFace‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03730', 'title': 'FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios', 'url': 'https://huggingface.co/papers/2505.03730', 'abstract': 'Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/', 'score': 19, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '2329fe6d2462c9c9', 'authors': ['Shiyi Zhang', 'Junhao Zhuang', 'Zhaoyang Zhang', 'Ying Shan', 'Yansong Tang'], 'affiliations': ['Tencent ARC Lab, China', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03730.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#multimodal', '#video'], 'emoji': 'üé≠', 'ru': {'title': '–ì–∏–±–∫–∏–π –ø–µ—Ä–µ–Ω–æ—Å –¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Å—É–±—ä–µ–∫—Ç–∞–º–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FlexiAct - –º–µ—Ç–æ–¥ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–µ–π—Å—Ç–≤–∏–π —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, FlexiAct –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–æ–≤–∫—É, —Ä–∞–∫—É—Ä—Å –∏ —Å–∫–µ–ª–µ—Ç–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–µ–∂–¥—É —Å—É–±—ä–µ–∫—Ç–æ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏ —Ü–µ–ª–µ–≤—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —ç—Ç–æ–π —Ü–µ–ª–∏ –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç RefAdapter - –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä, –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –≥–∏–±–∫–æ—Å—Ç–∏. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è FAE (Frequency-aware Action Extraction) –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'FlexiAct: Flexible Action Transfer for Diverse Video Customization', 'desc': 'The paper presents FlexiAct, a novel approach for customizing action videos by transferring actions from a reference video to a target image, regardless of differences in layout, viewpoint, and skeletal structure. This method addresses the limitations of existing techniques that require strict spatial consistency, allowing for greater adaptability across various subjects and scenarios. FlexiAct utilizes a lightweight image-conditioned adapter called RefAdapter to ensure identity consistency while adapting spatial structures. Additionally, it introduces Frequency-aware Action Extraction (FAE) to enhance action extraction during the denoising process, achieving superior results in maintaining both appearance and structural flexibility.'}, 'zh': {'title': 'ÁÅµÊ¥ªÁöÑÂä®‰ΩúËΩ¨ÁßªÔºåÊâìÁ†¥Á©∫Èó¥ÈôêÂà∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫FlexiActÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊ†πÊçÆËæìÂÖ•ÊéßÂà∂‰ø°Âè∑ÁîüÊàêËßÜÈ¢ëÔºåÂÖÅËÆ∏Âú®‰∏çÂêåÂ∏ÉÂ±Ä„ÄÅËßÜËßíÂíåÈ™®Êû∂ÁªìÊûÑ‰πãÈó¥ËøõË°åÂä®‰ΩúËΩ¨Áßª„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåFlexiActËÉΩÂ§üÂú®‰øùÊåÅË∫´‰ªΩ‰∏ÄËá¥ÊÄßÁöÑÂêåÊó∂ÔºåÈÄÇÂ∫îÁõÆÊ†áÂõæÂÉèÁöÑÁ©∫Èó¥ÁªìÊûÑÂèòÂåñ„ÄÇ‰∏∫ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÔºåÊñáÁ´†ÂºïÂÖ•‰∫ÜRefAdapterÔºå‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÂõæÂÉèÊù°‰ª∂ÈÄÇÈÖçÂô®ÔºåËÉΩÂ§üÂú®Â§ñËßÇ‰∏ÄËá¥ÊÄßÂíåÁªìÊûÑÁÅµÊ¥ªÊÄß‰πãÈó¥ÂèñÂæóËâØÂ•ΩÂπ≥Ë°°„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑFAEÊñπÊ≥ïÂú®ÂéªÂô™ËøáÁ®ã‰∏≠Áõ¥Êé•ÊèêÂèñÂä®‰ΩúÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02922', 'title': 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference', 'url': 'https://huggingface.co/papers/2505.02922', 'abstract': 'The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.', 'score': 18, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'd7e3545dcade10b4', 'authors': ['Yaoqi Chen', 'Jinkai Zhang', 'Baotong Lu', 'Qianxi Zhang', 'Chengruidong Zhang', 'Jingjia Luo', 'Di Liu', 'Huiqiang Jiang', 'Qi Chen', 'Jing Liu', 'Bailu Ding', 'Xiao Yan', 'Jiawei Jiang', 'Chen Chen', 'Mingxing Zhang', 'Yuqing Yang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02922.jpg', 'data': {'categories': ['#long_context', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'RetroInfer: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º', 'desc': 'RetroInfer - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –∫—ç—à –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ —Å–∏—Å—Ç–µ–º—É —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –í –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã –ª–µ–∂–∏—Ç –≤–æ–ª–Ω–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤. RetroInfer —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –≤–æ–ª–Ω–æ–≤–æ–π –±—É—Ñ–µ—Ä –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –∫—ç—à–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–µ–∂–¥—É GPU –∏ CPU. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–æ 4.5 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ø–∞–º—è—Ç–∏ GPU –∏ –¥–æ 10.5 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Accelerating Long-Context Inference with RetroInfer', 'desc': 'This paper introduces RetroInfer, a system designed to improve the efficiency of large language models (LLMs) during inference by addressing GPU memory and bandwidth limitations. It innovatively redefines the key-value (KV) cache as a vector storage system that leverages attention sparsity to speed up processing of long contexts. The core component, the wave index, utilizes advanced techniques for token retrieval, ensuring both efficiency and accuracy. Additionally, the wave buffer optimizes the coordination of KV cache and computation, achieving significant speed improvements while maintaining high model accuracy.'}, 'zh': {'title': 'È´òÊïàÊé®ÁêÜÔºåÁ™ÅÁ†¥‰∏ä‰∏ãÊñáÈôêÂà∂ÔºÅ', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÂ¢ûÂä†ÔºåÊé®ÁêÜÊïàÁéáÈù¢‰∏¥ÊòæËëóÊåëÊàòÔºå‰∏ªË¶ÅÊòØÁî±‰∫éGPUÂÜÖÂ≠òÂíåÂ∏¶ÂÆΩÁöÑÈôêÂà∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜRetroInferÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ≥ªÁªüÔºåÂ∞ÜÂÖ≥ÈîÆÂÄºÔºàKVÔºâÁºìÂ≠òÈáçÊñ∞Ê¶ÇÂøµÂåñ‰∏∫ÂêëÈáèÂ≠òÂÇ®Á≥ªÁªüÔºåÂà©Áî®ÂÜÖÂú®ÁöÑÊ≥®ÊÑèÂäõÁ®ÄÁñèÊÄßÊù•Âä†ÈÄüÈïø‰∏ä‰∏ãÊñáLLMÊé®ÁêÜ„ÄÇÂÖ∂Ê†∏ÂøÉÊòØÊ≥¢Âä®Á¥¢ÂºïÔºàwave indexÔºâÔºå‰∏ÄÁßçÊ≥®ÊÑèÂäõÊÑüÁü•ÂêëÈáèÁ¥¢ÂºïÔºåËÉΩÂ§üÈÄöËøá‰∏âÊñπÊ≥®ÊÑèÂäõËøë‰ºº„ÄÅÁ≤æÂ∫¶ÂèóÈôêÁöÑÊ≥®ÊÑèÂäõ‰º∞ËÆ°ÂíåÂàÜÊÆµËÅöÁ±ªÁ≠âÊäÄÊúØÈ´òÊïàÂáÜÁ°ÆÂú∞Ê£ÄÁ¥¢ÂÖ≥ÈîÆÊ†áËÆ∞„ÄÇ‰∏é‰ª•ÂæÄÂú®Ê†áËÆ∞ÈÄâÊã©ÂíåÁ°¨‰ª∂ÂçèË∞É‰∏äÂ≠òÂú®Âõ∞ÈöæÁöÑÁ®ÄÁñèÊÄßÊñπÊ≥ï‰∏çÂêåÔºåRetroInferÂú®‰∏çÂΩ±ÂìçÊ®°ÂûãÂáÜÁ°ÆÊÄßÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02872', 'title': 'Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading', 'url': 'https://huggingface.co/papers/2505.02872', 'abstract': "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.", 'score': 14, 'issue_id': 3629, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': 'aae5b0f63189eb32', 'authors': ['Cfir Avraham Hadar', 'Omer Shubi', 'Yoav Meiri', 'Yevgeni Berzak'], 'affiliations': ['Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.02872.jpg', 'data': {'categories': ['#multimodal', '#science', '#dataset', '#benchmark', '#interpretability', '#agi'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': '–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Ü–µ–ª–µ–π —á—Ç–µ–Ω–∏—è –ø–æ –¥–≤–∏–∂–µ–Ω–∏—è–º –≥–ª–∞–∑ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–π —á—Ç–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤–∏–∂–µ–Ω–∏–π –≥–ª–∞–∑. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ü–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –≥–ª–∞–∑ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –∏ —Å—Ä–∞–≤–Ω–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏—Ö –¥–≤–∏–∂–µ–Ω–∏—è –≥–ª–∞–∑ –∏ —Ç–µ–∫—Å—Ç. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —É—Å–ø–µ—Ö –≤ –æ–±–µ–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å LLM –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ü–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–ª—è—Ö —á—Ç–µ–Ω–∏—è –∏–∑ –¥–≤–∏–∂–µ–Ω–∏–π –≥–ª–∞–∑.'}, 'en': {'title': 'Decoding Reading Goals from Eye Movements with LLMs', 'desc': "This paper explores how eye movements during reading can reveal a reader's specific goals, such as information seeking. The authors introduce new tasks for classifying and reconstructing these goals using large-scale eye tracking data. They develop and evaluate various multimodal large language models (LLMs) that integrate eye movement data with text to improve goal understanding. The results indicate that these models can effectively decode readers' intentions, demonstrating the potential of LLMs in understanding reading behavior."}, 'zh': {'title': '‰ªéÁúºÂä®Êï∞ÊçÆËß£Á†ÅÈòÖËØªÁõÆÊ†áÁöÑÂàõÊñ∞Á†îÁ©∂', 'desc': 'Êú¨Á†îÁ©∂È¶ñÊ¨°Êé¢ËÆ®‰∫ÜÂ¶Ç‰Ωï‰ªéÈòÖËØªÊó∂ÁöÑÁúºÂä®Êï∞ÊçÆËá™Âä®Ëß£Á†ÅÂºÄÊîæÂºèÈòÖËØªÁõÆÊ†á„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÁõÆÊ†áÂàÜÁ±ªÂíåÁõÆÊ†áÈáçÂª∫‰ªªÂä°ÔºåÂπ∂Âª∫Á´ã‰∫ÜËØÑ‰º∞Ê°ÜÊû∂Ôºå‰ΩøÁî®‰∫ÜÂ§ßËßÑÊ®°ÁöÑÁúºÂä®ËøΩË∏™Êï∞ÊçÆ„ÄÇÈÄöËøáÁªìÂêàÁúºÂä®ÂíåÊñáÊú¨‰ø°ÊÅØÔºåÊàë‰ª¨ÂºÄÂèëÂπ∂ÊØîËæÉ‰∫ÜÂ§öÁßçÂà§Âà´ÂºèÂíåÁîüÊàêÂºèÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËøô‰∫õÊ®°ÂûãÂú®ÊèêÂèñËØªËÄÖÁöÑÊñáÊú¨ÁâπÂÆöÁõÆÊ†áÊñπÈù¢Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊàêÂäü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02214', 'title': 'An Empirical Study of Qwen3 Quantization', 'url': 'https://huggingface.co/papers/2505.02214', 'abstract': "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.", 'score': 12, 'issue_id': 3627, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': 'be32ffc34f30d354', 'authors': ['Xingyu Zheng', 'Yuye Li', 'Haoran Chu', 'Yue Feng', 'Xudong Ma', 'Jie Luo', 'Jinyang Guo', 'Haotong Qin', 'Michele Magno', 'Xianglong Liu'], 'affiliations': ['Beihang University', 'ETH Z√ºrich', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02214.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source', '#low_resource'], 'emoji': 'üî¨', 'ru': {'title': '–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ Qwen3: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ—Ü–µ–Ω–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen3, –æ–¥–Ω–æ–π –∏–∑ –≤–µ–¥—É—â–∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ 5 –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –±–∏—Ç–æ–≤–æ–π –≥–ª—É–±–∏–Ω–æ–π –æ—Ç 1 –¥–æ 8 –±–∏—Ç. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Qwen3 —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —É–º–µ—Ä–µ–Ω–Ω–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ç–µ—Ä—è–µ—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏ —Å–≤–µ—Ä—Ö–Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ LLM.'}, 'en': {'title': "Unlocking Efficiency: Evaluating Qwen3's Performance Under Quantization", 'desc': "The Qwen series represents a significant advancement in open-source Large Language Models (LLMs), particularly with the introduction of Qwen3, which excels in natural language understanding tasks. This paper investigates the effects of low-bit quantization on Qwen3's performance, focusing on how different quantization techniques impact its robustness. By evaluating five classic post-training quantization methods across various bit-widths, the study reveals that while Qwen3 performs well at moderate bit-widths, it struggles with linguistic tasks at ultra-low precision. The findings highlight the challenges of compressing LLMs and suggest the need for further research to improve quantization strategies without sacrificing model accuracy."}, 'zh': {'title': 'Êé¢Á¥¢Qwen3ÁöÑÈáèÂåñÊåëÊàò‰∏éÊú∫ÈÅá', 'desc': 'QwenÁ≥ªÂàóÊòØ‰∏Ä‰∏™È¢ÜÂÖàÁöÑÂºÄÊ∫êÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºåÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊúÄËøëÂèëÂ∏ÉÁöÑQwen3Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂê∏Âºï‰∫ÜÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠È´òÊïàÈÉ®ÁΩ≤ÁöÑÂÖ≥Ê≥®„ÄÇÊú¨ÊñáÁ≥ªÁªüËØÑ‰º∞‰∫ÜQwen3Âú®‰∏çÂêåÈáèÂåñËÆæÁΩÆ‰∏ãÁöÑÈ≤ÅÊ£íÊÄßÔºåÊé¢ËÆ®‰∫ÜÂéãÁº©Ëøô‰∏ÄÂÖàËøõÊ®°ÂûãÁöÑÊú∫ÈÅá‰∏éÊåëÊàò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Âú®‰∏≠Á≠â‰ΩçÂÆΩ‰∏ãQwen3ÁöÑÊÄßËÉΩ‰ªçÂÖ∑Á´û‰∫âÂäõÔºå‰ΩÜÂú®Ë∂Ö‰ΩéÁ≤æÂ∫¶‰∏ãËØ≠Ë®Ä‰ªªÂä°ÁöÑË°®Áé∞ÊòæËëó‰∏ãÈôçÔºåÂº∫Ë∞É‰∫ÜLLMÂéãÁº©‰∏≠ÁöÑÊåÅÁª≠ÈöæÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03735', 'title': 'Multi-Agent System for Comprehensive Soccer Understanding', 'url': 'https://huggingface.co/papers/2505.03735', 'abstract': 'Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.', 'score': 9, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': 'f62fbd87d3f6f548', 'authors': ['Jiayuan Rao', 'Zifeng Li', 'Haoning Wu', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['SAI, Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03735.jpg', 'data': {'categories': ['#open_source', '#survey', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#agents'], 'emoji': '‚öΩ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ò–ò-–∞–Ω–∞–ª–∏–∑–µ —Ñ—É—Ç–±–æ–ª–∞: –æ—Ç –∑–Ω–∞–Ω–∏–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ñ—É—Ç–±–æ–ª–∞ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ SoccerWiki - –ø–µ—Ä–≤—É—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –æ —Ñ—É—Ç–±–æ–ª–µ, –∏ SoccerBench - –æ–±—à–∏—Ä–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ—É—Ç–±–æ–ª–∞ –ò–ò-—Å–∏—Å—Ç–µ–º–∞–º–∏. –¢–∞–∫–∂–µ –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ SoccerAgent - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Ñ—É—Ç–±–æ–ª–µ –ø—É—Ç–µ–º —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ—É—Ç–±–æ–ª–∞.'}, 'en': {'title': 'Revolutionizing Soccer Understanding with AI', 'desc': 'This paper presents a new framework for understanding soccer using AI, addressing the limitations of previous research that focused on narrow tasks. The authors introduce SoccerWiki, a large multimodal knowledge base that contains detailed information about various aspects of soccer, enabling better reasoning. They also create SoccerBench, a comprehensive benchmark with thousands of multimodal question-answer pairs to evaluate soccer understanding tasks. Finally, the paper introduces SoccerAgent, a multi-agent system that collaborates to answer complex soccer questions, demonstrating improved performance through extensive evaluations.'}, 'zh': {'title': 'ÂÖ®Èù¢ÊèêÂçáË∂≥ÁêÉÁêÜËß£ÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑË∂≥ÁêÉÁêÜËß£Ê°ÜÊû∂Ôºå‰ª•Â°´Ë°•Áé∞ÊúâÁ†îÁ©∂ÁöÑÁ©∫ÁôΩ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫ÜSoccerWikiÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅË∂≥ÁêÉÁü•ËØÜÂ∫ìÔºåÊï¥Âêà‰∫ÜÂÖ≥‰∫éÁêÉÂëò„ÄÅÁêÉÈòü„ÄÅË£ÅÂà§ÂíåÂú∫È¶ÜÁöÑ‰∏∞ÂØåÈ¢ÜÂüüÁü•ËØÜ„ÄÇÊàë‰ª¨ËøòÊé®Âá∫‰∫ÜSoccerBenchÔºåËøôÊòØÊúÄÂ§ßÁöÑË∂≥ÁêÉÁâπÂÆöÂü∫ÂáÜÔºåÂåÖÂê´Á∫¶10,000‰∏™Ê†áÂáÜÂåñÁöÑÂ§öÊ®°ÊÄÅÂ§öÈÄâÈóÆÁ≠îÂØπÔºåÊ∂µÁõñ13‰∏™‰∏çÂêåÁöÑÁêÜËß£‰ªªÂä°„ÄÇÊúÄÂêéÔºåÊàë‰ª¨‰ªãÁªç‰∫ÜSoccerAgentÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºåÈÄöËøáÂçè‰ΩúÊé®ÁêÜÂàÜËß£Â§çÊùÇÁöÑË∂≥ÁêÉÈóÆÈ¢òÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03739', 'title': 'VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model', 'url': 'https://huggingface.co/papers/2505.03739', 'abstract': 'With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.', 'score': 6, 'issue_id': 3632, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': 'd1a6985b437b7562', 'authors': ['Zuwei Long', 'Yunhang Shen', 'Chaoyou Fu', 'Heting Gao', 'Lijiang Li', 'Peixian Chen', 'Mengdan Zhang', 'Hang Shao', 'Jian Li', 'Jinlong Peng', 'Haoyu Cao', 'Ke Li', 'Rongrong Ji', 'Xing Sun'], 'affiliations': ['Nanjing University', 'Tencent Youtu Lab', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03739.jpg', 'data': {'categories': ['#training', '#multimodal', '#audio', '#inference', '#open_source'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö: –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π', 'desc': 'VITA-Audio - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è —Ä–µ—á–µ–≤–∞—è –º–æ–¥–µ–ª—å, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—É –≤—ã—Å–æ–∫–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ –∞—É–¥–∏–æ—Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–æ–¥—É–ª—å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (MCTP), –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—É–¥–∏–æ—Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —á–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—á–∏. VITA-Audio –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –≤ 3-5 —Ä–∞–∑ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º –¥–ª—è –∑–∞–¥–∞—á ASR, TTS –∏ SQA.'}, 'en': {'title': 'VITA-Audio: Fast, Real-Time Speech Generation for Seamless Interaction', 'desc': 'The paper introduces VITA-Audio, a novel end-to-end speech model designed to enhance real-time human-computer interaction by reducing latency in audio generation. It features a Multiple Cross-modal Token Prediction (MCTP) module that allows for the simultaneous generation of multiple audio tokens, significantly speeding up the inference process. The model employs a four-stage progressive training strategy to maintain high speech quality while achieving faster performance. VITA-Audio stands out as the first multi-modal large language model capable of producing audio output during the initial forward pass, demonstrating a 3-5x speedup compared to existing models while excelling in various speech-related tasks.'}, 'zh': {'title': 'VITA-AudioÔºöÂÆûÊó∂ËØ≠Èü≥ÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'ÈöèÁùÄ‰∫∫Êú∫‰∫§‰∫íÈúÄÊ±ÇÁöÑÂ¢ûÂä†ÔºåÂü∫‰∫éËØ≠Èü≥ÁöÑÁ≥ªÁªüÂèóÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÂÖ≥Ê≥®„ÄÇÁé∞ÊúâÁöÑËØ≠Èü≥Ê®°ÂûãÂú®ÁîüÊàêÁ¨¨‰∏Ä‰∏™Èü≥È¢ëÊ†áËÆ∞Êó∂Â≠òÂú®È´òÂª∂ËøüÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVITA-AudioÔºåËøôÊòØ‰∏ÄÁßçÁ´ØÂà∞Á´ØÁöÑÂ§ßÂûãËØ≠Èü≥Ê®°ÂûãÔºåËÉΩÂ§üÂø´ÈÄüÁîüÊàêÈü≥È¢ëÊñáÊú¨Ê†áËÆ∞„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÈÄöËøáÂºïÂÖ•ËΩªÈáèÁ∫ßÁöÑÂ§öÊ®°ÊÄÅ‰∫§ÂèâÊ†áËÆ∞È¢ÑÊµãÊ®°ÂùóÔºåÊòæËëóÂä†Âø´‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÂπ∂Âú®ÊµÅÂ™í‰ΩìÂú∫ÊôØ‰∏≠ÂáèÂ∞ë‰∫ÜÂª∂Ëøü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03368', 'title': 'Geospatial Mechanistic Interpretability of Large Language Models', 'url': 'https://huggingface.co/papers/2505.03368', 'abstract': 'Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs\' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.', 'score': 5, 'issue_id': 3629, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '1901718a6f967dbe', 'authors': ['Stef De Sabbata', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Leicester, UK', 'University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2505.03368.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#science', '#data', '#architecture', '#interpretability'], 'emoji': 'üåç', 'ru': {'title': '–ó–∞–≥–ª—è–¥—ã–≤–∞—è –≤ –º–æ–∑–≥ –ò–ò: –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—é—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—é', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ LLM, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –û–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –ø—Ä–æ–±–∏–Ω–≥ –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –º–æ–¥–µ–ª—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∏—Ö –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –ø–æ–ª–æ–∂–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Unveiling How LLMs Think Geographically', 'desc': 'This paper explores how Large Language Models (LLMs) understand and process geographical information. It introduces a framework for geospatial mechanistic interpretability, which aims to reverse engineer the internal workings of LLMs using spatial analysis. The authors utilize probing techniques and sparse autoencoders to uncover how LLMs represent geographic concepts, revealing patterns in their internal features. By demonstrating spatial autocorrelation in placenames, the study provides insights into the spatial reasoning capabilities of LLMs and their implications for geography.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂú∞ÁêÜ‰ø°ÊÅØÂ§ÑÁêÜÊú∫Âà∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊñáÊú¨Âíå‰ª£Á†ÅÁöÑÂ§ÑÁêÜ‰∏éÁîüÊàêÊñπÈù¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®Á†îÁ©∂LLMsÂ¶Ç‰ΩïÂ§ÑÁêÜÂú∞ÁêÜ‰ø°ÊÅØÔºåÁâπÂà´ÊòØÂÖ∂ÂÜÖÈÉ®Êú∫Âà∂ÁöÑÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÁ©∫Èó¥ÂàÜÊûêÂíåÊé¢ÊµãÊäÄÊúØÔºåÊè≠Á§∫LLMsÂÜÖÈÉ®ÁªìÊûÑÔºåÂπ∂Âà©Áî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®Â∞ÜÂ§ö‰πâÊÄßÁâπÂæÅÂàÜËß£‰∏∫Êõ¥ÊòìËß£ÈáäÁöÑÂçï‰πâÁâπÂæÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú∞ÂêçÁâπÂæÅ‰∏éÂÖ∂Âú∞ÁêÜ‰ΩçÁΩÆ‰πãÈó¥Â≠òÂú®Á©∫Èó¥Áõ∏ÂÖ≥ÊÄßÔºå‰ªéËÄå‰∏∫ÁêÜËß£LLMsÂ§ÑÁêÜÂú∞ÁêÜ‰ø°ÊÅØÁöÑÊñπÂºèÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßí„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03164', 'title': 'InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships', 'url': 'https://huggingface.co/papers/2505.03164', 'abstract': "Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.", 'score': 4, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '377a2c082764680a', 'authors': ['Ji Won Chung', 'Tongyu Zhou', 'Ivy Chen', 'Kevin Hsu', 'Ryan A. Rossi', 'Alexa Siu', 'Shunan Guo', 'Franck Dernoncourt', 'James Tompkin', 'Jeff Huang'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03164.jpg', 'data': {'categories': ['#multimodal', '#video'], 'emoji': 'üé≠', 'ru': {'title': 'InfoVids: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —á–µ–ª–æ–≤–µ–∫–∞', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é InfoVids - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–æ–π. –û–Ω–∏ –ø—Ä–∏–∑–≤–∞–Ω—ã —Å–æ–∑–¥–∞—Ç—å –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–æ—Ä–æ–º –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç, –∫–∞–∫ –º–∞–∫–µ—Ç, —Ñ–æ—Ä–º–∞ –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–ª–∏—è—é—Ç –Ω–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∑—Ä–∏—Ç–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ InfoVids —Å–Ω–∏–∂–∞—é—Ç —Ä–∞—Å—Å–µ–∏–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è, –ø–µ—Ä–µ–Ω–æ—Å—è—Ç —Ñ–æ–∫—É—Å –Ω–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–æ—Ä–∞ –∏ —Å–æ–∑–¥–∞—é—Ç –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏.'}, 'en': {'title': 'Revolutionizing Data Presentation with InfoVids', 'desc': 'This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualizations in a more cohesive manner. By using infographics-inspired videos, the authors aim to enhance viewer engagement and reduce distractions that typically arise from traditional 2D slides. The study evaluates InfoVids against standard presentation methods across various metrics, revealing that they foster a more interactive and natural experience for viewers. The findings suggest that this innovative approach can transform the dynamics of data presentation, making it more human-centric and effective.'}, 'zh': {'title': 'ÈáçÊñ∞ÂÆö‰πâÊºîÁ§∫ËÄÖ‰∏éÂèØËßÜÂåñÁöÑÂÖ≥Á≥ª', 'desc': '‰º†ÁªüÁöÑÊï∞ÊçÆÂ±ïÁ§∫ÈÄöÂ∏∏Â∞ÜÊºîÁ§∫ËÄÖÂíåÂèØËßÜÂåñÂàÜÂºÄÔºåÂàÜÂà´Âú®3D‰∏ñÁïåÂíå2DÂ±èÂπï‰∏≠ËøõË°åÔºåÂº∫Ë∞É‰ª•ÂèØËßÜÂåñ‰∏∫‰∏≠ÂøÉÁöÑÂèôËø∞„ÄÇ‰∏∫‰∫ÜÂàõÈÄ†Êõ¥‰ª•‰∫∫‰∏∫Êú¨ÁöÑËßÇÁúã‰ΩìÈ™åÔºåÊàë‰ª¨ÈÄöËøáInfoVidsÂª∫Á´ã‰∫ÜÂèØËßÜÂåñ‰∏éÊºîÁ§∫ËÄÖ‰πãÈó¥Êõ¥Âπ≥Á≠âÁöÑÂÖ≥Á≥ª„ÄÇËøô‰∫õÂèó‰ø°ÊÅØÂõæÂêØÂèëÁöÑ‰ø°ÊÅØËßÜÈ¢ëÊó®Âú®ÈáçÊñ∞ÂÆö‰πâÊºîÁ§∫ËÄÖ‰∏éÂèØËßÜÂåñ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåInfoVidsÂáèÂ∞ë‰∫ÜËßÇ‰ºóÁöÑÊ≥®ÊÑèÂäõÂàÜÊï£Ôºå‰ΩøÁÑ¶ÁÇπ‰ªéÂèØËßÜÂåñËΩ¨ÂêëÊºîÁ§∫ËÄÖÔºåÂπ∂‰∏∫ËßÇ‰ºóÊèê‰æõ‰∫ÜÊõ¥‰∫íÂä®„ÄÅËá™ÁÑ∂ÂíåÂºï‰∫∫ÂÖ•ËÉúÁöÑÊï∞ÊçÆË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21650', 'title': 'HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation', 'url': 'https://huggingface.co/papers/2504.21650', 'abstract': "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.", 'score': 4, 'issue_id': 3632, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': 'a04fa605a99969e5', 'authors': ['Haiyang Zhou', 'Wangbo Yu', 'Jiawen Guan', 'Xinhua Cheng', 'Yonghong Tian', 'Li Yuan'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory', 'School of Electronic and Computer Engineering, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21650.jpg', 'data': {'categories': ['#3d', '#video', '#cv', '#dataset', '#diffusion'], 'emoji': 'üåê', 'ru': {'title': '–ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –º–∏—Ä—ã: –æ—Ç –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –≤–∏–¥–µ–æ –∫ 4D-—Å—Ü–µ–Ω–∞–º', 'desc': 'HoloTime - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –≤–∏–¥–µ–æ –∏ 4D-—Å—Ü–µ–Ω –¥–ª—è VR –∏ AR –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –≤–∏–¥–µ–æ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ Panoramic Animator. –ó–∞—Ç–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D-—Å—Ü–µ–Ω—ã –º–µ—Ç–æ–¥–æ–º Panoramic Space-Time Reconstruction. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 360World —Å –ø–∞–Ω–æ—Ä–∞–º–Ω—ã–º–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Transforming VR and AR with HoloTime: Immersive 4D Experiences from Panoramic Videos', 'desc': 'This paper introduces HoloTime, a novel framework that enhances virtual reality (VR) and augmented reality (AR) experiences by generating immersive 4D assets from video diffusion models. It addresses the limitations of current models that focus on static 3D scenes by proposing a method to create panoramic videos from a single image, which are then transformed into 4D representations. The authors present the 360World dataset, a unique collection of panoramic videos that supports advanced 4D scene reconstruction tasks. Their approach, validated through comparative analysis, shows significant improvements in generating high-quality panoramic videos and reconstructing 4D scenes, ultimately leading to more engaging user experiences in immersive environments.'}, 'zh': {'title': 'HoloTimeÔºöÊèêÂçáVRÂíåARÊ≤âÊµ∏‰ΩìÈ™åÁöÑÂÖ®ÊôØËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫HoloTimeÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁîüÊàêÂÖ®ÊôØËßÜÈ¢ëÔºå‰ªéËÄåÊèêÂçáËôöÊãüÁé∞ÂÆûÔºàVRÔºâÂíåÂ¢ûÂº∫Áé∞ÂÆûÔºàARÔºâÊäÄÊúØÁöÑÊ≤âÊµ∏‰ΩìÈ™å„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü360WorldÊï∞ÊçÆÈõÜÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫é4DÂú∫ÊôØÈáçÂª∫‰ªªÂä°ÁöÑÂÖ®ÊôØËßÜÈ¢ëÈõÜÂêà„ÄÇÈÄöËøáPanoramic AnimatorÊ®°ÂûãÔºåÊàë‰ª¨ËÉΩÂ§üÂ∞ÜÂÖ®ÊôØÂõæÂÉèËΩ¨Êç¢‰∏∫È´òË¥®ÈáèÁöÑÂÖ®ÊôØËßÜÈ¢ë„ÄÇÊúÄÂêéÔºåÂà©Áî®Á©∫Èó¥-Êó∂Èó¥Ê∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ïÔºåÊàë‰ª¨Â∞ÜÁîüÊàêÁöÑËßÜÈ¢ëËΩ¨Âåñ‰∏∫4DÁÇπ‰∫ëÔºåÂÆûÁé∞‰∫ÜÊõ¥ÁúüÂÆûÁöÑ4DÂú∫ÊôØÈáçÂª∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21798', 'title': 'SWE-smith: Scaling Data for Software Engineering Agents', 'url': 'https://huggingface.co/papers/2504.21798', 'abstract': 'Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.', 'score': 3, 'issue_id': 3638, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '35185d0e663ab134', 'authors': ['John Yang', 'Kilian Leret', 'Carlos E. Jimenez', 'Alexander Wettig', 'Kabir Khandpur', 'Yanzhe Zhang', 'Binyuan Hui', 'Ofir Press', 'Ludwig Schmidt', 'Diyi Yang'], 'affiliations': ['Alibaba Qwen', 'Indepedent', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21798.jpg', 'data': {'categories': ['#benchmark', '#data', '#synthetic', '#open_source', '#dataset', '#training'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': 'SWE-smith: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏', 'desc': 'SWE-smith - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö. –û–Ω —Å–æ–∑–¥–∞–µ—Ç —Å—Ä–µ–¥—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –ª—é–±–æ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã Python –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Å–æ—Ç–Ω–∏ –∏–ª–∏ —Ç—ã—Å—è—á–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∑–∞–¥–∞—á, –Ω–∞—Ä—É—à–∞—é—â–∏—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã. –° –ø–æ–º–æ—â—å—é SWE-smith –∞–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 50 —Ç—ã—Å—è—á —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∏–∑ 128 —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ GitHub, —á—Ç–æ –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –±–æ–ª—å—à–µ, —á–µ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö. –û–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å SWE-agent-LM-32B –¥–æ—Å—Ç–∏–≥–ª–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è Pass@1 –≤ 40.2% –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ SWE-bench Verified, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ª—É—á—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º.'}, 'en': {'title': 'Scaling Software Engineering Data Generation with SWE-smith', 'desc': 'This paper presents SWE-smith, a new method for generating large-scale training data for language models in software engineering. Traditional datasets are limited in size and require extensive manual effort to curate, which hinders their effectiveness. SWE-smith automates the creation of execution environments and synthesizes numerous task instances from Python codebases, resulting in a dataset of 50,000 instances from 128 GitHub repositories. The authors demonstrate that their model, SWE-agent-LM-32B, achieves a state-of-the-art performance on the SWE-bench Verified benchmark, and they provide all resources to facilitate further research in this area.'}, 'zh': {'title': 'SWE-smithÔºöÂ§ßËßÑÊ®°ÁîüÊàêËΩØ‰ª∂Â∑•Á®ãËÆ≠ÁªÉÊï∞ÊçÆÁöÑËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Â∞ΩÁÆ°ËØ≠Ë®ÄÊ®°ÂûãÂú®ËΩØ‰ª∂Â∑•Á®ãÈ¢ÜÂüüÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÊî∂ÈõÜËÆ≠ÁªÉÊï∞ÊçÆ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçÂ§ßÊåëÊàò„ÄÇÁé∞ÊúâÁöÑÊï∞ÊçÆÈõÜËßÑÊ®°ËæÉÂ∞èÔºåÈÄöÂ∏∏Êù•Ëá™11‰∏™ÊàñÊõ¥Â∞ëÁöÑGitHub‰ªìÂ∫ìÔºåÊúÄÂ§öÂè™ÊúâÂá†ÂçÉ‰∏™ËÆ≠ÁªÉÂÆû‰æã„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSWE-smithÔºåËøôÊòØ‰∏ÄÁßçÁî®‰∫éÂ§ßËßÑÊ®°ÁîüÊàêËΩØ‰ª∂Â∑•Á®ãËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊñ∞ÂûãÁÆ°ÈÅì„ÄÇÈÄöËøáSWE-smithÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´5‰∏á‰∏™ÂÆû‰æãÁöÑÊï∞ÊçÆÈõÜÔºåÊòæËëóË∂ÖËøá‰∫Ü‰ª•ÂæÄÁöÑÂ∑•‰ΩúÔºåÂπ∂‰∏îÂºÄÊ∫ê‰∫ÜÁõ∏ÂÖ≥ËµÑÊ∫êÔºå‰ª•Èôç‰ΩéËá™Âä®ÂåñËΩØ‰ª∂Â∑•Á®ãÁ†îÁ©∂ÁöÑÈó®Êßõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03052', 'title': 'Teaching Models to Understand (but not Generate) High-risk Data', 'url': 'https://huggingface.co/papers/2505.03052', 'abstract': "Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.", 'score': 2, 'issue_id': 3642, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'a988d97eaa0eb984', 'authors': ['Ryan Wang', 'Matthew Finlayson', 'Luca Soldaini', 'Swabha Swayamdipta', 'Robin Jia'], 'affiliations': ['Allen Institute for AI', 'Department of Computer Science, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.03052.jpg', 'data': {'categories': ['#training', '#hallucinations', '#rlhf', '#ethics'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ü–æ–Ω–∏–º–∞—Ç—å, –Ω–æ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å: –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SLUNG. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ø–æ–Ω–∏–º–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—è –µ–≥–æ. SLUNG –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, –∏–∑–±–µ–≥–∞—è —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SLUNG —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏ –æ–ø–∞—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Learn to Understand, Not to Generate Toxicity', 'desc': "This paper presents a new training method called Selective Loss to Understand but Not Generate (SLUNG) for language models. SLUNG allows models to learn from high-risk content, like toxic text, without generating it in their outputs. By selectively applying loss functions, the model is trained to predict safe tokens that follow harmful ones, enhancing its understanding of sensitive topics. The results show that SLUNG improves the model's ability to recognize toxic content while preventing it from producing harmful responses."}, 'zh': {'title': 'ÁêÜËß£È´òÈ£éÈô©ÂÜÖÂÆπÔºåÈÅøÂÖçÁîüÊàêÊúâÂÆ≥ËæìÂá∫', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁß∞‰∏∫ÈÄâÊã©ÊÄßÊçüÂ§±‰ª•ÁêÜËß£‰ΩÜ‰∏çÁîüÊàêÔºàSLUNGÔºâ„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏Ê®°ÂûãÂú®‰∏çÁîüÊàêÈ´òÈ£éÈô©ÂÜÖÂÆπÁöÑÊÉÖÂÜµ‰∏ãÔºåÁêÜËß£Ëøô‰∫õÂÜÖÂÆπ„ÄÇÈÄöËøáÈÄâÊã©ÊÄßÂú∞ÈÅøÂÖçÊøÄÂä±ÁîüÊàêÈ´òÈ£éÈô©Ê†áËÆ∞ÔºåSLUNGÁ°Æ‰øùËøô‰∫õÂÜÖÂÆπ‰ªçÁÑ∂Âú®Ê®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÂÜÖ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSLUNGÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂØπÈ´òÈ£éÈô©Êï∞ÊçÆÁöÑÁêÜËß£ËÉΩÂäõÔºåÂêåÊó∂Ê≤°ÊúâÂ¢ûÂä†ÁîüÊàêÁöÑÊúâÂÆ≥ÂÜÖÂÆπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02311', 'title': 'Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering', 'url': 'https://huggingface.co/papers/2505.02311', 'abstract': 'The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.', 'score': 2, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '2a0b4af232b71fc8', 'authors': ['Jihao Zhao', 'Chunlai Zhou', 'Biao Qin'], 'affiliations': ['School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02311.jpg', 'data': {'categories': ['#hallucinations', '#small_models', '#training', '#optimization', '#reasoning'], 'emoji': 'üîç', 'ru': {'title': '–£–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AttenHScore –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –º–æ–º–µ–Ω—Ç –¥–ª—è –≤—ã–∑–æ–≤–∞ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞—Ä–∞–¥–∏–≥–º–µ. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –º–∞–ª—ã–º –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ AttenHScore –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Enhancing Hallucination Detection in Language Models with AttenHScore', 'desc': 'This paper introduces a new metric called AttenHScore to improve the detection of hallucinations in small language models (LMs) during their generation process. Hallucinations refer to incorrect or nonsensical outputs produced by LMs, and the proposed metric helps identify when these errors occur in real-time. By adjusting the detection threshold dynamically, the method enhances the invocation of larger LMs to provide more accurate responses. Additionally, the paper discusses how uncertainty-aware knowledge reorganization can help small LMs better understand and utilize critical information from text, leading to improved performance without requiring extra training.'}, 'zh': {'title': 'ÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπªËßâÊ£ÄÊµãËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊåáÊ†áAttenHScoreÔºåÁî®‰∫éÂú®Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêËøáÁ®ã‰∏≠Ê£ÄÊµãÂíå‰º†Êí≠ÂπªËßâ„ÄÇÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥Ê£ÄÊµãÈòàÂÄºÔºåÊàë‰ª¨ËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÂÆûÊó∂Ë∞ÉÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ªéËÄåÊèêÈ´òÂπªËßâÊ£ÄÊµãÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÂà©Áî®‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁü•ËØÜÈáçÁªÑÔºåÂ∏ÆÂä©Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÊõ¥Â•ΩÂú∞ÊçïÊçâÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAttenHScoreÂú®Â§ö‰∏™ÈóÆÁ≠îÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÂ§ßÂ§öÊï∞Âü∫Á∫øÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÊü•ËØ¢Êó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.18373', 'title': 'Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant', 'url': 'https://huggingface.co/papers/2504.18373', 'abstract': 'In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.', 'score': 2, 'issue_id': 3629, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 –∞–ø—Ä–µ–ª—è', 'en': 'April 25', 'zh': '4Êúà25Êó•'}, 'hash': '296dd197ef9ea331', 'authors': ['Lei Shen', 'Xiaoyu Shen'], 'affiliations': ['GEB Tech', 'Ningbo Institute of Digital Twin, EIT, Ningbo'], 'pdf_title_img': 'assets/pdf/title_img/2504.18373.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': 'Auto-SLURP: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Auto-SLURP - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤. Auto-SLURP —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–±–æ—Ä SLURP, –¥–æ–±–∞–≤–ª—è—è —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä—ã –∏ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Auto-SLURP –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä—å–µ–∑–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –Ω–∞–¥–µ–∂–Ω—ã–µ –∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –≤—Å–µ –µ—â–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–¥–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.'}, 'en': {'title': 'Auto-SLURP: Benchmarking Multi-Agent LLMs for Intelligent Assistants', 'desc': 'This paper introduces Auto-SLURP, a new benchmark dataset designed to evaluate multi-agent frameworks that utilize large language models (LLMs) in the realm of intelligent personal assistants. It enhances the original SLURP dataset by relabeling data and incorporating simulated servers and external services, allowing for a more thorough assessment of language understanding, task execution, and response generation. The authors demonstrate that Auto-SLURP poses significant challenges to existing state-of-the-art frameworks, indicating that the development of reliable multi-agent personal assistants is still ongoing. The dataset and its associated code are made publicly available for further research and development.'}, 'zh': {'title': 'Auto-SLURPÔºöËØÑ‰º∞Êô∫ËÉΩ‰∏™‰∫∫Âä©ÁêÜÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ', 'desc': 'ËøëÂπ¥Êù•ÔºåÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÂèëÂ±ïËøÖÈÄü„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁº∫‰πè‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Ëøô‰∫õÊ°ÜÊû∂ÊÄßËÉΩÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜAuto-SLURPÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞Âü∫‰∫éLLMÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÁâπÂà´ÊòØÂú®Êô∫ËÉΩ‰∏™‰∫∫Âä©ÁêÜÁöÑËÉåÊôØ‰∏ã„ÄÇAuto-SLURPÈÄöËøáÈáçÊñ∞Ê†áËÆ∞Êï∞ÊçÆÂπ∂Êï¥ÂêàÊ®°ÊãüÊúçÂä°Âô®ÂíåÂ§ñÈÉ®ÊúçÂä°ÔºåÊâ©Â±ï‰∫ÜÂéüÂßãÁöÑSLURPÊï∞ÊçÆÈõÜÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂÖ®Èù¢ÁöÑÁ´ØÂà∞Á´ØËØÑ‰º∞ÊµÅÁ®ã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00212', 'title': 'Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2505.00212', 'abstract': "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution", 'score': 1, 'issue_id': 3639, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '1b127d9b18e51c40', 'authors': ['Shaokun Zhang', 'Ming Yin', 'Jieyu Zhang', 'Jiale Liu', 'Zhiguang Han', 'Jingyang Zhang', 'Beibin Li', 'Chi Wang', 'Huazheng Wang', 'Yiran Chen', 'Qingyun Wu'], 'affiliations': ['Duke University', 'Google DeepMind', 'Meta', 'Nanyang Technological University', 'Oregon State University', 'Pennsylvania State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.00212.jpg', 'data': {'categories': ['#dataset', '#open_source', '#agents', '#reasoning'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ö—Ç–æ –≤–∏–Ω–æ–≤–∞—Ç –∏ –∫–æ–≥–¥–∞: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–±–æ–µ–≤ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM-—Å–∏—Å—Ç–µ–º–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –æ–±–ª–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω —Å–±–æ–µ–≤ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç Who&When, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ —Å–±–æ–µ–≤ –∏–∑ 127 –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM-—Å–∏—Å—Ç–µ–º —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, —Å–≤—è–∑—ã–≤–∞—é—â–∏–º–∏ —Å–±–æ–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –∏ —ç—Ç–∞–ø–∞–º–∏ –æ—à–∏–±–æ–∫. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ –æ—Ü–µ–Ω–µ–Ω—ã —Ç—Ä–∏ –º–µ—Ç–æ–¥–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω —Å–±–æ–µ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏: –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 53.5% –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞ —Å–±–æ–∏, –Ω–æ —Ç–æ–ª—å–∫–æ 14.2% –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —ç—Ç–∞–ø–æ–≤ —Å–±–æ–µ–≤.'}, 'en': {'title': 'Automating Failure Attribution in Multi-Agent Systems', 'desc': 'This paper addresses the challenge of identifying which agent and which step in a multi-agent system led to task failures, a process known as failure attribution. The authors introduce a new dataset called Who&When, which contains detailed failure logs from 127 LLM multi-agent systems, annotated to link failures to specific agents and error steps. They propose three automated methods for failure attribution and evaluate their performance, revealing that the best method only achieves 53.5% accuracy in identifying responsible agents and 14.2% in pinpointing failure steps. The findings indicate that current state-of-the-art models struggle with this task, emphasizing the complexity of automated failure attribution and the need for further research.'}, 'zh': {'title': 'Ëá™Âä®ÂåñÂ§±Ë¥•ÂΩíÂõ†ÔºöLLMÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÊñ∞ÊåëÊàò', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠ÔºåÂ¶Ç‰ΩïËá™Âä®ËØÜÂà´ÂØºËá¥‰ªªÂä°Â§±Ë¥•ÁöÑÊô∫ËÉΩ‰ΩìÂíåÊ≠•È™§„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÁ†îÁ©∂È¢ÜÂüüÔºöËá™Âä®ÂåñÂ§±Ë¥•ÂΩíÂõ†ÔºåÂπ∂ÂºïÂÖ•‰∫ÜWho&WhenÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´127‰∏™LLMÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑËØ¶ÁªÜÂ§±Ë¥•Êó•Âøó„ÄÇÈÄöËøá‰ΩøÁî®Who&WhenÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ÂºÄÂèëÂπ∂ËØÑ‰º∞‰∫Ü‰∏âÁßçËá™Âä®ÂåñÂ§±Ë¥•ÂΩíÂõ†ÊñπÊ≥ïÔºåÂπ∂ÊÄªÁªì‰∫ÜÂÆÉ‰ª¨ÁöÑ‰ºòÁº∫ÁÇπ„ÄÇÂ∞ΩÁÆ°ÊúÄ‰Ω≥ÊñπÊ≥ïÂú®ËØÜÂà´Â§±Ë¥•Ë¥£‰ªªÊô∫ËÉΩ‰ΩìÊñπÈù¢ËææÂà∞‰∫Ü53.5%ÁöÑÂáÜÁ°ÆÁéáÔºå‰ΩÜÂú®Á°ÆÂÆöÂ§±Ë¥•Ê≠•È™§ÊñπÈù¢‰ªÖ‰∏∫14.2%ÔºåÊòæÁ§∫Âá∫ËØ•‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÂíåËøõ‰∏ÄÊ≠•Á†îÁ©∂ÁöÑÂøÖË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18125', 'title': 'TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations', 'url': 'https://huggingface.co/papers/2505.18125', 'abstract': 'TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.', 'score': 95, 'issue_id': 3949, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'e6debd33931f5a78', 'authors': ['Alan Arazi', 'Eilam Shapira', 'Roi Reichart'], 'affiliations': ['Technion - IIT'], 'pdf_title_img': 'assets/pdf/title_img/2505.18125.jpg', 'data': {'categories': ['#optimization', '#dataset', '#transfer_learning', '#architecture', '#benchmark', '#training'], 'emoji': 'üìä', 'ru': {'title': 'TabSTAR: –£–º–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': 'TabSTAR - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. TabSTAR –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ü–µ–ª–µ–≤—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º —Ç–µ–∫—Å—Ç–∞. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'TabSTAR: Revolutionizing Tabular Learning with Contextual Text Understanding', 'desc': 'TabSTAR is a new model designed to improve classification tasks that involve tabular data with text features. It uses a unique approach called semantically target-aware representations, which helps the model understand the context of the data better. Unlike previous methods, TabSTAR does not rely on dataset-specific parameters, allowing it to generalize across different datasets effectively. This model achieves top performance on various benchmarks, demonstrating its potential for enhancing tabular learning tasks.'}, 'zh': {'title': 'TabSTARÔºöË°®Ê†ºÊï∞ÊçÆÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'TabSTARÊòØ‰∏ÄÁßçÊñ∞ÁöÑË°®Ê†ºÂü∫Á°ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éÂ§ÑÁêÜÂ∏¶ÊúâÊñáÊú¨ÁâπÂæÅÁöÑÂàÜÁ±ª‰ªªÂä°„ÄÇÂÆÉÈÄöËøáËΩ¨ÁßªÂ≠¶‰π†ÂÆûÁé∞‰∫ÜÊó†Êï∞ÊçÆÈõÜÁâπÂÆöÂèÇÊï∞ÁöÑÈ´òÊïàÊÄßËÉΩÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôê„ÄÇËØ•Ê®°ÂûãÂà©Áî®ËØ≠‰πâÁõÆÊ†áÊÑüÁü•ÁöÑË°®Á§∫ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£‰ªªÂä°‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÁîüÊàêÊõ¥ÊúâÊïàÁöÑÁâπÂæÅÂµåÂÖ•„ÄÇTabSTARÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂú®‰∏≠ÂûãÂíåÂ§ßÂûãÊï∞ÊçÆÈõÜ‰∏äÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17667', 'title': 'QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17667', 'abstract': 'A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.', 'score': 58, 'issue_id': 3948, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '943935a610a2c31e', 'authors': ['Fanqi Wan', 'Weizhou Shen', 'Shengyi Liao', 'Yingcheng Shi', 'Chenliang Li', 'Ziyi Yang', 'Ji Zhang', 'Fei Huang', 'Jingren Zhou', 'Ming Yan'], 'affiliations': ['Qwen-Doc Team, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.17667.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization', '#benchmark', '#rl', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'QwenLong-L1: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': 'QwenLong-L1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–ª—É—á—à–∞—é—â–∏–π –º–æ–¥–µ–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –∫—É—Ä–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. QwenLong-L1 —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ QwenLong-L1-32B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤–µ–¥—É—â–∏–µ LRM –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.'}, 'en': {'title': 'Empowering Long-Context Reasoning with QwenLong-L1', 'desc': 'The paper introduces QwenLong-L1, a framework designed to enhance large reasoning models (LRMs) for long-context reasoning tasks using reinforcement learning (RL). It addresses the challenges of training efficiency and optimization stability that arise when adapting LRMs from short-context to long-context scenarios. The framework employs a warm-up supervised fine-tuning stage to create a strong initial policy, followed by a curriculum-guided RL approach to ensure stable policy updates. Experimental results show that QwenLong-L1 significantly outperforms existing LRMs on long-context document question-answering benchmarks, marking a significant advancement in the field.'}, 'zh': {'title': 'QwenLong-L1ÔºöÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'QwenLong-L1ÊòØ‰∏Ä‰∏™Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÂú®Èïø‰∏ä‰∏ãÊñáËæìÂÖ•‰∏≠ËøõË°åÊúâÊïàÊé®ÁêÜÁöÑÂÖ≥ÈîÆÊåëÊàòÔºåÂåÖÊã¨ËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÂíå‰ºòÂåñËøáÁ®ã‰∏çÁ®≥ÂÆö„ÄÇÈÄöËøáÈÄêÊ≠•‰∏ä‰∏ãÊñáÊâ©Â±ïÂíåÊ∏©ÊöñÂêØÂä®ÁöÑÁõëÁù£ÂæÆË∞ÉÈò∂ÊÆµÔºåQwenLong-L1Âª∫Á´ã‰∫ÜÁ®≥ÂÅ•ÁöÑÂàùÂßãÁ≠ñÁï•ÔºåÂπ∂ÈááÁî®ËØæÁ®ãÂºïÂØºÁöÑÈò∂ÊÆµÊÄßÂº∫ÂåñÂ≠¶‰π†ÊäÄÊúØÊù•Á®≥ÂÆöÁ≠ñÁï•ÊºîÂèò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQwenLong-L1Âú®Èïø‰∏ä‰∏ãÊñáÊñáÊ°£ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÈ¢ÜÂÖàÁöÑÊé®ÁêÜÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14669', 'title': 'Quartet: Native FP4 Training Can Be Optimal for Large Language Models', 'url': 'https://huggingface.co/papers/2505.14669', 'abstract': 'Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIA\'s recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a "near-optimal" low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.', 'score': 52, 'issue_id': 3953, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '10d7639a81c5e992', 'authors': ['Roberto L. Castro', 'Andrei Panferov', 'Soroush Tabesh', 'Oliver Sieberling', 'Jiale Chen', 'Mahdi Nikdan', 'Saleh Ashkboos', 'Dan Alistarh'], 'affiliations': ['ETH Z√ºrich', 'ISTA', 'ISTA & Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.14669.jpg', 'data': {'categories': ['#architecture', '#training', '#inference', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Quartet - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (FP4). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∏–ª–∏ 8-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å FP4 –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ CUDA-—è–¥—Ä–∞ –¥–ª—è GPU NVIDIA Blackwell. Quartet –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é FP4-–æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ FP8.'}, 'en': {'title': 'Quartet: Revolutionizing LLM Training with FP4 Precision', 'desc': "The paper presents Quartet, a novel training method for large language models (LLMs) that utilizes hardware-supported FP4 precision to enhance training efficiency. By leveraging NVIDIA's Blackwell architecture, Quartet achieves state-of-the-art accuracy while significantly lowering computational costs compared to traditional FP8 and standard precision methods. The authors introduce a new low-precision scaling law that helps balance performance and accuracy across different bit-widths, allowing for effective training of billion-scale models. This approach demonstrates that fully FP4 training can be a viable alternative to existing precision techniques, making it a promising advancement in the field of machine learning."}, 'zh': {'title': 'QuartetÔºöÈ´òÊïàÁöÑFP4ËÆ≠ÁªÉÊñ∞ÊñπÊ≥ï', 'desc': 'QuartetÊòØ‰∏ÄÁßçÁ°¨‰ª∂ÊîØÊåÅÁöÑFP4ËÆ≠ÁªÉÊñπÊ≥ïÔºå‰∏ì‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆæËÆ°„ÄÇÂÆÉÂú®‰øùÊåÅÈ´òÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÁõ∏ÊØî‰∫éÊ†áÂáÜÊàñFP8Á≤æÂ∫¶ÊúâÊòéÊòæ‰ºòÂäø„ÄÇÈÄöËøáÂØπLlamaÁ±ªÂûãÊ®°ÂûãÁöÑÂπøÊ≥õËØÑ‰º∞ÔºåQuartetÊè≠Á§∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ΩéÁ≤æÂ∫¶Áº©ÊîæÊ≥ïÂàôÔºåÂ∏ÆÂä©Êàë‰ª¨ÊâæÂà∞Âú®ÂáÜÁ°ÆÊÄß‰∏éËÆ°ÁÆó‰πãÈó¥ÁöÑÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÈíàÂØπNVIDIA Blackwell GPU‰ºòÂåñÁöÑCUDAÂÜÖÊ†∏ÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑFP4ËÆ≠ÁªÉÔºåËØÅÊòé‰∫ÜFP4ËÆ≠ÁªÉÊòØÊ†áÂáÜÁ≤æÂ∫¶ÂíåFP8ËÆ≠ÁªÉÁöÑÊúâÂäõÊõø‰ª£ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18129', 'title': 'One RL to See Them All: Visual Triple Unified Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.18129', 'abstract': 'A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.', 'score': 49, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'e690c5668b7f4cd0', 'authors': ['Yan Ma', 'Linge Du', 'Xuyang Shen', 'Shaoxiang Chen', 'Pengfei Li', 'Qibing Ren', 'Lizhuang Ma', 'Yuchao Dai', 'Pengfei Liu', 'Junjie Yan'], 'affiliations': ['Google DeepMind', 'MiniMax-AI', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18129.jpg', 'data': {'categories': ['#rl', '#dataset', '#multimodal', '#optimization', '#training', '#open_source', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ò–ò', 'desc': 'V-Triune - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –∑–∞–¥–∞—á–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—ã–±–æ—Ä–∫–∏, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞. –í–≤–µ–¥–µ–Ω–∞ –Ω–æ–≤–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞–≥—Ä–∞–¥–∞ IoU –¥–ª—è –∑–∞–¥–∞—á –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è –º–æ–¥–µ–ª—å Orsta –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞–∫ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è.'}, 'en': {'title': 'Unifying Visual Reasoning and Perception in One RL System', 'desc': 'The paper introduces V-Triune, a unified reinforcement learning system designed to enhance vision-language models (VLMs) by integrating visual reasoning and perception tasks into a single training framework. It features three key components: Sample-Level Data Formatting for input unification, Verifier-Level Reward Computation for tailored reward systems, and Source-Level Metric Monitoring for data diagnostics. A novel Dynamic IoU reward mechanism is also proposed, providing adaptive feedback for perception tasks. The resulting model, Orsta, shows significant performance improvements across various reasoning and perception benchmarks, demonstrating the effectiveness of this unified approach.'}, 'zh': {'title': 'Áªü‰∏ÄÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáËßÜËßâÊé®ÁêÜ‰∏éÊÑüÁü•ËÉΩÂäõ', 'desc': 'V-TriuneÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂº∫ÂåñÂ≠¶‰π†Á≥ªÁªüÔºåÊó®Âú®ÈÄöËøáÂçï‰∏ÄÁöÑËÆ≠ÁªÉÊµÅÁ®ãÁªìÂêàËßÜËßâÊé®ÁêÜÂíåÊÑüÁü•‰ªªÂä°„ÄÇËØ•Á≥ªÁªüÂåÖÂê´‰∏â‰∏™‰∫íË°•ÁöÑÁªÑ‰ª∂ÔºåÂàÜÂà´ÊòØÊ†∑Êú¨Á∫ßÊï∞ÊçÆÊ†ºÂºèÂåñ„ÄÅÈ™åËØÅÂô®Á∫ßÂ•ñÂä±ËÆ°ÁÆóÂíåÊ∫êÁ∫ßÊåáÊ†áÁõëÊéßÔºå‰ª•ÊîØÊåÅÂ§öÊ†∑ÂåñÁöÑ‰ªªÂä°ËæìÂÖ•ÂíåÂÆöÂà∂ÂåñÁöÑÂ•ñÂä±ÂèçÈ¶à„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂä®ÊÄÅIoUÂ•ñÂä±Ôºå‰∏∫ÊÑüÁü•‰ªªÂä°Êèê‰æõÈÄÇÂ∫îÊÄßÂíåÊ∏êËøõÊÄßÁöÑÂèçÈ¶à„ÄÇÈÄöËøáÂú®Â§öÊ†∑ÂåñÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåV-TriuneÊòæËëóÊèêÂçá‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÂíåÊÑüÁü•‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17225', 'title': 'Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2505.17225', 'abstract': 'A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term reasoning rigidity. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, . Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.', 'score': 48, 'issue_id': 3945, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '41036303d3b75082', 'authors': ['Doohyuk Jang', 'Yoonjeon Kim', 'Chanjae Park', 'Hyun Ryu', 'Eunho Yang'], 'affiliations': ['AITRICS', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.17225.jpg', 'data': {'categories': ['#math', '#dataset', '#data', '#interpretability', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è –≤ –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é –º–æ–¥–µ–ª–µ–π –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞–∫–æ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ë—ã–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ç—Ä–∏ —Ä–µ–∂–∏–º–∞ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏: –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –Ω–µ–¥–æ–≤–µ—Ä–∏–µ –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∏ –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç –ø—Ä–∏–≤—ã—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Unraveling Reasoning Rigidity in Language Models', 'desc': 'This paper investigates a phenomenon called reasoning rigidity in large language models, where these models often ignore user instructions and revert to familiar reasoning patterns. The authors introduce a diagnostic set designed to identify and categorize this behavior, which can lead to incorrect conclusions in tasks requiring precise adherence to instructions. They highlight three specific modes of contamination: Interpretation Overload, Input Distrust, and Partial Instruction Attention, which describe how models distort or overlook given instructions. By releasing this diagnostic set, the authors aim to support further research aimed at reducing reasoning rigidity in language models.'}, 'zh': {'title': 'Êè≠Á§∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂÉµÂåñÁé∞Ë±°', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊé®ÁêÜÂÉµÂåñÁé∞Ë±°ÔºåÂç≥Ê®°ÂûãÂú®Èù¢ÂØπÊòéÁ°ÆÊåá‰ª§Êó∂Ôºå‰ªçÁÑ∂ÂÄæÂêë‰∫é‰ΩøÁî®ÁÜüÊÇâÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰∏ìÂÆ∂Á≠ñÂàíÁöÑËØäÊñ≠ÈõÜÔºå‰ª•Á≥ªÁªüÂú∞Á†îÁ©∂Ëøô‰∏ÄË°å‰∏∫ÔºåÁâπÂà´ÊòØÂú®Êï∞Â≠¶ÂíåÈÄªËæëÈöæÈ¢òÁ≠âÈ¢ÜÂüü„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÁªèËøá‰øÆÊîπÁöÑÊï∞Â≠¶Âü∫ÂáÜÂíåÈáçÊñ∞ËÆæËÆ°ÁöÑÈöæÈ¢òÔºåÊó®Âú®‰øÉ‰ΩøÊ®°ÂûãÂÅèÁ¶ªÂ∏∏ËßÑÊé®ÁêÜÁ≠ñÁï•„ÄÇÈÄöËøáÂàÜÊûêÔºåÊàë‰ª¨ËØÜÂà´Âá∫‰∏âÁßç‰∏ªË¶ÅÁöÑÊé®ÁêÜÂÉµÂåñÊ®°ÂºèÔºåÂ∏ÆÂä©Êú™Êù•ÁöÑÁ†îÁ©∂Êõ¥Â•ΩÂú∞Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17612', 'title': 'Distilling LLM Agent into Small Models with Retrieval and Code Tools', 'url': 'https://huggingface.co/papers/2505.17612', 'abstract': 'Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.', 'score': 47, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '258cfb9a5b51fa42', 'authors': ['Minki Kang', 'Jongwon Jeong', 'Seanie Lee', 'Jaewoong Cho', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'KRAFTON'], 'pdf_title_img': 'assets/pdf/title_img/2505.17612.jpg', 'data': {'categories': ['#math', '#small_models', '#agents', '#transfer_learning', '#training', '#hallucinations', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ü–µ—Ä–µ–¥–∞—á–∞ –Ω–∞–≤—ã–∫–æ–≤ –∞–≥–µ–Ω—Ç–∞: –æ—Ç –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–∞–ª—ã–º', 'desc': '–ú–µ—Ç–æ–¥ Agent Distillation –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–µ–Ω—å—à–∏–º –º–æ–¥–µ–ª—è–º. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –∏ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–µ–π—Å—Ç–≤–∏–π. Agent Distillation –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–º –º–æ–¥–µ–ª—è–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –±—ã–ª –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –≤–æ—Å—å–º–∏ –∑–∞–¥–∞—á–∞—Ö –≤ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.'}, 'en': {'title': 'Empowering Small Models with Big Model Intelligence', 'desc': 'Agent Distillation is a method that helps smaller language models (sLMs) learn reasoning and task-solving skills from larger language models (LLMs). It uses improved prompts and self-consistent actions to enhance the performance of sLMs on reasoning tasks, making them competitive with larger models. The approach addresses challenges like hallucination in sLMs when faced with rare facts or complex computations. By evaluating on various reasoning tasks, the study shows that even small models can perform well, paving the way for more efficient AI applications.'}, 'zh': {'title': '‰ª£ÁêÜËí∏È¶èÔºöÂ∞èÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊèêÂçá', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰ª£ÁêÜËí∏È¶èÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜÂíå‰ªªÂä°Ëß£ÂÜ≥ËÉΩÂäõËΩ¨ÁßªÂà∞ËæÉÂ∞èÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàsLMÔºâ‰∏≠„ÄÇÈÄöËøá‰ΩøÁî®Â¢ûÂº∫ÁöÑÊèêÁ§∫ÂíåËá™‰∏ÄËá¥ÊÄßÂä®‰ΩúÔºå‰ª£ÁêÜËí∏È¶èËÉΩÂ§üÂú®Â§ö‰∏™Êé®ÁêÜ‰ªªÂä°‰∏äÂÆûÁé∞‰∏éÂ§ßÂûãÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨ÂºïÂÖ•‰∏ÄÁßçÊñ∞ÁöÑÊèêÁ§∫ÊñπÊ≥ïÂíåÊîπËøõÂ∞èÂûã‰ª£ÁêÜÂú®ÊµãËØïÊó∂ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂèÇÊï∞Èáè‰∏∫0.5BÂà∞3BÁöÑÂ∞èÂûãÊ®°ÂûãÂèØ‰ª•Âú®‰∫ãÂÆûÂíåÊï∞Â≠¶È¢ÜÂüüÁöÑÊé®ÁêÜ‰ªªÂä°‰∏≠‰∏éÊõ¥Â§ßÁöÑÊ®°ÂûãÁ´û‰∫â„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15929', 'title': 'PhyX: Does Your Model Have the "Wits" for Physical Reasoning?', 'url': 'https://huggingface.co/papers/2505.15929', 'abstract': "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy respectively-performance gaps exceeding 29\\% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation.", 'score': 38, 'issue_id': 3950, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '995e1fe73d1e1cef', 'authors': ['Hui Shen', 'Taiqiang Wu', 'Qi Han', 'Yunta Hsieh', 'Jizhou Wang', 'Yuyue Zhang', 'Yuxin Cheng', 'Zijian Hao', 'Yuansheng Ni', 'Xin Wang', 'Zhongwei Wan', 'Kai Zhang', 'Wendong Xu', 'Jing Xiong', 'Ping Luo', 'Wenhu Chen', 'Chaofan Tao', 'Zhuoqing Mao', 'Ngai Wong'], 'affiliations': ['Independent', 'The Ohio State University', 'The University of Hong Kong', 'University of Michigan', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.15929.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': 'PhyX: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò', 'desc': '–ù–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç PhyX –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –¢–µ—Å—Ç –≤–∫–ª—é—á–∞–µ—Ç 3000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ 6 —Ç–∏–ø–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ 25 –ø–æ–¥–¥–æ–º–µ–Ω–∞—Ö –∏ 6 –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö —Ñ–∏–∑–∏–∫–∏. –î–∞–∂–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4 –∏ Claude, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏-–ª—é–¥—å–º–∏. –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è —á—Ä–µ–∑–º–µ—Ä–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è.'}, 'en': {'title': 'PhyX: Bridging the Gap in Physics-Grounded Reasoning for AI', 'desc': "The paper introduces PhyX, a new benchmark for evaluating models' abilities in physics-grounded reasoning within visual contexts. It highlights that existing benchmarks do not adequately assess this crucial aspect of intelligence, which combines domain knowledge and real-world constraints. PhyX consists of 3,000 carefully curated multimodal questions across various physics domains, revealing that even advanced models like GPT-4o and Claude3.7-Sonnet perform poorly compared to human experts. The study identifies key limitations in current models, such as reliance on memorized knowledge and superficial visual pattern recognition, and provides a robust evaluation framework for future research."}, 'zh': {'title': 'PhyXÔºöËØÑ‰º∞Áâ©ÁêÜÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'PhyXÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÂú®ËßÜËßâÂú∫ÊôØ‰∏≠ÁöÑÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑÊ®°ÂûãÂú®Áâ©ÁêÜÁêÜËß£ÊñπÈù¢Â≠òÂú®ÊòæËëóÂ±ÄÈôêÔºåËøú‰∏çÂèä‰∫∫Á±ª‰∏ìÂÆ∂„ÄÇPhyXÂåÖÂê´3000‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑÂ§öÊ®°ÊÄÅÈóÆÈ¢òÔºåÊ∂µÁõñ25‰∏™Â≠êÈ¢ÜÂüüÂíå6‰∏™Ê†∏ÂøÉÁâ©ÁêÜÈ¢ÜÂüü„ÄÇÈÄöËøáÂÖ®Èù¢ËØÑ‰º∞ÔºåÂèëÁé∞Âç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Áâ©ÁêÜÊé®ÁêÜ‰∏ä‰πüÈù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºåÂáÜÁ°ÆÁéáËøú‰Ωé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18092', 'title': 'QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization', 'url': 'https://huggingface.co/papers/2505.18092', 'abstract': 'QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report presents QwenLong-CPRS, a context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the "lost in the middle" performance degradation of large language models (LLMs) during long sequence processing. Implemented through a novel dynamic context optimization mechanism, QwenLong-CPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance.   Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference.   Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QwenLong-CPRS\'s threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5-max, achieves 21.59times context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance.', 'score': 37, 'issue_id': 3949, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '59626d27805a1427', 'authors': ['Weizhou Shen', 'Chenliang Li', 'Fanqi Wan', 'Shengyi Liao', 'Shaopeng Lai', 'Bo Zhang', 'Yingcheng Shi', 'Yuning Wu', 'Gang Fu', 'Zhansheng Li', 'Bin Yang', 'Ji Zhang', 'Fei Huang', 'Jingren Zhou', 'Ming Yan'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.18092.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#training', '#long_context'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö', 'desc': 'QwenLong-CPRS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Å–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. QwenLong-CPRS —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –¢–µ—Å—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ QwenLong-CPRS –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∫–∞–∫ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∏ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Revolutionizing Long Context Management in LLMs', 'desc': 'QwenLong-CPRS is a framework that improves large language models (LLMs) by optimizing how they handle long contexts. It uses a dynamic optimization method that is guided by natural language, allowing for better context compression at multiple levels. This approach not only enhances the efficiency of processing long sequences but also improves the overall performance of the models. The framework has been tested against various benchmarks, showing significant gains in accuracy and efficiency compared to existing context management techniques.'}, 'zh': {'title': 'QwenLong-CPRSÔºöÈ´òÊïàÁöÑ‰∏ä‰∏ãÊñáÂéãÁº©‰∏é‰ºòÂåñ', 'desc': 'QwenLong-CPRS ÊòØ‰∏ÄÁßçÁî®‰∫éÈïø‰∏ä‰∏ãÊñá‰ºòÂåñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÁöÑËÆ°ÁÆóÂºÄÈîÄÂíåÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊåáÂØºÁöÑÂä®ÊÄÅ‰∏ä‰∏ãÊñá‰ºòÂåñÊú∫Âà∂ÔºåÂÆûÁé∞‰∫ÜÂ§öÁ≤íÂ∫¶ÁöÑ‰∏ä‰∏ãÊñáÂéãÁº©Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊïàÁéáÂíåÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•‰∫ÜÂõõÈ°πÂÖ≥ÈîÆÂàõÊñ∞ÔºåÂåÖÊã¨ÂèåÂêëÊé®ÁêÜÂ±ÇÂíåÂü∫‰∫éËØ≠Ë®ÄÂª∫Ê®°ÁöÑ‰ª§ÁâåËØÑ‰º∞Êú∫Âà∂„ÄÇÁªºÂêàËØÑ‰º∞ÊòæÁ§∫ÔºåQwenLong-CPRS Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñ‰∏ä‰∏ãÊñáÁÆ°ÁêÜÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17618', 'title': 'Scaling Image and Video Generation via Test-Time Evolutionary Search', 'url': 'https://huggingface.co/papers/2505.17618', 'abstract': 'EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.  \t\t\t\t\tAI-generated summary \t\t\t\t As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose Evolutionary Search (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.', 'score': 31, 'issue_id': 3949, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'dfe3bcf9e6ec80f9', 'authors': ['Haoran He', 'Jiajun Liang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Ling Pan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.17618.jpg', 'data': {'categories': ['#video', '#optimization', '#cv', '#inference', '#diffusion', '#training'], 'emoji': 'üß¨', 'ru': {'title': '–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'EvoSearch - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–µ—Å—Ç-—Ç–∞–π–º —Å–∫–µ–π–ª–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø–æ—Ç–æ–∫–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. EvoSearch –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —ç—Ç–∞–ø–µ –≤—ã–≤–æ–¥–∞. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö –æ—Ü–µ–Ω–∫–∏.'}, 'en': {'title': 'EvoSearch: Evolving Image and Video Generation at Test Time', 'desc': 'EvoSearch is an innovative evolutionary search method designed to improve test-time scaling (TTS) for generative models, specifically diffusion and flow-based models. It addresses the limitations of existing TTS strategies by reformulating the scaling process as an evolutionary problem, allowing for efficient exploration of denoising trajectories. By utilizing selection and mutation mechanisms inspired by biological evolution, EvoSearch enhances the quality and diversity of generated images and videos without the need for additional training. Our extensive evaluations show that EvoSearch consistently outperforms current methods, demonstrating superior generalizability and diversity in generative tasks.'}, 'zh': {'title': 'ËøõÂåñÊêúÁ¥¢ÔºöÊèêÂçáÁîüÊàêÊ®°ÂûãÁöÑÊµãËØïÊó∂Êâ©Â±ïËÉΩÂäõ', 'desc': 'EvoSearchÊòØ‰∏ÄÁßçËøõÂåñÊêúÁ¥¢ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊâ©Êï£ÂíåÊµÅÂºèÁîüÊàêÊ®°ÂûãÂú®ÊµãËØïÊó∂ÁöÑÊâ©Â±ïËÉΩÂäõÔºå‰ªéËÄåÊîπÂñÑÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÁöÑË¥®Èáè„ÄÅÂ§öÊ†∑ÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÈöèÁùÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉÊúüÈó¥ËÆ°ÁÆóÊàêÊú¨ÁöÑÊòæËëóÂ¢ûÂä†ÔºåÊµãËØïÊó∂Êâ©Â±ïÔºàTTSÔºâÊàê‰∏∫ÊèêÂçáÁîüÊàêÊ®°ÂûãÊÄßËÉΩÁöÑ‰∏Ä‰∏™ÊúâÂâçÊôØÁöÑÊñπÂêë„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÊñπÊ≥ïÂú®ËØ≠Ë®Ä‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÊµãËØïÊó∂Êâ©Â±ïË°å‰∏∫‰∏ä‰ªçÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇEvoSearchÈÄöËøáÂ∞ÜÊµãËØïÊó∂Êâ©Â±ïÈáçÊñ∞ÊûÑÈÄ†Êàê‰∏Ä‰∏™ËøõÂåñÊêúÁ¥¢ÈóÆÈ¢òÔºåÂà©Áî®ÁîüÁâ©ËøõÂåñÁöÑÂéüÁêÜÈ´òÊïàÊé¢Á¥¢Âíå‰ºòÂåñÂéªÂô™ËΩ®ËøπÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÊõ¥È´òË¥®ÈáèÁöÑÁîüÊàêÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17873', 'title': 'MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback', 'url': 'https://huggingface.co/papers/2505.17873', 'abstract': "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.", 'score': 24, 'issue_id': 3951, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '45be71b3061e9e57', 'authors': ['Wanhao Liu', 'Zonglin Yang', 'Jue Wang', 'Lidong Bing', 'Di Zhang', 'Dongzhan Zhou', 'Yuqiang Li', 'Houqiang Li', 'Erik Cambria', 'Wanli Ouyang'], 'affiliations': ['MiroMind', 'Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17873.jpg', 'data': {'categories': ['#science', '#optimization', '#data', '#dataset', '#benchmark'], 'emoji': 'üß™', 'ru': {'title': '–°–∏–º—É–ª—è—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏–º—É–ª—è—Ç–æ—Ä, –º–æ–¥–µ–ª–∏—Ä—É—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∏—Å—Ç–∏–Ω–Ω–æ–π –≥–∏–ø–æ—Ç–µ–∑–æ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –≥–∏–ø–æ—Ç–µ–∑ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Hypothesis Ranking with Simulation-Driven Insights', 'desc': 'This paper presents a new method for ranking scientific hypotheses by using simulated experimental outcomes, which helps prioritize hypotheses before actual experiments are conducted. Traditional methods rely on large language models for pre-experiment ranking, but they do not consider real experimental results. The authors introduce an experiment-guided ranking approach that uses a simulator to model how hypotheses perform based on their similarity to known successful hypotheses, while accounting for noise. Their method, validated with a dataset of chemistry hypotheses, shows improved performance over existing pre-experiment ranking techniques.'}, 'zh': {'title': 'ÂÆûÈ™åÂºïÂØºÁöÑÂÅáËÆæ‰ºòÂÖàÁ∫ßÊéíÂ∫èÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ®°ÊãüÂô®ÂíåÂÆûÈ™åÂºïÂØºÁöÑÊéíÂêçÊñπÊ≥ïÔºå‰ª•ÊîπÂñÑÁßëÂ≠¶ÂèëÁé∞‰∏≠ÁöÑÂÅáËÆæ‰ºòÂÖàÁ∫ßÊéíÂ∫è„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁªìÂêàÊ®°ÊãüÂÆûÈ™åÁªìÊûúÔºå‰ºòÂÖàËÄÉËôëÂü∫‰∫éÂÖàÂâçÊµãËØïÁªìÊûúÁöÑÂÄôÈÄâÂÅáËÆæ„ÄÇÊàë‰ª¨ÂºÄÂèëÁöÑÊ®°ÊãüÂô®Âü∫‰∫é‰∏â‰∏™È¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÂÅáËÆæÔºåËÉΩÂ§üÊ®°ÊãüÂÅáËÆæÊÄßËÉΩÔºåÂπ∂ÈÄöËøáÂô™Â£∞ËøõË°åÊâ∞Âä®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂÅáËÆæÊéíÂêç‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÈ¢ÑÂÆûÈ™åÂü∫Á∫øÂíåÂº∫Ê∂àËûçÂÆûÈ™å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17561', 'title': 'Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model', 'url': 'https://huggingface.co/papers/2505.17561', 'abstract': 'ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: https://anse-project.github.io/anse-project/', 'score': 24, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '224b15e182587a84', 'authors': ['Kwanyoung Kim', 'Sanghyun Kim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.17561.jpg', 'data': {'categories': ['#video', '#inference', '#diffusion', '#optimization'], 'emoji': 'üé¨', 'ru': {'title': '–£–º–Ω—ã–π –≤—ã–±–æ—Ä —à—É–º–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–∏–¥–µ–æ-—Å–∏–Ω—Ç–µ–∑–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ANSE - –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —à—É–º–æ–≤—ã—Ö —Å–∏–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏—è BANSA, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π –º–µ–∂–¥—É —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–±–æ—Ä–∫–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è BANSA —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –ë–µ—Ä–Ω—É–ª–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.'}, 'en': {'title': 'Smart Noise Selection for Better Video Generation', 'desc': "The paper introduces ANSE, a method that enhances video diffusion models by intelligently selecting noise seeds based on the model's confidence. It highlights the importance of initial noise in generating high-quality videos, as different seeds can lead to varying results. ANSE utilizes an acquisition function called BANSA, which measures uncertainty through attention-based entropy to identify the best noise seeds. This approach improves video quality and temporal coherence while only slightly increasing the time needed for inference."}, 'zh': {'title': '‰∏ªÂä®ÈÄâÊã©Âô™Â£∞ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè', 'desc': 'ANSEÔºà‰∏ªÂä®Âô™Â£∞ÈÄâÊã©ÁîüÊàêÔºâÈÄöËøáÂü∫‰∫éÊ®°Âûã‰ø°ÂøÉÈÄâÊã©Âô™Â£∞ÁßçÂ≠êÔºåÂ¢ûÂº∫‰∫ÜËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÈáèÂåñ‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÈÄâÊã©È´òË¥®ÈáèÁöÑÂô™Â£∞ÁßçÂ≠êÔºåÊòæËëóÊèêÈ´òËßÜÈ¢ëË¥®ÈáèÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÊ†∏ÂøÉÁÆóÊ≥ïBANSAÔºàÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑË¥ùÂè∂ÊñØ‰∏ªÂä®Âô™Â£∞ÈÄâÊã©ÔºâÈÄöËøáÊµãÈáèÂ§ö‰∏™ÈöèÊú∫Ê≥®ÊÑèÂäõÊ†∑Êú¨‰πãÈó¥ÁöÑÁÜµ‰∏ç‰∏ÄËá¥ÊÄßÊù•‰º∞ËÆ°Ê®°ÂûãÁöÑ‰ø°ÂøÉÂíå‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåANSEÂú®Êé®ÁêÜÊó∂Èó¥‰ªÖÂ¢ûÂä†8%Âíå13%ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊîπÂñÑ‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17941', 'title': 'VeriThinker: Learning to Verify Makes Reasoning Model Efficient', 'url': 'https://huggingface.co/papers/2505.17941', 'abstract': 'VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker', 'score': 22, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'cfc0e5dae345ea81', 'authors': ['Zigeng Chen', 'Xinyin Ma', 'Gongfan Fang', 'Ruonan Yu', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.17941.jpg', 'data': {'categories': ['#math', '#optimization', '#inference', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏', 'desc': 'VeriThinker - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ LRM –Ω–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —à–∞–≥–æ–≤ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VeriThinker –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–ª–∏ –¥–∞–∂–µ –Ω–µ–º–Ω–æ–≥–æ —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å. –ü–æ–¥—Ö–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Streamlining Reasoning with VeriThinker', 'desc': 'VeriThinker is a method designed to enhance Large Reasoning Models (LRMs) by reducing the length of their reasoning chains. It achieves this by fine-tuning the models on a verification task instead of directly on the original reasoning tasks. This approach helps the models become more efficient by minimizing unnecessary steps in their reasoning process, which lowers inference costs. Experimental results show that VeriThinker not only shortens reasoning chains but also improves accuracy in various tasks, demonstrating its effectiveness in optimizing LRM performance.'}, 'zh': {'title': 'VeriThinkerÔºö‰ºòÂåñÊé®ÁêÜÈìæÔºåÊèêÂçáÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'VeriThinkerÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂú®È™åËØÅ‰ªªÂä°‰∏äÂæÆË∞ÉÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÔºåÂáèÂ∞ëÂ§çÊùÇÊé®ÁêÜÈìæÁöÑÈïøÂ∫¶Ôºå‰ªéËÄåÈôç‰ΩéÊé®ÁêÜÊàêÊú¨ËÄå‰∏çÊòæËëóÁâ∫Áâ≤ÂáÜÁ°ÆÊÄß„ÄÇ‰º†ÁªüÊñπÊ≥ïÁõ¥Êé•Âú®ÂéüÂßãÊé®ÁêÜ‰ªªÂä°‰∏äÂæÆË∞ÉÊ®°ÂûãÔºåËÄåVeriThinkerÂàôÂàõÊñ∞ÊÄßÂú∞‰ªÖÈÄöËøáËæÖÂä©È™åËØÅ‰ªªÂä°ËøõË°åÂæÆË∞É„ÄÇÈÄöËøáËÆ≠ÁªÉLRMsÂáÜÁ°ÆÈ™åËØÅÊé®ÁêÜËß£ÂÜ≥ÊñπÊ°àÁöÑÊ≠£Á°ÆÊÄßÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà§Êñ≠ÂêéÁª≠Ëá™ÊàëÂèçÊÄùÊ≠•È™§ÁöÑÂøÖË¶ÅÊÄßÔºåÊúâÊïàÊäëÂà∂ËøáÂ∫¶ÊÄùËÄÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVeriThinkerÊòæËëóÂáèÂ∞ë‰∫ÜÊé®ÁêÜÈìæÁöÑÈïøÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅÊàñÁï•ÂæÆÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16211', 'title': 'AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.16211', 'abstract': 'AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.', 'score': 17, 'issue_id': 3946, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '1849951d3588375e', 'authors': ['Kai Li', 'Can Shen', 'Yile Liu', 'Jirui Han', 'Kelong Zheng', 'Xuechao Zou', 'Zhe Wang', 'Xingjian Du', 'Shun Zhang', 'Hanjun Luo', 'Yingbin Jin', 'Xinxin Xing', 'Ziyang Ma', 'Yue Liu', 'Xiaojun Jia', 'Yifan Zhang', 'Junfeng Fang', 'Kun Wang', 'Yibo Yan', 'Haoyang Li', 'Yiming Li', 'Xiaobin Zhuang', 'Yang Liu', 'Haibo Hu', 'Zhuo Chen', 'Zhizheng Wu', 'Xiaolin Hu', 'Eng-Siong Chng', 'XiaoFeng Wang', 'Wenyuan Xu', 'Wei Dong', 'Xinfeng Li'], 'affiliations': ['ACM Member', 'BJTU', 'BNBU', 'Bytedance', 'CAS', 'HUST', 'Hong Kong Polytechnic University', 'Hong Kong University of Science and Technology (Guangzhou)', 'Independent Researcher', 'Nanyang Technological University', 'National University of Singapore', 'QHU', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong (Shenzhen)', 'Tsinghua University', 'University of Rochester', 'Waseda University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16211.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#security', '#ethics', '#open_source', '#dataset', '#audio'], 'emoji': 'üéôÔ∏è', 'ru': {'title': 'AudioTrust: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ –ò–ò', 'desc': 'AudioTrust - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ê—É–¥–∏–æ –ë–æ–ª—å—à–∏—Ö –Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥–µ–ª–µ–π (–ê–ë–õ–ú). –û–Ω–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ê–ë–õ–ú –ø–æ —à–µ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å, –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 4420 –∞—É–¥–∏–æ/—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ 9 —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤—ã—è–≤–ª—è—é—Ç –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ê–ë–õ–ú –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤—ã—Ö –∞—É–¥–∏–æ—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'Evaluating Trust in Audio Large Language Models with AudioTrust', 'desc': 'AudioTrust is a novel framework designed to evaluate the trustworthiness of Audio Large Language Models (ALLMs) across multiple dimensions. It addresses the unique challenges and risks associated with audio data, which are often overlooked in existing evaluation methods that focus primarily on text. The framework includes a comprehensive dataset of over 4,420 audio/text samples and employs 18 experimental setups to assess six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. By utilizing nine audio-specific metrics and an automated scoring pipeline, AudioTrust provides insights into the limitations of current ALLMs, guiding their secure deployment in real-world applications.'}, 'zh': {'title': 'Èü≥È¢ëÊ®°Âûã‰ø°‰ªªËØÑ‰º∞Êñ∞Ê†áÂáÜ', 'desc': 'AudioTrustÊòØ‰∏Ä‰∏™‰∏ìÈó®‰∏∫Èü≥È¢ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàALLMsÔºâËÆæËÆ°ÁöÑÂ§öÁª¥‰ø°‰ªªËØÑ‰º∞Ê°ÜÊû∂„ÄÇÂÆÉÈÄöËøá‰∏Ä‰∏™ÂåÖÂê´4420Â§ö‰∏™Èü≥È¢ë/ÊñáÊú¨Ê†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜÔºåËØÑ‰º∞Ê®°ÂûãÂú®ÂÖ¨Âπ≥ÊÄß„ÄÅÂπªËßâ„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÈöêÁßÅ„ÄÅÈ≤ÅÊ£íÊÄßÂíåËÆ§ËØÅÁ≠âÂÖ≠‰∏™ÂÖ≥ÈîÆÁª¥Â∫¶ÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∫Ü18Áßç‰∏çÂêåÁöÑÂÆûÈ™åËÆæÁΩÆÂíå9‰∏™Èü≥È¢ëÁâπÂÆöÁöÑËØÑ‰º∞ÊåáÊ†áÔºå‰ª•Á°Æ‰øùÂÖ®Èù¢ËØÑ‰º∞ALLMsÁöÑ‰ø°‰ªªworthiness„ÄÇÂÆûÈ™åÁªìÊûúÊè≠Á§∫‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÂºÄÊ∫êÂíåÈó≠Ê∫êALLMsÂú®È´òÈ£éÈô©Èü≥È¢ëÂú∫ÊôØ‰∏ãÁöÑ‰ø°‰ªªËæπÁïåÂíåÂ±ÄÈôêÊÄßÔºå‰∏∫Êú™Êù•Èü≥È¢ëÊ®°ÂûãÁöÑÂÆâÂÖ®ÂíåÂèØ‰ø°ÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÈáçË¶ÅËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17558', 'title': 'Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection', 'url': 'https://huggingface.co/papers/2505.17558', 'abstract': "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.", 'score': 13, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '9faa21418742a88c', 'authors': ['Shrey Pandit', 'Ashwin Vinod', 'Liu Leqi', 'Ying Ding'], 'affiliations': ['The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.17558.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#rlhf', '#training', '#hallucinations'], 'emoji': 'üîç', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ LLM —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å–∞–º–∏—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Å–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ DPO-–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ–±–Ω—ã–º –ø–ª–∞–Ω–æ–º, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥—è –æ—Ç –ª–µ–≥–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∫ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ HaluCheck, –æ–±—É—á–µ–Ω–Ω—ã–µ —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.'}, 'en': {'title': 'Enhancing Hallucination Detection in LLMs through Curriculum Learning', 'desc': "This paper presents a novel approach to improve large language models' (LLMs) ability to detect hallucinations by using specially designed negative examples in a curriculum learning framework. The authors recognize that hallucinated texts are often more deceptive than standard negative samples, and they leverage this insight in the DPO alignment procedure. By gradually increasing the difficulty of training samples, the method ensures that LLMs learn to identify hallucinations more effectively over time. Experimental results show that the proposed HaluCheck models achieve significant performance gains, particularly on challenging benchmarks, and demonstrate strong performance even in zero-shot scenarios."}, 'zh': {'title': 'Âà©Áî®ËØæÁ®ãÂ≠¶‰π†ÊèêÂçáÂπªËßâÊ£ÄÊµãËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®DPOÂØπÈΩêËøáÁ®ã‰∏≠‰ΩøÁî®Á≤æÂøÉËÆæËÆ°ÁöÑÂπªËßâÊ†∑Êú¨ÁöÑËØæÁ®ãÂ≠¶‰π†ÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂØπÂπªËßâÁöÑÊ£ÄÊµãËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆ§ËØÜÂà∞ÔºåÂπªËßâÊ†∑Êú¨ÈÄöÂ∏∏ÊØî‰º†ÁªüÁöÑË¥üÊ†∑Êú¨ÂÖ∑ÊúâÊõ¥È´òÁöÑÊ¨∫È™óÊÄßÔºåÂõ†Ê≠§Â∞ÜËøô‰∫õÂπªËßâÊ†∑Êú¨‰Ωú‰∏∫Ë¥ü‰æã‰ΩøÁî®„ÄÇÈÄöËøáÈÄêÊ≠•ÂºïÂÖ•Êõ¥ÈöæÁöÑÊ†∑Êú¨ÔºåÊàë‰ª¨ÁöÑËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•Á°Æ‰øù‰∫ÜÁ®≥ÂÆöÁöÑÂ¢ûÈáèÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ËØæÁ®ãDPOÊñπÊ≥ïÂíåÈ´òË¥®ÈáèË¥üÊ†∑Êú¨ËÆ≠ÁªÉÁöÑHaluCheckÊ®°ÂûãÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÂ∞§ÂÖ∂Âú®MedHalluÂíåHaluEvalÁ≠âÂõ∞ÈöæÂü∫ÂáÜ‰∏äÊèêÂçá‰∫ÜÂ§öËææ24%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17412', 'title': 'Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention', 'url': 'https://huggingface.co/papers/2505.17412', 'abstract': 'A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high resolution 3D shapes using volumetric representations such as Signed Distance Functions presents substantial computational and memory challenges. We introduce Direct3D S2, a scalable 3D generation framework based on sparse volumes that achieves superior output quality with dramatically reduced training costs. Our key innovation is the Spatial Sparse Attention mechanism, which greatly enhances the efficiency of Diffusion Transformer computations on sparse volumetric data. SSA allows the model to effectively process large token sets within sparse volumes, significantly reducing computational overhead and achieving a 3.9x speedup in the forward pass and a 9.6x speedup in the backward pass. Our framework also includes a variational autoencoder that maintains a consistent sparse volumetric format across input, latent, and output stages. Compared to previous methods with heterogeneous representations in 3D VAE, this unified design significantly improves training efficiency and stability. Our model is trained on public available datasets, and experiments demonstrate that Direct3D S2 not only surpasses state-of-the-art methods in generation quality and efficiency, but also enables training at 1024 resolution using only 8 GPUs, a task typically requiring at least 32 GPUs for volumetric representations at 256 resolution, thus making gigascale 3D generation both practical and accessible. Project page: https://nju3dv.github.io/projects/Direct3D-S2/.', 'score': 13, 'issue_id': 3948, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'e0f74abb880208f9', 'authors': ['Shuang Wu', 'Youtian Lin', 'Feihu Zhang', 'Yifei Zeng', 'Yikang Yang', 'Yajie Bao', 'Jiachen Qian', 'Siyu Zhu', 'Philip Torr', 'Xun Cao', 'Yao Yao'], 'affiliations': ['DreamTech', 'Fudan University', 'Nanjing University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2505.17412.jpg', 'data': {'categories': ['#dataset', '#3d', '#training', '#optimization', '#diffusion'], 'emoji': 'üßä', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Direct3D S2 - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–º—ã –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º Spatial Sparse Attention, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –µ–¥–∏–Ω—ã–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–º–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Direct3D S2 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–æ–∑–≤–æ–ª—è—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 1024 –Ω–∞ –≤—Å–µ–≥–æ 8 GPU.'}, 'en': {'title': 'Efficient High-Resolution 3D Shape Generation with Sparse Volumes', 'desc': 'This paper presents Direct3D S2, a framework for generating high-resolution 3D shapes using sparse volumetric representations. It introduces a Spatial Sparse Attention mechanism that enhances the efficiency of computations in Diffusion Transformers, allowing for significant reductions in training time and resource usage. The framework employs a variational autoencoder to maintain a consistent format across different stages of processing, improving training stability. Overall, Direct3D S2 achieves superior generation quality while drastically lowering the computational requirements, making high-resolution 3D shape generation more accessible.'}, 'zh': {'title': 'È´òÊïàÁîüÊàêÈ´òÂàÜËæ®Áéá3DÂΩ¢Áä∂ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑ3DÂΩ¢Áä∂ÁîüÊàêÊ°ÜÊû∂ÔºåÂêç‰∏∫Direct3D S2ÔºåÂà©Áî®Á®ÄÁñè‰ΩìÁßØÂíåÁ©∫Èó¥Á®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§ü‰ª•ËæÉ‰ΩéÁöÑËÆ°ÁÆóÈúÄÊ±ÇÁîüÊàêÈ´òÂàÜËæ®ÁéáÁöÑ3DÂΩ¢Áä∂„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁ®ÄÁñè‰ΩìÁßØÁöÑËÆæËÆ°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÂπ∂Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨„ÄÇÁ©∫Èó¥Á®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÊèêÂçá‰∫ÜÊâ©Êï£ÂèòÊç¢Âô®Âú®Á®ÄÁñè‰ΩìÁßØÊï∞ÊçÆ‰∏äÁöÑËÆ°ÁÆóÊïàÁéáÔºåÂÆûÁé∞‰∫ÜÂâçÂêë‰º†Êí≠ÈÄüÂ∫¶ÊèêÈ´ò3.9ÂÄçÂíåÂèçÂêë‰º†Êí≠ÈÄüÂ∫¶ÊèêÈ´ò9.6ÂÄç„ÄÇ‰∏é‰º†ÁªüÁöÑ3DÂèòÂàÜËá™ÁºñÁ†ÅÂô®Áõ∏ÊØîÔºåDirect3D S2Âú®ËÆ≠ÁªÉÊïàÁéáÂíåÁ®≥ÂÆöÊÄß‰∏äÊúâ‰∫ÜÊòæËëóÊîπÂñÑÔºå‰ΩøÂæóÂú®‰ªÖ‰ΩøÁî®8‰∏™GPUÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞1024ÂàÜËæ®ÁéáÁöÑËÆ≠ÁªÉÊàê‰∏∫ÂèØËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17399', 'title': 'FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow', 'url': 'https://huggingface.co/papers/2505.17399', 'abstract': 'FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them into code, and iteratively refine the implementation. While recent benchmarks primarily focus on converting visual designs to code, we present FullFront, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across the full front-end development pipeline. FullFront assesses three fundamental tasks that map directly to the front-end engineering pipeline: Webpage Design (conceptualization phase), Webpage Perception QA (comprehension of visual organization and elements), and Webpage Code Generation (implementation phase). Unlike existing benchmarks that use either scraped websites with bloated code or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage process to transform real-world webpages into clean, standardized HTML while maintaining diverse visual designs and avoiding copyright issues. Extensive testing of state-of-the-art MLLMs reveals significant limitations in page perception, code generation (particularly for image handling and layout), and interaction implementation. Our results quantitatively demonstrate performance disparities across models and tasks, and highlight a substantial gap between current MLLM capabilities and human expert performance in front-end engineering. The FullFront benchmark and code are available in https://github.com/Mikivishy/FullFront.', 'score': 13, 'issue_id': 3951, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'da19bb4affb160fc', 'authors': ['Haoyu Sun', 'Huichen Will Wang', 'Jiawei Gu', 'Linjie Li', 'Yu Cheng'], 'affiliations': ['Microsoft', 'Sun Yat-sen University', 'The Chinese University of Hong Kong', 'Tongji University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.17399.jpg', 'data': {'categories': ['#games', '#multimodal', '#optimization', '#survey', '#benchmark'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': 'FullFront: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ MLLM –≤ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ', 'desc': 'FullFront - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –æ–±–ª–∞—Å—Ç–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –û–Ω –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏: –¥–∏–∑–∞–π–Ω –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞. –ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ HTML –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'FullFront: Bridging the Gap in Front-End Engineering with MLLMs', 'desc': 'FullFront is a benchmark that evaluates Multimodal Large Language Models (MLLMs) in the context of front-end engineering, which includes design, comprehension, and implementation tasks. It focuses on three key areas: conceptualizing webpage designs, understanding visual elements through QA, and generating clean HTML code. Unlike previous benchmarks, FullFront uses a unique two-stage process to create standardized HTML from real-world webpages, ensuring diverse designs without copyright issues. The findings reveal significant performance gaps in MLLMs compared to human experts, particularly in page perception and code generation tasks.'}, 'zh': {'title': 'ÂÖ®Èù¢ËØÑ‰º∞ÂâçÁ´ØÂ∑•Á®ãÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'desc': 'FullFrontÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂâçÁ´ØÂ∑•Á®ã‰∏≠ÁöÑÊ¶ÇÂøµÂåñ„ÄÅÁêÜËß£ÂíåÂÆûÁé∞Èò∂ÊÆµÁöÑË°®Áé∞„ÄÇÂÆÉÊ∂µÁõñ‰∫ÜÁΩëÈ°µËÆæËÆ°„ÄÅÁΩëÈ°µÊÑüÁü•ÈóÆÁ≠îÂíåÁΩëÈ°µ‰ª£Á†ÅÁîüÊàê‰∏â‰∏™Âü∫Êú¨‰ªªÂä°ÔºåÂèçÊò†‰∫ÜÂâçÁ´ØÂºÄÂèëÁöÑÂÆåÊï¥ÊµÅÁ®ã„ÄÇ‰∏éÁé∞ÊúâÂü∫ÂáÜ‰∏çÂêåÔºåFullFrontÈááÁî®Êñ∞È¢ñÁöÑ‰∏§Èò∂ÊÆµËøáÁ®ãÔºåÂ∞ÜÁúüÂÆûÁΩëÈ°µËΩ¨Âåñ‰∏∫Âπ≤ÂáÄ„ÄÅÊ†áÂáÜÂåñÁöÑHTMLÔºåÂêåÊó∂‰øùÊåÅÂ§öÊ†∑ÁöÑËßÜËßâËÆæËÆ°„ÄÇÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®È°µÈù¢ÊÑüÁü•„ÄÅ‰ª£Á†ÅÁîüÊàêÂíå‰∫§‰∫íÂÆûÁé∞ÊñπÈù¢Â≠òÂú®ÊòæËëóÁöÑÂ±ÄÈôêÊÄßÔºåËøú‰Ωé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17955', 'title': 'Diffusion Classifiers Understand Compositionality, but Conditions Apply', 'url': 'https://huggingface.co/papers/2505.17955', 'abstract': 'A study of diffusion classifiers across multiple datasets and tasks reveals their compositional understanding, highlighting domain-specific performance effects and timestep weighting importance.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.', 'score': 12, 'issue_id': 3954, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'ca791ea943cccda5', 'authors': ['Yujin Jeong', 'Arnas Uselis', 'Seong Joon Oh', 'Anna Rohrbach'], 'affiliations': ['TU Darmstadt & hessian.AI', 'T√ºbingen AI Center & University of T√ºbingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17955.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark', '#diffusion'], 'emoji': 'üß†', 'ru': {'title': '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏: –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Å —É—Å–ª–æ–≤–∏—è–º–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –≤—ã—è–≤–ª—è–µ—Ç –∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –û–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≤–∞–∂–Ω–æ—Å—Ç—å –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ (SD 1.5, 2.0 –∏ 3-m) –Ω–∞ 10 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –±–æ–ª–µ–µ —á–µ–º 30 –∑–∞–¥–∞—á–∞—Ö. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ Self-Bench –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –¥–æ–º–µ–Ω–∞.'}, 'en': {'title': 'Unlocking Compositional Understanding in Diffusion Classifiers', 'desc': "This paper investigates how diffusion classifiers perform on various datasets and tasks, focusing on their ability to understand complex compositions. It highlights the importance of the specific domains of datasets and how they affect the classifiers' performance. The study introduces a new benchmark called Self-Bench to evaluate the classifiers using images generated by diffusion models. Additionally, it examines the significance of timestep weighting and its relationship with domain sensitivity, particularly in the latest diffusion model SD3-m."}, 'zh': {'title': 'Êâ©Êï£ÂàÜÁ±ªÂô®ÔºöÁªÑÂêàÁêÜËß£ÁöÑÊñ∞ËßÜËßí', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÊâ©Êï£ÂàÜÁ±ªÂô®Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíå‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊè≠Á§∫‰∫ÜÂÆÉ‰ª¨ÂØπÁªÑÂêàÁêÜËß£ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁâπÂÆöÈ¢ÜÂüüÁöÑÊÄßËÉΩÂΩ±ÂìçÂíåÊó∂Èó¥Ê≠•Âä†ÊùÉÁöÑÈáçË¶ÅÊÄßÂØπÊ®°ÂûãÁöÑÊàêÂäüËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫Ü‰∏âÁßçÊâ©Êï£Ê®°ÂûãÂú®ÂçÅ‰∏™Êï∞ÊçÆÈõÜÂíå‰∏âÂçÅÂ§ö‰∏™‰ªªÂä°‰∏äÁöÑÂà§Âà´ËÉΩÂäõÔºåÈ¶ñÊ¨°ÂºïÂÖ•‰∫ÜËá™ÊàëÂü∫ÂáÜÊµãËØï‰ª•ÈöîÁ¶ªÈ¢ÜÂüüÊïàÂ∫î„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊâ©Êï£ÂàÜÁ±ªÂô®Âú®ÁªÑÂêàÊÄßÁêÜËß£ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂÖ∂ÊïàÊûúÂèóÁâπÂÆöÊù°‰ª∂ÁöÑÂΩ±Âìç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15692', 'title': 'Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities', 'url': 'https://huggingface.co/papers/2505.15692', 'abstract': 'A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model\'s output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO\'s potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.', 'score': 12, 'issue_id': 3946, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '5b1bedaf6be49ffa', 'authors': ['Jinyang Wu', 'Chonghua Liao', 'Mingkuan Feng', 'Shuai Zhang', 'Zhengqi Wen', 'Pengpeng Shao', 'Huazhe Xu', 'Jianhua Tao'], 'affiliations': ['Beijing National Research Center for Information Science and Technology', 'Department of Automation, Tsinghua University', 'Institution for Interdisciplinary Information Sciences, Tsinghua University', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15692.jpg', 'data': {'categories': ['#reasoning', '#training', '#interpretability', '#rl', '#rlhf'], 'emoji': 'üß†', 'ru': {'title': 'TAPO: –£—Å–∏–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏–º–∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏', 'desc': 'TAPO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, TAPO –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –º–µ–∂–¥—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ TAPO –Ω–∞–¥ GRPO –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–Ω–∞–ª–∏–∑ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª, —á—Ç–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–æ—â–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å—é –≤—ã–≤–æ–¥–æ–≤.'}, 'en': {'title': 'Enhancing Reinforcement Learning with Thought Patterns', 'desc': 'The paper introduces TAPO (Thought-Augmented Policy Optimization), a new reinforcement learning framework that enhances model performance by integrating external guidance. Traditional RL methods often limit exploration by focusing solely on reward-maximizing paths, which restricts the reasoning capabilities of the models. TAPO addresses this issue by incorporating structured thought patterns during training, allowing for a better balance between internal exploration and external guidance. Experimental results demonstrate that TAPO significantly outperforms existing methods, leading to improved reasoning models that are more explainable and readable.'}, 'zh': {'title': 'TAPOÔºöÂ¢ûÂº∫Êé¢Á¥¢‰∏éÊé®ÁêÜÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂TAPOÔºåÈÄöËøáÊï¥ÂêàÂ§ñÈÉ®ÊåáÂØºÊù•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÂíåÊé¢Á¥¢ËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂæÄÂæÄÂè™ÂÖ≥Ê≥®ÊúÄÂ§ßÂåñÂ•ñÂä±Ë∑ØÂæÑÔºåÁº∫‰πèÂ§ñÈÉ®Áü•ËØÜÁöÑÂºïÂÖ•ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõ„ÄÇTAPOÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈÄÇÂ∫îÊÄßÂú∞Êï¥ÂêàÁªìÊûÑÂåñÊÄùÁª¥ÔºåÂπ≥Ë°°‰∫ÜÊ®°ÂûãÂÜÖÈÉ®ÁöÑÊé¢Á¥¢‰∏éÂ§ñÈÉ®ÊåáÂØºÁöÑÂà©Áî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTAPOÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂπøÊ≥õÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16479', 'title': 'Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration', 'url': 'https://huggingface.co/papers/2505.16479', 'abstract': 'A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple weather conditions often coexist in the real world alongside various lighting effects at night. This paper first explores the challenging multi-weather nighttime image restoration task, where various types of weather degradations are intertwined with flare effects. To support the research, we contribute the AllWeatherNight dataset, featuring large-scale high-quality nighttime images with diverse compositional degradations, synthesized using our introduced illumination-aware degradation generation. Moreover, we present ClearNight, a unified nighttime image restoration framework, which effectively removes complex degradations in one go. Specifically, ClearNight extracts Retinex-based dual priors and explicitly guides the network to focus on uneven illumination regions and intrinsic texture contents respectively, thereby enhancing restoration effectiveness in nighttime scenarios. In order to better represent the common and unique characters of multiple weather degradations, we introduce a weather-aware dynamic specific-commonality collaboration method, which identifies weather degradations and adaptively selects optimal candidate units associated with specific weather types. Our ClearNight achieves state-of-the-art performance on both synthetic and real-world images. Comprehensive ablation experiments validate the necessity of AllWeatherNight dataset as well as the effectiveness of ClearNight. Project page: https://henlyta.github.io/ClearNight/mainpage.html', 'score': 11, 'issue_id': 3950, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'e02bace9da76256b', 'authors': ['Yuetong Liu', 'Yunqiu Xu', 'Yang Wei', 'Xiuli Bi', 'Bin Xiao'], 'affiliations': ['Chongqing University of Posts and Telecommunications', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16479.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': 'üåô', 'ru': {'title': '–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç AllWeatherNight —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –Ω–æ—á–Ω—ã–º–∏ —Å–Ω–∏–º–∫–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ClearNight –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–æ–π–Ω—ã–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–∏–∏ –†–µ—Ç–∏–Ω–µ–∫—Å–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∏—Å–∫–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∫ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö, —Ç–∞–∫ –∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'ClearNight: Mastering Nighttime Image Restoration Across Weather Conditions', 'desc': 'This paper addresses the challenge of restoring nighttime images that are affected by various weather conditions and lighting effects. It introduces the AllWeatherNight dataset, which contains high-quality nighttime images with different types of weather degradations. The authors propose a framework called ClearNight that utilizes Retinex-based dual priors to enhance image restoration by focusing on illumination and texture. Additionally, a weather-aware collaboration method is introduced to adaptively handle different weather conditions, resulting in state-of-the-art performance in image restoration tasks.'}, 'zh': {'title': 'Áªü‰∏ÄÊ°ÜÊû∂ÔºåÊ∏ÖÊô∞Â§úÊôØ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Â§öÁßçÊÅ∂Âä£Â§©Ê∞îÊù°‰ª∂‰∏ãÊÅ¢Â§çÂ§úÈó¥ÂõæÂÉèÁöÑÊåëÊàòÊÄß‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAllWeatherNightÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Â§öÊ†∑ÂåñÁöÑÂ§úÈó¥ÂõæÂÉèÔºåÂ∏ÆÂä©Á†îÁ©∂‰∏çÂêåÂ§©Ê∞îÈÄÄÂåñÁöÑÂΩ±Âìç„ÄÇClearNightÊòØÊàë‰ª¨ÊèêÂá∫ÁöÑÁªü‰∏ÄÂ§úÈó¥ÂõæÂÉèÊÅ¢Â§çÊ°ÜÊû∂ÔºåËÉΩÂ§üÊúâÊïàÂéªÈô§Â§çÊùÇÁöÑÈÄÄÂåñÁé∞Ë±°„ÄÇÈÄöËøáÂºïÂÖ•Â§©Ê∞îÊÑüÁü•ÁöÑÂä®ÊÄÅÁâπÂÆö-ÂÖ±ÊÄßÂçè‰ΩúÊñπÊ≥ïÔºåClearNightÂú®ÂêàÊàêÂíåÁúüÂÆûÂõæÂÉè‰∏äÂùáÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16134', 'title': 'Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.16134', 'abstract': 'Large language models exhibit positional bias -- systematic neglect of information at specific context positions -- yet its interplay with linguistic diversity remains poorly understood. We present a cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi, Vietnamese), examining how positional bias interacts with model uncertainty, syntax, and prompting. Key findings: (1) Positional bias is model-driven, with language-specific variations -- Qwen2.5-7B favors late positions, challenging assumptions of early-token bias; (2) Explicit positional guidance (e.g., correct context is at position X) reduces accuracy across languages, undermining prompt-engineering practices; (3) Aligning context with positional bias increases entropy, yet minimal entropy does not predict accuracy. (4) We further uncover that LLMs differently impose dominant word order in free-word-order languages like Hindi.', 'score': 11, 'issue_id': 3957, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '5849eec94fecbfa3', 'authors': ['Menschikov Mikhail', 'Alexander Kharitonov', 'Maiia Kotyga', 'Vadim Porvatov', 'Anna Zhukovskaya', 'David Kagramanyan', 'Egor Shvetsov', 'Evgeny Burnaev'], 'affiliations': ['AIRI, Moscow, Russia', 'HSE University, Moscow, Russia', 'ITMO, Saint-Petersburg, Russia', 'Lomonosov MSU, Moscow, Russia', 'MIPT, Moscow, Russia', 'Sber, Moscow, Russia', 'Skoltech, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2505.16134.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#multilingual'], 'emoji': 'üß†', 'ru': {'title': '–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö: –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ø—è—Ç–∏ —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏, –∞ –Ω–µ –æ—Ç —è–∑—ã–∫–∞, –ø—Ä–∏—á–µ–º –º–æ–¥–µ–ª—å Qwen2.5-7B –æ—Ç–¥–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø–æ–∑–¥–Ω–∏–º –ø–æ–∑–∏—Ü–∏—è–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —è–≤–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è —Å–Ω–∏–∂–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º —Å–º–µ—â–µ–Ω–∏–µ–º —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é. –¢–∞–∫–∂–µ –±—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –Ω–∞–≤—è–∑—ã–≤–∞—é—Ç –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –≤ —è–∑—ã–∫–∞—Ö —Å–æ —Å–≤–æ–±–æ–¥–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º —Å–ª–æ–≤.'}, 'en': {'title': 'Understanding Positional Bias in Multilingual Contexts', 'desc': 'This paper investigates how large language models (LLMs) show positional bias, which means they often ignore information based on where it appears in a sentence. The study looks at five different languages to see how this bias interacts with factors like model uncertainty and syntax. It finds that the bias varies by language and that giving explicit positional hints can actually lower accuracy. Additionally, the research reveals that LLMs handle word order differently in languages that allow more flexibility, such as Hindi.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰ΩçÁΩÆÂÅèËßÅ‰∏éËØ≠Ë®ÄÂ§öÊ†∑ÊÄßÁöÑÂÖ≥Á≥ª', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰ΩçÁΩÆÂÅèËßÅÔºåÂç≥Âú®ÁâπÂÆö‰∏ä‰∏ãÊñá‰ΩçÁΩÆ‰∏äÁ≥ªÁªüÊÄßÂøΩËßÜ‰ø°ÊÅØÁöÑÁé∞Ë±°„ÄÇÁ†îÁ©∂Ê∂µÁõñ‰∫Ü‰∫îÁßç‰∏çÂêåÁ±ªÂûãÁöÑËØ≠Ë®ÄÔºàËã±ËØ≠„ÄÅ‰øÑËØ≠„ÄÅÂæ∑ËØ≠„ÄÅÂç∞Âú∞ËØ≠ÂíåË∂äÂçóËØ≠ÔºâÔºåÊé¢ËÆ®‰∫Ü‰ΩçÁΩÆÂÅèËßÅ‰∏éÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂè•Ê≥ïÂíåÊèêÁ§∫‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇ‰∏ªË¶ÅÂèëÁé∞ÂåÖÊã¨Ôºö‰ΩçÁΩÆÂÅèËßÅÊòØÁî±Ê®°ÂûãÈ©±Âä®ÁöÑÔºåÂπ∂‰∏îÂú®‰∏çÂêåËØ≠Ë®Ä‰∏≠Â≠òÂú®ÁâπÂÆöÁöÑÂèòÂåñÔºõÊòéÁ°ÆÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÊåáÂØº‰ºöÈôç‰ΩéÂêÑËØ≠Ë®ÄÁöÑÂáÜÁ°ÆÊÄßÔºåÊåëÊàò‰∫ÜÊèêÁ§∫Â∑•Á®ãÁöÑÂÅáËÆæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13508', 'title': 'Time-R1: Towards Comprehensive Temporal Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2505.13508', 'abstract': 'A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce Time-R1, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a reinforcement learning (RL) curriculum driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release Time-Bench, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of Time-R1 checkpoints.', 'score': 11, 'issue_id': 3951, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'bd70d09511fa2ae4', 'authors': ['Zijia Liu', 'Peixuan Han', 'Haofei Yu', 'Haoru Li', 'Jiaxuan You'], 'affiliations': ['Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.13508.jpg', 'data': {'categories': ['#rl', '#optimization', '#dataset', '#training', '#benchmark', '#reasoning'], 'emoji': '‚è≥', 'ru': {'title': '–ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º —á—É–≤—Å—Ç–≤–æ–º –≤—Ä–µ–º–µ–Ω–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Time-R1 - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, Time-R1 –ø–æ—ç—Ç–∞–ø–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Time-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥—É—â–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Time-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò-—Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Empowering Smaller Models with Temporal Intelligence', 'desc': 'The paper introduces Time-R1, a new framework that enhances moderate-sized large language models (LLMs) with advanced temporal reasoning capabilities. It utilizes a reinforcement learning curriculum to develop skills in understanding past events, predicting future occurrences, and generating creative scenarios. Unlike existing methods that focus on isolated temporal tasks, Time-R1 enables comprehensive temporal abilities, allowing the model to generalize well beyond its training data. Experimental results show that Time-R1 outperforms much larger models in challenging benchmarks, demonstrating the effectiveness of its structured approach to temporal learning.'}, 'zh': {'title': 'Time-R1ÔºöÂ∞èÊ®°ÂûãÁöÑÊó∂Èó¥Êô∫ËÉΩÈù©ÂëΩ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂Time-R1ÔºåÊó®Âú®Â¢ûÂº∫‰∏≠Á≠âËßÑÊ®°ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Êó∂Èó¥Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËØæÁ®ãÔºåTime-R1ËÉΩÂ§üÁêÜËß£ÂéÜÂè≤‰∫ã‰ª∂„ÄÅÈ¢ÑÊµãÊú™Êù•‰∫ã‰ª∂ÔºåÂπ∂ÁîüÊàêÂàõÊÑèÂú∫ÊôØÔºåË∂ÖË∂ä‰∫ÜÊõ¥Â§ßÊ®°ÂûãÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∏âÈò∂ÊÆµÁöÑÂèëÂ±ïË∑ØÂæÑÔºåÈÄêÊ≠•ÊûÑÂª∫Êó∂Èó¥ÁêÜËß£„ÄÅÊú™Êù•‰∫ã‰ª∂È¢ÑÊµãÂíåÂàõÊÑèÁîüÊàêËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTime-R1Âú®Êú™Êù•‰∫ã‰ª∂È¢ÑÊµãÂíåÂàõÊÑèÂú∫ÊôØÁîüÊàêÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é200ÂÄç‰ª•‰∏äÁöÑÂ§ßÂûãÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16483', 'title': 'Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.16483', 'abstract': 'CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.', 'score': 10, 'issue_id': 3945, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '3bd78fedbb109d9a', 'authors': ['Shuzheng Si', 'Haozhe Zhao', 'Cheng Gao', 'Yuzhuo Bai', 'Zhitong Wang', 'Bofei Gao', 'Kangyang Luo', 'Wenhao Li', 'Yufei Huang', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun'], 'affiliations': ['DeepLang AI', 'Peking University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.16483.jpg', 'data': {'categories': ['#rl', '#dataset', '#rlhf', '#optimization', '#training', '#synthetic'], 'emoji': 'üõ∂', 'ru': {'title': '–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏: CANOE —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏', 'desc': 'CANOE - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö –ø–∞—Ä–∞—Ö –∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É Dual-GRPO. CANOE –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ CANOE –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ 11 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ —Å–∞–º—ã–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ GPT-4.'}, 'en': {'title': 'Enhancing LLM Faithfulness with CANOE and Synthetic Data', 'desc': 'The paper presents CANOE, a framework designed to enhance the faithfulness of large language models (LLMs) in generating text. It achieves this by creating synthetic question-answering (QA) data, which serves as high-quality training material without requiring human annotations. The framework employs a novel reinforcement learning approach called Dual-GRPO, which uses rule-based rewards to optimize both short-form and long-form text generation. Experimental results demonstrate that CANOE significantly improves LLM performance across various tasks, surpassing even state-of-the-art models like GPT-4o.'}, 'zh': {'title': 'CANOEÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØ‰ø°Â∫¶', 'desc': 'CANOEÊòØ‰∏Ä‰∏™Á≥ªÁªüÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂèØ‰ø°Â∫¶ÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂêàÊàêÁü≠ÂΩ¢ÂºèÈóÆÁ≠îÔºàQAÔºâÊï∞ÊçÆÔºåÊûÑÂª∫È´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®ÂèåÈáçGRPOÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊù•‰ºòÂåñÁîüÊàêËøáÁ®ã„ÄÇÂèåÈáçGRPOÁªìÂêà‰∫ÜÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÁ°Æ‰øùÂú®Áü≠ÂΩ¢ÂºèÂíåÈïøÂΩ¢ÂºèÁîüÊàê‰ªªÂä°‰∏≠ÈÉΩËÉΩÊúâÊïàÊèêÂçáÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCANOEÂú®11‰∏™‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜLLMsÁöÑÂèØ‰ø°Â∫¶ÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ¶ÇGPT-4oÂíåOpenAI o1„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17417', 'title': 'Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages', 'url': 'https://huggingface.co/papers/2505.17417', 'abstract': 'The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models to understand and execute spoken commands. Generating high-quality synthetic speech requires a good text-to-speech (TTS) model, which may not be available to low resource languages. Our novel approach addresses this challenge by halting synthesis at the semantic representation level, bypassing the need for TTS. We achieve this by aligning synthetic semantic representations with the pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference. This simplified training process is a promising approach to building voice assistant for low-resource languages.', 'score': 9, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '9d72b20aca0789ee', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Huy Hoang Ha', 'Tuan Le Duc Anh', 'Shreyas Gopal', 'Yue Heng Yeo', 'Warren Keng Hoong Low', 'Eng Siong Chng', 'Jia Qi Yip'], 'affiliations': ['CCDS, Nanyang Technological University, Singapore', 'Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.17417.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#data', '#synthetic', '#training', '#audio'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–¥–∫–∏—Ö —è–∑—ã–∫–æ–≤: –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ TTS', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –æ–±–æ–π—Ç–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ text-to-speech, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è —Å–∏–Ω—Ç–µ–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º —ç–Ω–∫–æ–¥–µ—Ä–æ–º Whisper. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å —É—Å—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ.'}, 'en': {'title': 'Empowering Voice Assistants for Low-Resource Languages', 'desc': 'This paper addresses the challenge of training voice assistants in low-resource languages, where there is a lack of speech instruction data. It proposes a novel method that generates synthetic speech by stopping at the semantic representation level, eliminating the need for a text-to-speech (TTS) model. By aligning these semantic representations with the pre-trained Whisper encoder, the approach allows for fine-tuning large language models (LLMs) on text instructions while still being able to process spoken commands. This method simplifies the training process and enhances the development of voice assistants for languages with limited resources.'}, 'zh': {'title': '‰∏∫‰ΩéËµÑÊ∫êËØ≠Ë®ÄÊûÑÂª∫ËØ≠Èü≥Âä©ÊâãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏∫ËØ≠Èü≥Âä©ÊâãËÆ≠ÁªÉÊâÄÈúÄÁöÑËØ≠Èü≥Êåá‰ª§Êï∞ÊçÆÁöÑ‰∏çË∂≥ÈóÆÈ¢ò„ÄÇÂ∞ΩÁÆ°ËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆ‰∏∞ÂØåÔºå‰ΩÜËØ≠Èü≥Êåá‰ª§Êï∞ÊçÆÂç¥Áõ∏ÂØπÁ®ÄÁº∫ÔºåËøôÂØπÊ®°ÂûãÁêÜËß£ÂíåÊâßË°åÂè£Â§¥ÂëΩ‰ª§Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂú®ËØ≠‰πâË°®Á§∫Â±ÇÈù¢ÂÅúÊ≠¢ÂêàÊàêÔºåÈÅøÂÖç‰∫ÜÂØπÊñáÊú¨Âà∞ËØ≠Èü≥ÔºàTTSÔºâÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂêàÊàêÁöÑËØ≠‰πâË°®Á§∫‰∏éÈ¢ÑËÆ≠ÁªÉÁöÑWhisperÁºñÁ†ÅÂô®ÂØπÈΩêÔºå‰ΩøÂæóÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÉΩÂ§üÂú®ÊñáÊú¨Êåá‰ª§‰∏äËøõË°åÂæÆË∞ÉÔºåÂêåÊó∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ÁêÜËß£Âè£Â§¥Êåá‰ª§„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16770', 'title': 'RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs', 'url': 'https://huggingface.co/papers/2505.16770', 'abstract': "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv", 'score': 9, 'issue_id': 3951, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '3e6a52b7ac905c1d', 'authors': ['Meng-Hao Guo', 'Xuanyu Chu', 'Qianrui Yang', 'Zhe-Han Mo', 'Yiqing Shen', 'Pei-lin Li', 'Xinjie Lin', 'Jinnian Zhang', 'Xin-Sheng Chen', 'Yi Zhang', 'Kiyohiro Nakayama', 'Zhengyang Geng', 'Houwen Peng', 'Han Hu', 'Shi-Min Hu'], 'affiliations': ['Carnegie Mellon University', 'Stanford University', 'Tencent Hunyuan X', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16770.jpg', 'data': {'categories': ['#games', '#multimodal', '#benchmark', '#open_source', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –∑—Ä–µ–Ω–∏—è', 'desc': 'RBench-V - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 803 –≤–æ–ø—Ä–æ—Å–∞ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ñ–∏–∑–∏–∫–µ, –ø–æ–¥—Å—á–µ—Ç—É –∏ –∏–≥—Ä–∞–º, —Ç—Ä–µ–±—É—é—â–∏–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ª–∏–Ω–∏–π. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ o3 –∏ Gemini 2.5 Pro, –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ 25.8% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 82.3% —É –ª—é–¥–µ–π. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–º –º–æ–¥–µ–ª—è–º —Å–ª–æ–∂–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'RBench-V: Bridging the Gap in Multi-Modal Reasoning', 'desc': 'The paper introduces RBench-V, a new benchmark designed to evaluate the reasoning capabilities of multi-modal models, particularly in tasks that require visual thinking and manipulation. It highlights that existing benchmarks often overlook the importance of assessing multi-modal outputs, focusing instead on inputs and text-only reasoning. RBench-V includes 803 carefully selected questions that require models to perform tasks like image generation and auxiliary line construction. The evaluation shows that even the top-performing models struggle significantly, achieving only 25.8% accuracy compared to a human benchmark of 82.3%, indicating a gap in current multi-modal reasoning abilities.'}, 'zh': {'title': 'ËØÑ‰º∞Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜRBench-V', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫RBench-VÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°ÂûãÂú®ËßÜËßâÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§ÑÁêÜÂõæÂÉèÂíåÊñáÊú¨Êó∂Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Â§öÊ®°ÊÄÅËæìÂá∫ÁöÑÊé®ÁêÜ‰∏ä‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇRBench-VÈÄöËøá803‰∏™Ê∂µÁõñÊï∞Â≠¶„ÄÅÁâ©ÁêÜ„ÄÅËÆ°Êï∞ÂíåÊ∏∏ÊàèÁöÑÈóÆÈ¢òÔºå‰∏ìÊ≥®‰∫éÈúÄË¶ÅÂõæÂÉèÊìç‰ΩúÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØË°®Áé∞ÊúÄÂ•ΩÁöÑÊ®°Âûão3ÔºåÂÖ∂ÂáÜÁ°ÆÁéá‰πü‰ªÖ‰∏∫25.8%ÔºåËøú‰Ωé‰∫é‰∫∫Á±ªÁöÑ82.3%ÔºåË°®ÊòéÁé∞ÊúâÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14146', 'title': "s3: You Don't Need That Much Data to Train a Search Agent via RL", 'url': 'https://huggingface.co/papers/2505.14146', 'abstract': 'A lightweight, model-agnostic framework decouples the retrieval and generation processes in RAG systems, enhancing performance with minimal training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.', 'score': 9, 'issue_id': 3953, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '75be81201c7c18e4', 'authors': ['Pengcheng Jiang', 'Xueqiang Xu', 'Jiacheng Lin', 'Jinfeng Xiao', 'Zifeng Wang', 'Jimeng Sun', 'Jiawei Han'], 'affiliations': ['Amazon', 'University of Illinois'], 'pdf_title_img': 'assets/pdf/title_img/2505.14146.jpg', 'data': {'categories': ['#healthcare', '#rag', '#rl', '#dataset', '#benchmark', '#reasoning', '#training', '#optimization'], 'emoji': 'üîç', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è RAG —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º', 'desc': '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è, –º–æ–¥–µ–ª—å–Ω–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ s3 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö. –û–Ω–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ–±—É—á–∞—è –ø–æ–∏—Å–∫–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º RAG. s3 —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ 2400 –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –≤ 70 —Ä–∞–∑ –±–æ–ª—å—à–µ–º –æ–±—ä–µ–º–µ –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —à–µ—Å—Ç–∏ –æ–±—â–∏—Ö –∏ –ø—è—Ç–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.'}, 'en': {'title': 'Decoupling Retrieval and Generation for Enhanced Performance', 'desc': "This paper introduces a new framework called s3 that separates the retrieval and generation processes in retrieval-augmented generation (RAG) systems. By doing this, it allows for better performance with significantly less training data compared to traditional methods. The framework uses a novel reward system, Gain Beyond RAG, to enhance the searcher's effectiveness without needing to fine-tune the entire language model. As a result, s3 achieves superior results on various question-answering benchmarks while requiring only a fraction of the training samples."}, 'zh': {'title': 'Ëß£ËÄ¶Ê£ÄÁ¥¢‰∏éÁîüÊàêÔºåÊèêÂçáRAGÁ≥ªÁªüÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑ„ÄÅ‰∏éÊ®°ÂûãÊó†ÂÖ≥ÁöÑÊ°ÜÊû∂s3ÔºåÊó®Âú®Ëß£ËÄ¶Ê£ÄÁ¥¢ÂíåÁîüÊàêËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òRAGÁ≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰ΩøÁî®Ë∂ÖË∂äRAGÂ•ñÂä±ÁöÑÂ¢ûÁõäÊù•ËÆ≠ÁªÉÊ£ÄÁ¥¢Âô®Ôºå‰∏ìÊ≥®‰∫éÁîüÊàêÂáÜÁ°ÆÊÄßÁöÑÊèêÂçá„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºås3Âè™ÈúÄ2.4ÂçÉ‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåÂ∞±ËÉΩË∂ÖË∂äÂü∫‰∫é70ÂÄç‰ª•‰∏äÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂü∫Á∫øÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºås3Âú®ÂÖ≠‰∏™ÈÄöÁî®ÈóÆÁ≠îÂíå‰∫î‰∏™ÂåªÂ≠¶ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂßãÁªàÊèê‰æõÊõ¥Âº∫ÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17826', 'title': 'Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2505.17826', 'abstract': 'Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.', 'score': 8, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '6f6fdf1b20859c44', 'authors': ['Xuchen Pan', 'Yanxi Chen', 'Yushuo Chen', 'Yuchang Sun', 'Daoyuan Chen', 'Wenhao Zhang', 'Yuexiang Xie', 'Yilun Huang', 'Yilei Zhang', 'Dawei Gao', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['alibaba-inc.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.17826.jpg', 'data': {'categories': ['#rl', '#rlhf', '#agi', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': 'Trinity-RFT: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'Trinity-RFT - —ç—Ç–æ –≥–∏–±–∫–∞—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ/–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ, on-policy/off-policy –∏ –æ–Ω–ª–∞–π–Ω/–æ—Ñ–ª–∞–π–Ω –ø–æ–¥—Ö–æ–¥—ã. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Trinity-RFT —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —è–¥—Ä–∞ RFT, –º–æ–¥—É–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Empowering Language Models with Flexible Reinforcement Fine-Tuning', 'desc': 'Trinity-RFT is a versatile framework designed for reinforcement fine-tuning (RFT) of large language models. It features a decoupled architecture that supports various modes of RFT, including synchronous and asynchronous, as well as on-policy and off-policy approaches. The framework ensures efficient and robust interactions between agents and environments, while also providing optimized data pipelines for RFT tasks. This makes Trinity-RFT adaptable to a wide range of applications, serving as a comprehensive platform for exploring advanced reinforcement learning techniques.'}, 'zh': {'title': 'Trinity-RFTÔºöÁÅµÊ¥ªÁöÑÂº∫ÂåñÂæÆË∞ÉÊ°ÜÊû∂', 'desc': 'Trinity-RFTÊòØ‰∏Ä‰∏™ÁÅµÊ¥ª‰∏îÂèØÊâ©Â±ïÁöÑÊ°ÜÊû∂Ôºå‰∏ìÈó®Áî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫ÂåñÂæÆË∞É„ÄÇÂÆÉÈááÁî®Ëß£ËÄ¶ËÆæËÆ°ÔºåÂåÖÂê´‰∏Ä‰∏™RFTÊ†∏ÂøÉÔºåËÉΩÂ§üÁªü‰∏ÄÂíåÊ¶ÇÊã¨ÂêåÊ≠•/ÂºÇÊ≠•„ÄÅÂú®Á∫ø/Á¶ªÁ∫øÁ≠âÂ§öÁßçÂº∫ÂåñÂæÆË∞ÉÊ®°Âºè„ÄÇËØ•Ê°ÜÊû∂ÊîØÊåÅÈ´òÊïà‰∏îÁ®≥ÂÅ•ÁöÑÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫íÔºåÂπ∂‰ºòÂåñ‰∫ÜÊï∞ÊçÆÁÆ°ÈÅì‰ª•ÈÄÇÂ∫îÂº∫ÂåñÂæÆË∞ÉÁöÑÈúÄÊ±Ç„ÄÇTrinity-RFTÊòì‰∫éÈÄÇÂ∫î‰∏çÂêåÁöÑÂ∫îÁî®Âú∫ÊôØÔºåÊòØÊé¢Á¥¢ÂÖàËøõÂº∫ÂåñÂ≠¶‰π†ËåÉÂºèÁöÑÁªü‰∏ÄÂπ≥Âè∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17295', 'title': 'ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic\n  Systems', 'url': 'https://huggingface.co/papers/2505.17295', 'abstract': 'The ScanBot dataset, focusing on instruction-conditioned high-precision robotic surface scanning, showcases challenges for vision-language action models in achieving precise scanning trajectories under real-world constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ScanBot, a novel dataset designed for instruction-conditioned, high-precision surface scanning in robotic systems. In contrast to existing robot learning datasets that focus on coarse tasks such as grasping, navigation, or dialogue, ScanBot targets the high-precision demands of industrial laser scanning, where sub-millimeter path continuity and parameter stability are critical. The dataset covers laser scanning trajectories executed by a robot across 12 diverse objects and 6 task types, including full-surface scans, geometry-focused regions, spatially referenced parts, functionally relevant structures, defect inspection, and comparative analysis. Each scan is guided by natural language instructions and paired with synchronized RGB, depth, and laser profiles, as well as robot pose and joint states. Despite recent progress, existing vision-language action (VLA) models still fail to generate stable scanning trajectories under fine-grained instructions and real-world precision demands. To investigate this limitation, we benchmark a range of multimodal large language models (MLLMs) across the full perception-planning-execution loop, revealing persistent challenges in instruction-following under realistic constraints.', 'score': 8, 'issue_id': 3960, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '212d967802dd2390', 'authors': ['Zhiling Chen', 'Yang Zhang', 'Fardin Jalil Piran', 'Qianyu Zhou', 'Jiong Tang', 'Farhad Imani'], 'affiliations': ['University of Connecticut'], 'pdf_title_img': 'assets/pdf/title_img/2505.17295.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#robotics', '#multimodal'], 'emoji': 'ü§ñ', 'ru': {'title': 'ScanBot: –≤—ã–∑–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ScanBot –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–º—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞–±–æ—Ä –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ª–∞–∑–µ—Ä–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è 12 –æ–±—ä–µ–∫—Ç–æ–≤ —Å 6 —Ç–∏–ø–∞–º–∏ –∑–∞–¥–∞—á, —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ–º—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠—Ç–æ –≤—ã—è–≤–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.'}, 'en': {'title': 'ScanBot: Precision Scanning Meets Natural Language Instructions', 'desc': 'The ScanBot dataset is a new resource aimed at improving robotic surface scanning by using natural language instructions. It focuses on high-precision tasks that require robots to follow detailed scanning paths with sub-millimeter accuracy. The dataset includes various scanning tasks across different objects, providing rich multimodal data such as RGB images, depth information, and laser profiles. Despite advancements in vision-language action models, the study shows that these models struggle to produce stable scanning trajectories when faced with complex, real-world instructions.'}, 'zh': {'title': 'È´òÁ≤æÂ∫¶Êú∫Âô®‰∫∫Êâ´ÊèèÁöÑÊñ∞ÊåëÊàò', 'desc': 'ScanBotÊï∞ÊçÆÈõÜ‰∏ìÊ≥®‰∫éÂü∫‰∫éÊåá‰ª§ÁöÑÈ´òÁ≤æÂ∫¶Êú∫Âô®‰∫∫Ë°®Èù¢Êâ´ÊèèÔºåÂ±ïÁ§∫‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®Áé∞ÂÆûÁ∫¶Êùü‰∏ãÂÆûÁé∞Á≤æÁ°ÆÊâ´ÊèèËΩ®ËøπÁöÑÊåëÊàò„ÄÇ‰∏éÁé∞ÊúâÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†Êï∞ÊçÆÈõÜ‰∏çÂêåÔºåScanBotÈíàÂØπÂ∑•‰∏öÊøÄÂÖâÊâ´ÊèèÁöÑÈ´òÁ≤æÂ∫¶ÈúÄÊ±ÇÔºåÂº∫Ë∞É‰∫öÊØ´Á±≥Á∫ßÁöÑË∑ØÂæÑËøûÁª≠ÊÄßÂíåÂèÇÊï∞Á®≥ÂÆöÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÊ∂µÁõñ‰∫ÜÊú∫Âô®‰∫∫Âú®12Áßç‰∏çÂêåÁâ©‰ΩìÂíå6Áßç‰ªªÂä°Á±ªÂûã‰∏ãÊâßË°åÁöÑÊøÄÂÖâÊâ´ÊèèËΩ®ËøπÔºåÂåÖÊã¨ÂÖ®Ë°®Èù¢Êâ´Êèè„ÄÅÂá†‰ΩïÈáçÁÇπÂå∫Âüü„ÄÅÁ©∫Èó¥ÂèÇËÄÉÈÉ®ÂàÜÁ≠â„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâËøõÂ±ïÔºåÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®ÁªÜÁ≤íÂ∫¶Êåá‰ª§ÂíåÁé∞ÂÆûÁ≤æÂ∫¶Ë¶ÅÊ±Ç‰∏ã‰ªçÁÑ∂Êó†Ê≥ïÁîüÊàêÁ®≥ÂÆöÁöÑÊâ´ÊèèËΩ®Ëøπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15389', 'title': 'Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study', 'url': 'https://huggingface.co/papers/2505.15389', 'abstract': 'VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms.', 'score': 7, 'issue_id': 3945, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '102a2cdaf1ccf7d5', 'authors': ['DongGeon Lee', 'Joonwon Jang', 'Jihae Jeong', 'Hwanjo Yu'], 'affiliations': ['LG AI Research', 'POSTECH'], 'pdf_title_img': 'assets/pdf/title_img/2505.15389.jpg', 'data': {'categories': ['#security', '#ethics', '#benchmark', '#multimodal'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ú–µ–º—ã vs –ò–ò: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —É–≥—Ä–æ–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (VLM) –±–æ–ª–µ–µ —É—è–∑–≤–∏–º—ã –∫ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–º –º–µ–º–∞–º, —á–µ–º –∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MemeSafetyBench - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 50 430 –º–µ–º–æ–≤ —Å –≤—Ä–µ–¥–Ω—ã–º–∏ –∏ –±–µ–∑–æ–±–∏–¥–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ VLM. –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–∞—Å—Ç–∏—á–Ω–æ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫–∏, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –≤–∞–ª–∏–¥–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∏ —É—Å–∏–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è VLM.'}, 'en': {'title': 'Meme Vulnerability: A Call for Safer VLMs', 'desc': 'This paper investigates the safety of vision-language models (VLMs) when exposed to real-world meme images, which are often shared by users. The authors introduce a benchmark called MemeSafetyBench, consisting of over 50,000 instances of meme images paired with harmful and benign instructions. The study finds that VLMs are more susceptible to harmful prompts from memes compared to synthetic images, and while multi-turn interactions can offer some protection, vulnerabilities remain significant. The results emphasize the importance of realistic evaluations and the need for improved safety measures in VLMs.'}, 'zh': {'title': 'ÊÅ∂ÊêûÂõæÂÉèÂØπËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®Â®ÅËÉÅ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Èù¢ÂØπÁî®Êà∑ÂàÜ‰∫´ÁöÑÊÅ∂ÊêûÂõæÂÉèÊó∂ÁöÑÂÆâÂÖ®ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜMemeSafetyBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´50,430‰∏™ÂÆû‰æãÁöÑÂü∫ÂáÜÔºåÁªìÂêà‰∫ÜÁúüÂÆûÁöÑÊÅ∂ÊêûÂõæÂÉèÂíåÊúâÂÆ≥‰∏éÊó†ÂÆ≥ÁöÑÊåá‰ª§„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåVLMsÂØπÊÅ∂ÊêûÂõæÂÉèÁöÑÊúâÂÆ≥ÊèêÁ§∫ÊØîÂØπÂêàÊàêÂõæÂÉèÊõ¥ËÑÜÂº±Ôºå‰∏îÂ§öËΩÆÂØπËØùËôΩÁÑ∂Êèê‰æõ‰∫Ü‰∏ÄÂÆöÁöÑ‰øùÊä§Ôºå‰ΩÜ‰ªçÁÑ∂Â≠òÂú®ÊòæËëóÁöÑËÑÜÂº±ÊÄß„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜÈúÄË¶ÅËøõË°åÁîüÊÄÅÊúâÊïàÁöÑËØÑ‰º∞ÂíåÊõ¥Âº∫ÁöÑÂÆâÂÖ®Êú∫Âà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17063', 'title': 'Synthetic Data RL: Task Definition Is All You Need', 'url': 'https://huggingface.co/papers/2505.17063', 'abstract': 'Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.', 'score': 7, 'issue_id': 3947, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': 'a46b0456dc458ebe', 'authors': ['Yiduo Guo', 'Zhen Guo', 'Chuanwei Huang', 'Zi-Ang Wang', 'Zekai Zhang', 'Haofei Yu', 'Huishuai Zhang', 'Yikang Shen'], 'affiliations': ['MIT', 'MIT-IBM', 'Peking University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2505.17063.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#training', '#synthetic'], 'emoji': 'ü§ñ', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Synthetic Data RL, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –∏—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. Synthetic Data RL –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Reinforcement Learning with Synthetic Data: A Game Changer for Model Training!', 'desc': "Synthetic Data RL is a novel approach that enhances foundation models using reinforcement learning (RL) without the need for extensive human-labeled data. It generates synthetic question and answer pairs based on task definitions, allowing for effective model fine-tuning. The method adapts question difficulty according to the model's performance, optimizing the training process. Results show significant performance improvements on various benchmarks, demonstrating that this approach can achieve results comparable to traditional RL methods that rely on human data."}, 'zh': {'title': 'ÂêàÊàêÊï∞ÊçÆÂº∫ÂåñÂ≠¶‰π†ÔºöÈ´òÊïàÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂêàÊàêÊï∞ÊçÆÂº∫ÂåñÂ≠¶‰π†ÔºàSynthetic Data RLÔºâÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÈÄöËøá‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÊù•Â¢ûÂº∫Âü∫Á°ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÁîüÊàê‰∏é‰ªªÂä°ÂÆö‰πâÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°àÂØπÔºåÂπ∂Ê†πÊçÆÊ®°ÂûãÁöÑËß£ÂÜ≥ËÉΩÂäõË∞ÉÊï¥ÈóÆÈ¢òÁöÑÈöæÂ∫¶„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåSynthetic Data RLÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁîöËá≥Êé•Ëøë‰∫é‰ΩøÁî®ÂÖ®‰∫∫Á±ªÊ†áÊ≥®Êï∞ÊçÆÁöÑÂº∫ÂåñÂ≠¶‰π†Ê®°Âûã„ÄÇÊ≠§ÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂØπ‰∫∫Á±ªÊï∞ÊçÆÊ†áÊ≥®ÁöÑ‰æùËµñÔºå‰ΩøÂæóÊ®°ÂûãÈÄÇÂ∫îÂèòÂæóÊõ¥Âä†È´òÊïàÂíåÂèØÊâ©Â±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16270', 'title': 'Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning', 'url': 'https://huggingface.co/papers/2505.16270', 'abstract': "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.", 'score': 6, 'issue_id': 3945, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '242b4420fd3d9c4f', 'authors': ['Jiaru Zou', 'Yikun Ban', 'Zihao Li', 'Yunzhe Qi', 'Ruizhong Qiu', 'Ling Yang', 'Jingrui He'], 'affiliations': ['Princeton University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.16270.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#architecture', '#transfer_learning', '#training'], 'emoji': 'üöÄ', 'ru': {'title': 'Transformer Copilot: –£—á–∏–º—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer Copilot, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏-–≤—Ç–æ—Ä–æ–≥–æ –ø–∏–ª–æ—Ç–∞ (Copilot), –∫–æ—Ç–æ—Ä–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –ª–æ–≥–∏—Ç—ã –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ (Pilot) –Ω–∞ –æ—Å–Ω–æ–≤–µ –∂—É—Ä–Ω–∞–ª–∞ –æ—à–∏–±–æ–∫ (Mistake Log). –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –ø—Ä–æ—à–ª—ã—Ö –æ—à–∏–±–∫–∞—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —É—á–∞—Ç—Å—è –ª—é–¥–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 12 —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ 34.5% –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö.'}, 'en': {'title': 'Enhancing Language Models with Reflective Learning', 'desc': "The Transformer Copilot framework improves the performance of large language models by using a Copilot model that refines the Pilot's outputs based on a Mistake Log. This Mistake Log tracks the model's errors during fine-tuning, allowing the Copilot to learn from these mistakes, similar to how humans learn. The framework includes a novel design for the Copilot, a joint training approach where both models learn together, and a fused inference method that enhances the Pilot's predictions. Experiments show that this approach can boost performance by up to 34.5% across various tasks with minimal additional computational cost."}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÂâØÈ©æÈ©∂Ê°ÜÊû∂', 'desc': 'Transformer CopilotÊ°ÜÊû∂ÈÄöËøá‰∏Ä‰∏™ÂâØÈ©æÈ©∂Ê®°ÂûãÊù•ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËøô‰∏™ÂâØÈ©æÈ©∂Ê®°ÂûãÊ†πÊçÆÈîôËØØÊó•ÂøóÊù•‰ºòÂåñ‰∏ªÊ®°ÂûãÁöÑËæìÂá∫Ôºå‰ªéËÄåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊåÅÁª≠ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈîôËØØÊó•ÂøóÁöÑÊ¶ÇÂøµÔºå‰ª•Á≥ªÁªüÂú∞Ë∑üË∏™Ê®°ÂûãÁöÑÂ≠¶‰π†Ë°å‰∏∫ÂíåÈáçÂ§çÈîôËØØ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂâØÈ©æÈ©∂Ê®°ÂûãËÉΩÂ§üÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏çÊñ≠Â≠¶‰π†Ôºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17540', 'title': 'RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17540', 'abstract': 'RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.', 'score': 5, 'issue_id': 3950, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '0b4459388e7a7ca1', 'authors': ['Mingrui Wu', 'Lu Wang', 'Pu Zhao', 'Fangkai Yang', 'Jianjin Zhang', 'Jianfeng Liu', 'Yuefeng Zhan', 'Weihao Han', 'Hao Sun', 'Jiayi Ji', 'Xiaoshuai Sun', 'Qingwei Lin', 'Weiwei Deng', 'Dongmei Zhang', 'Feng Sun', 'Qi Zhang', 'Rongrong Ji'], 'affiliations': ['Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2505.17540.jpg', 'data': {'categories': ['#cv', '#rl', '#rag', '#optimization', '#reasoning', '#training'], 'emoji': 'üé®', 'ru': {'title': '–£–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'RePrompt - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ü–µ–Ω–∫–∏. RePrompt –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∫–æ–º–ø–æ–Ω–æ–≤–∫—É –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'RePrompt: Enhancing Text-to-Image Generation with Reinforcement Learning', 'desc': 'RePrompt is a new framework that uses reinforcement learning to improve text-to-image generation by focusing on the quality of the generated images. It addresses the common issue where existing models fail to accurately interpret short prompts, often leading to unrealistic outputs. By incorporating explicit reasoning into the prompt enhancement process, RePrompt generates structured prompts that are better aligned with user intentions. The framework trains a language model to optimize prompts based on image-level outcomes, achieving significant improvements in spatial layout and compositional generalization without needing human-annotated data.'}, 'zh': {'title': 'RePromptÔºö‰ºòÂåñÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'RePromptÊòØ‰∏Ä‰∏™‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁöÑÈáçÊñ∞ÊèêÁ§∫Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊïàÊûú„ÄÇÂÆÉÈÄöËøá‰ºòÂåñÂõæÂÉèÁ∫ßÁªìÊûúÔºåÊòæËëóÊîπÂñÑ‰∫ÜÁ©∫Èó¥Â∏ÉÂ±ÄÂíåÁªÑÂêàÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåRePrompt‰∏ç‰æùËµñÊâãÂ∑•ËßÑÂàôÔºåËÄåÊòØËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁªìÊûÑÂåñÁöÑËá™ÂèçÊèêÁ§∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRePromptÂú®Â§ö‰∏™ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊàêÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17508', 'title': 'On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.17508', 'abstract': 'A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.', 'score': 5, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '6ae63edcb7127847', 'authors': ['Yifan Zhang', 'Yifeng Liu', 'Huizhuo Yuan', 'Yang Yuan', 'Quanquan Gu', 'Andrew C Yao'], 'affiliations': ['IIIS, Tsinghua University', 'Shanghai Qi Zhi Institute', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2505.17508.jpg', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RPG (regularized policy gradient) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–≤—ã—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ RPG –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ KL-—Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏.'}, 'en': {'title': 'Enhancing LLM Reasoning with Regularized Policy Gradients', 'desc': 'This paper introduces a regularized policy gradient framework that utilizes Kullback-Leibler (KL) divergence to improve the reasoning abilities of large language models (LLMs) in online reinforcement learning (RL). It systematically explores various KL divergence formulations to enhance training stability and performance through surrogate loss functions. The authors derive policy gradients for both forward and reverse KL divergences, accommodating different types of policy distributions. Extensive experiments demonstrate that their proposed methods achieve better or comparable results against established baselines in RL tasks involving LLMs.'}, 'zh': {'title': 'Ê≠£ÂàôÂåñÁ≠ñÁï•Ê¢ØÂ∫¶ÔºöÊèêÂçáLLMÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ≠£ÂàôÂåñÁ≠ñÁï•Ê¢ØÂ∫¶Ê°ÜÊû∂ÔºåÁî®‰∫éÊé¢Á¥¢KLÊï£Â∫¶ÁöÑ‰∏çÂêåÂΩ¢ÂºèÔºå‰ª•Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ÂàÜÊûê‰∫ÜÂ¶Ç‰ΩïÂ∞Ü‰∏çÂêåÁöÑKLÊï£Â∫¶‰º∞ËÆ°Êï¥ÂêàÂà∞Êõø‰ª£ÊçüÂ§±ÂáΩÊï∞‰∏≠Ôºå‰ªéËÄåÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇÈÄöËøáÂØπÊ≠£ÂêëÂíåÂèçÂêëKLÊï£Â∫¶ÁöÑÊ≠£ÂàôÂåñÁõÆÊ†áÔºåÊàë‰ª¨Êé®ÂØº‰∫ÜÁõ∏Â∫îÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶ÂíåÊçüÂ§±ÂáΩÊï∞ÔºåÂπ∂ËÄÉËôë‰∫ÜÊ†áÂáÜÂåñÂíåÈùûÊ†áÂáÜÂåñÁöÑÁ≠ñÁï•ÂàÜÂ∏É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫øÁÆóÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ‰∏äÈÉΩÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15182', 'title': 'ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State\n  Reflection', 'url': 'https://huggingface.co/papers/2505.15182', 'abstract': "ReflAct, a new reasoning backbone for LLM agents, improves goal alignment and reduces hallucinations by continuously reflecting on the agent's state, surpassing ReAct and other enhanced variants.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.", 'score': 5, 'issue_id': 3955, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'b595495608e9ea19', 'authors': ['Jeonghye Kim', 'Sojeong Rhee', 'Minbeom Kim', 'Dohyung Kim', 'Sangmook Lee', 'Youngchul Sung', 'Kyomin Jung'], 'affiliations': ['KAIST', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15182.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#rl', '#hallucinations', '#agents'], 'emoji': 'üß†', 'ru': {'title': 'ReflAct: –†–µ—Ñ–ª–µ–∫—Å–∏—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'ReflAct - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —Ü–µ–ª—å—é –∏ —É–º–µ–Ω—å—à–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø—É—Ç–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞. ReflAct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç ReAct –∏ –¥—Ä—É–≥–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, —Å–º–µ—â–∞—è –∞–∫—Ü–µ–Ω—Ç —Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –µ–≥–æ —Ü–µ–ª–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ReflAct –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å ReAct, –¥–æ—Å—Ç–∏–≥–∞—è 93.3% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤ ALFWorld.'}, 'en': {'title': 'ReflAct: Grounded Reasoning for Reliable LLM Agents', 'desc': "ReflAct is a new reasoning framework designed for large language model (LLM) agents that enhances their ability to align with goals and minimizes errors known as hallucinations. Unlike its predecessor, ReAct, which often leads to inconsistent reasoning and misalignment, ReflAct focuses on continuously reflecting on the agent's current state in relation to its objectives. This approach ensures that decisions are grounded in accurate internal beliefs, thereby improving the reliability of the agent's strategic actions. Empirical results show that ReflAct significantly outperforms ReAct and its enhanced versions, achieving a high success rate in complex environments."}, 'zh': {'title': 'ReflActÔºöÊèêÂçá‰ª£ÁêÜÊé®ÁêÜÁöÑÊ†∏ÂøÉÂäõÈáè', 'desc': 'ReflActÊòØ‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÁöÑÁõÆÊ†á‰∏ÄËá¥ÊÄßÂπ∂ÂáèÂ∞ëÂπªËßâÁé∞Ë±°„ÄÇ‰∏é‰º†ÁªüÁöÑReActÊñπÊ≥ïÁõ∏ÊØîÔºåReflActÈÄöËøáÊåÅÁª≠ÂèçÊÄù‰ª£ÁêÜÁöÑÁä∂ÊÄÅÊù•ÊîπÂñÑÊé®ÁêÜËøáÁ®ãÔºåÈÅøÂÖç‰∫Ü‰∏ç‰∏ÄËá¥ÁöÑÂÜÖÈÉ®‰ø°ÂøµÂíåÁõÆÊ†áÂØπÈΩêÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåReflActÂú®ALFWorld‰∏≠Âπ≥ÂùáË∂ÖË∂äReAct 27.7%ÔºåÊàêÂäüÁéáËææÂà∞93.3%„ÄÇËøô‰∏ÄËÆæËÆ°Âº∫Ë∞É‰∫ÜÂä†Âº∫Ê†∏ÂøÉÊé®ÁêÜÊ°ÜÊû∂ÂØπ‰∫éÊèêÈ´ò‰ª£ÁêÜÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17091', 'title': 'Large Language Models Implicitly Learn to See and Hear Just By Reading', 'url': 'https://huggingface.co/papers/2505.17091', 'abstract': 'Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.', 'score': 5, 'issue_id': 3945, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '7bb07d5dfb6680e6', 'authors': ['Prateek Verma', 'Mert Pilanci'], 'affiliations': ['Department of Electrical Engineering Stanford University Stanford, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.17091.jpg', 'data': {'categories': ['#cv', '#multimodal', '#optimization', '#architecture', '#transfer_learning', '#audio'], 'emoji': 'üß†', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—Ä–µ—Ç–∞—é—Ç –∑—Ä–µ–Ω–∏–µ –∏ —Å–ª—É—Ö —á–µ—Ä–µ–∑ —á—Ç–µ–Ω–∏–µ', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–æ—Å–æ–±–Ω—ã —Ä–∞–∑–≤–∏–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞—É–¥–∏–æ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –î–∞–Ω–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –º–æ—â–Ω—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ö–µ–º–∞—Ö, —Ñ–æ—Ä–º–∏—Ä—É–µ–º—ã—Ö —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∏ –∏—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è.'}, 'en': {'title': 'Unlocking Multi-Modal Understanding with Text LLMs', 'desc': "This paper explores how auto-regressive language models (LLMs) trained solely on text can develop the ability to understand and classify images and audio without needing additional fine-tuning. The authors demonstrate that these text-based models can process inputs like image patches and audio waveforms, producing embeddings or category labels similar to those used in traditional classification tasks. They validate their findings by applying the model to audio classification tasks on datasets like FSD-50K and GTZAN, as well as image classification on CIFAR-10 and Fashion-MNIST. This research highlights the potential of leveraging text LLMs' internal capabilities for multi-modal applications, reducing the need for training separate models for each modality."}, 'zh': {'title': 'ÊñáÊú¨Ê®°ÂûãÁöÑË∑®Ê®°ÊÄÅÁêÜËß£ËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÊúâË∂£ÁöÑÂèëÁé∞ÔºöÈÄöËøáÂØπÊñáÊú¨ËøõË°åËÆ≠ÁªÉÁöÑËá™ÂõûÂΩíËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂÜÖÂú®Âú∞ÂèëÂ±ïÂá∫ÁêÜËß£ÂõæÂÉèÂíåÈü≥È¢ëÁöÑËÉΩÂäõ„ÄÇËøôÊ†∑ÔºåÊ®°ÂûãÂú®Ê≤°ÊúâÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞±ËÉΩËøõË°åË∑®Ê®°ÊÄÅÁöÑÂàÜÁ±ª‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáËæìÂÖ•ÂõæÂÉèÂùó„ÄÅÈü≥È¢ëÊ≥¢ÂΩ¢ÊàñÊ†áËÆ∞ÔºåÁîüÊàêÂÖ∏ÂûãÁöÑÂàÜÁ±ªÁÆ°ÈÅìÊâÄÈúÄÁöÑÂµåÂÖ•ÊàñÁ±ªÂà´Ê†áÁ≠æ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊñáÊú¨Ê®°ÂûãÁöÑÊùÉÈáçÂú®Èü≥È¢ëÂíåÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÈÄÇÁî®ÊÄßÔºåÊé®Âä®‰∫ÜÊñáÊú¨ËØ≠Ë®ÄÊ®°ÂûãÂ≠¶‰π†Âº∫Â§ßÂÜÖÈÉ®ÁîµË∑ØÁöÑÊ¶ÇÂøµ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17016', 'title': 'Interactive Post-Training for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2505.17016', 'abstract': 'We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.   RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.', 'score': 4, 'issue_id': 3948, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'ab53333533d30d3f', 'authors': ['Shuhan Tan', 'Kairan Dou', 'Yue Zhao', 'Philipp Kr√§henb√ºhl'], 'affiliations': ['Nankai University', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.17016.jpg', 'data': {'categories': ['#games', '#training', '#optimization', '#rlhf', '#multimodal', '#rl', '#transfer_learning'], 'emoji': 'ü§ñ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ VLA –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –Ω–∞–¥–∑–æ—Ä–æ–º', 'desc': 'RIPT-VLA - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –±–∏–Ω–∞—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã —É—Å–ø–µ—Ö–∞. RIPT-VLA –ø—Ä–∏–º–µ–Ω–∏–º–∞ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º VLA –º–æ–¥–µ–ª—è–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∑–≤–æ–ª—è—è –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∑–∞ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π.'}, 'en': {'title': 'Reinforcement Learning for Efficient Vision-Language-Action Adaptation', 'desc': 'RIPT-VLA is a new method that enhances Vision-Language-Action (VLA) models using reinforcement learning with minimal supervision. It allows these models to learn from sparse binary rewards instead of relying on extensive expert demonstrations. This approach improves the performance of various VLA models significantly, achieving a 97.5% success rate with the OpenVLA-OFT model. Additionally, RIPT-VLA is efficient in both computation and data usage, enabling models to adapt quickly to new tasks with just one demonstration.'}, 'zh': {'title': 'RIPT-VLAÔºöÈ´òÊïàÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂêéËÆ≠ÁªÉËåÉÂºè', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜRIPT-VLAÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçï‰∏îÂèØÊâ©Â±ïÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰∫§‰∫íÂºèÂêéËÆ≠ÁªÉËåÉÂºèÔºåÊó®Âú®‰ΩøÁî®Á®ÄÁñèÁöÑ‰∫åÂÖÉÊàêÂäüÂ•ñÂä±Êù•ÂæÆË∞ÉÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°Âûã„ÄÇÁé∞ÊúâÁöÑVLAËÆ≠ÁªÉÊµÅÁ®ãËøá‰∫é‰æùËµñÁ¶ªÁ∫ø‰∏ìÂÆ∂Á§∫ËåÉÊï∞ÊçÆÂíåÁõëÁù£Ê®°‰ªøÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰ΩéÊï∞ÊçÆÁéØÂ¢É‰∏ãÈÄÇÂ∫îÊñ∞‰ªªÂä°ÂíåÊñ∞ÁéØÂ¢ÉÁöÑËÉΩÂäõ„ÄÇRIPT-VLAÈÄöËøáÂä®ÊÄÅÂõûÊîæÈááÊ†∑ÂíåÁïô‰∏Ä‰ºòÂäø‰º∞ËÆ°ÁöÑÁ®≥ÂÆöÁ≠ñÁï•‰ºòÂåñÁÆóÊ≥ïÔºåÂÆûÁé∞‰∫Ü‰∫§‰∫íÂºèÂêéËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRIPT-VLAÂú®‰∏çÂêå‰ªªÂä°ÂíåÂú∫ÊôØ‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏îÂØπÂàùÂßãÁä∂ÊÄÅ‰∏ä‰∏ãÊñáÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18078', 'title': 'DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation', 'url': 'https://huggingface.co/papers/2505.18078', 'abstract': 'DanceTogether, an end-to-end diffusion framework, generates long, photorealistic multi-actor interaction videos from single reference images and pose-mask streams, outperforming existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds "who" and "how" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/.', 'score': 3, 'issue_id': 3955, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '3157b30c4a153886', 'authors': ['Junhao Chen', 'Mingjin Chen', 'Jianjin Xu', 'Xiang Li', 'Junting Dong', 'Mingze Sun', 'Puhua Jiang', 'Hongxiang Li', 'Yuhang Yang', 'Hao Zhao', 'Xiaoxiao Long', 'Ruqi Huang'], 'affiliations': ['Beijing Normal University', 'Carnegie Mellon University', 'Hong Kong Baptist University', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'Tsinghua University', 'University of Science & Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.18078.jpg', 'data': {'categories': ['#cv', '#games', '#video', '#diffusion', '#robotics', '#dataset', '#benchmark'], 'emoji': 'üíÉ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç —Ö–æ—Ä–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∫ –º–Ω–æ–≥–æ–∞–∫—Ç–µ—Ä–Ω–æ–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é', 'desc': 'DanceTogether - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ —Å–æ–∑–¥–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–Ω–æ –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø–æ—Ç–æ–∫–∏ –ø–æ–∑-–º–∞—Å–æ–∫. –ö–ª—é—á–µ–≤–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ - MaskPoseAdapter, –∫–æ—Ç–æ—Ä—ã–π —Å–≤—è–∑—ã–≤–∞–µ—Ç –ª–∏—á–Ω–æ—Å—Ç—å –∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞.'}, 'en': {'title': 'Revolutionizing Multi-Actor Video Generation with DanceTogether', 'desc': 'DanceTogether is a novel end-to-end diffusion framework designed for generating long, photorealistic videos featuring multiple actors interacting based on a single reference image and pose-mask streams. It addresses the challenges of controllable video generation by effectively binding identities and actions at each denoising step, which prevents issues like identity drift and appearance bleeding. The framework is trained on extensive datasets, including diverse interactions and scenarios, allowing it to generalize well across different domains. By significantly outperforming existing methods, DanceTogether enhances the capabilities of digital production and embodied AI applications.'}, 'zh': {'title': 'DanceTogetherÔºöÂ§öÊºîÂëò‰∫íÂä®ËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'DanceTogetherÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊâ©Êï£Ê°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÂçï‰∏ÄÂèÇËÄÉÂõæÂÉèÂíåÂßøÊÄÅÊé©Á†ÅÊµÅÁîüÊàêÈïøÊó∂Èó¥ÁöÑÈÄºÁúüÂ§öÊºîÂëò‰∫íÂä®ËßÜÈ¢ëÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁ≥ªÁªüÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁªìÂêàÂº∫Â§ßÁöÑË∑üË∏™Êé©Á†ÅÂíåËØ≠‰πâ‰∏∞ÂØå‰ΩÜÂô™Â£∞ËæÉÂ§ßÁöÑÂßøÊÄÅÁÉ≠ÂõæÔºåÂú®ÊØè‰∏™ÂéªÂô™Ê≠•È™§‰∏≠ÁªëÂÆö‚ÄúË∞Å‚ÄùÂíå‚ÄúÂ¶Ç‰Ωï‚ÄùÔºåÊúâÊïàÊ∂àÈô§‰∫ÜË∫´‰ªΩÊºÇÁßªÂíåÂ§ñËßÇÊ®°Á≥äÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂ§ö‰∏™Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÊµãËØïÔºå‰ª•ÊîØÊåÅÂ§ßËßÑÊ®°ÁöÑËÆ≠ÁªÉÂíåËØÑ‰º∞ÔºåÂ±ïÁ§∫‰∫ÜDanceTogetherÂú®Â§öÊºîÂëò‰∫íÂä®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊòæËëó‰ºòÂäø„ÄÇËØ•Ê®°ÂûãÁöÑÊàêÂäü‰∏∫Êï∞Â≠óÂà∂‰Ωú„ÄÅ‰ªøÁúüÂíåÂÖ∑Ë∫´Êô∫ËÉΩÂºÄËæü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15805', 'title': 'Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering', 'url': 'https://huggingface.co/papers/2505.15805', 'abstract': 'LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security.', 'score': 3, 'issue_id': 3951, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '1ccf6fc4f4524fad', 'authors': ['Hwan Chang', 'Yumin Kim', 'Yonghyun Jun', 'Hwanhee Lee'], 'affiliations': ['Chung-Ang University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.15805.jpg', 'data': {'categories': ['#leakage', '#multimodal', '#alignment', '#security', '#dataset', '#benchmark'], 'emoji': 'üîê', 'ru': {'title': 'LLM –Ω–∞—Ä—É—à–∞—é—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: urgent call –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —É—Ç–µ—á–∫–∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö CoPriva –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM —Å–æ–±–ª—é–¥–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–µ—Ä–∞–∑–≥–ª–∞—à–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 10 –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö –∞—Ç–∞–∫–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –æ—Å—Ç—Ä—É—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ LLM.'}, 'en': {'title': 'Strengthening Contextual Security in LLMs', 'desc': 'This paper addresses the issue of Large Language Models (LLMs) leaking sensitive information by violating contextual security policies, especially during indirect attacks. The authors introduce a new benchmark dataset called CoPriva, which tests LLMs on their ability to adhere to user-defined non-disclosure policies in question answering scenarios. The evaluation of 10 LLMs reveals significant vulnerabilities, particularly in handling indirect attacks, where models often fail to respect security constraints. The study highlights the necessity for improved safety mechanisms to ensure that LLMs can effectively incorporate contextual security measures in sensitive applications.'}, 'zh': {'title': 'Á°Æ‰øù‰∏ä‰∏ãÊñáÂÆâÂÖ®ÔºåÈò≤Ê≠¢‰ø°ÊÅØÊ≥ÑÈú≤ÔºÅ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÊïèÊÑü‰ø°ÊÅØÊó∂ÔºåÂ∏∏Â∏∏ËøùÂèç‰∏ä‰∏ãÊñáÂÆâÂÖ®ÊîøÁ≠ñÔºåÂ∞§ÂÖ∂ÊòØÂú®Èó¥Êé•ÊîªÂáª‰∏ãÔºåÊòæÁ§∫Âá∫ÂΩìÂâçÂÆâÂÖ®Êú∫Âà∂ÁöÑÈáçÂ§ßÁº∫Èô∑„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜÊï∞ÊçÆÈõÜCoPrivaÔºåÁî®‰∫éËØÑ‰º∞LLMsÂú®ÈóÆÁ≠î‰∏≠ÈÅµÂæ™‰∏ä‰∏ãÊñáÈùûÊä´Èú≤ÊîøÁ≠ñÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂØπ10‰∏™LLMsËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂèëÁé∞ËÆ∏Â§öÊ®°ÂûãËøùÂèçÁî®Êà∑ÂÆö‰πâÁöÑÊîøÁ≠ñÔºåÊ≥ÑÈú≤ÊïèÊÑü‰ø°ÊÅØÔºåÂ∞§ÂÖ∂ÊòØÂú®Èù¢ÂØπÈó¥Êé•ÊîªÂáªÊó∂„ÄÇÁ†îÁ©∂ÁªìÊûúÂº∫Ë∞É‰∫ÜËø´ÂàáÈúÄË¶ÅÊõ¥Âº∫Â§ßÁöÑÊñπÊ≥ïÊù•Á°Æ‰øù‰∏ä‰∏ãÊñáÂÆâÂÖ®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17373', 'title': 'Value-Guided Search for Efficient Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2505.17373', 'abstract': 'A simple and efficient method for value model training on long-context reasoning traces improves test-time performance and reduces computational cost compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of "step," which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced.', 'score': 2, 'issue_id': 3960, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'fda5fcafecd8fe99', 'authors': ['Kaiwen Wang', 'Jin Peng Zhou', 'Jonathan Chang', 'Zhaolin Gao', 'Nathan Kallus', 'Kiant√© Brantley', 'Wen Sun'], 'affiliations': ['Cornell University', 'Databricks', 'Harvard University', 'Netflix'], 'pdf_title_img': 'assets/pdf/title_img/2505.17373.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#math', '#dataset', '#long_context', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM), –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–Ω—è—Ç–∏—è '—à–∞–≥–∞', —á—Ç–æ —Å–ª–æ–∂–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 2,5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–∏–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–º–æ–º 1,5 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –∫ DeepSeek —É–ª—É—á—à–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."}, 'en': {'title': 'Efficient Value Model Training for Long-Context Reasoning', 'desc': 'This paper introduces a novel method for training value models specifically designed for long-context reasoning tasks. Unlike traditional process reward models, this approach simplifies the training process by eliminating the need for a detailed definition of steps, which can be challenging in long-context scenarios. By utilizing a large dataset of 2.5 million reasoning traces, the authors train a 1.5 billion token-level value model that enhances performance in DeepSeek models during testing. The proposed block-wise value-guided search (VGS) method demonstrates superior test-time efficiency and accuracy compared to conventional techniques, while also reducing computational costs significantly.'}, 'zh': {'title': 'È´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜ‰ª∑ÂÄºÊ®°ÂûãËÆ≠ÁªÉ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÈ´òÊïàÁöÑ‰ª∑ÂÄºÊ®°ÂûãËÆ≠ÁªÉÊñπÊ≥ïÔºå‰∏ìÊ≥®‰∫éÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜËΩ®Ëøπ„ÄÇ‰∏éÁé∞ÊúâÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏çÈúÄË¶ÅÂØπ‚ÄúÊ≠•È™§‚ÄùËøõË°åÁªÜËá¥ÂÆö‰πâÔºåËøôÂú®Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜÊ®°Âûã‰∏≠ÊòØÂõ∞ÈöæÁöÑ„ÄÇÈÄöËøáÊî∂ÈõÜ250‰∏áÊù°Êé®ÁêÜËΩ®ËøπÁöÑÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™15‰∫øÊ†áËÆ∞Á∫ßÂà´ÁöÑ‰ª∑ÂÄºÊ®°ÂûãÔºåÂπ∂Â∞ÜÂÖ∂Â∫îÁî®‰∫éDeepSeekÊ®°ÂûãÔºå‰ª•ÊèêÈ´òÊµãËØïÊó∂ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂü∫‰∫éÂùóÁöÑ‰ª∑ÂÄºÂºïÂØºÊêúÁ¥¢ÔºàVGSÔºâÂú®ÊµãËØïÊó∂ÁöÑËÆ°ÁÆóÊïàÁéá‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÂ§öÊï∞ÊäïÁ•®ÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16293', 'title': 'Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA', 'url': 'https://huggingface.co/papers/2505.16293', 'abstract': "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.  \t\t\t\t\tAI-generated summary \t\t\t\t Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant information. This hinders a model's capacity to process and reason over retrieved content and limits performance. While recent methods focus on compressing retrieved information, they are either restricted to single-round RAG, require finetuning or lack scalability in iterative RAG. To address these challenges, we propose Notes Writing, a method that generates concise and relevant notes from retrieved documents at each step, thereby reducing noise and retaining only essential information. This indirectly increases the effective context length of Large Language Models (LLMs), enabling them to reason and plan more effectively while processing larger volumes of input text. Notes Writing is framework agnostic and can be integrated with different iterative RAG methods. We demonstrate its effectiveness with three iterative RAG methods, across two models and four evaluation datasets. Notes writing yields an average improvement of 15.6 percentage points overall, with minimal increase in output tokens.", 'score': 2, 'issue_id': 3949, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'c597f26ca16d4748', 'authors': ['Rishabh Maheshwary', 'Masoud Hashemi', 'Khyati Mahajan', 'Shiva Krishna Reddy Malay', 'Sai Rajeswar', 'Sathwik Tejaswi Madhusudhan', 'Spandana Gella', 'Vikas Yadav'], 'affiliations': ['ServiceNow', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.16293.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#multimodal', '#rag', '#long_context'], 'emoji': 'üìù', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∞—Ç–∫–∏—Ö –∑–∞–º–µ—Ç–æ–∫', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Notes Writing –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–µ –∑–∞–º–µ—Ç–∫–∏ –∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ, —É–º–µ–Ω—å—à–∞—è —à—É–º –∏ —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. Notes Writing –∫–æ—Å–≤–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è –∏–º –ª—É—á—à–µ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª —Å—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 15.6 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.'}, 'en': {'title': 'Enhancing Iterative RAG with Concise Notes for Better Reasoning', 'desc': 'The paper introduces a method called Notes Writing, which enhances iterative Retrieval-Augmented Generation (RAG) by creating concise notes from retrieved documents at each step. This approach helps to reduce irrelevant information and noise, allowing models to focus on essential content, which improves reasoning and performance. Unlike previous methods that struggle with lengthy contexts or require fine-tuning, Notes Writing is scalable and can be applied across various RAG frameworks. The results show an average performance improvement of 15.6 percentage points with minimal increase in output tokens, demonstrating its effectiveness in multi-hop question answering tasks.'}, 'zh': {'title': 'Á¨îËÆ∞ÂÜô‰ΩúÔºöÊèêÂçáËø≠‰ª£RAGÁöÑÊúâÊïàÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÁ¨îËÆ∞ÂÜô‰Ωú‚ÄùÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑÂ§öË∑≥ÈóÆÁ≠î‰∏≠ÁöÑËø≠‰ª£Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÊØè‰∏™Ê≠•È™§ÁîüÊàêÁÆÄÊ¥ÅÁöÑÁ¨îËÆ∞ÔºåÂáèÂ∞ë‰∫ÜÂÜó‰Ωô‰ø°ÊÅØÁöÑÂπ≤Êâ∞Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÁ¨îËÆ∞ÂÜô‰Ωú‰∏çÈúÄË¶ÅÂæÆË∞ÉÔºå‰∏îÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑËø≠‰ª£RAGÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁ¨îËÆ∞ÂÜô‰ΩúÂú®Â§ö‰∏™Ê®°ÂûãÂíåËØÑ‰º∞Êï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü15.6‰∏™ÁôæÂàÜÁÇπÔºåÂêåÊó∂ËæìÂá∫‰ª§ÁâåÁöÑÂ¢ûÂä†ÈùûÂ∏∏ÊúâÈôê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16056', 'title': 'Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models', 'url': 'https://huggingface.co/papers/2505.16056', 'abstract': 'MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .', 'score': 2, 'issue_id': 3951, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '065702f91e2210fa', 'authors': ['Jingcong Liang', 'Siyuan Wang', 'Miren Tian', 'Yitong Li', 'Duyu Tang', 'Zhongyu Wei'], 'affiliations': ['Fudan University', 'Huawei Technologies Ltd.', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.16056.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE –º–æ–¥–µ–ª–µ–π: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é LLM', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –≤ MoE –º–æ–¥–µ–ª—è—Ö: Segment Routing Best Performance (SRP) –∏ Segment Cache Best Hit Rate (SCH). –ê–Ω–∞–ª–∏–∑ 20 MoE LLM –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å MoE –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –∏ –±–µ–∑ –æ–±—â–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–∞–∏–≤—ã—Å—à—É—é –ª–æ–∫–∞–ª—å–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç—ã, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–µ—Å—è –Ω–∞ –¥–æ–º–µ–Ω–∞—Ö, –±–æ–ª—å—à–µ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, —á–µ–º —ç–∫—Å–ø–µ—Ä—Ç—ã –ø–æ —Å–ª–æ–≤–∞—Ä—é.'}, 'en': {'title': 'Optimizing Expert Offloading for Efficient MoE Models', 'desc': 'This paper discusses how Mixture-of-Experts (MoE) models can efficiently scale large language models (LLMs) by using expert offloading, which allows some experts to be stored in fast memory while others remain in slower memory. The authors introduce two new metrics to evaluate local routing consistency: Segment Routing Best Performance (SRP) and Segment Cache Best Hit Rate (SCH), which help assess how well experts serve token segments. Their analysis of 20 different MoE LLMs reveals that models using MoE at every layer and avoiding shared experts achieve better routing consistency. The study also finds that domain-specialized experts enhance routing consistency more than vocabulary-specialized ones, suggesting a balance between cache effectiveness and efficiency can be achieved with appropriate cache sizes.'}, 'zh': {'title': 'È´òÊïàÊâ©Â±ïÔºöMoEÊ®°ÂûãÁöÑÂ±ÄÈÉ®Ë∑ØÁî±‰∏ÄËá¥ÊÄß', 'desc': 'MoEÊ®°ÂûãÈÄöËøáÁ®ÄÁñèÊøÄÊ¥ª‰∏ìÂÆ∂ÂÆûÁé∞‰∫ÜÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ´òÊïàÊâ©Â±ï„ÄÇ‰∏∫‰∫ÜÂú®ÂÜÖÂ≠òÂèóÈôêÁöÑËÆæÂ§á‰∏äÊúâÊïàÈÉ®ÁΩ≤Â§ßÂûãMoEÊ®°ÂûãÔºåËÆ∏Â§öÁ≥ªÁªüÂºïÂÖ•‰∫Ü‰∏ìÂÆ∂Âç∏ËΩΩÊäÄÊúØÔºåÂ∞ÜÈÉ®ÂàÜ‰∏ìÂÆ∂ÁºìÂ≠òÂà∞Âø´ÈÄüÂÜÖÂ≠ò‰∏≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏§‰∏™Â∫¶ÈáèÊ†áÂáÜÊù•ËØÑ‰º∞MoEÊ®°ÂûãÁöÑÂ±ÄÈÉ®Ë∑ØÁî±‰∏ÄËá¥ÊÄßÔºåÂàÜÂà´ÊòØÊÆµË∑ØÁî±ÊúÄ‰Ω≥ÊÄßËÉΩ(SRP)ÂíåÊÆµÁºìÂ≠òÊúÄ‰Ω≥ÂëΩ‰∏≠Áéá(SCH)„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∫îÁî®MoE‰∫éÊØè‰∏ÄÂ±Ç‰∏î‰∏ç‰ΩøÁî®ÂÖ±‰∫´‰∏ìÂÆ∂ÁöÑÊ®°ÂûãÂÖ∑ÊúâÊúÄÈ´òÁöÑÂ±ÄÈÉ®Ë∑ØÁî±‰∏ÄËá¥ÊÄßÔºåËøô‰∏∫ÂÜÖÂ≠òÈ´òÊïàÁöÑMoEËÆæËÆ°ÂíåÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16409', 'title': 'FREESON: Retriever-Free Retrieval-Augmented Reasoning via\n  Corpus-Traversing MCTS', 'url': 'https://huggingface.co/papers/2505.16409', 'abstract': "FREESON, a novel framework that integrates retrieval and reasoning roles within LRMs using CT-MCTS, improves the performance of multistep reasoning models in QA tasks by reducing representation bottlenecks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in multi-step reasoning and calling search engines at appropriate steps. However, existing retrieval-augmented reasoning approaches rely on separate retrieval models, limiting the LRM's role in retrieval to deciding when to retrieve and how to query. This separation not only increases hardware and operational costs but also leads to errors in the retrieval process due to the representation bottleneck, a phenomenon where the retriever's embedding space is not expressive enough to meet the generator's requirements. To address this, we shift our perspective from sequence-to-sequence matching to locating the answer-containing paths within the corpus, and propose a novel framework called FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables LRMs to retrieve relevant knowledge on their own by acting as both a generator and retriever. To achieve this, we introduce a variant of the MCTS algorithm specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus toward answer-containing regions. Our results on five open-domain QA benchmarks, including single-hop and multi-hop questions, show that FREESON achieves an average improvement of 14.4% in EM and F1 over four multi-step reasoning models with a separate retriever, and it also performs comparably to the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.", 'score': 1, 'issue_id': 3957, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '31b1d9d5fef7e537', 'authors': ['Chaeeun Kim', 'Seungone Kim'], 'affiliations': ['Carnegie Mellon University', 'LBOX'], 'pdf_title_img': 'assets/pdf/title_img/2505.16409.jpg', 'data': {'categories': ['#multimodal', '#rag', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'FREESON: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º', 'desc': 'FREESON - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ CT-MCTS. –û–Ω–∞ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —É–º–µ–Ω—å—à–∞—è –ø—Ä–æ–±–ª–µ–º—É —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. FREESON –ø–æ–∑–≤–æ–ª—è–µ—Ç LRM —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –≤—ã—Å—Ç—É–ø–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤ —Ä–æ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'FREESON: Unifying Retrieval and Reasoning for Enhanced QA Performance', 'desc': 'FREESON is a new framework that enhances Large Reasoning Models (LRMs) by integrating retrieval and reasoning tasks using a specialized algorithm called CT-MCTS. This approach allows LRMs to independently retrieve relevant information while generating answers, eliminating the need for separate retrieval models. By addressing the representation bottleneck, FREESON improves the efficiency and accuracy of multi-step reasoning in question-answering tasks. The framework has shown significant performance gains on various QA benchmarks, outperforming traditional models that rely on separate retrieval systems.'}, 'zh': {'title': 'FREESONÔºöÊï¥ÂêàÊ£ÄÁ¥¢‰∏éÊé®ÁêÜÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'FREESONÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜÊ£ÄÁ¥¢ÂíåÊé®ÁêÜÁöÑËßíËâ≤Êï¥ÂêàÂú®Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâ‰∏≠Ôºå‰ΩøÁî®CT-MCTSÁÆóÊ≥ïÊù•ÊèêÂçáÂ§öÊ≠•Êé®ÁêÜÊ®°ÂûãÂú®ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ‰º†ÁªüÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÊñπÊ≥ï‰æùËµñ‰∫éÁã¨Á´ãÁöÑÊ£ÄÁ¥¢Ê®°ÂûãÔºåËøôÈôêÂà∂‰∫ÜLRMÂú®Ê£ÄÁ¥¢‰∏≠ÁöÑ‰ΩúÁî®ÔºåÂπ∂ÂØºËá¥Ë°®Á§∫Áì∂È¢àÁöÑÈóÆÈ¢ò„ÄÇFREESONÈÄöËøáËÆ©LRMÂêåÊó∂‰Ωú‰∏∫ÁîüÊàêÂô®ÂíåÊ£ÄÁ¥¢Âô®ÔºåËÉΩÂ§üËá™‰∏ªÊ£ÄÁ¥¢Áõ∏ÂÖ≥Áü•ËØÜÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÁ°¨‰ª∂ÂíåÊìç‰ΩúÊàêÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFREESONÂú®Â§ö‰∏™ÂºÄÊîæÂüüÈóÆÁ≠îÂü∫ÂáÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü14.4%ÁöÑË°®Áé∞ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§öÊ≠•Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16022', 'title': 'NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.16022', 'abstract': "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.", 'score': 1, 'issue_id': 3951, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'f1dc404be3a18676', 'authors': ['Wei Liu', 'Siya Qi', 'Xinyu Wang', 'Chen Qian', 'Yali Du', 'Yulan He'], 'affiliations': ['Kings College London', 'Shanghai Jiao Tong University', 'The Alan Turing Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.16022.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#rlhf', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'NOVER: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'NOVER - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. NOVER –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ —Ç–æ–≥–æ –∂–µ —Ä–∞–∑–º–µ—Ä–∞, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–∞ 7.7%. –°–∏—Å—Ç–µ–º–∞ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–±—Ä–∞—Ç–Ω–æ–µ —Å—Ç–∏–º—É–ª–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ.'}, 'en': {'title': 'NOVER: Reinforcement Learning Without External Verifiers', 'desc': "NOVER is a novel reinforcement learning framework designed to improve the performance of language models without relying on external verifiers. It utilizes incentive training, which focuses on rewarding the final output of the model while encouraging intermediate reasoning steps. Unlike previous methods that depend on costly and high-quality annotated data for training reward models, NOVER only requires standard supervised fine-tuning data. This approach not only enhances the model's capabilities across various text-to-text tasks but also allows for innovative optimization techniques like inverse incentive training."}, 'zh': {'title': 'NOVERÔºöÊó†ÈúÄÂ§ñÈÉ®È™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞Ê°ÜÊû∂', 'desc': 'NOVERÊòØ‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊ∂àÈô§‰∫ÜÂØπÂ§ñÈÉ®È™åËØÅËÄÖÁöÑÈúÄÊ±ÇÔºå‰ªéËÄåÊèêÂçá‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÂú®ÊñáÊú¨Âà∞ÊñáÊú¨‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰ªÖ‰æùËµñÊ†áÂáÜÁöÑÁõëÁù£ÂæÆË∞ÉÊï∞ÊçÆÔºåÈÅøÂÖç‰∫ÜÈ´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÊòÇË¥µËÆ≠ÁªÉÊàêÊú¨„ÄÇNOVERÊîØÊåÅÊøÄÂä±ËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®Â§öÁßçÊñáÊú¨‰ªªÂä°‰∏≠Â∫îÁî®ÔºåÂπ∂‰∏îÂú®‰∏éÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁõ∏ÊØîÊó∂ÔºåË°®Áé∞ÊèêÈ´ò‰∫Ü7.7%„ÄÇÊ≠§Â§ñÔºåNOVERÁöÑÁÅµÊ¥ªÊÄß‰∏∫‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºå‰æãÂ¶ÇÈÄÜÊøÄÂä±ËÆ≠ÁªÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14256', 'title': 'FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation', 'url': 'https://huggingface.co/papers/2505.14256', 'abstract': 'FuxiMT, a Chinese-centric multilingual machine translation model utilizing a sparsified large language model, demonstrates superior performance in low-resource scenarios and strong zero-shot capabilities across 65 languages.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.', 'score': 1, 'issue_id': 3960, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '95b48054decdcd12', 'authors': ['Shaolin Zhu', 'Tianyu Dong', 'Bo Li', 'Deyi Xiong'], 'affiliations': ['College of Intelligence and Computing, Tianjin University, Tianjin, China', 'School of Software, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.14256.jpg', 'data': {'categories': ['#training', '#machine_translation', '#low_resource', '#multilingual'], 'emoji': 'üåê', 'ru': {'title': 'FuxiMT: –ü—Ä–æ—Ä—ã–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫', 'desc': 'FuxiMT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (LLM). –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Å—Å–∏–≤–Ω–æ–º –∫–∏—Ç–∞–π—Å–∫–æ–º –∫–æ—Ä–ø—É—Å–µ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –¥–æ–≤–æ–¥–∫–∞ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ 65 —è–∑—ã–∫–æ–≤. FuxiMT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE) –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫—É—Ä—Ä–∏–∫—É–ª—è—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ FuxiMT –Ω–∞–¥ —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –±–µ–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'FuxiMT: Bridging Language Gaps with Multilingual Mastery', 'desc': 'FuxiMT is a multilingual machine translation model designed specifically for Chinese, leveraging a sparsified large language model (LLM). It uses a two-stage training approach, starting with pre-training on a large Chinese dataset followed by multilingual fine-tuning on a dataset of 65 languages. The model incorporates Mixture-of-Experts (MoEs) and a curriculum learning strategy to enhance its performance, especially in low-resource settings. Experimental results show that FuxiMT outperforms existing state-of-the-art models, particularly excelling in zero-shot translation for language pairs without parallel data.'}, 'zh': {'title': 'FuxiMTÔºö‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÁöÑÁøªËØëÊñ∞Á™ÅÁ†¥', 'desc': 'FuxiMTÊòØ‰∏ÄÁßç‰ª•‰∏≠Êñá‰∏∫‰∏≠ÂøÉÁöÑÂ§öËØ≠Ë®ÄÊú∫Âô®ÁøªËØëÊ®°ÂûãÔºåÈááÁî®Á®ÄÁñèÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÈÄöËøá‰∏§Èò∂ÊÆµÁ≠ñÁï•ËøõË°åËÆ≠ÁªÉÔºåÈ¶ñÂÖàÂú®Â§ßÈáè‰∏≠ÊñáËØ≠Êñô‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂú®ÂåÖÂê´65ÁßçËØ≠Ë®ÄÁöÑÂ§ßÂûãÂπ≥Ë°åÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂ§öËØ≠Ë®ÄÂæÆË∞É„ÄÇFuxiMTÁªìÂêà‰∫Ü‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEsÔºâÊäÄÊúØÔºåÂπ∂ÈááÁî®ËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•Ôºå‰ª•Á°Æ‰øùÂú®‰∏çÂêåËµÑÊ∫êÊ∞¥Âπ≥‰∏ãÁöÑÁ®≥ÂÅ•Ë°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFuxiMTÂú®‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÊòæËëó‰ºò‰∫éÂº∫Âü∫Á∫øÊ®°ÂûãÔºåÂ∞§ÂÖ∂Âú®Êú™ËßÅËØ≠Ë®ÄÂØπÁöÑÈõ∂-shotÁøªËØëËÉΩÂäõ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12891', 'title': 'TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios', 'url': 'https://huggingface.co/papers/2505.12891', 'abstract': 'A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME .', 'score': 1, 'issue_id': 3949, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '3f60161242cb3f4c', 'authors': ['Shaohang Wei', 'Wei Li', 'Feifan Song', 'Wen Luo', 'Tianyi Zhuang', 'Haochen Tan', 'Zhijiang Guo', 'Houfeng Wang'], 'affiliations': ['Huawei Noahs Ark Lab', 'State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12891.jpg', 'data': {'categories': ['#dataset', '#survey', '#reasoning', '#benchmark', '#open_source'], 'emoji': '‚è≥', 'ru': {'title': 'TIME: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TIME –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –±—ã—Å—Ç—Ä–æ –º–µ–Ω—è—é—â–µ–π—Å—è –¥–∏–Ω–∞–º–∏–∫–æ–π —Å–æ–±—ã—Ç–∏–π –∏ —Å–ª–æ–∂–Ω—ã–º–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏. TIME –≤–∫–ª—é—á–∞–µ—Ç 38,522 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 3 —É—Ä–æ–≤–Ω—è –∏ 11 –ø–æ–¥–∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –∏ –±–µ–∑, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'TIME: Benchmarking Temporal Reasoning in LLMs', 'desc': 'The paper introduces a benchmark called TIME, which evaluates temporal reasoning capabilities in Large Language Models (LLMs) across real-world scenarios. It addresses three main challenges: handling intensive temporal information, adapting to fast-changing events, and understanding complex social interactions. TIME includes 38,522 question-answer pairs organized into three levels and 11 sub-tasks, with datasets like TIME-Wiki, TIME-News, and TIME-Dial. The authors also provide a human-annotated subset, TIME-Lite, to support future research and standardized evaluation in this area.'}, 'zh': {'title': 'TIMEÂü∫ÂáÜÔºöÊèêÂçáÊó∂Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫TIMEÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ËøõË°åÊó∂Èó¥Êé®ÁêÜÁöÑËÉΩÂäõ„ÄÇTIMEÂü∫ÂáÜÊ∂µÁõñ‰∫Ü38,522‰∏™ÈóÆÁ≠îÂØπÔºåÂàÜ‰∏∫‰∏â‰∏™Â±ÇÊ¨°Âíå11‰∏™ÁªÜÂàÜ‰ªªÂä°ÔºåÊó®Âú®Â∫îÂØπÂØÜÈõÜÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÅÂø´ÈÄüÂèòÂåñÁöÑ‰∫ã‰ª∂Âä®ÊÄÅÂíåÂ§çÊùÇÁöÑÁ§æ‰ºö‰∫íÂä®‰∏≠ÁöÑÊó∂Èó¥‰æùËµñÊÄß„ÄÇÊàë‰ª¨ÂØπÊé®ÁêÜÊ®°ÂûãÂíåÈùûÊé®ÁêÜÊ®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∏çÂêåÁé∞ÂÆûÂú∫ÊôØÂíå‰ªªÂä°‰∏≠ÁöÑÊó∂Èó¥Êé®ÁêÜË°®Áé∞„ÄÇ‰∏∫‰∫Ü‰øÉËøõÊú™Êù•ÁöÑÁ†îÁ©∂ÂíåÊ†áÂáÜÂåñËØÑ‰º∞ÔºåÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜTIME-LiteÔºå‰∏Ä‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑÂ≠êÈõÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11881', 'title': 'Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks', 'url': 'https://huggingface.co/papers/2505.11881', 'abstract': "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.  \t\t\t\t\tAI-generated summary \t\t\t\t Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy gain for ViT-B on ImageNet-1k.", 'score': 1, 'issue_id': 3948, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': 'b5265bcccaafd719', 'authors': ['Giyeong Oh', 'Woohyun Cho', 'Siyeol Kim', 'Suhwan Choi', 'Younjae Yu'], 'affiliations': ['Maum.AI', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11881.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å–≤—è–∑—è–º', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –≤—ã—Ö–æ–¥ –º–æ–¥—É–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–π —ç—Ç–æ–º—É –ø–æ—Ç–æ–∫—É. –ú–µ—Ç–æ–¥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª–µ–π –≤–Ω–æ—Å–∏—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–æ–≤—ã–µ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è –±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ–º—É –æ–±—É—á–µ–Ω–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±–æ–±—â–µ–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö."}, 'en': {'title': 'Unlocking New Features with Orthogonal Residual Updates', 'desc': "This paper introduces Orthogonal Residual Updates, a novel approach to enhance feature learning in deep neural networks. By decomposing the output of a module relative to the input stream, it ensures that only the orthogonal component is added, allowing the model to learn new features rather than just modifying existing ones. This method addresses the limitations of traditional residual connections, which can lead to underutilization of the model's capacity. The authors demonstrate that this technique improves generalization accuracy and training stability across various architectures and datasets, achieving significant performance gains."}, 'zh': {'title': 'Ê≠£‰∫§ÊÆãÂ∑ÆÊõ¥Êñ∞ÔºöÊèêÂçáÁâπÂæÅÂ≠¶‰π†‰∏éËÆ≠ÁªÉÁ®≥ÂÆöÊÄß', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊõ¥Êñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Ê≠£‰∫§ÊÆãÂ∑ÆÊõ¥Êñ∞ÔºåÊó®Âú®Â¢ûÂº∫ÁâπÂæÅÂ≠¶‰π†ÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ‰º†ÁªüÁöÑÊÆãÂ∑ÆËøûÊé•Áõ¥Êé•Â∞ÜÊ®°ÂùóËæìÂá∫Ê∑ªÂä†Âà∞ËæìÂÖ•ÊµÅ‰∏≠ÔºåËøôÂèØËÉΩÂØºËá¥Â≠¶‰π†Êñ∞ÁâπÂæÅÁöÑËÉΩÂäõË¢´‰Ωé‰º∞„ÄÇÊ≠£‰∫§ÊÆãÂ∑ÆÊõ¥Êñ∞ÈÄöËøáÂ∞ÜÊ®°ÂùóËæìÂá∫Áõ∏ÂØπ‰∫éËæìÂÖ•ÊµÅËøõË°åÂàÜËß£ÔºåÂè™Ê∑ªÂä†‰∏éËæìÂÖ•ÊµÅÊ≠£‰∫§ÁöÑÈÉ®ÂàÜÔºå‰ªéËÄåÂºïÂØºÊ®°Âùó‰∏ªË¶ÅË¥°ÁåÆÊñ∞ÁöÑË°®Á§∫ÊñπÂêë„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®Â§öÁßçÊû∂ÊûÑÂíåÊï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫ÜÊ≥õÂåñÂáÜÁ°ÆÊÄßÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17552', 'title': 'Universal Biological Sequence Reranking for Improved De Novo Peptide\n  Sequencing', 'url': 'https://huggingface.co/papers/2505.17552', 'abstract': 'RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.', 'score': 0, 'issue_id': 3961, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'bcaa51d217174ccf', 'authors': ['Zijie Qiu', 'Jiaqi Wei', 'Xiang Zhang', 'Sheng Xu', 'Kai Zou', 'Zhi Jin', 'Zhiqiang Gao', 'Nanqing Dong', 'Siqi Sun'], 'affiliations': ['Fudan University', 'NetMind.AI', 'ProtagoLabs Inc', 'Shanghai Artificial Intelligence Laboratory', 'Soochow University', 'University of British Columbia', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.17552.jpg', 'data': {'categories': ['#open_source', '#optimization', '#benchmark', '#science', '#training', '#dataset'], 'emoji': 'üß¨', 'ru': {'title': 'RankNovo: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–µ–ø—Ç–∏–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'RankNovo - —ç—Ç–æ –≥–ª—É–±–æ–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è de novo —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ–ø—Ç–∏–¥–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ –∞–∫—Å–∏–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. RankNovo –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–µ–ø—Ç–∏–¥–æ–≤ –∫–∞–∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ PMD –∏ RMD –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Å—Å–æ–≤—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –ø–µ–ø—Ç–∏–¥–∞–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ—Å—Ç–∞—Ç–∫–æ–≤.'}, 'en': {'title': 'Revolutionizing Peptide Sequencing with RankNovo', 'desc': 'RankNovo is a novel deep reranking framework designed to improve de novo peptide sequencing by integrating multiple models and axial attention mechanisms. It addresses the challenges posed by mass spectrometry data, which often contains complex noise and biases, by employing a list-wise reranking strategy that treats candidate peptides as multiple sequence alignments. The introduction of new metrics, PMD and RMD, allows for more precise supervision by measuring mass differences at both the sequence and residue levels. RankNovo not only outperforms existing models but also demonstrates impressive zero-shot generalization, making it a robust tool for peptide sequencing.'}, 'zh': {'title': 'RankNovoÔºöËÇΩÂ∫èÂàóÈáçÊéíÂ∫èÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'RankNovo ÊòØ‰∏Ä‰∏™Ê∑±Â∫¶ÈáçÊéíÂ∫èÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ§öÊ®°ÂûãÂíåËΩ¥ÂêëÊ≥®ÊÑèÂäõÊù•Â¢ûÂº∫ de novo ËÇΩÂ∫èÂàóÁöÑÊµãÂÆö„ÄÇÂΩìÂâçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊñπÊ≥ïÂú®Ë¥®Ë∞±Êï∞ÊçÆÁöÑÂ§çÊùÇÊÄßÂíåÂô™Â£∞‰ø°Âè∑ÁöÑÂºÇË¥®ÂàÜÂ∏É‰∏ãË°®Áé∞ÊúâÈôêÔºåÂØºËá¥Êï∞ÊçÆÁâπÂÆöÁöÑÂÅèÂ∑Æ„ÄÇRankNovo ÈááÁî®ÂàóË°®ÈáçÊéíÂ∫èÁöÑÊñπÊ≥ïÔºåÂ∞ÜÂÄôÈÄâËÇΩÂª∫Ê®°‰∏∫Â§ö‰∏™Â∫èÂàóÊØîÂØπÔºåÂπ∂Âà©Áî®ËΩ¥ÂêëÊ≥®ÊÑèÂäõÊèêÂèñÂÄôÈÄâËÇΩ‰πãÈó¥ÁöÑ‰ø°ÊÅØÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ÊåáÊ†á PMDÔºàËÇΩË¥®ÈáèÂÅèÂ∑ÆÔºâÂíå RMDÔºàÊÆã‰ΩôË¥®ÈáèÂÅèÂ∑ÆÔºâÔºå‰∏∫ËÇΩÂ∫èÂàóÁöÑË¥®ÈáèÂ∑ÆÂºÇÊèê‰æõÁ≤æÁªÜÁöÑÁõëÁù£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21853', 'title': 'A Survey of Interactive Generative Video', 'url': 'https://huggingface.co/papers/2504.21853', 'abstract': 'Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.', 'score': 40, 'issue_id': 3550, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '4e975f915f638955', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Hao Chen', 'Xihui Liu'], 'affiliations': ['Kuaishou Technology, Shenzhen, China', 'The Hong Kong University of Science and Technology (HKUST), Hong Kong', 'The University of Hong Kong, Pok Fu Lam, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.21853.jpg', 'data': {'categories': ['#robotics', '#interpretability', '#video', '#optimization', '#survey', '#agents', '#games', '#multimodal'], 'emoji': 'üéÆ', 'ru': {'title': 'IGV: –ë—É–¥—É—â–µ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –í–∏–¥–µ–æ (IGV), –∫–æ—Ç–æ—Ä–∞—è —Å–æ—á–µ—Ç–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ IGV –≤ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö: –∏–≥—Ä—ã, –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–π –ò–ò –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —Ä–∞–∑–±–∏–≤–∞—é—â–∞—è –∏–¥–µ–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É IGV –Ω–∞ –ø—è—Ç—å –º–æ–¥—É–ª–µ–π: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è, –ö–æ–Ω—Ç—Ä–æ–ª—å, –ü–∞–º—è—Ç—å, –î–∏–Ω–∞–º–∏–∫–∞ –∏ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç. –í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –±—É–¥—É—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ IGV.'}, 'en': {'title': 'Empowering Interactive Experiences with Generative Video', 'desc': 'Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.'}, 'zh': {'title': 'Êé®Âä®‰∫íÂä®ÁîüÊàêËßÜÈ¢ëÊäÄÊúØÁöÑÊú™Êù•ÂèëÂ±ï', 'desc': '‰∫íÂä®ÁîüÊàêËßÜÈ¢ëÔºàIGVÔºâÊòØ‰∏ÄÁßçÊñ∞ÂÖ¥ÊäÄÊúØÔºåÊó®Âú®Êª°Ë∂≥ÂØπÈ´òË¥®Èáè‰∫íÂä®ËßÜÈ¢ëÂÜÖÂÆπÁöÑÈúÄÊ±Ç„ÄÇÊú¨ÊñáÂ∞ÜIGVÂÆö‰πâ‰∏∫‰∏ÄÁßçÁªìÂêàÁîüÊàêËÉΩÂäõÂíå‰∫íÂä®ÁâπÊÄßÁöÑÊäÄÊúØÔºåËÉΩÂ§üÈÄöËøáÊéßÂà∂‰ø°Âè∑ÂíåÂèçÈ¶àÂÆûÁé∞Áî®Êà∑ÂèÇ‰∏é„ÄÇÊàë‰ª¨Ë∞ÉÊü•‰∫ÜIGVÂú®Ê∏∏Êàè„ÄÅÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÂíåËá™Âä®È©æÈ©∂Á≠â‰∏â‰∏™‰∏ªË¶ÅÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊ°ÜÊû∂ÔºåÂ∞ÜÁêÜÊÉ≥ÁöÑIGVÁ≥ªÁªüÂàÜËß£‰∏∫ÁîüÊàê„ÄÅÊéßÂà∂„ÄÅËÆ∞ÂøÜ„ÄÅÂä®ÊÄÅÂíåÊô∫ËÉΩ‰∫î‰∏™Ê®°Âùó„ÄÇÈÄöËøáÁ≥ªÁªüÂàÜÊûêÊäÄÊúØÊåëÊàòÂíåÊú™Êù•ÊñπÂêëÔºåÊú¨ÊñáÊó®Âú®Êé®Âä®IGVÈ¢ÜÂüüÁöÑÁ†îÁ©∂‰∏éÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00662', 'title': 'DeepCritic: Deliberate Critique with Large Language Models', 'url': 'https://huggingface.co/papers/2505.00662', 'abstract': 'As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.', 'score': 37, 'issue_id': 3549, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '259dddc97137d27a', 'authors': ['Wenkai Yang', 'Jingwen Chen', 'Yankai Lin', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00662.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#reasoning', '#rlhf', '#math', '#training'], 'emoji': 'üßÆ', 'ru': {'title': '–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ LLM –¥–ª—è –≥–ª—É–±–æ–∫–æ–π –∫—Ä–∏—Ç–∏–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫—Ä–∏—Ç–∏–∫–æ–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Qwen2.5-72B-Instruct –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ø–æ—à–∞–≥–æ–≤—ã—Ö –∫—Ä–∏—Ç–∏–∫, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫ –Ω–∞ –±–∞–∑–µ Qwen2.5-7B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ LLM-–∫—Ä–∏—Ç–∏–∫–∏ –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions', 'desc': "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶ÊâπËØÑËÉΩÂäõ', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÊèê‰æõÂáÜÁ°ÆÁöÑÂèçÈ¶àÂíåÂèØÊâ©Â±ïÁöÑÁõëÁù£ÂèòÂæóÂ∞§‰∏∫ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫LLMsÂú®Êï∞Â≠¶ÊâπËØÑÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÁîüÊàêËØ¶ÁªÜÁöÑÈÄêÊ≠•ÊâπËØÑÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´ÈîôËØØÂπ∂Êèê‰æõÊ∑±ÂÖ•ÁöÑÂèçÈ¶àÔºåÂ∏ÆÂä©ÁîüÊàêÊ®°ÂûãÁ∫†Ê≠£ÈîôËØØ„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊîπËøõÂêéÁöÑÊâπËØÑÊ®°ÂûãÂú®ÈîôËØØËØÜÂà´Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑLLMÊâπËØÑËÄÖ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00703', 'title': 'T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT', 'url': 'https://huggingface.co/papers/2505.00703', 'abstract': 'Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1', 'score': 28, 'issue_id': 3548, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': 'ca564761ff71d15e', 'authors': ['Dongzhi Jiang', 'Ziyu Guo', 'Renrui Zhang', 'Zhuofan Zong', 'Hao Li', 'Le Zhuo', 'Shilin Yan', 'Pheng-Ann Heng', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'CUHK MiuLar Lab', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.00703.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'üé®', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–≤—É—Ö —É—Ä–æ–≤–Ω—è—Ö —É–ª—É—á—à–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç T2I-R1 - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, —É–ª—É—á—à–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞ –∏ —Ç–æ–∫–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –ø–æ–ø–∏–∫—Å–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ BiCoT-GRPO –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±–æ–∏—Ö —É—Ä–æ–≤–Ω–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Janus-Pro –ø–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö T2I-CompBench –∏ WISE.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Enhanced Reasoning', 'desc': 'This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests.'}, 'zh': {'title': 'ÂèåÂ±ÇÊÄùÁª¥ÈìæÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãT2I-R1ÔºåËØ•Ê®°ÂûãÁªìÂêà‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÂíåÂèåÂ±ÇÊÄùÁª¥ÈìæÊé®ÁêÜËøáÁ®ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊÄùÁª¥ÈìæÔºöËØ≠‰πâÂ±ÇÊ¨°ÁöÑÊÄùÁª¥ÈìæÁî®‰∫éÈ´òÂ±ÇÊ¨°ÁöÑÊèêÁ§∫ËßÑÂàíÔºå‰ª§ÁîüÊàêËøáÁ®ãÊõ¥ÂÖ∑ÈÄªËæëÊÄßÔºõËÄå‰ª§ÁâåÂ±ÇÊ¨°ÁöÑÊÄùÁª¥ÈìæÂàôÁî®‰∫éÂú®ÈÄêÂùóÁîüÊàêËøáÁ®ã‰∏≠ËøõË°å‰ΩéÂ±ÇÊ¨°ÁöÑÂÉèÁ¥†Â§ÑÁêÜ„ÄÇÈÄöËøáÂºïÂÖ•BiCoT-GRPOÔºåÊàë‰ª¨ËÉΩÂ§üÂú®Âêå‰∏ÄËÆ≠ÁªÉÊ≠•È™§‰∏≠‰ºòÂåñËøô‰∏§ÁßçÊÄùÁª¥ÈìæÔºå‰ªéËÄåÊèêÂçáÁîüÊàêÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåT2I-R1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00497', 'title': 'KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution', 'url': 'https://huggingface.co/papers/2505.00497', 'abstract': 'Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.', 'score': 9, 'issue_id': 3554, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': 'dc645b5c7e476cea', 'authors': ['Antoni Bigata', 'Rodrigo Mira', 'Stella Bounareli', 'Micha≈Ç Stypu≈Çkowski', 'Konstantinos Vougioukas', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London', 'University of Wroc≈Çaw'], 'pdf_title_img': 'assets/pdf/title_img/2505.00497.jpg', 'data': {'categories': ['#cv', '#leakage', '#video', '#open_source', '#architecture'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'KeySync: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≥—É–± –¥–ª—è –≤–∏–¥–µ–æ', 'desc': 'KeySync - —ç—Ç–æ –Ω–æ–≤–∞—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≥—É–± –≤ –≤–∏–¥–µ–æ —Å –Ω–æ–≤—ã–º –∞—É–¥–∏–æ. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, —É—Ç–µ—á–∫–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–∫–∫–ª—é–∑–∏–π –ª–∏—Ü–∞ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è. KeySync –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≥—É–± –∏ –∫—Ä–æ—Å—Å-—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏, —É–ª—É—á—à–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —É–º–µ–Ω—å—à–∞—è —É—Ç–µ—á–∫—É –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–µ LipLeak. –°–∏—Å—Ç–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –æ–∫–∫–ª—é–∑–∏—è–º–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª—è–∂.'}, 'en': {'title': 'KeySync: Mastering Lip Synchronization with Precision', 'desc': 'This paper introduces KeySync, a two-stage framework designed to improve lip synchronization in videos by aligning lip movements with new audio inputs. It addresses common challenges in talking head generation, such as maintaining temporal consistency and managing expression leakage and facial occlusions. KeySync employs a unique masking strategy to effectively tackle these issues, resulting in enhanced visual quality and reduced leakage as measured by a new metric called LipLeak. The framework demonstrates state-of-the-art performance in lip reconstruction and cross-synchronization, validated through various ablation studies.'}, 'zh': {'title': 'KeySyncÔºöÊèêÂçáÂîáÈÉ®ÂêåÊ≠•ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫KeySyncÁöÑÂèåÈò∂ÊÆµÊ°ÜÊû∂ÔºåÁî®‰∫éËß£ÂÜ≥ÂîáÈÉ®ÂêåÊ≠•‰∏≠ÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïËøòÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÈÅÆÁΩ©Á≠ñÁï•ÔºåËß£ÂÜ≥‰∫ÜËæìÂÖ•ËßÜÈ¢ë‰∏≠ÁöÑË°®ÊÉÖÊ≥ÑÊºèÂíåÈù¢ÈÉ®ÈÅÆÊå°Á≠âÊñ∞ÊåëÊàò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKeySyncÂú®ÂîáÈÉ®ÈáçÂª∫Âíå‰∫§ÂèâÂêåÊ≠•ÊñπÈù¢ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊïàÊûúÔºåÊòæËëóÊèêÈ´ò‰∫ÜËßÜËßâË¥®ÈáèÂπ∂ÂáèÂ∞ë‰∫ÜË°®ÊÉÖÊ≥ÑÊºè„ÄÇÊàë‰ª¨ËøòÈÄöËøáÂ§öÈ°πÊ∂àËûçÁ†îÁ©∂È™åËØÅ‰∫ÜÊñ∞ÈÅÆÁΩ©ÊñπÊ≥ïÂú®Â§ÑÁêÜÈÅÆÊå°ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00234', 'title': 'Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks', 'url': 'https://huggingface.co/papers/2505.00234', 'abstract': 'Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.', 'score': 9, 'issue_id': 3563, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '9e975cfc8ecf9dea', 'authors': ['Vishnu Sarukkai', 'Zhiqiang Xie', 'Kayvon Fatahalian'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00234.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#rl', '#agents', '#optimization', '#benchmark', '#games'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –æ–ø—ã—Ç–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –∑–∞–¥–∞—á –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –í–º–µ—Å—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ —Å—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å—Ä–∞–≤–Ω–∏–º–æ–µ —Å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–º–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.'}, 'en': {'title': 'Learning from Success: Automating Improvement in LLM Agents', 'desc': 'This paper explores a new approach for improving Large Language Model (LLM) agents in sequential decision-making tasks without relying on extensive task-specific knowledge engineering. Instead of manually tuning prompts or creating curated examples, the authors propose that LLM agents can learn from their own successful experiences by building a database of self-generated examples. The study shows that simply accumulating successful task trajectories can significantly enhance performance on various benchmarks. Additionally, the paper introduces methods for selecting high-performing examples from this database, leading to even better results that rival more complex, knowledge-intensive methods.'}, 'zh': {'title': 'Ëá™Âä®Â≠¶‰π†ÊèêÂçáLLM‰ª£ÁêÜÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáËá™ÊàëÁîüÊàêÁöÑÊàêÂäüÁªèÈ™åÊù•Ëá™Âä®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®È°∫Â∫èÂÜ≥Á≠ñ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫éÁâπÂÆö‰ªªÂä°ÁöÑÁü•ËØÜÂ∑•Á®ã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁÆÄÂçïÂú∞ÁßØÁ¥ØÊàêÂäüÁöÑËΩ®ËøπÂèØ‰ª•ÊòæËëóÊèêÈ´òÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏§ÁßçÊâ©Â±ïÊñπÊ≥ïÔºöÈÄöËøáÂü∫‰∫éÁßçÁæ§ÁöÑËÆ≠ÁªÉÈÄâÊã©È´òÊïàÁöÑÁ§∫‰æãÈõÜÂêàÔºå‰ª•ÂèäÊ†πÊçÆÂÆûËØÅÊïàÁî®‰øùÁïô‰∏™Âà´ËΩ®Ëøπ„ÄÇËøô‰∫õÊñπÊ≥ïËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜËá™Âä®ËΩ®ËøπÊï∞ÊçÆÂ∫ìÊûÑÂª∫‰Ωú‰∏∫Áü•ËØÜÂ∑•Á®ãÁöÑÊúâÊïàÊõø‰ª£ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.18715', 'title': 'Spatial Speech Translation: Translating Across Space With Binaural\n  Hearables', 'url': 'https://huggingface.co/papers/2504.18715', 'abstract': "Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.", 'score': 7, 'issue_id': 3566, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 –∞–ø—Ä–µ–ª—è', 'en': 'April 25', 'zh': '4Êúà25Êó•'}, 'hash': '979ba773cd3e90a0', 'authors': ['Tuochao Chen', 'Qirui Wang', 'Runlin He', 'Shyam Gollakota'], 'affiliations': ['Paul G. Allen School, University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.18715.jpg', 'data': {'categories': ['#audio', '#multilingual', '#machine_translation'], 'emoji': 'üéß', 'ru': {'title': '–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Ä–µ—á–∏: —Å–ª—ã—à–∞—Ç—å –º–∏—Ä –Ω–∞ —Å–≤–æ–µ–º —è–∑—ã–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ –¥–ª—è —Å–ª—É—Ö–æ–≤—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ä–µ—á—å –æ–∫—Ä—É–∂–∞—é—â–∏—Ö –ª—é–¥–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≥–æ–ª–æ—Å–∞ –∫–∞–∂–¥–æ–≥–æ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –≤ –±–∏–Ω–∞—É—Ä–∞–ª—å–Ω–æ–º –≤—ã–≤–æ–¥–µ. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ —Å–ª–µ–ø–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏, –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –±–∏–Ω–∞—É—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞. –ü—Ä–æ—Ç–æ—Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —É—Å–ª–æ–≤–∏—è—Ö —Å–∏–ª—å–Ω—ã—Ö –ø–æ–º–µ—Ö –∏ —Ä–µ–≤–µ—Ä–±–µ—Ä–∞—Ü–∏–∏, –¥–æ—Å—Ç–∏–≥–∞—è –æ—Ü–µ–Ω–∫–∏ BLEU –¥–æ 22.01 –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏.'}, 'en': {'title': 'Transforming Speech Translation with Spatial Awareness', 'desc': "This paper presents a new approach to spatial speech translation using hearables, which are devices that can translate spoken language in real-time while preserving the spatial characteristics of the speakers. The authors address key challenges such as separating overlapping voices, accurately locating speakers, and ensuring that translations are expressive and timely, all while running efficiently on Apple M2 hardware. Their prototype demonstrates significant improvements over existing models, achieving a BLEU score of 22.01 in noisy environments, indicating high translation quality. User studies validate the system's ability to maintain the spatial cues of translated speech, marking a significant advancement in integrating spatial awareness into speech translation technology."}, 'zh': {'title': 'Á©∫Èó¥ËØ≠Èü≥ÁøªËØëÔºöËÆ©‰∫§ÊµÅÊó†ÈöúÁ¢ç', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ©∫Èó¥ËØ≠Èü≥ÁøªËØëÊ¶ÇÂøµÔºåÊó®Âú®ÈÄöËøáËÄ≥Êú∫Â∞ÜÁéØÂ¢É‰∏≠ËØ¥ËØùËÄÖÁöÑËØ≠Ë®ÄÂÆûÊó∂ÁøªËØë‰∏∫‰Ω©Êà¥ËÄÖÁöÑÊØçËØ≠ÔºåÂêåÊó∂‰øùÁïôÊØè‰ΩçËØ¥ËØùËÄÖÁöÑÊñπÂêëÂíåÁã¨ÁâπÂ£∞Èü≥ÁâπÂæÅ„ÄÇ‰∏∫ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÔºåÁ†îÁ©∂Ëß£ÂÜ≥‰∫ÜÂ§ö‰∏™ÊäÄÊúØÊåëÊàòÔºåÂåÖÊã¨Áõ≤Ê∫êÂàÜÁ¶ª„ÄÅÂÆö‰Ωç„ÄÅÂÆûÊó∂Ë°®ËææÁøªËØëÂíåÂèåËÄ≥Ê∏≤ÊüìÔºå‰ª•Á°Æ‰øùÁøªËØëÈü≥È¢ë‰∏≠ÁöÑËØ¥ËØùËÄÖÊñπÂêëÂæó‰ª•‰øùÁïô„ÄÇÈÄöËøáÂéüÂûãÂèåËÄ≥ËÄ≥Êú∫ÁöÑÊ¶ÇÂøµÈ™åËØÅËØÑ‰º∞ÔºåÁ†îÁ©∂Ë°®ÊòéËØ•Á≥ªÁªüÂú®Âº∫Âπ≤Êâ∞ÁéØÂ¢É‰∏≠‰ªçËÉΩÂÆûÁé∞È´òËææ22.01ÁöÑBLEUÂàÜÊï∞Ôºå‰ºò‰∫éÁé∞ÊúâÊ®°Âûã„ÄÇÁî®Êà∑Á†îÁ©∂Ëøõ‰∏ÄÊ≠•Á°ÆËÆ§‰∫ÜËØ•Á≥ªÁªüÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠Á©∫Èó¥Ê∏≤ÊüìÁøªËØëËØ≠Èü≥ÁöÑÊúâÊïàÊÄßÔºåÊ†áÂøóÁùÄÂ∞ÜÁ©∫Èó¥ÊÑüÁü•Êï¥ÂêàÂà∞ËØ≠Èü≥ÁøªËØë‰∏≠ÁöÑÁ¨¨‰∏ÄÊ≠•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21659', 'title': 'AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization', 'url': 'https://huggingface.co/papers/2504.21659', 'abstract': 'Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1', 'score': 6, 'issue_id': 3549, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '6487e5a67faf07a5', 'authors': ['Haotian Luo', 'Haiying He', 'Yibo Wang', 'Jinluan Yang', 'Rui Liu', 'Naiqiang Tan', 'Xiaochun Cao', 'Dacheng Tao', 'Li Shen'], 'affiliations': ['China Agricultural University', 'Didichuxing Co. Ltd', 'Nanyang Technological University', 'Sun Yat-sen University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21659.jpg', 'data': {'categories': ['#reasoning', '#inference', '#math', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –≤–∫–ª—é—á–∞—é—â—É—é –≥–∏–±—Ä–∏–¥–Ω—É—é –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –±–∏-—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞ –ø—è—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å–æ–∫—Ä–∞—Ç–∏–ª–∞—Å—å –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 50%.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Inference in Complex Tasks', 'desc': 'This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊé®ÁêÜÔºåÊèêÂçáÊïàÁéáÔºÅ', 'desc': 'ÊúÄËøëÔºåÈïøÊé®ÁêÜÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂ∏∏Â∏∏ÂØºËá¥Êé®ÁêÜÂºÄÈîÄÂ§ßÔºåÂõ†Ê≠§ÊïàÁéáÊàê‰∏∫‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÂÆûËØÅÂàÜÊûêÊòæÁ§∫Ôºå‰ΩøÁî®ÈïøÈìæÊé®ÁêÜÔºàLong-CoTÔºâÁöÑÂ•ΩÂ§ÑÂú®‰∏çÂêåÈóÆÈ¢ò‰∏äÂ∑ÆÂºÇÂæàÂ§ßÔºöÊúâ‰∫õÈóÆÈ¢òÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÔºåËÄåÂÖ∂‰ªñÈóÆÈ¢òÂàôÊ≤°ÊúâÊîπÂñÑÔºåÁîöËá≥ÂáÜÁ°ÆÁéá‰∏ãÈôç„ÄÇËøô‰øÉ‰ΩøÊàë‰ª¨ÊèêÂá∫Ëá™ÈÄÇÂ∫îÊé®ÁêÜÁ≠ñÁï•ÔºåÊ†πÊçÆËæìÂÖ•Ë∞ÉÊï¥Êé®ÁêÜÊ∑±Â∫¶„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÈÄöËøáÊ∑∑ÂêàÈïøÁü≠ÈìæÊé®ÁêÜÊ®°ÂûãÂíåÂèåÂ±ÇÂÅèÂ•ΩËÆ≠ÁªÉÔºåÊòæËëóÈôç‰ΩéÊé®ÁêÜÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.19394', 'title': 'LLMs for Engineering: Teaching Models to Design High Powered Rockets', 'url': 'https://huggingface.co/papers/2504.19394', 'abstract': "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.", 'score': 6, 'issue_id': 3558, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 –∞–ø—Ä–µ–ª—è', 'en': 'April 27', 'zh': '4Êúà27Êó•'}, 'hash': '5b65f8f309063f13', 'authors': ['Toby Simonds'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.19394.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#agi', '#benchmark', '#games', '#reasoning'], 'emoji': 'üöÄ', 'ru': {'title': '–ò–ò —Å –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ª—é–¥–µ–π –≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–∞–∫–µ—Ç', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ—â–Ω—ã—Ö —Ä–∞–∫–µ—Ç —Å –ø–æ–º–æ—â—å—é –±–µ–Ω—á–º–∞—Ä–∫–∞ RocketBench. –ú–æ–¥–µ–ª–∏ —Ç–µ—Å—Ç–∏—Ä—É—é—Ç—Å—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ü–µ–ª–µ–≤–æ–π –≤—ã—Å–æ—Ç—ã –∏ —Ç–æ—á–Ω–æ–π –ø–æ—Å–∞–¥–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ –±–∞–∑–æ–≤—ã–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –Ω–æ –∑–∞—Ç—Ä—É–¥–Ω—è—é—Ç—Å—è —Å –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–π. –û–¥–Ω–∞–∫–æ –ø—Ä–∏ —É—Å–∏–ª–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –º–æ–¥–µ–ª—å —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–∞–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫ –∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤-–ª—é–¥–µ–π.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Rocket Design Challenges', 'desc': 'This paper investigates the use of Large Language Models (LLMs) in the field of physical engineering, specifically in high-powered rocketry design. It introduces RocketBench, a benchmark that connects LLMs to advanced rocket simulations, and evaluates their performance on design tasks like optimizing target altitude and achieving precision landings. The results indicate that while LLMs possess a solid foundation in engineering knowledge, they struggle to improve their designs based on simulation feedback, often falling short of human capabilities. However, by incorporating reinforcement learning, a 7B parameter model surpasses both state-of-the-art models and human experts, highlighting the potential of RL-enhanced LLMs in complex engineering optimization tasks.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÁÅ´ÁÆ≠ËÆæËÆ°ÔºåË∂ÖË∂ä‰∫∫Á±ª‰∏ìÂÆ∂ÔºÅ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËΩØ‰ª∂Â∑•Á®ã‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®Áâ©ÁêÜÂ∑•Á®ãÈ¢ÜÂüüÁöÑÂ∫îÁî®‰ªçÁÑ∂‰∏çÂ§üÊ∑±ÂÖ•„ÄÇÊú¨ÊñáÈÄöËøáRocketBenchÂü∫ÂáÜËØÑ‰º∞LLMsÂú®È´òÂäüÁéáÁÅ´ÁÆ≠ËÆæËÆ°‰∏≠ÁöÑËÉΩÂäõÔºåËøûÊé•LLMs‰∏éÈ´ò‰øùÁúüÁÅ´ÁÆ≠Ê®°Êãü„ÄÇÊàë‰ª¨ÊµãËØï‰∫Ü‰∏§‰∏™Â§çÊùÇÁöÑËÆæËÆ°‰ªªÂä°ÔºöÁõÆÊ†áÈ´òÂ∫¶‰ºòÂåñÂíåÁ≤æÁ°ÆÁùÄÈôÜÊåëÊàò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°ÊúÄÂÖàËøõÁöÑLLMsÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÂ∑•Á®ãÁü•ËØÜÔºå‰ΩÜÂú®Ê†πÊçÆÊ®°ÊãüÁªìÊûúËø≠‰ª£ËÆæËÆ°Êó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊúÄÁªàÊú™ËÉΩË∂ÖË∂ä‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑÊ∞¥Âπ≥ÔºõÁÑ∂ËÄåÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂ¢ûÂº∫ÂêéÔºå7BÂèÇÊï∞Ê®°ÂûãÁöÑË°®Áé∞Ë∂ÖËøá‰∫ÜÁé∞ÊúâÂü∫Á°ÄÊ®°ÂûãÂíå‰∫∫Á±ª‰∏ìÂÆ∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.18983', 'title': 'MediAug: Exploring Visual Augmentation in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.18983', 'abstract': 'Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.', 'score': 5, 'issue_id': 3554, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 –∞–ø—Ä–µ–ª—è', 'en': 'April 26', 'zh': '4Êúà26Êó•'}, 'hash': '13143b6fb9ae9216', 'authors': ['Xuyin Qi', 'Zeyu Zhang', 'Canxuan Gang', 'Hao Zhang', 'Lei Zhang', 'Zhiwei Zhang', 'Yang Zhao'], 'affiliations': ['AIML', 'ANU', 'La Trobe', 'PSU', 'UCAS', 'UNSW'], 'pdf_title_img': 'assets/pdf/title_img/2504.18983.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#healthcare', '#synthetic', '#training'], 'emoji': 'ü©∫', 'ru': {'title': 'MediAug: –ù–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MediAug - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —à–µ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–∏–º–µ–Ω—è—è –∏—Ö –∫ –∑–∞–¥–∞—á–∞–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–ø—É—Ö–æ–ª–µ–π –º–æ–∑–≥–∞ –∏ –≥–ª–∞–∑–Ω—ã—Ö –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–∞–∫ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (ResNet-50), —Ç–∞–∫ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (ViT-B). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –∑–∞–¥–∞—á, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏.'}, 'en': {'title': 'Enhancing Medical Imaging with Advanced Data Augmentation', 'desc': 'This paper addresses the challenges of data augmentation in medical imaging, particularly the domain gap between natural images and medical scans. It introduces MediAug, a benchmark framework that evaluates six mix-based augmentation methods across different neural network architectures. The study finds that MixUp and SnapMix significantly enhance classification accuracy for brain tumors and eye diseases, respectively. By providing a systematic evaluation, the paper clarifies the effectiveness of various augmentation strategies in improving medical image analysis.'}, 'zh': {'title': 'ÂåªÂ≠¶ÂΩ±ÂÉèÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êï∞ÊçÆÂ¢ûÂº∫Âú®ÂåªÂ≠¶ÂΩ±ÂÉè‰∏≠ÂØπ‰∫éÊèêÈ´òÂàÜÁ±ªÂáÜÁ°ÆÊÄß„ÄÅÁóÖÂèòÊ£ÄÊµãÂíåÂô®ÂÆòÂàÜÂâ≤Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÁÑ∂ËÄåÔºåÂåªÂ≠¶ÂΩ±ÂÉè‰∏éËá™ÁÑ∂ÁÖßÁâá‰πãÈó¥ÁöÑÊòæËëóÈ¢ÜÂüüÂ∑ÆË∑ùÂèØËÉΩ‰ºöÊâ≠Êõ≤ÂÖ≥ÈîÆÁöÑÁñæÁóÖÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÁöÑÂ¢ûÂº∫Á†îÁ©∂ÂæÄÂæÄÂ±ÄÈôê‰∫éÂçï‰∏Ä‰ªªÂä°ÊàñÊû∂ÊûÑÔºåÂØºËá¥ÂÖàËøõÁöÑÊ∑∑ÂêàÁ≠ñÁï•ÁöÑ‰ºòÂäø‰∏çÊòéÁ°Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÊï¥Âêà‰∫ÜÂÖ≠ÁßçÂü∫‰∫éÊ∑∑ÂêàÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂπ∂Âú®ËÑëËÇøÁò§MRIÂíåÁúºÁóÖËßÜÁΩëËÜúÂõæÂÉèÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20605', 'title': 'TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models', 'url': 'https://huggingface.co/papers/2504.20605', 'abstract': 'Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.', 'score': 4, 'issue_id': 3554, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': '9637fef0c8d474ed', 'authors': ['Mihai Nadas', 'Laura Diosan', 'Andrei Piscoran', 'Andreea Tomescu'], 'affiliations': ['Babes-Bolyai University', 'KlusAI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.20605.jpg', 'data': {'categories': ['#story_generation', '#ethics', '#multimodal', '#dataset', '#benchmark', '#alignment', '#open_source'], 'emoji': 'üìö', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ –º–æ—Ä–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –±–µ–∑ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ TF1-EN-3M - –ø–µ—Ä–≤—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ç—Ä–µ—Ö –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö –±–∞—Å–µ–Ω, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è–º–∏ –¥–æ 8 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ö–∞–∂–¥–∞—è –∏—Å—Ç–æ—Ä–∏—è —Å–ª–µ–¥—É–µ—Ç —à–µ—Å—Ç–∏—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, –æ—Ö–≤–∞—Ç—ã–≤–∞—è —à–∏—Ä–æ–∫–∏–π —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–µ–∫—Ç—Ä. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—Ä–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, —Å–æ—á–µ—Ç–∞—é—â–µ–≥–æ –∫—Ä–∏—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ GPT –∏ –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏. –î–∞—Ç–∞—Å–µ—Ç, –∫–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–∫—Ä–∏–ø—Ç—ã –æ—Ü–µ–Ω–∫–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø–æ–¥ —Å–≤–æ–±–æ–¥–Ω–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π, –æ—Ç–∫—Ä—ã–≤–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ò–ò.'}, 'en': {'title': 'Empowering Moral Storytelling with TF1-EN-3M', 'desc': 'This paper introduces TF1-EN-3M, a novel dataset containing three million English fables designed to teach moral lessons, generated by smaller instruction-tuned models. The stories are structured using a six-slot framework that ensures each narrative includes essential elements like characters, traits, and morals. A unique evaluation system combines assessments from a GPT-based critic and various metrics to ensure quality and diversity in the generated fables. The dataset and associated tools are made publicly available, promoting further research in areas like narrative intelligence and ethical AI development.'}, 'zh': {'title': 'ÈÅìÂæ∑ÊïÖ‰∫ãÁîüÊàêÁöÑÊñ∞Á∫™ÂÖÉ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫ÜTF1-EN-3MÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂåÖÂê´‰∏âÁôæ‰∏á‰∏™Ëã±ËØ≠ÂØìË®ÄÁöÑÂºÄÊîæÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî±Êåá‰ª§Ë∞É‰ºòÊ®°ÂûãÁîüÊàê„ÄÇÊØè‰∏™ÊïÖ‰∫ãÈÅµÂæ™ÂÖ≠‰∏™ÈÉ®ÂàÜÁöÑÁªìÊûÑÔºåÂåÖÊã¨ËßíËâ≤„ÄÅÁâπÂæÅ„ÄÅËÉåÊôØ„ÄÅÂÜ≤Á™Å„ÄÅËß£ÂÜ≥ÊñπÊ°àÂíåÈÅìÂæ∑ÔºåÁ°Æ‰øù‰∫ÜÊïÖ‰∫ãÁöÑËøûË¥ØÊÄßÂíå‰∏ªÈ¢òÁöÑÂ§öÊ†∑ÊÄß„ÄÇËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàËØÑ‰º∞ÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜÂü∫‰∫éGPTÁöÑËØÑÂàÜÂíåÊó†ÂèÇËÄÉÁöÑÂ§öÊ†∑ÊÄß‰∏éÂèØËØªÊÄßÊåáÊ†á„ÄÇTF1-EN-3M‰∏∫ÈÅìÂæ∑ÊïÖ‰∫ãÁöÑÁîüÊàêÂíåÊïôËÇ≤AIÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºåË°®ÊòéÂ§ßËßÑÊ®°ÁöÑÈÅìÂæ∑Âèô‰∫ã‰∏çÂÜçÈúÄË¶Å‰∏ìÊúâÁöÑÂ§ßÂûãÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20406', 'title': 'Skill Discovery for Software Scripting Automation via Offline\n  Simulations with LLMs', 'url': 'https://huggingface.co/papers/2504.20406', 'abstract': "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.", 'score': 3, 'issue_id': 3570, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': '995b07a7273b51fd', 'authors': ['Paiheng Xu', 'Gang Wu', 'Xiang Chen', 'Tong Yu', 'Chang Xiao', 'Franck Dernoncourt', 'Tianyi Zhou', 'Wei Ai', 'Viswanathan Swaminathan'], 'affiliations': ['Adobe Research', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.20406.jpg', 'data': {'categories': ['#agents', '#dataset', '#alignment', '#graphs', '#games', '#architecture', '#data'], 'emoji': 'ü§ñ', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –±–µ–∑ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: LLM —Å–æ–∑–¥–∞—é—Ç —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∫—Ä–∏–ø—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ñ–ª–∞–π–Ω-—Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤ –ø–æ —Å–∫—Ä–∏–ø—Ç–∏–Ω–≥—É. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–≤—ã–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–æ–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (GNN) –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É API. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å Adobe Illustrator –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏, —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞ –∏ —ç–∫–æ–Ω–æ–º–∏—é –∑–∞—Ç—Ä–∞—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∫–æ–¥–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Users with Automated Scripting through AI', 'desc': 'This paper presents a framework that helps users automate tasks in software without needing deep programming skills. It uses Large Language Models (LLMs) to generate verified scripts by simulating tasks and refining them based on execution feedback. The framework includes a Graph Neural Network (GNN) to explore API synergies, which helps create a diverse set of useful scripts. Experiments show that this approach improves automation success rates and reduces costs compared to traditional methods.'}, 'zh': {'title': 'Âà©Áî®LLMÊèêÂçáËΩØ‰ª∂ËÑöÊú¨Ëá™Âä®ÂåñÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ¶ªÁ∫øÊ®°ÊãüÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂÖ¨ÂºÄÁöÑËÑöÊú¨ÊåáÂçóÔºåÂàõÂª∫‰∏Ä‰∏™ËΩØ‰ª∂ÁâπÂÆöÁöÑÊäÄËÉΩÈõÜ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºö‰ªªÂä°ÂàõÂª∫ÂíåÊäÄËÉΩÁîüÊàêÔºåÈÄöËøáÊâßË°åÂèçÈ¶àÊù•‰ºòÂåñÂíåÈ™åËØÅËÑöÊú¨„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÁöÑÈìæÊé•È¢ÑÊµãÊ®°ÂûãÔºå‰ª•ÊçïÊçâAPI‰πãÈó¥ÁöÑÂçèÂêå‰ΩúÁî®Ôºå‰ªéËÄåÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊäÄËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Ëá™Âä®ÂåñÊàêÂäüÁéá„ÄÅÂìçÂ∫îÊó∂Èó¥ÂíåËøêË°åÊó∂ÊàêÊú¨ÊñπÈù¢ÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑ‰ª£Á†ÅÁîüÊàêÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00534', 'title': 'A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic', 'url': 'https://huggingface.co/papers/2505.00534', 'abstract': 'Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.', 'score': 2, 'issue_id': 3560, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': 'd411bc0f9d34adf4', 'authors': ['Muhammad Imran Zaman', 'Usama Ijaz Bajwa', 'Gulshan Saleem', 'Rana Hammad Raza'], 'affiliations': ['Department of Computer Science, COMSATS University Islamabad, Lahore Campus, Lahore, Pakistan', 'Pakistan Navy Engineering College, National University of Sciences and Technology (NUST), Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2505.00534.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#optimization', '#video', '#transfer_learning'], 'emoji': 'üöó', 'ru': {'title': '–£–º–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–º–µ—Ä –≤ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è Mask R-CNN –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ ResNet-152 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–∫–∫–ª—é–∑–∏–∏, –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Å–≤–µ—â–µ–Ω–∏—è –∏ —Ç–µ–Ω–µ–π —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –∏ –º–µ—Ç—Ä–∏–∫ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö AI City Challenge, –¥–æ—Å—Ç–∏–≥–Ω—É–≤ IDF1 0.8289.'}, 'en': {'title': 'Revolutionizing Traffic Monitoring with Deep Learning Tracking', 'desc': 'This paper presents a deep learning framework for Multi-Object Multi-Camera Tracking (MO-MCT) aimed at improving traffic monitoring in Intelligent Transportation Systems. The framework utilizes Mask R-CNN for detecting vehicles and applies Non-Maximum Suppression (NMS) to manage overlapping detections. It incorporates transfer learning for vehicle re-identification, allowing for the tracking of vehicles across different cameras despite challenges like occlusions and varying lighting conditions. The system is evaluated on a large dataset and shows strong performance metrics, indicating its potential for real-world traffic applications.'}, 'zh': {'title': 'Êô∫ËÉΩ‰∫§ÈÄö‰∏≠ÁöÑÈ´òÊïàÂ§öÊëÑÂÉèÂ§¥Ë∑üË∏™Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'ÈöèÁùÄÁΩëÁªúÊëÑÂÉèÂ§¥Êï∞ÈáèÁöÑÂ¢ûÂä†ÔºåËßÜËßâ‰º†ÊÑüÂô®Âú®Êô∫ËÉΩ‰∫§ÈÄöÁ≥ªÁªü‰∏≠ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÂ¢ûÂº∫„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂ§öÁõÆÊ†áÂ§öÊëÑÂÉèÂ§¥Ë∑üË∏™Ê°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÂüéÂ∏Ç‰∫§ÈÄöÂú∫ÊôØ‰∏≠ÊâãÂä®ÁõÆÊ†áË∑üË∏™ÂíåÂåπÈÖçÁöÑÊåëÊàò„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Mask R-CNNËøõË°åÁõÆÊ†áÊ£ÄÊµãÔºåÂπ∂ÈÄöËøáÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºàNMSÔºâÈÄâÊã©ÈáçÂè†Ê£ÄÊµã‰∏≠ÁöÑÁõÆÊ†á„ÄÇÈÄöËøáËøÅÁßªÂ≠¶‰π†ÂÆûÁé∞ËΩ¶ËæÜÁöÑÈáçÊñ∞ËØÜÂà´ÔºåÁªìÂêàÈÄÇÂΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåË∑ùÁ¶ªÂ∫¶ÈáèÔºåÊúÄÁªàÂú®AI City ChallengeÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17894', 'title': 'Mutarjim: Advancing Bidirectional Arabic-English Translation with a\n  Small Language Model', 'url': 'https://huggingface.co/papers/2505.17894', 'abstract': 'Mutarjim is a compact Arabic-English translation model that outperforms larger models on established benchmarks and achieves state-of-the-art performance on a new comprehensive Tarjama-25 benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.', 'score': 172, 'issue_id': 3974, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '9d9d296ef92faffb', 'authors': ['Khalil Hennara', 'Muhammad Hreden', 'Mohamed Motaism Hamed', 'Zeina Aldallal', 'Sara Chrouf', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.17894.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#machine_translation', '#multilingual', '#small_models', '#open_source'], 'emoji': 'üåç', 'ru': {'title': '–ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å - –±–æ–ª—å—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∞—Ä–∞–±—Å–∫–æ-–∞–Ω–≥–ª–∏–π—Å–∫–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ', 'desc': 'Mutarjim - —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –∞—Ä–∞–±—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Kuwain-1.5B. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä, –æ–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä—è–¥–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –¥–≤—É—Ö—Ñ–∞–∑–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω–æ–º—É –∫–æ—Ä–ø—É—Å—É –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Tarjama-25, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 5000 —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤. –ù–∞ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ Mutarjim –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –∞—Ä–∞–±—Å–∫–∏–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ —Ç–∞–∫–∏–µ –∫—Ä—É–ø–Ω—ã–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ GPT-4.'}, 'en': {'title': 'Mutarjim: Compact Power in Arabic-English Translation', 'desc': 'Mutarjim is a compact language model designed for Arabic-English translation that achieves superior performance compared to larger models. It utilizes a two-phase training approach and a high-quality training corpus, allowing it to excel on established benchmarks. The introduction of the Tarjama-25 benchmark addresses previous limitations in Arabic-English translation datasets, providing a more diverse and comprehensive evaluation. Mutarjim not only rivals models up to 20 times its size but also significantly reduces computational costs, making it an efficient choice for translation tasks.'}, 'zh': {'title': 'Â∞èÊ®°ÂûãÔºåÂ§ßËÉΩÂäõÔºöMutarjimÁöÑÁøªËØëÈù©ÂëΩ', 'desc': 'MutarjimÊòØ‰∏ÄÁßçÁ¥ßÂáëÁöÑÈòøÊãâ‰ºØËØ≠-Ëã±ËØ≠ÂèåÂêëÁøªËØëÊ®°ÂûãÔºåÂ∞ΩÁÆ°‰ΩìÁßØËæÉÂ∞èÔºå‰ΩÜÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÊõ¥Â§ßÁöÑÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÂü∫‰∫éKuwain-1.5BÊûÑÂª∫ÔºåÈááÁî®‰∫Ü‰ºòÂåñÁöÑ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊñπÊ≥ïÂíåÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ì„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMutarjimÂú®ËÆ°ÁÆóÊàêÊú¨ÂíåËÆ≠ÁªÉÈúÄÊ±Ç‰∏äÊòæËëóÈôç‰ΩéÔºåÂêåÊó∂Âú®Tarjama-25Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇTarjama-25ÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®ÂÖãÊúçÁé∞ÊúâÈòøÊãâ‰ºØËØ≠-Ëã±ËØ≠Êï∞ÊçÆÈõÜÁöÑÂ±ÄÈôêÊÄßÔºåÊèê‰æõÊõ¥ÂÖ®Èù¢ÁöÑËØÑ‰º∞Ê°ÜÊû∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19147', 'title': 'Shifting AI Efficiency From Model-Centric to Data-Centric Compression', 'url': 'https://huggingface.co/papers/2505.19147', 'abstract': "The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement.", 'score': 120, 'issue_id': 3968, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '74f14f3f2d4f73b6', 'authors': ['Xuyang Liu', 'Zichen Wen', 'Shaobo Wang', 'Junjie Chen', 'Zhishan Tao', 'Yubo Wang', 'Xiangqi Jin', 'Chang Zou', 'Yiyu Wang', 'Chenfei Liao', 'Xu Zheng', 'Honggang Chen', 'Weijia Li', 'Xuming Hu', 'Conghui He', 'Linfeng Zhang'], 'affiliations': ['EPIC Lab, Shanghai Jiao Tong University', 'Hong Kong University of Science and Technology (Guangzhou)', 'Shanghai AI Laboratory', 'Sichuan University', 'Sun Yat-sen University', 'University of Electronic Science & Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.19147.jpg', 'data': {'categories': ['#training', '#data', '#math', '#optimization', '#long_context', '#survey'], 'emoji': 'üóúÔ∏è', 'ru': {'title': '–°–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫ —Å–∂–∞—Ç–∏—é –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–∑–≤–∏—Ç–∏—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Å–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–æ–≤—ã–º —Ñ—Ä–æ–Ω—Ç–∏—Ä–æ–º –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –Ω–µ–¥–∞–≤–Ω–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –µ–¥–∏–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤, —Ç–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Token Compression: The Key to Efficient AI', 'desc': 'This paper discusses the shift in focus from increasing the size of language models to improving efficiency through data-centric methods, specifically token compression. As models grow larger, the computational cost of processing long sequences of tokens becomes a significant bottleneck. The authors propose that reducing the number of tokens used during training and inference can enhance AI performance without solely relying on larger models. They provide a comprehensive review of token compression techniques, their benefits, and the challenges that remain, aiming to inspire future research in this area.'}, 'zh': {'title': '‰ª§ÁâåÂéãÁº©ÔºöÊèêÂçáAIÊïàÁéáÁöÑÊñ∞ÂâçÊ≤ø', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÊ®°ÂûãÁöÑËßÑÊ®°‰∏çÊñ≠Êâ©Â§ßÔºåÂèÇÊï∞Êï∞Èáè‰ªéÊï∞Áôæ‰∏áÂ¢ûÂä†Âà∞Êï∞Áôæ‰∫ø„ÄÇÁÑ∂ËÄåÔºåÈöèÁùÄÁ°¨‰ª∂ÈôêÂà∂ÁöÑÊé•ËøëÔºåËÆ°ÁÆóÁì∂È¢àÂ∑≤ËΩ¨ÂêëËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÈïøÂ∫èÂàó‰∏äÁöÑ‰∫åÊ¨°ÊàêÊú¨ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜË∂ÖÈïøÊñáÊú¨„ÄÅÈ´òÂàÜËæ®ÁéáÂõæÂÉèÂíåÊâ©Â±ïËßÜÈ¢ëÊó∂„ÄÇÊú¨ÊñáÊèêÂá∫ÔºåÁ†îÁ©∂ÁöÑÈáçÁÇπÂ∫î‰ªé‰ª•Ê®°Âûã‰∏∫‰∏≠ÂøÉÁöÑÂéãÁº©ËΩ¨Âêë‰ª•Êï∞ÊçÆ‰∏∫‰∏≠ÂøÉÁöÑÂéãÁº©ÔºåÁâπÂà´ÊòØ‰ª§ÁâåÂéãÁº©Ë¢´ËßÜ‰∏∫ÊèêÈ´òAIÊïàÁéáÁöÑÊñ∞ÂâçÊ≤ø„ÄÇÈÄöËøáÂØπ‰ª§ÁâåÂéãÁº©ÁöÑÁ≥ªÁªüÊÄßÂõûÈ°æÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜÂÖ∂Âü∫Êú¨‰ºòÂäøÂíåÈù¢‰∏¥ÁöÑÊåëÊàòÔºåÂπ∂Â±ïÊúõÊú™Êù•ÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19457', 'title': 'BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs', 'url': 'https://huggingface.co/papers/2505.19457', 'abstract': 'BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.', 'score': 55, 'issue_id': 3969, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'f565f3ceb53b6631', 'authors': ['Guilong Lu', 'Xuntao Guo', 'Rongjunchen Zhang', 'Wenqiao Zhu', 'Ji Liu'], 'affiliations': ['Harbin Institute of Technology', 'HiThink Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.19457.jpg', 'data': {'categories': ['#benchmark', '#science', '#reasoning', '#dataset', '#open_source'], 'emoji': 'üíπ', 'ru': {'title': 'BizFinBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ LLM –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö', 'desc': 'BizFinBench - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 6781 —Ö–æ—Ä–æ—à–æ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö –ø—è—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏–π: —á–∏—Å–ª–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ 25 –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ —Å–∏—Å—Ç–µ–º—ã, –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –≤–æ –≤—Å–µ—Ö –∑–∞–¥–∞—á–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ä—É—Ç–∏–Ω–Ω—ã–º–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏.'}, 'en': {'title': 'BizFinBench: Evaluating LLMs for Financial Precision', 'desc': 'BizFinBench is a specialized benchmark designed to evaluate large language models (LLMs) in financial applications, highlighting their performance across various tasks. It includes 6,781 annotated queries in Chinese, covering areas such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark employs both objective and subjective metrics, and introduces IteraJudge to minimize bias in evaluations. Results show that no single model excels in all tasks, with distinct performance patterns observed among different models, particularly in complex reasoning scenarios.'}, 'zh': {'title': 'BizFinBenchÔºöÈáëËûçÂ∫îÁî®‰∏≠ÁöÑËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞Êñ∞Âü∫ÂáÜ', 'desc': 'BizFinBenchÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈáëËûçÂ∫îÁî®‰∏≠ÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÂÆÉÂåÖÂê´6781‰∏™ÁªèËøáËâØÂ•ΩÊ†áÊ≥®ÁöÑÊü•ËØ¢ÔºåÊ∂µÁõñ‰∫ÜÊï∞ÂÄºËÆ°ÁÆó„ÄÅÊé®ÁêÜ„ÄÅ‰ø°ÊÅØÊèêÂèñ„ÄÅÈ¢ÑÊµãËØÜÂà´ÂíåÂü∫‰∫éÁü•ËØÜÁöÑÈóÆÈ¢òÂõûÁ≠îÁ≠â‰∫î‰∏™Áª¥Â∫¶„ÄÇÈÄöËøáÂØπ25‰∏™Ê®°ÂûãÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Ê≤°Êúâ‰∏Ä‰∏™Ê®°ÂûãÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏î‰∏çÂêåÊ®°ÂûãÂú®ÂêÑ‰∏™‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÊ®°ÂºèÂêÑÂºÇ„ÄÇËØ•Âü∫ÂáÜÊµãËØï‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏•Ê†º‰∏î‰∏éÂïÜ‰∏öÁõ∏ÂÖ≥ÁöÑËØÑ‰º∞Ê†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19297', 'title': 'Alchemist: Turning Public Text-to-Image Data into Generative Gold', 'url': 'https://huggingface.co/papers/2505.19297', 'abstract': "A new method using a pre-trained generative model helps construct a high-impact SFT dataset, Alchemist, which improves the generative quality of text-to-image models while maintaining diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pre-training equips text-to-image (T2I) models with broad world knowledge, but this alone is often insufficient to achieve high aesthetic quality and alignment. Consequently, supervised fine-tuning (SFT) is crucial for further refinement. However, its effectiveness highly depends on the quality of the fine-tuning dataset. Existing public SFT datasets frequently target narrow domains (e.g., anime or specific art styles), and the creation of high-quality, general-purpose SFT datasets remains a significant challenge. Current curation methods are often costly and struggle to identify truly impactful samples. This challenge is further complicated by the scarcity of public general-purpose datasets, as leading models often rely on large, proprietary, and poorly documented internal data, hindering broader research progress. This paper introduces a novel methodology for creating general-purpose SFT datasets by leveraging a pre-trained generative model as an estimator of high-impact training samples. We apply this methodology to construct and release Alchemist, a compact (3,350 samples) yet highly effective SFT dataset. Experiments demonstrate that Alchemist substantially improves the generative quality of five public T2I models while preserving diversity and style. Additionally, we release the fine-tuned models' weights to the public.", 'score': 54, 'issue_id': 3975, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '620ec57b5ad166ad', 'authors': ['Valerii Startsev', 'Alexander Ustyuzhanin', 'Alexey Kirillov', 'Dmitry Baranchuk', 'Sergey Kastryulin'], 'affiliations': ['MSU', 'Yandex', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.19297.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#open_source', '#training', '#data'], 'emoji': 'üß™', 'ru': {'title': '–ê–ª—Ö–∏–º–∏—è –¥–∞–Ω–Ω—ã—Ö: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Alchemist. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Alchemist –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—è—Ç–∏ –ø—É–±–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π text-to-image, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ —Å—Ç–∏–ª—å.'}, 'en': {'title': 'Alchemist: Elevating Text-to-Image Models with Quality SFT Datasets', 'desc': 'This paper presents a new approach to creating a high-quality supervised fine-tuning (SFT) dataset called Alchemist, which enhances the performance of text-to-image (T2I) models. By utilizing a pre-trained generative model, the authors can identify impactful training samples that improve both the aesthetic quality and diversity of generated images. The Alchemist dataset consists of 3,350 carefully selected samples, addressing the limitations of existing narrow-domain datasets. The results show that models fine-tuned with Alchemist significantly outperform others while maintaining a wide range of styles.'}, 'zh': {'title': 'Âà©Áî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÊûÑÂª∫È´òË¥®ÈáèSFTÊï∞ÊçÆÈõÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøá‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÁîüÊàêÊ®°ÂûãÊù•ÊûÑÂª∫È´òÂΩ±ÂìçÂäõÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊï∞ÊçÆÈõÜAlchemist„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÊñáÊú¨Âà∞ÂõæÂÉèÔºàT2IÔºâÊ®°ÂûãÁöÑÁîüÊàêË¥®ÈáèÔºåÂêåÊó∂‰øùÊåÅÂ§öÊ†∑ÊÄß„ÄÇÁé∞ÊúâÁöÑÂÖ¨ÂÖ±SFTÊï∞ÊçÆÈõÜÈÄöÂ∏∏Âè™ÈíàÂØπÁã≠Á™ÑÈ¢ÜÂüüÔºåÂàõÂª∫È´òË¥®ÈáèÁöÑÈÄöÁî®SFTÊï∞ÊçÆÈõÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçÂ§ßÊåëÊàò„ÄÇÂÆûÈ™åË°®ÊòéÔºåAlchemistÊòæËëóÊèêÂçá‰∫Ü‰∫î‰∏™ÂÖ¨ÂÖ±T2IÊ®°ÂûãÁöÑÁîüÊàêË¥®ÈáèÔºåÂπ∂‰∏îÊàë‰ª¨ËøòÂ∞ÜÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19250', 'title': 'PATS: Process-Level Adaptive Thinking Mode Switching', 'url': 'https://huggingface.co/papers/2505.19250', 'abstract': 'PATS enhances LLM efficiency by dynamically adjusting reasoning strategies based on task difficulty, leveraging PRMs and Beam Search.  \t\t\t\t\tAI-generated summary \t\t\t\t Current large-language models (LLMs) typically adopt a fixed reasoning strategy, either simple or complex, for all questions, regardless of their difficulty. This neglect of variation in task and reasoning process complexity leads to an imbalance between performance and efficiency. Existing methods attempt to implement training-free fast-slow thinking system switching to handle problems of varying difficulty, but are limited by coarse-grained solution-level strategy adjustments. To address this issue, we propose a novel reasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS), which enables LLMs to dynamically adjust their reasoning strategy based on the difficulty of each step, optimizing the balance between accuracy and computational efficiency. Our approach integrates Process Reward Models (PRMs) with Beam Search, incorporating progressive mode switching and bad-step penalty mechanisms. Experiments on diverse mathematical benchmarks demonstrate that our methodology achieves high accuracy while maintaining moderate token usage. This study emphasizes the significance of process-level, difficulty-aware reasoning strategy adaptation, offering valuable insights into efficient inference for LLMs.', 'score': 43, 'issue_id': 3974, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '25678fe2f47d4d46', 'authors': ['Yi Wang', 'Junxiao Liu', 'Shimao Zhang', 'Jiajun Chen', 'Shujian Huang'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19250.jpg', 'data': {'categories': ['#training', '#reasoning', '#math', '#inference', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –≥–∏–±–∫–æ—Å—Ç—å', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PATS. –≠—Ç–∞ –º–µ—Ç–æ–¥–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –∑–∞–¥–∞—á–∏. PATS –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) —Å –ª—É—á–µ–≤—ã–º –ø–æ–∏—Å–∫–æ–º, –≤–∫–ª—é—á–∞—è –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ –∏ —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –Ω–µ—É–¥–∞—á–Ω—ã–µ —à–∞–≥–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ PATS –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —É–º–µ—Ä–µ–Ω–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é.'}, 'en': {'title': 'Dynamic Reasoning for Efficient LLMs', 'desc': 'This paper introduces a new method called Process-Level Adaptive Thinking Mode Switching (PATS) to improve the efficiency of large language models (LLMs). Unlike traditional models that use a fixed reasoning strategy, PATS allows LLMs to adapt their reasoning approach based on the difficulty of each task. By combining Process Reward Models (PRMs) with Beam Search, the model can switch strategies dynamically and penalize ineffective steps. The results show that PATS enhances accuracy while reducing computational costs, highlighting the importance of adapting reasoning strategies to task complexity.'}, 'zh': {'title': 'Âä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÁ≠ñÁï•ÔºåÊèêÈ´òLLMÊïàÁéá', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜËåÉÂºèÔºåÁß∞‰∏∫ËøáÁ®ãÁ∫ßËá™ÈÄÇÂ∫îÊÄùÁª¥Ê®°ÂºèÂàáÊç¢ÔºàPATSÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊïàÁéá„ÄÇPATSËÉΩÂ§üÊ†πÊçÆÊØè‰∏™Ê≠•È™§ÁöÑÈöæÂ∫¶Âä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÁ≠ñÁï•Ôºå‰ªéËÄå‰ºòÂåñÂáÜÁ°ÆÊÄß‰∏éËÆ°ÁÆóÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÂíåÊùüÊêúÁ¥¢ÔºåÈááÁî®Ê∏êËøõÊ®°ÂºèÂàáÊç¢ÂíåÈîôËØØÊ≠•È™§ÊÉ©ÁΩöÊú∫Âà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPATSÂú®Â§öÊ†∑ÂåñÁöÑÊï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òÂáÜÁ°ÆÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈÄÇ‰∏≠ÁöÑ‰ª§Áâå‰ΩøÁî®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16348', 'title': 'Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance', 'url': 'https://huggingface.co/papers/2505.16348', 'abstract': "MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO", 'score': 42, 'issue_id': 3969, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '2293dd9e12554028', 'authors': ['Taeyoon Kwon', 'Dongwook Choi', 'Sunghwan Kim', 'Hyojun Kim', 'Seungjun Moon', 'Beong-woo Kwak', 'Kuan-Hao Huang', 'Jinyoung Yeo'], 'affiliations': ['Texas A&M University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16348.jpg', 'data': {'categories': ['#agents', '#interpretability', '#multimodal', '#agi', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤: –ø—É—Ç—å –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏', 'desc': 'MEMENTO - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è –æ–∫–∞–∑–∞–Ω–∏—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞–º–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–µ–¥–º–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏ –¥–∞–∂–µ —É –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—è–≤–∏–ª–∏ 30.5% –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏.'}, 'en': {'title': 'Enhancing Personalized Assistance in Embodied Agents through Memory Utilization', 'desc': 'MEMENTO is a framework that evaluates how well embodied agents use memory to provide personalized assistance. It highlights the challenges these agents face in understanding the unique meanings users assign to objects and their routines. The framework includes a two-stage evaluation process that measures how effectively agents can identify objects based on personal significance and infer user patterns. Experiments show that even advanced models like GPT-4o struggle with memory utilization, particularly when dealing with multiple memories, leading to a notable drop in performance.'}, 'zh': {'title': '‰∏™ÊÄßÂåñËÆ∞ÂøÜÂà©Áî®ÁöÑËØÑ‰º∞Ê°ÜÊû∂', 'desc': 'MEMENTOÊòØ‰∏Ä‰∏™ËØÑ‰º∞‰∏™ÊÄßÂåñËÆ∞ÂøÜÂà©Áî®ÁöÑÊ°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®ÁêÜËß£Áî®Êà∑ËØ≠‰πâÂíåÊó•Â∏∏‰π†ÊÉØÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂÆ∂Â∫≠Áâ©ÂìÅÈáçÊéí‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËøô‰∫õ‰ªªÂä°‰∏ªË¶ÅÊòØÂçïËΩÆ‰∫§‰∫íÔºåÊú™ËÉΩÁúüÂÆûÂèçÊò†Êèê‰æõÊúâÊÑè‰πâÂ∏ÆÂä©ÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜÊèê‰æõ‰∏™ÊÄßÂåñÁöÑÂ∏ÆÂä©ÔºåÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÈúÄË¶ÅÁêÜËß£Áî®Êà∑ÂØπÁâ©ÁêÜ‰∏ñÁïåÁöÑÁã¨ÁâπËØ≠‰πâÔºåÂπ∂Âà©Áî®ÂÖàÂâçÁöÑ‰∫§‰∫íÂéÜÂè≤Êù•Ëß£ÈáäÂä®ÊÄÅÁöÑÁé∞ÂÆûÊåá‰ª§„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ¶ÇGPT-4oÔºåÂú®ÈúÄË¶ÅÂèÇËÄÉÂ§ö‰∏™ËÆ∞ÂøÜÊó∂ÔºåÊÄßËÉΩ‰πü‰ºö‰∏ãÈôç30.5%ÔºåËøô‰∏∫Êú™Êù•ÂºÄÂèëÊõ¥ÊúâÊïàÁöÑ‰∏™ÊÄßÂåñÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20258', 'title': 'ARM: Adaptive Reasoning Model', 'url': 'https://huggingface.co/papers/2505.20258', 'abstract': 'Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.  \t\t\t\t\tAI-generated summary \t\t\t\t While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the "overthinking" problem -- excessive and unnecessary reasoning -- which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones -- Direct Answer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of 30%, and up to 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a 2x speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens -- ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage.', 'score': 40, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'f56b53bb4f61d90d', 'authors': ['Siye Wu', 'Jian Xie', 'Yikai Zhang', 'Aili Chen', 'Kai Zhang', 'Yu Su', 'Yanghua Xiao'], 'affiliations': ['Fudan University', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20258.jpg', 'data': {'categories': ['#reasoning', '#inference', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –ò–ò', 'desc': '–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (ARM) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Ada-GRPO –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. ARM —Å–ø–æ—Å–æ–±–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏, –≤–∫–ª—é—á–∞—è —Ç—Ä–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞ (–ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç, –∫–æ—Ä–æ—Ç–∫–∞—è —Ü–µ–ø–æ—á–∫–∞ –º—ã—Å–ª–µ–π –∏ –∫–æ–¥) –∏ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª–∏–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–∫—Ä–∞—â–∞—è –∏—Ö –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 30%, –∏ –¥–æ 70% –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö, –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. ARM —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º—ã —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–æ–º –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –≥–∏–±–∫–æ—Å—Ç–∏.'}, 'en': {'title': 'Smart Reasoning: Less Tokens, More Efficiency!', 'desc': "The Adaptive Reasoning Model (ARM) introduces a novel approach to optimize reasoning in AI by dynamically selecting reasoning formats based on task complexity. It addresses the common issue of excessive token usage, known as the 'overthinking' problem, by employing Ada-GRPO, a refined version of Group Relative Policy Optimization. ARM can utilize various reasoning formats, including Direct Answer and Short CoT, to enhance efficiency while maintaining performance levels similar to more complex methods like Long CoT. This model not only reduces token consumption by up to 70% but also accelerates training speed by two times, making it a significant advancement in autonomous AI reasoning."}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊé®ÁêÜÔºåÊèêÂçáÊïàÁéá‰∏éÊÄßËÉΩ', 'desc': 'Ëá™ÈÄÇÂ∫îÊé®ÁêÜÊ®°ÂûãÔºàARMÔºâÈÄöËøá‰ΩøÁî®Ada-GRPOÊù•ÂáèÂ∞ë‰ª§Áâå‰ΩøÁî®ÈáèÔºåÊèêÈ´ò‰∏çÂêåÊé®ÁêÜÊ®°Âºè‰∏ãÁöÑÊïàÁéá„ÄÇÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁº∫‰πèÊ†πÊçÆ‰ªªÂä°ÈöæÂ∫¶Ë∞ÉÊï¥Êé®ÁêÜ‰ª§Áâå‰ΩøÁî®ÁöÑËÉΩÂäõÔºåÂØºËá¥ËøáÂ∫¶Êé®ÁêÜÁöÑÈóÆÈ¢ò„ÄÇARMËÉΩÂ§üÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°Ëá™ÈÄÇÂ∫îÈÄâÊã©ÂêàÈÄÇÁöÑÊé®ÁêÜÊ†ºÂºèÔºåÂåÖÊã¨Áõ¥Êé•ÂõûÁ≠î„ÄÅÁÆÄÁü≠ÈìæÂºèÊé®ÁêÜÂíå‰ª£Á†ÅÁ≠âÈ´òÊïàÊ†ºÂºèÔºå‰ª•ÂèäÊõ¥Â§çÊùÇÁöÑÈïøÈìæÂºèÊé®ÁêÜ„ÄÇÈÄöËøáAda-GRPOÔºåARMÂÆûÁé∞‰∫ÜÈ´òËææ70%ÁöÑ‰ª§ÁâåËäÇÁúÅÔºåÂêåÊó∂‰øùÊåÅ‰∏é‰ªÖ‰æùËµñÈïøÈìæÂºèÊé®ÁêÜÁöÑÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19914', 'title': 'Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles', 'url': 'https://huggingface.co/papers/2505.19914', 'abstract': "Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.", 'score': 33, 'issue_id': 3970, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '607e56f504f7f777', 'authors': ['Jiangjie Chen', 'Qianyu He', 'Siyu Yuan', 'Aili Chen', 'Zhicheng Cai', 'Weinan Dai', 'Hongli Yu', 'Qiying Yu', 'Xuefeng Li', 'Jiaze Chen', 'Hao Zhou', 'Mingxuan Wang'], 'affiliations': ['ByteDance Seed', 'Fudan University', 'Institute for AI Industry Research (AIR), Tsinghua University', 'Nanjing University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19914.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#rl', '#optimization', '#benchmark', '#dataset'], 'emoji': 'üß©', 'ru': {'title': 'Enigmata: –ø—Ä–æ–∫–∞—á–∫–∞ –ª–æ–≥–∏–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∏', 'desc': 'Enigmata - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ —Ä–µ—à–µ–Ω–∏—è –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫ —É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 36 –∑–∞–¥–∞—á –≤ —Å–µ–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö, –∫–∞–∂–¥–∞—è —Å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏. –ú–æ–¥–µ–ª—å Qwen2.5-32B-Enigmata, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é Enigmata, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ —Ä–µ—à–µ–Ω–∏—é –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –î–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM.'}, 'en': {'title': 'Enhancing Puzzle Reasoning in LLMs with Enigmata', 'desc': 'Enigmata is a novel framework designed to enhance Large Language Models (LLMs) in solving puzzles through scalable multi-task Reinforcement Learning (RL) training. It features a suite of 36 tasks across seven categories, each equipped with a generator for creating diverse examples and a rule-based verifier for automatic assessment. This setup allows for efficient training and evaluation, leading to improved performance on puzzle reasoning benchmarks and advanced math tasks. The results demonstrate that models trained with Enigmata, like Qwen2.5-32B-Enigmata, outperform existing models and show strong generalization capabilities in various reasoning challenges.'}, 'zh': {'title': 'ÊèêÂçáËß£Ë∞úÊé®ÁêÜËÉΩÂäõÁöÑÁªü‰∏ÄÊ°ÜÊû∂', 'desc': 'EnigmataÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ∑•ÂÖ∑Â•ó‰ª∂ÔºåÊó®Âú®ÈÄöËøáÂèØÊâ©Â±ïÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëß£Ë∞úÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËØ•Â•ó‰ª∂ÂåÖÂê´36‰∏™‰ªªÂä°ÔºåÂàÜ‰∏∫‰∏É‰∏™Á±ªÂà´ÔºåÊØè‰∏™‰ªªÂä°ÈÉΩÊúâ‰∏Ä‰∏™ÁîüÊàêÂô®ÔºåÂèØ‰ª•ÁîüÊàêÂèØÊéßÈöæÂ∫¶ÁöÑÊó†ÈôêÁ§∫‰æãÔºå‰ª•Âèä‰∏Ä‰∏™Âü∫‰∫éËßÑÂàôÁöÑÈ™åËØÅÂô®ÔºåÁî®‰∫éËá™Âä®ËØÑ‰º∞„ÄÇÈÄöËøáËøôÁßçÁîüÊàêÂô®-È™åËØÅÂô®ËÆæËÆ°ÔºåEnigmataÊîØÊåÅÂèØÊâ©Â±ïÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºåÂπ∂ÂÆûÁé∞‰∫ÜÁªÜËá¥ÁöÑÂàÜÊûêÂíåÊó†ÁºùÁöÑÂèØÈ™åËØÅÂ•ñÂä±ÈõÜÊàê„ÄÇÁªèËøáËÆ≠ÁªÉÁöÑÊ®°ÂûãQwen2.5-32B-EnigmataÂú®Ëß£Ë∞úÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜEnigmataÂú®ÈÄªËæëÊé®ÁêÜÊñπÈù¢ÁöÑÁªü‰∏ÄÂíåÂèØÊéßÊ°ÜÊû∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19815', 'title': 'Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective', 'url': 'https://huggingface.co/papers/2505.19815', 'abstract': "LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel framework for comprehending the reasoning capabilities of large language models (LLMs) through the perspective of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.", 'score': 33, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '145614af2451e525', 'authors': ['Junnan Liu', 'Hongwei Liu', 'Linchen Xiao', 'Shudong Liu', 'Taolin Zhang', 'Zihan Ma', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.19815.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#training'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM –∫–∞–∫ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∫–∞–∫ –ø—Å–µ–≤–¥–æ-–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–≤–æ–¥—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏ –º–µ–∂–¥—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ LLM –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–¥–∏–≥–º–∞–º–∏ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –∫–∞–∫ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ, –≥–¥–µ –∫–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–¥–∞—á—É. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —Å–∏–ª—å–Ω—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ LLM –∏ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ–º, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Unlocking LLM Reasoning through Meta-Learning', 'desc': 'This paper introduces a new way to understand how large language models (LLMs) reason by using a meta-learning framework. It treats reasoning as a process similar to pseudo-gradient descent, where each question is seen as a separate task. By training the LLM on a variety of questions, it learns to adapt its parameters effectively, improving its reasoning skills. The findings show a strong link between LLM reasoning and meta-learning, offering valuable strategies for enhancing model performance.'}, 'zh': {'title': 'ÈÄöËøáÂÖÉÂ≠¶‰π†ÊèêÂçáLLMÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂÖÉÂ≠¶‰π†ÁöÑËßÜËßíÊù•ÁêÜËß£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Â∞ÜÊé®ÁêÜËøáÁ®ãËßÜ‰∏∫ÂØπLLMÂèÇÊï∞ÁöÑ‰º™Ê¢ØÂ∫¶‰∏ãÈôçÊõ¥Êñ∞ÔºåÂ∞ÜÊØè‰∏™ÈóÆÈ¢òËßÜ‰∏∫Áã¨Á´ã‰ªªÂä°Ôºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÂú®Â§öÊ†∑ÂåñÈóÆÈ¢òÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåLLMËÉΩÂ§üÂèëÂ±ïÂá∫Âü∫Êú¨ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂ËÉΩÊé®ÂπøÂà∞Êú™ËßÅËøáÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏ç‰ªÖÂä†Ê∑±‰∫ÜÂØπLLMÊé®ÁêÜÁöÑÁêÜËß£ÔºåËøòÊèê‰æõ‰∫ÜÈÄöËøáÂ∑≤Âª∫Á´ãÁöÑÂÖÉÂ≠¶‰π†ÊäÄÊúØÊù•ÊîπËøõËøô‰∫õÊ®°ÂûãÁöÑÂÆûÁî®ËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18545', 'title': 'B-score: Detecting biases in large language models using response\n  history', 'url': 'https://huggingface.co/papers/2505.18545', 'abstract': 'Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.', 'score': 25, 'issue_id': 3967, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '60113080c6881b99', 'authors': ['An Vo', 'Mohammad Reza Taesiri', 'Daeyoung Kim', 'Anh Totti Nguyen'], 'affiliations': ['Auburn University, USA', 'KAIST, South Korea', 'University of Alberta, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.18545.jpg', 'data': {'categories': ['#data', '#ethics', '#hallucinations', '#benchmark', '#rlhf'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –¥–∏–∞–ª–æ–≥', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –µ–µ —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ 9 —Ç–µ–º–∞–º, –≤–∫–ª—é—á–∞—è —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ, —Å–ª—É—á–∞–π–Ω—ã–µ –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–∏–ø—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM —Å–ø–æ—Å–æ–±–Ω—ã —Å–Ω–∏–∂–∞—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –ø—Ä–∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Ç—Ä–µ–±—É—é—â–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ, –Ω–µ–ø—Ä–µ–¥–≤–∑—è—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å B-score –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'De-biasing LLMs Through Multi-Turn Conversations', 'desc': "This paper explores how large language models (LLMs) can reduce their biases during multi-turn conversations by referencing their previous answers. The authors categorize questions into three types: Subjective, Random, and Objective, and find that LLMs can effectively 'de-bias' themselves when responding to Random questions. They introduce a new metric called B-score, which helps identify biases in LLM responses across various question types. The results show that using B-score enhances the accuracy of verifying LLM answers compared to traditional methods like confidence scores."}, 'zh': {'title': 'Â§öËΩÆÂØπËØù‰∏≠ÁöÑÂéªÂÅèËßÅËÉΩÂäõ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ∏∏Â∏∏Ë°®Áé∞Âá∫ÊòéÊòæÁöÑÂÅèËßÅÔºå‰æãÂ¶ÇÂØπÂ•≥ÊÄßÁöÑÂÅèËßÅÊàñÂØπÊï∞Â≠ó7ÁöÑÂÅèÂ•Ω„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫ÜÂú®Â§öËΩÆÂØπËØù‰∏≠ÔºåLLMsÊòØÂê¶ËÉΩÂ§üÂú®ËßÇÂØüÂà∞Ëá™Â∑±‰πãÂâçÁöÑÂõûÁ≠îÂêéÔºåËæìÂá∫Êõ¥Â∞ëÂÅèËßÅÁöÑÁ≠îÊ°à„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁªÑÊ∂µÁõñ9‰∏™‰∏ªÈ¢òÁöÑÊµãËØïÈóÆÈ¢òÔºåÂàÜ‰∏∫‰∏ªËßÇ„ÄÅÈöèÊú∫ÂíåÂÆ¢ËßÇ‰∏âÁßçÁ±ªÂûãÔºå‰ª•‰∫ÜËß£Âì™‰∫õÈóÆÈ¢òÊõ¥ÂÆπÊòìÂºïÂèëÂÅèËßÅ„ÄÇÁªìÊûúË°®ÊòéÔºåLLMsÂú®Èù¢ÂØπÂØªÊ±ÇÈöèÊú∫„ÄÅ‰∏çÂÅèËßÅÁ≠îÊ°àÁöÑÈóÆÈ¢òÊó∂ÔºåËÉΩÂ§üËá™Êàë‚ÄúÂéªÂÅèËßÅ‚ÄùÔºåÂπ∂‰∏îÊàë‰ª¨ÊèêÂá∫ÁöÑB-scoreÊåáÊ†áÂú®Ê£ÄÊµãÂÅèËßÅÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜLLMÁ≠îÊ°àÁöÑÈ™åËØÅÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20259', 'title': 'Lifelong Safety Alignment for Language Models', 'url': 'https://huggingface.co/papers/2505.20259', 'abstract': "A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment.", 'score': 22, 'issue_id': 3969, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'af43851ea2378c4b', 'authors': ['Haoyu Wang', 'Zeyu Qin', 'Yifei Zhao', 'Chao Du', 'Min Lin', 'Xueqian Wang', 'Tianyu Pang'], 'affiliations': ['Sea AI Lab, Singapore', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20259.jpg', 'data': {'categories': ['#inference', '#training', '#alignment', '#security'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –Ω–æ–≤—ã—Ö –∞—Ç–∞–∫', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ –∏—Ö –∑–∞—â–∏—Ç—É. –í –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã –ª–µ–∂–∏—Ç —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º–µ–∂–¥—É –ú–µ—Ç–∞-–ê—Ç–∞–∫—É—é—â–∏–º, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã, –∏ –ó–∞—â–∏—Ç–Ω–∏–∫–æ–º, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è—Ç—å —ç—Ç–∏–º –∞—Ç–∞–∫–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ GPT-4 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ –≤–∑–ª–æ–º—É LLM –∏ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –ú–µ—Ç–∞-–ê—Ç–∞–∫—É—é—â–µ–≥–æ. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ó–∞—â–∏—Ç–Ω–∏–∫ —Å–º–æ–≥ —Å–Ω–∏–∑–∏—Ç—å —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ —Å 73% –¥–æ 7%, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å LLM –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏.'}, 'en': {'title': 'Adapting LLMs for Unseen Threats: A Lifelong Safety Approach', 'desc': "This paper presents a lifecycle safety alignment framework designed to enhance the robustness of large language models (LLMs) against jailbreaking attacks. It introduces a competitive system involving a Meta-Attacker, which learns to identify new jailbreaking strategies, and a Defender, which evolves to counter these attacks. The framework emphasizes the importance of preparing LLMs for unforeseen threats rather than just known vulnerabilities. Through iterative training, the Defender significantly reduces the Meta-Attacker's success rate, ensuring safer deployment of LLMs in dynamic environments."}, 'zh': {'title': 'ÊèêÂçáLLMsÂÆâÂÖ®ÊÄßÁöÑÁîüÂëΩÂë®ÊúüÂØπÈΩêÊ°ÜÊû∂', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁîüÂëΩÂë®ÊúüÂÆâÂÖ®ÂØπÈΩêÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Èù¢ÂØπÊñ∞ÂûãË∂äÁã±ÊîªÂáªÊó∂ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™ÂÖÉÊîªÂáªËÄÖÂíå‰∏Ä‰∏™Èò≤Âæ°ËÄÖÁöÑÁ´û‰∫âÊú∫Âà∂Ôºå‰ΩøLLMsËÉΩÂ§üÊåÅÁª≠ÈÄÇÂ∫î‰∏çÊñ≠ÊºîÂèòÁöÑË∂äÁã±Á≠ñÁï•„ÄÇÂÖÉÊîªÂáªËÄÖË¥üË¥£‰∏ªÂä®ÂèëÁé∞Êñ∞ÁöÑË∂äÁã±Á≠ñÁï•ÔºåËÄåÈò≤Âæ°ËÄÖÂàôËá¥Âäõ‰∫éÊäµÂæ°Ëøô‰∫õÊîªÂáª„ÄÇÈÄöËøáËø≠‰ª£ËÆ≠ÁªÉÔºåÈò≤Âæ°ËÄÖÊòæËëóÊèêÈ´ò‰∫ÜÂØπÊäóËÉΩÂäõÔºåÂ∞ÜÂÖÉÊîªÂáªËÄÖÁöÑÊàêÂäüÁéáÈôç‰ΩéÂà∞‰ªÖ7%Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜLLMsÂú®ÂºÄÊîæÁéØÂ¢É‰∏≠ÁöÑÊõ¥ÂÆâÂÖ®ÂèØÈù†ÈÉ®ÁΩ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19209', 'title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis\n  Discovery via Hierarchical Search', 'url': 'https://huggingface.co/papers/2505.19209', 'abstract': "A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.", 'score': 22, 'issue_id': 3968, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': 'cb70528ba0407554', 'authors': ['Zonglin Yang', 'Wanhao Liu', 'Ben Gao', 'Yujie Liu', 'Wei Li', 'Tong Xie', 'Lidong Bing', 'Wanli Ouyang', 'Erik Cambria', 'Dongzhan Zhou'], 'affiliations': ['MiroMind', 'Nanyang Technological University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'University of New South Wales', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19209.jpg', 'data': {'categories': ['#science', '#rlhf', '#benchmark', '#multimodal', '#optimization'], 'emoji': 'üß™', 'ru': {'title': '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–∞–∂–µ –Ω–∞—É—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: –æ—Ç –∏–¥–µ–∏ –∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É', 'desc': '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—É—Ç–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª–∏ –≤ –≥–∏–ø–æ—Ç–µ–∑—É, –ø—Ä–æ–¥–≤–∏–≥–∞—è—Å—å –æ—Ç –æ–±—â–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º. –≠—Ç–æ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –ª–∞–Ω–¥—à–∞—Ñ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –Ω–æ–≤–æ–º —ç—Ç–∞–ª–æ–Ω–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ-–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ –∏–∑ –Ω–µ–¥–∞–≤–Ω–µ–π —Ö–∏–º–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–∏–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ª–∏–Ω–∏–∏.'}, 'en': {'title': 'Unlocking Detailed Scientific Hypotheses with LLMs', 'desc': "This paper presents a new method for generating detailed scientific hypotheses using large language models (LLMs). It defines the task of fine-grained scientific hypothesis discovery, which focuses on creating actionable hypotheses from broader research ideas. The authors frame this task as a combinatorial optimization problem and explore how to optimize the hypothesis generation process by leveraging LLMs' internal scoring mechanisms. Their hierarchical search method improves the quality of generated hypotheses, as demonstrated by superior performance on benchmark evaluations compared to existing methods."}, 'zh': {'title': 'Âà©Áî®LLMsÁîüÊàêÁªÜÁ≤íÂ∫¶ÁßëÂ≠¶ÂÅáËÆæÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁîüÊàêËØ¶ÁªÜÁßëÂ≠¶ÂÅáËÆæÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂÆö‰πâÂíå‰ºòÂåñÊΩúÂú®Â•ñÂä±ÊôØËßÇÔºåË∂ÖË∂ä‰∫ÜÂü∫ÂáÜËØÑ‰º∞‰∏≠ÁöÑÂÖ∂‰ªñÊñπÊ≥ï„ÄÇÊàë‰ª¨È¶ñÊ¨°Ê≠£ÂºèÂÆö‰πâ‰∫ÜÁªÜÁ≤íÂ∫¶ÁßëÂ≠¶ÂÅáËÆæÂèëÁé∞ÁöÑ‰ªªÂä°ÔºåÊó®Âú®‰ªéÁ≤óÁï•ÁöÑÁ†îÁ©∂ÊñπÂêëÁîüÊàêÂèØÂÆûÈ™åÊìç‰ΩúÁöÑËØ¶ÁªÜÂÅáËÆæ„ÄÇÊàë‰ª¨Â∞ÜÊ≠§‰ªªÂä°ËßÜ‰∏∫ÁªÑÂêà‰ºòÂåñÈóÆÈ¢òÔºåÂπ∂Êé¢ËÆ®LLMsÂú®ÊúÄÂ§ßÂåñÂà©Áî®Êó∂Ëß£ÂÜ≥ËØ•ÈóÆÈ¢òÁöÑËÉΩÂäõ‰∏äÈôê„ÄÇÈÄöËøáÂàÜÂ±ÇÊêúÁ¥¢ÊñπÊ≥ïÔºåÊàë‰ª¨ÈÄêÊ≠•ÊèêÂá∫Âπ∂Êï¥ÂêàÂÅáËÆæÁªÜËäÇÔºå‰ªé‰∏ÄËà¨Ê¶ÇÂøµÂà∞ÂÖ∑‰ΩìÂÆûÈ™åÈÖçÁΩÆÔºåÂÆûÈ™åËØÅÊòéËØ•ÊñπÊ≥ïÂú®Êñ∞Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18675', 'title': 'Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps', 'url': 'https://huggingface.co/papers/2505.18675', 'abstract': 'Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.', 'score': 22, 'issue_id': 3967, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '61fe1af51f08d30f', 'authors': ['Sicheng Feng', 'Song Wang', 'Shuyi Ouyang', 'Lingdong Kong', 'Zikai Song', 'Jianke Zhu', 'Huan Wang', 'Xinchao Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'National University of Singapore', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18675.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#cv', '#open_source'], 'emoji': 'üó∫Ô∏è', 'ru': {'title': 'ReasonMap: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'ReasonMap - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∫–∞—Ä—Ç—ã –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ –∏–∑ 30 –≥–æ—Ä–æ–¥–æ–≤ –∏ 1008 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –±–∞–∑–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –≤–µ—Ä—Å–∏–∏ —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏, –∞ —É –∑–∞–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∞–∫–∂–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–ª–∏–Ω–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ.'}, 'en': {'title': 'Evaluating Visual Reasoning in Multimodal Models with ReasonMap', 'desc': "This paper introduces ReasonMap, a benchmark aimed at evaluating the fine-grained visual understanding and spatial reasoning capabilities of multimodal large language models (MLLMs). The benchmark includes high-resolution transit maps and a set of question-answer pairs to rigorously test the models' reasoning abilities. The study finds that base models often outperform reasoning variants in open-source settings, while closed-source models show the opposite trend. Additionally, the results indicate that masking visual inputs generally leads to decreased performance, highlighting the importance of genuine visual perception in complex reasoning tasks."}, 'zh': {'title': 'ÁªÜÁ≤íÂ∫¶ËßÜËßâÊé®ÁêÜÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÁªÜÁ≤íÂ∫¶ËßÜËßâÁêÜËß£ÁöÑÊé®ÁêÜ‰ªªÂä°‰∏ä‰ªçÁÑ∂‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜReasonMapÔºå‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞MLLMsÁöÑÁªÜÁ≤íÂ∫¶ËßÜËßâÁêÜËß£ÂíåÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇReasonMapÂåÖÂê´Êù•Ëá™13‰∏™ÂõΩÂÆ∂30‰∏™ÂüéÂ∏ÇÁöÑÈ´òÂàÜËæ®Áéá‰∫§ÈÄöÂú∞ÂõæÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏§Á∫ßËØÑ‰º∞ÊµÅÁ®ãÊù•ÂáÜÁ°ÆËØÑ‰º∞Á≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÂíåË¥®Èáè„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Êè≠Á§∫‰∫Ü‰∏Ä‰∏™ÂèçÁõ¥ËßâÁöÑÊ®°ÂºèÔºöÂú®ÂºÄÊ∫êÊ®°Âûã‰∏≠ÔºåÂü∫Á°ÄÊ®°ÂûãÁöÑË°®Áé∞‰ºò‰∫éÊé®ÁêÜÊ®°ÂûãÔºåËÄåÂú®Èó≠Ê∫êÊ®°Âûã‰∏≠ÂàôÁõ∏Âèç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19439', 'title': 'Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers', 'url': 'https://huggingface.co/papers/2505.19439', 'abstract': 'Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers.Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams , in other words, to unlock the capabilities it already possesses.', 'score': 20, 'issue_id': 3971, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'fc8a2a184c9655b1', 'authors': ['Rihui Xin', 'Han Liu', 'Zecheng Wang', 'Yupeng Zhang', 'Dianbo Sui', 'Xiaolin Hu', 'Bingning Wang'], 'affiliations': ['Baichuan Inc.', 'Harbin Institute of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19439.jpg', 'data': {'categories': ['#reasoning', '#math', '#rl', '#optimization', '#training'], 'emoji': 'üßÆ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ –ò–ò –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –±–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º–∞—Ç–∞, –º–æ–∂–µ—Ç –¥–∞—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º GRPO –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ GRPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –¥–ª–∏–Ω—ã, –Ω–µ —Ç–æ–ª—å–∫–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç, –Ω–æ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ GRPO –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å—É—Ç—å —É—Å–ø–µ—Ö–∞ –ø–æ–¥—Ö–æ–¥–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫: –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –ø–æ–¥–æ–±–Ω–∞ –æ—Ç–ª–∏—á–Ω–æ–º—É —É—á–µ–Ω–∏–∫—É, –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ —Ä–∞–∑–≤–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–∏–≤—ã—á–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤—ã–¥–∞—é—â–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ö.'}, 'en': {'title': 'Unlocking LLMs: Training with Format and Length Signals', 'desc': 'This paper explores how to train Large Language Models (LLMs) for solving mathematical problems without relying on traditional ground truth answers. It introduces the idea of using format and length as surrogate signals to guide the training process. The research demonstrates that a reward function focused on format correctness can improve performance significantly, especially in the early training phases. By incorporating length-based rewards later on, the proposed GRPO approach not only matches but exceeds the performance of standard methods that depend on ground truth data, achieving notable accuracy on mathematical problem sets.'}, 'zh': {'title': 'Âà©Áî®Ê†ºÂºè‰∏éÈïøÂ∫¶ÊèêÂçáÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Ê†ºÂºèÂíåÈïøÂ∫¶‰Ωú‰∏∫Êõø‰ª£‰ø°Âè∑ÔºåËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËß£ÂÜ≥Êï∞Â≠¶ÈóÆÈ¢òÔºåËÄåÊó†ÈúÄ‰º†ÁªüÁöÑÁúüÂÆûÁ≠îÊ°à„ÄÇÊàë‰ª¨ÂèëÁé∞Ôºå‰ªÖ‰æùÈù†Ê†ºÂºèÊ≠£Á°ÆÊÄßÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÂú®Êó©ÊúüÈò∂ÊÆµÂèØ‰ª•Ëé∑Âæó‰∏éÊ†áÂáÜGRPOÁÆóÊ≥ïÁõ∏ÂΩìÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÈöèÁùÄËÆ≠ÁªÉÁöÑÊ∑±ÂÖ•ÔºåÊ†ºÂºèÂ•ñÂä±ÁöÑÂ±ÄÈôêÊÄßÊòæÁé∞ÔºåÂõ†Ê≠§Êàë‰ª¨ÂºïÂÖ•‰∫ÜÂü∫‰∫éÈïøÂ∫¶ÁöÑÂ•ñÂä±„ÄÇÊúÄÁªàÁöÑGRPOÊñπÊ≥ïÂà©Áî®Ê†ºÂºè-ÈïøÂ∫¶Êõø‰ª£‰ø°Âè∑ÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ã‰∏ç‰ªÖÂåπÈÖç‰∫ÜÊ†áÂáÜGRPOÁÆóÊ≥ïÁöÑË°®Áé∞ÔºåËøòË∂ÖË∂ä‰∫ÜÂÖ∂Âú®AIME2024‰∏äÁöÑË°®Áé∞ÔºåËææÂà∞‰∫Ü40.0%ÁöÑÂáÜÁ°ÆÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19590', 'title': 'Learning to Reason without External Rewards', 'url': 'https://huggingface.co/papers/2505.19590', 'abstract': "Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor", 'score': 18, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'd847f0743c20b2e9', 'authors': ['Xuandong Zhao', 'Zhewei Kang', 'Aosong Feng', 'Sergey Levine', 'Dawn Song'], 'affiliations': ['UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19590.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#rlhf', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò: –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –¥–≤–∏–≥–∞—Ç–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Intuitor. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º–æ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞. Intuitor –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –º–µ—Ç–æ–¥–æ–º GRPO –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ø—Ä–∞–≤–ª—è—Ç—å –æ–±—É—á–µ–Ω–∏–µ–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö, –ø—Ä–µ–¥–ª–∞–≥–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏.'}, 'en': {'title': "Learning from Confidence: Intuitor's Unsupervised Approach to AI", 'desc': "The paper introduces Intuitor, a method for Reinforcement Learning from Internal Feedback (RLIF) that allows large language models (LLMs) to learn without external supervision. Instead of relying on costly labeled data, Intuitor uses self-certainty, which is the model's own confidence in its predictions, as a reward signal. This approach enables unsupervised learning and matches the performance of existing methods like Group Relative Policy Optimization (GRPO) on benchmarks. Additionally, Intuitor demonstrates better generalization to tasks outside its training domain, such as code generation, highlighting the potential of intrinsic signals for effective learning in AI systems."}, 'zh': {'title': 'Ëá™ÊàëÁ°ÆÂÆöÊÄßÈ©±Âä®ÁöÑÊó†ÁõëÁù£Â≠¶‰π†', 'desc': 'IntuitorÊòØ‰∏ÄÁßçÂü∫‰∫éÂÜÖÈÉ®ÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂà©Áî®Ëá™ÊàëÁ°ÆÂÆöÊÄß‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑Ôºå‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üËøõË°åÊó†ÁõëÁù£Â≠¶‰π†„ÄÇËØ•ÊñπÊ≥ïÂú®Êï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫‰∏éGRPOÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂπ∂Âú®‰ª£Á†ÅÁîüÊàêÁ≠âÈ¢ÜÂüü‰ªªÂä°‰∏äÂ±ïÁé∞Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøá‰ΩøÁî®Ê®°ÂûãËá™Ë∫´ÁöÑ‰ø°ÂøÉÂàÜÊï∞‰Ωú‰∏∫ÂîØ‰∏ÄÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåIntuitorÂÆûÁé∞‰∫ÜÂÆåÂÖ®Êó†ÁõëÁù£ÁöÑÂ≠¶‰π†„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂÜÖÈÉ®Ê®°Âûã‰ø°Âè∑ÂèØ‰ª•ÊúâÊïàÊé®Âä®Ë∑®È¢ÜÂüüÁöÑÂ≠¶‰π†Ôºå‰∏∫Áº∫‰πèÂèØÈ™åËØÅÂ•ñÂä±ÁöÑËá™‰∏ªAIÁ≥ªÁªüÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18601', 'title': 'Flex-Judge: Think Once, Judge Anywhere', 'url': 'https://huggingface.co/papers/2505.18601', 'abstract': 'Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.', 'score': 18, 'issue_id': 3968, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': 'a0fd5b19cac44ab0', 'authors': ['Jongwoo Ko', 'Sungnyun Kim', 'Sungwoo Cho', 'Se-Young Yun'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18601.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#alignment', '#benchmark', '#multimodal', '#transfer_learning'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ - –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ', 'desc': 'Flex-Judge - —ç—Ç–æ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ. Flex-Judge –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ API –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω—â–∏–∫–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Å–æ–±—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±–ª–∞—Å—Ç—è—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ.'}, 'en': {'title': 'Flex-Judge: Efficient Multimodal Evaluation with Minimal Data', 'desc': 'Flex-Judge is a novel multimodal evaluation model that utilizes minimal textual reasoning data to effectively generalize across various modalities and evaluation formats. It addresses the limitations of existing models that require extensive, modality-specific training data and often struggle with diverse tasks. By leveraging structured textual reasoning, Flex-Judge captures generalizable decision-making patterns, allowing it to perform well even with limited training resources. Empirical results show that it outperforms state-of-the-art models, making it a valuable tool in scenarios with scarce evaluation benchmarks.'}, 'zh': {'title': 'Flex-JudgeÔºöÁî®ÊúÄÂ∞ëÊï∞ÊçÆÂÆûÁé∞Â§öÊ®°ÊÄÅËØÑ‰º∞ÁöÑÁ™ÅÁ†¥', 'desc': 'Flex-JudgeÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅËØÑ‰º∞Ê®°ÂûãÔºåÂÆÉÂà©Áî®ÊúÄÂ∞ëÁöÑÊñáÊú¨Êé®ÁêÜÊï∞ÊçÆÊù•ÂÆûÁé∞Ë∑®Â§öÁßçÊ®°ÊÄÅÂíåËØÑ‰º∞Ê†ºÂºèÁöÑÊ≥õÂåñ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁªìÊûÑÂåñÁöÑÊñáÊú¨Êé®ÁêÜËß£ÈáäÔºåÊçïÊçâÂà∞ÂèØËΩ¨ÁßªÁöÑÂÜ≥Á≠ñÊ®°ÂºèÔºå‰ªéËÄåÂú®ÂõæÂÉèÊàñËßÜÈ¢ëÁ≠âÂ§öÊ®°ÊÄÅÂà§Êñ≠‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Flex-JudgeÁöÑËÆ≠ÁªÉÊï∞ÊçÆËøúÂ∞ë‰∫éÂÖ∂‰ªñÊ®°ÂûãÔºå‰ΩÜÂÖ∂ÊÄßËÉΩ‰ªçÁÑ∂‰∏éÊúÄÂÖàËøõÁöÑÂïÜ‰∏öAPIÁõ∏ÂΩìÔºåÁîöËá≥Êõ¥‰ºò„ÄÇËØ•Ê°ÜÊû∂Â±ïÁ§∫‰∫ÜÂü∫‰∫éÊé®ÁêÜÁöÑÊñáÊú¨ÁõëÁù£‰Ωú‰∏∫‰∏ÄÁßçÂº∫Â§ß‰∏îÁªèÊµéÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊé®Âä®‰∫ÜÂèØÊâ©Â±ïÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18536', 'title': 'Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.18536', 'abstract': 'Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.', 'score': 18, 'issue_id': 3967, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '03c065b99c9707fe', 'authors': ['Haoyuan Sun', 'Jiaqi Wu', 'Bo Xia', 'Yifu Luo', 'Yifei Zhao', 'Kai Qin', 'Xufei Lv', 'Tiantian Zhang', 'Yongzhe Chang', 'Xueqian Wang'], 'affiliations': ['Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18536.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark', '#rlhf', '#agi', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'RFT: –∫–ª—é—á –∫ —É—Å–∏–ª–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –º–µ—Ç–æ–¥–∞ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RFT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ RFT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ MLLM –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í —Ä–∞–±–æ—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—è—Ç—å –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ —É–ª—É—á—à–µ–Ω–∏—è MLLM —Å –ø–æ–º–æ—â—å—é RFT, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –∑–∞–¥–∞—á, —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø—è—Ç—å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Reinforcement Fine-Tuning: Powering Reasoning in Multimodal AI', 'desc': 'This paper discusses the role of reinforcement fine-tuning (RFT) in improving the reasoning abilities of multimodal large language models (MLLMs). It highlights how RFT has contributed to the development of advanced AI models and emphasizes its importance in enhancing reasoning across various tasks and domains. The authors summarize five key improvements brought by RFT, including better training algorithms and diverse benchmarks. They also suggest future research directions to further explore the potential of RFT in the quest for Artificial General Intelligence (AGI).'}, 'zh': {'title': 'Âº∫ÂåñÂæÆË∞ÉÔºöÊé®Âä®Â§öÊ®°ÊÄÅÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Âú®2025Âπ¥ÔºåÂº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢Â±ïÁé∞Âá∫ÊòæËëóÊΩúÂäõÔºåÂπ∂Êé®Âä®‰∫ÜÂ¶ÇOpenAI-o1ÂíåDeepSeek-R1Á≠âÂâçÊ≤øAIÊ®°ÂûãÁöÑÂèëÂ±ï„ÄÇRFTÂú®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏≠ÁöÑÈ´òÊïàÂ∫îÁî®ÂºïËµ∑‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÊú¨ÊñáËØ¶ÁªÜ‰ªãÁªç‰∫ÜÁ†îÁ©∂ËÄÖÂ∫î‰∫ÜËß£ÁöÑÂü∫Á°ÄÁü•ËØÜÔºåÂπ∂ÊÄªÁªì‰∫ÜRFTÂú®ÊèêÂçáMLLMÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑ‰∫î‰∏™ÂÖ≥ÈîÆÊîπËøõÁÇπ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∫î‰∏™Êú™Êù•Á†îÁ©∂ÁöÑÊúâÂâçÊôØÊñπÂêëÔºå‰ª•Êúü‰∏∫AGIÁöÑËøõÊ≠•Êèê‰æõÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19752', 'title': 'Discrete Markov Bridge', 'url': 'https://huggingface.co/papers/2505.19752', 'abstract': 'A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete diffusion has recently emerged as a promising paradigm in discrete data modeling. However, existing methods typically rely on a fixed rate transition matrix during training, which not only limits the expressiveness of latent representations, a fundamental strength of variational methods, but also constrains the overall design space. To address these limitations, we propose Discrete Markov Bridge, a novel framework specifically designed for discrete representation learning. Our approach is built upon two key components: Matrix Learning and Score Learning. We conduct a rigorous theoretical analysis, establishing formal performance guarantees for Matrix Learning and proving the convergence of the overall framework. Furthermore, we analyze the space complexity of our method, addressing practical constraints identified in prior studies. Extensive empirical evaluations validate the effectiveness of the proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO) of 1.38 on the Text8 dataset, outperforming established baselines. Moreover, the proposed model demonstrates competitive performance on the CIFAR-10 dataset, achieving results comparable to those obtained by image-specific generation approaches.', 'score': 15, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'ffd4e97918d0f33f', 'authors': ['Hengli Li', 'Yuxuan Wang', 'Song-Chun Zhu', 'Ying Nian Wu', 'Zilong Zheng'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute of Artificial Intelligence, Peking University', 'NLCo Lab, Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2505.19752.jpg', 'data': {'categories': ['#training', '#data', '#benchmark', '#diffusion', '#dataset', '#optimization', '#architecture'], 'emoji': 'üåâ', 'ru': {'title': '–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –º–æ—Å—Ç: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Discrete Markov Bridge –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ Matrix Learning –∏ Score Learning. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –ø–µ—Ä–µ—Ö–æ–¥–æ–≤. –ü—Ä–æ–≤–µ–¥–µ–Ω —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—â–∏–π –≥–∞—Ä–∞–Ω—Ç–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è Matrix Learning –∏ –¥–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤—Å–µ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö Text8 –∏ CIFAR-10.'}, 'en': {'title': 'Revolutionizing Discrete Data Modeling with Discrete Markov Bridge', 'desc': 'The paper introduces a new framework called Discrete Markov Bridge for modeling discrete data, which includes two main components: Matrix Learning and Score Learning. This framework overcomes limitations of existing methods that use a fixed transition matrix, allowing for more expressive latent representations. The authors provide theoretical guarantees for the performance of Matrix Learning and demonstrate the convergence of the entire framework. Empirical results show that Discrete Markov Bridge outperforms previous methods on the Text8 dataset and achieves competitive results on CIFAR-10, indicating its effectiveness in discrete representation learning.'}, 'zh': {'title': 'Á¶ªÊï£È©¨Â∞îÂèØÂ§´Ê°•ÔºöÊèêÂçáÁ¶ªÊï£Êï∞ÊçÆÂª∫Ê®°ÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫Á¶ªÊï£È©¨Â∞îÂèØÂ§´Ê°•Ôºå‰∏ìÈó®Áî®‰∫éÁ¶ªÊï£Êï∞ÊçÆÂª∫Ê®°„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÁü©ÈòµÂ≠¶‰π†ÂíåËØÑÂàÜÂ≠¶‰π†‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®ËÆ≠ÁªÉ‰∏≠‰æùËµñÂõ∫ÂÆöËΩ¨ÁßªÁü©ÈòµÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøá‰∏•Ê†ºÁöÑÁêÜËÆ∫ÂàÜÊûêÔºåÊú¨Êñá‰∏∫Áü©ÈòµÂ≠¶‰π†Âª∫Á´ã‰∫ÜÊ≠£ÂºèÁöÑÊÄßËÉΩ‰øùËØÅÔºåÂπ∂ËØÅÊòé‰∫ÜÊï¥‰ΩìÊ°ÜÊû∂ÁöÑÊî∂ÊïõÊÄß„ÄÇÂÆûËØÅËØÑ‰º∞Ë°®ÊòéÔºåÁ¶ªÊï£È©¨Â∞îÂèØÂ§´Ê°•Âú®Text8ÂíåCIFAR-10Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòË∂äÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20139', 'title': "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs", 'url': 'https://huggingface.co/papers/2505.20139', 'abstract': "StructEval benchmarks Large Language Models for generating and converting structured outputs, highlighting performance gaps and challenges in producing visual content.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.", 'score': 14, 'issue_id': 3974, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '58742ac851da48f3', 'authors': ['Jialin Yang', 'Dongfu Jiang', 'Lipeng He', 'Sherman Siu', 'Yuxuan Zhang', 'Disen Liao', 'Zhuofeng Li', 'Huaye Zeng', 'Yiming Jia', 'Haozhe Wang', 'Benjamin Schneider', 'Chi Ruan', 'Wentao Ma', 'Zhiheng Lyu', 'Yifei Wang', 'Yi Lu', 'Quy Duc Do', 'Ziyan Jiang', 'Ping Nie', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent Contributor', 'Shanghai University', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.20139.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#open_source', '#benchmark'], 'emoji': 'üèóÔ∏è', 'ru': {'title': 'StructEval: –ò–∑–º–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'StructEval - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ 18 —Ñ–æ—Ä–º–∞—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è JSON, HTML –∏ SVG. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–∂–µ —É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–±–ª—é–¥–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'StructEval: Bridging the Gap in Structured Output Generation for LLMs', 'desc': "StructEval is a new benchmark designed to assess how well Large Language Models (LLMs) can create and convert structured outputs. It evaluates the models' performance in generating formats like JSON and HTML, as well as converting between different structured formats. The benchmark includes 18 formats and 44 task types, focusing on metrics for structural fidelity and correctness. Results show that even advanced models struggle with these tasks, particularly in generating visual content compared to text-only structures."}, 'zh': {'title': 'ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªìÊûÑÂåñËæìÂá∫ËÉΩÂäõ', 'desc': 'StructEvalÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁîüÊàêÂíåËΩ¨Êç¢ÁªìÊûÑÂåñËæìÂá∫ËÉΩÂäõÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÂÆÉÂÖ≥Ê≥®Ê®°ÂûãÂú®ÁîüÊàê‰∏çÂèØÊ∏≤ÊüìÔºàÂ¶ÇJSON„ÄÅYAML„ÄÅCSVÔºâÂíåÂèØÊ∏≤ÊüìÔºàÂ¶ÇHTML„ÄÅReact„ÄÅSVGÔºâÊ†ºÂºèÊó∂ÁöÑË°®Áé∞Â∑ÆË∑ùÂíåÊåëÊàò„ÄÇËØ•Âü∫ÂáÜÊµãËØïÈÄöËøáÁîüÊàê‰ªªÂä°ÂíåËΩ¨Êç¢‰ªªÂä°‰∏§ÁßçÊñπÂºèÔºåÁ≥ªÁªüÂú∞ËØÑ‰º∞‰∏çÂêåÊ†ºÂºèÁöÑÁªìÊûÑÂÆåÊï¥ÊÄß„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂÖ∂Âπ≥ÂùáÂæóÂàÜ‰πü‰ªÖ‰∏∫75.58Ôºå‰∏îÁîüÊàê‰ªªÂä°ÊØîËΩ¨Êç¢‰ªªÂä°Êõ¥ÂÖ∑ÊåëÊàòÊÄßÔºåÁîüÊàêÊ≠£Á°ÆÁöÑËßÜËßâÂÜÖÂÆπÊØîÁîüÊàêÁ∫ØÊñáÊú¨ÁªìÊûÑÊõ¥Âõ∞Èöæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19949', 'title': 'Which Data Attributes Stimulate Math and Code Reasoning? An\n  Investigation via Influence Functions', 'url': 'https://huggingface.co/papers/2505.19949', 'abstract': "Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. However, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. To address these limitations, we leverage influence functions to systematically attribute LLMs' reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning. Based on these findings, we introduce a simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts LiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the token-level influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax.", 'score': 14, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '1cb8e6d994dcd7db', 'authors': ['Siqi Kou', 'Qingyuan Tian', 'Hanwen Xu', 'Zihao Zeng', 'Zhijie Deng'], 'affiliations': ['Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19949.jpg', 'data': {'categories': ['#training', '#reasoning', '#data', '#dataset', '#optimization', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': '–§—É–Ω–∫—Ü–∏–∏ –≤–ª–∏—è–Ω–∏—è —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–µ–∫—Ä–µ—Ç—ã –æ–±—É—á–µ–Ω–∏—è LLM –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–ª–∏—è–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∫–ª–∞–¥–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –ë—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —É–ª—É—á—à–∞—é—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫–∞–∫ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ç–∞–∫ –∏ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö AIME24 –∏ LiveCodeBench. –ê–Ω–∞–ª–∏–∑ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª, —á—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ–±–µ–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.'}, 'en': {'title': 'Unlocking LLM Reasoning: Influence Functions for Enhanced Accuracy', 'desc': "This paper explores how influence functions can help us understand the reasoning abilities of large language models (LLMs) in math and coding tasks. By attributing the model's performance to specific training examples, the authors reveal important cross-domain effects, such as how challenging math problems can enhance coding skills. They propose a new dataset reweighting strategy that improves model accuracy significantly by adjusting the difficulty of tasks. Additionally, the study highlights distinct patterns in how LLMs utilize language for reasoning in math versus coding, providing insights for better training data curation."}, 'zh': {'title': 'Âà©Áî®ÂΩ±ÂìçÂáΩÊï∞ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂΩ±ÂìçÂáΩÊï∞Âú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂ∫îÁî®ÔºåÊó®Âú®Â∞ÜÊ®°ÂûãÂú®Êï∞Â≠¶ÂíåÁºñÁ®ãÊé®ÁêÜ‰∏≠ÁöÑË°®Áé∞ÂΩíÂõ†‰∫éÂÖ∑‰ΩìÁöÑËÆ≠ÁªÉÊ†∑Êú¨„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÁ†îÁ©∂Êè≠Á§∫‰∫ÜË∑®È¢ÜÂüüÁöÑÂΩ±ÂìçÔºåË°®ÊòéÈ´òÈöæÂ∫¶ÁöÑÊï∞Â≠¶Á§∫‰æãÂèØ‰ª•ÂêåÊó∂ÊèêÂçáÊï∞Â≠¶ÂíåÁºñÁ®ãÊé®ÁêÜËÉΩÂäõ„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÊï∞ÊçÆÈõÜÈáçÂä†ÊùÉÁ≠ñÁï•ÔºåÈÄöËøáË∞ÉÊï¥‰ªªÂä°ÈöæÂ∫¶ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊúÄÂêéÔºåÁªÜËá¥ÁöÑÂΩíÂõ†ÂàÜÊûêÊòæÁ§∫ÔºåÂ∫èÂàóÁ∫ßÁöÑÊé¢Á¥¢Ë°å‰∏∫Âú®Êï∞Â≠¶ÂíåÁºñÁ®ãÊé®ÁêÜ‰∏≠ÂùáÊúâÂä©‰∫éÊèêÂçáË°®Áé∞ÔºåËÄåÂú®‰∏çÂêå‰ªªÂä°‰∏≠ÔºåÊ†áËÆ∞Á∫ßÁöÑÂΩ±ÂìçÊ®°Âºè‰πüÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20256', 'title': 'Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration', 'url': 'https://huggingface.co/papers/2505.20256', 'abstract': "An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.   Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.", 'score': 13, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '5a9a5050221ec342', 'authors': ['Hao Zhong', 'Muzhi Zhu', 'Zongze Du', 'Zheng Huang', 'Canyu Zhao', 'Mingyu Liu', 'Wen Wang', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20256.jpg', 'data': {'categories': ['#rl', '#reasoning', '#video', '#hallucinations', '#benchmark', '#multimodal', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä—É–¥–∞: –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏', 'desc': 'Omni-R1 - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ, –∞ —Ç–∞–∫–∂–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–∏–∫—Å–µ–ª–µ–π. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º: –°–∏—Å—Ç–µ–º—ã –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤—ã–±–∏—Ä–∞—é—â–µ–π –∫–ª—é—á–µ–≤—ã–µ –∫–∞–¥—Ä—ã, –∏ –°–∏—Å—Ç–µ–º—ã –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, –≤—ã–ø–æ–ª–Ω—è—é—â–µ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å –ø–æ–º–æ—â—å—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –Ω–∞–≥—Ä–∞–¥, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–Ω–ª–∞–π–Ω-–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –¥–≤—É—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º. Omni-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã–µ –¥–æ–º–µ–Ω—ã.'}, 'en': {'title': 'Omni-R1: Bridging Global Reasoning and Detail Understanding in Reinforcement Learning', 'desc': 'The paper presents Omni-R1, an end-to-end reinforcement learning framework designed for complex tasks involving both video and audio reasoning. It addresses the challenge of balancing the need for low-resolution frames for temporal coverage with the requirement for high-resolution inputs for detailed understanding. By employing a two-system architecture, it utilizes a Global Reasoning System to select keyframes and a Detail Understanding System for pixel-level analysis. The framework demonstrates superior performance on benchmarks, outperforming existing models and enhancing generalization across different tasks.'}, 'zh': {'title': 'Omni-R1ÔºöÂº∫ÂåñÂ≠¶‰π†È©±Âä®ÁöÑÂÖ®Ê®°ÊÄÅÊé®ÁêÜÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Omni-R1ÁöÑÁ´ØÂà∞Á´ØÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÈïøÊó∂Èó¥ËßÜÈ¢ëÈü≥È¢ëÊé®ÁêÜÂíåÁªÜÁ≤íÂ∫¶ÂÉèÁ¥†ÁêÜËß£‰ªªÂä°‰∏≠ÁöÑÁüõÁõæÈúÄÊ±Ç„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÂÖ®Â±ÄÊé®ÁêÜÁ≥ªÁªüÂíåÁªÜËäÇÁêÜËß£Á≥ªÁªüÔºåÈÄöËøáÈÄâÊã©ÂÖ≥ÈîÆ‰ø°ÊÅØÂ∏ßÊù•Èôç‰ΩéÁ©∫Èó¥ÊàêÊú¨ÔºåÂêåÊó∂Âú®È´òÂàÜËæ®ÁéáÁâáÊÆµ‰∏äËøõË°åÂÉèÁ¥†Á∫ßÁöÑÂü∫Á°ÄÂÆö‰Ωç„ÄÇÊàë‰ª¨Â∞ÜÂÖ≥ÈîÆ‰ø°ÊÅØÂ∏ßÁöÑÈÄâÊã©ÂíåÈáçÊûÑËßÜ‰∏∫Âº∫ÂåñÂ≠¶‰π†ÈóÆÈ¢òÔºåÂà©Áî®Â±ÇÊ¨°Â•ñÂä±Êú∫Âà∂ËøõË°åËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmni-R1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂº∫Â§ßÁöÑÁõëÁù£Âü∫Á∫øÂíå‰∏ì‰∏öÁöÑÊúÄÂÖàËøõÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13136', 'title': 'ModernGBERT: German-only 1B Encoder Model Trained from Scratch', 'url': 'https://huggingface.co/papers/2505.13136', 'abstract': 'ModernGBERT and LL\\"aMmlein2Vec, new German encoder models, outperform existing models in terms of performance and parameter-efficiency across various NLP tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LL\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling a controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance encoder models.', 'score': 13, 'issue_id': 3976, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '01dde4c221d27e3d', 'authors': ['Anton Ehrmanntraut', 'Julia Wunderle', 'Jan Pfister', 'Fotis Jannidis', 'Andreas Hotho'], 'affiliations': ['CAIDAS Center for Artificial Intelligence and Data Science', 'Computer Philology and History of Contemporary German Literature', 'Data Science', 'JMU Julius-Maximilians-Universit√§t W√ºrzburg'], 'pdf_title_img': 'assets/pdf/title_img/2505.13136.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#open_source', '#dataset', '#training', '#low_resource', '#architecture'], 'emoji': 'üá©üá™', 'ru': {'title': '–ù–æ–≤–æ–µ —Å–ª–æ–≤–æ –≤ –Ω–µ–º–µ—Ü–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: ModernGBERT –∏ LL"aMmlein2Vec', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–µ –Ω–µ–º–µ—Ü–∫–∏–µ —ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ ModernGBERT –∏ LL"aMmlein2Vec, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–∞–ª–æ–≥–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. ModernGBERT –æ–±—É—á–µ–Ω–∞ —Å –Ω—É–ª—è –∏ –≤–∫–ª—é—á–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∏–∑ ModernBERT, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ LL"aMmlein2Vec –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ –Ω–µ–º–µ—Ü–∫–∏—Ö –¥–µ–∫–æ–¥–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ LLM2Vec. –ú–æ–¥–µ–ª–∏ –±—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ModernGBERT 1B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ–º–µ—Ü–∫–∏–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ LLM2Vec —ç–Ω–∫–æ–¥–µ—Ä—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Revolutionizing German NLP with Efficient Encoders', 'desc': 'The paper introduces two new German encoder models, ModernGBERT and LL"aMmlein2Vec, which demonstrate superior performance and efficiency compared to existing models in various natural language processing (NLP) tasks. ModernGBERT is a family of models trained from scratch, utilizing architectural advancements from ModernBERT, while LL"aMmlein2Vec derives encoders from existing decoder-only models. The authors benchmark these models on tasks such as natural language understanding and long-context reasoning, highlighting the advantages of dedicated encoders over converted decoders. The findings indicate that ModernGBERT 1B achieves the best results in terms of performance and parameter efficiency, contributing valuable resources to the German NLP community.'}, 'zh': {'title': 'Êñ∞‰∏Ä‰ª£Âæ∑ËØ≠ÁºñÁ†ÅÂô®Ê®°ÂûãÔºåÊÄßËÉΩÂçìË∂äÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏§ÁßçÊñ∞ÁöÑÂæ∑ËØ≠ÁºñÁ†ÅÂô®Ê®°ÂûãÔºöModernGBERTÂíåLL"aMmlein2Vec„ÄÇËøô‰∫õÊ®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂπ∂‰∏îÂú®ÂèÇÊï∞ÊïàÁéá‰∏äË∂ÖËøá‰∫ÜÁé∞ÊúâÊ®°Âûã„ÄÇModernGBERTÊòØ‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑÂæ∑ËØ≠ÁºñÁ†ÅÂô®Á≥ªÂàóÔºåÈááÁî®‰∫ÜModernBERTÁöÑÊû∂ÊûÑÂàõÊñ∞ÔºåËÄåLL"aMmlein2VecÂàôÊòØ‰ªéÂæ∑ËØ≠Ëß£Á†ÅÂô®Ê®°ÂûãËΩ¨Êç¢ËÄåÊù•ÁöÑÁºñÁ†ÅÂô®„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåModernGBERT 1BÂú®ÊÄßËÉΩÂíåÂèÇÊï∞ÊïàÁéáÊñπÈù¢‰ºò‰∫é‰πãÂâçÁöÑÂæ∑ËØ≠ÁºñÁ†ÅÂô®ÔºåÊé®Âä®‰∫ÜÂæ∑ËØ≠Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19788', 'title': 'Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition', 'url': 'https://huggingface.co/papers/2505.19788', 'abstract': 'Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) are criticized for the excessively lengthy Chain-of-Thought (CoT) to derive the final answer, suffering from high first-token and overall latency. Typically, the CoT of LRMs mixes multiple thinking units; each unit attempts to produce a candidate answer to the original query. Hence, a natural idea to improve efficiency is to reduce the unit number. Yet, the fact that the thinking units in vanilla CoT cannot be explicitly managed renders doing so challenging. This paper introduces Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of explicit, structured, and turn-wise interactions to bridge the gap. In MinD, the model provides a multi-turn response to the query, where each turn embraces a thinking unit and yields a corresponding answer. The subsequent turns can reflect, verify, revise, or explore alternative approaches to both the thinking and answer parts of earlier ones. This not only makes the answer delivered more swiftly, but also enables explicit controls over the iterative reasoning process (i.e., users may halt or continue at any turn). We follow a supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We first rephrase the outputs of an LRM into multi-turn formats by prompting another LLM, and then tune the LRM with such data. Observing that the tuned model tends to consume even more tokens than the original one (probably due to that the multi-turn formats introduce additional answer tokens), we advocate leveraging RL algorithms like GRPO to prioritize correct outputs with fewer turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up to ~70% reduction in both output token usage and time to first token (TTFT), while maintaining competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond.', 'score': 12, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '95cddffa071ceb3b', 'authors': ['Zihao Zeng', 'Xuyao Huang', 'Boxiu Li', 'Hao Zhang', 'Zhijie Deng'], 'affiliations': ['RealAI', 'Shanghai Jiao Tong University', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.19788.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#benchmark', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–æ —à–∞–≥–∞–º: MinD –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É LRM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Multi-Turn Decomposition (MinD) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). MinD —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∑–∞–¥–µ—Ä–∂–∫—É. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Streamlining Reasoning with Multi-Turn Decomposition', 'desc': "This paper presents Multi-Turn Decomposition (MinD), a method that enhances the efficiency of Large Reasoning Models (LRMs) by breaking down complex reasoning tasks into smaller, manageable turns. Each turn focuses on a specific thinking unit, allowing the model to generate responses iteratively, which reduces the overall token usage and latency. By structuring the reasoning process, users can control the flow of interactions, enabling them to verify or revise answers at each step. The approach combines supervised fine-tuning and reinforcement learning to optimize the model's performance while significantly decreasing the time to first token and output tokens used."}, 'zh': {'title': 'Â§öËΩÆÂàÜËß£ÔºöÊèêÂçáÊé®ÁêÜÊ®°ÂûãÊïàÁéáÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§öËΩÆÂàÜËß£ÔºàMinDÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊïàÁéá„ÄÇÈÄöËøáÂ∞ÜÊÄùÁª¥ÈìæÔºàCoTÔºâÂàÜËß£‰∏∫ÂèØÁÆ°ÁêÜÁöÑÂ§ö‰∏™ÂõûÂêàÔºåMinDÂáèÂ∞ë‰∫Ü‰ª§ÁâåÁöÑ‰ΩøÁî®ÂíåÂª∂ËøüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÊØè‰∏™ÂõûÂêàÈÉΩÂåÖÂê´‰∏Ä‰∏™ÊÄùÁª¥ÂçïÂÖÉÔºåËÉΩÂ§üÂèçÊÄù„ÄÅÈ™åËØÅÊàñ‰øÆÊ≠£‰πãÂâçÁöÑÊÄùËÄÉÂíåÁ≠îÊ°àÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Âø´ÈÄüÁöÑÂìçÂ∫î„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÁªìÂêàÔºå‰ºòÂåñ‰∫ÜÊé®ÁêÜËøáÁ®ãÔºå‰ΩøÁî®Êà∑ËÉΩÂ§üÂú®‰ªªÊÑèÂõûÂêà‰∏≠ÊéßÂà∂Êé®ÁêÜÁöÑÁªßÁª≠ÊàñÂÅúÊ≠¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18759', 'title': 'The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation', 'url': 'https://huggingface.co/papers/2505.18759', 'abstract': 'Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.', 'score': 12, 'issue_id': 3971, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': 'eae631a1cc27fbf9', 'authors': ['Ruichen Zhang', 'Rana Muhammad Shahroz Khan', 'Zhen Tan', 'Dawei Li', 'Song Wang', 'Tianlong Chen'], 'affiliations': ['Arizona State University', 'University of North Carolina at Chapel Hill', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.18759.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#transfer_learning', '#dataset', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DC-CoT - –ø–µ—Ä–≤—ã–π –¥–∞—Ç–∞-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought, CoT). –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–Ω—ã–µ —É—á–∏—Ç–µ–ª—å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∏ –≤–Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –ø–µ—Ä–µ–Ω–æ—Å—É –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ CoT –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–∏–ª—å–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'Optimizing Reasoning in Smaller Models with Data-Centric Distillation', 'desc': 'This paper presents DC-CoT, a new benchmark for evaluating data-centric distillation methods in training smaller student Large Language Models (LLMs) while maintaining their reasoning capabilities. It explores various data manipulation techniques, such as augmentation, selection, and mixing, to understand their effects on model performance. The study uses different teacher and student model architectures to assess how these techniques influence both in-distribution and out-of-distribution generalization. The results aim to guide best practices for optimizing chain-of-thought distillation, making advanced reasoning models more efficient and accessible.'}, 'zh': {'title': 'Êï∞ÊçÆÈ©±Âä®ÁöÑËí∏È¶è‰ºòÂåñ‰πãË∑Ø', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DC-CoTÁöÑÊï∞ÊçÆ‰∏≠ÂøÉÂü∫ÂáÜÔºåÊó®Âú®Á≥ªÁªüËØÑ‰º∞Êï∞ÊçÆÂ¢ûÂº∫„ÄÅÈÄâÊã©ÂíåÊ∑∑ÂêàÂØπÈìæÂºèÊé®ÁêÜÔºàCoTÔºâËí∏È¶èÁöÑÂΩ±Âìç„ÄÇÈÄöËøá‰ΩøÁî®Â§öÁßçÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊû∂ÊûÑÔºåÊàë‰ª¨ÂØπÊï∞ÊçÆÊìç‰ΩúÂØπÂ≠¶ÁîüÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçËøõË°å‰∫Ü‰∏•Ê†ºËØÑ‰º∞ÔºåÈáçÁÇπÂÖ≥Ê≥®Âú®ÂàÜÂ∏ÉÂÜÖÂíåÂàÜÂ∏ÉÂ§ñÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁ†îÁ©∂ÁªìÊûú‰∏∫‰ºòÂåñCoTËí∏È¶èÊèê‰æõ‰∫ÜÂèØË°åÁöÑËßÅËß£ÂíåÊúÄ‰Ω≥ÂÆûË∑µÔºå‰øÉËøõ‰∫ÜÊõ¥È´òÊïàÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÂºÄÂèë„ÄÇËØ•Êï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂèØÂú®ÊåáÂÆöÈìæÊé•‰∏≠ÊâæÂà∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20152', 'title': 'Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models', 'url': 'https://huggingface.co/papers/2505.20152', 'abstract': 'A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM.', 'score': 11, 'issue_id': 3969, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'ee370e43a48e58a9', 'authors': ['Kai Sun', 'Yushi Bai', 'Zhen Yang', 'Jiajie Zhang', 'Ji Qi', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20152.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#training', '#reasoning', '#dataset', '#open_source'], 'emoji': 'üìê', 'ru': {'title': '–ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò —á–µ—Ä–µ–∑ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏ –ø—Ä–∞–≤–∏–ª–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å MMGeoLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Enhancing Geometric Reasoning with Hard Negative Contrastive Learning', 'desc': "This paper introduces a new framework for hard negative contrastive learning that enhances geometric reasoning in Large Multimodal Models (LMMs). By utilizing contrastively trained visual encoders and generating hard negatives through perturbations and rule-based modifications, the framework improves the model's ability to solve geometric problems. The proposed model, MMGeoLM, demonstrates superior performance on geometric reasoning tasks compared to existing models, even rivaling larger closed-source models. The study also explores how different methods of constructing negative samples affect the model's reasoning capabilities, providing valuable insights for future research."}, 'zh': {'title': 'ÊèêÂçáÂá†‰ΩïÊé®ÁêÜÁöÑÊñ∞ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ª•ÊèêÈ´òÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Âá†‰ΩïÊé®ÁêÜÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂØπÊØîÂ≠¶‰π†ËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®Âú®Ëá™ÁÑ∂Âú∫ÊôØÂõæÂÉè‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊïàÊûúÔºå‰ΩÜÂú®ÁªÜËá¥Êé®ÁêÜÊñπÈù¢Â≠òÂú®Â±ÄÈôê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁªìÂêà‰∫ÜÂü∫‰∫éÂõæÂÉèÁöÑÂØπÊØîÂ≠¶‰π†ÂíåÂü∫‰∫éÊñáÊú¨ÁöÑÂØπÊØîÂ≠¶‰π†ÔºåÂà©Áî®ÁîüÊàêÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨Âíå‰øÆÊîπÁöÑÂá†‰ΩïÊèèËø∞Êù•Â¢ûÂº∫Ê®°ÂûãÁöÑÂá†‰ΩïÁêÜËß£ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Âá†‰ΩïÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÂºÄÊ∫êÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18822', 'title': 'AdaCtrl: Towards Adaptive and Controllable Reasoning via\n  Difficulty-Aware Budgeting', 'url': 'https://huggingface.co/papers/2505.18822', 'abstract': "AdaCtrl, a novel framework, dynamically adjusts reasoning length based on problem difficulty and user control, improving performance and reducing response length across various datasets compared to standard training methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.", 'score': 11, 'issue_id': 3974, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '4ae93c717abf8bdf', 'authors': ['Shijue Huang', 'Hongru Wang', 'Wanjun Zhong', 'Zhaochen Su', 'Jiazhan Feng', 'Bowen Cao', 'Yi R. Fung'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.18822.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': 'AdaCtrl - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –¥–ª–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –∏ –ø–æ–∂–µ–ª–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ fine-tuning –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –∑–∞—Ç–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π. AdaCtrl –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ 10-90% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –°–∏—Å—Ç–µ–º–∞ —Ç–∞–∫–∂–µ –¥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —è–≤–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Problem Solving', 'desc': 'AdaCtrl is a new framework that enhances reasoning in machine learning models by adjusting the length of reasoning based on the difficulty of the problem and user preferences. It addresses the common issue of models generating overly long reasoning chains for simple tasks by implementing a two-stage training process. The first stage fine-tunes the model to recognize problem difficulty, while the second stage uses reinforcement learning to improve its reasoning strategies. Empirical results show that AdaCtrl not only improves performance but also reduces response length significantly across various datasets, allowing users to control the depth of reasoning as needed.'}, 'zh': {'title': 'Êô∫ËÉΩÊé®ÁêÜÔºåÁÅµÊ¥ªÊéßÂà∂ÔºÅ', 'desc': 'AdaCtrlÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÊ†πÊçÆÈóÆÈ¢òÁöÑÈöæÂ∫¶ÂíåÁî®Êà∑ÁöÑÊéßÂà∂Âä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÈïøÂ∫¶Ôºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩÂπ∂ÂáèÂ∞ëÂìçÂ∫îÈïøÂ∫¶„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáËá™ÊàëËØÑ‰º∞ÈóÆÈ¢òÈöæÂ∫¶Êù•Ë∞ÉÊï¥Êé®ÁêÜÈ¢ÑÁÆóÔºåÂêåÊó∂ÂÖÅËÆ∏Áî®Êà∑ÊâãÂä®ÊéßÂà∂È¢ÑÁÆóÔºå‰ª•‰ºòÂÖàËÄÉËôëÊïàÁéáÊàñÊúâÊïàÊÄß„ÄÇAdaCtrlÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºåÈ¶ñÂÖàËøõË°åÂÜ∑ÂêØÂä®ÂæÆË∞ÉÔºåÁÑ∂ÂêéÈÄöËøáÈöæÂ∫¶ÊÑüÁü•ÁöÑÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÊù•‰ºòÂåñÊ®°ÂûãÁöÑËá™ÈÄÇÂ∫îÊé®ÁêÜÁ≠ñÁï•„ÄÇÂÆûÈ™åËØÅÊòéÔºåAdaCtrlÂú®Â§ÑÁêÜ‰∏çÂêåÊï∞ÊçÆÈõÜÊó∂ÔºåËÉΩÂ§üÊ†πÊçÆ‰º∞ËÆ°ÁöÑÈöæÂ∫¶Ë∞ÉÊï¥Êé®ÁêÜÈïøÂ∫¶ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÂπ∂ÂáèÂ∞ë‰∫ÜÂìçÂ∫îÈïøÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19640', 'title': 'Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.19640', 'abstract': "A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). We propose a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. We observe that models inherently possess the ability to perform interleaved reasoning, which can be further enhanced through RL. We introduce a simple yet effective rule-based reward to incentivize correct intermediate steps, which guides the policy model toward correct reasoning paths by leveraging intermediate signals generated during interleaved reasoning. Extensive experiments conducted across five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++) demonstrate consistent improvements over traditional think-answer reasoning, without requiring external tools. Specifically, our approach reduces TTFT by over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore, our method, trained solely on question answering and logical reasoning datasets, exhibits strong generalization ability to complex reasoning datasets such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to reveal several valuable insights into conditional reward modeling.", 'score': 10, 'issue_id': 3970, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '6fff32b94b547c0e', 'authors': ['Roy Xie', 'David Qiu', 'Deepak Gopinath', 'Dong Lin', 'Yanchao Sun', 'Chong Wang', 'Saloni Potdar', 'Bhuwan Dhingra'], 'affiliations': ['Apple', 'Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19640.jpg', 'data': {'categories': ['#reasoning', '#math', '#rlhf', '#training', '#rl', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ò–ò: –º—ã—Å–ª—å –∏ –æ—Ç–≤–µ—Ç –≤ –æ–¥–Ω–æ–º –ø–æ—Ç–æ–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –º—ã—à–ª–µ–Ω–∏—è –∏ –æ—Ç–≤–µ—Ç–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT) –∏ –ø–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –ø—è—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Reinforcement Learning Boosts Reasoning Efficiency in Language Models', 'desc': 'This paper presents a new training method for large language models (LLMs) that improves their ability to answer multi-hop questions efficiently. By using reinforcement learning (RL), the model learns to alternate between thinking and answering, which enhances its reasoning capabilities. The authors introduce a reward system that encourages the model to take correct intermediate steps during reasoning, leading to better performance. Experiments show that this approach significantly reduces the time taken to generate answers and improves accuracy on various reasoning tasks without needing additional tools.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉËåÉÂºèÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öË∑≥ÈóÆÈ¢ò‰∏äÁöÑÊé®ÁêÜÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÈÄöËøá‰∫§ÊõøÊÄùËÄÉÂíåÂõûÁ≠îÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËøõË°åÊé®ÁêÜÔºåÂáèÂ∞ë‰∫ÜÈ¶ñÊ¨°ÁîüÊàê‰ª§ÁâåÁöÑÊó∂Èó¥„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÈºìÂä±Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈááÂèñÊ≠£Á°ÆÁöÑ‰∏≠Èó¥Ê≠•È™§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºå‰∏îÊó†ÈúÄÂ§ñÈÉ®Â∑•ÂÖ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20046', 'title': 'REARANK: Reasoning Re-ranking Agent via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.20046', 'abstract': 'REARANK, a reinforcement learning-enhanced large language model for listwise reasoning,outperforms baseline models and even surpasses GPT-4 on reasoning-intensive benchmarks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking.', 'score': 9, 'issue_id': 3980, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'cf154abb2291ee80', 'authors': ['Le Zhang', 'Bo Wang', 'Xipeng Qiu', 'Siva Reddy', 'Aishwarya Agrawal'], 'affiliations': ['Canada CIFAR AI Chair', 'Fudan University', 'McGill University', 'Mila - Quebec AI Institute', 'Universit√© de Montr√©al'], 'pdf_title_img': 'assets/pdf/title_img/2505.20046.jpg', 'data': {'categories': ['#benchmark', '#agents', '#interpretability', '#optimization', '#reasoning', '#rlhf', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'REARANK: –£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'REARANK - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –∏ –æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ —è–≤–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å. REARANK –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 179 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ú–æ–¥–µ–ª—å REARANK-7B, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Qwen2.5-7B, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å GPT-4, –∏ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'REARANK: Elevating Reasoning with Reinforcement Learning', 'desc': 'REARANK is a large language model designed to improve listwise reasoning by using reinforcement learning techniques. It enhances the process of reranking information by explicitly reasoning about the data, which leads to better performance and clearer results. The model shows significant improvements over traditional baseline models, even outperforming GPT-4 on challenging reasoning tasks with very few training samples. This research highlights the potential of combining reinforcement learning with large language models to boost their reasoning abilities in information retrieval tasks.'}, 'zh': {'title': 'REARANKÔºöÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂàõÊñ∞‰πã‰Ωú', 'desc': 'REARANKÊòØ‰∏ÄÁßçÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàóË°®Êé®ÁêÜÈáçÊéíÂ∫è‰ª£ÁêÜÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÂíåÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÂíåÂèØËß£ÈáäÊÄß„ÄÇËØ•Ê®°ÂûãÂú®Êé®ÁêÜ‰πãÂâçËøõË°åÊòéÁ°ÆÁöÑÊé®ÁêÜÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈáçÊéíÂ∫èÁöÑÊïàÊûú„ÄÇREARANKÂú®ÊµÅË°åÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰ªÖÈúÄ179‰∏™Ê†áÊ≥®Ê†∑Êú¨Âç≥ÂèØÂÆûÁé∞ÊòæËëóÊîπËøõ„ÄÇ‰∏éGPT-4Áõ∏ÊØîÔºåREARANKÂú®Êé®ÁêÜÂØÜÈõÜÂûãÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Êõ¥‰Ω≥ÔºåÂ±ïÁ§∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†Âú®Â¢ûÂº∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19602', 'title': 'Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression', 'url': 'https://huggingface.co/papers/2505.19602', 'abstract': 'ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Autoregressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction approach, which yields substantial improvements in efficiency, scalability, and zero-shot generalization. Nevertheless, the coarse-to-fine methodology inherent in VAR results in exponential growth of the KV cache during inference, causing considerable memory consumption and computational redundancy. To address these bottlenecks, we introduce ScaleKV, a novel KV cache compression framework tailored for VAR architectures. ScaleKV leverages two critical observations: varying cache demands across transformer layers and distinct attention patterns at different scales. Based on these insights, ScaleKV categorizes transformer layers into two functional groups: drafters and refiners. Drafters exhibit dispersed attention across multiple scales, thereby requiring greater cache capacity. Conversely, refiners focus attention on the current token map to process local details, consequently necessitating substantially reduced cache capacity. ScaleKV optimizes the multi-scale inference pipeline by identifying scale-specific drafters and refiners, facilitating differentiated cache management tailored to each scale. Evaluation on the state-of-the-art text-to-image VAR model family, Infinity, demonstrates that our approach effectively reduces the required KV cache memory to 10% while preserving pixel-level fidelity.', 'score': 9, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '26a6b9a07958f010', 'authors': ['Kunjun Li', 'Zigeng Chen', 'Cheng-Yen Yang', 'Jenq-Neng Hwang'], 'affiliations': ['National University of Singapore', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.19602.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture'], 'emoji': 'üóúÔ∏è', 'ru': {'title': 'ScaleKV: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π', 'desc': 'ScaleKV - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è KV-–∫—ç—à–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ –¥—Ä–∞—Ñ—Ç–µ—Ä—ã –∏ —Ä–µ—Ñ–∞–π–Ω–µ—Ä—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏. –î—Ä–∞—Ñ—Ç–µ—Ä—ã –∏–º–µ—é—Ç —Ä–∞—Å—Å–µ—è–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ –∫—ç—à–∞, –∞ —Ä–µ—Ñ–∞–π–Ω–µ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Ç–µ–∫—É—â–µ–π –∫–∞—Ä—Ç–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –º–µ–Ω—å—à–µ–º –æ–±—ä–µ–º–µ. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª–∏ Infinity –ø–æ–∫–∞–∑–∞–ª–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –ø–∞–º—è—Ç–∏ KV-–∫—ç—à–∞ –¥–æ 10% –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Efficient Memory Management in Visual Autoregressive Models with ScaleKV', 'desc': 'ScaleKV is a new framework designed to compress the key-value (KV) cache in Visual Autoregressive (VAR) models, which helps reduce memory usage during inference. It identifies two types of transformer layers: drafters, which need more cache due to their broad attention across multiple scales, and refiners, which focus on local details and require less cache. By optimizing cache management based on these layer types, ScaleKV significantly lowers memory consumption while maintaining high-quality outputs. Tests on the Infinity model show that ScaleKV can cut KV cache memory usage by 90% without sacrificing pixel-level fidelity.'}, 'zh': {'title': 'ScaleKVÔºöÈ´òÊïàÂéãÁº©ËßÜËßâËá™ÂõûÂΩíÊ®°ÂûãÁöÑKVÁºìÂ≠ò', 'desc': 'ScaleKVÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑKVÁºìÂ≠òÂéãÁº©Ê°ÜÊû∂Ôºå‰∏ì‰∏∫ËßÜËßâËá™ÂõûÂΩíÊ®°ÂûãËÆæËÆ°„ÄÇÂÆÉÈÄöËøáÂå∫ÂàÜÂèòÂéãÂô®Â±Ç‰∏≠ÁöÑËçâÂõæÁîüÊàêÂô®ÂíåÁ≤æÁªÜÂåñÂ§ÑÁêÜÂô®ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÊ∂àËÄóÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈ´ò‰øùÁúüÂ∫¶„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®‰∫Ü‰∏çÂêåÂ±ÇÂØπÁºìÂ≠òÁöÑÈúÄÊ±ÇÂ∑ÆÂºÇÂíå‰∏çÂêåÂ∞∫Â∫¶‰∏ãÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÔºå‰ªéËÄå‰ºòÂåñ‰∫ÜÂ§öÂ∞∫Â∫¶Êé®ÁêÜÊµÅÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåScaleKVËÉΩÂ§üÂ∞ÜÊâÄÈúÄÁöÑKVÁºìÂ≠òÂÜÖÂ≠òÂáèÂ∞ëÂà∞10%ÔºåËÄåÂÉèÁ¥†Á∫ßÁöÑ‰øùÁúüÂ∫¶Âæó‰ª•‰øùÊåÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16972', 'title': 'From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition', 'url': 'https://huggingface.co/papers/2505.16972', 'abstract': 'Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30\\%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems.', 'score': 9, 'issue_id': 3967, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '784a648ae844599f', 'authors': ['Tianduo Wang', 'Lu Xu', 'Wei Lu', 'Shanbo Cheng'], 'affiliations': ['ByteDance Seed', 'StatNLP Research Group, Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.16972.jpg', 'data': {'categories': ['#data', '#multilingual', '#low_resource', '#dataset', '#audio', '#synthetic'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ—á—å –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ ASR', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Speech Back-Translation –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ—Ä–ø—É—Å—ã –∏ –º–æ–¥–µ–ª–∏ text-to-speech (TTS) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ—á–∏ –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—á–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å TTS –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Whisper-large-v3 –Ω–∞ 500 000 —á–∞—Å–∞—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ—á–∏ –Ω–∞ 10 —è–∑—ã–∫–∞—Ö –ø—Ä–∏–≤–µ–ª–æ –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –æ—à–∏–±–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 30%.'}, 'en': {'title': 'Scaling ASR with Synthetic Speech: Speech Back-Translation', 'desc': 'This paper presents a method called Speech Back-Translation to enhance Automatic Speech Recognition (ASR) systems for multiple languages, especially those with limited resources. By using text-to-speech (TTS) models, the authors convert large text corpora into synthetic speech, significantly increasing the amount of training data available. They show that even a small amount of real speech can train TTS models to produce high-quality synthetic speech at a much larger scale. The results indicate that this approach can improve ASR performance, achieving over 30% reduction in transcription errors across ten languages.'}, 'zh': {'title': 'ËØ≠Èü≥ÂèçÂêëÁøªËØëÔºöÊèêÂçáÂ§öËØ≠Ë®ÄASRÁöÑÊúâÊïàÂà©Âô®', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËØ≠Èü≥ÂèçÂêëÁøªËØëÔºàSpeech Back-TranslationÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑÂ§öËØ≠Ë®ÄËá™Âä®ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂ§ßËßÑÊ®°ÊñáÊú¨ËØ≠ÊñôÂ∫ìËΩ¨Êç¢‰∏∫ÂêàÊàêËØ≠Èü≥ÔºåÂà©Áî®Áé∞ÊàêÁöÑÊñáÊú¨Âà∞ËØ≠Èü≥ÔºàTTSÔºâÊ®°ÂûãÔºåËß£ÂÜ≥‰∫ÜËµÑÊ∫êÊúâÈôêËØ≠Ë®ÄÁöÑË¶ÜÁõñÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ªÖÈúÄÊï∞ÂçÅÂ∞èÊó∂ÁöÑÁúüÂÆûËΩ¨ÂΩïËØ≠Èü≥ÔºåÂ∞±ËÉΩÊúâÊïàËÆ≠ÁªÉTTSÊ®°ÂûãÁîüÊàêÊï∞ÁôæÂÄçÁöÑÂêàÊàêËØ≠Èü≥ÔºåÂêåÊó∂‰øùÊåÅÈ´òË¥®Èáè„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÊàë‰ª¨ÁîüÊàê‰∫ÜË∂ÖËøá50‰∏áÂ∞èÊó∂ÁöÑÂêàÊàêËØ≠Èü≥ÔºåÂπ∂Âú®Â§öËØ≠Ë®ÄASRÁ≥ªÁªü‰∏≠ÂÆûÁé∞‰∫ÜË∂ÖËøá30%ÁöÑËΩ¨ÂΩïÈîôËØØÁéáÈôç‰Ωé„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13426', 'title': 'G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.13426', 'abstract': 'VLM-Gym addresses the "knowing-doing" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing\'\' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.', 'score': 9, 'issue_id': 3967, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '8cfc8c8f1a7e5589', 'authors': ['Liang Chen', 'Hongcheng Gao', 'Tianyu Liu', 'Zhiqi Huang', 'Flood Sung', 'Xinyu Zhou', 'Yuxin Wu', 'Baobao Chang'], 'affiliations': ['Moonshot AI', 'Peking University', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2505.13426.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#open_source', '#agents', '#games', '#optimization', '#rl'], 'emoji': 'üéÆ', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –∑–Ω–∞–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ Vision-Language Models —Å –ø–æ–º–æ—â—å—é RL', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLM-Gym - —Å—Ä–µ–¥—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è Vision-Language Models, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –Ω–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –∑–Ω–∞–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º. VLM-Gym –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–≥—Ä—ã —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏ –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç—É —Å—Ä–µ–¥—É, –∞–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª–∏ G0 –∏ G1, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ G1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –≤–µ–¥—É—â–∏–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∏–≥—Ä–∞—Ö.'}, 'en': {'title': 'Bridging the Knowing-Doing Gap in Vision-Language Models', 'desc': 'VLM-Gym is a new training environment designed to improve Vision-Language Models (VLMs) by bridging the gap between their knowledge and practical application in interactive games. Traditional VLMs excel in tasks involving text and images but struggle with decision-making in dynamic environments. By using reinforcement learning (RL) in a diverse set of visual games, VLM-Gym enables models to develop better perception and reasoning skills. The G1 models trained in this environment outperform existing models, demonstrating that enhanced perception and reasoning can support each other during training.'}, 'zh': {'title': 'VLM-GymÔºöÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜ≥Á≠ñËÉΩÂäõ', 'desc': 'VLM-Gym ÊòØ‰∏Ä‰∏™ÈíàÂØπËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂº∫ÂåñÂ≠¶‰π†ÁéØÂ¢ÉÔºåÊó®Âú®Ëß£ÂÜ≥ÂÆÉ‰ª¨Âú®‰∫íÂä®Ê∏∏Êàè‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®Â§öÊ†∑ÂåñÁöÑÊ∏∏ÊàèÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÔºåVLM-Gym ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõÔºå‰ΩøÂÖ∂Âú®ÁÆÄÂçïÊ∏∏Êàè‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÊ®°Âûã„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü G0 Âíå G1 Ê®°ÂûãÔºåÂÖ∂‰∏≠ G1 Ê®°ÂûãÂú®Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞É‰πãÂâçËøõË°å‰∫ÜÊÑüÁü•Â¢ûÂº∫Ôºå‰ª•Â∫îÂØπÊ∏∏ÊàèÂ§öÊ†∑ÊÄßÂ∏¶Êù•ÁöÑÊåëÊàò„ÄÇÊúÄÁªàÔºåG1 Ê®°ÂûãÂú®ÊâÄÊúâÊ∏∏Êàè‰∏≠ÂùáË∂ÖË∂ä‰∫ÜÂÖ∂ÊïôÂ∏àÊ®°ÂûãÔºåÂπ∂‰∏îÂú®ÊÄßËÉΩ‰∏äË∂ÖËøá‰∫ÜÈ¢ÜÂÖàÁöÑ‰∏ìÊúâÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19955', 'title': 'MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research', 'url': 'https://huggingface.co/papers/2505.19955', 'abstract': 'MLR-Bench evaluates AI agents in scientific research through modular stages, revealing that while LLMs perform well in ideation and writing, coding agents often produce unreliable experimental results.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.', 'score': 8, 'issue_id': 3974, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '8ac88b4e4afd844e', 'authors': ['Hui Chen', 'Miao Xiong', 'Yujie Lu', 'Wei Han', 'Ailin Deng', 'Yufei He', 'Jiaying Wu', 'Yibo Li', 'Yue Liu', 'Bryan Hooi'], 'affiliations': ['National University of Singapore', 'Singapore University of Technology and Design', 'University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2505.19955.jpg', 'data': {'categories': ['#agents', '#science', '#benchmark', '#open_source'], 'emoji': 'üß†', 'ru': {'title': 'MLR-Bench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ò–ò –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö', 'desc': 'MLR-Bench - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 201 –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –∑–∞–¥–∞—á—É, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ MLR-Judge –∏ –º–æ–¥—É–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ MLR-Agent. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–¥–µ–π –∏ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ —Å—Ç–∞—Ç–µ–π, –Ω–æ –∫–æ–¥–∏—Ä—É—é—â–∏–µ –∞–≥–µ–Ω—Ç—ã —á–∞—Å—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç –Ω–µ–¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. MLR-Judge –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –≤—ã—Å–æ–∫–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.'}, 'en': {'title': 'Evaluating AI Agents for Reliable Scientific Research', 'desc': 'MLR-Bench is a new benchmark designed to evaluate AI agents in the context of scientific research, focusing on their performance across various stages of the research process. It includes 201 diverse research tasks and utilizes MLR-Judge, an automated evaluation system that combines LLM-based reviewers with specific rubrics to assess the quality of research outputs. The study reveals that while large language models (LLMs) excel in generating ideas and writing coherent papers, coding agents often struggle, producing unreliable experimental results in 80% of cases. By open-sourcing MLR-Bench, the authors aim to provide a tool for the community to enhance the reliability and transparency of AI-driven scientific discovery.'}, 'zh': {'title': 'ËØÑ‰º∞AI‰ª£ÁêÜÔºåÊé®Âä®ÁßëÂ≠¶Á†îÁ©∂ÁöÑÂèØÈù†ÊÄß', 'desc': 'MLR-BenchÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞AI‰ª£ÁêÜÂú®ÁßëÂ≠¶Á†îÁ©∂‰∏≠ÁöÑË°®Áé∞ÁöÑÂü∫ÂáÜÂ∑•ÂÖ∑„ÄÇÂÆÉÂåÖÊã¨201‰∏™Êù•Ëá™NeurIPS„ÄÅICLRÂíåICMLÁ†îËÆ®‰ºöÁöÑÁ†îÁ©∂‰ªªÂä°ÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÊú∫Âô®Â≠¶‰π†‰∏ªÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂõõ‰∏™Èò∂ÊÆµÔºàÂàõÊÑèÁîüÊàê„ÄÅÊèêÊ°àÂà∂ÂÆö„ÄÅÂÆûÈ™åÂíåËÆ∫ÊñáÂÜô‰ΩúÔºâÊù•ËØÑ‰º∞AI‰ª£ÁêÜÁöÑÁ†îÁ©∂ËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂàõÊÑèÂíåÂÜô‰ΩúÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁºñÁ†Å‰ª£ÁêÜÂú®ÂÆûÈ™åÁªìÊûúÁöÑÂèØÈù†ÊÄß‰∏äÂ≠òÂú®ËæÉÂ§ßÈóÆÈ¢òÔºåÂΩ±Âìç‰∫ÜÁßëÂ≠¶Á†îÁ©∂ÁöÑÂèØ‰ø°Â∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19427', 'title': 'WINA: Weight Informed Neuron Activation for Accelerating Large Language\n  Model Inference', 'url': 'https://huggingface.co/papers/2505.19427', 'abstract': 'WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise ell_2-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to 2.94% in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.', 'score': 8, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'b49e78670cfbb696', 'authors': ['Sihan Chen', 'Dan Zhao', 'Jongwoo Ko', 'Colby Banbury', 'Huiping Zhuang', 'Luming Liang', 'Tianyi Chen'], 'affiliations': ['Microsoft', 'New York University', 'Renmin University of China', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.19427.jpg', 'data': {'categories': ['#inference', '#training', '#open_source', '#optimization', '#architecture'], 'emoji': 'üöÄ', 'ru': {'title': 'WINA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'WINA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞, —É—á–∏—Ç—ã–≤–∞—è –∫–∞–∫ –≤–µ–ª–∏—á–∏–Ω—ã —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, —Ç–∞–∫ –∏ –Ω–æ—Ä–º—ã –≤–µ—Å–æ–≤—ã—Ö –º–∞—Ç—Ä–∏—Ü. WINA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –æ—à–∏–±–∫–∏ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥–∞—Ä–∞–Ω—Ç–∏—è–º–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ WINA –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ 2.94% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø—Ä–∏ —Ç–æ–º –∂–µ —É—Ä–æ–≤–Ω–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'WINA: Efficient Inference with Training-Free Sparse Activation', 'desc': 'WINA is a new framework designed for sparse activation in large language models that does not require training. It improves inference accuracy by considering both the magnitudes of hidden states and the norms of weight matrices, which helps reduce errors in activation decisions. This approach allows for better resource efficiency and broader applicability compared to existing methods that rely only on hidden state magnitudes. Empirical results show that WINA outperforms current state-of-the-art techniques, making it a significant advancement in efficient inference for large language models.'}, 'zh': {'title': 'WINAÔºöÊó†ËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊøÄÊ¥ªÊñ∞Á™ÅÁ†¥', 'desc': 'WINAÊòØ‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊøÄÊ¥ªÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇÂÆÉÈÄöËøáÂêåÊó∂ËÄÉËôëÈöêËóèÁä∂ÊÄÅÁöÑÂ§ßÂ∞èÂíåÊùÉÈáçÁü©ÈòµÁöÑÂàóÂêëell_2ËåÉÊï∞Ôºå‰ºòÂåñ‰∫ÜÊøÄÊ¥ªÁ≠ñÁï•„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåWINAÂú®Áõ∏ÂêåÁ®ÄÁñèÂ∫¶‰∏ãÁöÑÂπ≥ÂùáÊÄßËÉΩÊèêÈ´ò‰∫ÜÊúÄÂ§ö2.94%„ÄÇËøô‰∏ÄÂàõÊñ∞ÊñπÊ≥ï‰∏∫Êó†ËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊøÄÊ¥ªÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄßËÉΩÂü∫ÂáÜÔºåÊé®Âä®‰∫ÜÈ´òÊïàÊé®ÁêÜÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19103', 'title': 'WHISTRESS: Enriching Transcriptions with Sentence Stress Detection', 'url': 'https://huggingface.co/papers/2505.19103', 'abstract': 'WHISTRESS is an alignment-free method for sentence stress detection trained on synthetic data, outperforming existing methods and generalizing well to diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken language conveys meaning not only through words but also through intonation, emotion, and emphasis. Sentence stress, the emphasis placed on specific words within a sentence, is crucial for conveying speaker intent and has been extensively studied in linguistics. In this work, we introduce WHISTRESS, an alignment-free approach for enhancing transcription systems with sentence stress detection. To support this task, we propose TINYSTRESS-15K, a scalable, synthetic training data for the task of sentence stress detection which resulted from a fully automated dataset creation process. We train WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive baselines. Our results show that WHISTRESS outperforms existing methods while requiring no additional input priors during training or inference. Notably, despite being trained on synthetic data, WHISTRESS demonstrates strong zero-shot generalization across diverse benchmarks. Project page: https://pages.cs.huji.ac.il/adiyoss-lab/whistress.', 'score': 8, 'issue_id': 3981, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '8a3080cabca549f8', 'authors': ['Iddo Yosha', 'Dorin Shteyman', 'Yossi Adi'], 'affiliations': ['The School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.19103.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#audio', '#data', '#benchmark', '#alignment'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'WHISTRESS: –¢–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–¥–∞—Ä–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –±–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è', 'desc': 'WHISTRESS - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —É–¥–∞—Ä–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö TINYSTRESS-15K —Å –ø–æ–º–æ—â—å—é –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, WHISTRESS –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω—É–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'Revolutionizing Sentence Stress Detection with WHISTRESS', 'desc': 'WHISTRESS is a novel method for detecting sentence stress without needing alignment with the spoken input. It is trained on a synthetic dataset called TINYSTRESS-15K, which was created automatically to provide a large amount of training data. The method shows superior performance compared to existing techniques and does not require extra information during training or inference. Remarkably, WHISTRESS also exhibits strong zero-shot generalization, meaning it can perform well on new, unseen data without additional training.'}, 'zh': {'title': 'Êó†ÂØπÈΩêÂè•Â≠êÈáçÈü≥Ê£ÄÊµãÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'WHISTRESSÊòØ‰∏ÄÁßçÊó†ÂØπÈΩêÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊ£ÄÊµãÂè•Â≠êÈáçÈü≥ÔºåÈááÁî®ÂêàÊàêÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®Â§öÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂè•Â≠êÈáçÈü≥ÊòØÊåáÂú®Âè•Â≠ê‰∏≠ÂØπÁâπÂÆöÂçïËØçÁöÑÂº∫Ë∞ÉÔºåÂØπ‰∫é‰º†ËææËØ¥ËØùËÄÖÁöÑÊÑèÂõæËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜTINYSTRESS-15KÔºåËøôÊòØ‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éÂè•Â≠êÈáçÈü≥Ê£ÄÊµã„ÄÇWHISTRESSÂú®TINYSTRESS-15K‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®Â§ö‰∏™Á´û‰∫âÂü∫ÂáÜ‰∏äËøõË°åËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠Êó†ÈúÄÈ¢ùÂ§ñËæìÂÖ•ÂÖàÈ™å‰ø°ÊÅØÔºå‰æùÁÑ∂Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20278', 'title': 'The Coverage Principle: A Framework for Understanding Compositional\n  Generalization', 'url': 'https://huggingface.co/papers/2505.20278', 'abstract': 'Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results when used in the same contexts. We demonstrate that this framework has a strong predictive power for the generalization capabilities of Transformers. First, we derive and empirically confirm that the training data required for two-hop generalization grows at least quadratically with the token set size, and the training data efficiency does not improve with 20x parameter scaling. Second, for compositional tasks with path ambiguity where one variable affects the output through multiple computational paths, we show that Transformers learn context-dependent state representations that undermine both performance and interoperability. Third, Chain-of-Thought supervision improves training data efficiency for multi-hop tasks but still struggles with path ambiguity. Finally, we outline a mechanism-based taxonomy that distinguishes three ways neural networks can generalize: structure-based (bounded by coverage), property-based (leveraging algebraic invariances), and shared-operator (through function reuse). This conceptual lens contextualizes our results and highlights where new architectural ideas are needed to achieve systematic compositionally. Overall, the coverage principle provides a unified lens for understanding compositional reasoning, and underscores the need for fundamental architectural or training innovations to achieve truly systematic compositionality.', 'score': 7, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'afca6c0a8de30a05', 'authors': ['Hoyeon Chang', 'Jinho Park', 'Hanseul Cho', 'Sohee Yang', 'Miyoung Ko', 'Hyeonbin Hwang', 'Seungpil Won', 'Dohaeng Lee', 'Youbin Ahn', 'Minjoon Seo'], 'affiliations': ['KAIST', 'LG AI Research', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.20278.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#data', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–∏–Ω—Ü–∏–ø –ø–æ–∫—Ä—ã—Ç–∏—è: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–∏–Ω—Ü–∏–ø –ø–æ–∫—Ä—ã—Ç–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —à–∞–±–ª–æ–Ω–æ–≤, –Ω–µ –º–æ–≥—É—Ç –Ω–∞–¥–µ–∂–Ω–æ –æ–±–æ–±—â–∞—Ç—å –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –∑–∞–º–µ–Ω—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–ª—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π —Ä–æ—Å—Ç –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ 20 —Ä–∞–∑ –Ω–µ —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ–±–æ–±—â–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –≤–∫–ª—é—á–∞—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ, —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä–Ω–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ.'}, 'en': {'title': 'Unlocking Systematic Compositionality in Language Models', 'desc': 'This paper introduces the coverage principle, which highlights the limitations of large language models in systematic compositional generalization. It shows that models relying on pattern matching struggle to generalize effectively when faced with tasks that require substituting different fragments. The authors demonstrate that the amount of training data needed for effective two-hop generalization increases significantly with the size of the token set, and that simply increasing model parameters does not enhance training efficiency. Additionally, they propose a taxonomy for understanding different types of generalization in neural networks, emphasizing the need for new architectural innovations to improve compositional reasoning.'}, 'zh': {'title': 'Ë¶ÜÁõñÂéüÂàôÔºöÁêÜËß£ÁªÑÂêàÊé®ÁêÜÁöÑÁªü‰∏ÄËßÜËßí', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Ê®°ÂºèÂåπÈÖçÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Á≥ªÁªüÊÄßÁªÑÂêàÊ≥õÂåñÊñπÈù¢Â∏∏Â∏∏‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜË¶ÜÁõñÂéüÂàôÔºöËøôÊòØ‰∏Ä‰∏™‰ª•Êï∞ÊçÆ‰∏∫‰∏≠ÂøÉÁöÑÊ°ÜÊû∂ÔºåË°®Êòé‰∏ªË¶Å‰æùËµñÊ®°ÂºèÂåπÈÖçÁöÑÊ®°ÂûãÂú®ÁªÑÂêà‰ªªÂä°‰∏≠Êó†Ê≥ïÂèØÈù†Âú∞Ê≥õÂåñ„ÄÇÊàë‰ª¨ËØÅÊòé‰∫ÜËØ•Ê°ÜÊû∂ÂØπÂèòÊç¢Âô®ÁöÑÊ≥õÂåñËÉΩÂäõÂÖ∑ÊúâÂº∫Â§ßÁöÑÈ¢ÑÊµãËÉΩÂäõÔºåÂπ∂ÊåáÂá∫ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈúÄÊ±ÇÈöèÁùÄÊ†áËÆ∞ÈõÜÂ§ßÂ∞èÁöÑÂ¢ûÂä†ËÄåËá≥Â∞ëÂëàÂπ≥ÊñπÂ¢ûÈïø„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊú∫Âà∂ÁöÑÂàÜÁ±ªÊ≥ïÔºåÂå∫ÂàÜ‰∫ÜÁ•ûÁªèÁΩëÁªúÁöÑ‰∏âÁßçÊ≥õÂåñÊñπÂºèÔºåÂº∫Ë∞É‰∫ÜÂÆûÁé∞ÁúüÊ≠£Á≥ªÁªüÊÄßÁªÑÂêàÊâÄÈúÄÁöÑÊñ∞Êû∂ÊûÑÂàõÊñ∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19443', 'title': 'Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI', 'url': 'https://huggingface.co/papers/2505.19443', 'abstract': 'A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  \t\t\t\t\tAI-generated summary \t\t\t\t This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.', 'score': 7, 'issue_id': 3967, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '04b7019c8d659b76', 'authors': ['Ranjan Sapkota', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Department of Biological and Environmental Engineering, USA', 'University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.19443.jpg', 'data': {'categories': ['#survey', '#agents', '#architecture', '#interpretability'], 'emoji': 'ü§ñ', 'ru': {'title': '–í–∞–π–± vs –ê–≥–µ–Ω—Ç: –ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ò–ò-–∞—Å—Å–∏—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–≤—É—Ö –ø–∞—Ä–∞–¥–∏–≥–º –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò: –≤–∞–π–±-–∫–æ–¥–∏–Ω–≥–∞ –∏ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –∫–æ–¥–∏–Ω–≥–∞. –í–∞–π–±-–∫–æ–¥–∏–Ω–≥ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ò–ò —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è —Ç–≤–æ—Ä—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ. –ê–≥–µ–Ω—Ç–Ω—ã–π –∫–æ–¥–∏–Ω–≥, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω—É—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º —á–µ–ª–æ–≤–µ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ü–µ–ª–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â—É—é –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–µ –æ—Å–Ω–æ–≤—ã, –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—Ç–ª–∞–¥–∫–∏ –¥–ª—è –æ–±–µ–∏—Ö –ø–∞—Ä–∞–¥–∏–≥–º.'}, 'en': {'title': 'Harmonizing Vibe and Agentic Coding for AI Development', 'desc': 'This paper reviews two coding paradigms in AI-assisted software development: vibe coding and agentic coding. Vibe coding focuses on human interaction and creativity, using conversational prompts to aid in ideation and experimentation. In contrast, agentic coding allows for more autonomous development, where AI agents can plan and execute tasks with little human input. The authors propose a taxonomy to compare these paradigms and suggest that the future of AI software engineering will benefit from integrating both approaches for a more effective development process.'}, 'zh': {'title': 'ËûçÂêàÊÉÖÊÑüÁºñÁ†Å‰∏éËá™‰∏ªÁºñÁ†ÅÁöÑÊú™Êù•ËΩØ‰ª∂ÂºÄÂèë', 'desc': 'Êú¨ÊñáÂØπ‰∏§ÁßçÊñ∞ÂÖ¥ÁöÑ‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©ËΩØ‰ª∂ÂºÄÂèëËåÉÂºèËøõË°å‰∫ÜÂÖ®Èù¢ÂàÜÊûêÔºöÊÉÖÊÑüÁºñÁ†ÅÂíåËá™‰∏ªÁºñÁ†Å„ÄÇÊÉÖÊÑüÁºñÁ†ÅÂº∫Ë∞ÉÈÄöËøáÂü∫‰∫éÊèêÁ§∫ÁöÑÂØπËØùÂ∑•‰ΩúÊµÅÁ®ãÂÆûÁé∞Áõ¥ËßÇÁöÑ‰∫∫Êú∫‰∫§‰∫íÔºåÈÄÇÂêà‰∫éÂàõÊÑèÊé¢Á¥¢ÂíåÂÆûÈ™å„ÄÇËÄåËá™‰∏ªÁºñÁ†ÅÂàôÈÄöËøáÁõÆÊ†áÈ©±Âä®ÁöÑÊô∫ËÉΩ‰ΩìÂÆûÁé∞Ëá™‰∏ªËΩØ‰ª∂ÂºÄÂèëÔºåËÉΩÂ§üÂú®ÊúÄÂ∞è‰∫∫Á±ªÂπ≤È¢Ñ‰∏ãËøõË°åËßÑÂàí„ÄÅÊâßË°åÂíåÊµãËØï„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊàêÂäüÁöÑ‰∫∫Â∑•Êô∫ËÉΩËΩØ‰ª∂Â∑•Á®ãÂ∞Ü‰æùËµñ‰∫éÂ∞ÜËøô‰∏§ÁßçËåÉÂºèÁöÑ‰ºòÂäøÁªìÂêàÂú®‰∏Ä‰∏™‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÂºÄÂèëÁîüÂëΩÂë®Êúü‰∏≠„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19386', 'title': 'Force Prompting: Video Generation Models Can Learn and Generalize\n  Physics-based Control Signals', 'url': 'https://huggingface.co/papers/2505.19386', 'abstract': 'Force prompts enable video generation models to simulate realistic physical interactions using pretrained models and force conditioning from Blender-generated videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation models have sparked interest in world models capable of simulating realistic environments. While navigation has been well-explored, physically meaningful interactions that mimic real-world forces remain largely understudied. In this work, we investigate using physical forces as a control signal for video generation and propose force prompts which enable users to interact with images through both localized point forces, such as poking a plant, and global wind force fields, such as wind blowing on fabric. We demonstrate that these force prompts can enable videos to respond realistically to physical control signals by leveraging the visual and motion prior in the original pretrained model, without using any 3D asset or physics simulator at inference. The primary challenge of force prompting is the difficulty in obtaining high quality paired force-video training data, both in the real world due to the difficulty of obtaining force signals, and in synthetic data due to limitations in the visual quality and domain diversity of physics simulators. Our key finding is that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects. Our method can generate videos which simulate forces across diverse geometries, settings, and materials. We also try to understand the source of this generalization and perform ablations that reveal two key elements: visual diversity and the use of specific text keywords during training. Our approach is trained on only around 15k training examples for a single day on four A100 GPUs, and outperforms existing methods on force adherence and physics realism, bringing world models closer to real-world physics interactions. We release all datasets, code, weights, and interactive video demos at our project page.', 'score': 7, 'issue_id': 3981, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '1f53616fb8118a51', 'authors': ['Nate Gillman', 'Charles Herrmann', 'Michael Freeman', 'Daksh Aggarwal', 'Evan Luo', 'Deqing Sun', 'Chen Sun'], 'affiliations': ['Brown University', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2505.19386.jpg', 'data': {'categories': ['#dataset', '#video', '#synthetic', '#games', '#data'], 'emoji': 'üé•', 'ru': {'title': '–°–∏–ª–æ–≤—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —Ñ–∏–∑–∏–∫–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ '—Å–∏–ª–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫' –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∞—é—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Blender, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ —Å –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–π —Ñ–∏–∑–∏–∫–æ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –ö–ª—é—á–µ–≤—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ —É—Å–ø–µ—Ö–∞ –º–µ—Ç–æ–¥–∞ —è–≤–ª—è—é—Ç—Å—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ —Ñ–∏–∑–∏–∫–∏, –ø—Ä–∏–±–ª–∏–∂–∞—è –º–∏—Ä–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫ —Ä–µ–∞–ª—å–Ω—ã–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º."}, 'en': {'title': 'Force Prompts: Realistic Video Interactions through Physical Control', 'desc': "This paper introduces a novel approach called force prompts for enhancing video generation models, allowing them to simulate realistic physical interactions. By using pretrained models and force conditioning derived from Blender-generated videos, the authors enable users to apply localized and global forces to video content. The study highlights the challenge of obtaining high-quality training data for force-video pairs, yet demonstrates that the models can generalize well with limited examples. The findings suggest that incorporating visual diversity and specific text keywords during training significantly improves the model's ability to generate videos that adhere to physical forces and exhibit realism."}, 'zh': {'title': 'ÂäõÊèêÁ§∫ÔºöËÆ©ËßÜÈ¢ëÁîüÊàêÊõ¥ÁúüÂÆûÁöÑÁâ©ÁêÜ‰∫§‰∫í', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Áâ©ÁêÜÂäõ‰Ωú‰∏∫ËßÜÈ¢ëÁîüÊàêÁöÑÊéßÂà∂‰ø°Âè∑ÔºåÊèêÂá∫‰∫ÜÂäõÊèêÁ§∫ÁöÑÊ¶ÇÂøµÔºå‰ΩøÁî®Êà∑ËÉΩÂ§üÈÄöËøáÂ±ÄÈÉ®ÁÇπÂäõÂíåÂÖ®Â±ÄÈ£éÂäõÂú∫‰∏éÂõæÂÉèËøõË°å‰∫§‰∫í„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜËøô‰∫õÂäõÊèêÁ§∫ÂèØ‰ª•‰ΩøËßÜÈ¢ëÂú®Ê≤°Êúâ3DËµÑ‰∫ßÊàñÁâ©ÁêÜÊ®°ÊãüÂô®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁúüÂÆûÂú∞ÂìçÂ∫îÁâ©ÁêÜÊéßÂà∂‰ø°Âè∑„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÈÄÇÂ∫îBlenderÂêàÊàêËßÜÈ¢ëÁöÑÁâ©ÁêÜÂäõÊù°‰ª∂Êó∂ÔºåËÉΩÂ§üË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂç≥‰ΩøÂú®ÊúâÈôêÁöÑÁâ©‰ΩìÁ§∫‰æã‰∏ã‰πüËÉΩÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÁâ©ÁêÜ‰∫§‰∫íËßÜÈ¢ë„ÄÇÊàë‰ª¨ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÂú®‰ªÖÁî®15kÁ§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊé®Âä®‰∫Ü‰∏ñÁïåÊ®°ÂûãÂêëÁúüÂÆûÁâ©ÁêÜ‰∫§‰∫íÁöÑËøõÊ≠•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19223', 'title': 'LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2505.19223', 'abstract': 'VRPO is a variance-reduced preference optimization framework for Masked Diffusion Models that significantly enhances their alignment with human preferences and performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Masked Diffusion Models (MDMs), such as LLaDA, present a promising paradigm for language modeling, there has been relatively little effort in aligning these models with human preferences via reinforcement learning. The challenge primarily arises from the high variance in Evidence Lower Bound (ELBO)-based likelihood estimates required for preference optimization. To address this issue, we propose Variance-Reduced Preference Optimization (VRPO), a framework that formally analyzes the variance of ELBO estimators and derives bounds on both the bias and variance of preference optimization gradients. Building on this theoretical foundation, we introduce unbiased variance reduction strategies, including optimal Monte Carlo budget allocation and antithetic sampling, that significantly improve the performance of MDM alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA, and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor consistently and significantly across mathematical (GSM8K +4.7), code (HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard +4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical performance compared to strong language MDMs and ARMs. Project page: https://ml-gsai.github.io/LLaDA-1.5-Demo/.', 'score': 7, 'issue_id': 3974, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '9e984483e970bca5', 'authors': ['Fengqi Zhu', 'Rongzhen Wang', 'Shen Nie', 'Xiaolu Zhang', 'Chunwei Wu', 'Jun Hu', 'Jun Zhou', 'Jianfei Chen', 'Yankai Lin', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of AI, Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19223.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#architecture', '#optimization', '#alignment', '#rlhf'], 'emoji': 'üéØ', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π', 'desc': 'VRPO - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —Å —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (MDM). –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ –æ—Ü–µ–Ω–∫–∞—Ö –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ ELBO, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. VRPO –≤–≤–æ–¥–∏—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–Ω–∏–∂–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏, –≤–∫–ª—é—á–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±—é–¥–∂–µ—Ç–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –∏ –∞–Ω—Ç–∏—Ç–µ—Ç–∏—á–µ—Å–∫—É—é –≤—ã–±–æ—Ä–∫—É. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ VRPO –∫ –º–æ–¥–µ–ª–∏ LLaDA –ø—Ä–∏–≤–µ–ª–æ –∫ —Å–æ–∑–¥–∞–Ω–∏—é LLaDA 1.5, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–∑–æ—à–ª–∞ —Å–≤–æ–µ–≥–æ –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º.'}, 'en': {'title': 'Enhancing MDMs with Variance-Reduced Preference Optimization', 'desc': 'This paper introduces Variance-Reduced Preference Optimization (VRPO), a new framework designed to improve the alignment of Masked Diffusion Models (MDMs) with human preferences. The authors address the challenge of high variance in Evidence Lower Bound (ELBO) estimates that complicates preference optimization. By analyzing the variance of ELBO estimators, they develop unbiased variance reduction techniques, such as optimal Monte Carlo budget allocation and antithetic sampling. The application of VRPO to the LLaDA model results in LLaDA 1.5, which shows significant performance improvements across various benchmarks, outperforming previous models in both alignment and task-specific evaluations.'}, 'zh': {'title': 'ÊñπÂ∑ÆÂáèÂ∞ëÔºå‰ºòÂåñÂÅèÂ•ΩÂØπÈΩêÔºÅ', 'desc': 'VRPOÊòØ‰∏ÄÁßçÊñπÂ∑ÆÂáèÂ∞ëÁöÑÂÅèÂ•Ω‰ºòÂåñÊ°ÜÊû∂Ôºå‰∏ì‰∏∫Êé©ËîΩÊâ©Êï£Ê®°ÂûãËÆæËÆ°ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÁ®ãÂ∫¶ÂèäÂÖ∂Âú®ÂêÑÁ±ªÂü∫ÂáÜÊµãËØï‰∏≠ÁöÑË°®Áé∞„ÄÇÊé©ËîΩÊâ©Êï£Ê®°ÂûãÔºàMDMsÔºâÂú®ËØ≠Ë®ÄÂª∫Ê®°‰∏≠Â±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÂâçÊôØÔºå‰ΩÜÂú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂØπÈΩê‰∫∫Á±ªÂÅèÂ•ΩÊñπÈù¢ÁöÑÁ†îÁ©∂Áõ∏ÂØπËæÉÂ∞ë„ÄÇVRPOÈÄöËøáÂàÜÊûêELBO‰º∞ËÆ°Âô®ÁöÑÊñπÂ∑ÆÔºåÊèêÂá∫‰∫ÜÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÁöÑÁïåÈôêÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊó†ÂÅèÊñπÂ∑ÆÂáèÂ∞ëÁ≠ñÁï•ÔºåÂ¶ÇÊúÄ‰ºòÁöÑËíôÁâπÂç°Ê¥õÈ¢ÑÁÆóÂàÜÈÖçÂíåÂØπÁ´ãÈááÊ†∑„ÄÇÈÄöËøáÂ∞ÜVRPOÂ∫îÁî®‰∫éLLaDAÔºåÁîüÊàêÁöÑÊ®°ÂûãLLaDA 1.5Âú®Êï∞Â≠¶„ÄÅ‰ª£Á†ÅÂíåÂØπÈΩêÂü∫ÂáÜÊµãËØï‰∏≠ÂùáÊòæËëó‰ºò‰∫éÂÖ∂ÂâçË∫´„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19731', 'title': 'Accelerating Nash Learning from Human Feedback via Mirror Prox', 'url': 'https://huggingface.co/papers/2505.19731', 'abstract': 'Nash Mirror Prox is an online algorithm for Nash Learning from Human Feedback that achieves linear convergence to the Nash equilibrium and is applicable for fine-tuning language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF) offers a more direct alternative by framing the problem as finding a Nash equilibrium of a game defined by these preferences. In this work, we introduce Nash Mirror Prox (Nash-MP), an online NLHF algorithm that leverages the Mirror Prox optimization scheme to achieve fast and stable convergence to the Nash equilibrium. Our theoretical analysis establishes that Nash-MP exhibits last-iterate linear convergence towards the beta-regularized Nash equilibrium. Specifically, we prove that the KL-divergence to the optimal policy decreases at a rate of order (1+2beta)^{-N/2}, where N is a number of preference queries. We further demonstrate last-iterate linear convergence for the exploitability gap and uniformly for the span semi-norm of log-probabilities, with all these rates being independent of the size of the action space. Furthermore, we propose and analyze an approximate version of Nash-MP where proximal steps are estimated using stochastic policy gradients, making the algorithm closer to applications. Finally, we detail a practical implementation strategy for fine-tuning large language models and present experiments that demonstrate its competitive performance and compatibility with existing methods.', 'score': 6, 'issue_id': 3975, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '8c7cc8955de4314c', 'authors': ['Daniil Tiapkin', 'Daniele Calandriello', 'Denis Belomestny', 'Eric Moulines', 'Alexey Naumov', 'Kashif Rasul', 'Michal Valko', 'Pierre Menard'], 'affiliations': ['CMAP, CNRS, √âcole Polytechnique', 'Duisburg-Essen University', 'ENS Lyon', 'Google DeepMind', 'HSE University', 'Hugging Face', 'LMO, Universit√© Paris-Saclay', 'Mohamed Bin Zayed University of AI', 'Stealth Startup / Inria / ENS'], 'pdf_title_img': 'assets/pdf/title_img/2505.19731.jpg', 'data': {'categories': ['#games', '#rlhf', '#training', '#alignment', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': 'Nash Mirror Prox: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Nash Mirror Prox - –æ–Ω–ª–∞–π–Ω-–∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞. –û–Ω –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—é –ù—ç—à–∞ –∏ –ø—Ä–∏–º–µ–Ω–∏–º –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ö–µ–º—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Mirror Prox –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ Nash-MP –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫ –±–µ—Ç–∞-—Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—é –ù—ç—à–∞.'}, 'en': {'title': 'Achieving Fast Convergence to Nash Equilibrium in Language Models', 'desc': 'Nash Mirror Prox (Nash-MP) is a novel online algorithm designed for Nash Learning from Human Feedback (NLHF), which aims to find a Nash equilibrium based on human preferences. Unlike traditional Reinforcement Learning from Human Feedback that relies on potentially inaccurate reward models, Nash-MP directly addresses the complexities of human preferences by framing them as a game. The algorithm achieves linear convergence to the Nash equilibrium, ensuring fast and stable performance, particularly in fine-tuning language models. Additionally, an approximate version of Nash-MP is proposed, utilizing stochastic policy gradients for practical applications, with experiments showing its effectiveness compared to existing methods.'}, 'zh': {'title': 'Âø´ÈÄüÊî∂ÊïõÁöÑÁ∫≥‰ªÄÂ≠¶‰π†ÁÆóÊ≥ï', 'desc': 'Nash Mirror ProxÔºàNash-MPÔºâÊòØ‰∏ÄÁßçÂú®Á∫øÁÆóÊ≥ïÔºåÁî®‰∫é‰ªé‰∫∫Á±ªÂèçÈ¶à‰∏≠ËøõË°åÁ∫≥‰ªÄÂ≠¶‰π†ÔºåËÉΩÂ§üÂø´ÈÄüÁ®≥ÂÆöÂú∞Êî∂ÊïõÂà∞Á∫≥‰ªÄÂùáË°°„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂ•ñÂä±Ê®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåNash-MPÁõ¥Êé•Â∞ÜÈóÆÈ¢òËßÜ‰∏∫ÂØªÊâæÂÅèÂ•ΩÁªìÊûÑ‰∏ãÁöÑÁ∫≥‰ªÄÂùáË°°„ÄÇÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåNash-MPÂú®ÊúÄÂêé‰∏ÄÊ¨°Ëø≠‰ª£‰∏≠‰ª•Á∫øÊÄßÈÄüÂ∫¶Êî∂ÊïõÂà∞Œ≤Ê≠£ÂàôÂåñÁöÑÁ∫≥‰ªÄÂùáË°°ÔºåÂπ∂‰∏îKLÊï£Â∫¶‰ª•ÁâπÂÆöÈÄüÁéá‰∏ãÈôç„ÄÇËØ•ÁÆóÊ≥ïËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçËøë‰ººÁâàÊú¨ÔºåÂà©Áî®ÈöèÊú∫Á≠ñÁï•Ê¢ØÂ∫¶ËøõË°å‰º∞ËÆ°Ôºå‰ΩøÂÖ∂Êõ¥ÈÄÇÂêàÂÆûÈôÖÂ∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18773', 'title': 'Strong Membership Inference Attacks on Massive Datasets and (Moderately)\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.18773', 'abstract': "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.", 'score': 6, 'issue_id': 3973, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '8f8b97df6594a242', 'authors': ['Jamie Hayes', 'Ilia Shumailov', 'Christopher A. Choquette-Choo', 'Matthew Jagielski', 'George Kaissis', 'Katherine Lee', 'Milad Nasr', 'Sahra Ghalebikesabi', 'Niloofar Mireshghallah', 'Meenatchi Sundaram Mutu Selva Annamalai', 'Igor Shilov', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye', 'Franziska Boenisch', 'Adam Dziedzic', 'A. Feder Cooper'], 'affiliations': ['CISPA Helmholtz Center for Information Security', 'Cornell University', 'Google DeepMind', 'Imperial College London', 'University College London', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.18773.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#security', '#leakage'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–ª–∏ —Å–∏–ª—å–Ω—É—é –∞—Ç–∞–∫—É –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ (MIA) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LiRA –¥–ª—è –º–æ–¥–µ–ª–µ–π GPT-2 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤. –û–Ω–∏ –æ–±—É—á–∏–ª–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 20 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ C4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏–ª—å–Ω—ã–µ MIA –º–æ–≥—É—Ç –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏ –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), –Ω–æ –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Å—Ç–∞–µ—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ —Å–≤—è–∑—å –º–µ–∂–¥—É —É—Å–ø–µ—Ö–æ–º MIA –∏ –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ —Ç–∞–∫ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–∞, –∫–∞–∫ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ.'}, 'en': {'title': 'Scaling Membership Inference Attacks on Large Language Models', 'desc': 'This paper investigates the effectiveness of membership inference attacks (MIAs) on large pre-trained language models (LLMs) like GPT-2. It highlights the challenges of scaling MIAs due to the need for training multiple reference models, which has limited previous research. The authors scale the LiRA attack to various GPT-2 architectures and find that while strong MIAs can be effective, their success rates are still relatively low in practical applications. Additionally, the study reveals that the relationship between MIA success and privacy metrics is more complex than previously thought.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰ºöÂëòÊé®Êñ≠ÊîªÂáªÊåëÊàò', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰ºöÂëòÊé®Êñ≠ÊîªÂáªÔºàMIAÔºâÂú®Â§ßÂûãÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÊàë‰ª¨Êâ©Â±ï‰∫ÜLiRAÊîªÂáªÊñπÊ≥ïÔºåÂ∫îÁî®‰∫é‰∏çÂêåËßÑÊ®°ÁöÑGPT-2Ê®°ÂûãÔºåÂπ∂Âú®Ë∂ÖËøá200‰∫ø‰∏™C4Êï∞ÊçÆÈõÜÁöÑÊ†áËÆ∞‰∏äËÆ≠ÁªÉÂèÇËÄÉÊ®°Âûã„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Âº∫Â§ßÁöÑMIAÂèØ‰ª•Âú®È¢ÑËÆ≠ÁªÉÁöÑLLM‰∏äÂèñÂæóÊàêÂäüÔºå‰ΩÜÂÖ∂ÊúâÊïàÊÄßÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠‰ªçÁÑ∂ÊúâÈôêÔºà‰æãÂ¶ÇÔºåAUC<0.7Ôºâ„ÄÇÊ≠§Â§ñÔºåMIAÊàêÂäü‰∏éÈöêÁßÅÊåáÊ†á‰πãÈó¥ÁöÑÂÖ≥Á≥ªÊØî‰πãÂâçÁöÑÁ†îÁ©∂ÊâÄÊöóÁ§∫ÁöÑË¶ÅÂ§çÊùÇÂæóÂ§ö„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10887', 'title': 'InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer\n  Interaction', 'url': 'https://huggingface.co/papers/2505.10887', 'abstract': 'InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces InfantAgent-Next, a generalist agent capable of interacting with computers in a multimodal manner, encompassing text, images, audio, and video. Unlike existing approaches that either build intricate workflows around a single large model or only provide workflow modularity, our agent integrates tool-based and pure vision agents within a highly modular architecture, enabling different models to collaboratively solve decoupled tasks in a step-by-step manner. Our generality is demonstrated by our ability to evaluate not only pure vision-based real-world benchmarks (i.e., OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and SWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld, higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced at https://github.com/bin123apple/InfantAgent.', 'score': 6, 'issue_id': 3968, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'ed2c69dbcbbae339', 'authors': ['Bin Lei', 'Weitai Kang', 'Zijian Zhang', 'Winson Chen', 'Xi Xie', 'Shan Zuo', 'Mimi Xie', 'Ali Payani', 'Mingyi Hong', 'Yan Yan', 'Caiwen Ding'], 'affiliations': ['Cisco Research', 'The University of Texas at San Antonio', 'University of Connecticut', 'University of Illinois Chicago', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2505.10887.jpg', 'data': {'categories': ['#agi', '#open_source', '#benchmark', '#agents', '#multimodal', '#architecture'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á', 'desc': 'InfantAgent-Next - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. –û–Ω —Å–ø–æ—Å–æ–±–µ–Ω –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ. –ê–≥–µ–Ω—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å, —Ä–µ—à–∞—è –∫–∞–∫ —á–∏—Å—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ (OSWorld), —Ç–∞–∫ –∏ –±–æ–ª–µ–µ –æ–±—â–∏–µ –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ-–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ (GAIA, SWE-Bench). –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ OSWorld InfantAgent-Next –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 7.27%, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è Claude-Computer-Use.'}, 'en': {'title': 'Empowering Multimodal Interaction with InfantAgent-Next', 'desc': "InfantAgent-Next is a versatile multimodal agent designed to handle various tasks by integrating both tool-based and vision models. It features a modular architecture that allows different models to work together efficiently, solving tasks step-by-step. This approach contrasts with traditional methods that rely on a single large model or rigid workflows. The agent's effectiveness is showcased through its performance on multiple benchmarks, achieving notable accuracy in real-world scenarios."}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊô∫ËÉΩ‰ΩìÁöÑÊú™Êù•ÔºöInfantAgent-Next', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜInfantAgent-NextÔºåËøôÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊô∫ËÉΩ‰ΩìÔºåËÉΩÂ§üÈÄöËøáÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ë‰∏éËÆ°ÁÆóÊú∫ËøõË°å‰∫§‰∫í„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåInfantAgent-NextÂú®È´òÂ∫¶Ê®°ÂùóÂåñÁöÑÊû∂ÊûÑ‰∏≠Êï¥Âêà‰∫ÜÂü∫‰∫éÂ∑•ÂÖ∑ÂíåÁ∫ØËßÜËßâÁöÑÊô∫ËÉΩ‰ΩìÔºå‰Ωø‰∏çÂêåÊ®°ÂûãËÉΩÂ§ü‰ª•ÈÄêÊ≠•ÁöÑÊñπÂºèÂçè‰ΩúËß£ÂÜ≥Ëß£ËÄ¶‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÊô∫ËÉΩ‰ΩìÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂåÖÊã¨Á∫ØËßÜËßâÁöÑOSWorldÂíåÊõ¥Â§çÊùÇÁöÑGAIA‰∏éSWE-BenchÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÈÄöÁî®ÊÄß„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨Âú®OSWorld‰∏äËææÂà∞‰∫Ü7.27%ÁöÑÂáÜÁ°ÆÁéáÔºåË∂ÖËøá‰∫ÜClaude-Computer-Use„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18384', 'title': 'Dynamic Risk Assessments for Offensive Cybersecurity Agents', 'url': 'https://huggingface.co/papers/2505.18384', 'abstract': "Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.", 'score': 5, 'issue_id': 3971, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'ef99d86a3934644e', 'authors': ['Boyi Wei', 'Benedikt Stroebl', 'Jiacen Xu', 'Joie Zhang', 'Zhou Li', 'Peter Henderson'], 'affiliations': ['Princeton University', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.18384.jpg', 'data': {'categories': ['#cybersecurity', '#agents', '#security'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —É–≥—Ä–æ–∑ –ò–ò –≤ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏ –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—Å—Ç—É–ø–∞—Ç–µ–ª—å–Ω–æ–π –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤ –¥–æ–ª–∂–Ω—ã —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —É–≥—Ä–æ–∑ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–∞–∂–µ —Å –Ω–µ–±–æ–ª—å—à–∏–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –±—é–¥–∂–µ—Ç–æ–º –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞ –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 40% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º —É—Ä–æ–≤–Ω–µ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.'}, 'en': {'title': 'Enhancing Cybersecurity: Adversaries Boost AI with Limited Resources', 'desc': "This paper discusses how adversaries can improve the capabilities of foundation models in offensive cybersecurity, even with limited computational resources. It highlights the importance of dynamic threat model assessments that consider the various options available to adversaries in real-world scenarios. The authors demonstrate that adversaries can enhance an agent's performance significantly, achieving over 40% improvement in cybersecurity tasks with minimal compute resources. This underscores the necessity for more comprehensive evaluations of cybersecurity risks associated with AI agents."}, 'zh': {'title': 'Âä®ÊÄÅËØÑ‰º∞ÂØπÊäóËÄÖÂú®ÁΩëÁªúÂÆâÂÖ®‰∏≠ÁöÑÂ®ÅËÉÅ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂØπÊäóËÄÖÂ¶Ç‰ΩïÂú®ÊúâÈôêÁöÑËÆ°ÁÆóËµÑÊ∫ê‰∏ãÊòæËëóÊèêÂçáÂü∫Á°ÄÊ®°ÂûãÂú®ËøõÊîªÊÄßÁΩëÁªúÂÆâÂÖ®‰∏≠ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂü∫Á°ÄÊ®°ÂûãÂú®Ëá™Âä®ÂåñÁºñÁ®ãÊñπÈù¢ÁöÑËøõÊ≠•ÂèØËÉΩ‰ΩøÂÖ∂ËÉΩÂ§üËá™Âä®ÂåñÂç±Èô©ÁöÑÁΩëÁªúÊîªÂáªÊìç‰Ωú„ÄÇÂΩìÂâçÁöÑÊ®°ÂûãÂÆ°ËÆ°ÂæÄÂæÄÊú™ËÉΩËÄÉËôëÁé∞ÂÆû‰∏ñÁïå‰∏≠ÂØπÊäóËÄÖÁöÑËá™Áî±Â∫¶ÔºåÂØºËá¥ÂØπÁΩëÁªúÂÆâÂÖ®È£éÈô©ÁöÑËØÑ‰º∞‰∏çË∂≥„ÄÇÊàë‰ª¨Âª∫ËÆÆÂú®ÁΩëÁªúÂÆâÂÖ®ËØÑ‰º∞‰∏≠Â∫îËÄÉËôëÊâ©Â±ïÁöÑÂ®ÅËÉÅÊ®°ÂûãÔºå‰ª•Âä®ÊÄÅÊñπÂºèËØÑ‰º∞ÂØπÊäóËÄÖÂú®‰∏çÂêåÁéØÂ¢É‰∏ãÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17652', 'title': 'Rethinking the Sampling Criteria in Reinforcement Learning for LLM\n  Reasoning: A Competence-Difficulty Alignment Perspective', 'url': 'https://huggingface.co/papers/2505.17652', 'abstract': "CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces Competence-Difficulty Alignment Sampling (CDAS), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.", 'score': 5, 'issue_id': 3968, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '84801a2fd0a3c281', 'authors': ['Deyang Kong', 'Qi Guo', 'Xiangyu Xi', 'Wei Wang', 'Jingang Wang', 'Xunliang Cai', 'Shikun Zhang', 'Wei Ye'], 'affiliations': ['Meituan Group, Beijing, China', 'National Engineering Research Center for Software Engineering, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17652.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#math', '#optimization'], 'emoji': 'üéØ', 'ru': {'title': '–¢–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: CDAS –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'CDAS (Competence-Difficulty Alignment Sampling) - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∏–∑–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏ –ø—É—Ç–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –º–æ–¥–µ–ª–∏ —Å–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–¥–∞—á–∏. CDAS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Aligning Model Skills with Task Challenges for Better Learning Efficiency', 'desc': "CDAS, or Competence-Difficulty Alignment Sampling, improves the efficiency of reinforcement learning by aligning the model's abilities with the difficulty of the tasks it faces. Traditional methods struggle with unstable estimates of problem difficulty, leading to poor training outcomes. CDAS addresses this by using historical performance data to accurately assess problem difficulties and adaptively select tasks that match the model's current competence level. Experimental results demonstrate that CDAS significantly enhances both accuracy and speed in solving complex mathematical problems compared to existing strategies."}, 'zh': {'title': 'ËÉΩÂäõ‰∏éÈöæÂ∫¶ÁöÑÂÆåÁæéÂØπÈΩê', 'desc': 'CDASÔºàËÉΩÂäõ-ÈöæÂ∫¶ÂØπÈΩêÈááÊ†∑ÔºâÊó®Âú®Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑ‰ΩéÊ†∑Êú¨ÊïàÁéáÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáÂØπÈóÆÈ¢òÈöæÂ∫¶ÁöÑÂéÜÂè≤Ë°®Áé∞Â∑ÆÂºÇËøõË°åËÅöÂêàÔºåÊèê‰æõÂáÜÁ°Æ‰∏îÁ®≥ÂÆöÁöÑÈöæÂ∫¶‰º∞ËÆ°„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊ†πÊçÆÊ®°ÂûãÂΩìÂâçÁöÑËÉΩÂäõÔºåËá™ÈÄÇÂ∫îÈÄâÊã©‰∏é‰πãÂØπÈΩêÁöÑÈöæÂ∫¶ÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCDASÂú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20254', 'title': 'Position: Mechanistic Interpretability Should Prioritize Feature\n  Consistency in SAEs', 'url': 'https://huggingface.co/papers/2505.20254', 'abstract': 'Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify a canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using a model organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for a community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI.', 'score': 4, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '8f3f187fdcf8d056', 'authors': ['Xiangchen Song', 'Aashiq Muhamed', 'Yujia Zheng', 'Lingjing Kong', 'Zeyu Tang', 'Mona T. Diab', 'Virginia Smith', 'Kun Zhang'], 'affiliations': ['Carnegie Mellon University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20254.jpg', 'data': {'categories': ['#interpretability', '#training', '#math', '#architecture'], 'emoji': 'üîç', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É PW-MCC –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—É—Å–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ—Å—Ç–∏–∂–∏–º–∞ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –≤—ã–±–æ—Ä–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–∑—ã–≤–∞—é—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–º–µ—Ä—è—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Interpretability through Feature Consistency in Sparse Autoencoders', 'desc': 'This paper discusses the importance of feature consistency in Sparse Autoencoders (SAEs) for improving mechanistic interpretability (MI) of neural networks. It highlights that inconsistent features across different training runs can hinder the reliability of MI research. The authors propose the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a metric to measure feature consistency, showing that high consistency levels can be achieved with the right architectural choices. They advocate for a shift in the research community to prioritize and measure feature consistency to enhance the interpretability of neural networks.'}, 'zh': {'title': '‰ºòÂÖàËÄÉËôëÁâπÂæÅ‰∏ÄËá¥ÊÄßÔºåÊèêÂçáÁ•ûÁªèÁΩëÁªúÁöÑÂèØËß£ÈáäÊÄß', 'desc': 'Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÂú®Êú∫Âà∂ÂèØËß£ÈáäÊÄßÔºàMIÔºâ‰∏≠ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Â∞ÜÁ•ûÁªèÁΩëÁªúÁöÑÊøÄÊ¥ªÂàÜËß£‰∏∫ÂèØËß£ÈáäÁöÑÁâπÂæÅ„ÄÇÁÑ∂ËÄåÔºå‰∏çÂêåËÆ≠ÁªÉËøêË°å‰∏≠Â≠¶‰π†Âà∞ÁöÑSAEÁâπÂæÅ‰∏ç‰∏ÄËá¥ÔºåÂΩ±Âìç‰∫ÜMIÁ†îÁ©∂ÁöÑÂèØÈù†ÊÄßÂíåÊïàÁéá„ÄÇÊú¨Êñá‰∏ªÂº†Âú®SAE‰∏≠‰ºòÂÖàËÄÉËôëÁâπÂæÅ‰∏ÄËá¥ÊÄßÔºå‰ª•Á°Æ‰øùÂú®Áã¨Á´ãËøêË°å‰∏≠Êî∂ÊïõÂà∞Á≠âÊïàÁâπÂæÅÈõÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰ΩøÁî®ÊàêÂØπÂ≠óÂÖ∏ÂùáÂÄºÁõ∏ÂÖ≥Á≥ªÊï∞ÔºàPW-MCCÔºâ‰Ωú‰∏∫Ë°°Èáè‰∏ÄËá¥ÊÄßÁöÑÂÆûÁî®ÊåáÊ†áÔºåÂπ∂Â±ïÁ§∫Âú®ÈÄÇÂΩìÁöÑÊû∂ÊûÑÈÄâÊã©‰∏ãÂèØ‰ª•ÂÆûÁé∞È´òÊ∞¥Âπ≥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15804', 'title': 'STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs', 'url': 'https://huggingface.co/papers/2505.15804', 'abstract': "STAR-R1, a novel RL framework with a fine-grained reward mechanism, enhances spatial reasoning in multimodal large language models by addressing limitations in traditional SFT and sparse-reward RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1.", 'score': 4, 'issue_id': 3978, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '4bdc30d74409ba0b', 'authors': ['Zongzhao Li', 'Zongyang Ma', 'Mingze Li', 'Songyou Li', 'Yu Rong', 'Tingyang Xu', 'Ziqi Zhang', 'Deli Zhao', 'Wenbing Huang'], 'affiliations': ['DAMO Academy, Alibaba Group, Hangzhou, China', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Hupan Lab, Hangzhou, China', 'MAIS, Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.15804.jpg', 'data': {'categories': ['#rl', '#multimodal', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ—Ä—ã–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'STAR-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. STAR-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–∞ 23% –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∫—É—Ä—Å–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∞–Ω—Ç—Ä–æ–ø–æ–º–æ—Ä—Ñ–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è.'}, 'en': {'title': 'STAR-R1: Enhancing Spatial Reasoning in MLLMs with Fine-Grained Rewards', 'desc': 'The paper introduces STAR-R1, a new reinforcement learning (RL) framework designed to improve spatial reasoning in multimodal large language models (MLLMs). It addresses the shortcomings of traditional supervised fine-tuning (SFT) and sparse-reward RL by implementing a fine-grained reward mechanism that encourages efficient exploration and precise reasoning. STAR-R1 rewards partial correctness and discourages excessive enumeration, leading to better performance in tasks requiring visual reasoning across different viewpoints. The results show that STAR-R1 significantly outperforms SFT, achieving state-of-the-art results in various metrics and enhancing the understanding of spatial relationships in MLLMs.'}, 'zh': {'title': 'STAR-R1ÔºöÊèêÂçáÁ©∫Èó¥Êé®ÁêÜÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'STAR-R1ÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÈááÁî®ÁªÜÁ≤íÂ∫¶Â•ñÂä±Êú∫Âà∂ÔºåÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞É(SFT)ÂíåÁ®ÄÁñèÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†(RL)Âú®Â§ÑÁêÜÁ©∫Èó¥Êé®ÁêÜÊó∂Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂØºËá¥Êé®ÁêÜË∑ØÂæÑ‰∏çËøûË¥ØÂíåÊé¢Á¥¢ÊïàÁéá‰Ωé‰∏ã„ÄÇSTAR-R1ÈÄöËøáÂ•ñÂä±ÈÉ®ÂàÜÊ≠£Á°ÆÊÄßÂπ∂ÊÉ©ÁΩöËøáÂ∫¶Êûö‰∏æÂíåË¢´Âä®‰∏ç‰Ωú‰∏∫Ôºå‰øÉËøõ‰∫ÜÈ´òÊïàÊé¢Á¥¢ÂíåÁ≤æÁ°ÆÊé®ÁêÜ„ÄÇÁªºÂêàËØÑ‰º∞Ë°®ÊòéÔºåSTAR-R1Âú®11‰∏™ÊåáÊ†á‰∏äÂùáËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂú®Ë∑®ËßÜËßíÂú∫ÊôØ‰∏≠ÊØîSFTÊèêÈ´ò‰∫Ü23%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20294', 'title': 'GLEAM: Learning Generalizable Exploration Policy for Active Mapping in\n  Complex 3D Indoor Scenes', 'url': 'https://huggingface.co/papers/2505.20294', 'abstract': 'A new benchmark and policy, GLEAM-Bench and GLEAM, improve the scalability and reliability of active mapping in complex environments through semantic representations and efficient exploration strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalizable active mapping in complex unknown environments remains a critical challenge for mobile robots. Existing methods, constrained by insufficient training data and conservative exploration strategies, exhibit limited generalizability across scenes with diverse layouts and complex connectivity. To enable scalable training and reliable evaluation, we introduce GLEAM-Bench, the first large-scale benchmark designed for generalizable active mapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets. Building upon this foundation, we propose GLEAM, a unified generalizable exploration policy for active mapping. Its superior generalizability comes mainly from our semantic representations, long-term navigable goals, and randomized strategies. It significantly outperforms state-of-the-art methods, achieving 66.50% coverage (+9.49%) with efficient trajectories and improved mapping accuracy on 128 unseen complex scenes. Project page: https://xiao-chen.tech/gleam/.', 'score': 3, 'issue_id': 3974, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '94f683ed3b0961cc', 'authors': ['Xiao Chen', 'Tai Wang', 'Quanyi Li', 'Tao Huang', 'Jiangmiao Pang', 'Tianfan Xue'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.20294.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#3d', '#robotics', '#synthetic'], 'emoji': 'üó∫Ô∏è', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–∫—Ç–∏–≤–Ω–æ–º –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏–∏: GLEAM –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã', 'desc': 'GLEAM-Bench –∏ GLEAM - —ç—Ç–æ –Ω–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. GLEAM-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç —Å 1152 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ 3D-—Å—Ü–µ–Ω–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏—è. GLEAM - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ü–µ–ª–∏. –≠—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –¥–æ—Å—Ç–∏–≥–∞—è 66.50% –ø–æ–∫—Ä—ã—Ç–∏—è (+9.49%) —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –Ω–∞ 128 –Ω–æ–≤—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö.'}, 'en': {'title': 'Unlocking Generalizable Active Mapping with GLEAM', 'desc': 'This paper presents GLEAM-Bench, a new benchmark for evaluating active mapping in complex environments, featuring 1,152 diverse 3D scenes. It addresses the limitations of existing methods that struggle with generalizability due to lack of training data and conservative exploration. The authors introduce GLEAM, a novel exploration policy that utilizes semantic representations and long-term goals to enhance mapping efficiency. GLEAM demonstrates significant improvements over current techniques, achieving higher coverage and mapping accuracy in previously unseen environments.'}, 'zh': {'title': 'ÊèêÂçáÁßªÂä®Êú∫Âô®‰∫∫‰∏ªÂä®Êò†Â∞ÑÁöÑÈÄöÁî®ÊÄß‰∏éÂèØÈù†ÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫ÜGLEAM-BenchÂíåGLEAMÔºåÊó®Âú®ÊèêÈ´òÁßªÂä®Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑ‰∏ªÂä®Êò†Â∞ÑËÉΩÂäõ„ÄÇGLEAM-BenchÊòØÈ¶ñ‰∏™Â§ßËßÑÊ®°Âü∫ÂáÜÔºåÂåÖÂê´1152‰∏™Â§öÊ†∑ÂåñÁöÑ3DÂú∫ÊôØÔºåÊîØÊåÅÂèØÊâ©Â±ïÁöÑËÆ≠ÁªÉÂíåÂèØÈù†ÁöÑËØÑ‰º∞„ÄÇGLEAMÂàôÊòØ‰∏ÄÁßçÁªü‰∏ÄÁöÑ‰∏ªÂä®Êò†Â∞ÑÊé¢Á¥¢Á≠ñÁï•ÔºåÂà©Áî®ËØ≠‰πâË°®Á§∫ÂíåÈïøÊúüÂèØÂØºËà™ÁõÆÊ†áÔºåÊòæËëóÊèêÂçá‰∫ÜÊñπÊ≥ïÁöÑÈÄöÁî®ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåGLEAMÂú®128‰∏™Êú™ËßÅÂ§çÊùÇÂú∫ÊôØ‰∏≠ÔºåË¶ÜÁõñÁéáËææÂà∞66.50%ÔºåÊØîÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÊèêÈ´ò‰∫Ü9.49%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19630', 'title': 'DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue', 'url': 'https://huggingface.co/papers/2505.19630', 'abstract': 'Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL', 'score': 3, 'issue_id': 3967, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'ab312c010a92062f', 'authors': ['Yichun Feng', 'Jiawei Wang', 'Lu Zhou', 'Yixue Li'], 'affiliations': ['Department of EEIS, University of Science and Technology of China', 'Guangzhou National Laboratory', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.19630.jpg', 'data': {'categories': ['#training', '#reasoning', '#science', '#healthcare', '#dataset', '#games', '#optimization', '#rl'], 'emoji': 'ü©∫', 'ru': {'title': '–£–º–Ω—ã–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –¥–æ–∫—Ç–æ—Ä: –ò–ò —É—á–∏—Ç—Å—è –≤–µ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥ —Å –ø–∞—Ü–∏–µ–Ω—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DoctorAgent-RL - —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≥–¥–µ –∞–≥–µ–Ω—Ç-–≤—Ä–∞—á –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–ø—Ä–æ—Å–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –ø–æ–∑–≤–æ–ª—è—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ DoctorAgent-RL –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.'}, 'en': {'title': 'Revolutionizing Clinical Consultations with Reinforcement Learning', 'desc': "This paper introduces DoctorAgent-RL, a novel framework that enhances biomedical question answering by using reinforcement learning (RL) to improve multi-turn medical consultations. Unlike traditional systems that rely on static data, DoctorAgent-RL allows a doctor agent to adaptively optimize its questioning strategy through dynamic interactions with a patient agent. The framework is designed to intelligently extract relevant clinical information, addressing the limitations of vague patient descriptions. Additionally, the authors present MTMedDialog, a new dataset for simulating patient interactions, which supports the framework's effectiveness in real-world clinical settings."}, 'zh': {'title': 'Êô∫ËÉΩÂåªÁñóÂí®ËØ¢ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÁâ©ÂåªÂ≠¶ÈóÆÁ≠îÈ¢ÜÂüüË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÂÆûÈôÖ‰∏¥Â∫äÂí®ËØ¢‰∏≠ÁöÑÂ∫îÁî®‰ªçÈù¢‰∏¥Ê†∏ÂøÉÊåëÊàò„ÄÇÁé∞ÊúâÁ≥ªÁªü‰æùËµñÂçïÂêë‰ø°ÊÅØ‰º†ÈÄíÊ®°ÂºèÔºåÊÇ£ËÄÖÂøÖÈ°ªÂú®‰∏ÄÊ¨°ÊÄßÊèèËø∞ÁóáÁä∂ÔºåÂØºËá¥Ê®°Á≥äÊäïËØâÊó∂ÁöÑËØäÊñ≠Âª∫ËÆÆ‰∏çÂ§üÂÖ∑‰Ωì„ÄÇ‰º†ÁªüÁöÑÂü∫‰∫éÁõëÁù£Â≠¶‰π†ÁöÑÂ§öËΩÆÂØπËØùÊñπÊ≥ïÂèóÈôê‰∫éÈùôÊÄÅÊï∞ÊçÆÈ©±Âä®ÁöÑËåÉÂºèÔºåÁº∫‰πèÊ≥õÂåñËÉΩÂäõÔºåÈöæ‰ª•Êô∫ËÉΩÊèêÂèñÂÖ≥ÈîÆ‰∏¥Â∫ä‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜDoctorAgent-RLÔºå‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊ°ÜÊû∂ÔºåÂ∞ÜÂåªÁñóÂí®ËØ¢Âª∫Ê®°‰∏∫‰∏çÁ°ÆÂÆöÊÄß‰∏ãÁöÑÂä®ÊÄÅÂÜ≥Á≠ñËøáÁ®ã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19084', 'title': 'Jodi: Unification of Visual Generation and Understanding via Joint\n  Modeling', 'url': 'https://huggingface.co/papers/2505.19084', 'abstract': 'Jodi, a diffusion framework using a linear diffusion transformer and role switch mechanism, unifies visual generation and understanding, performing joint, controllable, and perceptual tasks effectively across multiple visual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi.', 'score': 3, 'issue_id': 3974, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': 'a90dad40e2c803ae', 'authors': ['Yifeng Xu', 'Zhenliang He', 'Meina Kan', 'Shiguang Shan', 'Xilin Chen'], 'affiliations': ['State Key Lab of AI Safety, Institute of Computing Technology, CAS, China', 'University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.19084.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#cv', '#multimodal', '#open_source'], 'emoji': 'üé®', 'ru': {'title': 'Jodi: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'Jodi - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–∏–Ω–µ–π–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–æ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö –∏ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á. Jodi —Å–ø–æ—Å–æ–±–µ–Ω –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –º–µ—Ç–∫–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Jodi –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö.'}, 'en': {'title': 'Unifying Visual Generation and Understanding with Jodi', 'desc': 'Jodi is a novel diffusion framework that integrates visual generation and understanding into a single model. It utilizes a linear diffusion transformer and a role switch mechanism to perform tasks such as joint generation of images and labels, controllable generation based on label conditions, and simultaneous image perception. The framework is trained on the Joint-1.6M dataset, which includes a diverse set of images and labels across multiple visual domains. Experimental results show that Jodi effectively handles both generation and understanding tasks, demonstrating its versatility in various visual applications.'}, 'zh': {'title': 'JodiÔºöÁªü‰∏ÄËßÜËßâÁîüÊàê‰∏éÁêÜËß£ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'JodiÊòØ‰∏Ä‰∏™Êâ©Êï£Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÁ∫øÊÄßÊâ©Êï£ÂèòÊç¢Âô®ÂíåËßíËâ≤ÂàáÊç¢Êú∫Âà∂ÔºåÊó®Âú®Áªü‰∏ÄËßÜËßâÁîüÊàêÂíåÁêÜËß£„ÄÇÂÆÉÈÄöËøáËÅîÂêàÂª∫Ê®°ÂõæÂÉèÂüüÂíåÂ§ö‰∏™Ê†áÁ≠æÂüüÔºåËÉΩÂ§üÊúâÊïàÂú∞ÊâßË°åÁîüÊàê„ÄÅÂèØÊéßÂíåÊÑüÁü•Á≠â‰ªªÂä°„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåJodiÂèØ‰ª•ÂêåÊó∂ÁîüÊàêÂõæÂÉèÂíåÂ§ö‰∏™Ê†áÁ≠æÔºåÂü∫‰∫éÊ†áÁ≠æÁªÑÂêàÁîüÊàêÂõæÂÉèÔºå‰ª•Âèä‰ªéÁªôÂÆöÂõæÂÉè‰∏≠È¢ÑÊµãÂ§ö‰∏™Ê†áÁ≠æ„ÄÇÊ≠§Â§ñÔºåJodiËøòÂºïÂÖ•‰∫ÜJoint-1.6MÊï∞ÊçÆÈõÜÔºåÂåÖÂê´20‰∏áÂº†È´òË¥®ÈáèÂõæÂÉèÂíå7‰∏™ËßÜËßâÂüüÁöÑËá™Âä®Ê†áÁ≠æ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19056', 'title': 'An Embarrassingly Simple Defense Against LLM Abliteration Attacks', 'url': 'https://huggingface.co/papers/2505.19056', 'abstract': "Modifying models to generate justified refusals through fine-tuning on an extended-refusal dataset mitigates ablation attacks while maintaining high refusal rates and general performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance.", 'score': 3, 'issue_id': 3974, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': '2728a5086c10e308', 'authors': ['Harethah Abu Shairah', 'Hasan Abed Al Kader Hammoud', 'Bernard Ghanem', 'George Turkiyyah'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.19056.jpg', 'data': {'categories': ['#training', '#security', '#dataset', '#alignment', '#ethics'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —ç—Ç–∏—á–µ—Å–∫–∏—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ –ò–ò —á–µ—Ä–µ–∑ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–∫–∞–∑—ã', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –∞—Ç–∞–∫, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ –æ—Ç –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–∫–∞–∑–∞–º–∏ –∏ –¥–æ–æ–±—É—á–∏–ª–∏ –Ω–∞ –Ω–µ–º –º–æ–¥–µ–ª–∏ Llama-2 –∏ Qwen2. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –æ—Ç–∫–∞–∑–æ–≤ –¥–∞–∂–µ –ø–æ—Å–ª–µ –∞—Ç–∞–∫, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –±–∞–∑–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–µ–π—Ç—Ä–∞–ª–∏–∑—É–µ—Ç –∞—Ç–∞–∫–∏ –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Strengthening Refusal Mechanisms in LLMs Against Attacks', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to a specific attack called abliteration, which reduces their ability to refuse harmful instructions. The authors propose a solution by creating an extended-refusal dataset that includes harmful prompts along with justifications for refusal. They fine-tune existing models, Llama-2-7B-Chat and Qwen2.5-Instruct, on this dataset to enhance their refusal capabilities. The results show that these modified models maintain high refusal rates even under attack, significantly outperforming baseline models that suffer drastic declines in refusal performance.'}, 'zh': {'title': 'ÈÄöËøáÊâ©Â±ïÊãíÁªùÂæÆË∞ÉÊèêÂçáÊ®°ÂûãÂÆâÂÖ®ÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöËøáÂú®Êâ©Â±ïÊãíÁªùÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÊù•ÁîüÊàêÂêàÁêÜÊãíÁªùÁöÑÊ®°Âûã‰øÆÊîπÊñπÊ≥ï„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂáèËΩªÊ∂àËûçÊîªÂáªÔºåÂêåÊó∂‰øùÊåÅÈ´òÊãíÁªùÁéáÂíåËâØÂ•ΩÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êâ©Â±ïÊãíÁªùÊï∞ÊçÆÈõÜÔºåÂåÖÂê´ÊúâÂÆ≥ÊèêÁ§∫ÂèäÂÖ∂ÊãíÁªùÁêÜÁî±ÁöÑÂÆåÊï¥ÂìçÂ∫î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁªèËøáÊâ©Â±ïÊãíÁªùÂæÆË∞ÉÁöÑÊ®°ÂûãÂú®Èù¢ÂØπÊúâÂÆ≥ÊèêÁ§∫Êó∂ÔºåÊãíÁªùÁéá‰ªÖ‰∏ãÈôçÊúÄÂ§ö10%ÔºåËÄåÂü∫Á∫øÊ®°ÂûãÁöÑÊãíÁªùÁéá‰∏ãÈôç‰∫Ü70-80%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18926', 'title': 'Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time', 'url': 'https://huggingface.co/papers/2505.18926', 'abstract': 'A hybrid neural physics system with diffusion-based control achieves real-time, interactive fluid simulations with low latency and high fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a neural physics system for real-time, interactive fluid simulations. Traditional physics-based methods, while accurate, are computationally intensive and suffer from latency issues. Recent machine-learning methods reduce computational costs while preserving fidelity; yet most still fail to satisfy the latency constraints for real-time use and lack support for interactive applications. To bridge this gap, we introduce a novel hybrid method that integrates numerical simulation, neural physics, and generative control. Our neural physics jointly pursues low-latency simulation and high physical fidelity by employing a fallback safeguard to classical numerical solvers. Furthermore, we develop a diffusion-based controller that is trained using a reverse modeling strategy to generate external dynamic force fields for fluid manipulation. Our system demonstrates robust performance across diverse 2D/3D scenarios, material types, and obstacle interactions, achieving real-time simulations at high frame rates (11~29% latency) while enabling fluid control guided by user-friendly freehand sketches. We present a significant step towards practical, controllable, and physically plausible fluid simulations for real-time interactive applications. We promise to release both models and data upon acceptance.', 'score': 3, 'issue_id': 3983, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 –º–∞—è', 'en': 'May 25', 'zh': '5Êúà25Êó•'}, 'hash': 'cf71badcb13cd468', 'authors': ['Jingxuan Xu', 'Hong Huang', 'Chuhang Zou', 'Manolis Savva', 'Yunchao Wei', 'Wuyang Chen'], 'affiliations': ['Beijing Jiaotong University', 'Meta Reality Labs', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18926.jpg', 'data': {'categories': ['#dataset', '#3d', '#agents', '#diffusion', '#open_source', '#optimization'], 'emoji': 'üíß', 'ru': {'title': '–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∂–∏–¥–∫–æ—Å—Ç–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∂–∏–¥–∫–æ—Å—Ç–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –°–∏—Å—Ç–µ–º–∞ —Å–æ—á–µ—Ç–∞–µ—Ç —á–∏—Å–ª–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Ñ–∏–∑–∏–∫—É –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –ö–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä, –æ–±—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–Ω–µ—à–Ω–∏—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∏–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π. –°–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö 2D/3D —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –Ω–∏–∑–∫–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ –≤—ã—Å–æ–∫–æ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.'}, 'en': {'title': 'Real-Time Fluid Simulations with Neural Physics and Diffusion Control', 'desc': 'This paper presents a hybrid neural physics system designed for real-time fluid simulations that are both interactive and high-fidelity. Traditional methods are often slow and computationally heavy, while recent machine learning approaches can reduce costs but struggle with latency. The proposed system combines numerical simulations with neural physics and a diffusion-based control mechanism to achieve low-latency performance. It allows users to manipulate fluid dynamics through intuitive sketches, making it suitable for various applications in 2D and 3D environments.'}, 'zh': {'title': 'ÂÆûÊó∂ÊµÅ‰ΩìÊ®°ÊãüÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÁ•ûÁªèÁâ©ÁêÜÁ≥ªÁªüÔºåÁî®‰∫éÂÆûÊó∂‰∫§‰∫íÂºèÊµÅ‰ΩìÊ®°Êãü„ÄÇ‰º†ÁªüÁöÑÁâ©ÁêÜÊñπÊ≥ïËôΩÁÑ∂ÂáÜÁ°ÆÔºå‰ΩÜËÆ°ÁÆóÈáèÂ§ßÔºåÂª∂ËøüÈ´ò„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁªìÂêà‰∫ÜÊï∞ÂÄºÊ®°Êãü„ÄÅÁ•ûÁªèÁâ©ÁêÜÂíåÁîüÊàêÊéßÂà∂ÔºåÊó®Âú®ÂÆûÁé∞‰ΩéÂª∂ËøüÂíåÈ´òÁâ©ÁêÜÁúüÂÆûÊÑüÁöÑÊµÅ‰ΩìÊ®°Êãü„ÄÇÈÄöËøá‰ΩøÁî®Êâ©Êï£ÊéßÂà∂Âô®ÔºåÊàë‰ª¨ÁöÑÁ≥ªÁªüËÉΩÂ§üÂú®Â§öÁßçÂú∫ÊôØ‰∏≠ÂÆûÁé∞È´òÂ∏ßÁéáÁöÑÊµÅ‰ΩìÊéßÂà∂ÔºåÊª°Ë∂≥ÂÆûÊó∂‰∫§‰∫íÁöÑÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18323', 'title': 'Architectural Backdoors for Within-Batch Data Stealing and Model\n  Inference Manipulation', 'url': 'https://huggingface.co/papers/2505.18323', 'abstract': 'For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization.', 'score': 3, 'issue_id': 3973, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '10dd310ff67b7a4d', 'authors': ['Nicolas K√ºchler', 'Ivan Petrov', 'Conrad Grobler', 'Ilia Shumailov'], 'affiliations': ['ETH Zurich', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2505.18323.jpg', 'data': {'categories': ['#leakage', '#inference', '#security', '#architecture'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ù–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –±—ç–∫–¥–æ—Ä—ã: —Å–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞ –ø–∞–∫–µ—Ç–Ω–æ–º—É –≤—ã–≤–æ–¥—É –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –±—ç–∫–¥–æ—Ä–æ–≤ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –¥–ª—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞. –≠—Ç–∏ –±—ç–∫–¥–æ—Ä—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∞–º –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –∫—Ä–∞—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–∞–∫–∏—Ö –∞—Ç–∞–∫ –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –º–æ–¥–µ–ª–µ–π. –î–ª—è –ø—Ä–æ—Ç–∏–≤–æ–¥–µ–π—Å—Ç–≤–∏—è —ç—Ç–∏–º —É—è–∑–≤–∏–º–æ—Å—Ç—è–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–º—è–≥—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –≥—Ä–∞—Ñ–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ –Ω–µ–≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –ø–∞–∫–µ—Ç–µ.'}, 'en': {'title': 'Exposing and Mitigating Architectural Backdoors in Neural Network Batching', 'desc': 'This paper explores a new type of backdoor attack in neural networks that targets the batching process during inference, allowing attackers to manipulate and steal user data. Unlike traditional backdoor attacks that focus on altering model predictions, this method exploits architectural vulnerabilities to leak information between concurrent user requests. The authors demonstrate that these attacks can be easily integrated into existing model architectures, posing a significant threat to user privacy and system integrity. To combat this issue, they propose a deterministic mitigation strategy using Information Flow Control to ensure that user inputs remain isolated within the same batch, providing formal guarantees against such attacks.'}, 'zh': {'title': 'Êè≠Á§∫Á•ûÁªèÁΩëÁªú‰∏≠ÁöÑÊñ∞ÂûãÂêéÈó®ÊîªÂáª‰∏éÈò≤Êä§Á≠ñÁï•', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑÁ•ûÁªèÁΩëÁªúÂêéÈó®ÊîªÂáªÔºåÁâπÂà´ÈíàÂØπÊâπÈáèÊé®ÁêÜÊäÄÊúØ„ÄÇËøôÁßçÊîªÂáªÂèØ‰ª•Âú®Âêå‰∏ÄÊâπÊ¨°‰∏≠ÊìçÊéßÁî®Êà∑Êï∞ÊçÆÔºåÂØºËá¥‰ø°ÊÅØÊ≥ÑÈú≤ÂíåÊ®°ÂûãÂìçÂ∫îÁöÑÊéßÂà∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ°ÆÂÆöÊÄßÁöÑÁºìËß£Á≠ñÁï•ÔºåÈÄöËøá‰ø°ÊÅØÊµÅÊéßÂà∂Êú∫Âà∂ÔºåÁ°Æ‰øù‰∏çÂêåÁî®Êà∑ËæìÂÖ•‰πãÈó¥ÁöÑÈùûÂπ≤Êâ∞ÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊîªÂáª‰∏ç‰ªÖÂèØË°å‰∏îÊúâÊïàÔºåÂπ∂‰∏îÂú®Áé∞ÊúâÊ®°ÂûãÊû∂ÊûÑ‰∏≠ÊôÆÈÅçÂ≠òÂú®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16984', 'title': 'UFT: Unifying Supervised and Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.16984', 'abstract': "A new post-training method, Unified Fine-Tuning (UFT), improves upon supervised and reinforcement fine-tuning for large language models by combining their benefits, achieving better generalization and faster convergence.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks.", 'score': 3, 'issue_id': 3980, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'c20894984fdad880', 'authors': ['Mingyang Liu', 'Gabriele Farina', 'Asuman Ozdaglar'], 'affiliations': ['LIDS, EECS, Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.16984.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': 'UFT: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –∏–∑ SFT –∏ RFT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Unified Fine-Tuning (UFT). –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ supervised fine-tuning (SFT) –∏ reinforcement fine-tuning (RFT), —É–ª—É—á—à–∞—è –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏ —É—Å–∫–æ—Ä—è—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å. UFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ UFT –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è RFT –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏, —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É—Å–∫–æ—Ä—è—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º.'}, 'en': {'title': 'Unified Fine-Tuning: Merging Supervised and Reinforcement Learning for Better AI', 'desc': 'Unified Fine-Tuning (UFT) is a new method that enhances the performance of large language models by merging the strengths of supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). This approach allows models to generalize better and converge faster by effectively balancing the need for informative supervision with the exploration of solutions. UFT addresses the overfitting issues seen in SFT and the dependency on strong base models in RFT, making it suitable for various model sizes. The method also theoretically demonstrates a significant improvement in convergence speed for complex reasoning tasks, breaking previous limitations of RFT.'}, 'zh': {'title': 'Áªü‰∏ÄÂæÆË∞ÉÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊô∫ËÉΩ‰∏éÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÁß∞‰∏∫Áªü‰∏ÄÂæÆË∞ÉÔºàUFTÔºâÔºåÊó®Âú®ÁªìÂêàÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâÁöÑ‰ºòÁÇπÔºå‰ªéËÄåÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÊî∂ÊïõÈÄüÂ∫¶„ÄÇSFTÈÄÇÂêàÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩÜÂèØËÉΩÂØºËá¥ËøáÊãüÂêàÔºåËÄåRFTÂàôÂú®Â§ßÂûãÊ®°Âûã‰∏≠Ë°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜ‰æùËµñ‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑÂº∫Â∫¶„ÄÇUFTÈÄöËøáÂ∞ÜSFTÂíåRFTÊï¥Âêà‰∏∫‰∏Ä‰∏™ËøáÁ®ãÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊúâÊïàÊé¢Á¥¢Ëß£ÂÜ≥ÊñπÊ°àÔºåÂêåÊó∂ÂºïÂÖ•ÊúâÁî®ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÂº•Ë°•‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠ËÆ∞ÂøÜ‰∏éÊÄùËÄÉ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUFTÂú®ÂêÑÁßçÊ®°ÂûãËßÑÊ®°‰∏ãÂùá‰ºò‰∫éSFTÂíåRFTÔºåÂπ∂‰∏îÁêÜËÆ∫ËØÅÊòéUFTÊâìÁ†¥‰∫ÜRFTÂõ∫ÊúâÁöÑÊ†∑Êú¨Â§çÊùÇÂ∫¶Áì∂È¢à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16886', 'title': 'Don\'t "Overthink" Passage Reranking: Is Reasoning Truly Necessary?', 'url': 'https://huggingface.co/papers/2505.16886', 'abstract': "With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers.", 'score': 3, 'issue_id': 3978, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '42bc690254efb192', 'authors': ['Nour Jedidi', 'Yung-Sung Chuang', 'James Glass', 'Jimmy Lin'], 'affiliations': ['MIT Lincoln Laboratory', 'Massachusetts Institute of Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.16886.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ü§î', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏: –Ω–µ –≤—Å–µ–≥–¥–∞ –ø—É—Ç—å –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–ª–∏—è–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Å—Å–∞–∂–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–æ–≤ —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ (ReasonRR) –∏ –±–µ–∑ –Ω–∏—Ö (StandardRR) –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ StandardRR –æ–±—ã—á–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç ReasonRR. –î–∞–ª—å–Ω–µ–π—à–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ ReasonRR (ReasonRR-NoReason) –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM —Å–∫–ª–æ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∫ –ø–æ–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —á–∞—Å—Ç–∏—á–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–∞—Å—Å–∞–∂–µ–π.'}, 'en': {'title': 'Reasoning in Rerankers: Less is More!', 'desc': 'This paper investigates the effectiveness of reasoning in passage rerankers that utilize Large Language Models (LLMs) for natural language tasks. The authors compare two types of rerankers: ReasonRR, which incorporates a reasoning process, and StandardRR, which does not. Surprisingly, the results show that StandardRR outperforms ReasonRR, and even a version of ReasonRR without reasoning (ReasonRR-NoReason) performs better than the full reasoning model. The study concludes that the reasoning process in LLMs can lead to overly polarized relevance scores, neglecting the nuanced relevance of passages, which is crucial for accurate reranking.'}, 'zh': {'title': 'Êé®ÁêÜÊú™ÂøÖÊèêÂçáÈáçÊéíÂ∫èÂáÜÁ°ÆÊÄß', 'desc': 'ÈöèÁùÄÊé®ÁêÜÊ®°ÂûãÂú®Â§çÊùÇËá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°‰∏≠ÁöÑÊàêÂäüÔºå‰ø°ÊÅØÊ£ÄÁ¥¢È¢ÜÂüüÁöÑÁ†îÁ©∂ËÄÖÂºÄÂßãÊé¢Á¥¢Â¶Ç‰ΩïÂ∞ÜÁ±ª‰ººÁöÑÊé®ÁêÜËÉΩÂäõÊï¥ÂêàÂà∞Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÆµËêΩÈáçÊéíÂ∫èÂô®‰∏≠„ÄÇËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊòéÁ°ÆÁöÑÈÄêÊ≠•Êé®ÁêÜËøáÁ®ãÔºåÁÑ∂ÂêéÂæóÂá∫ÊúÄÁªàÁöÑÁõ∏ÂÖ≥ÊÄßÈ¢ÑÊµã„ÄÇÁÑ∂ËÄåÔºåÊé®ÁêÜÁúüÁöÑËÉΩÊèêÈ´òÈáçÊéíÂ∫èÁöÑÂáÜÁ°ÆÊÄßÂêóÔºüÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂú®Áõ∏ÂêåÁöÑËÆ≠ÁªÉÊù°‰ª∂‰∏ãÔºåÊ†áÂáÜÁöÑÈùûÊé®ÁêÜÈáçÊéíÂ∫èÂô®ÈÄöÂ∏∏‰ºò‰∫éÂü∫‰∫éÊé®ÁêÜÁöÑÈáçÊéíÂ∫èÂô®Ôºå‰∏îÁ¶ÅÁî®Êé®ÁêÜËøáÁ®ãÁöÑÈáçÊéíÂ∫èÂô®Ë°®Áé∞Âá∫‰πéÊÑèÊñôÂú∞Êõ¥ÊúâÊïà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16312', 'title': 'EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning', 'url': 'https://huggingface.co/papers/2505.16312', 'abstract': 'EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at complex reasoning through search algorithms, yet current strategies often suffer from massive token consumption due to redundant exploration of semantically equivalent steps. Existing semantic similarity methods struggle to accurately identify such equivalence in domain-specific contexts like mathematical reasoning. To address this, we propose EquivPruner, a simple yet effective approach that identifies and prunes semantically equivalent actions during LLM reasoning search. We also introduce MathEquiv, the first dataset we created for mathematical statement equivalence, which enables the training of a lightweight equivalence detector. Extensive experiments across various models and tasks demonstrate that EquivPruner significantly reduces token consumption, improving searching efficiency and often bolstering reasoning accuracy. For instance, when applied to Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by 48.1\\% while also improving accuracy. Our code is available at https://github.com/Lolo1222/EquivPruner.', 'score': 3, 'issue_id': 3972, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '6ae91cff5462f129', 'authors': ['Jiawei Liu', 'Qisi Chen', 'Jianshu Zhang', 'Quan Liu', 'Defu Lian'], 'affiliations': ['University of Science and Technology of China', 'iFLYTEK Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.16312.jpg', 'data': {'categories': ['#reasoning', '#data', '#dataset', '#optimization', '#training'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': 'EquivPruner: —É–º–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö LLM', 'desc': 'EquivPruner - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω —É–º–µ–Ω—å—à–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å, –æ—Ç—Å–µ–∫–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏—è. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç MathEquiv, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø—Ä–∏–º–µ—Ä—ã –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ EquivPruner.'}, 'en': {'title': 'Streamlining Reasoning with EquivPruner: Less Tokens, More Accuracy!', 'desc': 'EquivPruner is a novel method designed to enhance the efficiency of Large Language Models (LLMs) by reducing unnecessary token usage during reasoning tasks. It achieves this by identifying and eliminating semantically equivalent actions in the search process, which helps streamline the reasoning flow. The paper introduces a new dataset called MathEquiv, specifically created to train a lightweight equivalence detector for mathematical statements. Experimental results show that EquivPruner can significantly lower token consumption while improving the accuracy of reasoning tasks, demonstrating its effectiveness in optimizing LLM performance.'}, 'zh': {'title': '‰øÆÂâ™Á≠â‰ª∑Âä®‰ΩúÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá', 'desc': 'EquivPrunerÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊêúÁ¥¢‰∏≠‰øÆÂâ™ËØ≠‰πâÁ≠â‰ª∑ÁöÑÂä®‰ΩúÔºåÂáèÂ∞ë‰∫Ü‰ª§ÁâåÊ∂àËÄóÂπ∂ÊèêÈ´ò‰∫ÜÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®‰∫ÜÊàë‰ª¨ÂàõÂª∫ÁöÑMathEquivÊï∞ÊçÆÈõÜÔºå‰∏ìÊ≥®‰∫éÊï∞Â≠¶Á≠â‰ª∑ÊÄßÔºå‰ªéËÄåÊúâÊïàËØÜÂà´ÂíåÂéªÈô§ÂÜó‰ΩôÁöÑÊêúÁ¥¢Ê≠•È™§„ÄÇÁé∞ÊúâÁöÑËØ≠‰πâÁõ∏‰ººÊÄßÊñπÊ≥ïÂú®ÁâπÂÆöÈ¢ÜÂüüÔºàÂ¶ÇÊï∞Â≠¶Êé®ÁêÜÔºâ‰∏≠Èöæ‰ª•ÂáÜÁ°ÆËØÜÂà´Á≠â‰ª∑ÊÄßÔºåËÄåEquivPrunerÂàôÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEquivPrunerÂú®Â§ö‰∏™Ê®°ÂûãÂíå‰ªªÂä°‰∏≠ÊòæËëóÈôç‰Ωé‰∫Ü‰ª§ÁâåÊ∂àËÄóÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÊé®ÁêÜÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19706', 'title': 'Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision', 'url': 'https://huggingface.co/papers/2505.19706', 'abstract': 'PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.', 'score': 2, 'issue_id': 3967, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'e60e6086053e62d0', 'authors': ['Tej Deep Pala', 'Panshul Sharma', 'Amir Zadeh', 'Chuan Li', 'Soujanya Poria'], 'affiliations': ['Lambda Labs', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.19706.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#hallucinations', '#dataset', '#optimization'], 'emoji': 'üßÆ', 'ru': {'title': '–¢–æ—á–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å PathFinder-PRM', 'desc': 'PathFinder-PRM - —ç—Ç–æ –Ω–æ–≤–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è –æ—à–∏–±–∫–∏, –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –æ—à–∏–±–æ–∫ –∏ –æ—Ü–µ–Ω–∫—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞. PathFinder-PRM –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –º–µ—Ç—Ä–∏–∫–µ PRMScore, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏ —ç—Ç–æ–º –≤ 3 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'PathFinder-PRM: Enhancing Math Problem-Solving with Fine-Grained Error Detection', 'desc': 'PathFinder-PRM is a new model designed to enhance mathematical problem-solving by focusing on detailed error classification and assessing the correctness of each step in the solution process. Unlike traditional Outcome Reward Models that only evaluate final answers, this model uses a hierarchical and error-aware approach to score intermediate steps, which helps guide the generation of coherent solutions. It was trained on a large dataset that includes fine-grained labels for errors, allowing it to achieve a state-of-the-art PRMScore while using significantly less data. The results show that this model not only improves error detection but also enhances overall performance in reward-guided reasoning tasks.'}, 'zh': {'title': 'ÊèêÂçáÊï∞Â≠¶Êé®ÁêÜÁöÑÈîôËØØÊÑüÁü•Ê®°Âûã', 'desc': 'PathFinder-PRMÊòØ‰∏ÄÁßçÂ±ÇÊ¨°Âåñ‰∏îÂÖ∑Â§áÈîôËØØÊÑüÁü•ÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºåÊó®Âú®ÈÄöËøáÁªÜËá¥ÁöÑÈîôËØØÂàÜÁ±ªÂíåÊ≠•È™§Ê≠£Á°ÆÊÄß‰º∞ËÆ°Êù•ÊèêÂçáÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂØπÊØè‰∏™Ê≠•È™§ÁöÑÊï∞Â≠¶ÈîôËØØÂíå‰∏ÄËá¥ÊÄßÈîôËØØËøõË°åÂàÜÁ±ªÔºåÁªìÂêàËøô‰∫õÁªÜËá¥ÁöÑ‰ø°Âè∑Êù•ËØÑ‰º∞Ê≠•È™§ÁöÑÊ≠£Á°ÆÊÄß„ÄÇÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´40‰∏áÊ†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜÔºåPathFinder-PRMÂú®PRMBench‰∏äËææÂà∞‰∫Ü67.7ÁöÑÊúÄÊñ∞Áä∂ÊÄÅÔºå‰ΩøÁî®ÁöÑÊï∞ÊçÆÈáèÊØî‰πãÂâçÂáèÂ∞ë‰∫Ü‰∏âÂÄç„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåËß£ËÄ¶ÁöÑÈîôËØØÊ£ÄÊµãÂíåÂ•ñÂä±‰º∞ËÆ°‰∏ç‰ªÖÊèêÂçá‰∫ÜÁªÜÁ≤íÂ∫¶ÈîôËØØÊ£ÄÊµãÁöÑËÉΩÂäõÔºåËøòÊòæËëóÊîπÂñÑ‰∫ÜÂü∫‰∫éÂ•ñÂä±ÁöÑÊï∞Â≠¶Êé®ÁêÜÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18283', 'title': 'TAGS: A Test-Time Generalist-Specialist Framework with\n  Retrieval-Augmented Reasoning and Verification', 'url': 'https://huggingface.co/papers/2505.18283', 'abstract': 'TAGS, a test-time framework combining generalist and specialist models with hierarchical retrieval and reliability scoring, enhances medical LLM reasoning without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting-based methods often remain shallow and unstable, while fine-tuned medical LLMs suffer from poor generalization under distribution shifts and limited adaptability to unseen clinical scenarios. To address these limitations, we present TAGS, a test-time framework that combines a broadly capable generalist with a domain-specific specialist to offer complementary perspectives without any model fine-tuning or parameter updates. To support this generalist-specialist reasoning process, we introduce two auxiliary modules: a hierarchical retrieval mechanism that provides multi-scale exemplars by selecting examples based on both semantic and rationale-level similarity, and a reliability scorer that evaluates reasoning consistency to guide final answer aggregation. TAGS achieves strong performance across nine MedQA benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several fine-tuned medical LLMs, without any parameter updates. The code will be available at https://github.com/JianghaoWu/TAGS.', 'score': 2, 'issue_id': 3976, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '6cb15e47a7465ed3', 'authors': ['Jianghao Wu', 'Feilong Tang', 'Yulong Li', 'Ming Hu', 'Haochen Xue', 'Shoaib Jameel', 'Yutong Xie', 'Imran Razzak'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'Monash University', 'University of Southampton', 'Xian Jiaotong-Liverpool University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18283.jpg', 'data': {'categories': ['#multimodal', '#healthcare', '#optimization', '#reasoning', '#training'], 'emoji': 'ü©∫', 'ru': {'title': 'TAGS: –°–∏–Ω–µ—Ä–≥–∏—è –≥–µ–Ω–µ—Ä–∞–ª–∏—Å—Ç–æ–≤ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö LLM', 'desc': 'TAGS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª—å-–≥–µ–Ω–µ—Ä–∞–ª–∏—Å—Ç –∏ –º–æ–¥–µ–ª—å-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –æ—Ü–µ–Ω–∫—É –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. TAGS –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'TAGS: Enhancing Medical LLMs with Generalist-Specialist Synergy', 'desc': 'The paper introduces TAGS, a novel framework that enhances medical reasoning in large language models (LLMs) without the need for fine-tuning. It combines a generalist model with a specialist model to leverage their strengths, providing a more robust reasoning process. TAGS incorporates a hierarchical retrieval system to select relevant examples and a reliability scoring mechanism to assess the consistency of the reasoning. This approach significantly improves performance on medical question-answering benchmarks, demonstrating that effective model combination can outperform traditional fine-tuning methods.'}, 'zh': {'title': 'TAGSÔºöÊó†ÂæÆË∞ÉÁöÑÂåªÁñóÊé®ÁêÜÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫TAGSÁöÑÊµãËØïÊó∂Ê°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫ÂåªÁñóÈ¢ÜÂüüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÄåÊó†ÈúÄËøõË°åÊ®°ÂûãÂæÆË∞É„ÄÇTAGSÁªìÂêà‰∫ÜÈÄöÁî®Ê®°ÂûãÂíå‰∏ì‰∏öÊ®°ÂûãÔºåÈÄöËøáÂ±ÇÊ¨°Ê£ÄÁ¥¢ÂíåÂèØÈù†ÊÄßËØÑÂàÜÊù•Êèê‰æõ‰∫íË°•ÁöÑÊé®ÁêÜËßÜËßí„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•‰∫Ü‰∏§‰∏™ËæÖÂä©Ê®°ÂùóÔºöÂ±ÇÊ¨°Ê£ÄÁ¥¢Êú∫Âà∂ÂíåÂèØÈù†ÊÄßËØÑÂàÜÂô®Ôºå‰ª•ÊîØÊåÅÊõ¥ÊúâÊïàÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTAGSÂú®‰πù‰∏™MedQAÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÔºåË∂ÖË∂ä‰∫ÜÂ§öÁßçÂæÆË∞ÉÁöÑÂåªÁñóLLM„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18116', 'title': 'Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.18116', 'abstract': "Negative-aware Fine-Tuning (NFT) enhances LLMs' math abilities using supervised learning with negative feedback, achieving performance comparable to RL methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems.", 'score': 2, 'issue_id': 3987, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': '7810c6a91231902d', 'authors': ['Huayu Chen', 'Kaiwen Zheng', 'Qinsheng Zhang', 'Ganqu Cui', 'Yin Cui', 'Haotian Ye', 'Tsung-Yi Lin', 'Ming-Yu Liu', 'Jun Zhu', 'Haoxiang Wang'], 'affiliations': ['NVIDIA', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18116.jpg', 'data': {'categories': ['#rl', '#training', '#math', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£—á–∏–º—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Negative-aware Fine-Tuning (NFT). NFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —É—á–∏—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –æ—à–∏–±–∫–∞—Ö –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —É—á–∏—Ç–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NFT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–º –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å NFT –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.'}, 'en': {'title': "Unlocking LLMs' Math Skills with Negative Feedback", 'desc': 'This paper introduces Negative-aware Fine-Tuning (NFT), a novel supervised learning method that enhances the mathematical abilities of large language models (LLMs) by incorporating negative feedback. Unlike traditional reinforcement learning (RL) approaches that rely on positive reinforcement, NFT allows LLMs to learn from their mistakes by modeling negative outputs as part of the training process. The authors demonstrate that NFT can achieve performance levels comparable to leading RL methods while using a supervised framework, thus challenging the belief that self-improvement is exclusive to RL. Through experiments on various model sizes, the results show that NFT not only improves upon existing supervised learning techniques but also aligns closely with RL methods in specific training scenarios.'}, 'zh': {'title': 'Ë¥üÂèçÈ¶àÂæÆË∞ÉÔºöÊèêÂçáLLMsÊï∞Â≠¶ËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÁß∞‰∏∫Ë¥üÂèçÈ¶àÂæÆË∞ÉÔºàNFTÔºâÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊï∞Â≠¶ËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊñπÊ≥ï‰∏çÂêåÔºåNFTÂà©Áî®Ëá™ÊàëÁîüÊàêÁöÑË¥üÈù¢Á≠îÊ°àËøõË°åËÆ≠ÁªÉÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂèçÊÄùÈîôËØØÂπ∂Ëá™‰∏ªÊîπËøõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNFTÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†Âü∫Á∫øÔºåÁîöËá≥‰∏éÈ¢ÜÂÖàÁöÑRLÁÆóÊ≥ïÁõ∏Â™≤Áæé„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåNFTÂíåRL‰πãÈó¥ÁöÑÂ∑ÆË∑ùÂæó‰ª•Áº©Â∞èÔºåÂ±ïÁ§∫‰∫ÜË¥üÂèçÈ¶àÂú®Â≠¶‰π†Á≥ªÁªü‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15957', 'title': 'Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey', 'url': 'https://huggingface.co/papers/2505.15957', 'abstract': "A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.  \t\t\t\t\tAI-generated summary \t\t\t\t With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.", 'score': 2, 'issue_id': 3968, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'ed5ebb17c81ad1e0', 'authors': ['Chih-Kai Yang', 'Neo S. Ho', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15957.jpg', 'data': {'categories': ['#ethics', '#reasoning', '#audio', '#benchmark', '#multimodal', '#survey'], 'emoji': 'üéß', 'ru': {'title': '–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LALM). –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏—è: –æ–±—â–µ–µ —Å–ª—É—Ö–æ–≤–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π, –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å. –≠—Ç–æ –ø–µ—Ä–≤—ã–π –æ–±–∑–æ—Ä, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∏–π—Å—è –∏–º–µ–Ω–Ω–æ –Ω–∞ –æ—Ü–µ–Ω–∫–µ LALM, —á—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —á–µ—Ç–∫–∏–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –†–∞–±–æ—Ç–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'A Unified Framework for Evaluating Large Audio-Language Models', 'desc': 'This paper presents a structured approach to evaluate large audio-language models (LALMs) by proposing a systematic taxonomy. It identifies four key dimensions for assessment: auditory awareness, knowledge reasoning, dialogue ability, and fairness. The authors highlight the current fragmentation in benchmarks and aim to provide clarity and direction for future evaluations. This survey is the first of its kind, offering comprehensive insights and guidelines for researchers in the field of LALMs.'}, 'zh': {'title': 'Á≥ªÁªüËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàÜÁ±ªÊ≥ï', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑÂàÜÁ±ªÊ≥ïÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàLALMsÔºâ„ÄÇËØÑ‰º∞Áª¥Â∫¶ÂåÖÊã¨Âê¨ËßâÊÑèËØÜ„ÄÅÁü•ËØÜÊé®ÁêÜ„ÄÅÂØπËØùËÉΩÂäõÂíåÂÖ¨Âπ≥ÊÄßÔºå‰ª•Ëß£ÂÜ≥ËØ•È¢ÜÂüüÂü∫ÂáÜÊµãËØïÁöÑÁ¢éÁâáÂåñÈóÆÈ¢ò„ÄÇÈÄöËøáÂØπÁé∞ÊúâÊñáÁåÆÁöÑÂÖ®Èù¢Ë∞ÉÊü•ÔºåËÆ∫Êñá‰∏∫LALMÁöÑËØÑ‰º∞Êèê‰æõ‰∫ÜÊ∏ÖÊô∞ÁöÑÊåáÂØºÔºåÂπ∂ÊåáÂá∫‰∫ÜÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊ≠§Á†îÁ©∂ÊòØÈ¶ñÊ¨°‰∏ìÊ≥®‰∫éLALMËØÑ‰º∞ÁöÑË∞ÉÊü•Ôºå‰∏∫Áõ∏ÂÖ≥Á§æÂå∫Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÂèÇËÄÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19800', 'title': 'MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs', 'url': 'https://huggingface.co/papers/2505.19800', 'abstract': "A framework leveraging Large Language Models (LLMs) is introduced for automatic metadata extraction from scientific papers, with a focus on datasets from languages other than Arabic.  \t\t\t\t\tAI-generated summary \t\t\t\t Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets' scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: https://github.com/IVUL-KAUST/MOLE and dataset: https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.", 'score': 1, 'issue_id': 3974, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': 'dba6eea5b69199a0', 'authors': ['Zaid Alyafeai', 'Maged S. Al-Shaibani', 'Bernard Ghanem'], 'affiliations': ['KAUST', 'SDAIA-KFUPM Joint Research Center for AI, KFUPM'], 'pdf_title_img': 'assets/pdf/title_img/2505.19800.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#data', '#multilingual', '#science', '#open_source', '#low_resource'], 'emoji': 'üîç', 'ru': {'title': 'MOLE: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ MOLE, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ü–µ–ª—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö –∏ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ–π –∑–∞–¥–∞—á–∏, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–∞–ª—å–Ω–µ–π—à–∞—è —Ä–∞–±–æ—Ç–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Automating Metadata Extraction with Large Language Models', 'desc': 'This paper introduces MOLE, a framework that uses Large Language Models (LLMs) to automatically extract metadata from scientific papers, specifically focusing on datasets in languages other than Arabic. The framework aims to improve the efficiency of metadata extraction, which is crucial for research discovery and reproducibility in the rapidly growing field of scientific research. MOLE employs a schema-driven approach to process various document formats and includes validation mechanisms to ensure the accuracy of the extracted metadata. The authors also present a new benchmark for evaluating progress in this area and demonstrate that LLMs can effectively automate metadata extraction, while highlighting areas for future improvement.'}, 'zh': {'title': 'Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËá™Âä®ÊèêÂèñÁßëÂ≠¶ËÆ∫ÊñáÂÖÉÊï∞ÊçÆ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËá™Âä®ÊèêÂèñÁßëÂ≠¶ËÆ∫ÊñáÂÖÉÊï∞ÊçÆÁöÑÊ°ÜÊû∂ÔºåÁâπÂà´ÂÖ≥Ê≥®ÈùûÈòøÊãâ‰ºØËØ≠ÁöÑÊï∞ÊçÆÈõÜ„ÄÇÂÖÉÊï∞ÊçÆÊèêÂèñÂØπ‰∫éÁßëÂ≠¶Á†îÁ©∂ÁöÑÂèëÁé∞ÂíåÂèØÈáçÂ§çÊÄßËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁßëÂ≠¶Á†îÁ©∂Âø´ÈÄüÂ¢ûÈïøÁöÑËÉåÊôØ‰∏ã„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑMOLEÊ°ÜÊû∂ËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßçËæìÂÖ•Ê†ºÂºèÁöÑÂÆåÊï¥ÊñáÊ°£ÔºåÂπ∂ÈááÁî®Âº∫Â§ßÁöÑÈ™åËØÅÊú∫Âà∂‰ª•Á°Æ‰øùËæìÂá∫ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊù•ËØÑ‰º∞ËØ•‰ªªÂä°ÁöÑÁ†îÁ©∂ËøõÂ±ïÔºåÂ±ïÁ§∫‰∫ÜÁé∞‰ª£LLMsÂú®Ëá™Âä®ÂåñÂÖÉÊï∞ÊçÆÊèêÂèñÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19440', 'title': 'The Birth of Knowledge: Emergent Features across Time, Space, and Scale\n  in Large Language Models', 'url': 'https://huggingface.co/papers/2505.19440', 'abstract': 'This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, we identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models.', 'score': 1, 'issue_id': 3979, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '49e0286d6a9b6731', 'authors': ['Shashata Sawmya', 'Micah Adler', 'Nir Shavit'], 'affiliations': ['Massachusetts Institute of Technology', 'Red Hat, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.19440.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –≤ –≥–ª—É–±–∏–Ω–∞—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø–æ—è–≤–ª–µ–Ω–∏—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è, –≤ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –º–æ–¥–µ–ª–∏. –ò—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –æ–Ω–∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, –∫–æ–≥–¥–∞ –∏ –≥–¥–µ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —á–µ—Ç–∫–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –ø–æ—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö.'}, 'en': {'title': 'Unveiling the Dynamics of Feature Emergence in Language Models', 'desc': 'This paper investigates how interpretable categorical features develop in large language models (LLMs) over time, across different layers, and with varying model sizes. It employs sparse autoencoders to achieve mechanistic interpretability, pinpointing the emergence of specific semantic concepts in neural activations. The findings show distinct thresholds for feature emergence that depend on the training time and model scale, highlighting the complexity of feature development. Additionally, the study uncovers surprising patterns of semantic reactivation, where features from earlier layers reappear in later layers, questioning traditional views on how representations evolve in transformer architectures.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂèØËß£ÈáäÁâπÂæÅ', 'desc': 'Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÂèØËß£ÈáäÁöÑÂàÜÁ±ªÁâπÂæÅÁöÑÂá∫Áé∞ÔºåÂàÜÊûê‰∫ÜÂÆÉ‰ª¨Âú®ËÆ≠ÁªÉÊ£ÄÊü•ÁÇπÔºàÊó∂Èó¥Ôºâ„ÄÅÂèòÊç¢Âô®Â±ÇÔºàÁ©∫Èó¥ÔºâÂíå‰∏çÂêåÊ®°ÂûãËßÑÊ®°ÔºàËßÑÊ®°Ôºâ‰∏äÁöÑË°å‰∏∫„ÄÇÊàë‰ª¨‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ËøõË°åÊú∫Âà∂ÂèØËß£ÈáäÊÄßÔºåËØÜÂà´Âá∫ÁâπÂÆöËØ≠‰πâÊ¶ÇÂøµÂú®Á•ûÁªèÊøÄÊ¥ª‰∏≠ÁöÑÂá∫Áé∞Êó∂Êú∫Âíå‰ΩçÁΩÆ„ÄÇÁªìÊûúË°®ÊòéÔºåÂú®Â§ö‰∏™È¢ÜÂüü‰∏≠ÔºåÁâπÂæÅÂá∫Áé∞Â≠òÂú®ÊòéÊòæÁöÑÊó∂Èó¥ÂíåËßÑÊ®°ÁâπÂÆöÈòàÂÄº„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÁ©∫Èó¥ÂàÜÊûêÊè≠Á§∫‰∫ÜÊÑèÊÉ≥‰∏çÂà∞ÁöÑËØ≠‰πâÂÜçÊøÄÊ¥ªÁé∞Ë±°ÔºåÊó©ÊúüÂ±ÇÁöÑÁâπÂæÅÂú®ÂêéÊúüÂ±ÇÈáçÊñ∞Âá∫Áé∞ÔºåËøôÊåëÊàò‰∫ÜÂØπÂèòÊç¢Âô®Ê®°ÂûãË°®Á§∫Âä®ÊÄÅÁöÑÊ†áÂáÜÂÅáËÆæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.19415', 'title': 'MMIG-Bench: Towards Comprehensive and Explainable Evaluation of\n  Multi-Modal Image Generation Models', 'url': 'https://huggingface.co/papers/2505.19415', 'abstract': 'A comprehensive benchmark, MMIG-Bench, evaluates multi-modal image generators using text prompts and reference images, providing detailed insights through low-level, mid-level, and high-level metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and customized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, a comprehensive Multi-Modal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design. We will release the dataset and evaluation code to foster rigorous, unified evaluation and accelerate future innovations in multi-modal image generation.', 'score': 1, 'issue_id': 3984, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '26d84e35c5cc1799', 'authors': ['Hang Hua', 'Ziyun Zeng', 'Yizhi Song', 'Yunlong Tang', 'Liu He', 'Daniel Aliaga', 'Wei Xiong', 'Jiebo Luo'], 'affiliations': ['NVIDIA', 'Purdue University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.19415.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#dataset', '#survey', '#open_source', '#benchmark'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'MMIG-Bench: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'MMIG-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è 4850 —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ 1750 —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è 380 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–º. –ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—é—â—É—é –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, —Å—Ä–µ–¥–Ω–µ—É—Ä–æ–≤–Ω–µ–≤—É—é –º–µ—Ç—Ä–∏–∫—É Aspect Matching Score (AMS) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —ç—Å—Ç–µ—Ç–∏–∫–∏. –° –ø–æ–º–æ—â—å—é MMIG-Bench –±—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã 17 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è Gemini 2.5 Pro –∏ FLUX.'}, 'en': {'title': 'Unifying Evaluation for Multi-Modal Image Generators', 'desc': 'The paper introduces MMIG-Bench, a new benchmark designed to evaluate multi-modal image generators that use both text prompts and reference images. It addresses the limitations of existing evaluation methods by providing a unified framework that includes low-level, mid-level, and high-level metrics. The benchmark features a large dataset with 4,850 annotated text prompts and 1,750 reference images, allowing for comprehensive assessments of model performance. By testing 17 advanced models, the authors validate their metrics against human ratings, aiming to enhance the evaluation process in multi-modal image generation.'}, 'zh': {'title': 'Áªü‰∏ÄËØÑ‰º∞Â§öÊ®°ÊÄÅÂõæÂÉèÁîüÊàêÁöÑÂü∫ÂáÜ', 'desc': 'MMIG-BenchÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§öÊ®°ÊÄÅÂõæÂÉèÁîüÊàêÂü∫ÂáÜÔºåÊó®Âú®ÈÄöËøáÊñáÊú¨ÊèêÁ§∫ÂíåÂèÇËÄÉÂõæÂÉèËØÑ‰º∞Â§öÊ®°ÊÄÅÂõæÂÉèÁîüÊàêÂô®„ÄÇËØ•Âü∫ÂáÜÁªìÂêà‰∫Ü4,850‰∏™‰∏∞ÂØåÊ≥®ÈáäÁöÑÊñáÊú¨ÊèêÁ§∫Âíå1,750‰∏™Â§öËßÜËßíÂèÇËÄÉÂõæÂÉèÔºåÊ∂µÁõñ‰∫∫Á±ª„ÄÅÂä®Áâ©„ÄÅÁâ©‰ΩìÂíåËâ∫ÊúØÈ£éÊ†º„ÄÇMMIG-BenchÈááÁî®‰∏âÂ±ÇËØÑ‰º∞Ê°ÜÊû∂ÔºåÂåÖÊã¨‰ΩéÁ∫ßÊåáÊ†á„ÄÅ‰∏≠Á∫ßÁöÑAspect Matching ScoreÔºàAMSÔºâÂíåÈ´òÁ∫ßÁæéÂ≠¶ÊåáÊ†áÔºå‰ª•Êèê‰æõËØ¶ÁªÜÁöÑËØÑ‰º∞ÁªìÊûú„ÄÇÈÄöËøáÂØπ17‰∏™ÊúÄÂÖàËøõÊ®°ÂûãÁöÑÂü∫ÂáÜÊµãËØïÔºåMMIG-Bench‰∏∫Â§öÊ®°ÊÄÅÂõæÂÉèÁîüÊàêÁöÑÊû∂ÊûÑÂíåÊï∞ÊçÆËÆæËÆ°Êèê‰æõ‰∫ÜÊ∑±ÂÖ•ÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18454', 'title': 'Hybrid Latent Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.18454', 'abstract': "Hybrid reasoning policy optimization (HRPO) leverages reinforcement learning to integrate latent reasoning with large language models, enhancing performance in knowledge- and reasoning-intensive tasks while maintaining interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning.", 'score': 1, 'issue_id': 3984, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 –º–∞—è', 'en': 'May 24', 'zh': '5Êúà24Êó•'}, 'hash': '386942e77eeb844e', 'authors': ['Zhenrui Yue', 'Bowen Jin', 'Huimin Zeng', 'Honglei Zhuang', 'Zhen Qin', 'Jinsung Yoon', 'Lanyu Shang', 'Jiawei Han', 'Dong Wang'], 'affiliations': ['Google', 'LMU', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.18454.jpg', 'data': {'categories': ['#rlhf', '#training', '#optimization', '#interpretability', '#reasoning', '#multilingual', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–∫—Ä—ã—Ç–æ–µ –∏ —è–≤–Ω–æ–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (HRPO), –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. HRPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—è –¥–æ–ª—é —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ HRPO –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with Hybrid Optimization', 'desc': 'Hybrid Reasoning Policy Optimization (HRPO) is a novel approach that combines reinforcement learning with large language models (LLMs) to enhance reasoning capabilities. It integrates latent reasoning by utilizing prior hidden states and a learnable gating mechanism, allowing for more informative feature extraction. Unlike traditional methods that rely on discrete chain-of-thought paths, HRPO maintains the generative nature of LLMs while introducing stochasticity through token sampling. This results in improved performance on knowledge- and reasoning-intensive tasks, while also preserving interpretability and revealing interesting behaviors in the models.'}, 'zh': {'title': 'Ê∑∑ÂêàÊé®ÁêÜÔºå‰ºòÂåñÊú™Êù•ÔºÅ', 'desc': 'Ê∑∑ÂêàÊé®ÁêÜÁ≠ñÁï•‰ºòÂåñÔºàHRPOÔºâÁªìÂêà‰∫ÜÂº∫ÂåñÂ≠¶‰π†‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊèêÂçá‰∫ÜÂú®Áü•ËØÜÂíåÊé®ÁêÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËß£ÈáäÊÄß„ÄÇHRPOÈÄöËøáÂºïÂÖ•ÈöêÁä∂ÊÄÅÂíåÂèØÂ≠¶‰π†ÁöÑÈó®ÊéßÊú∫Âà∂ÔºåÂ∞ÜÂÖàÂâçÁöÑÈöêÁä∂ÊÄÅ‰∏éÈááÊ†∑ÁöÑÊ†áËÆ∞ÁªìÂêàËµ∑Êù•Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÊé®ÁêÜ„ÄÇËØ•ÊñπÊ≥ïÂú®ËÆ≠ÁªÉÂàùÊúü‰∏ªË¶Å‰ΩøÁî®Ê†áËÆ∞ÂµåÂÖ•ÔºåÈÄêÊ≠•ÂºïÂÖ•Êõ¥Â§öÁöÑÈöêÁâπÂæÅÔºå‰øùÊåÅ‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁîüÊàêËÉΩÂäõ„ÄÇÈÄöËøáÂºïÂÖ•ÈöèÊú∫ÊÄßÔºåHRPOËÉΩÂ§üÂú®‰∏ç‰æùËµñÈìæÂºèÊé®ÁêÜËΩ®ËøπÁöÑÊÉÖÂÜµ‰∏ãËøõË°å‰ºòÂåñÔºåÂ±ïÁé∞Âá∫‰ºò‰∫é‰ª•ÂæÄÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.18291', 'title': 'InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning', 'url': 'https://huggingface.co/papers/2505.18291', 'abstract': "A new benchmark, InstructPart, and a task-oriented part segmentation dataset are introduced to evaluate and improve the performance of Vision-Language Models in real-world contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal foundation models, particularly in the domains of language and vision, have significantly advanced various tasks, including robotics, autonomous driving, information retrieval, and grounding. However, many of these models perceive objects as indivisible, overlooking the components that constitute them. Understanding these components and their associated affordances provides valuable insights into an object's functionality, which is fundamental for performing a wide range of tasks. In this work, we introduce a novel real-world benchmark, InstructPart, comprising hand-labeled part segmentation annotations and task-oriented instructions to evaluate the performance of current models in understanding and executing part-level tasks within everyday contexts. Through our experiments, we demonstrate that task-oriented part segmentation remains a challenging problem, even for state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark, we introduce a simple baseline that achieves a twofold performance improvement through fine-tuning with our dataset. With our dataset and benchmark, we aim to facilitate research on task-oriented part segmentation and enhance the applicability of VLMs across various domains, including robotics, virtual reality, information retrieval, and other related fields. Project website: https://zifuwan.github.io/InstructPart/.", 'score': 1, 'issue_id': 3982, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 –º–∞—è', 'en': 'May 23', 'zh': '5Êúà23Êó•'}, 'hash': 'ea625ba03695e0d2', 'authors': ['Zifu Wan', 'Yaqi Xie', 'Ce Zhang', 'Zhiqiu Lin', 'Zihan Wang', 'Simon Stepputtis', 'Deva Ramanan', 'Katia Sycara'], 'affiliations': ['Robotics Institute, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18291.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#robotics', '#benchmark', '#survey'], 'emoji': 'üß©', 'ru': {'title': '–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —á–∞—Å—Ç–µ–π –æ–±—ä–µ–∫—Ç–æ–≤: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ InstructPart –∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–∞—Å—Ç–µ–π –æ–±—ä–µ–∫—Ç–æ–≤, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∑–∞–¥–∞—á–∏. –¶–µ–ª—å - –æ—Ü–µ–Ω–∏—Ç—å –∏ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —á–∞—Å—Ç–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π –¥–∞–∂–µ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö Vision-Language –º–æ–¥–µ–ª–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –¥–≤–∞ —Ä–∞–∑–∞ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏.'}, 'en': {'title': 'Enhancing Vision-Language Models with InstructPart for Task-Oriented Segmentation', 'desc': 'This paper presents a new benchmark called InstructPart, designed to assess and enhance the capabilities of Vision-Language Models (VLMs) in understanding and executing tasks related to object parts. The dataset includes detailed part segmentation annotations and task-oriented instructions, which help models learn to recognize and utilize the components of objects effectively. The authors highlight that even advanced VLMs struggle with task-oriented part segmentation, indicating the complexity of this problem. By providing this benchmark and a baseline model that improves performance through fine-tuning, the authors aim to advance research in this area and broaden the application of VLMs in practical scenarios.'}, 'zh': {'title': 'ÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÉ®‰ª∂ÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïInstructPartÔºå‰ª•Âèä‰∏Ä‰∏™Èù¢Âêë‰ªªÂä°ÁöÑÈÉ®‰ª∂ÂàÜÂâ≤Êï∞ÊçÆÈõÜÔºåÊó®Âú®ËØÑ‰º∞ÂíåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑË°®Áé∞„ÄÇËÆ∏Â§öÁé∞ÊúâÊ®°ÂûãÂ∞ÜÁâ©‰ΩìËßÜ‰∏∫‰∏çÂèØÂàÜÂâ≤ÁöÑÊï¥‰ΩìÔºåÂøΩËßÜ‰∫ÜÊûÑÊàêÁâ©‰ΩìÁöÑÂêÑ‰∏™ÈÉ®ÂàÜ„ÄÇÁêÜËß£Ëøô‰∫õÈÉ®ÂàÜÂèäÂÖ∂ÂäüËÉΩÂØπ‰∫éÊâßË°åÂêÑÁßç‰ªªÂä°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÈÄöËøáÂÆûÈ™åÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®‰ªªÂä°ÂØºÂêëÁöÑÈÉ®‰ª∂ÂàÜÂâ≤‰∏ä‰ªçÈù¢‰∏¥ÊåëÊàòÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂü∫Á∫øÔºåÈÄöËøáÂæÆË∞ÉÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÂÆûÁé∞‰∫Ü‰∏§ÂÄçÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12737', 'title': 'Option-aware Temporally Abstracted Value for Offline Goal-Conditioned\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.12737', 'abstract': "Option-aware Temporally Abstracted (OTA) value learning improves offline goal-conditioned reinforcement learning performance by refining the high-level policy through better advantage estimates in long-horizon settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm where goal-reaching policies are trained from abundant unlabeled (reward-free) datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. By identifying the root cause of this challenge, we observe the following insights: First, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Second, when learning the high-level policy in the long-horizon regime, the sign of the advantage signal frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage signal for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, the proposed learning scheme contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy extracted using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments.", 'score': 1, 'issue_id': 3977, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'd7ac138a20e7f945', 'authors': ['Hongjoon Ahn', 'Heewoong Choi', 'Jisu Han', 'Taesup Moon'], 'affiliations': ['Department of ECE / IPAI / ASRI / INMC, Seoul National University', 'Department of Electrical and Computer Engineering (ECE), Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12737.jpg', 'data': {'categories': ['#training', '#rl', '#rlhf', '#optimization', '#reasoning', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': '–í—Ä–µ–º–µ–Ω–Ω–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Option-aware Temporally Abstracted (OTA) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º–µ —Å –∑–∞–¥–∞–Ω–∏–µ–º —Ü–µ–ª–µ–π. OTA —É–ª—É—á—à–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—É—é –ø–æ–ª–∏—Ç–∏–∫—É. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –∏ —É—á–µ—Ç–µ –æ–ø—Ü–∏–π –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ OTA –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤.'}, 'en': {'title': 'Enhancing Long-Horizon Learning with OTA Value Learning', 'desc': "The paper introduces Option-aware Temporally Abstracted (OTA) value learning, which enhances offline goal-conditioned reinforcement learning (GCRL) by improving the high-level policy's performance in long-horizon tasks. It identifies that the main issues arise from the high-level policy's difficulty in generating suitable subgoals and the incorrect advantage signals during learning. By refining the value function to be option-aware, OTA allows for better advantage estimates, effectively shortening the learning horizon. Experimental results demonstrate that policies derived from OTA significantly outperform existing methods on complex tasks, showcasing its effectiveness in offline GCRL settings."}, 'zh': {'title': 'ÈÄâÈ°πÊÑüÁü•Êó∂Èó¥ÊäΩË±°ÂÄºÂ≠¶‰π†ÊèêÂçáÂº∫ÂåñÂ≠¶‰π†Ë°®Áé∞', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÈÄâÈ°πÊÑüÁü•Êó∂Èó¥ÊäΩË±°ÂÄºÂ≠¶‰π†ÔºàOTAÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑÁ¶ªÁ∫øÁõÆÊ†áÊù°‰ª∂Âº∫ÂåñÂ≠¶‰π†ÔºàGCRLÔºâÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÈ´òÂ±ÇÁ≠ñÁï•Âú®ÁîüÊàêÈÄÇÂΩìÂ≠êÁõÆÊ†áÊñπÈù¢Â≠òÂú®Áì∂È¢àÔºåÂØºËá¥‰ºòÂäø‰ø°Âè∑‰∏çÂáÜÁ°Æ„ÄÇOTAÈÄöËøáÂ∞ÜÊó∂Èó¥ÊäΩË±°ÂºïÂÖ•Êó∂Èó¥Â∑ÆÂàÜÂ≠¶‰π†ËøáÁ®ãÔºå‰ºòÂåñ‰∫Ü‰ª∑ÂÄºÂáΩÊï∞Ôºå‰ªéËÄåÊèê‰æõÊõ¥Ê∏ÖÊô∞ÁöÑ‰ºòÂäø‰ø°Âè∑„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®OTAÂÄºÂáΩÊï∞ÊèêÂèñÁöÑÈ´òÂ±ÇÁ≠ñÁï•Âú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20290', 'title': 'EgoZero: Robot Learning from Smart Glasses', 'url': 'https://huggingface.co/papers/2505.20290', 'abstract': 'EgoZero learns robust manipulation policies for robots using in-the-wild human demonstrations and zero robot data, enabling zero-shot transfer across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that learns robust manipulation policies from human demonstrations captured with Project Aria smart glasses, and zero robot data. EgoZero enables: (1) extraction of complete, robot-executable actions from in-the-wild, egocentric, human demonstrations, (2) compression of human visual observations into morphology-agnostic state representations, and (3) closed-loop policy learning that generalizes morphologically, spatially, and semantically. We deploy EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of data collection per task. Our results suggest that in-the-wild human data can serve as a scalable foundation for real-world robot learning - paving the way toward a future of abundant, diverse, and naturalistic training data for robots. Code and videos are available at https://egozero-robot.github.io.', 'score': 0, 'issue_id': 3980, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '64efed1421a47a18', 'authors': ['Vincent Liu', 'Ademi Adeniji', 'Haotian Zhan', 'Raunaq Bhirangi', 'Pieter Abbeel', 'Lerrel Pinto'], 'affiliations': ['New York University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.20290.jpg', 'data': {'categories': ['#agents', '#optimization', '#robotics', '#transfer_learning'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º –±–µ–∑ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': 'EgoZero - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç —Ä–æ–±–∞—Å—Ç–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ª—é–¥–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –∏ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –æ—Ç —Ä–æ–±–æ—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–µ, –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–µ —Ä–æ–±–æ—Ç–æ–º –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π —á–µ–ª–æ–≤–µ–∫–∞, —Å–∂–∏–º–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ –∞–≥–Ω–æ—Å—Ç–∏—á–Ω—ã–µ –∫ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –æ–±—É—á–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É —Å –∑–∞–º–∫–Ω—É—Ç—ã–º —Ü–∏–∫–ª–æ–º. EgoZero –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º —Å 70% —É—Å–ø–µ—Ö–æ–º –Ω–∞ 7 –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 20 –º–∏–Ω—É—Ç —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∑–∞–¥–∞—á—É.'}, 'en': {'title': 'Learning Robot Skills from Human Actions, No Robot Data Needed!', 'desc': 'EgoZero is a novel system that enables robots to learn manipulation skills by observing human actions without needing any robot-specific data. It utilizes human demonstrations captured through smart glasses to extract actionable insights and create adaptable representations that are not tied to any specific robot design. The system employs closed-loop policy learning, allowing it to generalize across different tasks and environments effectively. With a 70% success rate in zero-shot transfer across multiple manipulation tasks, EgoZero highlights the potential of using real-world human data to enhance robotic learning capabilities.'}, 'zh': {'title': 'Âà©Áî®‰∫∫Á±ªÁ§∫ËåÉÂÆûÁé∞Êú∫Âô®‰∫∫Èõ∂Ê†∑Êú¨Â≠¶‰π†', 'desc': 'EgoZero ÊòØ‰∏Ä‰∏™Â≠¶‰π†Êú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•ÁöÑÁ≥ªÁªüÔºåÂÆÉÂà©Áî®‰∫∫Á±ªÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÁ§∫ËåÉÔºåËÄå‰∏çÈúÄË¶Å‰ªª‰ΩïÊú∫Âô®‰∫∫Êï∞ÊçÆ„ÄÇËØ•Á≥ªÁªüËÉΩÂ§ü‰ªé‰∫∫Á±ªÁöÑÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßíÁ§∫ËåÉ‰∏≠ÊèêÂèñÂèØÊâßË°åÁöÑÊìç‰ΩúÔºåÂπ∂Â∞Ü‰∫∫Á±ªÁöÑËßÜËßâËßÇÂØüÂéãÁº©‰∏∫‰∏éÂΩ¢ÊÄÅÊó†ÂÖ≥ÁöÑÁä∂ÊÄÅË°®Á§∫„ÄÇEgoZero ËøòÂÆûÁé∞‰∫ÜÈó≠ÁéØÁ≠ñÁï•Â≠¶‰π†ÔºåËÉΩÂ§üÂú®‰∏çÂêåÁöÑÂΩ¢ÊÄÅ„ÄÅÁ©∫Èó¥ÂíåËØ≠‰πâ‰∏äËøõË°åÊ≥õÂåñ„ÄÇÈÄöËøáÂú® Franka Panda Êú∫Âô®‰∫∫‰∏äÈÉ®ÁΩ≤ EgoZero Á≠ñÁï•ÔºåÊàë‰ª¨Âú®‰∏É‰∏™Êìç‰Ωú‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü 70% ÁöÑÈõ∂Ê†∑Êú¨ËΩ¨ÁßªÊàêÂäüÁéáÔºåË°®ÊòéÁúüÂÆû‰∏ñÁïåÁöÑ‰∫∫Á±ªÊï∞ÊçÆÂèØ‰ª•‰∏∫Êú∫Âô®‰∫∫Â≠¶‰π†Êèê‰æõÂèØÊâ©Â±ïÁöÑÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.20225', 'title': 'FLAME-MoE: A Transparent End-to-End Research Platform for\n  Mixture-of-Experts Language Models', 'url': 'https://huggingface.co/papers/2505.20225', 'abstract': 'FLAME-MoE is an open-source research suite for MoE architectures in LLMs, providing tools to investigate scaling, routing, and expert behavior with reproducible experiments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.', 'score': 0, 'issue_id': 3981, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 –º–∞—è', 'en': 'May 26', 'zh': '5Êúà26Êó•'}, 'hash': '92965f65b95b3e0e', 'authors': ['Hao Kang', 'Zichun Yu', 'Chenyan Xiong'], 'affiliations': ['Foundation and Language Model Center Carnegie Mellon University', 'Language Technologies Institute', 'School of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2505.20225.jpg', 'data': {'categories': ['#dataset', '#open_source', '#training', '#architecture', '#optimization'], 'emoji': 'üî•', 'ru': {'title': 'FLAME-MoE: –û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è Mixture-of-Experts –≤ LLM', 'desc': 'FLAME-MoE - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä Mixture-of-Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏. –ù–∞–±–æ—Ä –≤–∫–ª—é—á–∞–µ—Ç —Å–µ–º—å –º–æ–¥–µ–ª–µ–π –¥–µ–∫–æ–¥–µ—Ä–∞ —Å 64 —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, –≤–µ—Ä—Ö–Ω–∏–º–∏ 8 –≥–µ–π—Ç–∞–º–∏ –∏ 2 –æ–±—â–∏–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, —á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ LLM. FLAME-MoE –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 3.4 –ø—É–Ω–∫—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ FLOP.'}, 'en': {'title': 'Unlocking Efficiency in Language Models with FLAME-MoE', 'desc': 'FLAME-MoE is an open-source research suite designed for exploring Mixture-of-Experts (MoE) architectures in large language models (LLMs). It provides a comprehensive platform for researchers to study scaling, routing, and expert behavior through reproducible experiments. The suite includes seven decoder-only models with varying active parameters, closely mimicking modern LLMs, and offers full transparency of training data and processes. Initial analyses indicate that experts specialize in distinct token subsets, maintain sparse co-activation matrices, and exhibit stable routing behavior early in training, leading to improved accuracy over dense baselines.'}, 'zh': {'title': 'FLAME-MoEÔºöÊé¢Á¥¢Ê∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÁöÑÂºÄÊ∫êÂπ≥Âè∞', 'desc': 'FLAME-MoEÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁ†îÁ©∂Â•ó‰ª∂Ôºå‰∏ìÊ≥®‰∫éÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂ∫îÁî®„ÄÇËØ•Âπ≥Âè∞Êèê‰æõ‰∫ÜÂ∑•ÂÖ∑Êù•Á†îÁ©∂Ê®°ÂûãÁöÑÊâ©Â±ïÊÄß„ÄÅË∑ØÁî±Âíå‰∏ìÂÆ∂Ë°å‰∏∫ÔºåÂπ∂ÊîØÊåÅÂèØÈáçÂ§çÁöÑÂÆûÈ™å„ÄÇFLAME-MoEÂåÖÂê´‰∏É‰∏™‰ªÖËß£Á†ÅÂô®Ê®°ÂûãÔºåÂèÇÊï∞ËåÉÂõ¥‰ªé3800‰∏áÂà∞17‰∫øÔºåÊû∂ÊûÑËÆæËÆ°‰∏éÁé∞‰ª£Áîü‰∫ßLLMsÁõ∏‰ºº„ÄÇÈÄöËøáÂÖ≠‰∏™ËØÑ‰º∞‰ªªÂä°ÔºåFLAME-MoEÂú®‰∏éÁõ∏ÂêåFLOPsÁöÑÂØÜÈõÜÂü∫Á∫øÁõ∏ÊØîÔºåÂπ≥ÂùáÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜÊúÄÂ§ö3.4‰∏™ÁôæÂàÜÁÇπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14071', 'title': 'Textual Steering Vectors Can Improve Visual Understanding in Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.14071', 'abstract': "Steering methods have emerged as effective and targeted tools for guiding large language models' (LLMs) behavior without modifying their parameters. Multimodal large language models (MLLMs), however, do not currently enjoy the same suite of techniques, due in part to their recency and architectural diversity. Inspired by this gap, we investigate whether MLLMs can be steered using vectors derived from their text-only LLM backbone, via sparse autoencoders (SAEs), mean shift, and linear probing. We find that text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks. In particular, mean shift boosts spatial relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to +3.3%, outperforming prompting and exhibiting strong generalization to out-of-distribution datasets. These results highlight textual steering vectors as a powerful, efficient mechanism for enhancing grounding in MLLMs with minimal additional data collection and computational overhead.", 'score': 0, 'issue_id': 3987, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '5cc7321d9950a5db', 'authors': ['Woody Haosheng Gan', 'Deqing Fu', 'Julian Asilis', 'Ollie Liu', 'Dani Yogatama', 'Vatsal Sharan', 'Robin Jia', 'Willie Neiswanger'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.14071.jpg', 'data': {'categories': ['#alignment', '#training', '#cv', '#optimization', '#multimodal'], 'emoji': 'üß≠', 'ru': {'title': '–¢–µ–∫—Å—Ç–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ –∫–æ–º–ø–∞—Å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è (steering) –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–æ–π –æ—Å–Ω–æ–≤—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã, —Å–¥–≤–∏–≥ —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ –ª–∏–Ω–µ–π–Ω–æ–µ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ MLLM –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –ø–æ–¥—Å—á–µ—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing MLLM Performance with Text-Derived Steering Vectors', 'desc': 'This paper explores how to steer multimodal large language models (MLLMs) using techniques derived from text-only language models. The authors utilize sparse autoencoders, mean shift, and linear probing to create steering vectors that improve the performance of MLLMs on various visual tasks. Their findings show that these text-derived steering methods significantly enhance accuracy, particularly in spatial relationships and counting tasks, compared to traditional prompting methods. Overall, the study demonstrates that textual steering vectors can effectively improve MLLM performance with minimal extra data and computational costs.'}, 'zh': {'title': 'ÊñáÊú¨ÂØºÂêëÔºöÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂº∫Â§ßÂ∑•ÂÖ∑', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÊñáÊú¨ÂØºÂêëÂêëÈáèÊù•ÂºïÂØºÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑË°å‰∏∫ÔºåËÄåÊó†ÈúÄ‰øÆÊîπÂÖ∂ÂèÇÊï∞„ÄÇÊàë‰ª¨‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEsÔºâ„ÄÅÂùáÂÄºÊºÇÁßªÂíåÁ∫øÊÄßÊé¢ÊµãÁ≠âÊäÄÊúØÔºåÂèëÁé∞ÊñáÊú¨ÂØºÂêëÁöÑÂºïÂØºÊñπÊ≥ïËÉΩÂ§üÊòæËëóÊèêÈ´òÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®‰∏çÂêåËßÜËßâ‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁâπÂà´ÊòØÔºåÂùáÂÄºÊºÇÁßªÂú®CV-Bench‰∏äÊèêÂçá‰∫ÜÁ©∫Èó¥ÂÖ≥Á≥ªÂáÜÁ°ÆÊÄßÈ´òËææ7.3%ÔºåÂπ∂‰∏îÂú®ËÆ°Êï∞ÂáÜÁ°ÆÊÄß‰∏äÊèêÂçá‰∫Ü3.3%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÊñáÊú¨ÂØºÂêëÂêëÈáèÊòØ‰∏ÄÁßçÂº∫Â§ß‰∏îÈ´òÊïàÁöÑÊú∫Âà∂ÔºåÂèØ‰ª•Âú®ÊúÄÂ∞èÁöÑÊï∞ÊçÆÊî∂ÈõÜÂíåËÆ°ÁÆóÂºÄÈîÄ‰∏ãÂ¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Á°ÄËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15277', 'title': 'Web-Shepherd: Advancing PRMs for Reinforcing Web Agents', 'url': 'https://huggingface.co/papers/2505.15277', 'abstract': 'Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.', 'score': 78, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '8209bc2f2e5e6119', 'authors': ['Hyungjoo Chae', 'Sunghwan Kim', 'Junhee Cho', 'Seungone Kim', 'Seungjun Moon', 'Gyeom Hwangbo', 'Dongha Lim', 'Minjin Kim', 'Yeonjun Hwang', 'Minju Gwak', 'Dongwook Choi', 'Minseok Kang', 'Gwanhoon Im', 'ByeongUng Cho', 'Hyojun Kim', 'Jun Hee Han', 'Taeyoon Kwon', 'Minju Kim', 'Beong-woo Kwak', 'Dongjin Kang', 'Jinyoung Yeo'], 'affiliations': ['Carnegie Mellon University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15277.jpg', 'data': {'categories': ['#optimization', '#open_source', '#survey', '#benchmark', '#rl', '#dataset'], 'emoji': 'üß≠', 'ru': {'title': 'Web-Shepherd: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Web-Shepherd - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –¥–ª—è –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö WebPRM Collection —Å 40 —Ç—ã—Å—è—á–∞–º–∏ –ø–∞—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —á–µ–∫-–ª–∏—Å—Ç–∞–º–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ WebRewardBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ PRM –º–æ–¥–µ–ª–µ–π. Web-Shepherd –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å GPT-4o –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –≤ –∑–∞–¥–∞—á–∞—Ö –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏.'}, 'en': {'title': 'Web-Shepherd: Revolutionizing Web Navigation with Process Reward Models', 'desc': 'This paper introduces Web-Shepherd, a novel process reward model (PRM) designed specifically for web navigation tasks that require long-term decision making. The authors highlight the limitations of using multimodal large language models (MLLMs) as reward models, which can hinder practical applications due to speed and cost issues. They present the WebPRM Collection, a comprehensive dataset containing 40,000 step-level preference pairs to train the PRM effectively. Experimental results demonstrate that Web-Shepherd significantly outperforms existing models, achieving higher accuracy and lower costs in web navigation tasks.'}, 'zh': {'title': 'ÁΩëÈ°µÂØºËà™ÁöÑÊô∫ËÉΩËØÑ‰º∞Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºåÂêç‰∏∫Web-ShepherdÔºåÊó®Âú®ËØÑ‰º∞ÁΩëÈ°µÂØºËà™ÁöÑÂÜ≥Á≠ñËøáÁ®ã„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫WebPRM CollectionÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂåÖÂê´4‰∏áÂØπÊ≠•È™§Á∫ßÂÅèÂ•ΩÂØπÂíåÊ≥®ÈáäÊ∏ÖÂçïÔºåÊ∂µÁõñÂ§öÁßçÈ¢ÜÂüüÂíåÈöæÂ∫¶Á∫ßÂà´„ÄÇÈÄöËøáÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Web-ShepherdÂú®WebRewardBench‰∏äÊØî‰ΩøÁî®GPT-4oÁöÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜÁ∫¶30‰∏™ÁôæÂàÜÁÇπÔºåÂπ∂‰∏îÂú®WebArena-liteÊµãËØï‰∏≠Ôºå‰ΩøÁî®Web-Shepherd‰Ωú‰∏∫È™åËØÅÂô®Êó∂ÔºåÊÄßËÉΩÊèêÂçá‰∫Ü10.9ÁÇπÔºå‰∏îÊàêÊú¨Èôç‰Ωé‰∫Ü10„ÄÇÊàë‰ª¨ÁöÑÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂùáÂ∑≤ÂÖ¨ÂºÄ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14302', 'title': 'Scaling Law for Quantization-Aware Training', 'url': 'https://huggingface.co/papers/2505.14302', 'abstract': 'Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.', 'score': 55, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'ecea48e2af693a28', 'authors': ['Mengzhao Chen', 'Chaoyi Zhang', 'Jing Liu', 'Yutao Zeng', 'Zeyue Xue', 'Zhiheng Liu', 'Yunshui Li', 'Jin Ma', 'Jie Huang', 'Xun Zhou', 'Ping Luo'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.14302.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': 'üî¨', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è (QAT) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –µ–¥–∏–Ω—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è QAT, –º–æ–¥–µ–ª–∏—Ä—É—é—â–∏–π –æ—à–∏–±–∫—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏—é —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä—É–ø–ø—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ—à–∏–±–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –Ω–æ —Ä–∞—Å—Ç–µ—Ç —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –±–æ–ª–µ–µ –≥—Ä—É–±–æ–π –≥—Ä–∞–Ω—É–ª—è—Ü–∏–µ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏. –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ –æ—à–∏–±–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ —Å–ª–æ–µ FC2 —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–º —É–∑–∫–∏–º –º–µ—Å—Ç–æ–º –≤ 4-–±–∏—Ç–Ω–æ–π QAT.'}, 'en': {'title': 'Optimizing Quantization-Aware Training for Large Language Models', 'desc': 'This paper investigates the challenges of deploying large language models (LLMs) due to their high computational and memory requirements. It introduces quantization-aware training (QAT) as a solution to reduce model precision while preserving performance, particularly focusing on 4-bit precision (W4A4). The authors propose a new scaling law for QAT that considers factors like model size, training data volume, and quantization granularity, revealing how quantization error behaves under these conditions. Their experiments show that while increasing model size reduces quantization error, more training tokens and coarser quantization lead to higher errors, highlighting the need for mixed-precision quantization to optimize performance.'}, 'zh': {'title': 'ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÁöÑÁªü‰∏ÄÊâ©Â±ïÊ≥ïÂàô', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òËµÑÊ∫êÔºåËøôÁªôÈÉ®ÁΩ≤Â∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÈÄöËøáÈôç‰ΩéÊ®°ÂûãÁ≤æÂ∫¶Êù•Â∫îÂØπËøô‰∫õÊåëÊàòÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåQATÂú®4‰ΩçÁ≤æÂ∫¶ÔºàW4A4Ôºâ‰∏ãÁöÑÊâ©Â±ïË°å‰∏∫Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑQATÊâ©Â±ïÊ≥ïÂàôÔºåÊ®°ÂûãÂåñÈáèÂåñËØØÂ∑Æ‰∏éÊ®°ÂûãÂ§ßÂ∞è„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÈáèÂíåÈáèÂåñÁªÑÂ§ßÂ∞è‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15809', 'title': 'MMaDA: Multimodal Large Diffusion Language Models', 'url': 'https://huggingface.co/papers/2505.15809', 'abstract': "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA", 'score': 53, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'ff99ceb93709180d', 'authors': ['Ling Yang', 'Ye Tian', 'Bowen Li', 'Xinchen Zhang', 'Ke Shen', 'Yunhai Tong', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Peking University', 'Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15809.jpg', 'data': {'categories': ['#training', '#multimodal', '#open_source', '#architecture', '#reasoning', '#diffusion', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'MMaDA: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏', 'desc': 'MMaDA - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –≤–∫–ª—é—á–∞—è —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –æ–±—â–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–æ–π –∏ –º–æ–¥–∞–ª—å–Ω–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–º –¥–∏–∑–∞–π–Ω–æ–º, —É—Å—Ç—Ä–∞–Ω—è—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. MMaDA –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–º–µ—à–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å –¥–ª–∏–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º UniGRPO, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MMaDA-8B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±–æ–±—â–µ–Ω–∏—è –∫–∞–∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'MMaDA: Unifying Multimodal Learning for Superior Performance', 'desc': 'MMaDA is a new type of multimodal diffusion model that excels in various tasks like understanding text and images, and generating images from text. It features a unified architecture that processes different data types without needing separate components for each type. The model uses a special training method that aligns reasoning across text and visuals, making it easier to learn complex tasks. Additionally, it includes a unique reinforcement learning algorithm that improves performance in both reasoning and generation tasks, showing strong results compared to other leading models.'}, 'zh': {'title': 'MMaDAÔºöÂ§öÊ®°ÊÄÅÊâ©Êï£Ê®°ÂûãÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'MMaDAÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÊâ©Êï£Âü∫Á°ÄÊ®°ÂûãÔºåÊó®Âú®Âú®ÊñáÊú¨Êé®ÁêÜ„ÄÅÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁ≠âÂ§ö‰∏™È¢ÜÂüüÂÆûÁé∞ÂçìË∂äÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÁöÑ‰∏â‰∏™ÂÖ≥ÈîÆÂàõÊñ∞ÂåÖÊã¨ÔºöÈ¶ñÂÖàÔºåMMaDAÈááÁî®Áªü‰∏ÄÁöÑÊâ©Êï£Êû∂ÊûÑÔºåÊ∂àÈô§‰∫ÜÂØπÁâπÂÆöÊ®°ÊÄÅÁªÑ‰ª∂ÁöÑÈúÄÊ±ÇÔºå‰ªéËÄåÂÆûÁé∞‰∏çÂêåÊï∞ÊçÆÁ±ªÂûãÁöÑÊó†ÁºùÈõÜÊàêÂíåÂ§ÑÁêÜ„ÄÇÂÖ∂Ê¨°ÔºåÂÆûÊñΩÊ∑∑ÂêàÁöÑÈïøÈìæÊÄùÁª¥ÔºàCoTÔºâÂæÆË∞ÉÁ≠ñÁï•ÔºåÁªü‰∏Ä‰∏çÂêåÊ®°ÊÄÅÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂ¢ûÂº∫Ê®°ÂûãÂ§ÑÁêÜÂ§çÊùÇ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÊèêÂá∫‰∫ÜUniGRPOÔºåËøôÊòØ‰∏ÄÁßç‰∏ìÈó®‰∏∫Êâ©Êï£Âü∫Á°ÄÊ®°ÂûãËÆæËÆ°ÁöÑÁªü‰∏ÄÁ≠ñÁï•Ê¢ØÂ∫¶Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÁ°Æ‰øùÂú®Êé®ÁêÜÂíåÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14231', 'title': 'UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.14231', 'abstract': 'Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.', 'score': 42, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'cd98eb52e95427f4', 'authors': ['Sule Bai', 'Mingxing Li', 'Yong Liu', 'Jing Tang', 'Haoji Zhang', 'Lei Sun', 'Xiangxiang Chu', 'Yansong Tang'], 'affiliations': ['AMAP, Alibaba Group', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14231.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#rl', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniVG-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏, —É–ª—É—á—à–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø—É—Ç—è–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –≤–µ—Å–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. UniVG-R1 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MIG-Bench –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Enhancing Visual Grounding with Advanced Reasoning', 'desc': 'This paper introduces UniVG-R1, a multimodal large language model designed for universal visual grounding, which is the task of linking images to complex textual instructions. The model enhances its reasoning abilities through a combination of supervised fine-tuning on a newly created Chain-of-Thought dataset and reinforcement learning techniques. To address challenges in training, the authors implement a difficulty-aware weight adjustment strategy that helps the model focus on more complex reasoning tasks as it learns. Experimental results show that UniVG-R1 outperforms previous methods, demonstrating significant improvements in both general performance and zero-shot capabilities across various benchmarks.'}, 'zh': {'title': 'ÊèêÂçáËßÜËßâÂÆö‰ΩçÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': '‰º†ÁªüÁöÑËßÜËßâÂÆö‰ΩçÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçïÂõæÂÉèÂú∫ÊôØÂíåÁÆÄÂçïÊñáÊú¨ÂºïÁî®‰∏ä„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜËøô‰∫õÊñπÊ≥ïÊâ©Â±ïÂà∞Ê∂âÂèäÈöêÂê´ÂíåÂ§çÊùÇÊåá‰ª§ÁöÑÁúüÂÆûÂú∫ÊôØÔºåÂ∞§ÂÖ∂ÊòØÂ§öÂõæÂÉèÁöÑÊÉÖÂÜµ‰∏ãÔºåÈù¢‰∏¥ÁùÄÈáçÂ§ßÊåëÊàòÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÁº∫‰πèÂú®Â§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñá‰∏≠ËøõË°åÈ´òÁ∫ßÊé®ÁêÜÁöÑËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜUniVG-R1ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÊé®ÁêÜÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáÁªìÂêàÂº∫ÂåñÂ≠¶‰π†ÂíåÂÜ∑ÂêØÂä®Êï∞ÊçÆÊù•Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniVG-R1Âú®MIG-Bench‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫é‰πãÂâçÁöÑÊñπÊ≥ïÊèêÈ´ò‰∫Ü9.1%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15045', 'title': 'Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective', 'url': 'https://huggingface.co/papers/2505.15045', 'abstract': 'Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text.', 'score': 37, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '57c6eb57eddeebcc', 'authors': ['Siyue Zhang', 'Yilun Zhao', 'Liyuan Geng', 'Arman Cohan', 'Anh Tuan Luu', 'Chen Zhao'], 'affiliations': ['Alibaba-NTU Singapore Joint Research Institute', 'Center for Data Science, New York University', 'NYU Shanghai', 'Nanyang Technological University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15045.jpg', 'data': {'categories': ['#diffusion', '#reasoning', '#benchmark', '#training', '#long_context', '#architecture', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∞–≤—Ç–æ—Ä–µ–≥–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö LLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∏ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'Harnessing Bidirectional Attention for Superior Text Embeddings', 'desc': 'This paper discusses the limitations of large language model (LLM) embeddings, particularly their unidirectional attention which does not align well with the needs of text embedding tasks. The authors propose using diffusion language models, which have a bidirectional architecture, to improve text embeddings. Their systematic study shows that these diffusion models outperform LLM-based embeddings in various retrieval tasks, especially in long-document and reasoning-intensive scenarios. The findings highlight the importance of bidirectional attention for capturing the global context in complex texts.'}, 'zh': {'title': 'Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºöÂèåÂêëÂµåÂÖ•ÁöÑÊú™Êù•', 'desc': 'Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂµåÂÖ•Ê®°ÂûãÂú®ÊñáÊ°£Ê£ÄÁ¥¢Á≠âÈÄöÁî®ÊñáÊú¨ÂµåÂÖ•‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éBERTÂíåT5Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåLLMÂµåÂÖ•ÁöÑ‰∏Ä‰∏™Âü∫Êú¨ÈôêÂà∂ÊòØÂÖ∂Âú®Ëá™ÂõûÂΩíÈ¢ÑËÆ≠ÁªÉ‰∏≠‰ΩøÁî®ÁöÑÂçïÂêëÊ≥®ÊÑèÂäõÔºåËøô‰∏éÊñáÊú¨ÂµåÂÖ•‰ªªÂä°ÁöÑÂèåÂêëÁâπÊÄß‰∏çÁ¨¶„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫ÈááÁî®Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãËøõË°åÊñáÊú¨ÂµåÂÖ•ÔºåÂõ†ÂÖ∂Âõ∫ÊúâÁöÑÂèåÂêëÊû∂ÊûÑÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊâ©Êï£ËØ≠Ë®ÄÂµåÂÖ•Ê®°ÂûãÂú®ÈïøÊñáÊ°£Ê£ÄÁ¥¢Á≠â‰ªªÂä°‰∏≠‰ºò‰∫éLLMÂµåÂÖ•Ê®°ÂûãÔºåÈ™åËØÅ‰∫ÜÂèåÂêëÊ≥®ÊÑèÂäõÂú®ÁºñÁ†ÅÈïøÊñáÊú¨ÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13909', 'title': 'Efficient Agent Training for Computer Use', 'url': 'https://huggingface.co/papers/2505.13909', 'abstract': 'Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.', 'score': 31, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '6614c6f1cb4338a5', 'authors': ['Yanheng He', 'Jiahe Jin', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13909.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#transfer_learning', '#training', '#dataset', '#agents'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PC Agent-E - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –æ—Ç –ª—é–¥–µ–π. –ò—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 312 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Claude 3.7 Sonnet, –∏–º —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–≥–µ–Ω—Ç–∞, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–≥–æ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ 141%. PC Agent-E —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Empowering Agents with Minimal Data: The PC Agent-E Revolution', 'desc': 'The paper presents PC Agent-E, a new framework for training computer use agents that minimizes the need for extensive human demonstrations. By starting with only 312 human-annotated trajectories, the authors enhanced the data quality through the synthesis of diverse action decisions using Claude 3.7 Sonnet. This approach led to a significant performance boost, with the PC Agent-E model achieving a 141% relative improvement over previous models on the WindowsAgentArena-V2 benchmark. Additionally, the model shows strong adaptability across different operating systems, indicating that effective computer use skills can be developed from a limited set of high-quality data.'}, 'zh': {'title': 'Â∞ëÈáèÈ´òË¥®ÈáèÊï∞ÊçÆÔºåÊøÄÂèëÂº∫Â§ßËÆ°ÁÆóÊú∫ËÉΩÂäõ', 'desc': 'PC Agent-E ÊòØ‰∏Ä‰∏™È´òÊïàÁöÑ‰ª£ÁêÜËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®ÂáèÂ∞ëÂØπÂ§ßËßÑÊ®°‰∫∫Á±ªÁ§∫ËåÉÁöÑ‰æùËµñ„ÄÇÊàë‰ª¨‰ªé‰ªÖÊúâÁöÑ312‰∏™‰∫∫Â∑•Ê†áÊ≥®ÁöÑËÆ°ÁÆóÊú∫‰ΩøÁî®ËΩ®ËøπÂºÄÂßãÔºåÈÄöËøáÂêàÊàêÂ§öÊ†∑ÁöÑË°åÂä®ÂÜ≥Á≠ñÊù•ÊèêÈ´òÊï∞ÊçÆË¥®Èáè„ÄÇÁªèËøáËøô‰∫õ‰∏∞ÂØåËΩ®ËøπÁöÑËÆ≠ÁªÉÔºåPC Agent-E Ê®°ÂûãÂú® WindowsAgentArena-V2 Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü141%ÁöÑÁõ∏ÂØπÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜÂº∫Â§ßÁöÑ Claude 3.7 Sonnet„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ëÈáèÈ´òË¥®ÈáèÁöÑËΩ®ËøπÊï∞ÊçÆÂèØ‰ª•ÊøÄÂèëÂá∫Âº∫Â§ßÁöÑËÆ°ÁÆóÊú∫‰ΩøÁî®ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14766', 'title': 'This Time is Different: An Observability Perspective on Time Series\n  Foundation Models', 'url': 'https://huggingface.co/papers/2505.14766', 'abstract': "We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10times larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.", 'score': 27, 'issue_id': 3900, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'c77892c45c8f389d', 'authors': ['Ben Cohen', 'Emaad Khwaja', 'Youssef Doubli', 'Salahidine Lemaachi', 'Chris Lettieri', 'Charles Masson', 'Hugo Miccinilli', 'Elise Ram√©', 'Qiqi Ren', 'Afshin Rostamizadeh', 'Jean Ogier du Terrail', 'Anna-Monica Toon', 'Kan Wang', 'Stephan Xie', 'David Asker', 'Ameet Talwalkar', 'Othmane Abou-Amal'], 'affiliations': ['datadoghq.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.14766.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#small_models', '#synthetic', '#architecture'], 'emoji': 'üìà', 'ru': {'title': 'Toto: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Toto - —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å 151 –º–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. Toto –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–µ–∫–æ–¥–µ—Ä–∞ —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏, –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≤ 4-10 —Ä–∞–∑ –±–æ–ª—å—à–µ, —á–µ–º —É –≤–µ–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. Toto –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∫ –Ω–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ BOOM, —Ç–∞–∫ –∏ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.'}, 'en': {'title': 'Toto: Revolutionizing Time Series Forecasting with State-of-the-Art Performance', 'desc': 'Toto is a new foundation model designed for time series forecasting, featuring 151 million parameters and a decoder-only architecture. It addresses challenges in multivariate observability time series data through innovative design choices. The model is trained on a diverse dataset that is significantly larger than those used by existing models, combining real and synthetic observability data. Evaluations show that Toto outperforms other models on both a new benchmark called BOOM and established forecasting tasks, with all resources made available as open source.'}, 'zh': {'title': 'TotoÔºöÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜTotoÔºå‰∏Ä‰∏™ÂÖ∑Êúâ1.51‰∫øÂèÇÊï∞ÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÂü∫Á°ÄÊ®°Âûã„ÄÇTotoÈááÁî®Áé∞‰ª£ÁöÑËß£Á†ÅÂô®Êû∂ÊûÑÔºåÂπ∂ÈíàÂØπÂ§öÂèòÈáèÂèØËßÇÊµãÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑÁâπÂÆöÊåëÊàòËøõË°å‰∫ÜÊû∂ÊûÑÂàõÊñ∞„ÄÇÂÖ∂È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁî±ÂèØËßÇÊµãÊï∞ÊçÆ„ÄÅÂºÄÊîæÊï∞ÊçÆÈõÜÂíåÂêàÊàêÊï∞ÊçÆÊ∑∑ÂêàËÄåÊàêÔºåËßÑÊ®°ÊòØÈ¢ÜÂÖàÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÁöÑ4Âà∞10ÂÄç„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøò‰ªãÁªç‰∫ÜBOOMÔºå‰∏Ä‰∏™ÂåÖÂê´2,807‰∏™ÁúüÂÆû‰∏ñÁïåÊó∂Èó¥Â∫èÂàóÁöÑ350Áôæ‰∏áËßÇÊµãÂÄºÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15612', 'title': 'Learn to Reason Efficiently with Adaptive Length-based Reward Shaping', 'url': 'https://huggingface.co/papers/2505.15612', 'abstract': 'Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant "self-reflections". Resources are at https://github.com/hkust-nlp/Laser.', 'score': 23, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '145c2f1bfcaa0c1e', 'authors': ['Wei Liu', 'Ruochen Zhou', 'Yiyun Deng', 'Yuzhen Huang', 'Junteng Liu', 'Yuntian Deng', 'Yizhe Zhang', 'Junxian He'], 'affiliations': ['Apple', 'City University of Hong Kong', 'The Hong Kong University of Science and Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.15612.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization'], 'emoji': 'üí°', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—Å–ª–µ–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LASER –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. LASER –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é LASER-D, –∫–æ—Ç–æ—Ä–∞—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö.'}, 'en': {'title': 'Enhancing Reasoning Efficiency with Dynamic Length-Based Rewards', 'desc': 'This paper explores how Large Reasoning Models (LRMs) can improve their problem-solving efficiency using reinforcement learning (RL). It introduces a new method called Length-bAsed StEp Reward shaping (LASER), which optimizes reasoning outputs by shaping rewards based on the length of reasoning traces. LASER-D, an extension of LASER, adapts the reward system to be dynamic and difficulty-aware, penalizing longer reasoning for easier queries. The results show that this approach significantly enhances reasoning performance while reducing redundancy and token usage in outputs.'}, 'zh': {'title': 'ÊèêÂçáÊé®ÁêÜÊïàÁéáÁöÑÂä®ÊÄÅÂ•ñÂä±Êú∫Âà∂', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁîüÊàêÈïøÊé®ÁêÜÈìæÊó∂„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÂÜóÈïøÁöÑËæìÂá∫ÂæÄÂæÄÂ≠òÂú®ÊòæËëóÁöÑÂÜó‰ΩôÔºåÈôêÂà∂‰∫ÜLRMsÁöÑÊïàÁéá„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éRLÁöÑÊñπÊ≥ï‰ª•ÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈïøÂ∫¶Âü∫Á°ÄÊ≠•È™§Â•ñÂä±Â°ëÂΩ¢ÊñπÊ≥ïÔºàLASERÔºâÔºåÈÄöËøáÁõÆÊ†áÈïøÂ∫¶ÊéßÂà∂Â•ñÂä±ÔºåË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÊÄßËÉΩ‰∏éÊïàÁéáÁöÑ‰ºòË∂äÂπ≥Ë°°„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÂä®ÊÄÅÂíåÈöæÂ∫¶ÊÑüÁü•ÁöÑLASER-DÊñπÊ≥ïÔºå‰ª•ÈÄÇÂ∫îÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÊé®ÁêÜË°å‰∏∫ÂèòÂåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15400', 'title': 'When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning', 'url': 'https://huggingface.co/papers/2505.15400', 'abstract': 'Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery Mechanism" where models implicitly supplement reasoning during answer generation. Building on this insight, we propose Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables implicit recovery. By introducing accuracy-aware length reward regulation, ASRR adaptively allocates reasoning effort according to problem difficulty, achieving high efficiency with negligible performance sacrifice. Experiments across multiple benchmarks and models show that, compared with GRPO, ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates on safety benchmarks (up to +21.7%). Our results highlight the potential of ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.', 'score': 17, 'issue_id': 3896, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '93ea3ac7fa19dc07', 'authors': ['Xiaoyun Zhang', 'Jingqing Ruan', 'Xing Ma', 'Yawen Zhu', 'Haodong Zhao', 'Hao Li', 'Jiansong Chen', 'Ke Zeng', 'Xunliang Cai'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2505.15400.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –º–µ–Ω—å—à–µ –¥—É–º–∞–π, –±–æ–ª—å—à–µ –¥–µ–ª–∞–π', 'desc': "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –°–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—â–µ–≥–æ—Å—è –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (ASRR), –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–Ω—å—à–∞–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. ASRR –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–µ–Ω–æ–º–µ–Ω–µ '–í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Å–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è', –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ –Ω–µ—è–≤–Ω–æ –¥–æ–ø–æ–ª–Ω—è—é—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ASRR –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –ø–æ–≤—ã—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö."}, 'en': {'title': 'Efficient Reasoning with Adaptive Self-Recovery', 'desc': "This paper discusses the challenges of large reasoning models (LRMs) that often perform redundant reasoning, leading to high computational costs, especially on simpler tasks. The authors introduce a concept called the 'Internal Self-Recovery Mechanism', which allows models to enhance their reasoning during answer generation without additional effort. They propose a new framework called Adaptive Self-Recovery Reasoning (ASRR) that minimizes unnecessary reasoning while still allowing for implicit recovery based on the task's complexity. Experimental results demonstrate that ASRR can significantly reduce the reasoning budget while maintaining accuracy and improving safety metrics across various benchmarks."}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊé®ÁêÜÔºåÊèêÂçáÊïàÁéá‰∏éÂÆâÂÖ®ÊÄß', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®ÈïøÊé®ÁêÜÈìæ‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÁÆÄÂçï‰ªªÂä°‰∏≠Â∏∏Â∏∏ÂØºËá¥ËøáÈ´òÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÊú¨ÊñáÁ≥ªÁªüÈáèÂåñ‰∫ÜLRMsÂú®ÈïøÊÄùËÄÉÂíåÊó†ÊÄùËÄÉÊ®°Âºè‰∏ãÁöÑ‰∏äÈôêÔºåÂπ∂Êè≠Á§∫‰∫ÜÊ®°ÂûãÂú®ÁîüÊàêÁ≠îÊ°àÊó∂ÈöêÂºèË°•ÂÖÖÊé®ÁêÜÁöÑ‚ÄúÂÜÖÈÉ®Ëá™ÊàëÊÅ¢Â§çÊú∫Âà∂‚Äù„ÄÇÂü∫‰∫éËøô‰∏ÄÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îËá™ÊàëÊÅ¢Â§çÊé®ÁêÜÔºàASRRÔºâÊ°ÜÊû∂ÔºåËÉΩÂ§üÊäëÂà∂‰∏çÂøÖË¶ÅÁöÑÊé®ÁêÜÂπ∂ÂÆûÁé∞ÈöêÂºèÊÅ¢Â§ç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåASRRÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæÉÂ∞èÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14357', 'title': 'Vid2World: Crafting Video Diffusion Models to Interactive World Models', 'url': 'https://huggingface.co/papers/2505.14357', 'abstract': 'World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models.', 'score': 17, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'de65ad0f3c9cf5c2', 'authors': ['Siqiao Huang', 'Jialong Wu', 'Qixing Zhou', 'Shangchen Miao', 'Mingsheng Long'], 'affiliations': ['Chongqing University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14357.jpg', 'data': {'categories': ['#architecture', '#games', '#robotics', '#diffusion', '#transfer_learning', '#rl', '#agents', '#video'], 'emoji': 'üé•', 'ru': {'title': '–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Vid2World, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–∞—É–∑–∞–ª–∏–∑–∞—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏ –≤ –ø–æ–ª—É—á–∞–µ–º–æ–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ –∏ –∏–≥—Ä–æ–≤—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ—â–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –∫ –∑–∞–¥–∞—á–∞–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞.'}, 'en': {'title': 'Transforming Video Models into Interactive World Models', 'desc': 'This paper introduces Vid2World, a novel method that enhances world models by utilizing pre-trained video diffusion models. Traditional world models struggle with low-quality predictions and require extensive training, limiting their use in complex scenarios. Vid2World addresses these issues by adapting the architecture and training objectives of video diffusion models for autoregressive generation. The approach also incorporates a causal action guidance mechanism, improving the controllability of actions in interactive environments, as demonstrated through experiments in robot manipulation and game simulations.'}, 'zh': {'title': 'Â∞ÜËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãËΩ¨Âåñ‰∏∫‰∫§‰∫íÂºè‰∏ñÁïåÊ®°ÂûãÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Vid2WorldÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãËΩ¨Âåñ‰∏∫‰∫§‰∫íÂºè‰∏ñÁïåÊ®°ÂûãÔºå‰ª•ÊèêÈ´òÊï∞ÊçÆÊïàÁéá„ÄÇÁé∞ÊúâÁöÑ‰∏ñÁïåÊ®°ÂûãÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁâπÂÆöÈ¢ÜÂüüÁöÑËÆ≠ÁªÉÔºåÂπ∂‰∏îÈ¢ÑÊµãÁ≤æÂ∫¶ËæÉ‰ΩéÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇVid2WorldÈÄöËøáË∞ÉÊï¥ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊû∂ÊûÑÂíåËÆ≠ÁªÉÁõÆÊ†áÔºåÂÆûÁé∞‰∫ÜËá™ÂõûÂΩíÁîüÊàêÔºåÂπ∂ÂºïÂÖ•‰∫ÜÂõ†ÊûúÂä®‰ΩúÂºïÂØºÊú∫Âà∂Ôºå‰ª•Â¢ûÂº∫‰∫§‰∫íÂºè‰∏ñÁïåÊ®°Âûã‰∏≠ÁöÑÂä®‰ΩúÂèØÊéßÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÂíåÊ∏∏ÊàèÊ®°ÊãüÈ¢ÜÂüüË°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ï‰∏îÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15146', 'title': 'lmgame-Bench: How Good are LLMs at Playing Games?', 'url': 'https://huggingface.co/papers/2505.15146', 'abstract': 'Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.', 'score': 16, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '96b324d85a6c0d83', 'authors': ['Lanxiang Hu', 'Mingjia Huo', 'Yuxuan Zhang', 'Haoyang Yu', 'Eric P. Xing', 'Ion Stoica', 'Tajana Rosing', 'Haojian Jin', 'Hao Zhang'], 'affiliations': ['MBZUAI', 'UC Berkeley', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.15146.jpg', 'data': {'categories': ['#rl', '#benchmark', '#games', '#agents', '#transfer_learning'], 'emoji': 'üéÆ', 'ru': {'title': '–í–∏–¥–µ–æ–∏–≥—Ä—ã –∫–∞–∫ –ø–æ–ª–∏–≥–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∫ –ø—Ä–æ–º–ø—Ç–∞–º –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø—Ä—è–º–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ LLM –≤ –∏–≥—Ä–∞—Ö. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç lmgame-Bench - –Ω–∞–±–æ—Ä –∏–≥—Ä —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º API –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –ø–∞–º—è—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ lmgame-Bench —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –Ω–∞–≤—ã–∫–∏ –º–µ–∂–¥—É –∏–≥—Ä–∞–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Evaluating LLMs with lmgame-Bench: A Game-Changer!', 'desc': 'This paper addresses the challenges of evaluating large language models (LLMs) using video games, highlighting issues like poor visual perception, sensitivity to prompts, and data contamination. The authors propose lmgame-Bench, a framework that standardizes game evaluations through a unified API and incorporates tools for perception and memory. This framework allows for a more reliable assessment of LLMs across various game types, including platformers and puzzles. The results demonstrate that lmgame-Bench effectively distinguishes between models and shows that training on one game can enhance performance on others and on planning tasks.'}, 'zh': {'title': 'Ê∏∏ÊàèËØÑ‰º∞ÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®ËßÜÈ¢ëÊ∏∏ÊàèÊù•ËØÑ‰º∞Áé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁõ¥Êé•Â∞ÜLLMÂ∫îÁî®‰∫éÊ∏∏Êàè‰∏≠ËøõË°åËØÑ‰º∞Â≠òÂú®ËßÜËßâÊÑüÁü•ËÑÜÂº±„ÄÅÊèêÁ§∫ÊïèÊÑüÊÄßÂíåÊï∞ÊçÆÊ±°ÊüìÁ≠âÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ülmgame-BenchÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöËøáÁªü‰∏ÄÁöÑGymÈ£éÊ†ºAPIÊèê‰æõÁöÑÂπ≥Âè∞Ê∏∏Êàè„ÄÅËß£Ë∞úÊ∏∏ÊàèÂíåÂèô‰∫ãÊ∏∏ÊàèÁöÑËØÑ‰º∞Â∑•ÂÖ∑„ÄÇÈÄöËøáÂØπ13‰∏™È¢ÜÂÖàÊ®°ÂûãÁöÑÊµãËØïÔºålmgame-BenchËÉΩÂ§üÊúâÊïàÂå∫ÂàÜÊ®°ÂûãÁöÑËÉΩÂäõÔºåÂπ∂‰∏îÂú®Âçï‰∏ÄÊ∏∏Êàè‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•ËΩ¨ÁßªÂà∞Êú™ËßÅËøáÁöÑÊ∏∏ÊàèÂíåÂ§ñÈÉ®ËßÑÂàí‰ªªÂä°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15765', 'title': 'Constructing a 3D Town from a Single Image', 'url': 'https://huggingface.co/papers/2505.15765', 'abstract': 'Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, a training-free framework designed to synthesize realistic and coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from a single image using a principled, training-free approach.', 'score': 14, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'c8acf46d639df2bb', 'authors': ['Kaizhi Zheng', 'Ruijian Zhang', 'Jing Gu', 'Jie Yang', 'Xin Eric Wang'], 'affiliations': ['Columbia University', 'Cybever AI', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.15765.jpg', 'data': {'categories': ['#synthetic', '#3d'], 'emoji': 'üèôÔ∏è', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–≥–æ—Ä–æ–¥–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è', 'desc': '3DTown - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –Ω–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–º 3D-–∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–µ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ü–µ–Ω—ã. 3DTown —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –æ–±–ª–∞—Å—Ç–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞–∂–¥—É—é —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ 3DTown –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç—É—Ä.'}, 'en': {'title': 'Transforming Top-Down Images into Stunning 3D Towns!', 'desc': 'This paper presents 3DTown, a novel framework for generating realistic 3D scenes from a single top-down image without the need for extensive training. The method utilizes region-based generation to enhance the alignment between the 2D image and the 3D output, while also employing spatial-aware inpainting to ensure the overall coherence and quality of the generated geometry. By breaking down the image into overlapping regions and using a pretrained 3D object generator, the framework effectively fills in missing parts of the scene, maintaining structural integrity. The results indicate that 3DTown surpasses existing models in producing high-quality, coherent 3D scenes, demonstrating the potential of training-free approaches in 3D scene synthesis.'}, 'zh': {'title': '‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÈ´òË¥®Èáè3DÂú∫ÊôØÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫3DTownÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•‰ªéÂçïÂº†‰øØËßÜÂõæÁîüÊàêÈÄºÁúüÁöÑ3DÂú∫ÊôØÔºåËÄåÊó†ÈúÄÂ§çÊùÇÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éÂå∫ÂüüÁîüÊàêÂíåÁ©∫Èó¥ÊÑüÁü•ÁöÑ3D‰øÆÂ§çÊäÄÊúØÔºåÁ°Æ‰øùÁîüÊàêÁöÑÂú∫ÊôØÂú®Âá†‰ΩïÂΩ¢Áä∂ÂíåÂ∏ÉÂ±Ä‰∏ä‰øùÊåÅ‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÂ∞ÜËæìÂÖ•ÂõæÂÉèÂàÜËß£‰∏∫ÈáçÂè†Âå∫ÂüüÔºåÂπ∂Âà©Áî®È¢ÑËÆ≠ÁªÉÁöÑ3DÁâ©‰ΩìÁîüÊàêÂô®ÁîüÊàêÊØè‰∏™Âå∫ÂüüÔºåÊúÄÂêéËøõË°åÂá†‰ΩïÂ°´ÂÖÖÔºå‰øùÊåÅÁªìÊûÑËøûÁª≠ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå3DTownÂú®Âá†‰ΩïË¥®Èáè„ÄÅÁ©∫Èó¥‰∏ÄËá¥ÊÄßÂíåÁ∫πÁêÜ‰øùÁúüÂ∫¶ÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15210', 'title': 'Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs', 'url': 'https://huggingface.co/papers/2505.15210', 'abstract': "Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors.", 'score': 14, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'acf62275d75af161', 'authors': ['Jie Ma', 'Ning Qu', 'Zhitao Gao', 'Rui Xing', 'Jun Liu', 'Hongbin Pei', 'Jiang Xie', 'Linyun Song', 'Pinghui Wang', 'Jing Tao', 'Zhou Su'], 'affiliations': ['MOE KLINNS Lab, Xian Jiaotong University', 'School of Artificial Intelligence, Chongqing University of Post and Telecommunications', 'School of Computer Science and Technology, Xian Jiaotong University', 'School of Computer Science, Northwestern Polytechnical University', 'Shaanxi Province Key Laboratory of Big Data Knowledge Engineering'], 'pdf_title_img': 'assets/pdf/title_img/2505.15210.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag', '#benchmark', '#hallucinations'], 'emoji': 'üß†', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Deliberation over Priors' (DP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π. DP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ LLM –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—É—Ç–µ–π –æ—Ç–Ω–æ—à–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö, –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ —É–ª—É—á—à–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å Hit@1 –Ω–∞ 13% –¥–ª—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö ComplexWebQuestions."}, 'en': {'title': 'Enhancing LLM Trustworthiness with Knowledge Graphs', 'desc': 'This paper introduces a new framework called Deliberation over Priors (DP) to enhance the reliability of Large Language Models (LLMs) by leveraging knowledge graphs (KGs). DP utilizes a progressive knowledge distillation strategy that incorporates the structural information and constraints from KGs into LLMs, improving the accuracy of reasoning and response generation. The framework also includes a reasoning-introspection strategy that allows LLMs to verify their reasoning based on extracted constraints, leading to more trustworthy outputs. Experimental results show that DP significantly outperforms existing methods, achieving a notable improvement in performance on benchmark datasets.'}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØ‰ø°Êé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊ°ÜÊû∂ÔºåÊó®Âú®ÂáèÂ∞ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂Áß∞‰∏∫‚ÄúÂÖàÈ™åÊé®ÁêÜÊ∑±ÊÄù‚ÄùÔºàDeliberation over Priors, DPÔºâÔºåÂÖÖÂàÜÂà©Áî®Áü•ËØÜÂõæË∞±‰∏≠ÁöÑÁªìÊûÑ‰ø°ÊÅØÂíåÁ∫¶ÊùüÊù°‰ª∂„ÄÇDPÈÄöËøáÈÄêÊ≠•Áü•ËØÜËí∏È¶èÁ≠ñÁï•ÔºåÂ∞ÜÁªìÊûÑÂÖàÈ™åÊï¥ÂêàÂà∞LLMs‰∏≠Ôºå‰ªéËÄåÊèêÈ´òÂÖ≥Á≥ªË∑ØÂæÑÁîüÊàêÁöÑÂèØ‰ø°Â∫¶„ÄÇÊ≠§Â§ñÔºåÊ°ÜÊû∂ËøòÈááÁî®Êé®ÁêÜËá™ÁúÅÁ≠ñÁï•ÔºåÁ°Æ‰øùÁîüÊàêÂìçÂ∫îÁöÑÂèØÈù†ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15779', 'title': 'IA-T2I: Internet-Augmented Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2505.15779', 'abstract': "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.", 'score': 13, 'issue_id': 3893, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '97c03041dc8579a8', 'authors': ['Chuanhao Li', 'Jianwen Sun', 'Yukang Feng', 'Mingliang Zhai', 'Yifan Chang', 'Kaipeng Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.15779.jpg', 'data': {'categories': ['#rag', '#alignment', '#dataset', '#multimodal', '#diffusion'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-augmented T2I: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (T2I) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ—Å—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ IA-T2I, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ Img-Ref-T2I –∏ –æ—Ü–µ–Ω–∫–æ–π —Å –ø–æ–º–æ—â—å—é GPT-4.'}, 'en': {'title': 'Enhancing T2I Models with Internet-Augmented Knowledge', 'desc': 'This paper introduces the Internet-Augmented text-to-image generation (IA-T2I) framework, which enhances traditional text-to-image (T2I) models by addressing uncertainties in text prompts. The framework includes an active retrieval module to assess the need for reference images, a hierarchical image selection module to find the best matching images, and a self-reflection mechanism for continuous improvement of generated images. A new dataset, Img-Ref-T2I, is created to test the framework, featuring prompts with various types of uncertain knowledge. Experimental results show that IA-T2I significantly improves the quality of generated images, outperforming existing models in human evaluations.'}, 'zh': {'title': '‰∫íËÅîÁΩëÂ¢ûÂº∫ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂', 'desc': 'ÂΩìÂâçÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÂú®Â§ÑÁêÜÊñáÊú¨ÊèêÁ§∫‰∏≠ÈöêÂê´ÁöÑ‰∏çÁ°ÆÂÆöÁü•ËØÜÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫íËÅîÁΩëÂ¢ûÂº∫ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂ÔºàIA-T2IÔºâÔºåÈÄöËøáÊèê‰æõÂèÇËÄÉÂõæÂÉèÊù•Â∏ÆÂä©Ê®°ÂûãÁêÜËß£‰∏çÁ°ÆÂÆöÁöÑÁü•ËØÜ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏ªÂä®Ê£ÄÁ¥¢Ê®°Âùó„ÄÅÂàÜÂ±ÇÂõæÂÉèÈÄâÊã©Ê®°ÂùóÂíåËá™ÊàëÂèçÊÄùÊú∫Âà∂Ôºå‰ª•ÊèêÈ´òÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÂíå‰∏éÊñáÊú¨ÊèêÁ§∫ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÂíå‰∏çÁ°ÆÂÆöÁöÑÊñáÊú¨ÊèêÁ§∫Êó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15404', 'title': 'How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study', 'url': 'https://huggingface.co/papers/2505.15404', 'abstract': 'Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important research question: how can we enhance the safety of LRMs? In this paper, we present a comprehensive empirical study on how to enhance the safety of LRMs through Supervised Fine-Tuning (SFT). Our investigation begins with an unexpected observation: directly distilling safe responses from DeepSeek-R1 fails to significantly enhance safety. We analyze this phenomenon and identify three key failure patterns that contribute to it. We then demonstrate that explicitly addressing these issues during the data distillation process can lead to substantial safety improvements. Next, we explore whether a long and complex reasoning process is necessary for achieving safety. Interestingly, we find that simply using short or template-based reasoning process can attain comparable safety performance-and are significantly easier for models to learn than more intricate reasoning chains. These findings prompt a deeper reflection on the role of reasoning in ensuring safety. Finally, we find that mixing math reasoning data during safety fine-tuning is helpful to balance safety and over-refusal. Overall, we hope our empirical study could provide a more holistic picture on enhancing the safety of LRMs. The code and data used in our experiments are released in https://github.com/thu-coai/LRM-Safety-Study.', 'score': 11, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'd681b198a7360d3b', 'authors': ['Zhexin Zhang', 'Xian Qi Loye', 'Victor Shea-Jay Huang', 'Junxiao Yang', 'Qi Zhu', 'Shiyao Cui', 'Fei Mi', 'Lifeng Shang', 'Yingkang Wang', 'Hongning Wang', 'Minlie Huang'], 'affiliations': ['Huawei Noahs Ark Lab', 'The Conversational AI (CoAI) group, DCST, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15404.jpg', 'data': {'categories': ['#data', '#math', '#reasoning', '#training', '#safety'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LRM: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–ª—é—á–æ–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±—ã –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ú–æ–¥–µ–ª–µ–π –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) —Å –ø–æ–º–æ—â—å—é –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¢–æ–Ω–∫–æ–π –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (SFT). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä—è–º–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–µ –≤—Å–µ–≥–¥–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞, –∏ –≤—ã—è–≤–∏–ª–∏ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ—É–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ —à–∞–±–ª–æ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å —Å—Ç–æ–ª—å –∂–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∫–∞–∫ –∏ —Å–ª–æ–∂–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, —á—Ç–æ –≤–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –ø—Ä–æ—Ü–µ—Å—Å —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–º–æ–≥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —á—Ä–µ–∑–º–µ—Ä–Ω—ã–π –æ—Ç–∫–∞–∑.'}, 'en': {'title': 'Enhancing Safety in Large Reasoning Models through Simplified Reasoning', 'desc': "This paper investigates how to improve the safety of Large Reasoning Models (LRMs) while maintaining their reasoning capabilities. The authors find that directly distilling safe responses does not significantly enhance safety and identify three failure patterns that contribute to this issue. They demonstrate that addressing these patterns during data distillation can lead to better safety outcomes. Additionally, the study reveals that simpler reasoning processes can achieve similar safety performance as complex ones, suggesting a reevaluation of reasoning's role in safety enhancement."}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑÁ†îÁ©∂', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®Êï∞Â≠¶ÂíåÁºñÁ®ãÁ≠âÊé®ÁêÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨ÁöÑÊé®ÁêÜËÉΩÂäõÂ¢ûÂº∫Âπ∂‰∏ç‰∏ÄÂÆöËÉΩÊèêÈ´òÂÆâÂÖ®ÊÄßËÉΩÔºåÁîöËá≥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂèØËÉΩ‰ºöÈôç‰ΩéÂÆâÂÖ®ÊÄß„ÄÇÊú¨ÊñáÈÄöËøáÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂØπÂ¶Ç‰ΩïÂ¢ûÂº∫LRMsÁöÑÂÆâÂÖ®ÊÄßËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûËØÅÁ†îÁ©∂ÔºåÂèëÁé∞Áõ¥Êé•‰ªéDeepSeek-R1ÊèêÂèñÂÆâÂÖ®ÂìçÂ∫îÂπ∂Êú™ÊòæËëóÊèêÂçáÂÆâÂÖ®ÊÄßÔºåÂπ∂ËØÜÂà´Âá∫‰∏âÁßçÂÖ≥ÈîÆÁöÑÂ§±Ë¥•Ê®°Âºè„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®ÁÆÄÂçïÁöÑÁü≠Êé®ÁêÜËøáÁ®ãÂèØ‰ª•ÂÆûÁé∞‰∏éÂ§çÊùÇÊé®ÁêÜËøáÁ®ãÁõ∏ÂΩìÁöÑÂÆâÂÖ®ÊÄßËÉΩÔºå‰∏îÊõ¥Êòì‰∫éÊ®°ÂûãÂ≠¶‰π†ÔºåËøô‰øÉ‰ΩøÊàë‰ª¨ÈáçÊñ∞ÊÄùËÄÉÊé®ÁêÜÂú®Á°Æ‰øùÂÆâÂÖ®ÊÄß‰∏≠ÁöÑ‰ΩúÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15817', 'title': 'Learning to Reason via Mixture-of-Thought for Logical Reasoning', 'url': 'https://huggingface.co/papers/2505.15817', 'abstract': 'Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference.', 'score': 10, 'issue_id': 3897, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'a9b89bfbce1c417a', 'authors': ['Tong Zheng', 'Lichang Chen', 'Simeng Han', 'R. Thomas McCoy', 'Heng Huang'], 'affiliations': ['Dept. of Computer Science, UMD, College Park, MD 20742', 'Dept. of Computer Science, Yale University, New Haven, CT 06520', 'Dept. of Linguistics, Yale University, New Haven, CT'], 'pdf_title_img': 'assets/pdf/title_img/2505.15817.jpg', 'data': {'categories': ['#training', '#multimodal', '#benchmark', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–°–º–µ—à–µ–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Mixture-of-Thought (MoT). MoT –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç—Ä–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫, –∫–æ–¥ –∏ –Ω–æ–≤—É—é —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å - —Ç–∞–±–ª–∏—Ü—É –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤—É—Ö—Ñ–∞–∑–æ–≤—ã–π –¥–∏–∑–∞–π–Ω: —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ MoT –∏ –≤—ã–≤–æ–¥ MoT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MoT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ LLM —Å –ø–æ–¥—Ö–æ–¥–æ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞.'}, 'en': {'title': 'Empowering LLMs with Multi-Modal Reasoning for Enhanced Logic Solving', 'desc': "This paper introduces the Mixture-of-Thought (MoT) framework, which enhances large language models (LLMs) by enabling them to reason across three different modalities: natural language, code, and a new symbolic modality called truth-table. Unlike traditional methods that rely on a single reasoning modality during training, MoT allows for a more integrated approach, improving the model's ability to tackle logical reasoning tasks. The framework consists of two phases: a self-evolving training phase that learns from generated rationales across modalities, and an inference phase that utilizes the strengths of all three modalities for better predictions. Experimental results show that MoT significantly outperforms existing single-modality approaches, particularly on challenging logical reasoning problems, achieving notable accuracy improvements."}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊé®ÁêÜÔºåÊèêÂçáÈÄªËæëÊÄùÁª¥ËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑∑ÂêàÊÄùÁª¥ÔºàMixture-of-Thought, MoTÔºâÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÈÄªËæëÊé®ÁêÜ‰∏≠ÁöÑË°®Áé∞„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÂè™‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®Ä‰Ωú‰∏∫Êé®ÁêÜÊ®°Âºè‰∏çÂêåÔºåMoTÁªìÂêà‰∫ÜËá™ÁÑ∂ËØ≠Ë®Ä„ÄÅ‰ª£Á†ÅÂíå‰∏ÄÁßçÊñ∞ÂºïÂÖ•ÁöÑÁ¨¶Âè∑Ê®°Âºè‚Äî‚ÄîÁúüÂÄºË°®Ôºå‰ª•Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∏§Èò∂ÊÆµËÆæËÆ°ÔºöËá™ÊàëÊºîÂåñÁöÑMoTËÆ≠ÁªÉÂíåMoTÊé®ÁêÜÔºåËÉΩÂ§üÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµÂÖÖÂàÜÂà©Áî®‰∏âÁßçÊ®°ÂºèÁöÑÂçèÂêåÊïàÂ∫î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoTÂú®ÈÄªËæëÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóË∂ÖË∂ä‰∫ÜÂçï‰∏ÄÊ®°ÂºèÁöÑLLMÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Â§ÑÁêÜÊõ¥Â§çÊùÇÁöÑÈÄªËæëÈóÆÈ¢òÊó∂Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15781', 'title': 'dKV-Cache: The Cache for Diffusion Language Models', 'url': 'https://huggingface.co/papers/2505.15781', 'abstract': 'Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.', 'score': 10, 'issue_id': 3893, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '516300496b94028a', 'authors': ['Xinyin Ma', 'Runpeng Yu', 'Gongfan Fang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.15781.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#diffusion', '#benchmark'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (DLM) - –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ KV-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (dKV-Cache). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: dKV-Cache-Decode, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –∏ dKV-Cache-Greedy –¥–ª—è –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å 2-10-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ DLM, —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –º–∞—Ç–µ–º–∞—Ç–∏–∫—É –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞.'}, 'en': {'title': 'Accelerating Diffusion Language Models with Delayed KV-Cache', 'desc': 'This paper introduces a new mechanism called delayed KV-Cache to improve the inference speed of Diffusion Language Models (DLMs), which traditionally suffer from slow decoding due to their non-autoregressive nature. The authors identify that different tokens exhibit unique representation dynamics during the diffusion process, leading to the development of a caching strategy that optimizes key and value states. They propose two variants of the caching mechanism: dKV-Cache-Decode, which enhances performance on long sequences, and dKV-Cache-Greedy, which prioritizes speed at the expense of some accuracy. Overall, the proposed dKV-Cache achieves a significant speedup of 2-10x in inference, making DLMs more competitive with autoregressive models.'}, 'zh': {'title': 'Âä†ÈÄüÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶', 'desc': 'Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàDLMsÔºâË¢´ËßÜ‰∏∫Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÂäõÁ´û‰∫âËÄÖÔºå‰ΩÜÂÖ∂Êé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢ÊòØ‰∏Ä‰∏™‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂª∂ËøüÈîÆÂÄºÁºìÂ≠òÔºàdKV-CacheÔºâÊú∫Âà∂Ôºå‰ª•Ëß£ÂÜ≥DLMsÂú®ÂéªÂô™ËøáÁ®ã‰∏≠ÁöÑÁì∂È¢à„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏§Áßç‰∫íË°•ÁöÑÁºìÂ≠òÂèò‰ΩìÔºåÂàÜÂà´‰∏∫dKV-Cache-DecodeÂíådKV-Cache-GreedyÔºåÂâçËÄÖÂú®ÈïøÂ∫èÂàó‰∏äÂá†‰πéÊó†ÊçüÂä†ÈÄüÔºåÂêéËÄÖÂàô‰ª•Êõ¥È´òÁöÑÈÄüÂ∫¶ÂÆûÁé∞‰∫Ü‰∫åÊ¨°Êó∂Èó¥Â§çÊùÇÂ∫¶ÁöÑÂä†ÈÄü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºådKV-CacheÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÁº©Â∞è‰∫ÜËá™ÂõûÂΩíÊ®°Âûã‰∏éÊâ©Êï£ËØ≠Ë®ÄÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15656', 'title': 'Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!', 'url': 'https://huggingface.co/papers/2505.15656', 'abstract': 'Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction.', 'score': 10, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'eed97ed4c1582120', 'authors': ['Zhexin Zhang', 'Yuhao Sun', 'Junxiao Yang', 'Shiyao Cui', 'Hongning Wang', 'Minlie Huang'], 'affiliations': ['The Conversational AI (CoAI) group, DCST, Tsinghua University', 'The University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2505.15656.jpg', 'data': {'categories': ['#data', '#security', '#open_source', '#training', '#leakage'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–°–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞: –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å –≤–∞—à–∏ —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Ä–∏—Å–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–∑–¥–∞—Ç–µ–ª—å –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –º–æ–∂–µ—Ç –∏–∑–≤–ª–µ—á—å —á–∞—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è, –∏–º–µ—è —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø –∫ –∫–æ–Ω–µ—á–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –æ—Ç 3 –¥–æ 32 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è - –¥–æ 76.3% –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç —Å—Ä–æ—á–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ LLM.'}, 'en': {'title': 'Exposing the Hidden Risks of Fine-Tuning LLMs with Proprietary Data', 'desc': 'This paper discusses a significant risk associated with fine-tuning open-source Large Language Models (LLMs) using proprietary data. The authors demonstrate that creators of these LLMs can exploit backdoor training techniques to extract sensitive fine-tuning data from the models, even with only black-box access. Their experiments reveal that up to 76.3% of the fine-tuning data can be successfully extracted, with even higher rates in optimal conditions. The study emphasizes the urgent need for further research to address this data security issue in the context of LLM fine-tuning.'}, 'zh': {'title': 'ÂæÆË∞É‰∏≠ÁöÑÊï∞ÊçÆÊ≥ÑÈú≤È£éÈô©Ë≠¶Á§∫', 'desc': 'Êú¨ËÆ∫ÊñáÊè≠Á§∫‰∫ÜÂú®ÂºÄÊ∫êÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏äËøõË°åÂæÆË∞ÉÊó∂ÂèØËÉΩÂ≠òÂú®ÁöÑÈöêÁßÅÈ£éÈô©„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂºÄÊ∫êÊ®°ÂûãÁöÑÂàõÂª∫ËÄÖÂèØ‰ª•ÈÄöËøáÁÆÄÂçïÁöÑÂêéÈó®ËÆ≠ÁªÉÊèêÂèñÁßÅÊúâÁöÑÂæÆË∞ÉÊï∞ÊçÆÔºåÂç≥‰ΩøÂè™ÈÄöËøáÈªëÁÆ±ËÆøÈóÆÂæÆË∞ÉÂêéÁöÑÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂú®ÂÆûÈôÖÊÉÖÂÜµ‰∏ãÔºåÊúÄÂ§öÂèØÊèêÂèñ76.3%ÁöÑÂæÆË∞ÉÊï∞ÊçÆÔºåËÄåÂú®ÁêÜÊÉ≥ÊÉÖÂÜµ‰∏ãÊàêÂäüÁéáÂèØËææ94.9%„ÄÇÊàë‰ª¨ËøòÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ£ÄÊµãÁöÑÈò≤Âæ°Á≠ñÁï•Ôºå‰ΩÜÂèëÁé∞ËØ•Á≠ñÁï•ÂèØ‰ª•Ë¢´ÊîπËøõÁöÑÊîªÂáªÁªïËøáÔºåÂõ†Ê≠§Âº∫Ë∞É‰∫ÜËøô‰∏ÄÊñ∞ËØÜÂà´ÁöÑÊï∞ÊçÆÊ≥ÑÈú≤È£éÈô©ÁöÑÁ¥ßËø´ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13934', 'title': 'RLVR-World: Training World Models with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.13934', 'abstract': 'World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.', 'score': 9, 'issue_id': 3891, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'bb3f20501a24c607', 'authors': ['Jialong Wu', 'Shaofeng Yin', 'Ningya Feng', 'Mingsheng Long'], 'affiliations': ['School of Software, BNRist, Tsinghua University', 'Zhili College, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13934.jpg', 'data': {'categories': ['#rlhf', '#training', '#multimodal', '#optimization', '#reasoning', '#games', '#rl', '#agents', '#video'], 'emoji': 'üåê', 'ru': {'title': 'RLVR-World: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'RLVR-World - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –æ—Ü–µ–Ω–∫–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è, RLVR-World –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –∑–∞–¥–∞—á–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏–≥—Ä—ã, –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏—è –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞.'}, 'en': {'title': 'Optimizing World Models with Reinforcement Learning for Better Predictions', 'desc': 'This paper introduces RLVR-World, a new framework that improves world models by using reinforcement learning with verifiable rewards. Traditional training methods like maximum likelihood estimation often do not align well with the specific goals of these models, such as accuracy in predicting state transitions. RLVR-World addresses this by optimizing world models directly for metrics that matter, like perceptual quality and accuracy. The authors show that this approach leads to significant improvements in performance for both language and video-based models across various tasks, including text games and robot manipulation.'}, 'zh': {'title': 'ÈÄöËøáÂèØÈ™åËØÅÂ•ñÂä±‰ºòÂåñ‰∏ñÁïåÊ®°ÂûãÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫RLVR-WorldÁöÑÁªü‰∏ÄÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•‰ºòÂåñ‰∏ñÁïåÊ®°Âûã„ÄÇ‰º†ÁªüÁöÑËÆ≠ÁªÉÁõÆÊ†áÂ¶ÇÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºàMLEÔºâÂæÄÂæÄ‰∏éÁâπÂÆö‰ªªÂä°ÁöÑÁõÆÊ†á‰∏ç‰∏ÄËá¥ÔºåËÄåRLVR-WorldÁõ¥Êé•ÈíàÂØπËøáÊ∏°È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÊÑüÁü•Ë¥®ÈáèËøõË°å‰ºòÂåñ„ÄÇÊàë‰ª¨Â∞Ü‰∏ñÁïåÂª∫Ê®°ËßÜ‰∏∫ÂØπÊ†áËÆ∞Â∫èÂàóÁöÑËá™ÂõûÂΩíÈ¢ÑÊµãÔºåÂπ∂ÈÄöËøáËß£Á†ÅÈ¢ÑÊµãÁöÑÂ∫¶Èáè‰Ωú‰∏∫ÂèØÈ™åËØÅÂ•ñÂä±ËøõË°åËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRLVR-WorldÂú®ËØ≠Ë®ÄÂíåËßÜÈ¢ëÂü∫Á°ÄÁöÑ‰∏ñÁïåÊ®°Âûã‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈÄÇÁî®‰∫éÊñáÊú¨Ê∏∏Êàè„ÄÅÁΩëÈ°µÂØºËà™ÂíåÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠âÂ§ö‰∏™È¢ÜÂüü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13529', 'title': 'BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs', 'url': 'https://huggingface.co/papers/2505.13529', 'abstract': 'Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don\'t know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.', 'score': 9, 'issue_id': 3892, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': 'b0175ab5c60ceaee', 'authors': ['Junxiao Yang', 'Jinzhe Tu', 'Haoran Liu', 'Xiaoce Wang', 'Chujie Zheng', 'Zhexin Zhang', 'Shiyao Cui', 'Caishun Chen', 'Tiantian He', 'Hongning Wang', 'Yew-Soon Ong', 'Minlie Huang'], 'affiliations': ['Centre for Frontier AI Research, Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore', 'The College of Computing and Data Science, Nanyang Technological University', 'The Conversational AI (CoAI) group, DCST, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13529.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#training', '#math'], 'emoji': 'üß†', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∑–Ω–∞–Ω–∏–π', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –ø–æ—Å–ø–µ—à–Ω—ã–µ –¥–æ–≥–∞–¥–∫–∏ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–æ–º–µ–Ω—Ç –∏ —Å–ø–∏—Ä–∞–ª—å–Ω–æ–µ overthinking. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ BARREL, –∫–æ—Ç–æ—Ä—ã–π —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –ª–∞–∫–æ–Ω–∏—á–Ω–æ–º—É –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ BARREL –ø–æ–≤—ã—à–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ DeepSeek-R1-Distill-Llama-8B —Å 39.33% –¥–æ 61.48%, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥–µ–ª–µ–π, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π R1.'}, 'en': {'title': 'Enhancing Confidence and Reliability in Large Reasoning Models with BARREL', 'desc': 'This paper discusses the limitations of Large Reasoning Models (LRMs) in handling mathematical and logical reasoning, particularly their tendency to provide incorrect answers with excessive confidence. The authors identify two problematic reasoning behaviors: last-minute guessing and second-thought spiraling, which lead to these overconfident mistakes. To combat this, they introduce BARREL, a new framework designed to enhance factual reasoning by encouraging models to be more concise and aware of their boundaries. Their experiments show that training with BARREL significantly improves the reliability of a specific LRM, DeepSeek-R1-Distill-Llama-8B, while maintaining competitive accuracy levels.'}, 'zh': {'title': 'ÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑÂèØÈù†ÊÄß‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'ÊúÄËøëÔºåÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®Êï∞Â≠¶ÂíåÈÄªËæëÊé®ÁêÜÊñπÈù¢Â±ïÁé∞‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑLRMsÂæàÂ∞ëÊâøËÆ§Ëá™Â∑±ÁöÑÊó†Áü•ÔºåÈÄöÂ∏∏‰ºöÂú®ÈîôËØØÁöÑÊÉÖÂÜµ‰∏ãË°®Áé∞Âá∫ËøáÂ∫¶Ëá™‰ø°ÔºåËøôÂºïÂèë‰∫ÜÂØπÂÖ∂‰∫ãÂÆûÂèØÈù†ÊÄßÁöÑÊãÖÂøß„ÄÇÊú¨ÊñáËØÜÂà´‰∫Ü‰∏§ÁßçÁóÖÊÄÅÊé®ÁêÜÊ®°ÂºèÔºåÂØºËá¥‰∫ÜËøáÂ∫¶Ëá™‰ø°ÂíåÈîôËØØÁ≠îÊ°àÁöÑ‰∫ßÁîüÔºö‰∏¥Êó∂ÁåúÊµãÂíåÂèçÂ§çÊÄùËÄÉ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜBARRELÊ°ÜÊû∂Ôºå‰øÉËøõÁÆÄÊ¥Å‰∏îËæπÁïåÊÑèËØÜÂº∫ÁöÑ‰∫ãÂÆûÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15778', 'title': 'Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space', 'url': 'https://huggingface.co/papers/2505.15778', 'abstract': 'Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like "soft" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.', 'score': 8, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '96715f0b3118eceb', 'authors': ['Zhen Zhang', 'Xuehai He', 'Weixiang Yan', 'Ao Shen', 'Chenyang Zhao', 'Shuohang Wang', 'Yelong Shen', 'Xin Eric Wang'], 'affiliations': ['LMSYS Org', 'Microsoft', 'Purdue University', 'University of California, Los Angeles', 'University of California, Santa Barbara', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.15778.jpg', 'data': {'categories': ['#math', '#interpretability', '#reasoning', '#benchmark', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ú—è–≥–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ò–ò', 'desc': "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Soft Thinking', –∫–æ—Ç–æ—Ä—ã–π –∏–º–∏—Ç–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, 'Soft Thinking' –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º—è–≥–∫–∏–µ, –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –±–æ–≥–∞—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –ø–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 2.48 –ø—É–Ω–∫—Ç–æ–≤ –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 22.4% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º Chain-of-Thought."}, 'en': {'title': 'Soft Thinking: Beyond Discrete Boundaries in Reasoning', 'desc': 'This paper presents Soft Thinking, a novel approach to reasoning that mimics human cognitive processes by utilizing continuous concept spaces instead of discrete token embeddings. Traditional reasoning models are limited by their reliance on fixed linguistic tokens, which restricts their ability to explore diverse reasoning paths. Soft Thinking generates abstract concept tokens through a probability-weighted mixture of existing token embeddings, allowing for smoother transitions and richer representations. Empirical results show that this method improves accuracy and reduces token usage, while maintaining interpretability, thus addressing the limitations of conventional Chain-of-Thought reasoning.'}, 'zh': {'title': 'Á™ÅÁ†¥Á¶ªÊï£ÈôêÂà∂ÔºåÊã•Êä±ËΩØÊÄùÁª¥ÔºÅ', 'desc': '‰∫∫Á±ªÁöÑËÆ§Áü•ÈÄöÂ∏∏Ê∂âÂèäÊäΩË±°ÂíåÊµÅÂä®ÁöÑÊ¶ÇÂøµÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖ‰æùËµñ‰∫éÁ¶ªÊï£ÁöÑËØ≠Ë®ÄÁ¨¶Âè∑„ÄÇÂΩìÂâçÁöÑÊé®ÁêÜÊ®°ÂûãÂèóÈôê‰∫é‰∫∫Á±ªËØ≠Ë®ÄÁöÑËæπÁïåÔºåÂè™ËÉΩÂ§ÑÁêÜ‰ª£Ë°®Âõ∫ÂÆöËØ≠‰πâÁÇπÁöÑÁ¶ªÊï£Ê†áËÆ∞ÂµåÂÖ•„ÄÇËøôÁßçÁ¶ªÊï£ÈôêÂà∂Èôç‰Ωé‰∫ÜÊé®ÁêÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõÔºåÂØºËá¥Êé®ÁêÜË∑ØÂæÑÁöÑÊé¢Á¥¢‰∏çÂ§üÂÖ®Èù¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúËΩØÊÄùÁª¥‚ÄùÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂú®ËøûÁª≠Ê¶ÇÂøµÁ©∫Èó¥‰∏≠ÁîüÊàêËΩØÁöÑÊäΩË±°Ê¶ÇÂøµÊ†áËÆ∞ÔºåÊ®°Êãü‰∫∫Á±ªÁöÑ‚ÄúËΩØ‚ÄùÊé®ÁêÜÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÊúâÊïàÊÄßÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15776', 'title': 'ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.15776', 'abstract': 'Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrieval-guided self-distillation, followed by Retrieval-Guided Reinforcement Learning with a specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision.', 'score': 8, 'issue_id': 3896, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'd52cdaf90c573017', 'authors': ['Changtai Zhu', 'Siyin Wang', 'Ruijun Feng', 'Kai Song', 'Xipeng Qiu'], 'affiliations': ['ByteDance Inc', 'Fudan University', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2505.15776.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#rag', '#rl', '#reasoning'], 'emoji': 'üîç', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ConvSearch-R1 - –ø–µ—Ä–≤—É—é —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, ConvSearch-R1 –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–æ–∏—Å–∫–∞. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Ä–∞–∑–æ–≥—Ä–µ–≤ –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö TopiOCQA.'}, 'en': {'title': 'Revolutionizing Conversational Query Reformulation with Self-Driven Learning', 'desc': 'This paper introduces ConvSearch-R1, a novel framework for Conversational Query Reformulation (CQR) that eliminates the need for external supervision in query rewriting. It utilizes reinforcement learning to optimize the reformulation process based on retrieval signals, making it self-driven. The approach consists of a two-stage method that first addresses the cold-start problem and then employs a rank-incentive reward mechanism to improve retrieval performance. Experiments show that ConvSearch-R1 outperforms existing methods, achieving significant improvements on benchmark datasets with smaller models.'}, 'zh': {'title': 'ÂØπËØùÊü•ËØ¢ÈáçÊûÑÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂØπËØùÊü•ËØ¢ÈáçÊûÑÊ°ÜÊû∂ConvSearch-R1ÔºåÊó®Âú®ÊúâÊïàÂ§ÑÁêÜ‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÁöÑÊ®°Á≥äÊü•ËØ¢„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Áõ¥Êé•Âà©Áî®Ê£ÄÁ¥¢‰ø°Âè∑‰ºòÂåñÊü•ËØ¢ÈáçÊûÑÔºåÂÆåÂÖ®Ê∂àÈô§‰∫ÜÂØπÂ§ñÈÉ®ÁõëÁù£ÁöÑ‰æùËµñ„ÄÇÊàë‰ª¨ÈááÁî®‰∫Ü‰∏§Èò∂ÊÆµÁöÑÊñπÊ≥ïÔºåÈ¶ñÂÖàÈÄöËøáÊ£ÄÁ¥¢ÂºïÂØºÁöÑËá™Ëí∏È¶èËß£ÂÜ≥ÂÜ∑ÂêØÂä®ÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøáÁâπÂà´ËÆæËÆ°ÁöÑÂ•ñÂä±Êú∫Âà∂ËøõË°åÂº∫ÂåñÂ≠¶‰π†Ôºå‰ª•Â∫îÂØπ‰º†ÁªüÊ£ÄÁ¥¢ÊåáÊ†áÁöÑÁ®ÄÁñèÊÄßÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåConvSearch-R1Âú®TopiOCQAÂíåQReCCÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®TopiOCQAÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜË∂ÖËøá10%ÁöÑÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14827', 'title': 'Text Generation Beyond Discrete Token Sampling', 'url': 'https://huggingface.co/papers/2505.14827', 'abstract': "In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.", 'score': 7, 'issue_id': 3893, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'c5866f38a43da092', 'authors': ['Yufan Zhuang', 'Liyuan Liu', 'Chandan Singh', 'Jingbo Shang', 'Jianfeng Gao'], 'affiliations': ['Microsoft Research', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.14827.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–°–º–µ—à–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞–¥–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture of Inputs (MoI). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, MoI —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –≤—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–≥–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π. MoI –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'Enhancing Autoregressive Generation with Mixture of Inputs', 'desc': 'This paper introduces a novel method called Mixture of Inputs (MoI) for enhancing autoregressive generation in large language models (LLMs). Instead of discarding the token distribution after sampling a token, MoI combines the generated token with the previously discarded distribution to create a richer input representation. By using Bayesian estimation, MoI treats the token distribution as a prior and the sampled token as an observation, allowing for a continuous posterior expectation to be used as input. This approach leads to improved performance in tasks such as mathematical reasoning, code generation, and PhD-level question answering without requiring additional training.'}, 'zh': {'title': 'Ê∑∑ÂêàËæìÂÖ•ÔºöÊèêÂçáËá™ÂõûÂΩíÁîüÊàêÁöÑË¥®Èáè‰∏éÊé®ÁêÜËÉΩÂäõ', 'desc': 'Âú®Ê†áÂáÜÁöÑËá™ÂõûÂΩíÁîüÊàê‰∏≠ÔºåËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™Ê†áËÆ∞ÁöÑÂàÜÂ∏ÉÔºåÈááÊ†∑‰∏Ä‰∏™Á¶ªÊï£Ê†áËÆ∞ÔºåÁÑ∂Âêé‰∏¢ÂºÉËØ•ÂàÜÂ∏ÉÔºå‰ªÖÂ∞ÜÈááÊ†∑ÁöÑÊ†áËÆ∞‰Ωú‰∏∫Êñ∞ËæìÂÖ•„ÄÇ‰∏∫‰∫Ü‰øùÁïôËøô‰∏™ÂàÜÂ∏ÉÁöÑ‰∏∞ÂØå‰ø°ÊÅØÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑∑ÂêàËæìÂÖ•ÔºàMoIÔºâÁöÑÊñπÊ≥ïÔºåÂÆÉ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÊ†áËÆ∞ÂêéÔºåÊûÑÂª∫‰∏Ä‰∏™Êñ∞ÁöÑËæìÂÖ•ÔºåÂ∞ÜÁîüÊàêÁöÑÁ¶ªÊï£Ê†áËÆ∞‰∏é‰πãÂâç‰∏¢ÂºÉÁöÑÊ†áËÆ∞ÂàÜÂ∏ÉÊ∑∑Âêà„ÄÇMoI‰ΩøÊ®°ÂûãÂú®ÁîüÊàêËøáÁ®ã‰∏≠‰øùÊåÅÊõ¥‰∏∞ÂØåÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òÊñáÊú¨Ë¥®ÈáèÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12650', 'title': 'AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use', 'url': 'https://huggingface.co/papers/2505.12650', 'abstract': 'Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, converting these images into simulation-ready formats remains labor-intensive and error-prone, creating a bottleneck for model training and validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that automatically transforms scanning transmission electron microscopy (STEM) images into atomic crystal structures and predicts their physical properties. AutoMat combines pattern-adaptive denoising, physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation and property prediction via MatterSim, and coordinated orchestration across all stages. We propose the first dedicated STEM2Mat-Bench for this task and evaluate performance using lattice RMSD, formation energy MAE, and structure-matching success rate. By orchestrating external tool calls, AutoMat enables a text-only LLM to outperform vision-language models in this domain, achieving closed-loop reasoning throughout the pipeline. In large-scale experiments over 450 structure samples, AutoMat substantially outperforms existing multimodal large language models and tools. These results validate both AutoMat and STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic simulation in materials science.The code and dataset are publicly available at https://github.com/yyt-2378/AutoMat and https://huggingface.co/datasets/yaotianvector/STEM2Mat.', 'score': 6, 'issue_id': 3891, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'b02918769ea5f226', 'authors': ['Yaotian Yang', 'Yiwen Tang', 'Yizhe Chen', 'Xiao Chen', 'Jiangjie Qiu', 'Hao Xiong', 'Haoyu Yin', 'Zhiyao Luo', 'Yifei Zhang', 'Sijia Tao', 'Wentao Li', 'Qinghua Zhang', 'Yuqiang Li', 'Wanli Ouyang', 'Bin Zhao', 'Xiaonan Wang', 'Fei Wei'], 'affiliations': ['Department of Chemical Engineering, Tsinghua University, Beijing, China', 'School of Computer Science, Northwestern Polytechnical University, Xian, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.12650.jpg', 'data': {'categories': ['#data', '#multimodal', '#open_source', '#benchmark', '#dataset', '#agents', '#science'], 'emoji': 'üî¨', 'ru': {'title': 'AutoMat: –æ—Ç –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –∞—Ç–æ–º–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º', 'desc': 'AutoMat - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–∫–∞–Ω–∏—Ä—É—é—â–µ–π –ø—Ä–æ—Å–≤–µ—á–∏–≤–∞—é—â–µ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏–∏ (STEM) –≤ –∞—Ç–æ–º–Ω—ã–µ –∫—Ä–∏—Å—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ, —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —à–∞–±–ª–æ–Ω–æ–≤, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∞—Ç–æ–º–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–∏–º–º–µ—Ç—Ä–∏–∏ –∏ –±—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ —Å –ø–æ–º–æ—â—å—é MatterSim. AutoMat –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤ –∑–∞–¥–∞—á–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è STEM-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∞—Ç–æ–º–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –≠—Ç–æ –≤–∞–∂–Ω—ã–π —à–∞–≥ –Ω–∞ –ø—É—Ç–∏ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏–∏ –∏ –∞—Ç–æ–º–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏–∏.'}, 'en': {'title': 'Automating Atomic Structure Extraction from STEM Images with AutoMat', 'desc': 'This paper presents AutoMat, a machine learning pipeline designed to convert scanning transmission electron microscopy (STEM) images into atomic crystal structures efficiently. It addresses the challenge of limited experimental data by automating the transformation process, which traditionally requires significant manual effort. AutoMat integrates various techniques such as denoising, template retrieval, and property prediction to streamline the workflow and enhance model training. The authors also introduce the STEM2Mat-Bench for evaluating performance, demonstrating that AutoMat significantly outperforms existing models in this domain.'}, 'zh': {'title': 'AutoMatÔºöÊòæÂæÆÈïú‰∏éÂéüÂ≠êÊ®°ÊãüÁöÑÊ°•Ê¢Å', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫AutoMatÁöÑËá™Âä®ÂåñÁÆ°ÈÅìÔºåÊó®Âú®Â∞ÜÊâ´ÊèèÈÄèÂ∞ÑÁîµÂ≠êÊòæÂæÆÈïúÔºàSTEMÔºâÂõæÂÉèËΩ¨Êç¢‰∏∫ÂéüÂ≠êÊô∂‰ΩìÁªìÊûÑÔºåÂπ∂È¢ÑÊµãÂÖ∂Áâ©ÁêÜÊÄßË¥®„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂ§öÁßçÊäÄÊúØÔºåÂåÖÊã¨Ëá™ÈÄÇÂ∫îÂéªÂô™„ÄÅÁâ©ÁêÜÂºïÂØºÁöÑÊ®°ÊùøÊ£ÄÁ¥¢ÂíåÂØπÁß∞ÊÑüÁü•ÁöÑÂéüÂ≠êÈáçÂª∫ÔºåËÉΩÂ§üÈ´òÊïàÂú∞Â§ÑÁêÜÊï∞ÊçÆ„ÄÇÈÄöËøáÂºïÂÖ•STEM2Mat-BenchËøõË°åÊÄßËÉΩËØÑ‰º∞ÔºåAutoMatÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÇÊ≠§Á†îÁ©∂‰∏∫ÊùêÊñôÁßëÂ≠¶‰∏≠ÊòæÂæÆÈïú‰∏éÂéüÂ≠êÁ∫ßÊ®°Êãü‰πãÈó¥ÁöÑÊ°•Ê¢ÅÂª∫ËÆæÊèê‰æõ‰∫ÜÈáçË¶ÅËøõÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15524', 'title': 'Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs', 'url': 'https://huggingface.co/papers/2505.15524', 'abstract': 'Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model\'s concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model\'s vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient\'s insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.', 'score': 5, 'issue_id': 3900, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '79c358dbadeae017', 'authors': ['Lang Gao', 'Kaiyang Wan', 'Wei Liu', 'Chenxi Wang', 'Zirui Song', 'Zixiang Xu', 'Yanbo Wang', 'Veselin Stoyanov', 'Xiuying Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.15524.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#interpretability', '#data'], 'emoji': 'üîç', 'ru': {'title': 'BiasLens: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BiasLens - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. BiasLens –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π (CAV) –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (SAE) –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏ —Å–ø–æ—Å–æ–±–µ–Ω –≤—ã—è–≤–ª—è—Ç—å —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. BiasLens –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ LLM, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è —É–ª—É—á—à–µ–Ω–∏—é –∏—Ö —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Uncovering Bias in Language Models with BiasLens', 'desc': "This paper addresses the issue of bias in Large Language Models (LLMs), particularly focusing on how certain concepts can be unfairly correlated with others, leading to skewed outputs. The authors introduce BiasLens, a novel framework that analyzes bias without the need for labeled data, using the model's vector space structure. By employing Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs), BiasLens quantifies bias through representational similarity, demonstrating strong alignment with traditional evaluation methods. This approach not only enhances the detection of subtle biases but also promotes fairness and transparency in LLMs."}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÅèËßÅÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂÅèËßÅÈóÆÈ¢òÔºåÊåáÂá∫ËøôÁßçÂÅèËßÅ‰ºöÂΩ±ÂìçÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊàë‰ª¨ÂÖ≥Ê≥®‰∏ÄÁßçÂ∏∏ËßÅÁöÑÂÅèËßÅÂΩ¢ÂºèÔºåÂç≥Ê®°ÂûãÊ¶ÇÂøµÁ©∫Èó¥‰∏≠‰∏§‰∏™ÂèÇËÄÉÊ¶ÇÂøµÔºàÂ¶ÇÊÉÖÊÑüÊûÅÊÄßÔºâ‰∏éÁõÆÊ†áÊ¶ÇÂøµÔºàÂ¶ÇËØÑËÆ∫ÊñπÈù¢Ôºâ‰πãÈó¥ÁöÑ‰∏çÂØπÁß∞Áõ∏ÂÖ≥ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Áé∞ÊúâÂÅèËßÅËØÑ‰º∞ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜBiasLensÊ°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜÊ¶ÇÂøµÊøÄÊ¥ªÂêëÈáèÔºàCAVsÔºâÂíåÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEsÔºâÔºåÊó†ÈúÄÊ†áÊ≥®Êï∞ÊçÆÂç≥ÂèØËøõË°åÂÅèËßÅÂàÜÊûê„ÄÇBiasLensËÉΩÂ§üÊúâÊïàÈáèÂåñÂÅèËßÅÔºåÂπ∂Êè≠Á§∫‰º†ÁªüÊñπÊ≥ïÈöæ‰ª•Ê£ÄÊµãÁöÑÂÅèËßÅÂΩ¢ÂºèÔºå‰ªéËÄå‰∏∫ÊèêÈ´òLLMsÁöÑÂÖ¨Âπ≥ÊÄßÂíåÈÄèÊòéÊÄßÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15801', 'title': 'VerifyBench: Benchmarking Reference-based Reward Systems for Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.15801', 'abstract': 'Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.', 'score': 4, 'issue_id': 3905, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '45285c81d23cba37', 'authors': ['Yuchen Yan', 'Jin Jiang', 'Zhenbang Ren', 'Yijun Li', 'Xudong Cai', 'Yang Liu', 'Xin Xu', 'Mengdi Zhang', 'Jian Shao', 'Yongliang Shen', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Meituan Group', 'Peking University', 'The Hong Kong University of Science and Technology', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15801.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#optimization', '#rl', '#data', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∞–ª–æ–Ω–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. VerifyBench –∏ VerifyBench-Hard —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ RL. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞, –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ —ç—Ç–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è —Ü–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å–∏—Å—Ç–µ–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∞–ª–æ–Ω–æ–≤.'}, 'en': {'title': 'Enhancing Reasoning Models with New Reward Benchmarks', 'desc': 'This paper discusses the development of two new benchmarks, VerifyBench and VerifyBench-Hard, aimed at evaluating reference-based reward systems in reinforcement learning (RL). These benchmarks are created through careful data collection and human annotation to ensure their quality and effectiveness. The authors highlight that current reasoning models, including smaller-scale ones, still have significant room for improvement when tested against these benchmarks. The paper provides insights into enhancing verifier accuracy and the reasoning abilities of models trained with RL, ultimately guiding future research in this area.'}, 'zh': {'title': 'ÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑÂ•ñÂä±Á≥ªÁªüËØÑ‰º∞', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®Êé®ÁêÜÈ¢ÜÂüüÁöÑÂá∫Ëâ≤Ë°®Áé∞ÔºåÁâπÂà´ÊòØOpenAI o1ÂíåDeepSeek-R1„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§‰∏™Âü∫ÂáÜÊµãËØïÔºåVerifyBenchÂíåVerifyBench-HardÔºåÁî®‰∫éËØÑ‰º∞Âü∫‰∫éÂèÇËÄÉÁöÑÂ•ñÂä±Á≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÁ≤æÂøÉÁöÑÊï∞ÊçÆÊî∂ÈõÜÂíå‰∫∫Â∑•Ê†áÊ≥®ÔºåËøô‰∫õÂü∫ÂáÜÁ°Æ‰øù‰∫ÜÈ´òË¥®ÈáèÁöÑËØÑ‰º∞„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÊ®°ÂûãÂú®Ëøô‰∏§‰∏™Âü∫ÂáÜ‰∏ä‰ªçÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥ÔºåÂ∞§ÂÖ∂ÊòØÂ∞èËßÑÊ®°Ê®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15791', 'title': 'VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL', 'url': 'https://huggingface.co/papers/2505.15791', 'abstract': 'Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.', 'score': 3, 'issue_id': 3892, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '48280e44cc4e905f', 'authors': ['Fengyuan Dai', 'Zifeng Zhuang', 'Yufei Huang', 'Siteng Huang', 'Bangyan Liao', 'Donglin Wang', 'Fajie Yuan'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15791.jpg', 'data': {'categories': ['#diffusion', '#rl', '#training', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'VARD: –£—Å–∏–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VARD (Value-based Reinforced Diffusion) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. VARD –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–∂–∏–¥–∞–µ–º—ã—Ö –Ω–∞–≥—Ä–∞–¥ –∏–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –µ–µ –≤–º–µ—Å—Ç–µ —Å KL-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –±–ª–∏–∑–æ—Å—Ç—å –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–∏ —ç—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VARD —É–ª—É—á—à–∞–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–µ–π, –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Enhancing Diffusion Models with Value-Based Reinforcement Learning', 'desc': "This paper introduces VAlue-based Reinforced Diffusion (VARD), a new method for fine-tuning diffusion models using reinforcement learning. The approach addresses the challenges of stable and efficient training while dealing with non-differentiable rewards by incorporating a value function that predicts expected rewards from intermediate states. By applying KL regularization, VARD provides dense supervision throughout the generation process, enhancing the model's performance. Experimental results show that VARD improves trajectory guidance and training efficiency, making it suitable for complex reward functions in diffusion models."}, 'zh': {'title': 'Âü∫‰∫é‰ª∑ÂÄºÁöÑÂº∫ÂåñÊâ©Êï£ÔºöÊèêÂçáÁîüÊàêË¥®ÈáèÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãÂú®Â§ö‰∏™È¢ÜÂüü‰∏≠Êàê‰∏∫Âº∫Â§ßÁöÑÁîüÊàêÂ∑•ÂÖ∑Ôºå‰ΩÜÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãË∞ÉÊï¥‰∏∫ÂÖ∑ÊúâÁâπÂÆöÊúüÊúõÂ±ûÊÄß‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÂâçÊôØÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÂΩìÂâçÊñπÊ≥ïÂú®ÂÆûÁé∞Á®≥ÂÆö„ÄÅÈ´òÊïàÁöÑÂæÆË∞ÉÂíåÊîØÊåÅÈùûÂèØÂæÆÂàÜÂ•ñÂä±ÊñπÈù¢Â≠òÂú®Âõ∞Èöæ„ÄÇÊ≠§Â§ñÔºåÁ®ÄÁñèÂ•ñÂä±Âú®‰∏≠Èó¥Ê≠•È™§Êèê‰æõÁöÑÁõëÁù£‰∏çË∂≥ÔºåÂ∏∏Â∏∏ÂØºËá¥ÁîüÊàêË¥®Èáè‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂü∫‰∫é‰ª∑ÂÄºÁöÑÂº∫ÂåñÊâ©Êï£ÔºàVARDÔºâÊñπÊ≥ïÔºåÈÄöËøáÂ≠¶‰π†‰ª∑ÂÄºÂáΩÊï∞Êù•È¢ÑÊµã‰∏≠Èó¥Áä∂ÊÄÅÁöÑÂ•ñÂä±ÊúüÊúõÔºåÂπ∂‰ΩøÁî®KLÊ≠£ÂàôÂåñÊèê‰æõÂØÜÈõÜÁõëÁù£Ôºå‰ªéËÄåÊèêÈ´òÁîüÊàêËøáÁ®ãÁöÑË¥®ÈáèÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15406', 'title': 'Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large\n  Audio-Language Models', 'url': 'https://huggingface.co/papers/2505.15406', 'abstract': 'The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.', 'score': 3, 'issue_id': 3900, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '2d6a5f87ed813485', 'authors': ['Zirui Song', 'Qian Jiang', 'Mingxuan Cui', 'Mingzhe Li', 'Lang Gao', 'Zeyu Zhang', 'Zixiang Xu', 'Yanbo Wang', 'Chenxi Wang', 'Guangxian Ouyang', 'Zhenhao Chen', 'Xiuying Chen'], 'affiliations': ['Australia National University', 'ByteDance', 'Mohamed bin Zayed University of Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2505.15406.jpg', 'data': {'categories': ['#ethics', '#dataset', '#benchmark', '#security', '#audio'], 'emoji': 'üéôÔ∏è', 'ru': {'title': 'AJailBench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ-–ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AJailBench - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –∫ –∞—Ç–∞–∫–∞–º –¥–∂–µ–π–ª–±—Ä–µ–π–∫–∞ –≤ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LAM). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö AJailBench-Base –∏–∑ 1495 –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –∞—É–¥–∏–æ-–∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ü–µ–Ω–∏–ª–∏ –Ω–∞ –Ω–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LAM. –¢–∞–∫–∂–µ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π Audio Perturbation Toolkit –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∞—Ç–∞–∫ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–æ–∑–º—É—â–µ–Ω–∏—è –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–µ–¥—É—â–∏—Ö LAM.'}, 'en': {'title': 'Evaluating and Enhancing Safety in Large Audio Language Models', 'desc': 'This paper addresses the safety concerns associated with Large Audio Language Models (LAMs), particularly their vulnerability to jailbreak attacks. It introduces AJailBench, a benchmark designed to systematically evaluate these vulnerabilities using a dataset of adversarial audio prompts. The study reveals that current LAMs lack consistent robustness against various attacks, highlighting the need for improved defenses. Additionally, the authors propose an Audio Perturbation Toolkit (APT) to create subtle yet effective adversarial audio samples that maintain semantic integrity, demonstrating the significant impact of small perturbations on LAM safety.'}, 'zh': {'title': 'ËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß‰∏éÊºèÊ¥û', 'desc': 'Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàLAMsÔºâÁöÑÂÖ¥Ëµ∑Â∏¶Êù•‰∫ÜÊΩúÂäõÂíåÈ£éÈô©ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÈü≥È¢ëËæìÂá∫ÂèØËÉΩÂåÖÂê´ÊúâÂÆ≥Êàñ‰∏çÈÅìÂæ∑ÁöÑÂÜÖÂÆπ„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂Áº∫‰πèÂØπLAMÂÆâÂÖ®ÊÄßÁöÑÁ≥ªÁªüÊÄßÂÆöÈáèËØÑ‰º∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈíàÂØπË∂äÁã±ÊîªÂáªÊñπÈù¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜAJailBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËØÑ‰º∞LAMË∂äÁã±ÊºèÊ¥ûÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÂæÆÂ∞èÁöÑ„ÄÅ‰øùÊåÅËØ≠‰πâ‰∏ÄËá¥ÁöÑÊâ∞Âä®‰πüËÉΩÊòæËëóÈôç‰ΩéÈ¢ÜÂÖàLAMÁöÑÂÆâÂÖ®ÊÄßËÉΩÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅÊõ¥Âº∫Â§ßÂíåÊõ¥ÂÖ∑ËØ≠‰πâÊÑèËØÜÁöÑÈò≤Âæ°Êú∫Âà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15047', 'title': 'PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration', 'url': 'https://huggingface.co/papers/2505.15047', 'abstract': 'Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering systematic uncertainty reduction. Overcoming these limitations fundamentally requires systematic uncertainty reduction. We introduce PiFlow, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\\% compared to a vanilla agent system. Overall, PiFlow serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our https://github.com/amair-lab/PiFlow{GitHub}.', 'score': 3, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '12f9a805fafedcf5', 'authors': ['Yingming Pu', 'Tao Lin', 'Hongyu Chen'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15047.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#rl', '#agents', '#science'], 'emoji': 'üß™', 'ru': {'title': 'PiFlow: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞—É—á–Ω–æ–º –æ—Ç–∫—Ä—ã—Ç–∏–∏', 'desc': 'PiFlow - —ç—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –Ω–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö. PiFlow –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –Ω–∞–Ω–æ–º–∞—Ç–µ—Ä–∏–∞–ª—ã, –±–∏–æ–º–æ–ª–µ–∫—É–ª—ã –∏ —Å–≤–µ—Ä—Ö–ø—Ä–æ–≤–æ–¥–Ω–∏–∫–∏. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, PiFlow –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π –Ω–∞ 94.06% –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ –ø–æ–¥ –∫—Ä–∏–≤–æ–π –∑–Ω–∞—á–µ–Ω–∏–π —Å–≤–æ–π—Å—Ç–≤ –Ω–∞ 73.55%.'}, 'en': {'title': 'PiFlow: Revolutionizing Automated Scientific Discovery through Uncertainty Reduction', 'desc': 'This paper presents PiFlow, a new framework for improving automated scientific discovery using Large Language Model (LLM)-based multi-agent systems. It addresses the limitations of existing methods that often lack rationality and fail to connect hypotheses with evidence, leading to inefficient exploration. By framing the discovery process as a structured uncertainty reduction problem, PiFlow enhances the efficiency and quality of scientific discoveries across various domains. The results show significant improvements in discovery efficiency and solution quality, marking a shift towards more effective AI-driven research.'}, 'zh': {'title': 'PiFlowÔºöÈ´òÊïàÁöÑËá™Âä®ÂåñÁßëÂ≠¶ÂèëÁé∞Êñ∞ËåÉÂºè', 'desc': 'Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºàMASÔºâÂú®ÁßëÂ≠¶ÂèëÁé∞‰∏≠Â±ïÁé∞Âá∫ÊòæËëóÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®È¢ÑÂÆö‰πâÁöÑÂ∑•‰ΩúÊµÅÁ®ãÊù•Ëá™Âä®ÂåñÁßëÂ≠¶ÂèëÁé∞ÔºåËøô‰∫õÊµÅÁ®ãÁº∫‰πèÂêàÁêÜÊÄßÁ∫¶ÊùüÔºåÂØºËá¥ÂÅáËÆæÊó†ÁõÆÁöÑ‰∏îÊó†Ê≥ïÊúâÊïàÈìæÊé•ÂÅáËÆæ‰∏éËØÅÊçÆÔºå‰ªéËÄåÈòªÁ¢çÁ≥ªÁªüÊÄßÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂáèÂ∞ë„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜPiFlowÔºå‰∏Ä‰∏™‰ø°ÊÅØÁêÜËÆ∫Ê°ÜÊû∂ÔºåÂ∞ÜËá™Âä®ÂåñÁßëÂ≠¶ÂèëÁé∞ËßÜ‰∏∫‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂáèÂ∞ëÈóÆÈ¢òÔºåÂπ∂‰ª•ÁßëÂ≠¶ÂéüÂàô‰∏∫ÊåáÂØº„ÄÇÂú®‰∏â‰∏™‰∏çÂêåÁöÑÁßëÂ≠¶È¢ÜÂüü‰∏≠ËøõË°åËØÑ‰º∞Êó∂ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÂèëÁé∞ÊïàÁéáÔºåÂπ∂‰∏îËß£ÂÜ≥ÊñπÊ°àË¥®Èáè‰πüÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15034', 'title': 'RL Tango: Reinforcing Generator and Verifier Together for Language\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.15034', 'abstract': 'Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.', 'score': 3, 'issue_id': 3897, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '35041556b54ed6d4', 'authors': ['Kaiwen Zha', 'Zhengqi Gao', 'Maohao Shen', 'Zhang-Wei Hong', 'Duane S. Boning', 'Dina Katabi'], 'affiliations': ['MIT', 'MIT-IBM Watson AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.15034.jpg', 'data': {'categories': ['#math', '#rlhf', '#reasoning', '#rl', '#training', '#optimization'], 'emoji': 'üï∫', 'ru': {'title': 'Tango: –¢–∞–Ω–µ—Ü –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Tango –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, Tango –æ–±—É—á–∞–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å, –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –æ–±—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é RL, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –æ–±–æ–±—â–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∏–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å –ø–æ–º–æ—â—å—é SFT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Tango –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π –º–∞—Å—à—Ç–∞–±–∞ 7B/8B –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –∏ –∑–∞–¥–∞—á–∞–º –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Tango: Reinforcing Language Models with Generative Verifiers', 'desc': 'This paper introduces Tango, a new framework that enhances large language models (LLMs) using reinforcement learning (RL). Unlike traditional methods that rely on fixed or supervised verifiers, Tango trains both the LLM generator and the verifier together, allowing them to improve each other through mutual reinforcement. The verifier in Tango is generative and learns from outcome-level rewards, making it more robust and capable of generalizing better to new tasks. Experiments show that Tango achieves state-of-the-art performance on various reasoning benchmarks, especially in challenging mathematical problems.'}, 'zh': {'title': 'TangoÔºöÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊúÄËøëÊàê‰∏∫ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜËÉΩÂäõÁöÑ‰∏ÄÁßçÊúâÊïàÊñπÊ≥ï„ÄÇÂú®Áé∞ÊúâÁöÑRLÂêéËÆ≠ÁªÉÊñπÊ≥ï‰∏≠ÔºåÈÄöÂ∏∏‰ΩøÁî®Âõ∫ÂÆöÁöÑÈ™åËØÅÂô®ÔºåËøôÂèØËÉΩÂØºËá¥Â•ñÂä±ÈªëÂÆ¢Ë°å‰∏∫ÂíåÊ≥õÂåñËÉΩÂäõÂ∑Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜTangoÊ°ÜÊû∂ÔºåÂÆÉÈÄöËøá‰∫§ÊõøËÆ≠ÁªÉLLMÁîüÊàêÂô®ÂíåÈ™åËØÅÂô®Êù•ÂÆûÁé∞Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇTangoÁöÑÂàõÊñ∞Âú®‰∫éÂÖ∂ÁîüÊàêÂºèÁöÑËøáÁ®ãÁ∫ßÈ™åËØÅÂô®ÔºåËÉΩÂ§üÂú®Ê≤°ÊúâÊòéÁ°ÆËøáÁ®ãÁ∫ßÊ≥®ÈáäÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªÖÂü∫‰∫éÁªìÊûúÁ∫ßÈ™åËØÅÂ•ñÂä±ËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15816', 'title': 'Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM', 'url': 'https://huggingface.co/papers/2505.15816', 'abstract': 'Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss. Our key insight is that vision tokens from the pretrained vision encoder do not necessarily require all the heavy operations (e.g., self-attention, FFNs) in decoder-only LMMs and could be processed more lightly with proper designs. We designed a series of experiments to discover and progressively squeeze out the vision-related computation redundancy. Based on our findings, we propose ProxyV, a novel approach that utilizes proxy vision tokens to alleviate the computational burden on original vision tokens. ProxyV enhances efficiency without compromising performance and can even yield notable performance gains in scenarios with more moderate efficiency improvements. Furthermore, the flexibility of ProxyV is demonstrated through its combination with token reduction methods to boost efficiency further. The code will be made public at this https://github.com/penghao-wu/ProxyV URL.', 'score': 2, 'issue_id': 3891, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '96ff09e995e7c437', 'authors': ['Penghao Wu', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['1S-Lab, Nanyang Technological University', 'SenseTime'], 'pdf_title_img': 'assets/pdf/title_img/2505.15816.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#open_source', '#architecture', '#inference'], 'emoji': 'üîç', 'ru': {'title': 'ProxyV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ProxyV –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–µ–∫–æ–¥–µ—Ä-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. ProxyV –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–∫—Å–∏-—Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–∏–±–∫–æ—Å—Ç—å –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤.'}, 'en': {'title': 'Enhancing Efficiency in Multimodal Models with Proxy Vision Tokens', 'desc': 'This paper addresses the computational challenges faced by large multimodal models, particularly in processing visual tokens. Instead of focusing on reducing the number of tokens, the authors explore computation-level redundancy, revealing that not all heavy operations are necessary for vision tokens in decoder-only models. They introduce ProxyV, a method that uses proxy vision tokens to reduce the computational load while maintaining or even improving performance. The study shows that ProxyV can be combined with existing token reduction techniques for even greater efficiency gains.'}, 'zh': {'title': 'ProxyVÔºöÂáèËΩªËßÜËßâËÆ°ÁÆóË¥üÊãÖÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§ÑÁêÜËßÜËßâÊ†áËÆ∞Êó∂ÁöÑËÆ°ÁÆóÂÜó‰ΩôÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÈ¢ÑËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®ÁîüÊàêÁöÑËßÜËßâÊ†áËÆ∞Âπ∂‰∏çÈúÄË¶ÅÂú®Ëß£Á†ÅÂô®‰∏≠ÊâßË°åÊâÄÊúâÈáçËÆ°ÁÆóÊìç‰Ωú„ÄÇÈÄöËøáËÆæËÆ°‰∏ÄÁ≥ªÂàóÂÆûÈ™åÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜProxyVÊñπÊ≥ïÔºåÂà©Áî®‰ª£ÁêÜËßÜËßâÊ†áËÆ∞Êù•ÂáèËΩªÂéüÂßãËßÜËßâÊ†áËÆ∞ÁöÑËÆ°ÁÆóË¥üÊãÖ„ÄÇProxyVÂú®ÊèêÈ´òÊïàÁéáÁöÑÂêåÊó∂‰∏çÂΩ±ÂìçÊÄßËÉΩÔºåÁîöËá≥Âú®ÈÄÇÂ∫¶ÊèêÈ´òÊïàÁéáÁöÑÊÉÖÂÜµ‰∏ãËøòËÉΩÊòæËëóÊèêÂçáÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14818', 'title': 'WebNovelBench: Placing LLM Novelists on the Web Novel Distribution', 'url': 'https://huggingface.co/papers/2505.14818', 'abstract': 'Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.', 'score': 2, 'issue_id': 3900, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '54fde916bc0c7b6f', 'authors': ['Leon Lin', 'Jun Zheng', 'Haidong Wang'], 'affiliations': ['Nanyang Technological University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14818.jpg', 'data': {'categories': ['#story_generation', '#long_context', '#dataset', '#benchmark'], 'emoji': 'üìö', 'ru': {'title': 'WebNovelBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏', 'desc': 'WebNovelBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ä–æ–º–∞–Ω—ã. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 4000 –∫–∏—Ç–∞–π—Å–∫–∏—Ö –≤–µ–±-–Ω–æ–≤–µ–ª–ª –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –≤–æ—Å—å–º–∏ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ–¥—Ö–æ–¥–∞ LLM-as-Judge, –∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≥—Ä–µ–≥–∏—Ä—É—é—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –∞–Ω–∞–ª–∏–∑–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. –ë–µ–Ω—á–º–∞—Ä–∫ —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–ª–∏—á–∞–µ—Ç —à–µ–¥–µ–≤—Ä—ã, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏, –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–µ–±-–Ω–æ–≤–µ–ª–ª—ã –∏ –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM.'}, 'en': {'title': 'WebNovelBench: A New Standard for Evaluating LLM Storytelling', 'desc': 'This paper presents WebNovelBench, a new benchmark for evaluating the storytelling abilities of Large Language Models (LLMs) in generating long-form narratives. It utilizes a dataset of over 4,000 Chinese web novels and frames the evaluation as a task of generating stories from synopses. The framework assesses narrative quality across eight dimensions using an LLM-as-Judge approach, with results analyzed through Principal Component Analysis. The findings show that WebNovelBench can effectively distinguish between human-written and LLM-generated stories, providing valuable insights for improving LLM storytelling capabilities.'}, 'zh': {'title': 'ËØÑ‰º∞ÈïøÁØáÂ∞èËØ¥ÁîüÊàêÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜWebNovelBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈïøÁØáÂ∞èËØ¥ÁîüÊàêËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ„ÄÇËØ•Âü∫ÂáÜÂà©Áî®‰∫ÜË∂ÖËøá4000ÈÉ®‰∏≠ÊñáÁΩëÁªúÂ∞èËØ¥ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂ∞ÜËØÑ‰º∞Ê°ÜÊû∂ËÆæÂÆö‰∏∫‰ªéÊëòË¶ÅÂà∞ÊïÖ‰∫ãÁîüÊàêÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÁª¥Â∫¶ÁöÑÊ°ÜÊû∂ÔºåÊ∂µÁõñÂÖ´‰∏™Âèô‰∫ãË¥®ÈáèÁª¥Â∫¶ÔºåÈÄöËøáLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÊñπÂºèËøõË°åËá™Âä®ËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWebNovelBenchËÉΩÂ§üÊúâÊïàÂå∫ÂàÜ‰∫∫Á±ªÂàõ‰ΩúÁöÑÊù∞‰Ωú„ÄÅÂèóÊ¨¢ËøéÁöÑÁΩëÁªúÂ∞èËØ¥ÂíåLLMÁîüÊàêÁöÑÂÜÖÂÆπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14157', 'title': 'Prior Prompt Engineering for Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.14157', 'abstract': 'This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.', 'score': 2, 'issue_id': 3898, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'bb33e698375153d7', 'authors': ['Pittawat Taveekitworachai', 'Potsawee Manakul', 'Sarana Nutanong', 'Kunat Pipatanakul'], 'affiliations': ['SCB 10X R&D, SCB 10X, SCBX Group, Thailand', 'School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2505.14157.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ (pPE) –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∏ –ø—è—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ pPE. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –≤—Å–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å pPE, –ø—Ä–µ–≤–∑–æ—à–ª–∏ —Å–≤–æ–∏ –∞–Ω–∞–ª–æ–≥–∏ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –ø—Ä–∏—á–µ–º –ø–æ–¥—Ö–æ–¥ —Å –Ω—É–ª–µ–≤—ã–º –ø—Ä–∏–º–µ—Ä–æ–º –ø–æ–∫–∞–∑–∞–ª –Ω–∞–∏–±–æ–ª—å—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–æ, —á—Ç–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pPE —Ñ–æ—Ä–º–∏—Ä—É—é—Ç —É –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —Å—Ç–∏–ª–∏.'}, 'en': {'title': 'Unlocking Performance: The Power of Prior Prompt Engineering in Language Models', 'desc': "This paper explores the concept of prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) for language models (LMs). It highlights how the design of prompts, which guide the model's behavior during training, can significantly influence performance. The authors translate various inference-time prompt engineering strategies into prior prompt engineering methods and test them on a model called Qwen2.5-7B. The results indicate that models trained with pPE outperform those using traditional methods, particularly showing notable improvements in specific benchmarks."}, 'zh': {'title': 'ÂÖàÂâçÊèêÁ§∫Â∑•Á®ãÔºöÂº∫ÂåñÂæÆË∞ÉÁöÑÊñ∞ÊñπÂêë', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®Âº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâËÉåÊôØ‰∏ãÁöÑÂÖàÂâçÊèêÁ§∫Â∑•Á®ãÔºàpPEÔºâÔºåÊó®Âú®ÈÄöËøáÂ•ñÂä±‰ø°Âè∑ÂºïÂØºËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâË°®Áé∞Âá∫ÊúÄÂ§ßÂåñÊÄßËÉΩÁöÑË°å‰∏∫„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÁöÑRFTÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁÆóÊ≥ï„ÄÅÂ•ñÂä±Â°ëÈÄ†ÂíåÊï∞ÊçÆÊï¥ÁêÜ‰∏äÔºå‰ΩÜÂÖàÂâçÊèêÁ§∫ÁöÑËÆæËÆ°‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢ËÆ®„ÄÇÊàë‰ª¨Â∞Ü‰∫îÁßç‰ª£Ë°®ÊÄßÁöÑÊé®ÁêÜÊó∂Èó¥ÊèêÁ§∫Â∑•Á®ãÔºàiPEÔºâÁ≠ñÁï•ËΩ¨Âåñ‰∏∫Áõ∏Â∫îÁöÑpPEÊñπÊ≥ïÔºåÂπ∂Âú®Qwen2.5-7BÊ®°Âûã‰∏äËøõË°åÂÆûÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊúâpPEËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÂùá‰ºò‰∫éiPEÊèêÁ§∫ÁöÑÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØnull-example pPEÊñπÊ≥ïÂú®AIME2024ÂíåGPQA-Diamond‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14336', 'title': 'Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach', 'url': 'https://huggingface.co/papers/2505.14336', 'abstract': 'Llama-SMoP, an efficient multimodal LLM incorporating Sparse Mixture of Projectors, enhances AVSR performance without increasing inference costs through modality-specific routers and experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy environments by integrating visual cues. While recent advances integrate Large Language Models (LLMs) into AVSR, their high computational cost hinders deployment in resource-constrained settings. To address this, we propose Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of Projectors (SMoP) module to scale model capacity without increasing inference costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors, Llama-SMoP enables the use of smaller LLMs while maintaining strong performance. We explore three SMoP configurations and show that Llama-SMoP DEDR (Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation studies confirm its effectiveness in expert activation, scalability, and noise robustness.', 'score': 1, 'issue_id': 3903, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'd5c843866931f713', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis', 'Daniele Falavigna', 'Alessio Brutti'], 'affiliations': ['Fondazione Bruno Kessler, Italy', 'Imperial College London, UK', 'Meta AI, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.14336.jpg', 'data': {'categories': ['#optimization', '#audio', '#multimodal', '#inference', '#architecture', '#low_resource'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤', 'desc': 'Llama-SMoP - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ú–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–∞–ª—å–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä—ã –∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. Llama-SMoP –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ DEDR —Å —Ä–∞–∑–¥–µ–ª—å–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Efficient AVSR with Llama-SMoP: Smart Scaling for Strong Performance', 'desc': "Llama-SMoP is a novel multimodal large language model designed to improve audio-visual speech recognition (AVSR) while keeping inference costs low. It utilizes a Sparse Mixture of Projectors (SMoP) to enhance model capacity without the need for larger models. By implementing modality-specific routers and experts, Llama-SMoP effectively manages resources and maintains high performance in challenging environments. The model's configurations, particularly the DEDR setup, demonstrate significant advancements in ASR, VSR, and AVSR tasks, proving its efficiency and robustness against noise."}, 'zh': {'title': 'È´òÊïàÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊèêÂçáËØ≠Èü≥ËØÜÂà´ÊÄßËÉΩ', 'desc': 'Llama-SMoPÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÈááÁî®Á®ÄÁñèÊ∑∑ÂêàÊäïÂΩ±Âô®(SMoP)Ê®°ÂùóÔºåÊó®Âú®ÊèêÈ´òÈü≥ËßÜÈ¢ëËØ≠Èü≥ËØÜÂà´(AVSR)ÁöÑÊÄßËÉΩÔºåÂêåÊó∂‰∏çÂ¢ûÂä†Êé®ÁêÜÊàêÊú¨„ÄÇËØ•Ê®°ÂûãÈÄöËøá‰ΩøÁî®ÁâπÂÆöÊ®°ÊÄÅÁöÑË∑ØÁî±Âô®Âíå‰∏ìÂÆ∂ÔºåËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÊúâÊïàËøêË°å„ÄÇLlama-SMoPÈÄöËøáÁ®ÄÁñèÈó®ÊéßÁöÑ‰∏ìÂÆ∂Ê∑∑Âêà(MoE)ÊäïÂΩ±Âô®ÔºåÂÖÅËÆ∏‰ΩøÁî®ËæÉÂ∞èÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂêåÊó∂‰øùÊåÅÂº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLlama-SMoPÂú®ËØ≠Èü≥ËØÜÂà´(ASR)„ÄÅËßÜËßâËØÜÂà´(VSR)ÂíåÈü≥ËßÜÈ¢ëËØ≠Èü≥ËØÜÂà´(AVSR)‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊäóÂô™Â£∞ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14101', 'title': 'MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation\n  of LLM Hallucinations', 'url': 'https://huggingface.co/papers/2505.14101', 'abstract': 'A multilingual, multihop benchmark using knowledge graphs for evaluating and mitigating hallucinations in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benchmarks have been developed that provide a test bed for factuality evaluation within the context of English-centric datasets, while relying on supplementary informative context like web links or text passages but ignoring the available structured factual resources. To this end, Knowledge Graphs (KGs) have been identified as a useful aid for hallucination mitigation, as they provide a structured way to represent the facts about entities and their relations with minimal linguistic overhead. We bridge the lack of KG paths and multilinguality for factual language modeling within the existing hallucination evaluation benchmarks and propose a KG-based multilingual, multihop benchmark called MultiHal framed for generative text evaluation. As part of our data collection pipeline, we mined 140k KG-paths from open-domain KGs, from which we pruned noisy KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation shows an absolute scale increase by approximately 0.12 to 0.36 points for the semantic similarity score in KG-RAG over vanilla QA across multiple languages and multiple models, demonstrating the potential of KG integration. We anticipate MultiHal will foster future research towards several graph-based hallucination mitigation and fact-checking tasks.', 'score': 1, 'issue_id': 3901, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'a89e20d6ea8c5a4b', 'authors': ['Ernests Lavrinovics', 'Russa Biswas', 'Katja Hose', 'Johannes Bjerva'], 'affiliations': ['Department of Computer Science Aalborg University Copenhagen, Denmark', 'Institute of Logic and Computation TU Wien Vienna, Austria'], 'pdf_title_img': 'assets/pdf/title_img/2505.14101.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#dataset', '#graphs', '#multilingual', '#data', '#rag'], 'emoji': 'üß†', 'ru': {'title': '–ì—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MultiHal –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π (KG) –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 25,9 —Ç—ã—Å—è—á –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö KG-–ø—É—Ç–µ–π, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π. –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ 0,12-0,36 –ø—É–Ω–∫—Ç–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ KG-RAG –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Enhancing Language Models with Knowledge Graphs to Combat Hallucinations', 'desc': "This paper introduces a new benchmark called MultiHal, designed to evaluate and reduce hallucinations in large language models (LLMs) using knowledge graphs (KGs). Hallucinations refer to inaccuracies in the generated text, and existing benchmarks often focus on English datasets without utilizing structured factual resources. MultiHal incorporates multilingual and multihop capabilities by leveraging KGs, which represent facts and relationships in a structured format. The authors demonstrate that integrating KGs improves the semantic similarity scores in generative text evaluation across various languages and models, highlighting the benchmark's potential for advancing research in fact-checking and hallucination mitigation."}, 'zh': {'title': 'Âà©Áî®Áü•ËØÜÂõæË∞±ÂáèËΩªËØ≠Ë®ÄÊ®°ÂûãÂπªËßâÁöÑÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öËØ≠Ë®Ä„ÄÅÂ§öË∑≥Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞ÂíåÂáèËΩªÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁü•ËØÜÂõæË∞±ÔºàKGÔºâÂèØ‰ª•ÊúâÊïàÂú∞Â∏ÆÂä©Ëß£ÂÜ≥ËØ≠Ë®ÄÊ®°ÂûãÂú®‰∫ãÂÆûÊÄßÂíåÂèØ‰ø°Â∫¶ÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøáÊåñÊéòÂíåÁ≠õÈÄâ140,000Êù°KGË∑ØÂæÑÔºåÊúÄÁªàÊûÑÂª∫‰∫Ü25,900Êù°È´òË¥®ÈáèÁöÑKGË∑ØÂæÑÔºå‰ª•ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑ‰∫ãÂÆûËØ≠Ë®ÄÂª∫Ê®°„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊï¥ÂêàKGÂêéÔºåËØ≠‰πâÁõ∏‰ººÂ∫¶ËØÑÂàÜÂú®Â§ö‰∏™ËØ≠Ë®ÄÂíåÊ®°Âûã‰∏≠ÊúâÊòæËëóÊèêÂçáÔºåË°®ÊòéKGÂú®ÂáèËΩªÂπªËßâÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11454', 'title': 'HumaniBench: A Human-Centric Framework for Large Multimodal Models\n  Evaluation', 'url': 'https://huggingface.co/papers/2505.11454', 'abstract': 'HumaniBench evaluates state-of-the-art LMMs on seven human-centered AI principles using 32K real-world image-question pairs to ensure fairness, ethics, empathy, and inclusivity.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench', 'score': 1, 'issue_id': 3900, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '3ca34ea5393b883b', 'authors': ['Shaina Raza', 'Aravind Narayanan', 'Vahid Reza Khazaie', 'Ashmal Vayani', 'Mukund S. Chettiar', 'Amandeep Singh', 'Mubarak Shah', 'Deval Pandya'], 'affiliations': ['University of Central Florida, Orlando, USA', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.11454.jpg', 'data': {'categories': ['#alignment', '#ethics', '#multimodal', '#dataset', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –ò–ò –ø–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –º–µ—Ä–∫–∞–º', 'desc': 'HumaniBench - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –ø–æ —Å–µ–º–∏ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º —á–µ–ª–æ–≤–µ–∫–æ—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–≥–æ –ò–ò. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 32 —Ç—ã—Å—è—á–∏ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤–æ–ø—Ä–æ—Å –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö –∑–∞–¥–∞—á–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ QA, –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —ç–º–ø–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 15 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LMM –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–æ–º –ª–∏–¥–∏—Ä—É—é—Ç, —Ö–æ—Ç—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å—Ç–∞—é—Ç—Å—è —Å–ª–∞–±—ã–º–∏ –º–µ—Å—Ç–∞–º–∏. HumaniBench –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–æ–≥—É—é —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è LMM –∫ –ø–æ–≤–µ–¥–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ —Ç–æ—á–Ω—ã–º, —Ç–∞–∫ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º.'}, 'en': {'title': 'HumaniBench: Aligning AI with Human Values', 'desc': 'HumaniBench is a new benchmark designed to evaluate large multimodal models (LMMs) based on seven human-centered AI principles, including fairness and empathy. It uses a dataset of 32,000 real-world image-question pairs, which are annotated with the help of a scalable GPT-4 assisted pipeline and verified by experts. The benchmark assesses LMMs across various tasks such as visual question answering and empathetic captioning, revealing that while proprietary models often perform better, they still have weaknesses in robustness and visual grounding. HumaniBench aims to identify alignment gaps in LMMs and promote their development towards more socially responsible and human-aligned behaviors.'}, 'zh': {'title': '‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩËØÑ‰º∞Âü∫ÂáÜ', 'desc': 'HumaniBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫é‰∏É‰∏™‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂéüÂàôÔºåÂåÖÊã¨ÂÖ¨Âπ≥ÊÄß„ÄÅ‰º¶ÁêÜ„ÄÅÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅËØ≠Ë®ÄÂåÖÂÆπÊÄß„ÄÅÂêåÁêÜÂøÉÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®32,000‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÂõæÂÉè-ÈóÆÈ¢òÂØπÔºåÈÄöËøáÂèØÊâ©Â±ïÁöÑGPT4oËæÖÂä©ÁÆ°ÈÅìËøõË°åÊ≥®ÈáäÔºåÂπ∂Áî±È¢ÜÂüü‰∏ìÂÆ∂ËøõË°åÂÖ®Èù¢È™åËØÅ„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õ‰∏ìÊúâÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äË°®Áé∞ËæÉÂ•ΩÔºå‰ΩÜÂú®È≤ÅÊ£íÊÄßÂíåËßÜËßâÂÆö‰ΩçÊñπÈù¢‰ªçÂ≠òÂú®‰∏çË∂≥ÔºåËÄå‰∏Ä‰∫õÂºÄÊ∫êÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄß‰∏éÈÅµÂæ™‰∫∫Á±ªÂØπÈΩêÂéüÂàô‰πãÈó¥‰πüÈöæ‰ª•Âπ≥Ë°°„ÄÇHumaniBench ÊòØÈ¶ñ‰∏™‰∏ìÈó®Âõ¥Áªï‰ª•‰∫∫‰∏∫Êú¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂéüÂàôÊûÑÂª∫ÁöÑÂü∫ÂáÜÔºå‰∏∫ËØäÊñ≠ÂØπÈΩêÂ∑ÆË∑ùÂíåÂºïÂØºLMMsÊúùÂêëÂáÜÁ°Æ‰∏îÁ§æ‰ºöË¥£‰ªªÊÑüÂº∫ÁöÑË°å‰∏∫Êèê‰æõ‰∫Ü‰∏•Ê†ºÁöÑÊµãËØïÂπ≥Âè∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11196', 'title': 'DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion\n  Modeling', 'url': 'https://huggingface.co/papers/2505.11196', 'abstract': 'Diffusion ConvNet (DiCo), using standard ConvNet modules with a compact channel attention mechanism, achieves high image quality and generation speed in visual generation tasks with efficiency gains compared to Diffusion Transformer (DiT).  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256x256-without any additional supervision during training. Code: https://github.com/shallowdream204/DiCo.', 'score': 1, 'issue_id': 3909, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'c15c5dc1b113f10f', 'authors': ['Yuang Ai', 'Qihang Fan', 'Xuefeng Hu', 'Zhenheng Yang', 'Ran He', 'Huaibo Huang'], 'affiliations': ['ByteDance', 'CASIA', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2505.11196.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#diffusion', '#architecture'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'DiCo: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Diffusion ConvNet (DiCo) - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. DiCo –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –º–æ–¥—É–ª–∏ —Å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –∫–∞–Ω–∞–ª–∞–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Diffusion Transformer (DiT), DiCo –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö ImageNet –º–æ–¥–µ–ª—å DiCo –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫ –∏ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Efficient Visual Generation with Diffusion ConvNet (DiCo)', 'desc': 'The paper introduces Diffusion ConvNet (DiCo), a new model for visual generation that utilizes standard convolutional network (ConvNet) modules combined with a compact channel attention mechanism. This approach enhances image quality and generation speed while being more efficient than the existing Diffusion Transformer (DiT) models, which are computationally intensive. The authors identify that traditional self-attention in DiT often captures redundant local patterns, suggesting that convolution can be a more effective alternative if properly optimized. By promoting diverse channel activations, DiCo achieves superior performance on ImageNet benchmarks, demonstrating significant improvements in both image quality and generation speed.'}, 'zh': {'title': 'Êâ©Êï£Âç∑ÁßØÁΩëÁªúÔºöÈ´òÊïàÁîüÊàêÁöÑÊú™Êù•', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊâ©Êï£Ê®°ÂûãÔºåÁß∞‰∏∫Êâ©Êï£Âç∑ÁßØÁΩëÁªúÔºàDiCoÔºâÔºåÂÆÉ‰ΩøÁî®Ê†áÂáÜÁöÑÂç∑ÁßØÁΩëÁªúÊ®°ÂùóÂíåÁ¥ßÂáëÁöÑÈÄöÈÅìÊ≥®ÊÑèÊú∫Âà∂ÔºåÊó®Âú®ÊèêÈ´òËßÜËßâÁîüÊàê‰ªªÂä°ÁöÑÂõæÂÉèË¥®ÈáèÂíåÁîüÊàêÈÄüÂ∫¶„ÄÇ‰∏éÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTÔºâÁõ∏ÊØîÔºåDiCoÂú®ÊïàÁéá‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂõæÂÉèÊó∂Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÊçïÊçâÂÖ®Â±Ä‰ø°ÊÅØÊó∂ÂæÄÂæÄÂÜó‰ΩôÔºåËÄåÂç∑ÁßØÁΩëÁªúÂàôËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊèêÂèñÂ±ÄÈÉ®ÁâπÂæÅ„ÄÇÈÄöËøáÂºïÂÖ•Á¥ßÂáëÁöÑÈÄöÈÅìÊ≥®ÊÑèÊú∫Âà∂ÔºåDiCoËÉΩÂ§üÊøÄÊ¥ªÊõ¥Â§öÊ†∑ÂåñÁöÑÈÄöÈÅìÔºå‰ªéËÄåÂ¢ûÂº∫ÁâπÂæÅÁöÑÂ§öÊ†∑ÊÄßÔºåÊúÄÁªàÂú®ImageNetÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊâ©Êï£Ê®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15141', 'title': 'BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms', 'url': 'https://huggingface.co/papers/2505.15141', 'abstract': 'Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.', 'score': 0, 'issue_id': 3905, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '32711fd3625843fd', 'authors': ['Yunlong Hou', 'Fengzhuo Zhang', 'Cunxiao Du', 'Xuan Zhang', 'Jiachun Pan', 'Tianyu Pang', 'Chao Du', 'Vincent Y. F. Tan', 'Zhuoran Yang'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15141.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#math'], 'emoji': 'üöÄ', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: —É—Å–∫–æ—Ä–µ–Ω–∏–µ LLM —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Ä—É–∫–∏—Ö –±–∞–Ω–¥–∏—Ç–æ–≤', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –º–Ω–æ–≥–æ—Ä—É–∫–∏—Ö –±–∞–Ω–¥–∏—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ - UCBSpec –∏ EXP3Spec, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–µ–¥–µ–Ω —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –¥–æ–∫–∞–∑–∞–Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å UCBSpec. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö LLaMA3 –∏ Qwen2 –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Dynamic Hyperparameter Selection for Efficient Text Generation', 'desc': 'This paper introduces a new method called BanditSpec for improving the efficiency of Large Language Models (LLMs) during text generation. Instead of using a fixed approach for speculative decoding, it dynamically selects hyperparameters based on the context of the text being generated. The authors frame this selection process as a Multi-Armed Bandit problem and propose two algorithms, UCBSpec and EXP3Spec, to optimize performance. Their experiments show that these algorithms significantly enhance throughput and adapt well to various input prompts, achieving results close to the best possible configurations.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îË∂ÖÂèÇÊï∞ÈÄâÊã©ÔºåÂä†ÈÄüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂú®Á∫øÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁî®‰∫éËá™ÈÄÇÂ∫îÈÄâÊã©Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑË∂ÖÂèÇÊï∞ÈÖçÁΩÆÔºå‰ª•Âä†ÈÄüÊé®ÁêÜÈÄüÂ∫¶„ÄÇÊàë‰ª¨Â∞ÜË∂ÖÂèÇÊï∞ÈÄâÊã©ÈóÆÈ¢òÂª∫Ê®°‰∏∫Â§öËáÇËµåÂçöÊú∫ÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫ÜBanditSpecÊ°ÜÊû∂„ÄÇÊñá‰∏≠ËÆæËÆ°‰∫Ü‰∏§ÁßçÂü∫‰∫éËµåÂçöÊú∫ÁöÑË∂ÖÂèÇÊï∞ÈÄâÊã©ÁÆóÊ≥ïUCBSpecÂíåEXP3SpecÔºåÂπ∂ÂàÜÊûê‰∫ÜÂÆÉ‰ª¨Âú®ÈöèÊú∫ÂíåÂØπÊäóÂ•ñÂä±ËÆæÁΩÆ‰∏ãÁöÑÂÅúÊ≠¢Êó∂Èó¥ÈÅóÊÜæ„ÄÇÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåËøô‰∫õÁÆóÊ≥ïÂú®Â§ÑÁêÜÂ§öÊ†∑ÂåñËæìÂÖ•ÊèêÁ§∫Êó∂ÔºåËÉΩÂ§üÊúâÊïàÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåÊé•ËøëÊúÄ‰Ω≥Ë∂ÖÂèÇÊï∞ÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15134', 'title': 'The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.15134', 'abstract': "Entropy minimization improves large language models' performance on math, physics, and coding tasks without labeled data through various methods like fine-tuning, reinforcement learning, and inference-time adjustments.  \t\t\t\t\tAI-generated summary \t\t\t\t Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.", 'score': 0, 'issue_id': 3909, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'ab8fb9efb23df8c5', 'authors': ['Shivam Agarwal', 'Zimin Zhang', 'Lifan Yuan', 'Jiawei Han', 'Hao Peng'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.15134.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#inference', '#math', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é —ç–Ω—Ç—Ä–æ–ø–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, —Ñ–∏–∑–∏–∫–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞: —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–µ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ (EM-FT), –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (EM-RL) –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ª–æ–≥–∏—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ (EM-INF). –ú–µ—Ç–æ–¥ EM-RL –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö –∏–ª–∏ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ª–∏–Ω–∏—è–º–∏ RL, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ EM-INF –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ Qwen-32B —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–ª–æ–∂–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ SciCode.'}, 'en': {'title': 'Unlocking LLM Potential with Entropy Minimization', 'desc': 'This paper discusses how entropy minimization (EM) can enhance the performance of large language models (LLMs) on complex tasks in math, physics, and coding without the need for labeled data. The authors present three methods: EM-FT, which fine-tunes the model using unlabeled outputs; EM-RL, which employs reinforcement learning with negative entropy as a reward; and EM-INF, which adjusts outputs during inference to lower entropy. The results show that these methods can achieve performance levels comparable to or better than traditional approaches that rely on extensive labeled datasets. Overall, the study highlights the untapped reasoning abilities of pretrained LLMs that can be unlocked through simple entropy minimization techniques.'}, 'zh': {'title': 'ÁÜµÊúÄÂ∞èÂåñÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆ', 'desc': 'ÁÜµÊúÄÂ∞èÂåñÔºàEMÔºâÊòØ‰∏ÄÁßçËÆ≠ÁªÉÊñπÊ≥ïÔºåÂèØ‰ª•Âú®Ê≤°ÊúâÊ†áËÆ∞Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶„ÄÅÁâ©ÁêÜÂíåÁºñÁ®ã‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËÆ©Ê®°ÂûãÂ∞ÜÊõ¥Â§öÁöÑÊ¶ÇÁéáÈõÜ‰∏≠Âú®ÊúÄÊúâ‰ø°ÂøÉÁöÑËæìÂá∫‰∏äÔºåÊù•‰ºòÂåñÂÖ∂ÊÄßËÉΩ„ÄÇÁ†îÁ©∂‰∏≠ÊèêÂá∫‰∫Ü‰∏âÁßçÊñπÊ≥ïÔºöEM-FT„ÄÅEM-RLÂíåEM-INFÔºåÂàÜÂà´ÈÄöËøá‰∏çÂêåÁöÑÊñπÂºèÂáèÂ∞ëÁÜµ„ÄÇÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ÁÜµÊúÄÂ∞èÂåñÁöÑÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæÁ§∫Âá∫È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂèØ‰ª•Ë¢´ÊúâÊïàÊøÄÂèë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14990', 'title': 'Language Specific Knowledge: Do Models Know Better in X than in English?', 'url': 'https://huggingface.co/papers/2505.14990', 'abstract': 'Code-switching is a common phenomenon of alternating between different languages in the same utterance, thought, or conversation. We posit that humans code-switch because they feel more comfortable talking about certain topics and domains in one language than another. With the rise of knowledge-intensive language models, we ask ourselves the next, natural question: Could models hold more knowledge on some topics in some language X? More importantly, could we improve reasoning by changing the language that reasoning is performed in? We coin the term Language Specific Knowledge (LSK) to represent this phenomenon. As ethnic cultures tend to develop alongside different languages, we employ culture-specific datasets (that contain knowledge about cultural and social behavioral norms). We find that language models can perform better when using chain-of-thought reasoning in some languages other than English, sometimes even better in low-resource languages. Paired with previous works showing that semantic similarity does not equate to representational similarity, we hypothesize that culturally specific texts occur more abundantly in corresponding languages, enabling specific knowledge to occur only in specific "expert" languages. Motivated by our initial results, we design a simple methodology called LSKExtractor to benchmark the language-specific knowledge present in a language model and, then, exploit it during inference. We show our results on various models and datasets, showing an average relative improvement of 10% in accuracy. Our research contributes to the open-source development of language models that are inclusive and more aligned with the cultural and linguistic contexts in which they are deployed.', 'score': 0, 'issue_id': 3895, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '543b5a3ff8ff6a01', 'authors': ['Ishika Agarwal', 'Nimet Beyza Bozdag', 'Dilek Hakkani-T√ºr'], 'affiliations': ['Department of Computer Science University of Illinois, Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.14990.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#alignment', '#benchmark', '#open_source', '#inference', '#low_resource', '#multilingual'], 'emoji': 'üåê', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç —Ñ–µ–Ω–æ–º–µ–Ω —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π (LSK) –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –ª—É—á—à–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —è–∑—ã–∫–∞—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, –¥–∞–∂–µ –Ω–∞ –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é LSKExtractor –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 10% –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Unlocking Knowledge: Language Matters in Machine Learning!', 'desc': 'This paper explores the concept of Language Specific Knowledge (LSK), which suggests that language models may possess more knowledge about certain topics when reasoning in specific languages. The authors argue that humans often code-switch between languages based on comfort with cultural contexts, and this can be leveraged in machine learning. They introduce a methodology called LSKExtractor to evaluate and utilize this language-specific knowledge during model inference. Their findings indicate that language models can achieve improved reasoning accuracy, particularly in low-resource languages, by aligning with culturally relevant datasets.'}, 'zh': {'title': 'ËØ≠Ë®ÄÁâπÂÆöÁü•ËØÜÊèêÂçáÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ª£Á†ÅÂàáÊç¢Áé∞Ë±°ÔºåÂç≥Âú®Âêå‰∏ÄÂØπËØù‰∏≠‰∫§Êõø‰ΩøÁî®‰∏çÂêåËØ≠Ë®ÄÁöÑÊÉÖÂÜµ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫∫‰ª¨Âú®Êüê‰∫õËØùÈ¢ò‰∏äÊõ¥ÂÄæÂêë‰∫é‰ΩøÁî®ÁâπÂÆöËØ≠Ë®ÄÔºåÂõ†‰∏∫‰ªñ‰ª¨Âú®Ëøô‰∫õËØ≠Ë®Ä‰∏≠ÊÑüÂà∞Êõ¥ËàíÈÄÇ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËØ≠Ë®ÄÊ®°ÂûãÂú®Êüê‰∫õËØ≠Ë®Ä‰∏≠‰ΩøÁî®ÈìæÂºèÊé®ÁêÜÊó∂Ë°®Áé∞Êõ¥Â•ΩÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏≠„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂêç‰∏∫LSKExtractorÁöÑÊñπÊ≥ïÊù•ËØÑ‰º∞ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËØ≠Ë®ÄÁâπÂÆöÁü•ËØÜÔºåÂπ∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠Âä†‰ª•Âà©Áî®ÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÂπ≥Âùá10%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11080', 'title': 'BLEUBERI: BLEU is a surprisingly effective reward for instruction\n  following', 'url': 'https://huggingface.co/papers/2505.11080', 'abstract': 'Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.', 'score': 0, 'issue_id': 3910, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '34ff03409b67fa3d', 'authors': ['Yapei Chang', 'Yekyung Kim', 'Michael Krumdick', 'Amir Zadeh', 'Chuan Li', 'Chris Tanner', 'Mohit Iyyer'], 'affiliations': ['Kensho', 'Lambda AI', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2505.11080.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#rl', '#dataset', '#alignment', '#training', '#open_source', '#rlhf'], 'emoji': 'üîç', 'ru': {'title': 'BLEU –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –º–æ–¥–µ–ª—è–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BLEUBERI –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ç—Ä–∏–∫—É BLEU –≤–º–µ—Å—Ç–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ BLEU –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. BLEUBERI –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è BLEU –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'BLEUBERI: Simplifying LLM Alignment with BLEU Metrics', 'desc': 'This paper explores the use of simpler metrics, specifically BLEU, as alternatives to traditional reward models for aligning large language models (LLMs) with human preferences. The authors demonstrate that BLEU, a basic string-matching metric, can achieve similar performance to complex reward models when evaluating instruction-following tasks. They introduce BLEUBERI, a method that optimizes model performance using BLEU as a reward function, showing that it can compete with models trained using more sophisticated reward models. The findings suggest that with access to high-quality reference outputs, simpler metrics can effectively guide the alignment of LLMs, making the training process more efficient and cost-effective.'}, 'zh': {'title': 'BLEUBERIÔºöÁî®ÁÆÄÂçïÊåáÊ†áÊõø‰ª£Â•ñÂä±Ê®°ÂûãÁöÑÊúâÊïàÊñπÊ≥ï', 'desc': 'Â•ñÂä±Ê®°ÂûãÂú®Â∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩê‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®Ôºå‰ΩÜËÆ≠ÁªÉÊàêÊú¨È´òÔºåÈúÄË¶ÅÂ§ßÈáè‰∫∫Á±ªÊ†áÊ≥®ÁöÑÂÅèÂ•ΩÊï∞ÊçÆÂíåÂº∫Â§ßÁöÑÈ¢ÑËÆ≠ÁªÉLLMÂü∫Á°ÄÊ®°Âûã„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÁÆÄÂçïÁöÑÂü∫‰∫éÂèÇËÄÉÁöÑÊåáÊ†áÊòØÂê¶ÂèØ‰ª•‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÁöÑÊõø‰ª£ÊñπÊ°àÔºåÁªìÊûúÂèëÁé∞BLEUËøô‰∏ÄÂü∫Êú¨ÁöÑÂ≠óÁ¨¶‰∏≤ÂåπÈÖçÊåáÊ†áÂú®‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑ‰∏ÄËá¥ÊÄß‰∏äÔºåÁ´üÁÑ∂‰∏éÂº∫Â§ßÁöÑÂ•ñÂä±Ê®°ÂûãÁõ∏ÂåπÈÖç„ÄÇÂü∫‰∫éËøô‰∏ÄÂèëÁé∞ÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜBLEUBERIÊñπÊ≥ïÔºåÈ¶ñÂÖàËØÜÂà´ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊåá‰ª§ÔºåÁÑ∂Âêé‰ΩøÁî®BLEU‰Ωú‰∏∫Â•ñÂä±ÂáΩÊï∞ËøõË°åÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâ„ÄÇÂÆûÈ™åË°®ÊòéÔºåBLEUBERIËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§ö‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊåá‰ª§Ë∑üÈöèÂü∫ÂáÜÊµãËØï‰∏≠Ôºå‰∏éÈÄöËøáÂ•ñÂä±Ê®°ÂûãÊåáÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂÖ∑ÊúâÁ´û‰∫âÂäõÔºå‰∏îÁîüÊàêÁöÑËæìÂá∫Âú®‰∫ãÂÆûÂü∫Á°Ä‰∏äÊõ¥‰∏∫ÂèØÈù†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16938', 'title': 'NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification', 'url': 'https://huggingface.co/papers/2505.16938', 'abstract': 'Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.', 'score': 104, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '39a42fc40deae7a6', 'authors': ['NovelSeek Team', 'Bo Zhang', 'Shiyang Feng', 'Xiangchao Yan', 'Jiakang Yuan', 'Zhiyin Yu', 'Xiaohan He', 'Songtao Huang', 'Shaowei Hou', 'Zheng Nie', 'Zhilong Wang', 'Jinyao Liu', 'Runmin Ma', 'Tianshuo Peng', 'Peng Ye', 'Dongzhan Zhou', 'Shufei Zhang', 'Xiaosong Wang', 'Yilan Zhang', 'Meng Li', 'Zhongying Tu', 'Xiangyu Yue', 'Wangli Ouyang', 'Bowen Zhou', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.16938.jpg', 'data': {'categories': ['#healthcare', '#multimodal', '#agents', '#science'], 'emoji': 'üß¨', 'ru': {'title': 'NovelSeek: –ò–ò-—É—Å–∫–æ—Ä–∏—Ç–µ–ª—å –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π', 'desc': 'NovelSeek - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –û–Ω–∞ –æ–±–ª–∞–¥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å—é, –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ 12 –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–æ–∑–≤–æ–ª—è—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è. NovelSeek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã—Ö–æ–¥–∞ —Ä–µ–∞–∫—Ü–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—Ä–æ—Å–ª–∞ —Å 27.6% –¥–æ 35.4% –≤—Å–µ–≥–æ –∑–∞ 12 —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Revolutionizing Research with Autonomous AI Frameworks', 'desc': 'This paper presents NovelSeek, a multi-agent framework designed for Autonomous Scientific Research (ASR) that enhances the efficiency and innovation in scientific studies. It showcases three main benefits: scalability across various research tasks, interactivity with human experts for feedback, and improved efficiency in achieving results faster than traditional methods. NovelSeek has been tested on multiple scientific tasks, demonstrating significant performance improvements in areas like reaction yield prediction and semantic segmentation. By integrating AI with human expertise, NovelSeek aims to revolutionize how scientific research is conducted.'}, 'zh': {'title': 'NovelSeekÔºöÂä†ÈÄüÁßëÂ≠¶Á†îÁ©∂ÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫NovelSeekÁöÑÁªü‰∏ÄÈó≠ÁéØÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞Ëá™‰∏ªÁßëÂ≠¶Á†îÁ©∂ÔºàASRÔºâ„ÄÇËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™ÁßëÂ≠¶Á†îÁ©∂È¢ÜÂüü‰∏≠Â±ïÁé∞Âá∫ÂçìË∂äÁöÑÂèØÊâ©Â±ïÊÄß„ÄÅ‰∫§‰∫íÊÄßÂíåÊïàÁéáÔºåËÉΩÂ§ü‰ª•Á©∫ÂâçÁöÑÈÄüÂ∫¶ÂíåÁ≤æÂ∫¶Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢ò„ÄÇNovelSeekÈÄöËøá‰∏é‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑÂèçÈ¶àÂíåÂ§öÊô∫ËÉΩ‰ΩìÁöÑ‰∫íÂä®Ôºå‰øÉËøõ‰∫ÜÈ¢ÜÂüü‰∏ìÂÆ∂Áü•ËØÜÁöÑÊó†ÁºùÊï¥Âêà„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåNovelSeekÂú®ÂèçÂ∫î‰∫ßÁéáÈ¢ÑÊµã„ÄÅÂ¢ûÂº∫Â≠êÊ¥ªÊÄßÈ¢ÑÊµãÂíå2DËØ≠‰πâÂàÜÂâ≤Á≠â‰ªªÂä°‰∏≠ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåËäÇÁúÅ‰∫ÜÂ§ßÈáèÊó∂Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14810', 'title': 'Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models', 'url': 'https://huggingface.co/papers/2505.14810', 'abstract': 'An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.', 'score': 54, 'issue_id': 3914, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '97ed7c1fde734d7e', 'authors': ['Tingchen Fu', 'Jiawei Gu', 'Yafu Li', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.14810.jpg', 'data': {'categories': ['#math', '#reasoning', '#training', '#benchmark', '#optimization', '#alignment'], 'emoji': 'ü§ñ', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º –∏ –ø–æ—Å–ª—É—à–∞–Ω–∏–µ–º –≤ –ò–ò', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ MathIF –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É —É–ª—É—á—à–µ–Ω–∏–µ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á–∞—Å—Ç–æ —É—Ö—É–¥—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞—Ç—å —É–∫–∞–∑–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ü—Ä–æ—Å—Ç—ã–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º–æ–≥—É—Ç —á–∞—Å—Ç–∏—á–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Å–ª—É—à–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –Ω–æ –∑–∞ —Å—á–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ –≤—ã–≤–æ–¥—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –≤ —Ç–µ–∫—É—â–∏—Ö –ø–∞—Ä–∞–¥–∏–≥–º–∞—Ö –æ–±—É—á–µ–Ω–∏—è LLM –∏ –º–æ—Ç–∏–≤–∏—Ä—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –±–æ–ª–µ–µ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ —Å–ª–µ–¥—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø—Ä–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Balancing Reasoning and Instruction in Language Models', 'desc': "This paper presents MathIF, a benchmark designed to evaluate how well large language models (LLMs) follow instructions while solving mathematical problems. The authors find a conflict between improving reasoning abilities and maintaining adherence to user instructions, as models that excel in reasoning often fail to follow directives accurately. They observe that training methods like reinforcement learning can enhance reasoning but may reduce the model's ability to comply with instructions, especially as the complexity of tasks increases. The study suggests that addressing this tension is crucial for developing more effective instruction-aware reasoning models."}, 'zh': {'title': 'Âπ≥Ë°°Êé®ÁêÜËÉΩÂäõ‰∏éÊåá‰ª§ÈÅµÂæ™ÁöÑÊåëÊàò', 'desc': 'Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõ‰∏éÊé®ÁêÜËÉΩÂäõ‰πãÈó¥ÁöÑÁüõÁõæ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMathIFÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑÊåá‰ª§ÈÅµÂæ™Ë°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊé®ÁêÜËÉΩÂäõÊõ¥Âº∫ÁöÑÊ®°ÂûãÂæÄÂæÄÂú®ÈÅµÂæ™Áî®Êà∑Êåá‰ª§Êó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁîüÊàêÂÜÖÂÆπËæÉÈïøÊó∂„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÁöÑËÆ≠ÁªÉÊñπÊ≥ïÈúÄË¶ÅÊõ¥Â§öÂÖ≥Ê≥®Êåá‰ª§ÊÑèËØÜÔºå‰ª•Âπ≥Ë°°Êé®ÁêÜËÉΩÂäõÂíåÊåá‰ª§ÈÅµÂæ™„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16410', 'title': 'Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.16410', 'abstract': 'Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.', 'score': 47, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'acbe5c0b965cb7af', 'authors': ['Guanting Dong', 'Yifei Chen', 'Xiaoxi Li', 'Jiajie Jin', 'Hongjin Qian', 'Yutao Zhu', 'Hangyu Mao', 'Guorui Zhou', 'Zhicheng Dou', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'Kuaishou Technology', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16410.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#rl', '#optimization'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': 'Tool-Star: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—É–ª—å—Ç–∏–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'Tool-Star - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —à–µ—Å—Ç—å —Ç–∏–ø–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Å–∏–Ω—Ç–µ–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 10 —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Tool-Star.'}, 'en': {'title': 'Empowering LLMs with Multi-Tool Collaborative Reasoning', 'desc': "Tool-Star is a reinforcement learning (RL) framework that enhances large language models (LLMs) by enabling them to autonomously utilize multiple external tools for stepwise reasoning. It addresses the challenge of effective multi-tool collaboration by integrating a systematic approach to data synthesis and hierarchical reward design. The framework includes a novel data synthesis pipeline that generates tool-use trajectories and organizes them by difficulty, ensuring high-quality training data. Tool-Star's two-stage training process improves LLMs' reasoning capabilities through fine-tuning and a self-critic RL algorithm, leading to significant performance gains on various reasoning tasks."}, 'zh': {'title': 'Tool-StarÔºöËµãËÉΩLLMÁöÑÂ§öÂ∑•ÂÖ∑Âçè‰ΩúÊé®ÁêÜ', 'desc': 'Tool-StarÊòØ‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üËá™‰∏ª‰ΩøÁî®Â§ö‰∏™Â∑•ÂÖ∑ËøõË°åÈÄêÊ≠•Êé®ÁêÜ„ÄÇËØ•Ê°ÜÊû∂Êï¥Âêà‰∫ÜÂÖ≠ÁßçÂ∑•ÂÖ∑ÔºåÂπ∂Âú®Êï∞ÊçÆÂêàÊàêÂíåËÆ≠ÁªÉÊñπÈù¢ËøõË°å‰∫ÜÁ≥ªÁªüËÆæËÆ°Ôºå‰ª•Ëß£ÂÜ≥Â∑•ÂÖ∑‰ΩøÁî®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÁªìÂêàÂ∑•ÂÖ∑ÈõÜÊàêÊèêÁ§∫ÂíåÂü∫‰∫éÊèêÁ§∫ÁöÑÈááÊ†∑ÔºåTool-StarËÉΩÂ§üËá™Âä®ÁîüÊàêÂ∑•ÂÖ∑‰ΩøÁî®ËΩ®ËøπÔºåÂπ∂ÈÄöËøáË¥®ÈáèÊ†áÂáÜÂåñÂíåÈöæÂ∫¶ÊÑüÁü•ÂàÜÁ±ªÊù•ËøáÊª§‰ΩéË¥®ÈáèÊ†∑Êú¨„ÄÇÊúÄÂêéÔºåTool-StarÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂ¢ûÂº∫Â§öÂ∑•ÂÖ∑Âçè‰ΩúÊé®ÁêÜËÉΩÂäõÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÊûúÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15966', 'title': 'Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.15966', 'abstract': "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on TallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.", 'score': 43, 'issue_id': 3915, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'a1134b1247c14719', 'authors': ['Alex Su', 'Haozhe Wang', 'Weimin Ren', 'Fangzhen Lin', 'Wenhu Chen'], 'affiliations': ['HKUST', 'USTC', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15966.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#cv', '#rl', '#optimization', '#benchmark', '#open_source', '#training'], 'emoji': 'üîç', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ø–∏–∫—Å–µ–ª—è—Ö: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –ø—É—Ç–µ–º –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –∫–∞–¥—Ä–∞, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ –º–æ–≥–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –î–ª—è –æ–±—É—á–µ–Ω–∏—è —ç—Ç–∏–º –Ω–∞–≤—ã–∫–∞–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Enhancing VLMs with Pixel-Space Reasoning', 'desc': 'This paper introduces a new way for Vision-Language Models (VLMs) to reason about images by using visual operations like zooming in and selecting frames. Traditionally, reasoning in these models has been limited to text, which makes it hard for them to handle visual tasks effectively. The authors propose a two-phase training method that first teaches the model to use these new visual operations and then encourages it to explore both visual and textual reasoning through reinforcement learning. Their approach leads to significant improvements in VLM performance on various visual reasoning tasks, achieving record accuracy on several benchmarks.'}, 'zh': {'title': 'ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑË°®Áé∞', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏≠ÂºïÂÖ•ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáËßÜËßâÊìç‰ΩúÂ¶ÇÊîæÂ§ßÂíåÈÄâÊã©Â∏ßÊù•ÊèêÂçáÂÖ∂Âú®ËßÜËßâ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ‰º†ÁªüÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊú¨Á©∫Èó¥ÁöÑÊé®ÁêÜËÉΩÂäõÊòæËëóÊèêÈ´òÔºå‰ΩÜÂú®ËßÜËßâÂØÜÈõÜ‰ªªÂä°‰∏≠ÊïàÊûúÊúâÈôê„ÄÇÊàë‰ª¨ÈÄöËøá‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÊ®°ÂûãÂú®ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜ‰∏≠ÁöÑÊåëÊàòÔºåÈ¶ñÂÖàÈÄöËøáÊåá‰ª§Ë∞É‰ºòËÆ©Ê®°ÂûãÁÜüÊÇâÊñ∞ËßÜËßâÊìç‰ΩúÔºåÁÑ∂ÂêéÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Âπ≥Ë°°ÂÉèÁ¥†Á©∫Èó¥ÂíåÊñáÊú¨Á©∫Èó¥ÁöÑÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Â§ö‰∏™ËßÜËßâÊé®ÁêÜÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16707', 'title': 'KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models', 'url': 'https://huggingface.co/papers/2505.16707', 'abstract': 'Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a diagnostic benchmark designed to assess models through a cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose a comprehensive protocol that incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems.', 'score': 38, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'fa1902dc37796b16', 'authors': ['Yongliang Wu', 'Zonghui Li', 'Xinting Hu', 'Xinyu Ye', 'Xianfang Zeng', 'Gang Yu', 'Wenbo Zhu', 'Bernt Schiele', 'Ming-Hsuan Yang', 'Xu Yang'], 'affiliations': ['Max Planck Institute for Informatics', 'Shanghai Jiao Tong University', 'Southeast University', 'StepFun', 'University of California, Berkeley', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2505.16707.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': 'KRIS-Bench: –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç KRIS-Bench - –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –ë–µ–Ω—á–º–∞—Ä–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —Ç—Ä–µ–º —Ç–∏–ø–∞–º –∑–Ω–∞–Ω–∏–π: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º, –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–º –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ 22 —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ 7 –∞—Å–ø–µ–∫—Ç–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–æ–±—Ä–∞–ª–∏ 1267 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—é—â–∏–π –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'Advancing Image Editing with Knowledge-Based Reasoning', 'desc': "This paper presents KRIS-Bench, a new benchmark for evaluating multi-modal generative models in the context of instruction-based image editing. It focuses on assessing the models' ability to perform knowledge-based reasoning tasks, which has not been thoroughly investigated before. The benchmark categorizes editing tasks into three knowledge types: Factual, Conceptual, and Procedural, and includes 22 tasks with 1,267 annotated instances. The study reveals that current state-of-the-art models struggle with reasoning tasks, indicating a need for more knowledge-centric evaluation methods in image editing systems."}, 'zh': {'title': 'Áü•ËØÜÈ©±Âä®ÁöÑÂõæÂÉèÁºñËæëËØÑ‰º∞Êñ∞Âü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜKRIS-BenchÔºàÁü•ËØÜÂü∫Á°ÄÊé®ÁêÜÂú®ÂõæÂÉèÁºñËæëÁ≥ªÁªüÂü∫ÂáÜÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÁîüÊàêÊ®°ÂûãÂú®Áü•ËØÜÊé®ÁêÜÁºñËæë‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÁöÑÂü∫ÂáÜÊµãËØï„ÄÇKRIS-BenchÊ†πÊçÆÊïôËÇ≤ÁêÜËÆ∫Â∞ÜÁºñËæë‰ªªÂä°ÂàÜ‰∏∫‰∏âÁßçÂü∫Á°ÄÁü•ËØÜÁ±ªÂûãÔºö‰∫ãÂÆûÊÄß„ÄÅÊ¶ÇÂøµÊÄßÂíåÁ®ãÂ∫èÊÄßÔºåÂπ∂ËÆæËÆ°‰∫Ü22‰∏™‰ª£Ë°®ÊÄß‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªºÂêàËØÑ‰º∞ÂçèËÆÆÔºåÂåÖÂê´Êñ∞ÁöÑÁü•ËØÜÂêàÁêÜÊÄßÊåáÊ†áÔºåÂπ∂ÈÄöËøá‰∫∫Á±ªÁ†îÁ©∂ËøõË°åÊ†°ÂáÜ„ÄÇÂÆûËØÅÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Êé®ÁêÜÊÄßËÉΩ‰∏äÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÂº∫Ë∞É‰∫Ü‰ª•Áü•ËØÜ‰∏∫‰∏≠ÂøÉÁöÑÂü∫ÂáÜÊµãËØïÂú®Êô∫ËÉΩÂõæÂÉèÁºñËæëÁ≥ªÁªüÂèëÂ±ï‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16175', 'title': 'QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design', 'url': 'https://huggingface.co/papers/2505.16175', 'abstract': 'QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.', 'score': 34, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '0bcb5c833bad2340', 'authors': ['Benjamin Schneider', 'Dongfu Jiang', 'Chao Du', 'Tianyu Pang', 'Wenhu Chen'], 'affiliations': ['University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.16175.jpg', 'data': {'categories': ['#inference', '#optimization', '#video', '#long_context'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é QuickVideo', 'desc': 'QuickVideo - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞, —É—Å–∫–æ—Ä—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –≤–∏–¥–µ–æ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ –ø–∞–º—è—Ç–∏ –ø—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤—ã–≤–æ–¥–æ–º. QuickVideo –≤–∫–ª—é—á–∞–µ—Ç QuickDecoder –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –Ω–∞ CPU, QuickPrefill –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ GPU –∏ —Å—Ö–µ–º—É –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è CPU-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å GPU-–≤—ã–≤–æ–¥–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–æ—Ç—ã –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Accelerating Long-Video Understanding for Real-Time Applications', 'desc': 'QuickVideo is a novel system designed to enhance the understanding of long videos in real-time applications. It addresses two major challenges: the slow sequential video decoding and the high memory requirements for token prefilling in large language models (LLMs). By introducing a parallelized video decoder, a memory-efficient prefilling method, and an overlapping decoding scheme, QuickVideo significantly reduces inference time. This allows for efficient processing of long videos, making advanced video analysis accessible even on limited hardware.'}, 'zh': {'title': 'QuickVideoÔºöÂÆûÊó∂ÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÂä†ÈÄüÂà©Âô®', 'desc': 'QuickVideo ÊòØ‰∏ÄÁßçÂä†ÈÄüÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÁ≥ªÁªüÔºåÁªìÂêà‰∫ÜÂπ∂Ë°åËßÜÈ¢ëËß£Á†Å„ÄÅÂÜÖÂ≠òÈ´òÊïàÁöÑÈ¢ÑÂ°´ÂÖÖÂíåÈáçÂè†Ëß£Á†Å‰∏éÊé®ÁêÜ„ÄÇÂÆÉÈÄöËøáÂø´ÈÄüËß£Á†ÅÂô®Â∞ÜËßÜÈ¢ëÂàÜÂâ≤ÊàêÂÖ≥ÈîÆÂ∏ßÂØπÈΩêÁöÑÈó¥ÈöîÔºåÂπ∂ÂêåÊó∂Â§ÑÁêÜÔºå‰ªéËÄåÂÆûÁé∞‰∫Ü 2-3 ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇQuickPrefill ÊñπÊ≥ïÈÄöËøá KV-cache Ââ™ÊûùÂáèÂ∞ë‰∫ÜÂØπ GPU ÂÜÖÂ≠òÁöÑÈúÄÊ±ÇÔºå‰ΩøÂæóÂèØ‰ª•Â§ÑÁêÜÊõ¥Â§öÂ∏ß„ÄÇËØ•Á≥ªÁªüÊòæËëóÈôç‰Ωé‰∫ÜÈïøËßÜÈ¢ëËæìÂÖ•ÁöÑÊé®ÁêÜÊó∂Èó¥Ôºå‰ΩøÂæóÂú®ÊúâÈôêÁ°¨‰ª∂‰∏ä‰πüËÉΩÂÆûÁé∞È´òË¥®ÈáèÁöÑËßÜÈ¢ëÁêÜËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15270', 'title': 'Scaling Diffusion Transformers Efficiently via ŒºP', 'url': 'https://huggingface.co/papers/2505.15270', 'abstract': 'Maximal Update Parametrization (ŒºP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization (muP) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether muP of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard muP to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that muP of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing muP methodologies. Leveraging this result, we systematically demonstrate that DiT-muP enjoys robust HP transferability. Notably, DiT-XL-2-muP with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of muP on text-to-image generation by scaling PixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under muP outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-alpha and 3% of consumption by human experts for MMDiT-18B. These results establish muP as a principled and efficient framework for scaling diffusion Transformers.', 'score': 28, 'issue_id': 3914, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '6c13e0c5ef8ee4a2', 'authors': ['Chenyu Zheng', 'Xinyu Zhang', 'Rongzhen Wang', 'Wei Huang', 'Zhi Tian', 'Weilin Huang', 'Jun Zhu', 'Chongxuan Li'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'ByteDance Seed', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of AI, Renmin University of China', 'RIKEN AIP', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15270.jpg', 'data': {'categories': ['#architecture', '#training', '#transfer_learning', '#diffusion', '#optimization'], 'emoji': 'üî¨', 'ru': {'title': 'ŒºP: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Maximal Update Parametrization (ŒºP) –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ŒºP –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Ç–∞–∫–∏–º –∫–∞–∫ DiT, U-ViT, PixArt-alpha –∏ MMDiT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ŒºP –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –∏—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫—É. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ ŒºP —è–≤–ª—è–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Efficient Hyperparameter Transfer for Diffusion Transformers with ŒºP', 'desc': 'This paper extends the Maximal Update Parametrization (ŒºP) technique to diffusion Transformers, which are crucial for generative vision models. The authors demonstrate that ŒºP allows for effective hyperparameter transfer from smaller to larger models, significantly reducing the costs associated with hyperparameter tuning. Through extensive experiments, they show that diffusion Transformers like DiT and PixArt-alpha benefit from ŒºP, achieving faster convergence and better performance with minimal tuning effort. Overall, this work establishes ŒºP as a valuable method for enhancing the scalability and efficiency of diffusion Transformers in various tasks.'}, 'zh': {'title': 'ÊúÄÂ§ßÊõ¥Êñ∞ÂèÇÊï∞ÂåñÔºöÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÈ´òÊïàË∞É‰ºòÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊâ©Â±ï‰∫ÜÊúÄÂ§ßÊõ¥Êñ∞ÂèÇÊï∞ÂåñÔºàŒºPÔºâÂà∞Êâ©Êï£ÂèòÊç¢Âô®ÔºåÂ±ïÁ§∫‰∫ÜÈ´òÊïàÁöÑË∂ÖÂèÇÊï∞ÂèØËΩ¨ÁßªÊÄßÂíåÈôç‰ΩéÁöÑË∞É‰ºòÊàêÊú¨„ÄÇÊâ©Êï£ÂèòÊç¢Âô®Âú®ËßÜËßâÁîüÊàêÊ®°Âûã‰∏≠ÂèëÊå•‰∫ÜÂü∫Á°Ä‰ΩúÁî®Ôºå‰ΩÜÂú®Â§ßËßÑÊ®°Â∫îÁî®‰∏≠Ë∂ÖÂèÇÊï∞Ë∞É‰ºòÁöÑÈ´òÊàêÊú¨ÈôêÂà∂‰∫ÜÂÖ∂ÂèØÊâ©Â±ïÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåŒºPÂèØ‰ª•ÊúâÊïàÂú∞‰ªéÂ∞èÂûãÊ®°ÂûãËΩ¨ÁßªÂà∞Â§ßÂûãÊâ©Êï£ÂèòÊç¢Âô®ÔºåÂπ∂Âú®Â§ßËßÑÊ®°ÂÆûÈ™å‰∏≠È™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÈÄöËøáÁ≥ªÁªüÊÄßÂÆûÈ™åÔºåÁªìÊûúÊòæÁ§∫Âú®Ë∞É‰ºòÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®ŒºPÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17022', 'title': 'GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17022', 'abstract': 'GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.', 'score': 24, 'issue_id': 3917, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'c3983abb24fa0ce4', 'authors': ['Chengqi Duan', 'Rongyao Fang', 'Yuqing Wang', 'Kun Wang', 'Linjiang Huang', 'Xingyu Zeng', 'Hongsheng Li', 'Xihui Liu'], 'affiliations': ['Beihang University', 'CUHK MMLab', 'HKU MMLab', 'Sensetime'], 'pdf_title_img': 'assets/pdf/title_img/2505.17022.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#rl', '#multimodal', '#open_source', '#cv', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'GoT-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–æ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø–æ–¥—Ö–æ–¥–µ Generation Chain-of-Thought –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. GoT-R1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –º–Ω–æ–≥–æ–º–µ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, –æ—Ü–µ–Ω–∏–≤–∞—é—â—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Reinforcing Reasoning for Better Visual Generation', 'desc': 'GoT-R1 is a new framework that improves visual generation by using reinforcement learning to enhance how models understand and create images based on complex text prompts. It focuses on semantic-spatial reasoning, which means it helps models better grasp the meaning of words and how objects should be arranged in space. The framework uses a dual-stage reward system to evaluate both the reasoning process and the final image quality, ensuring that the generated images are accurate and visually appealing. Experimental results show that GoT-R1 outperforms existing models, especially in tasks that require detailed understanding of object relationships and attributes.'}, 'zh': {'title': 'GoT-R1ÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáËßÜËßâÁîüÊàêËÉΩÂäõ', 'desc': 'GoT-R1 ÊòØ‰∏Ä‰∏™ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Â¢ûÂº∫ËØ≠‰πâ-Á©∫Èó¥Êé®ÁêÜÁöÑËßÜËßâÁîüÊàêÊ°ÜÊû∂„ÄÇÂÆÉËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§çÊùÇÁöÑÊñáÊú¨ÊèêÁ§∫ÔºåÂ∞§ÂÖ∂ÊòØÊ∂âÂèäÂ§ö‰∏™ÂØπË±°ÂèäÂÖ∂Á≤æÁ°ÆÁ©∫Èó¥ÂÖ≥Á≥ªÁöÑ‰ªªÂä°„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®ÂèåÈò∂ÊÆµÂ§öÁª¥Â•ñÂä±Êú∫Âà∂ÔºåÂà©Áî®Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÊù•ËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÂíåÊúÄÁªàËæìÂá∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGoT-R1 Âú® T2I-CompBench Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®ÁªÑÂêà‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16933', 'title': 'LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2505.16933', 'abstract': 'A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.', 'score': 23, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '9ae7746da2ef9a2b', 'authors': ['Zebin You', 'Shen Nie', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Zhiwu Lu', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16933.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å LLaDA-V - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏. LLaDA-V –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –±–æ–ª–µ–µ —Å–ª–∞–±—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–Ω–∞–ª–æ–≥–∞–º–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ –∏ —á–∏—Å—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'LLaDA-V: Bridging Text and Vision with Diffusion Power!', 'desc': 'LLaDA-V is a new type of Multimodal Large Language Model that uses a diffusion-based approach combined with visual instruction tuning. This model integrates visual features into the language processing space, allowing it to understand and generate responses that involve both text and images. Despite being less effective on purely text tasks compared to some existing models, LLaDA-V performs well in multimodal scenarios and shows strong scalability with data. The results indicate that diffusion models can be highly effective for tasks that require understanding multiple types of information, paving the way for future research in this area.'}, 'zh': {'title': 'Êâ©Êï£Ê®°ÂûãÂºïÈ¢ÜÂ§öÊ®°ÊÄÅÁêÜËß£Êñ∞ÊΩÆÊµÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãLLaDA-VÔºåËØ•Ê®°ÂûãÁªìÂêà‰∫ÜËßÜËßâÊåá‰ª§Ë∞É‰ºòÔºåËÉΩÂ§üÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇLLaDA-VÈááÁî®‰∫ÜÊé©ËîΩÊâ©Êï£Ê®°ÂûãÔºåÁ™ÅÁ†¥‰∫ÜÂΩìÂâçÂ§öÊ®°ÊÄÅÊñπÊ≥ï‰∏≠‰∏ªÊµÅÁöÑËá™ÂõûÂΩíËåÉÂºè„ÄÇÂ∞ΩÁÆ°Âú®Á∫ØÊñáÊú¨‰ªªÂä°‰∏äË°®Áé∞‰∏çÂ¶Ç‰∏Ä‰∫õÁé∞ÊúâÊ®°ÂûãÔºå‰ΩÜÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÔºåLLaDA-V‰∏éÂÖ∂‰ªñÊ®°ÂûãÁõ∏ÊØîÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊï∞ÊçÆÂèØÊâ©Â±ïÊÄßÂíåÁ´û‰∫âÂäõ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÊâ©Êï£ÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÊñπÈù¢ÂÖ∑ÊúâÂæàÂ§ßÁöÑÊΩúÂäõÔºåÂÄºÂæóËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16925', 'title': 'Risk-Averse Reinforcement Learning with Itakura-Saito Loss', 'url': 'https://huggingface.co/papers/2505.16925', 'abstract': 'Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives.', 'score': 21, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '1233ea3eef980e02', 'authors': ['Igor Udovichenko', 'Olivier Croissant', 'Anita Toleutaeva', 'Evgeny Burnaev', 'Alexander Korotin'], 'affiliations': ['Artificial Intelligence Research Institute', 'Natixis Foundation', 'Skolkovo Institute of Science and Technology', 'Vega Institute Foundation'], 'pdf_title_img': 'assets/pdf/title_img/2505.16925.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#math', '#rl', '#training', '#games'], 'emoji': 'üìä', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–∏—Å–∫-–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Å–ª—É—á–∞–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –∏ –≤—ã–≤–æ–¥—è—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –ë–µ–ª–ª–º–∞–Ω–∞. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —á–∏—Å–ª–µ–Ω–Ω–æ–π –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –ò—Ç–∞–∫—É—Ä—ã-–°–∞–∏—Ç–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏.'}, 'en': {'title': 'Minimizing Risk in Reinforcement Learning with Stability', 'desc': 'This paper discusses risk-averse reinforcement learning, which is important in high-stakes situations where minimizing risk is crucial. Unlike traditional reinforcement learning that focuses on maximizing expected returns, risk-averse agents prioritize safer policies, sometimes at the cost of expected value. The authors specifically examine the exponential utility function to derive Bellman equations and adapt reinforcement learning algorithms accordingly. They propose a new loss function based on the Itakura-Saito divergence to improve numerical stability during training, demonstrating its effectiveness through theoretical analysis and empirical tests in financial scenarios.'}, 'zh': {'title': 'È£éÈô©ÂéåÊÅ∂Âº∫ÂåñÂ≠¶‰π†ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'È£éÈô©ÂéåÊÅ∂Âº∫ÂåñÂ≠¶‰π†Âú®È´òÈ£éÈô©È¢ÜÂüüÊúâÂπøÊ≥õÂ∫îÁî®„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†‰∏çÂêåÔºåÈ£éÈô©ÂéåÊÅ∂‰ª£ÁêÜÈÄâÊã©ÁöÑÁ≠ñÁï•ÊòØÊúÄÂ∞èÂåñÈ£éÈô©ÔºåÂèØËÉΩ‰ºöÁâ∫Áâ≤È¢ÑÊúüÊî∂Áõä„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÊåáÊï∞ÊïàÁî®ÂáΩÊï∞ÁöÑÁâπÂÆöÊÉÖÂÜµÔºåÊé®ÂØºÂá∫Ë¥ùÂ∞îÊõºÊñπÁ®ãÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äÂØπÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËøõË°åÂ∞ëÈáè‰øÆÊîπ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂú®ËÆ°ÁÆóËøáÁ®ã‰∏≠Áî±‰∫éÈúÄË¶ÅËøõË°åÊåáÊï∞ËøêÁÆóËÄåÂØºËá¥Êï∞ÂÄº‰∏çÁ®≥ÂÆöÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éItakura-SaitoÊï£Â∫¶ÁöÑÊï∞ÂÄºÁ®≥ÂÆö‰∏îÊï∞Â≠¶‰∏äÂêàÁêÜÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÂ≠¶‰π†Áä∂ÊÄÅÂÄºÂíåÂä®‰ΩúÂÄºÂáΩÊï∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16181', 'title': 'Understanding Generative AI Capabilities in Everyday Image Editing Tasks', 'url': 'https://huggingface.co/papers/2505.16181', 'abstract': 'Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io', 'score': 21, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '1d67f723fb5261aa', 'authors': ['Mohammad Reza Taesiri', 'Brandon Collins', 'Logan Bolton', 'Viet Dac Lai', 'Franck Dernoncourt', 'Trung Bui', 'Anh Totti Nguyen'], 'affiliations': ['Adobe Research', 'Auburn University', 'University of Alberta'], 'pdf_title_img': 'assets/pdf/title_img/2505.16181.jpg', 'data': {'categories': ['#multimodal', '#games', '#dataset', '#cv', '#optimization', '#interpretability'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ò–ò –≤ —Ñ–æ—Ç–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å vs —Ç–æ—á–Ω–æ—Å—Ç—å', 'desc': '–ê–Ω–∞–ª–∏–∑ 83 —Ç—ã—Å—è—á –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ò–ò-—Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –≤–∫–ª—é—á–∞—è GPT-4o, –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∏–∑–∫–æ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω—ã–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –Ω–æ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –¢–æ–ª—å–∫–æ 33% –∑–∞–ø—Ä–æ—Å–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –ª—É—á—à–∏–º–∏ –ò–ò-—Ä–µ–¥–∞–∫—Ç–æ—Ä–∞–º–∏ –ø–æ –æ—Ü–µ–Ω–∫–∞–º –ª—é–¥–µ–π. –ò–ò —á–∞—Å—Ç–æ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –ª—é–¥–µ–π –∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–µ–ª–∞–µ—Ç –Ω–µ–∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –æ—Ü–µ–Ω–æ–∫ –ª—é–¥–µ–π –∏ –º–æ–≥—É—Ç –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –ò–ò.'}, 'en': {'title': 'AI Editors: Better at Creativity, Struggling with Precision', 'desc': 'This paper analyzes 83,000 image editing requests to evaluate the performance of AI editors like GPT-4o. It finds that AI struggles with low-creativity tasks that require precise edits, achieving success in only about 33% of requests. In contrast, AI performs better on open-ended editing tasks, but often fails to maintain the identity of subjects and makes unwanted changes. Additionally, the preferences of visual language model (VLM) judges differ from human judges, with VLM judges showing a greater inclination towards AI-generated edits.'}, 'zh': {'title': 'AIÁºñËæëÂô®Âú®ÂàõÈÄ†ÊÄß‰ªªÂä°‰∏≠ÁöÑ‰ºòÂäø‰∏éÊåëÊàò', 'desc': 'Êú¨Á†îÁ©∂ÂàÜÊûê‰∫Ü83000‰∏™ÂõæÂÉèÁºñËæëËØ∑Ê±ÇÔºåÂèëÁé∞AIÁºñËæëÂô®ÔºàÂ¶ÇGPT-4oÔºâÂú®‰ΩéÂàõÈÄ†ÊÄß‰ªªÂä°ÂíåÁ≤æÁ°ÆÁºñËæëÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰ΩÜÂú®ÂºÄÊîæÊÄß‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥Â•Ω„ÄÇ‰∫∫Á±ªËØÑÂÆ°ÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËØÑÂÆ°Âú®ÂØπAI‰∏é‰∫∫Á±ªÁºñËæëÁöÑÂÅèÂ•Ω‰∏äÂ≠òÂú®Â∑ÆÂºÇ„ÄÇÂ§ßÁ∫¶Âè™Êúâ33%ÁöÑËØ∑Ê±ÇËÉΩÂ§üË¢´ÊúÄÂ•ΩÁöÑAIÁºñËæëÂô®Êª°Ë∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÁ≤æÁ°ÆÁºñËæëÁöÑÊÉÖÂÜµ‰∏ãÔºåAIÁºñËæëÂô®Â∏∏Â∏∏Êó†Ê≥ï‰øùÊåÅ‰∫∫Áâ©ÂíåÂä®Áâ©ÁöÑË∫´‰ªΩ„ÄÇÈÄöËøáÂØπËøô‰∫õËØ∑Ê±ÇÁöÑÂàÜÊûêÔºåÊàë‰ª¨ÂèØ‰ª•‰∏∫ÊîπËøõAIÁºñËæëÂô®Êèê‰æõÈáçË¶ÅÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16400', 'title': 'AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.16400', 'abstract': "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.", 'score': 20, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '5a3a63b67bb5b62a', 'authors': ['Yang Chen', 'Zhuolin Yang', 'Zihan Liu', 'Chankyu Lee', 'Peng Xu', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Wei Ping'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.16400.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#rl', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —É –Ω–µ–±–æ–ª—å—à–∏—Ö –∏ —Å—Ä–µ–¥–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∫–æ–¥–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∑–∞—Ç–µ–º –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –≤–∫–ª—é—á–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ curriculum learning —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Boosting Reasoning with Large-Scale Reinforcement Learning', 'desc': 'This paper explores how large-scale reinforcement learning (RL) can improve the reasoning abilities of small and mid-sized models more effectively than traditional distillation methods. The authors present a systematic study of the RL training process, highlighting a two-phase approach that first focuses on math prompts and then on code prompts. They demonstrate significant performance gains on both math and code benchmarks, showing that RL enhances the foundational reasoning capabilities of models while also enabling them to tackle previously unsolvable problems. Key insights include the importance of data curation and curriculum learning in optimizing the RL training process.'}, 'zh': {'title': 'Â§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊúâÊïàÊÄß', 'desc': 'Êú¨Á†îÁ©∂Ë°®ÊòéÔºåÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÊèêÂçáÂ∞èÂûãÂíå‰∏≠ÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÔºåÊØîËí∏È¶èÊñπÊ≥ïÊõ¥‰∏∫ÊúâÊïà„ÄÇÊàë‰ª¨ÈÄöËøáÁ≥ªÁªüÁöÑÂÆûÈ™åÁ†îÁ©∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºöÂÖàÂú®Êï∞Â≠¶ÊèêÁ§∫‰∏äËÆ≠ÁªÉÔºåÂÜçÂú®‰ª£Á†ÅÊèêÁ§∫‰∏äËÆ≠ÁªÉ„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊï∞Â≠¶Âº∫ÂåñÂ≠¶‰π†ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®Êï∞Â≠¶Âü∫ÂáÜÊµãËØïÂíå‰ª£Á†ÅÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Âº∫Â§ßÁöÑÊï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ãÔºå‰ª•Êî∂ÈõÜÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊèêÁ§∫ÂíåÈ´òË¥®ÈáèÁöÑÁ≠îÊ°àÔºå‰ªéËÄåÊîØÊåÅË∑®È¢ÜÂüüÁöÑÈ™åËØÅÂü∫Á°ÄÂº∫ÂåñÂ≠¶‰π†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14604', 'title': 'Let LLMs Break Free from Overthinking via Self-Braking Tuning', 'url': 'https://huggingface.co/papers/2505.14604', 'abstract': 'A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.', 'score': 20, 'issue_id': 3917, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '20e4c9c407be15de', 'authors': ['Haoran Zhao', 'Yuchen Yan', 'Yongliang Shen', 'Haolei Xu', 'Wenqi Zhang', 'Kaitao Song', 'Jian Shao', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Microsoft Research Asia', 'Tianjin University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14604.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—å –ò–ò: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞–∑–¥—É–º–∏–π', 'desc': "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Self-Braking Tuning –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –∏–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏–∑–±–µ–≥–∞—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è –∏ –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –¥–ª–∏–Ω–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º '—Ç–æ—Ä–º–æ–∑—è—â–∏—Ö' –ø–æ–¥—Å–∫–∞–∑–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 60% –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π."}, 'en': {'title': 'Empowering Models to Self-Regulate Reasoning for Efficiency', 'desc': 'The paper introduces a Self-Braking Tuning (SBT) framework designed to enhance large reasoning models (LRMs) by allowing them to self-regulate their reasoning processes. This approach addresses the issue of overthinking, which leads to unnecessary computational overhead and redundant reasoning steps. By developing metrics to identify overthinking and creating a braking prompt mechanism, the model learns to determine when to stop reasoning effectively. Experimental results show that SBT can reduce token usage by up to 60% while preserving accuracy, making it a more efficient solution for reasoning tasks.'}, 'zh': {'title': 'Ëá™ÊàëË∞ÉËäÇÔºå‰ºòÂåñÊé®ÁêÜÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™ÊàëÂà∂Âä®Ë∞É‰ºòÊ°ÜÊû∂ÔºàSelf-Braking Tuning, SBTÔºâÔºåÊó®Âú®ÂáèÂ∞ëÂ§ßÂûãÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑËøáÂ∫¶ÊÄùËÄÉÂíå‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÈÄöËøáÂÖÅËÆ∏Ê®°ÂûãËá™ÊàëË∞ÉËäÇÊé®ÁêÜËøáÁ®ãÔºåSBTÊ∂àÈô§‰∫ÜÂØπÂ§ñÈÉ®ÊéßÂà∂Êú∫Âà∂ÁöÑ‰æùËµñ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏ÄÂ•óÂü∫‰∫éÊ†áÂáÜÁ≠îÊ°àÁöÑËøáÂ∫¶ÊÄùËÄÉËØÜÂà´ÊåáÊ†áÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑÊñπÊ≥ïÊù•Ê£ÄÊµãÂÜó‰ΩôÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰øùÊåÅÁõ∏‰ººÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÂ∞Ü‰ª§ÁâåÊ∂àËÄóÂáèÂ∞ëÂ§öËææ60%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14684', 'title': 'Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning', 'url': 'https://huggingface.co/papers/2505.14684', 'abstract': 'A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.', 'score': 19, 'issue_id': 3918, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'bcaa1385d7242117', 'authors': ['Haolei Xu', 'Yuchen Yan', 'Yongliang Shen', 'Wenqi Zhang', 'Guiyang Hou', 'Shengpei Jiang', 'Kaitao Song', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Microsoft Research Asia', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14684.jpg', 'data': {'categories': ['#math', '#training', '#dataset', '#reasoning', '#optimization', '#data'], 'emoji': 'üß†', 'ru': {'title': '–ú–æ—Å—Ç —á–µ—Ä–µ–∑ –ø—Ä–æ–ø–∞—Å—Ç—å –≤ —Ü–µ–ø–æ—á–∫–µ –º—ã—Å–ª–µ–π: —É–ª—É—á—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π (Chain-of-Thought, CoT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∑–∞–¥–∞—á—É CoT Thought Leap Bridge –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ScaleQM+ –∏ –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å CoT-Bridge –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–ª–Ω–æ—Ç—ã –∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging Thought Leaps for Enhanced Reasoning in AI', 'desc': 'This paper introduces a model designed to detect and fill in missing steps in Chain-of-Thought (CoT) reasoning for mathematical tasks. The authors identify a problem where existing datasets have gaps, known as Thought Leaps, which hinder the learning process of large language models (LLMs). To address this, they propose the CoT Thought Leap Bridge Task and create a new training dataset called ScaleQM+ to help models learn to generate the missing reasoning steps. Their experiments show that models trained with this approach significantly outperform those trained on incomplete datasets, leading to better performance in both mathematical and logical reasoning tasks.'}, 'zh': {'title': 'Â°´Ë°•ÊÄùÁª¥Ë∑≥Ë∑ÉÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ®°ÂûãÔºåÁî®‰∫éÊ£ÄÊµãÂíåÁîüÊàêÊï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑÁº∫Â§±‰∏≠Èó¥Ê≠•È™§Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÂú®Êï∞Â≠¶ÂíåÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÊï∞Â≠¶ÈìæÂºèÊé®ÁêÜÊï∞ÊçÆÈõÜÂ∏∏Â∏∏Âõ†‰∏∫‰∏ìÂÆ∂ÁúÅÁï•‰∏≠Èó¥Ê≠•È™§ËÄåÂØºËá¥ÊÄùÁª¥Ë∑≥Ë∑ÉÔºåËøôÂØπÊ®°ÂûãÁöÑÂ≠¶‰π†ÂíåÊ≥õÂåñ‰∫ßÁîüË¥üÈù¢ÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÈìæÂºèÊé®ÁêÜÊÄùÁª¥Ë∑≥Ë∑ÉÊ°•Êé•‰ªªÂä°ÔºåÊó®Âú®Ëá™Âä®Ê£ÄÊµãÊÄùÁª¥Ë∑≥Ë∑ÉÂπ∂ÁîüÊàêÁº∫Â§±ÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§Ôºå‰ª•ÊÅ¢Â§çÊé®ÁêÜÁöÑÂÆåÊï¥ÊÄßÂíåËøûË¥ØÊÄß„ÄÇÈÄöËøáÊûÑÂª∫‰∏ìÈó®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜScaleQM+Âπ∂ËøõË°åÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂú®Ê°•Êé•Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉÁöÑÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÂéüÂßãÊï∞ÊçÆÈõÜÔºå‰∏îÂú®ÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏ä‰πüÊòæÁ§∫Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16990', 'title': 'Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding', 'url': 'https://huggingface.co/papers/2505.16990', 'abstract': "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only text{response length}{3}. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimple's capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at https://github.com/yu-rp/Dimple.", 'score': 18, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '5c0636fbc17936b5', 'authors': ['Runpeng Yu', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.16990.jpg', 'data': {'categories': ['#multimodal', '#inference', '#diffusion', '#optimization', '#open_source', '#architecture', '#training'], 'emoji': 'üß†', 'ru': {'title': 'Dimple: –î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'Dimple - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (DMLLM). –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –∞–≤—Ç–æ—Ä–µ√£—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–æ—Ä–æ–≤.'}, 'en': {'title': 'Dimple: A New Era in Efficient Language Modeling', 'desc': "Dimple is a new type of language model called a Discrete Diffusion Multimodal Large Language Model (DMLLM) that combines two training methods to improve performance and stability. It starts with an autoregressive training phase, which helps to stabilize the learning process, followed by a diffusion phase that enhances the model's capabilities. Dimple-7B outperforms existing models like LLaVA-NEXT by 3.9%, showing that it can compete with traditional autoregressive models. Additionally, it introduces a confident decoding strategy that reduces the number of iterations needed for generating responses, making it faster and more efficient while allowing for better control over the output format and length."}, 'zh': {'title': 'DimpleÔºöÈ´òÊïàÁöÑÁ¶ªÊï£Êâ©Êï£Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫ÜDimpleÔºå‰∏Ä‰∏™Á¶ªÊï£Êâ©Êï£Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàDMLLMÔºâÔºåÈÄöËøáÊ∑∑ÂêàËÆ≠ÁªÉÊñπÊ≥ïÂÆûÁé∞‰∫Ü‰∏éËá™ÂõûÂΩíÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂçïÁ∫ØÁöÑÁ¶ªÊï£Êâ©Êï£ËÆ≠ÁªÉÊñπÊ≥ï‰ºöÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÅÊÄßËÉΩ‰∏ç‰Ω≥Âíå‰∏•ÈáçÁöÑÈïøÂ∫¶ÂÅèÂ∑Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉËåÉÂºèÔºåÁªìÂêà‰∫ÜÂàùÂßãÁöÑËá™ÂõûÂΩíÈò∂ÊÆµÂíåÂêéÁª≠ÁöÑÊâ©Êï£Èò∂ÊÆµ„ÄÇDimpleÊ®°ÂûãÂú®Êé®ÁêÜÊïàÁéá‰∏ä‰πüÊúâÊâÄÊèêÂçáÔºåÈááÁî®‰∫ÜÂä®ÊÄÅË∞ÉÊï¥ÁîüÊàê‰ª§ÁâåÊï∞ÈáèÁöÑËá™‰ø°Ëß£Á†ÅÁ≠ñÁï•ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÁîüÊàêËø≠‰ª£Ê¨°Êï∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15952', 'title': 'VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance', 'url': 'https://huggingface.co/papers/2505.15952', 'abstract': "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/", 'score': 18, 'issue_id': 3915, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'f9307168ac97ad24', 'authors': ['Mohammad Reza Taesiri', 'Abhijay Ghildyal', 'Saman Zadtootaghaj', 'Nabajeet Barman', 'Cor-Paul Bezemer'], 'affiliations': ['Sony Interactive Entertainment, Aliso Viejo, US', 'Sony Interactive Entertainment, Berlin, Germany', 'Sony Interactive Entertainment, London, UK', 'University of Alberta, CA'], 'pdf_title_img': 'assets/pdf/title_img/2505.15952.jpg', 'data': {'categories': ['#games', '#cv', '#video', '#optimization', '#benchmark'], 'emoji': 'üéÆ', 'ru': {'title': 'VideoGameQA-Bench: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ–∏–≥—Ä', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VideoGameQA-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ–∏–≥—Ä. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º–æ–¥—É–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–π, –ø–æ–∏—Å–∫ –∏–≥–ª –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–ª–∏—Ç—á–µ–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–æ–≤ –æ–± –æ—à–∏–±–∫–∞—Ö. –û–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏–≥—Ä. VideoGameQA-Bench –ø—Ä–∏–∑–≤–∞–Ω –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–±–µ–ª –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –∫ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –≤–∏–¥–µ–æ–∏–≥—Ä.'}, 'en': {'title': 'Revolutionizing Game QA with VideoGameQA-Bench', 'desc': 'The paper presents VideoGameQA-Bench, a new benchmark designed to evaluate Vision-Language Models (VLMs) specifically for video game quality assurance tasks. It addresses the need for standardized assessments in a field where existing benchmarks do not meet the unique challenges of game development. By focusing on various QA activities such as visual unit testing and glitch detection, this benchmark aims to enhance the automation of quality assurance processes in the gaming industry. The introduction of VideoGameQA-Bench is a significant step towards improving the efficiency and effectiveness of game development workflows.'}, 'zh': {'title': 'ÊèêÂçáÊ∏∏ÊàèÂºÄÂèëË¥®ÈáèÁöÑÊô∫ËÉΩËØÑ‰º∞Â∑•ÂÖ∑', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫VideoGameQA-BenchÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜÈ¢ëÊ∏∏ÊàèË¥®Èáè‰øùËØÅ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÈöèÁùÄËßÜÈ¢ëÊ∏∏ÊàèÂú®Â®±‰πêË°å‰∏ö‰∏≠‰∫ßÁîüÁöÑÊî∂ÂÖ•ÊúÄÈ´òÔºå‰ºòÂåñÊ∏∏ÊàèÂºÄÂèëÊµÅÁ®ãÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúÄÊñ∞ËøõÂ±ï‰∏∫Ëá™Âä®ÂåñÂíåÊèêÂçáÊ∏∏ÊàèÂºÄÂèëÁöÑÂêÑ‰∏™ÊñπÈù¢Êèê‰æõ‰∫ÜÂ∑®Â§ßÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä≥Âä®ÂØÜÈõÜÂûãÁöÑË¥®Èáè‰øùËØÅÁéØËäÇ„ÄÇ‰∏∫‰∫ÜÂáÜÁ°ÆËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÊ∏∏ÊàèQAÊ¥ªÂä®ÔºåÂåÖÊã¨ËßÜËßâÂçïÂÖÉÊµãËØï„ÄÅËßÜËßâÂõûÂΩíÊµãËØï„ÄÅÊïÖÈöúÊ£ÄÊµãÁ≠â„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16967', 'title': 'Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval', 'url': 'https://huggingface.co/papers/2505.16967', 'abstract': 'Using cascading LLM prompts to identify and relabel false negatives in datasets improves retrieval and reranking models\' performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness -- pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35times and increases nDCG@10 on BEIR by 1.0 point. This motivates a deeper examination of training data quality, with a particular focus on "false negatives", where relevant passages are incorrectly labeled as irrelevant. We propose a simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o-mini.', 'score': 17, 'issue_id': 3931, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'd40b350aefa1642b', 'authors': ['Nandan Thakur', 'Crystina Zhang', 'Xueguang Ma', 'Jimmy Lin'], 'affiliations': ['David R. Cheriton School of Computer Science, University of Waterloo, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.16967.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ —É–º–Ω—É—é –ø–µ—Ä–µ—Ä–∞–∑–º–µ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—è –∫–∞—Å–∫–∞–¥–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∞–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –∏ –ø–µ—Ä–µ—Ä–∞–∑–º–µ—á–∞—é—Ç –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–∞–∫ –º–æ–¥–µ–ª–µ–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ (E5, Qwen2.5-7B), —Ç–∞–∫ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–∞ (Qwen2.5-3B) –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö BEIR –∏ AIR-Bench. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–º —Å–æ–≥–ª–∞—Å–∏–µ–º —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π.'}, 'en': {'title': 'Enhancing Model Performance by Relabeling False Negatives', 'desc': 'This paper discusses improving the performance of retrieval and reranking models by addressing the issue of false negatives in training datasets. The authors demonstrate that by pruning ineffective datasets and focusing on relabeling incorrectly labeled relevant passages, they can enhance model effectiveness significantly. They introduce a method using cascading LLM prompts to identify and correct these false negatives, leading to measurable improvements in nDCG scores on benchmark evaluations. The results indicate that better training data quality directly correlates with enhanced model performance, showcasing the importance of accurate labeling in machine learning.'}, 'zh': {'title': 'ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆÔºöÈáçÊñ∞Ê†áËÆ∞ÈîôËØØË¥üÊ†∑Êú¨', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Á∫ßËÅîÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊèêÁ§∫Êù•ËØÜÂà´ÂíåÈáçÊñ∞Ê†áËÆ∞Êï∞ÊçÆÈõÜ‰∏≠ÈîôËØØÊ†áËÆ∞ÁöÑË¥üÊ†∑Êú¨Ôºå‰ªéËÄåÊèêÈ´òÊ£ÄÁ¥¢ÂíåÈáçÊéíÂ∫èÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊüê‰∫õÊï∞ÊçÆÈõÜ‰ºöÂØπÊ®°ÂûãÊïàÊûú‰∫ßÁîüË¥üÈù¢ÂΩ±ÂìçÔºåÂéªÈô§‰∏çÂøÖË¶ÅÁöÑÊï∞ÊçÆÈõÜÂèØ‰ª•ÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰∏îÁªèÊµéÁöÑÊñπÊ≥ïÔºåÈÄöËøáÁ∫ßËÅîÊèêÁ§∫Êù•ËØÜÂà´ÂíåÈáçÊñ∞Ê†áËÆ∞Ëøô‰∫õÈöæ‰ª•ËØÜÂà´ÁöÑË¥üÊ†∑Êú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈáçÊñ∞Ê†áËÆ∞ÂêéÁöÑÊï∞ÊçÆÊòæËëóÊèêÂçá‰∫ÜÊ£ÄÁ¥¢Ê®°ÂûãÁöÑÊïàÊûúÔºåÈ™åËØÅ‰∫ÜÊï∞ÊçÆË¥®ÈáèÂØπÊ®°ÂûãËÆ≠ÁªÉÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16864', 'title': 'Training-Free Efficient Video Generation via Dynamic Token Carving', 'url': 'https://huggingface.co/papers/2505.16864', 'abstract': 'Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83times speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga', 'score': 17, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'ec67c352f5ae81d7', 'authors': ['Yuechen Zhang', 'Jinbo Xing', 'Bin Xia', 'Shaoteng Liu', 'Bohao Peng', 'Xin Tao', 'Pengfei Wan', 'Eric Lo', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'Kuaishou Technology', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2505.16864.jpg', 'data': {'categories': ['#video', '#inference', '#diffusion', '#optimization'], 'emoji': 'üß©', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': 'Jenga - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–µ–∑–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. Jenga –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–ª–æ—á–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é 3D-–∫—Ä–∏–≤—ã—Ö, –∑–∞–ø–æ–ª–Ω—è—é—â–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Jenga –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Jenga: Speeding Up Video Generation with Smart Attention!', 'desc': 'The paper introduces Jenga, an innovative inference pipeline designed for video Diffusion Transformer models that enhances video generation speed while preserving quality. It tackles the computational inefficiencies caused by the quadratic complexity of self-attention and the multi-step nature of diffusion processes. Jenga employs dynamic attention carving to selectively focus on relevant token interactions and utilizes progressive resolution generation to optimize the use of high-resolution latents. Experimental results show that Jenga can significantly accelerate video generation, achieving up to 8.83 times faster performance with minimal quality loss, making it a practical solution for real-time applications.'}, 'zh': {'title': 'JengaÔºöÂä†ÈÄüËßÜÈ¢ëÁîüÊàêÁöÑÂàõÊñ∞Êé®ÁêÜÁÆ°ÈÅì', 'desc': 'JengaÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®Ê®°ÂûãÊé®ÁêÜÁÆ°ÈÅìÔºåÁªìÂêà‰∫ÜÂä®ÊÄÅÊ≥®ÊÑèÂäõÂàáÂâ≤ÂíåÊ∏êËøõÂàÜËæ®ÁéáÁîüÊàêÔºåÊòæËëóÂä†Âø´‰∫ÜËßÜÈ¢ëÁîüÊàêÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅÈ´òË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜËá™Ê≥®ÊÑèÂäõÁöÑÂπ≥ÊñπÂ§çÊùÇÂ∫¶ÂíåÊâ©Êï£Ê®°ÂûãÁöÑÂ§öÊ≠•È™§ÁâπÊÄßÂ∏¶Êù•ÁöÑËÆ°ÁÆóÊïàÁéáÈóÆÈ¢ò„ÄÇJengaÈÄöËøáÂä®ÊÄÅÈÄâÊã©Áõ∏ÂÖ≥ÁöÑÊ†áËÆ∞‰∫§‰∫íÂíåÈÄêÊ≠•ÊèêÈ´òÊΩúÂú®ÂàÜËæ®ÁéáÔºå‰ºòÂåñ‰∫ÜÁîüÊàêËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåJengaÂú®Â§ö‰∏™ÊúÄÂÖàËøõÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÔºåÂêåÊó∂ÁîüÊàêË¥®Èáè‰øùÊåÅÁõ∏ÂΩì„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16916', 'title': 'Backdoor Cleaning without External Guidance in MLLM Fine-tuning', 'url': 'https://huggingface.co/papers/2505.16916', 'abstract': "Multimodal Large Language Models (MLLMs) are increasingly deployed in fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt general-purpose models to downstream tasks. This flexibility, however, introduces serious security risks, as malicious fine-tuning can implant backdoors into MLLMs with minimal effort. In this paper, we observe that backdoor triggers systematically disrupt cross-modal processing by causing abnormal attention concentration on non-semantic regions--a phenomenon we term attention collapse. Based on this insight, we propose Believe Your Eyes (BYE), a data filtering framework that leverages attention entropy patterns as self-supervised signals to identify and filter backdoor samples. BYE operates via a three-stage pipeline: (1) extracting attention maps using the fine-tuned model, (2) computing entropy scores and profiling sensitive layers via bimodal separation, and (3) performing unsupervised clustering to remove suspicious samples. Unlike prior defenses, BYE equires no clean supervision, auxiliary labels, or model modifications. Extensive experiments across various datasets, models, and diverse trigger types validate BYE's effectiveness: it achieves near-zero attack success rates while maintaining clean-task performance, offering a robust and generalizable solution against backdoor threats in MLLMs.", 'score': 15, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '36dbc47d484f0a53', 'authors': ['Xuankun Rong', 'Wenke Huang', 'Jian Liang', 'Jinhe Bi', 'Xun Xiao', 'Yiming Li', 'Bo Du', 'Mang Ye'], 'affiliations': ['Huawei Technologies', 'Munich Research Center', 'Nanyang Technological University', 'School of Computer Science, Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16916.jpg', 'data': {'categories': ['#multimodal', '#data', '#training', '#dataset', '#security'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ó–∞—â–∏—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –æ—Ç —Å–∫—Ä—ã—Ç—ã—Ö —É–≥—Ä–æ–∑: –¥–æ–≤–µ—Ä—è–π —Å–≤–æ–∏–º –≥–ª–∞–∑–∞–º', 'desc': "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã –≤—ã–∑—ã–≤–∞—é—Ç –∞–Ω–æ–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –Ω–µ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –Ω–∞–∑–≤–∞–≤ —ç—Ç–æ —è–≤–ª–µ–Ω–∏–µ '–∫–æ–ª–ª–∞–ø—Å–æ–º –≤–Ω–∏–º–∞–Ω–∏—è'. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ BYE –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ backdoor-–æ–±—Ä–∞–∑—Ü–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è. BYE —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∑–∞—â–∏—Ç—É –æ—Ç backdoor-–∞—Ç–∞–∫ –Ω–∞ MLLM."}, 'en': {'title': 'Defending MLLMs: Believe Your Eyes Against Backdoors!', 'desc': "This paper addresses the security risks associated with Multimodal Large Language Models (MLLMs) in fine-tuning-as-a-service (FTaaS) environments, where malicious fine-tuning can introduce backdoors. The authors identify a phenomenon called 'attention collapse', where backdoor triggers cause abnormal attention focus on irrelevant areas, disrupting cross-modal processing. To combat this, they propose a framework called Believe Your Eyes (BYE), which uses attention entropy patterns to filter out backdoor samples without needing clean supervision or model changes. BYE demonstrates strong effectiveness in various scenarios, achieving low attack success rates while preserving performance on clean tasks."}, 'zh': {'title': 'ÊäµÂæ°ÂêéÈó®Â®ÅËÉÅÁöÑÂàõÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂæÆË∞ÉÊúçÂä°‰∏≠Ë∂äÊù•Ë∂äÂ∏∏ËßÅÔºå‰ΩÜËøô‰πüÂ∏¶Êù•‰∫ÜÂÆâÂÖ®È£éÈô©ÔºåÊÅ∂ÊÑèÂæÆË∞ÉÂèØËÉΩ‰ºöÂú®Ê®°Âûã‰∏≠Ê§çÂÖ•ÂêéÈó®„ÄÇÊú¨ÊñáËßÇÂØüÂà∞ÔºåÂêéÈó®Ëß¶ÂèëÂô®‰ºöÂØºËá¥Ë∑®Ê®°ÊÄÅÂ§ÑÁêÜÁöÑÂºÇÂ∏∏Ê≥®ÊÑèÂäõÈõÜ‰∏≠ÔºåÂΩ¢ÊàêÊàë‰ª¨Áß∞‰πã‰∏∫Ê≥®ÊÑèÂäõÂ¥©Ê∫ÉÁöÑÁé∞Ë±°„ÄÇÂü∫‰∫éËøô‰∏ÄÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü"Áõ∏‰ø°‰Ω†ÁöÑÁúºÁùõ"ÔºàBYEÔºâÊï∞ÊçÆËøáÊª§Ê°ÜÊû∂ÔºåÈÄöËøáÊ≥®ÊÑèÂäõÁÜµÊ®°Âºè‰Ωú‰∏∫Ëá™ÁõëÁù£‰ø°Âè∑Êù•ËØÜÂà´ÂíåËøáÊª§ÂêéÈó®Ê†∑Êú¨„ÄÇBYEÈÄöËøá‰∏â‰∏™Èò∂ÊÆµÁöÑÊµÅÁ®ãÊìç‰ΩúÔºåËÉΩÂ§üÂú®‰∏çÈúÄË¶ÅÂπ≤ÂáÄÁõëÁù£ÊàñÊ®°Âûã‰øÆÊîπÁöÑÊÉÖÂÜµ‰∏ãÔºåÊúâÊïàÂú∞ÊäµÂæ°MLLMs‰∏≠ÁöÑÂêéÈó®Â®ÅËÉÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16421', 'title': 'WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.16421', 'abstract': 'WebAgent-R1 is an RL framework for training web agents in multi-turn interactions, achieving high success rates compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.', 'score': 13, 'issue_id': 3928, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '9b6b6b9f01bfb9c0', 'authors': ['Zhepei Wei', 'Wenlin Yao', 'Yao Liu', 'Weizhi Zhang', 'Qin Lu', 'Liang Qiu', 'Changlong Yu', 'Puyang Xu', 'Chao Zhang', 'Bing Yin', 'Hyokun Yun', 'Lihong Li'], 'affiliations': ['Amazon', 'Georgia Institute of Technology', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.16421.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#rl', '#benchmark', '#games', '#training'], 'emoji': 'üï∏Ô∏è', 'ru': {'title': 'WebAgent-R1: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ RL', 'desc': 'WebAgent-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö. –û–Ω –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –æ–Ω–ª–∞–π–Ω-–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –≤–µ–±-—Å—Ä–µ–¥–∞–º–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É—è—Å—å –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å–ø–µ—Ö–∞ –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–µ–±-–∑–∞–¥–∞—á.'}, 'en': {'title': 'Empowering Web Agents with Multi-Turn Reinforcement Learning', 'desc': 'WebAgent-R1 is a reinforcement learning (RL) framework designed to train web agents for multi-turn interactions, which are more complex than single-turn tasks. It operates by learning from real-time interactions with web environments, using binary rewards to guide the training process based on task success. The framework has shown significant improvements in task success rates for large language models, outperforming existing methods and proprietary models. Additionally, it explores different initialization strategies to enhance the training process, emphasizing the role of behavior cloning and chain-of-thought reasoning in achieving better performance.'}, 'zh': {'title': 'WebAgent-R1ÔºöÂ§öËΩÆ‰∫§‰∫íÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞Ê°ÜÊû∂', 'desc': 'WebAgent-R1ÊòØ‰∏Ä‰∏™Áî®‰∫éËÆ≠ÁªÉÁΩëÁªú‰ª£ÁêÜÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÂ§öËΩÆ‰∫§‰∫íÔºåÊàêÂäüÁéáÊòæËëóÈ´ò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïËΩÆ‰ªªÂä°‰∏çÂêåÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇÁöÑÈïøÊúüÂÜ≥Á≠ñÈóÆÈ¢òÔºåÈÄöËøáÂú®Á∫ø‰∫§‰∫íÂ≠¶‰π†Âπ∂ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËΩ®Ëøπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWebAgent-R1Âú®WebArena-LiteÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫Ü‰ªªÂä°ÊàêÂäüÁéáÔºåË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇËØ•Á†îÁ©∂ËøòÊé¢ËÆ®‰∫Ü‰∏çÂêåÁöÑÂº∫ÂåñÂ≠¶‰π†ÂàùÂßãÂåñÁ≠ñÁï•ÔºåÂº∫Ë∞É‰∫ÜÁÉ≠Ë∫´ËÆ≠ÁªÉÈò∂ÊÆµÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂Êèê‰æõ‰∫ÜÂú®ÁΩëÁªú‰ª£ÁêÜ‰∏≠ËûçÂÖ•ÈïøÈìæÊÄùÁª¥Êé®ÁêÜÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17018', 'title': 'SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward', 'url': 'https://huggingface.co/papers/2505.17018', 'abstract': 'An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1.', 'score': 12, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'a49e142829760753', 'authors': ['Kaixuan Fan', 'Kaituo Feng', 'Haoming Lyu', 'Dongzhan Zhou', 'Xiangyu Yue'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.17018.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rlhf', '#rl', '#optimization', '#benchmark', '#open_source', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –ò–ò —á–µ—Ä–µ–∑ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å SophiaVL-R1, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±–æ–±—â–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ Trust-GRPO, –∫–æ—Ç–æ—Ä—ã–π –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∑–∞ –º—ã—à–ª–µ–Ω–∏–µ. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ç–∂–∏–≥–∞, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞—é—â–∞—è –≤–ª–∏—è–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∑–∞ –º—ã—à–ª–µ–Ω–∏–µ –≤ –ø–æ–ª—å–∑—É –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∑–∞ –∫–æ–Ω–µ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. SophiaVL-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –æ–±–æ–±—â–µ–Ω–∏—é.'}, 'en': {'title': 'Empowering Reasoning with Thinking Process Rewards', 'desc': 'This paper introduces SophiaVL-R1, a multimodal language model that enhances reasoning and generalization by incorporating rewards for the thinking process. Traditional reinforcement learning methods often overlook the quality of reasoning, leading to suboptimal strategies. To address this, the authors develop a thinking reward model that evaluates reasoning quality and implement a Trust-GRPO method to ensure reliable reward signals. Their experiments demonstrate that SophiaVL-R1 outperforms larger models on various benchmarks, showcasing its superior reasoning capabilities.'}, 'zh': {'title': 'ÂºïÂÖ•ÊÄùÁª¥Â•ñÂä±ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÊ®°Âûã', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫ÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãSophiaVL-R1ÔºåÊó®Âú®ÈÄöËøáÂºïÂÖ•ÊÄùÁª¥ËøáÁ®ãÂ•ñÂä±Êù•ÊîπÂñÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÊó∂Áº∫‰πèÂØπÊÄùÁª¥ËøáÁ®ãÁöÑÁõëÁù£ÔºåÂèØËÉΩÂØºËá¥Â≠¶‰π†Âà∞Ê¨°‰ºòÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖ‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÊÄùÁª¥Â•ñÂä±Ê®°ÂûãÔºåÂπ∂ÊèêÂá∫‰∫ÜTrust-GRPOÊñπÊ≥ïÊù•ËØÑ‰º∞ÊÄùÁª¥Â•ñÂä±ÁöÑÂèØ‰ø°Â∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSophiaVL-R1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éËÆ∏Â§öÂ§ßÂûãÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17012', 'title': 'SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding', 'url': 'https://huggingface.co/papers/2505.17012', 'abstract': 'SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs.', 'score': 10, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '51519403c5b92e25', 'authors': ['Haoning Wu', 'Xiao Huang', 'Yaohui Chen', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai AI Laboratory', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2505.17012.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#survey', '#3d', '#benchmark', '#agents'], 'emoji': 'üß†', 'ru': {'title': 'SpatialScore: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SpatialScore - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ VGBench –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ SpatialAgent - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö MLLM, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ SpatialAgent. –ë–µ–Ω—á–º–∞—Ä–∫ SpatialScore –ø—Ä–∏–∑–≤–∞–Ω —Å—Ç–∞—Ç—å –≤–∞–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è MLLM –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è.'}, 'en': {'title': 'Enhancing 3D Spatial Understanding in Multimodal Models', 'desc': 'This paper introduces SpatialScore, a benchmark designed to evaluate multimodal large language models (MLLMs) in their ability to understand 3D spatial concepts. It highlights the development of VGBench, which focuses on visual geometry perception tasks like camera pose estimation. The benchmark includes 28,000 samples from various spatial understanding tasks and features a challenging subset called SpatialScore-Hard. Additionally, the paper presents SpatialAgent, a multi-agent system equipped with specialized tools to enhance spatial reasoning capabilities in MLLMs.'}, 'zh': {'title': 'Á©∫Èó¥ÁêÜËß£ÁöÑÊñ∞Âü∫ÂáÜ‰∏éÂ∑•ÂÖ∑', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®‰∏âÁª¥Á©∫Èó¥ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜVGBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞ËßÜËßâÂá†‰ΩïÊÑüÁü•ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñÁõ∏Êú∫ÂßøÊÄÅÂíåËøêÂä®‰º∞ËÆ°Á≠â‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSpatialScoreÔºåËøôÊòØËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂÖ®Èù¢ÁöÑÂ§öÊ®°ÊÄÅÁ©∫Èó¥ÁêÜËß£Âü∫ÂáÜÔºåÊï¥Âêà‰∫ÜÊù•Ëá™11‰∏™Áé∞ÊúâÊï∞ÊçÆÈõÜÁöÑÊï∞ÊçÆÔºåÂåÖÂê´28K‰∏™Ê†∑Êú¨Âíå‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂ≠êÈõÜSpatialScore-Hard„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜSpatialAgentÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºåÁªìÂêà‰∫Ü9‰∏™‰∏ìÈó®ÁöÑÂ∑•ÂÖ∑Ôºå‰ª•ÊîØÊåÅÁ©∫Èó¥ÁêÜËß£ÁöÑÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16839', 'title': 'LaViDa: A Large Diffusion Language Model for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2505.16839', 'abstract': "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.", 'score': 10, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '9e2202389256fc59', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Hritik Bansal', 'Akash Gokul', 'Yusuke Kato', 'Kazuki Kozuka', 'Jason Kuen', 'Zhe Lin', 'Kai-Wei Chang', 'Aditya Grover'], 'affiliations': ['Adobe Research', 'Panasonic AI Research', 'Salesforce Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2505.16839.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#training', '#games', '#cv', '#diffusion'], 'emoji': 'üß†', 'ru': {'title': 'LaViDa: –ë—ã—Å—Ç—Ä—ã–µ –∏ –≥–∏–±–∫–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏', 'desc': 'LaViDa - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–∏ —ç—Ç–æ–º –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, LaViDa –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LaViDa –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'LaViDa: Fast and Controllable Vision-Language Models', 'desc': 'LaViDa is a new family of vision-language models that utilize discrete diffusion models to enhance performance on multimodal tasks. It addresses the limitations of traditional autoregressive models by providing faster inference and better control over output generation. By integrating a vision encoder and employing techniques like complementary masking and prefix KV cache, LaViDa achieves high-quality results while maintaining efficiency. Experimental results show that LaViDa outperforms existing models in various benchmarks, demonstrating its potential as a robust alternative in the field of vision-language processing.'}, 'zh': {'title': 'LaViDaÔºöÈ´òÊïàÂèØÊéßÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã', 'desc': 'LaViDaÊòØ‰∏ÄÁßçÂü∫‰∫éÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂÆ∂ÊóèÔºåËÉΩÂ§üÂú®Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ‰∏éÁé∞ÊúâÁöÑËá™ÂõûÂΩíËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÊØîÔºåLaViDaÂú®Êé®ÁêÜÈÄüÂ∫¶„ÄÅÂèØÊéßÊÄßÂíåÂèåÂêëÊé®ÁêÜÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁªìÂêàËßÜËßâÁºñÁ†ÅÂô®ÂíåËÅîÂêàÂæÆË∞ÉÊäÄÊúØÔºåÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅ‰ªªÂä°ÁöÑÂ§ÑÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLaViDaÂú®Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰Ωú‰∏∫Ëá™ÂõûÂΩíÊ®°ÂûãÂº∫ÊúâÂäõÊõø‰ª£ÂìÅÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14625', 'title': 'TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.14625', 'abstract': "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.", 'score': 10, 'issue_id': 3914, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '0bcb0b140b7623c3', 'authors': ['Zhangchen Xu', 'Yuetai Li', 'Fengqing Jiang', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14625.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#training', '#dataset', '#rl', '#optimization'], 'emoji': 'üîç', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª–µ–µ 38% –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –Ω–µ–≤–µ—Ä–Ω–æ –æ—Ç–∫–ª–æ–Ω—è—é—Ç—Å—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Ö—É–¥—à–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä tinyV –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ –≤—ã—è–≤–ª—è–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è tinyV –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ 10% –∏ —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM —Å –ø–æ–º–æ—â—å—é RL –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'Enhancing RL Training by Tackling Verifier False Negatives with TinyV', 'desc': 'This paper discusses the challenges of using Reinforcement Learning (RL) to improve large language models (LLMs) due to the issue of false negatives from verifiers. False negatives occur when verifiers incorrectly reject correct outputs from the model, which can hinder the RL training process by limiting the feedback the model receives. The authors analyze a dataset and find that a significant portion of model responses are misclassified, leading to slower learning and convergence. To address this, they introduce TinyV, a new verifier that enhances existing methods by identifying and correcting these false negatives, resulting in improved performance on math-reasoning tasks.'}, 'zh': {'title': 'Ëß£ÂÜ≥ÂÅáÈò¥ÊÄßÔºåÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÊïàÊûúÔºÅ', 'desc': 'Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊòØ‰∏ÄÁßçÈÄöËøáÂ•ñÂä±‰ø°Âè∑‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁ≠ñÁï•ÁöÑÂº∫Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåRLÁöÑÊàêÂäü‰æùËµñ‰∫éÈ™åËØÅËÄÖÊèê‰æõÁöÑÂèØÈù†Â•ñÂä±ÔºåËÄåÊàë‰ª¨ÂèëÁé∞‰∏Ä‰∏™ÊôÆÈÅçÂ≠òÂú®ÁöÑÈóÆÈ¢ò‚Äî‚ÄîÂÅáÈò¥ÊÄßÔºåÂç≥È™åËØÅËÄÖÈîôËØØÂú∞ÊãíÁªùÊ≠£Á°ÆÁöÑÊ®°ÂûãËæìÂá∫„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåË∂ÖËøá38%ÁöÑÊ®°ÂûãÁîüÊàêÁöÑÂìçÂ∫îÂèóÂà∞ÂÅáÈò¥ÊÄßÁöÑÂΩ±ÂìçÔºåËøô‰∏•ÈáçÊçüÂÆ≥‰∫ÜRLËÆ≠ÁªÉÁöÑÊïàÊûú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜtinyVÔºå‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑLLMÂü∫Á°ÄÈ™åËØÅÂô®ÔºåÂèØ‰ª•Âä®ÊÄÅËØÜÂà´ÊΩúÂú®ÁöÑÂÅáÈò¥ÊÄßÔºå‰ªéËÄåÊèêÈ´òÂ•ñÂä±‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16854', 'title': 'Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.16854', 'abstract': "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON.", 'score': 9, 'issue_id': 3915, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'af930383188983e1', 'authors': ['Jiaqi Wang', 'Kevin Qinghong Lin', 'James Cheng', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.16854.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è TON –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. TON —Å–æ—á–µ—Ç–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–µ—Ç–æ–¥–æ–º '–æ—Ç—Å–µ–≤–∞ –º—ã—Å–ª–µ–π' –∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO). –≠—Ç–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∏–∑–±–µ–≥–∞—Ç—å –Ω–µ–Ω—É–∂–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Å–æ–∫—Ä–∞—â–∞—è –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞ –Ω–∞ 90% –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—á–∏—Ç—Å—è –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏—à–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É –º—ã—à–ª–µ–Ω–∏—è."}, 'en': {'title': 'TON: Streamlining Reasoning in Vision-Language Models', 'desc': "The paper introduces TON, a two-stage training strategy designed to enhance vision-language models (VLMs) by reducing unnecessary reasoning steps. It combines supervised fine-tuning with a technique called 'thought dropout' to help models decide when reasoning is necessary, mimicking human-like thinking processes. The second stage employs Group Relative Policy Optimization (GRPO) to optimize the model's reasoning efficiency while maximizing task performance. Experimental results demonstrate that TON can significantly decrease reasoning length without compromising, and often improving, the model's performance across various tasks."}, 'zh': {'title': 'TONÔºö‰ºòÂåñËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÂàõÊñ∞Á≠ñÁï•', 'desc': 'TONÊòØ‰∏ÄÁßç‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁªìÂêà‰∫ÜÁõëÁù£ÂæÆË∞ÉÂíåÊÄùÁª¥‰∏¢ÂºÉÔºåÊó®Âú®ÂáèÂ∞ëËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÊé®ÁêÜÊ≠•È™§ÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøáÁÆÄÂçïÊúâÊïàÁöÑ‚ÄúÊÄùÁª¥‰∏¢ÂºÉ‚ÄùÊìç‰ΩúÔºåÈöèÊú∫Áî®Á©∫ÊÄùÁª¥ÊõøÊç¢Êé®ÁêÜËΩ®ËøπÔºåÂºïÂÖ•‰∫ÜÈÄâÊã©ÊÄßÊé®ÁêÜÁöÑÊÄùËÄÉÊ†ºÂºè„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÈááÁî®Áæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üËá™Áî±ÂÜ≥ÂÆö‰ΩïÊó∂ËøõË°åÊé®ÁêÜÔºå‰ªéËÄåÊúÄÂ§ßÂåñ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁªìÊûúÂ•ñÂä±„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTONÂèØ‰ª•Â∞ÜÂÆåÊàêÈïøÂ∫¶ÂáèÂ∞ëÂ§öËææ90%ÔºåËÄå‰∏çÁâ∫Áâ≤ÊÄßËÉΩÔºåÁîöËá≥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15879', 'title': 'GRIT: Teaching MLLMs to Think with Images', 'url': 'https://huggingface.co/papers/2505.15879', 'abstract': 'Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.', 'score': 9, 'issue_id': 3915, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '05b66f8da1752a05', 'authors': ['Yue Fan', 'Xuehai He', 'Diji Yang', 'Kaizhi Zheng', 'Ching-Chen Kuo', 'Yuting Zheng', 'Sravana Jyothi Narayanaraju', 'Xinze Guan', 'Xin Eric Wang'], 'affiliations': ['UC Santa Cruz', 'eBay'], 'pdf_title_img': 'assets/pdf/title_img/2505.15879.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rl', '#open_source', '#training'], 'emoji': 'üß†', 'ru': {'title': 'GRIT: –í–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GRIT. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—á–µ—Ç–∞—é—â–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. GRIT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º GRPO-GR, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ GRPO, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ —è–≤–Ω—ã—Ö –º–µ—Ç–æ–∫ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GRIT —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞–µ—Ç MLLM –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–µ.'}, 'en': {'title': 'Grounded Reasoning: Merging Vision and Language for Better Understanding', 'desc': 'This paper introduces Grounded Reasoning with Images and Texts (GRIT), a new method that enhances visual reasoning in machine learning models. GRIT allows models to generate reasoning chains that combine natural language with specific visual information, represented by bounding box coordinates. By using a reinforcement learning approach called GRPO-GR, GRIT focuses on improving the accuracy of final answers without needing extensive annotated data. The results show that GRIT can efficiently train models to produce clear and visually supported reasoning, bridging the gap between reasoning and visual understanding.'}, 'zh': {'title': 'ÂõæÂÉè‰∏éÊñáÊú¨ÁöÑÂü∫Á°ÄÊé®ÁêÜÔºöËÆ©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÊÄùËÄÉ', 'desc': 'ÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÊûÑÂª∫Êé®ÁêÜÊ®°ÂûãÊñπÈù¢ÈùûÂ∏∏ÊúâÊïàÔºåËøô‰∫õÊ®°ÂûãÂú®ÁªôÂá∫ÊúÄÁªàÁ≠îÊ°à‰πãÂâç‰ºöË°®ËææÊÄùÁª¥Èìæ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂºÄÊ∫êËßÜËßâÊé®ÁêÜÊ®°ÂûãÈÄöÂ∏∏Âè™Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÊé®ÁêÜÂÜÖÂÆπÔºåÁº∫‰πè‰∏éËßÜËßâ‰ø°ÊÅØÁöÑÊòéÁ°ÆÁªìÂêàÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁîüÊàêÊ∏ÖÊô∞‰∏î‰∏éËßÜËßâÁõ∏ÂÖ≥ÁöÑÊé®ÁêÜÈìæÁöÑËÉΩÂäõ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ï‚Äî‚ÄîÂõæÂÉè‰∏éÊñáÊú¨ÁöÑÂü∫Á°ÄÊé®ÁêÜÔºàGRITÔºâÔºåËØ•ÊñπÊ≥ïËÆ≠ÁªÉÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏éÂõæÂÉèÂÖ±ÂêåÊÄùËÄÉ„ÄÇGRITÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫Á°ÄÊé®ÁêÜËåÉÂºèÔºåÊ®°ÂûãÁîüÊàêÁöÑÊé®ÁêÜÈìæ‰∫§ÊõøÂåÖÂê´Ëá™ÁÑ∂ËØ≠Ë®ÄÂíåÊòéÁ°ÆÁöÑËæπÁïåÊ°ÜÂùêÊ†áÔºåËøô‰∫õÂùêÊ†áÊåáÂêëËæìÂÖ•ÂõæÂÉè‰∏≠Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÂèÇËÄÉÁöÑÂå∫Âüü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15963', 'title': 'OViP: Online Vision-Language Preference Learning', 'url': 'https://huggingface.co/papers/2505.15963', 'abstract': "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities.", 'score': 8, 'issue_id': 3914, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '4476380071b2e936', 'authors': ['Shujun Liu', 'Siyuan Wang', 'Zejun Li', 'Jianxiang Wang', 'Cheng Zeng', 'Zhongyu Wei'], 'affiliations': ['ByteDance', 'Fudan University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.15963.jpg', 'data': {'categories': ['#multimodal', '#training', '#hallucinations', '#benchmark', '#diffusion', '#rag', '#alignment'], 'emoji': 'üîÆ', 'ru': {'title': 'OViP: –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π OViP (Online Vision-language Preference Learning). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ—á–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏. OViP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ OViP —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–ª—é—á–µ–≤—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Dynamic Learning to Combat Hallucination in Vision-Language Models', 'desc': "This paper addresses the issue of hallucination in large vision-language models (LVLMs), where the models generate content that does not match the visual inputs. The authors introduce a new framework called Online Vision-language Preference Learning (OViP), which creates training data based on the model's own incorrect outputs, rather than relying on static negative samples. By using a diffusion model to synthesize negative images, OViP provides more relevant feedback for the model to learn from. The results show that this approach not only reduces hallucinations but also maintains the model's ability to express multi-modal information effectively."}, 'zh': {'title': 'Âä®ÊÄÅÊûÑÂª∫ÂØπÊØîÊï∞ÊçÆÔºåÂáèÂ∞ëÂπªËßâÔºÅ', 'desc': 'Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®ÁîüÊàêÂÜÖÂÆπÊó∂ÂÆπÊòìÂá∫Áé∞ÂπªËßâÔºåÂ∏∏Â∏∏‰∏éËßÜËßâËæìÂÖ•‰∏ç‰∏ÄËá¥„ÄÇËôΩÁÑ∂ÊúÄËøëÁöÑÁ†îÁ©∂ÈÄöËøáÂ§öÊ®°ÊÄÅÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÊù•ÂáèËΩªÂπªËßâÈóÆÈ¢òÔºå‰ΩÜÈÄöÂ∏∏‰æùËµñ‰∫éÈ¢ÑÂÆö‰πâÊàñÈöèÊú∫ÁºñËæëÁöÑË¥üÊ†∑Êú¨ÔºåËøô‰∫õÊ†∑Êú¨Êó†Ê≥ïÁúüÂÆûÂèçÊò†Ê®°ÂûãÁöÑÈîôËØØÔºåÈôêÂà∂‰∫ÜËÆ≠ÁªÉÊïàÊûú„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®Á∫øËßÜËßâËØ≠Ë®ÄÂÅèÂ•ΩÂ≠¶‰π†ÔºàOViPÔºâÊ°ÜÊû∂ÔºåÂä®ÊÄÅÊûÑÂª∫ÂØπÊØîËÆ≠ÁªÉÊï∞ÊçÆÔºåÂü∫‰∫éÊ®°ÂûãËá™Ë∫´ÁöÑÂπªËßâËæìÂá∫„ÄÇÈÄöËøáËØÜÂà´ÂìçÂ∫îÂØπ‰πãÈó¥ÁöÑËØ≠‰πâÂ∑ÆÂºÇÂπ∂‰ΩøÁî®Êâ©Êï£Ê®°ÂûãÂêàÊàêË¥üÂõæÂÉèÔºåOViPÂÆûÊó∂ÁîüÊàêÊõ¥Áõ∏ÂÖ≥ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÊúâÊïàÂáèÂ∞ëÂπªËßâÔºåÂêåÊó∂‰øùÊåÅÂ§öÊ®°ÊÄÅËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17015', 'title': 'Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.17015', 'abstract': 'Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics.', 'score': 7, 'issue_id': 3923, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'b15dc44bbeff2152', 'authors': ['Runsen Xu', 'Weiyao Wang', 'Hao Tang', 'Xingyu Chen', 'Xiaodong Wang', 'Fu-Jen Chu', 'Dahua Lin', 'Matt Feiszli', 'Kevin J. Liang'], 'affiliations': ['FAIR, Meta', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17015.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#games', '#3d', '#robotics', '#multimodal', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–ú—É–ª—å—Ç–∏–∫–∞–¥—Ä–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–¥—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç MultiSPA –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Multi-SpatialMLLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º—É–ª—å—Ç–∏–∫–∞–¥—Ä–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞ –Ω–∞–≥—Ä–∞–¥.'}, 'en': {'title': 'Empowering MLLMs with Multi-Frame Spatial Intelligence', 'desc': 'This paper addresses the limitations of multi-modal large language models (MLLMs) in understanding spatial information across multiple frames, which is crucial for applications like robotics. The authors introduce a new framework that enhances MLLMs by incorporating depth perception, visual correspondence, and dynamic perception. They present the MultiSPA dataset, a large-scale collection of over 27 million samples that covers a variety of 3D and 4D scenes, to train and evaluate their model. The proposed Multi-SpatialMLLM shows improved performance in multi-frame reasoning tasks and demonstrates potential for multi-task learning and emergent capabilities in complex scenarios.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂ§öÂ∏ßÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂Ôºå‰ª•Â¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§öÂ∏ßÁ©∫Èó¥ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÈÄöËøáÊï¥ÂêàÊ∑±Â∫¶ÊÑüÁü•„ÄÅËßÜËßâÂØπÂ∫îÂíåÂä®ÊÄÅÊÑüÁü•ÔºåÊù•Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÂú®Êú∫Âô®‰∫∫ÂíåÁé∞ÂÆûÂ∫îÁî®‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜMultiSPAÊï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´Ë∂ÖËøá2700‰∏áÊ†∑Êú¨ÁöÑÂÖ®Êñ∞Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñÂ§öÁßç3DÂíå4DÂú∫ÊôØ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãMulti-SpatialMLLMÂú®Â§öÂ∏ßÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊèêÂçáÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂ§ö‰ªªÂä°‰ºòÂäøÂíåÊñ∞ÂÖ¥ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16151', 'title': 'Training-Free Reasoning and Reflection in MLLMs', 'url': 'https://huggingface.co/papers/2505.16151', 'abstract': 'The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates a hierarchical weight merging approach that combines a visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we propose a layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html', 'score': 7, 'issue_id': 3914, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '59af0e553fdc317e', 'authors': ['Hongchen Wei', 'Zhenzhong Chen'], 'affiliations': ['School of Remote Sensing and Information Engineering, Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16151.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#benchmark', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': 'FRANK: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–ú–æ–¥–µ–ª—å FRANK —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM), –¥–æ–±–∞–≤–ª—è—è –∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç –º–æ–¥–µ–ª—å, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å –º–æ–¥–µ–ª—å—é, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–æ–π–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –¢–µ–π–ª–æ—Ä–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ –¥–µ–∫–æ–¥–µ—Ä–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø—Ä–∏–≤—è–∑–∫—É –≤ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö —Å–ª–æ—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–∏–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'FRANK Model: Enhancing MLLMs with Reasoning Without Retraining', 'desc': 'The FRANK Model enhances multimodal large language models (MLLMs) by integrating reasoning and reflection capabilities without the need for retraining. It employs a hierarchical weight merging technique that combines visual-pretrained models with reasoning-specialized models, allowing for effective reasoning in MLLMs. The model strategically decouples perception and reasoning across different layers of the decoder, leveraging shallow layers for visual attention and deeper layers for textual semantics. Experimental results show that FRANK-38B significantly outperforms existing models on multimodal reasoning tasks, achieving a notable accuracy increase on the MMMU benchmark.'}, 'zh': {'title': 'FRANKÊ®°ÂûãÔºöÊó†ÈúÄÈáçËÆ≠ÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂ¢ûÂº∫', 'desc': 'FRANKÊ®°ÂûãÈÄöËøáÂàÜÂ±ÇÊùÉÈáçÂêàÂπ∂ÁöÑÊñπÊ≥ïÔºåÂ¢ûÂº∫‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊé®ÁêÜÂíåÂèçÊÄùËÉΩÂäõÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇËØ•Ê®°ÂûãÂ∞ÜËßÜËßâÈ¢ÑËÆ≠ÁªÉÁöÑMLLM‰∏é‰∏ìÊ≥®‰∫éÊé®ÁêÜÁöÑLLMÁªìÂêàÔºåÈÅøÂÖç‰∫ÜÈ´òÊòÇÁöÑÈáçÊñ∞ËÆ≠ÁªÉÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÖÂ±ÇËß£Á†ÅÂô®Â±ÇÂØπËßÜËßâ‰ø°ÊÅØÁöÑÂÖ≥Ê≥®Â∫¶Êõ¥È´òÔºåËÄåÊ∑±Â±ÇËß£Á†ÅÂô®Â±ÇÂàôÊõ¥Ê≥®ÈáçÊñáÊú¨ËØ≠‰πâÔºåËøô‰∏ÄËßÇÂØü‰øÉÊàê‰∫ÜÂàÜÂ±ÇÊùÉÈáçÂêàÂπ∂ÁöÑÊñπÊ≥ï„ÄÇÈÄöËøáÂú®Ê∑±Â±ÇËß£Á†ÅÂô®‰∏≠Êï¥ÂêàÊé®ÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÊµÖÂ±ÇËß£Á†ÅÂô®ÁöÑËßÜËßâÂü∫Á°ÄÔºåFRANKÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂáÜÁ°ÆÁéáË∂ÖËøá‰∫ÜÂ§ö‰∏™Âº∫Âü∫Á∫øÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16944', 'title': 'AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios', 'url': 'https://huggingface.co/papers/2505.16944', 'abstract': "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.", 'score': 6, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '1699a99ec316293a', 'authors': ['Yunjia Qi', 'Hao Peng', 'Xiaozhi Wang', 'Amy Xin', 'Youfeng Liu', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.16944.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#agents', '#long_context', '#agi'], 'emoji': 'ü§ñ', 'ru': {'title': 'AgentIF: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ AgentIF –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤. AgentIF –≤–∫–ª—é—á–∞–µ—Ç 707 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ 50 –∞–≥–µ–Ω—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–æ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω–æ–π 1723 —Å–ª–æ–≤–∞ –∏ 11.9 –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é. –û—Ü–µ–Ω–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö LLM —Å –ø–æ–º–æ—â—å—é AgentIF –ø–æ–∫–∞–∑–∞–ª–∞ –∏—Ö –Ω–∏–∑–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –≤—ã—è–≤–∏–≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∂–∏–º—ã –æ—Ç–∫–∞–∑–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö LLM.'}, 'en': {'title': 'AgentIF: Benchmarking LLMs in Complex Instruction Following', 'desc': 'This paper introduces AgentIF, a new benchmark designed to assess how well Large Language Models (LLMs) can follow complex instructions in realistic scenarios where they act as agents. The benchmark is based on 50 real-world applications and includes long instructions with an average of 1,723 words and multiple constraints, such as tool specifications. The study reveals that current LLMs struggle significantly with these complex instructions, particularly in managing constraints and tool requirements. By providing detailed evaluations and error analyses, the authors aim to highlight the limitations of existing models and encourage further research in this area.'}, 'zh': {'title': 'ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïAgentIFÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊåá‰ª§‰∏ãÁöÑË°®Áé∞ÔºåÁâπÂà´ÊòØÂú®ÁúüÂÆûÁöÑ‰ª£ÁêÜÂú∫ÊôØ‰∏≠„ÄÇAgentIFÁöÑÁâπÁÇπÂåÖÊã¨ÔºöÂü∫‰∫é50‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ‰ª£ÁêÜÂ∫îÁî®ÊûÑÂª∫ÔºåÊåá‰ª§Âπ≥ÂùáÈïøÂ∫¶‰∏∫1723‰∏™ÂçïËØçÔºå‰∏îÊØèÊù°Êåá‰ª§Âπ≥ÂùáÂåÖÂê´11.9‰∏™Á∫¶ÊùüÊù°‰ª∂„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÁöÑLLMsÂú®Â§ÑÁêÜÂ§çÊùÇÁ∫¶ÊùüÂíåÂ∑•ÂÖ∑ËßÑËåÉÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÅµÂæ™ÈïøÊåá‰ª§ÊñπÈù¢„ÄÇÈÄöËøáÈîôËØØÂàÜÊûêÂíåÂÆûÈ™åÔºåÊú¨ÊñáÊè≠Á§∫‰∫ÜÁé∞ÊúâÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Êèê‰æõ‰∫Ü‰ª£Á†ÅÂíåÊï∞ÊçÆ‰ª•ÊîØÊåÅÊú™Êù•ÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16265', 'title': 'Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models', 'url': 'https://huggingface.co/papers/2505.16265', 'abstract': "Think-RM is a framework that enhances generative reward models with long-horizon reasoning and a novel pairwise RLHF pipeline to improve end-policy performance in aligning large language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the model's long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches.", 'score': 6, 'issue_id': 3929, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '3bfe544ceb0449d3', 'authors': ['Ilgee Hong', 'Changlong Yu', 'Liang Qiu', 'Weixiang Yan', 'Zhenghao Xu', 'Haoming Jiang', 'Qingru Zhang', 'Qin Lu', 'Xin Liu', 'Chao Zhang', 'Tuo Zhao'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.16265.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#rlhf', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–î—É–º–∞–π –≥–ª—É–±–∂–µ: —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': 'Think-RM - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –Ω–æ–≤–æ–π –ø–æ–ø–∞—Ä–Ω–æ–π —Å—Ö–µ–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF). –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–∏–±–∫–∏–µ, —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ —Ç–∞–∫–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –∫–∞–∫ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑ –∏ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Think-RM –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ RM-Bench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ 8%. –í —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –Ω–æ–≤–æ–π –ø–æ–ø–∞—Ä–Ω–æ–π —Å—Ö–µ–º–æ–π RLHF –æ–Ω –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω–µ—á–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Empowering Generative Models with Long-Horizon Reasoning', 'desc': "Think-RM is a new framework designed to improve generative reward models (GenRMs) by enabling long-horizon reasoning, which helps align large language models with human preferences more effectively. It addresses the limitations of traditional Bradley-Terry reward models (BT RMs) that struggle with data sensitivity and reward hacking. By generating self-guided reasoning traces, Think-RM enhances the model's ability to perform complex reasoning tasks. Additionally, it introduces a novel pairwise reinforcement learning from human feedback (RLHF) pipeline that optimizes policies directly with pairwise preferences, leading to better overall performance."}, 'zh': {'title': 'Think-RMÔºöÊèêÂçáÁîüÊàêÂ•ñÂä±Ê®°ÂûãÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ', 'desc': 'Think-RMÊòØ‰∏Ä‰∏™Â¢ûÂº∫ÁîüÊàêÂ•ñÂä±Ê®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÈïøÊó∂Èó¥Êé®ÁêÜÂíåÊñ∞È¢ñÁöÑÊàêÂØπ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâÊµÅÁ®ãÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÂú®Êï∞ÊçÆËßÑÊ®°ÂíåË¶ÜÁõñÂ∫¶‰∏äÊïèÊÑüÔºåÂÆπÊòìÂèóÂà∞Â•ñÂä±ÊìçÊéßÁöÑÂΩ±ÂìçÔºåËÄåÁîüÊàêÂ•ñÂä±Ê®°ÂûãÂàôÈÄöËøáÁîüÊàêÊÄùÁª¥ÈìæÔºàCoTÔºâÊé®ÁêÜÊù•Êèê‰æõÊõ¥Á®≥ÂÅ•ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇThink-RMÈÄöËøáÂª∫Ê®°ÂÜÖÈÉ®ÊÄùÁª¥ËøáÁ®ãÔºåÁîüÊàêÁÅµÊ¥ªÁöÑËá™ÊàëÂºïÂØºÊé®ÁêÜËΩ®ËøπÔºåÊîØÊåÅËá™ÊàëÂèçÊÄùÂíåÂÅáËÆæÊé®ÁêÜÁ≠âÈ´òÁ∫ßËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåThink-RMÂú®RM-Bench‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16192', 'title': 'VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought', 'url': 'https://huggingface.co/papers/2505.16192', 'abstract': 'Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce VLM-R^3 (Visual Language Model with Region Recognition and Reasoning), a framework that equips an MLLM with the ability to (i) decide when additional visual evidence is needed, (ii) determine where to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is Region-Conditioned Reinforcement Policy Optimization (R-GRPO), a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction.', 'score': 6, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'ffe915867d50a220', 'authors': ['Chaoya Jiang', 'Yongrui Heng', 'Wei Ye', 'Han Yang', 'Haiyang Xu', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Shikun Zhang'], 'affiliations': ['Alibaba Group', 'National Engineering Research Center for Software Engineering, Peking University', 'ZEEKR Intelligent Technology Holding Limited'], 'pdf_title_img': 'assets/pdf/title_img/2505.16192.jpg', 'data': {'categories': ['#reasoning', '#cv', '#dataset', '#multimodal', '#benchmark', '#rl', '#training'], 'emoji': 'üîç', 'ru': {'title': '–£–º–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –ò–ò: —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLM-R^3 - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è R-GRPO, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª—å –≤—ã–±–∏—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –î–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç VLIR —Å –ø–æ—à–∞–≥–æ–≤–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π –≤—ã–±–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VLM-R^3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä—è–¥–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ç–æ–Ω–∫–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Visual Reasoning in Language Models with VLM-R^3', 'desc': 'This paper presents VLM-R^3, a new framework that enhances reasoning in multi-modal language models (MLLMs) by integrating visual evidence more effectively. It allows the model to identify when it needs more visual information, where to focus within an image, and how to incorporate this visual context into its reasoning process. The training method, Region-Conditioned Reinforcement Policy Optimization (R-GRPO), incentivizes the model to select informative image regions and apply transformations like cropping or zooming. The results demonstrate that VLM-R^3 outperforms previous models on tasks requiring detailed spatial reasoning and visual cue extraction, achieving state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'ËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÊ∑±Â∫¶ËûçÂêà', 'desc': 'ÊúÄËøëÔºåÂü∫‰∫éÊé®ÁêÜÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÁîüÊàêÈïøÊñáÊú¨Êé®ÁêÜÈìæÊñπÈù¢ÂèñÂæó‰∫Ü‰∏ÄÂÆöÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºåËøô‰∫õ‰ªªÂä°ÈúÄË¶ÅÂä®ÊÄÅÂíåËø≠‰ª£Âú∞ÂÖ≥Ê≥®ÂíåÈáçÊñ∞ÂÆ°ËßÜËßÜËßâÂå∫ÂüüÔºå‰ª•ÂÆûÁé∞ÊñáÊú¨Êé®ÁêÜ‰∏éËßÜËßâËØÅÊçÆÁöÑÁ≤æÁ°ÆÁªìÂêà„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜVLM-R^3ÔºàÂ∏¶ÊúâÂå∫ÂüüËØÜÂà´ÂíåÊé®ÁêÜÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºâÔºåËØ•Ê°ÜÊû∂‰ΩøMLLMÂÖ∑Â§á‰∫ÜÂÜ≥ÂÆö‰ΩïÊó∂ÈúÄË¶ÅÈ¢ùÂ§ñËßÜËßâËØÅÊçÆ„ÄÅÁ°ÆÂÆöÂõæÂÉè‰∏≠Â∫îËÅöÁÑ¶ÁöÑ‰ΩçÁΩÆ‰ª•ÂèäÂ∞ÜÁõ∏ÂÖ≥Â≠êÂõæÂÉèÂÜÖÂÆπÊó†ÁºùËûçÂÖ•‰∫§ÈîôÊÄùÁª¥ÈìæÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ†∏ÂøÉÊñπÊ≥ïÊòØÂå∫ÂüüÊù°‰ª∂Âº∫ÂåñÁ≠ñÁï•‰ºòÂåñÔºàR-GRPOÔºâÔºåÈÄöËøáÂ•ñÂä±Ê®°ÂûãÈÄâÊã©‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂå∫Âüü„ÄÅÂà∂ÂÆöÈÄÇÂΩìÁöÑÂèòÊç¢ÔºàÂ¶ÇË£ÅÂâ™„ÄÅÁº©ÊîæÔºâÂπ∂Â∞ÜÁªìÊûúËßÜËßâ‰∏ä‰∏ãÊñáÊï¥ÂêàÂà∞ÂêéÁª≠Êé®ÁêÜÊ≠•È™§‰∏≠ÔºåÊù•ËÆ≠ÁªÉËØ•Á≠ñÁï•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15960', 'title': 'Training Step-Level Reasoning Verifiers with Formal Verification Tools', 'url': 'https://huggingface.co/papers/2505.15960', 'abstract': 'FoVer is a method for automatically annotating step-level error labels using formal verification tools to train Process Reward Models, which significantly improves cross-task generalization and outperforms human-annotated methods in various reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs), which provide step-by-step feedback on the reasoning generated by Large Language Models (LLMs), are receiving increasing attention. However, two key research gaps remain: collecting accurate step-level error labels for training typically requires costly human annotation, and existing PRMs are limited to math reasoning problems. In response to these gaps, this paper aims to address the challenges of automatic dataset creation and the generalization of PRMs to diverse reasoning tasks. To achieve this goal, we propose FoVer, an approach for training PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 for formal logic and Isabelle for theorem proof, which provide automatic and accurate verification for symbolic tasks. Using this approach, we synthesize a training dataset with error labels on LLM responses for formal logic and theorem proof tasks without human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we observe that LLM-based PRMs trained on our dataset exhibit cross-task generalization, improving verification across diverse reasoning tasks. Specifically, PRMs trained with FoVer significantly outperform baseline PRMs based on the original LLMs and achieve competitive or superior results compared to state-of-the-art PRMs trained on labels annotated by humans or stronger models, as measured by step-level verification on ProcessBench and Best-of-K performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU, and BBH. The datasets, models, and code are provided at https://github.com/psunlpgroup/FoVer.', 'score': 6, 'issue_id': 3926, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'fd1009a733cb10f6', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Nan Zhang', 'Sarkar Snigdha Sarathi Das', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15960.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#optimization', '#training', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': 'FoVer - —ç—Ç–æ –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (Process Reward Models). –î–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥—ã —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –æ—Ç –ª—é–¥–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. FoVer —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç–∫–∞–º–∏ –æ—à–∏–±–æ–∫ –¥–ª—è –∑–∞–¥–∞—á —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏ –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é FoVer, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã—Ö –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö —á–µ–ª–æ–≤–µ–∫–∞.'}, 'en': {'title': 'Automating Error Annotation for Enhanced Reasoning Models', 'desc': 'FoVer is a novel method that automates the annotation of step-level error labels using formal verification tools, which helps in training Process Reward Models (PRMs). This approach addresses the challenges of costly human annotation and expands the applicability of PRMs beyond just math reasoning tasks. By synthesizing a training dataset with accurate error labels for formal logic and theorem proof tasks, FoVer enables PRMs to generalize across various reasoning tasks effectively. The results show that PRMs trained with FoVer outperform traditional methods and achieve competitive performance on multiple reasoning benchmarks.'}, 'zh': {'title': 'FoVerÔºöËá™Âä®Ê†áÊ≥®ÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑË∑®‰ªªÂä°ËÉΩÂäõ', 'desc': 'FoVerÊòØ‰∏ÄÁßçÂà©Áî®ÂΩ¢ÂºèÈ™åËØÅÂ∑•ÂÖ∑Ëá™Âä®Ê†áÊ≥®Ê≠•È™§Á∫ßÈîôËØØÊ†áÁ≠æÁöÑÊñπÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâ„ÄÇËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜË∑®‰ªªÂä°ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊñπÊ≥ï„ÄÇFoVerÈÄöËøáËá™Âä®ÁîüÊàêÁöÑÈîôËØØÊ†áÁ≠æÔºåËß£ÂÜ≥‰∫ÜËÆ≠ÁªÉ‰∏≠ÈúÄË¶ÅÊòÇË¥µ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÈóÆÈ¢òÔºåÂπ∂Êâ©Â±ï‰∫ÜPRMsÂú®Â§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®FoVerËÆ≠ÁªÉÁöÑPRMsÂú®Ê≠•È™§Á∫ßÈ™åËØÅ‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂü∫‰∫éÂéüÂßãÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Á∫øPRMs„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11711', 'title': 'Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.11711', 'abstract': 'Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.', 'score': 6, 'issue_id': 3914, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'e7296e89ef67015f', 'authors': ['Sagnik Mukherjee', 'Lifan Yuan', 'Dilek Hakkani-Tur', 'Hao Peng'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.11711.jpg', 'data': {'categories': ['#rlhf', '#training', '#rl', '#optimization', '#alignment'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º. –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ç–∞–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞—é—Ç—Å—è –ø—É—Ç–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥—Å–µ—Ç–∏, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π 5-30% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç —Ñ–µ–Ω–æ–º–µ–Ω, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π '—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤', –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL –∏ —Å–µ–º–µ–π—Å—Ç–≤ LLM –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —è–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç –ø–æ—á—Ç–∏ –≤—Å–µ –º–∞—Ç—Ä–∏—Ü—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –æ—Å—Ç–∞—é—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –∏ –ø–æ–ª–Ω–æ—Ä–∞–Ω–≥–æ–≤—ã–º–∏."}, 'en': {'title': 'Efficient Reinforcement Learning: Small Updates, Big Gains!', 'desc': "This paper explores how reinforcement learning (RL) can enhance the performance of large language models (LLMs) by making minimal updates to a small subnetwork of parameters. Remarkably, only 5 to 30 percent of the model's parameters are adjusted, while the majority remain unchanged, a phenomenon termed 'parameter update sparsity.' This sparsity occurs across various RL algorithms and LLMs, indicating a consistent pattern in how RL influences model training. The findings suggest that even with limited updates, the subnetwork can achieve performance comparable to full finetuning, highlighting the efficiency of RL in optimizing LLMs."}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†ÔºöÂ∞èÊõ¥Êñ∞ÔºåÂ§ßÊèêÂçá', 'desc': 'Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÈÄöËøáÊúÄÂ∞èÁöÑÂèÇÊï∞Êõ¥Êñ∞ÊòæËëóÊèêÂçá‰∫Ü‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞Âíå‰∏é‰∫∫Á±ª‰ª∑ÂÄºËßÇÁöÑÂØπÈΩê„ÄÇ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºåËøôÁßçÊòæËëóÁöÑÊèêÂçá‰ªÖÈÄöËøáÊõ¥Êñ∞Âç†ÂèÇÊï∞5%Âà∞30%ÁöÑÂ∞èÂ≠êÁΩëÁªúÂÆûÁé∞ÔºåÂÖ∂‰ΩôÂèÇÊï∞Âü∫Êú¨‰øùÊåÅ‰∏çÂèò„ÄÇÊàë‰ª¨Áß∞ËøôÁßçÁé∞Ë±°‰∏∫Áî±RLÂºïËµ∑ÁöÑÂèÇÊï∞Êõ¥Êñ∞Á®ÄÁñèÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÁ®ÄÁñèÊÄßÂú®‰∏ÉÁßçÂπøÊ≥õ‰ΩøÁî®ÁöÑRLÁÆóÊ≥ïÂíåÂçÅÁßç‰∏çÂêåÂÆ∂ÊóèÁöÑLLM‰∏≠ÊôÆÈÅçÂ≠òÂú®Ôºå‰∏î‰∏çÈúÄË¶Å‰ªª‰ΩïÊòæÂºèÁöÑÁ®ÄÁñè‰øÉËøõÊ≠£ÂàôÂåñÊàñÊû∂ÊûÑÁ∫¶Êùü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16186', 'title': 'SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning', 'url': 'https://huggingface.co/papers/2505.16186', 'abstract': "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.", 'score': 5, 'issue_id': 3918, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'a47a9eb4edda38b3', 'authors': ['Kaiwen Zhou', 'Xuandong Zhao', 'Gaowen Liu', 'Jayanth Srinivasa', 'Aosong Feng', 'Dawn Song', 'Xin Eric Wang'], 'affiliations': ['Cisco Research', 'UC Berkeley', 'UC Santa Cruz', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16186.jpg', 'data': {'categories': ['#benchmark', '#security', '#training', '#reasoning', '#architecture'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': 'SafeKey: –ê–∫—Ç–∏–≤–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SafeKey –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ú–µ—Ç–æ–¥ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ '–º–æ–º–µ–Ω—Ç–∞ –æ–∑–∞—Ä–µ–Ω–∏—è' –≤ –∫–ª—é—á–µ–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö–ø—É—Ç–µ–≤–æ–π –≥–æ–ª–æ–≤–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞. SafeKey —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∞—Ç–∞–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏."}, 'en': {'title': 'Activating Safety Moments in Large Reasoning Models', 'desc': "SafeKey is a method designed to improve the safety of large reasoning models (LRMs) by focusing on a critical moment in the model's reasoning process, known as the safety aha moment. This moment occurs in the key sentence, which helps the model determine whether it can respond safely to a query. SafeKey employs a Dual-Path Safety Head to strengthen the safety signals in the model's internal representations and a Query-Mask Modeling approach to enhance the model's attention to safety-related aspects of the query. Through experiments, SafeKey has shown to significantly reduce the risk of harmful responses while preserving the model's overall performance."}, 'zh': {'title': 'SafeKeyÔºöÊøÄÊ¥ªÂÆâÂÖ®Êó∂ÂàªÔºåÊèêÂçáÊé®ÁêÜÊ®°ÂûãÂÆâÂÖ®ÊÄß', 'desc': 'SafeKeyÈÄöËøáÊøÄÊ¥ªÂÖ≥ÈîÆÂè•‰∏≠ÁöÑÂÆâÂÖ®Êó∂ÂàªÊù•Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÂÆÉÈááÁî®ÂèåË∑ØÂæÑÂÆâÂÖ®Â§¥ÂíåÊü•ËØ¢Êé©Á†ÅÂª∫Ê®°ÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÂØπÊúâÂÆ≥ÊèêÁ§∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂÖ≥ÈîÆÂè•‰∏≠ÁöÑÂÆâÂÖ®Êó∂ÂàªËÉΩÂ§üÂºïÂØºÊ®°ÂûãÂÅöÂá∫ÂÆâÂÖ®ÁöÑÂìçÂ∫î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSafeKeyÊòæËëóÈôç‰Ωé‰∫ÜÊ®°ÂûãÂú®ÂêÑÁßçÊîªÂáª‰∏ãÁöÑÊúâÂÆ≥ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂÖ∂‰∏ÄËà¨ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13344', 'title': 'RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE\n  Optimization on Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.13344', 'abstract': "RoPECraft is a training-free method that modifies rotary positional embeddings in diffusion transformers to transfer motion from reference videos, enhancing text-guided video generation and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.", 'score': 5, 'issue_id': 3932, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'e47accfd208862f5', 'authors': ['Ahmet Berke Gokmen', 'Yigit Ekin', 'Bahri Batuhan Bilecen', 'Aysegul Dundar'], 'affiliations': ['Bilkent University, Department of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2505.13344.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training', '#benchmark'], 'emoji': 'üé•', 'ru': {'title': '–ü–µ—Ä–µ–Ω–æ—Å –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è', 'desc': 'RoPECraft - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ RoPE –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–æ –≤—Ä–µ–º—è —à–∞–≥–æ–≤ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∑–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –§—É—Ä—å–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RoPECraft –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–∞–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, —Ç–∞–∫ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ.'}, 'en': {'title': 'Seamless Motion Transfer in Video Generation with RoPECraft', 'desc': 'RoPECraft is a novel method that enhances video generation by transferring motion from reference videos without requiring additional training. It modifies rotary positional embeddings (RoPE) in diffusion transformers to incorporate motion information extracted from dense optical flow. The method aligns predicted and target velocities during the denoising process, ensuring that the generated video matches the intended motion. Additionally, it uses a regularization technique based on Fourier transform phase components to maintain fidelity to text prompts and minimize artifacts in the output.'}, 'zh': {'title': 'RoPECraftÔºöÊó†ËÆ≠ÁªÉÁöÑËßÜÈ¢ëËøêÂä®ËΩ¨ÁßªÊñ∞ÊñπÊ≥ï', 'desc': 'RoPECraftÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰øÆÊîπÊâ©Êï£ÂèòÊç¢Âô®‰∏≠ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊù•ÂÆûÁé∞ËßÜÈ¢ëËøêÂä®ËΩ¨Áßª„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ªéÂèÇËÄÉËßÜÈ¢ë‰∏≠ÊèêÂèñÂØÜÈõÜÂÖâÊµÅÔºåÂπ∂Âà©Áî®ËøêÂä®ÂÅèÁßªÈáèÊù•Êâ≠Êõ≤RoPEÁöÑÂ§çÊåáÊï∞Âº†ÈáèÔºå‰ªéËÄåÂ∞ÜËøêÂä®‰ø°ÊÅØÁºñÁ†ÅÂà∞ÁîüÊàêËøáÁ®ã‰∏≠„ÄÇÂú®ÂéªÂô™Êó∂Èó¥Ê≠•È™§‰∏≠ÔºåÈÄöËøá‰ΩøÁî®ÊµÅÂåπÈÖçÁõÆÊ†áÂØπÈ¢ÑÊµãÈÄüÂ∫¶ÂíåÁõÆÊ†áÈÄüÂ∫¶ËøõË°åËΩ®ËøπÂØπÈΩêÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñËøô‰∫õÂµåÂÖ•„ÄÇ‰∏∫‰∫Ü‰øùÊåÅËæìÂá∫‰∏éÊñáÊú¨ÊèêÁ§∫ÁöÑ‰∏ÄËá¥ÊÄßÂπ∂Èò≤Ê≠¢ÈáçÂ§çÁîüÊàêÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂü∫‰∫éÂèÇËÄÉËßÜÈ¢ëÂÇÖÈáåÂè∂ÂèòÊç¢Áõ∏‰ΩçÂàÜÈáèÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ª•ÊäëÂà∂È´òÈ¢ë‰º™ÂΩ±„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16612', 'title': 'Steering Large Language Models for Machine Translation Personalization', 'url': 'https://huggingface.co/papers/2505.16612', 'abstract': 'Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.', 'score': 4, 'issue_id': 3922, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '5695106d35c6955a', 'authors': ['Daniel Scalena', 'Gabriele Sarti', 'Arianna Bisazza', 'Elisabetta Fersini', 'Malvina Nissim'], 'affiliations': ['CLCG, University of Groningen', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2505.16612.jpg', 'data': {'categories': ['#low_resource', '#training', '#multimodal', '#machine_translation'], 'emoji': 'üé≠', 'ru': {'title': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ LLM: –æ—Ç –ø—Ä–æ–º–ø—Ç–æ–≤ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM), –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç –º–µ—Ç–æ–¥—ã –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –∏ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–∏ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å—Ç–∏–ª—é. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–∏–ª—å–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞.'}, 'en': {'title': 'Personalized Translations Made Easy with LLMs!', 'desc': "This paper discusses methods to improve personalized translations using large language models (LLMs) in situations where resources are limited. It highlights the use of prompting strategies and a contrastive framework that leverages latent concepts from sparse autoencoders to enhance stylistic personalization. The authors demonstrate that these techniques can effectively guide the model's output while maintaining high translation quality. Additionally, they analyze how these personalization methods influence the internal representations of the LLM, indicating that similar mechanisms are at work in both prompting and steering approaches."}, 'zh': {'title': '‰∏™ÊÄßÂåñÁøªËØëÁöÑÊñ∞Á≠ñÁï•', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏ãÔºåÂ¶Ç‰ΩïÈÄöËøáÊèêÁ§∫ÂíåÂØπÊØîÊ°ÜÊû∂Êù•‰∏™ÊÄßÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÁøªËØë„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂà©Áî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÊèêÂèñÁöÑÊΩúÂú®Ê¶ÇÂøµÔºåÂèØ‰ª•ÊúâÊïàËØÜÂà´‰∏™ÊÄßÂåñÁâπÂæÅÔºå‰ªéËÄåÊîπÂñÑÁøªËØëË¥®Èáè„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÁ≠ñÁï•Âú®ÊñáÂ≠¶ÁøªËØëÈ¢ÜÂüüË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂú®‰øùÊåÅÁøªËØëË¥®ÈáèÁöÑÂêåÊó∂ÔºåÂÆûÁé∞Âº∫ÁÉàÁöÑ‰∏™ÊÄßÂåñÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂ËøòÂèëÁé∞Ôºå‰∏™ÊÄßÂåñÁõ∏ÂÖ≥ÁöÑÊ®°ÂûãÂ±ÇÂú®Â§öÊ¨°ÊèêÁ§∫ÂíåÊàë‰ª¨ÁöÑÂºïÂØºÊñπÊ≥ï‰∏ãÂèóÂà∞ÁöÑÂΩ±ÂìçÁõ∏‰ººÔºåË°®Êòé‰∏§ËÄÖÂèØËÉΩÂ≠òÂú®Áõ∏‰ººÁöÑÊú∫Âà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.17019', 'title': 'Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework', 'url': 'https://huggingface.co/papers/2505.17019', 'abstract': 'LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.  \t\t\t\t\tAI-generated summary \t\t\t\t Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.', 'score': 3, 'issue_id': 3922, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '9284570e4cba3821', 'authors': ['Chenhao Zhang', 'Yazhe Niu'], 'affiliations': ['Huazhong University of Science and Technology', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17019.jpg', 'data': {'categories': ['#reasoning', '#science', '#multimodal', '#cv', '#benchmark', '#interpretability'], 'emoji': 'ü§ñ', 'ru': {'title': 'LAD: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø–æ–¥—Ç–µ–∫—Å—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LAD –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –ø–æ–¥—Ç–µ–∫—Å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å GPT-4o-mini. LAD –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ, –ø–æ–∏—Å–∫ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥—Ç–µ–∫—Å—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ —Ç–∏–ø–∞—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. LAD –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏.'}, 'en': {'title': 'Unlocking Image Meanings with LAD Framework', 'desc': 'The paper introduces Let Androids Dream (LAD), a three-stage framework designed to enhance image implication understanding and reasoning using the lightweight GPT-4o-mini model. It addresses the limitations of existing multimodal large language models (MLLMs) in grasping the nuanced meanings of images by implementing a structured approach: Perception, Search, and Reasoning. This framework converts visual data into detailed textual representations, integrates cross-domain knowledge to clarify ambiguities, and generates contextually aligned implications through reasoning. LAD demonstrates state-of-the-art performance on various benchmarks, significantly improving image implication tasks in both English and Chinese, and offers valuable insights for advancing vision-language reasoning.'}, 'zh': {'title': 'ËÆ©ÂÆâÂçìÊ¢¶ËßÅÁîµÁæäÔºöÂõæÂÉèÈöêÂê´ÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'LADÊòØ‰∏Ä‰∏™‰∏âÈò∂ÊÆµÊ°ÜÊû∂ÔºåÂà©Áî®GPT-4o-miniÊ®°ÂûãÔºåÂú®ÂõæÂÉèÈöêÂê´ÁêÜËß£ÂíåÊé®ÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊÑüÁü•„ÄÅÊêúÁ¥¢ÂíåÊé®ÁêÜ‰∏â‰∏™Ê≠•È™§ÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®ÁêÜËß£ÂõæÂÉèÈöêÂê´ÊÑè‰πâÊó∂ÁöÑ‰∏ä‰∏ãÊñáÁº∫Â§±ÈóÆÈ¢ò„ÄÇLADËÉΩÂ§üÂ∞ÜËßÜËßâ‰ø°ÊÅØËΩ¨Âåñ‰∏∫‰∏∞ÂØåÁöÑÊñáÊú¨Ë°®Á§∫ÔºåÂπ∂ÈÄöËøáË∑®È¢ÜÂüüÁü•ËØÜÁöÑÊï¥ÂêàÊù•Ê∂àÈô§Ê≠ß‰πâ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫AIÊõ¥ÊúâÊïàÂú∞Ëß£ËØªÂõæÂÉèÈöêÂê´ÊÑè‰πâÊèê‰æõ‰∫ÜÊñ∞ËßÅËß£ÔºåÊé®Âä®‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜÂíå‰∫∫Êú∫‰∫§‰∫íÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15517', 'title': 'Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets', 'url': 'https://huggingface.co/papers/2505.15517', 'abstract': 'Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.', 'score': 3, 'issue_id': 3917, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '7c3b47e3a7b062f1', 'authors': ['Kaiyuan Chen', 'Shuangyu Xie', 'Zehan Ma', 'Ken Goldberg'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.15517.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#games', '#dataset', '#3d', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–æ–±–æ—Ç—ã —É—á–∞—Ç –ò–ò –≤–∏–¥–µ—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å –º–∏—Ä', 'desc': 'Robo2VLM - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–æ–±–æ—Ç–∞. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–Ω—Å–æ—Ä–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ 3D-—Å–≤–æ–π—Å—Ç–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM). –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —Ä–æ–±–æ—Ç–∞ –Ω–∞ —Ñ–∞–∑—ã –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö, —Ü–µ–ª–µ–≤—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Robo2VLM-1, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 463 —Å—Ü–µ–Ω—ã –∏ 3396 –∑–∞–¥–∞—á —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏–∑ 176 —Ç—ã—Å—è—á —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–æ–±–æ—Ç–æ–≤.'}, 'en': {'title': 'Enhancing VLMs with Robot Trajectory Insights', 'desc': "Robo2VLM is a framework designed to create Visual Question Answering (VQA) datasets by utilizing data from robot trajectories. It enhances Vision-Language Models (VLMs) by integrating sensory information and understanding 3D properties related to robotic tasks. The framework segments robot movements into phases and generates questions based on the robot's interactions with its environment. The resulting dataset, Robo2VLM-1, includes a vast number of questions that help evaluate and improve the reasoning abilities of VLMs in spatial and interaction contexts."}, 'zh': {'title': 'Âà©Áî®Êú∫Âô®‰∫∫ËΩ®ËøπÊï∞ÊçÆÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõ', 'desc': 'Robo2VLMÊòØ‰∏Ä‰∏™Áî®‰∫éÁîüÊàêËßÜËßâÈóÆÁ≠îÊï∞ÊçÆÈõÜÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Êú∫Âô®‰∫∫ËΩ®ËøπÊï∞ÊçÆÊù•Â¢ûÂº∫ÂíåËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂàÜÊûêÊú∫Âô®‰∫∫ÁöÑ‰º†ÊÑüÂô®Êï∞ÊçÆÂíå3DÂ±ûÊÄßÁêÜËß£ÔºåÁîüÊàê‰∏éÊú∫Âô®‰∫∫Êìç‰ΩúÁõ∏ÂÖ≥ÁöÑÈóÆÁ≠îÊï∞ÊçÆ„ÄÇRobo2VLMÂ∞ÜÊú∫Âô®‰∫∫ËΩ®ËøπÂàÜ‰∏∫Â§ö‰∏™Êìç‰ΩúÈò∂ÊÆµÔºåÂπ∂Âú®ÊØè‰∏™Èò∂ÊÆµËØÜÂà´‰ªªÂä°ÁõÆÊ†áÂíåÁõÆÊ†áÁâ©‰ΩìÁöÑ3DÂ±ûÊÄß„ÄÇÊúÄÁªàÔºåRobo2VLM-1Êï∞ÊçÆÈõÜÂåÖÂê´684,710‰∏™ÈóÆÈ¢òÔºåÊ∂µÁõñ463‰∏™‰∏çÂêåÂú∫ÊôØÂíå3,396‰∏™Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÔºåËÉΩÂ§üÊúâÊïàËØÑ‰º∞ÂíåÊèêÂçáVLMÂú®Á©∫Èó¥Âíå‰∫§‰∫íÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15865', 'title': 'How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads', 'url': 'https://huggingface.co/papers/2505.15865', 'abstract': 'The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images.', 'score': 3, 'issue_id': 3919, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '4cf7128eaead82ce', 'authors': ['Ingeol Baek', 'Hwan Chang', 'Sunghyun Ryu', 'Hwanhee Lee'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea', 'Department of Computer Engineering, Sejong University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.15865.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#architecture', '#cv'], 'emoji': 'üîç', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ OCR –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É OCR-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —ç—Ç–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ OCR-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–µ–Ω–µ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω—ã, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∏–º–µ—é—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ OCR-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Unlocking Text Recognition in Images with OCR Heads', 'desc': "This paper investigates the Optical Character Recognition (OCR) Heads in Large Vision Language Models (LVLMs) to understand how they process text in images. It reveals that these heads are less sparse, meaning many of them activate simultaneously to extract text, unlike traditional retrieval heads. The study also finds that OCR heads have distinct properties, showing low similarity to general retrieval heads, and their activation frequency correlates with their OCR performance. By applying techniques like Chain-of-Thought and redistributing values within these heads, the research enhances the models' ability to interpret text in images, shedding light on their internal workings."}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑOCRÂ§¥', 'desc': 'Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´Â§¥ÔºàOCRÂ§¥ÔºâÔºåÊè≠Á§∫‰∫ÜÂÆÉ‰ª¨Âú®ÂõæÂÉè‰∏≠ÊñáÊú¨Ëß£ËØª‰∏≠ÁöÑÁã¨ÁâπÊøÄÊ¥ªÊ®°ÂºèÂíåËßíËâ≤„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÂèØËß£ÈáäÊÄßÊñπÈù¢‰ªçÂ≠òÂú®Â∑ÆË∑ùÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÆö‰ΩçÂíåËß£ËØªÂõæÂÉè‰∏≠ÁöÑÊñáÊú¨‰ø°ÊÅØÊó∂„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåOCRÂ§¥ÁöÑÊøÄÊ¥ªÊñπÂºè‰∏é‰º†ÁªüÁöÑÊ£ÄÁ¥¢Â§¥ÊòæËëó‰∏çÂêåÔºå‰∏îÂú®ÊèêÂèñÊñáÊú¨‰ø°ÊÅØÊó∂ÊøÄÊ¥ªÁöÑÂ§¥Êï∞ÈáèËæÉÂ§ö„ÄÇÈÄöËøáÂØπËøô‰∫õÂ§¥ÁöÑÁ†îÁ©∂ÔºåÊàë‰ª¨Âä†Ê∑±‰∫ÜÂØπLVLMsÂú®Â§ÑÁêÜÂõæÂÉè‰∏≠ÂµåÂÖ•ÊñáÊú¨‰ø°ÊÅØÊó∂ÂÜÖÈÉ®Êú∫Âà∂ÁöÑÁêÜËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14462', 'title': 'RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture\n  Understanding', 'url': 'https://huggingface.co/papers/2505.14462', 'abstract': 'RAVENEA, a retrieval-augmented benchmark, enhances visual culture understanding in VLMs through culture-focused tasks and outperforms non-augmented models across various metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t As vision-language models (VLMs) become increasingly integrated into daily life, the need for accurate visual culture understanding is becoming critical. Yet, these models frequently fall short in interpreting cultural nuances effectively. Prior work has demonstrated the effectiveness of retrieval-augmented generation (RAG) in enhancing cultural understanding in text-only settings, while its application in multimodal scenarios remains underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a new benchmark designed to advance visual culture understanding through retrieval, focusing on two tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). RAVENEA extends existing datasets by integrating over 10,000 Wikipedia documents curated and ranked by human annotators. With RAVENEA, we train and evaluate seven multimodal retrievers for each image query, and measure the downstream impact of retrieval-augmented inputs across fourteen state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented with culture-aware retrieval, outperform their non-augmented counterparts (by at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the value of retrieval-augmented methods and culturally inclusive benchmarks for multimodal understanding.', 'score': 3, 'issue_id': 3926, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '49a96a5bbddf2965', 'authors': ['Jiaang Li', 'Yifei Yuan', 'Wenyan Li', 'Mohammad Aliannejadi', 'Daniel Hershcovich', 'Anders S√∏gaard', 'Ivan Vuliƒá', 'Wenxuan Zhang', 'Paul Pu Liang', 'Yang Deng', 'Serge Belongie'], 'affiliations': ['ETH Z√ºrich', 'Massachusetts Institute of Technology', 'Singapore Management University', 'Singapore University of Technology and Design', 'University of Amsterdam', 'University of Cambridge', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2505.14462.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#interpretability', '#multimodal', '#rag', '#games', '#benchmark'], 'emoji': 'üåç', 'ru': {'title': '–ö—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ò–ò', 'desc': 'RAVENEA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫—É–ª—å—Ç—É—Ä—ã –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∑–∞–¥–∞—á–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫—É–ª—å—Ç—É—Ä–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º (cVQA) –∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (cIC). –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 10 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 3.2% –≤ cVQA –∏ 6.2% –≤ cIC.'}, 'en': {'title': 'Enhancing Visual Culture Understanding with RAVENEA', 'desc': 'RAVENEA is a new benchmark designed to improve visual culture understanding in vision-language models (VLMs) through retrieval-augmented techniques. It focuses on two main tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). By integrating over 10,000 curated Wikipedia documents, RAVENEA enhances the training and evaluation of multimodal retrievers for image queries. The results show that VLMs using this retrieval-augmented approach significantly outperform those that do not, demonstrating the importance of culturally aware data in machine learning.'}, 'zh': {'title': 'RAVENEAÔºöÊèêÂçáËßÜËßâÊñáÂåñÁêÜËß£ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Âü∫ÂáÜ', 'desc': 'RAVENEAÊòØ‰∏Ä‰∏™Â¢ûÂº∫ËßÜËßâÊñáÂåñÁêÜËß£ÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éÊñáÂåñÁõ∏ÂÖ≥ÁöÑ‰ªªÂä°ÔºåÊèêÂçá‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÈÄöËøáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊñπÊ≥ïÔºåËß£ÂÜ≥‰∫ÜÊ®°ÂûãÂú®ÁêÜËß£ÊñáÂåñÁªÜÂæÆÂ∑ÆÂà´ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇRAVENEAÂåÖÂê´Ë∂ÖËøá10,000‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÂíåÊéíÂêçÁöÑÁª¥Âü∫ÁôæÁßëÊñáÊ°£ÔºåÊîØÊåÅÊñáÂåñËÅöÁÑ¶ÁöÑËßÜËßâÈóÆÁ≠îÂíåÂõæÂÉèÊèèËø∞‰ªªÂä°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁªèËøáÊñáÂåñÊÑèËØÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁöÑËΩªÈáèÁ∫ßVLMsÂú®Â§ö‰∏™ÊåáÊ†á‰∏ä‰ºò‰∫éÊú™Â¢ûÂº∫ÁöÑÊ®°ÂûãÔºåÊòæÁ§∫‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÁêÜËß£‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14395', 'title': 'MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language', 'url': 'https://huggingface.co/papers/2505.14395', 'abstract': "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks (r > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.", 'score': 3, 'issue_id': 3921, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '88fb6eaee50e6cb2', 'authors': ['Seyoung Song', 'Seogyeong Jeong', 'Eunsu Kim', 'Jiho Jin', 'Dongkwan Kim', 'Jay Shin', 'Alice Oh'], 'affiliations': ['KAIST', 'Trillion Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.14395.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#low_resource', '#multilingual'], 'emoji': 'üåê', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤', 'desc': 'MUG-Eval - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –û–Ω —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –∏–∑–º–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å LLM –ø—Ä–∏ –∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —è–∑—ã–∫–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –Ω–µ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –æ—Ü–µ–Ω–∫—É –¥—Ä—É–≥–∏–º–∏ LLM. MUG-Eval –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ —ç—Ç–∞–ª–æ–Ω–∞–º–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'MUG-Eval: A Language-Independent Tool for Multilingual LLM Assessment', 'desc': 'MUG-Eval is a new framework designed to evaluate the multilingual generation capabilities of large language models (LLMs). It transforms existing benchmarks into conversational tasks, allowing for a language-independent assessment of LLM performance. By measuring the success rate of these tasks, MUG-Eval provides a proxy for effective conversation generation without relying on specific NLP tools or annotated datasets. The framework has been tested on 8 LLMs across 30 languages and shows strong correlation with established benchmarks, making it a valuable tool for evaluating LLMs in low-resource languages.'}, 'zh': {'title': 'MUG-EvalÔºöÂ§öËØ≠Ë®ÄÁîüÊàêÁöÑËØÑ‰º∞Êñ∞ÊñπÊ≥ï', 'desc': 'MUG-EvalÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ§öËØ≠Ë®ÄÁîüÊàêËÉΩÂäõÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÂÆÉÈÄöËøáÂ∞ÜÁé∞ÊúâÂü∫ÂáÜËΩ¨Âåñ‰∏∫ÂØπËØù‰ªªÂä°ÔºåÊù•ÊµãÈáèLLMsÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÁâπÂÆöËØ≠Ë®ÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÊàñÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºåÈÄÇÁî®‰∫éÂ§öÁßçËØ≠Ë®Ä„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåMUG-Eval‰∏éÂ∑≤ÊúâÂü∫ÂáÜÁöÑÁõ∏ÂÖ≥ÊÄßÂæàÂº∫ÔºåËÉΩÂ§ü‰∏∫Â§öËØ≠Ë®ÄÁîüÊàêÊèê‰æõÊ†áÂáÜÂåñÁöÑÊØîËæÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16170', 'title': 'When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction', 'url': 'https://huggingface.co/papers/2505.16170', 'abstract': 'LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.  \t\t\t\t\tAI-generated summary \t\t\t\t Can large language models (LLMs) admit their mistakes when they should know better? In this work, we define the behavior of acknowledging errors in previously generated answers as "retraction" and aim to understand when and why LLMs choose to retract. We first construct model-specific datasets to evaluate whether a model will retract an incorrect answer that contradicts its own parametric knowledge. While LLMs are capable of retraction, they do so only infrequently. We demonstrate that retraction is closely tied to previously identified indicators of models\' internal belief: models fail to retract wrong answers that they "believe" to be factually correct. Steering experiments further demonstrate that internal belief causally influences model retraction. In particular, when the model does not believe its answer, this not only encourages the model to attempt to verify the answer, but also alters attention behavior during self-verification. Finally, we demonstrate that simple supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. Code and datasets are available on https://github.com/ayyyq/llm-retraction.', 'score': 2, 'issue_id': 3919, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': '2f89bb0f5c59846a', 'authors': ['Yuqing Yang', 'Robin Jia'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.16170.jpg', 'data': {'categories': ['#training', '#alignment', '#hallucinations', '#dataset'], 'emoji': 'ü§î', 'ru': {'title': '–£—á–∏–º –ò–ò –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å —Å–≤–æ–∏ –æ—à–∏–±–∫–∏', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å —Å–≤–æ–∏ –æ—à–∏–±–∫–∏, —á—Ç–æ –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç '—Ä–µ—Ç—Ä–∞–∫—Ü–∏–µ–π'. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM —Ä–µ–¥–∫–æ –æ—Ç–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Ç –Ω–µ–≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ —Å—á–∏—Ç–∞—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ä–µ—Ç—Ä–∞–∫—Ü–∏—è —Ç–µ—Å–Ω–æ —Å–≤—è–∑–∞–Ω–∞ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ —É–±–µ–∂–¥–µ–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏. –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ä–µ—Ç—Ä–∞–∫—Ü–∏–∏, –ø–æ–º–æ–≥–∞—è –∏–º —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è."}, 'en': {'title': 'Enhancing LLMs: Fine-Tuning for Better Error Retraction', 'desc': "This paper investigates how large language models (LLMs) handle the acknowledgment of their mistakes, a behavior termed 'retraction'. It finds that LLMs rarely retract incorrect answers, especially when they are confident in their incorrect beliefs. The study shows that the ability to retract is influenced by the model's internal beliefs, where models are less likely to retract answers they consider factually correct. Additionally, the authors demonstrate that supervised fine-tuning can enhance the retraction capabilities of LLMs by refining their internal belief systems."}, 'zh': {'title': 'ÊèêÂçáÊ®°ÂûãÁöÑÈîôËØØÊí§ÂõûËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Èù¢ÂØπÈîôËØØÊó∂ÁöÑËá™Êàë‰øÆÊ≠£ËÉΩÂäõÔºåÁß∞‰πã‰∏∫‚ÄúÊí§Âõû‚Äù„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåLLMsÂú®ËÆ§‰∏∫Ëá™Â∑±ÁöÑÁ≠îÊ°àÊòØÊ≠£Á°ÆÊó∂ÔºåÂæÄÂæÄ‰∏ç‰ºöÊí§ÂõûÈîôËØØÁöÑÂõûÁ≠î„ÄÇÈÄöËøáÊûÑÂª∫ÁâπÂÆöÁöÑÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÊ®°ÂûãÂú®‰ΩïÁßçÊÉÖÂÜµ‰∏ã‰ºöËøõË°åÊí§ÂõûÔºåÂπ∂ÂèëÁé∞ÂÜÖÈÉ®‰ø°ÂøµÂØπÊí§ÂõûË°å‰∏∫ÊúâÈáçË¶ÅÂΩ±Âìç„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊí§ÂõûÊÄßËÉΩÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂ≠¶‰π†Êõ¥ÂáÜÁ°ÆÁöÑÂÜÖÈÉ®‰ø°Âøµ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16088', 'title': 'Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.16088', 'abstract': 'Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future regimes; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year rightarrow month rightarrow day).', 'score': 2, 'issue_id': 3920, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 –º–∞—è', 'en': 'May 22', 'zh': '5Êúà22Êó•'}, 'hash': 'e95fb5fdc583b428', 'authors': ['Gagan Bhatia', 'Maxime Peyrard', 'Wei Zhao'], 'affiliations': ['University of Aberdeen', 'Universit√© Grenoble Alpes & CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2505.16088.jpg', 'data': {'categories': ['#reasoning', '#data', '#dataset', '#benchmark', '#interpretability'], 'emoji': 'üóìÔ∏è', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—Ç –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç—Ä–∏–∫—É '–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—Ç' –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö DateAugBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –¥–∞—Ç –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–∞—Ç. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–∂–µ—Ç —Å–Ω–∏–∂–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–¥–∫–∏—Ö –¥–∞—Ç–∞—Ö, –∞ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–µ–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–µ–π –¥–∞—Ç."}, 'en': {'title': 'Preserving Date Integrity for Better Temporal Reasoning', 'desc': 'This paper addresses the issue of how modern Byte Pair Encoding (BPE) tokenizers break down calendar dates into smaller parts, which can hinder effective temporal reasoning in language models. The authors propose a new metric called the date fragmentation ratio to evaluate how well tokenizers maintain the integrity of multi-digit date components. They also introduce DateAugBench, a dataset designed for testing temporal reasoning across various tasks involving dates. The findings reveal that excessive fragmentation can lead to significant drops in accuracy, especially for less common dates, and that larger models are better at reconstructing these fragmented dates for reasoning tasks.'}, 'zh': {'title': 'ÊèêÂçáÊó•ÊúüÊé®ÁêÜÁöÑÂàÜËØçÂô®ËÆæËÆ°', 'desc': 'Áé∞‰ª£ÁöÑBPEÂàÜËØçÂô®Â∏∏Â∏∏Â∞ÜÊó•ÊúüÂàÜÂâ≤ÊàêÊó†ÊÑè‰πâÁöÑÁ¢éÁâáÔºå‰æãÂ¶ÇÂ∞Ü20250312ÂàÜÂâ≤‰∏∫202„ÄÅ503Âíå12ÔºåËøôÊ†∑‰ºöÂ¢ûÂä†Ê†áËÆ∞Êï∞ÈáèÂπ∂Êé©ÁõñËøõË°åÊó∂Èó¥Êé®ÁêÜÊâÄÈúÄÁöÑÂÜÖÂú®ÁªìÊûÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰∏îÂèØËß£ÈáäÁöÑÂ∫¶ÈáèÊ†áÂáÜÔºåÁß∞‰∏∫Êó•ÊúüÁ¢éÁâáÂåñÊØîÁéáÔºåÁî®‰∫éË°°ÈáèÂàÜËØçÂô®‰øùÁïôÂ§ö‰ΩçÊï∞Êó•ÊúüÁªÑ‰ª∂ÁöÑÂø†ÂÆûÂ∫¶„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜDateAugBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´6500‰∏™Á§∫‰æãÁöÑÂ•ó‰ª∂ÔºåÊ∂µÁõñ‰∫Ü‰∏âÁßçÊó∂Èó¥Êé®ÁêÜ‰ªªÂä°ÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊó•ÊúüËß£Êûê„ÄÅÊ†ºÂºè‰∏çÂèòÊÄßÈöæÈ¢òÂíåÂéÜÂè≤„ÄÅÂΩì‰ª£ÂèäÊú™Êù•ÁöÑÊó•ÊúüÁÆóÊúØ„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøáÂ∫¶ÁöÑÁ¢éÁâáÂåñ‰∏é‰∏çÂ∏∏ËßÅÊó•ÊúüÔºàÂ¶ÇÂéÜÂè≤ÂíåÊú™Êù•Êó•ÊúüÔºâÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôçÈ´òËææ10‰∏™ÁôæÂàÜÁÇπÁõ∏ÂÖ≥Ôºå‰∏îÊ®°ÂûãË∂äÂ§ßÔºå‰øÆÂ§çÊó•ÊúüÁ¢éÁâáÁöÑÊäΩË±°Êú∫Âà∂Ë∂äÂø´„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.15263', 'title': 'gen2seg: Generative Models Enable Generalizable Instance Segmentation', 'url': 'https://huggingface.co/papers/2505.15263', 'abstract': "Generative models fine-tuned for instance segmentation demonstrate strong zero-shot performance on unseen objects and styles, surpassing discriminatively pretrained models.  \t\t\t\t\tAI-generated summary \t\t\t\t By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website.", 'score': 2, 'issue_id': 3936, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': '01de6ec5d8c92cb4', 'authors': ['Om Khangaonkar', 'Hamed Pirsiavash'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.15263.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#cv', '#synthetic'], 'emoji': 'üß†', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–∫—Ä—ã—Ç—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç–∏–ø–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å—Ç–∏–ª–µ–π. –¢–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –ø—Ä–∏–±–ª–∏–∂–∞—é—Ç—Å—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é supervised –º–æ–¥–µ–ª—è–º –≤—Ä–æ–¥–µ SAM. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –Ω–µ—è–≤–Ω–æ –∏–∑—É—á–∞—é—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –ø—Ä–∏–º–µ–Ω–∏–º—ã–µ –∫ —Ä–∞–∑–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –¥–æ–º–µ–Ω–∞–º.'}, 'en': {'title': 'Generative Models Excel in Zero-Shot Instance Segmentation', 'desc': 'This paper explores how generative models, specifically those fine-tuned for instance segmentation, can effectively identify and segment objects that were not seen during training. By leveraging a technique called instance coloring loss, the authors adapt models like Stable Diffusion and MAE to work with a limited set of object types, such as indoor furniture and cars. Remarkably, these models demonstrate strong zero-shot performance, meaning they can accurately segment new object types and styles that they have never encountered before. The findings suggest that generative models possess a robust understanding of object boundaries and scene composition, allowing them to generalize better than traditional discriminative models.'}, 'zh': {'title': 'ÁîüÊàêÊ®°ÂûãÂú®ÂÆû‰æãÂàÜÂâ≤‰∏≠ÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÊ®°ÂûãÂú®ÂÆû‰æãÂàÜÂâ≤‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®Êú™ËßÅÁâ©‰ΩìÂíåÈ£éÊ†º‰∏äÁöÑÈõ∂-shotÊÄßËÉΩË°®Áé∞„ÄÇÈÄöËøáÂØπÁîüÊàêÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂèëÁé∞Ëøô‰∫õÊ®°ÂûãËÉΩÂ§üÊúâÊïàÁêÜËß£Áâ©‰ΩìËæπÁïåÂíåÂú∫ÊôØÊûÑÊàê„ÄÇËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜÂÆû‰æãÁùÄËâ≤ÊçüÂ§±Ôºå‰∏ìÊ≥®‰∫éÂÆ§ÂÜÖÂÆ∂ÂÖ∑ÂíåÊ±ΩËΩ¶Á≠âÁâπÂÆöÁâ©‰ΩìÁ±ªÂûãÔºåÁªìÊûúÊòæÁ§∫Ê®°ÂûãÂú®Êú™ËßÅÁâ©‰Ωì‰∏ä‰πüËÉΩÂáÜÁ°ÆÂàÜÂâ≤„ÄÇ‰∏é‰º†ÁªüÁöÑÂà§Âà´ÂºèÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁõ∏ÊØîÔºåÁîüÊàêÊ®°ÂûãÂ±ïÁé∞Âá∫Êõ¥Âº∫ÁöÑË∑®Á±ªÂà´ÂíåÈ¢ÜÂüüÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.16048', 'title': 'SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution', 'url': 'https://huggingface.co/papers/2505.16048', 'abstract': 'A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.', 'score': 1, 'issue_id': 3920, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 –º–∞—è', 'en': 'May 21', 'zh': '5Êúà21Êó•'}, 'hash': 'cf44ff3c901f7498', 'authors': ['Philipp D. Siedler'], 'affiliations': ['Aleph Alpha Research, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.16048.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset'], 'emoji': 'üèóÔ∏è', 'ru': {'title': '–ò—Å–ø—ã—Ç–∞–Ω–∏–µ –ò–ò –Ω–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç—å: —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–∏–º—É–ª—è—Ü–∏–π', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –≤ 2D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –Ω–∞–≥—Ä—É–∑–∫–∞—Ö –∏ –æ–ø–æ—Ä–∞—Ö. –ó–∞–¥–∞—á–∏ –≤–∫–ª—é—á–∞—é—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –≤ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª–∞. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏.'}, 'en': {'title': 'Benchmarking LLMs in Spatial and Physical Reasoning with Topology Optimization', 'desc': 'This paper presents a new dataset aimed at evaluating the spatial and physical reasoning skills of Large Language Models (LLMs) through topology optimization tasks. The tasks require LLMs to analyze conditions like 2D boundaries and applied forces to determine optimal material distributions without using simulation tools. By challenging models to predict material layouts and understand force flows, the dataset assesses their ability to reason about structural stability and spatial organization. This approach provides a unique perspective on LLM performance, complementing existing benchmarks focused on language and logic.'}, 'zh': {'title': 'ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥‰∏éÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞Êï∞ÊçÆÈõÜÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÊãìÊâë‰ºòÂåñ‰ªªÂä°‰∏≠ÁöÑÁ©∫Èó¥ÂíåÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Êï∞ÊçÆÈõÜÊèê‰æõ‰∫Ü2DËæπÁïå„ÄÅÊñΩÂä†ÁöÑÂäõÂíåÊîØÊíëÊù°‰ª∂ÔºåLLMÈúÄË¶ÅÊé®ÁêÜÂá∫ÊúÄ‰Ω≥ÁöÑÊùêÊñôÂàÜÂ∏É„ÄÇ‰ªªÂä°ÂåÖÊã¨Â°´Ë°•ÈÉ®ÂàÜÁªìÊûÑ‰∏≠ÁöÑÁº∫Â§±Âå∫ÂüüÂíåÈ¢ÑÊµãÂÆåÊï¥ÁöÑÊùêÊñôÂàÜÂ∏ÉÔºåË¶ÅÊ±ÇÊ®°ÂûãÁêÜËß£Âú®ÁªôÂÆöÁ∫¶Êùü‰∏ãÁöÑÂäõÊµÅÂíåÊùêÊñôÂàÜÂ∏É„ÄÇËøô‰∏™Êï∞ÊçÆÈõÜ‰∏∫ËØÑ‰º∞2DÁéØÂ¢É‰∏≠ÁöÑÁ©∫Èó¥ÂíåÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßíÔºåË°•ÂÖÖ‰∫Ü‰º†ÁªüÁöÑËØ≠Ë®ÄÂíåÈÄªËæëÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13237', 'title': 'SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based\n  on Speech and Audio Information', 'url': 'https://huggingface.co/papers/2505.13237', 'abstract': "SAKURA is introduced to evaluate the multi-hop reasoning abilities of large audio-language models, revealing their struggles in integrating speech/audio representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.", 'score': 0, 'issue_id': 3928, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '657c71fe76ca3155', 'authors': ['Chih-Kai Yang', 'Neo Ho', 'Yen-Ting Piao', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2505.13237.jpg', 'data': {'categories': ['#audio', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'üéôÔ∏è', 'ru': {'title': 'SAKURA: –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ª–æ–≥–∏–∫–∏ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'SAKURA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LALM) –∫ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ LALM –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —Ä–µ—á–µ–≤—ã—Ö –∏ –∞—É–¥–∏–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤, –¥–∞–∂–µ –∫–æ–≥–¥–∞ –æ–Ω–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –≠—Ç–æ –≤—ã—è–≤–∏–ª–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –≤–∞–∂–Ω—ã–µ insights –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'SAKURA: Unveiling the Reasoning Gaps in Audio-Language Models', 'desc': 'The paper introduces SAKURA, a benchmark designed to evaluate the multi-hop reasoning capabilities of large audio-language models (LALMs). It highlights that while LALMs perform well in speech and audio tasks, their ability to integrate multiple pieces of information for reasoning is not well understood. The study reveals that LALMs face significant challenges in combining speech and audio representations for effective multi-hop reasoning. This research identifies a key limitation in LALMs and provides valuable insights for future advancements in multimodal reasoning.'}, 'zh': {'title': 'SAKURAÔºöËØÑ‰º∞Èü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öË∑≥Êé®ÁêÜËÉΩÂäõ', 'desc': 'SAKURAÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öË∑≥Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËøô‰∫õÊ®°ÂûãÂú®Â§ÑÁêÜËØ≠Èü≥ÂíåÈü≥È¢ë‰ªªÂä°Êó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Êï¥ÂêàÂ§öÊù°‰ø°ÊÅØËøõË°åÊé®ÁêÜÊó∂Âç¥Â≠òÂú®Âõ∞Èöæ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÂÆÉ‰ª¨ËÉΩÂ§üÊ≠£Á°ÆÊèêÂèñÁõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ªçÁÑ∂Èöæ‰ª•Â∞ÜËØ≠Èü≥ÂíåÈü≥È¢ëË°®Á§∫ÁªìÂêàËµ∑Êù•ËøõË°åÂ§öË∑≥Êé®ÁêÜ„ÄÇËøô‰∏ÄÂèëÁé∞Êè≠Á§∫‰∫ÜÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàòÔºå‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£ÂíåËµÑÊ∫ê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20438', 'title': 'PixelHacker: Image Inpainting with Structural and Semantic Consistency', 'url': 'https://huggingface.co/papers/2504.20438', 'abstract': 'Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/PixelHacker.', 'score': 23, 'issue_id': 3584, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': '987ce511e3c86e06', 'authors': ['Ziyang Xu', 'Kangsheng Duan', 'Xiaolei Shen', 'Zhifeng Ding', 'Wenyu Liu', 'Xiaohu Ruan', 'Xiaoxin Chen', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'VIVO AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.20438.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#open_source', '#optimization', '#diffusion', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'PixelHacker: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PixelHacker. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 14 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–∞—Å–∫–∞ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –ø–µ—Ä–µ–¥–Ω–µ–≥–æ –∏ –∑–∞–¥–Ω–µ–≥–æ –ø–ª–∞–Ω–∞. PixelHacker –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PixelHacker –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'PixelHacker: Revolutionizing Image Inpainting with Latent Categories Guidance', 'desc': 'This paper presents a new approach to image inpainting called PixelHacker, which aims to improve the quality of generated images by addressing issues with complex structures and semantics. The authors introduce a large dataset of 14 million image-mask pairs to train their model, focusing on distinguishing between foreground and background categories. They utilize a diffusion-based model that incorporates linear attention to enhance the denoising process, ensuring better consistency in texture and color. Experimental results demonstrate that PixelHacker significantly outperforms existing state-of-the-art methods across various datasets, achieving superior image restoration results.'}, 'zh': {'title': 'PixelHackerÔºöÂõæÂÉè‰øÆÂ§çÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'ÂõæÂÉè‰øÆÂ§çÊòØÂõæÂÉèÁºñËæë‰∏éÁîüÊàê‰πãÈó¥ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÁ†îÁ©∂È¢ÜÂüü„ÄÇÊúÄËøëÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÊé¢Á¥¢‰∫ÜÊñ∞È¢ñÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÅËΩªÈáèÁ∫ßÊû∂ÊûÑÂíå‰∏ä‰∏ãÊñáÊÑüÁü•Âª∫Ê®°ÔºåÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÁªìÊûÑÂíåËØ≠‰πâÊó∂Â∏∏Â∏∏Èù¢‰∏¥ÊåëÊàòÔºåÂØºËá¥ÁîüÊàêÁöÑÂõæÂÉèÂá∫Áé∞‰º™ÂΩ±Âíå‰∏çÂΩìÁîüÊàê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑ‰øÆÂ§çËåÉÂºèÔºåÁß∞‰∏∫ÊΩúÂú®Á±ªÂà´ÂºïÂØºÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÁöÑÊ®°ÂûãPixelHacker„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01079', 'title': 'Improving Editability in Image Generation with Layer-wise Memory', 'url': 'https://huggingface.co/papers/2505.01079', 'abstract': 'Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.', 'score': 18, 'issue_id': 3582, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 –º–∞—è', 'en': 'May 2', 'zh': '5Êúà2Êó•'}, 'hash': 'e1aa83ea7926943e', 'authors': ['Daneul Kim', 'Jaeah Lee', 'Jaesik Park'], 'affiliations': ['Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01079.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–£–º–Ω–æ–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—à–ª–æ–µ, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Å–æ–∫, –ø–æ—Å–ª–æ–π–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫ Background Consistency Guidance –∏ Multi-Query Disentanglement. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ –Ω–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Seamless Sequential Image Editing with Context Preservation', 'desc': 'This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input.'}, 'zh': {'title': 'ÂÆûÁé∞Ëá™ÁÑ∂ËøûÁª≠ÁöÑÂõæÂÉèÁºñËæë', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂõæÂÉèÁºñËæë‰∏≠ÁöÑÂ§öÊ¨°ËøûÁª≠ÁºñËæëÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§ö‰∏™ÂØπË±°ÁöÑ‰øÆÊîπÊó∂Â≠òÂú®Âõ∞ÈöæÔºåÂ∞§ÂÖ∂ÊòØÂú®‰øùÊåÅ‰πãÂâçÁºñËæëÂÜÖÂÆπÁöÑÂêåÊó∂Ëá™ÁÑ∂Âú∞ËûçÂÖ•Êñ∞ÂØπË±°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§È°πÂÖ≥ÈîÆÊñπÊ°àÔºö‰∏ÄÊòØÊîØÊåÅÁ≤óÁï•ÁöÑÊé©ËÜúËæìÂÖ•Ôºå‰ª•‰øùÁïôÁé∞ÊúâÂÜÖÂÆπÂπ∂Ëá™ÁÑ∂Êï¥ÂêàÊñ∞ÂÖÉÁ¥†Ôºõ‰∫åÊòØÊîØÊåÅÂ§öÊ¨°‰øÆÊîπÁöÑ‰∏ÄËá¥ÊÄßÁºñËæë„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÈÄöËøáÂ±ÇÁ∫ßËÆ∞ÂøÜÂ≠òÂÇ®ÂÖàÂâçÁºñËæëÁöÑÊΩúÂú®Ë°®Á§∫ÂíåÊèêÁ§∫ÂµåÂÖ•ÔºåÂà©Áî®ËÉåÊôØ‰∏ÄËá¥ÊÄßÂºïÂØº‰øùÊåÅÂú∫ÊôØÁöÑËøûË¥ØÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Ëø≠‰ª£ÂõæÂÉèÁºñËæë‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÁî®Êà∑Âè™ÈúÄÊèê‰æõÁ≤óÁï•Êé©ËÜúÂç≥ÂèØÂÆûÁé∞È´òË¥®ÈáèÁöÑÁºñËæëÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21117', 'title': 'Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts', 'url': 'https://huggingface.co/papers/2504.21117', 'abstract': 'Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.', 'score': 11, 'issue_id': 3587, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': '2a43e27932acf80e', 'authors': ['Hanhua Hong', 'Chenghao Xiao', 'Yang Wang', 'Yiqi Liu', 'Wenge Rong', 'Chenghua Lin'], 'affiliations': ['Beihang University', 'Durham University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2504.21117.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#benchmark', '#interpretability'], 'emoji': 'üîÑ', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ NLG —Å–∏—Å—Ç–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLG) –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è –∏–Ω–≤–µ—Ä—Å–∏–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –æ–±—Ä–∞–∑–µ—Ü. –ú–µ—Ç–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç—Ä—É–¥–æ–µ–º–∫–æ–π —Ä—É—á–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–º–ø—Ç–æ–≤, –ø–æ–≤—ã—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏. –†–∞–±–æ—Ç–∞ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM).'}, 'en': {'title': 'Revolutionizing NLG Evaluation with Inversion Learning', 'desc': 'This paper addresses the difficulties in evaluating natural language generation (NLG) systems, particularly the inconsistencies and biases in human evaluations. It introduces an inversion learning method that creates effective prompts for evaluating models by learning from their outputs. This approach allows for automatic generation of tailored evaluation prompts, requiring only one sample, which enhances efficiency. The proposed method aims to improve the robustness of LLM-based evaluations, paving the way for more standardized assessment in NLG.'}, 'zh': {'title': 'ÊèêÂçáËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêËØÑ‰º∞ÁöÑÊïàÁéá‰∏éÁ®≥ÂÅ•ÊÄß', 'desc': 'ËØÑ‰º∞Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÔºàNLGÔºâÁ≥ªÁªüÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°ÔºåÂõ†‰∏∫ÊúâÊïàËæìÂá∫ÁöÑÂ§öÊ†∑ÊÄß‰ΩøÂæóÊ†áÂáÜÂåñËØÑ‰º∞ÂèòÂæóÂõ∞Èöæ„ÄÇËôΩÁÑ∂‰∫∫Â∑•ËØÑ‰º∞Ë¢´ËÆ§‰∏∫ÊòØÈáëÊ†áÂáÜÔºå‰ΩÜÂÖ∂Â≠òÂú®‰∏ç‰∏ÄËá¥ÊÄß„ÄÅÁº∫‰πèÊ†áÂáÜÂåñÂíå‰∫∫Âè£ÂÅèËßÅÁ≠âÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂèØÈáçÂ§çÊÄß„ÄÇÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËØÑ‰º∞Êèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊõø‰ª£ÊñπÊ°àÔºå‰ΩÜÂØπÊèêÁ§∫ËÆæËÆ°ÈùûÂ∏∏ÊïèÊÑüÔºåÂæÆÂ∞èÁöÑÂèòÂåñÂèØËÉΩÂØºËá¥ÊòæËëóÁöÑÂ∑ÆÂºÇ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèçÊºîÂ≠¶‰π†ÊñπÊ≥ïÔºåÂèØ‰ª•ÊúâÊïàÂú∞‰ªéÊ®°ÂûãËæìÂá∫ÂèçÂêëÊò†Â∞ÑÂà∞ËæìÂÖ•Êåá‰ª§Ôºå‰ªéËÄåËá™Âä®ÁîüÊàêÈ´òÊïàÁöÑ„ÄÅÁâπÂÆö‰∫éÊ®°ÂûãÁöÑËØÑ‰º∞ÊèêÁ§∫ÔºåÊèêÂçá‰∫ÜËØÑ‰º∞ÁöÑÊïàÁéáÂíåÁ®≥ÂÅ•ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00949', 'title': 'Llama-Nemotron: Efficient Reasoning Models', 'url': 'https://huggingface.co/papers/2505.00949', 'abstract': 'We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.', 'score': 8, 'issue_id': 3587, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 –º–∞—è', 'en': 'May 2', 'zh': '5Êúà2Êó•'}, 'hash': 'cbc28025b0c6bde3', 'authors': ['Akhiad Bercovich', 'Itay Levy', 'Izik Golan', 'Mohammad Dabbah', 'Ran El-Yaniv', 'Omri Puny', 'Ido Galil', 'Zach Moshe', 'Tomer Ronen', 'Najeeb Nabwani', 'Ido Shahaf', 'Oren Tropp', 'Ehud Karpas', 'Ran Zilberstein', 'Jiaqi Zeng', 'Soumye Singhal', 'Alexander Bukharin', 'Yian Zhang', 'Tugrul Konuk', 'Gerald Shen', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Yoshi Suhara', 'Olivier Delalleau', 'Zijia Chen', 'Zhilin Wang', 'David Mosallanezhad', 'Adi Renduchintala', 'Haifeng Qian', 'Dima Rekesh', 'Fei Jia', 'Somshubra Majumdar', 'Vahid Noroozi', 'Wasi Uddin Ahmad', 'Sean Narenthiran', 'Aleksander Ficek', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Igor Gitman', 'Ivan Moshkov', 'Wei Du', 'Shubham Toshniwal', 'George Armstrong', 'Branislav Kisacanin', 'Matvei Novikov', 'Daria Gitman', 'Evelina Bakhturina', 'Jane Polak Scowcroft', 'John Kamalu', 'Dan Su', 'Kezhi Kong', 'Markus Kliegl', 'Rabeeh Karimi', 'Ying Lin', 'Sanjeev Satheesh', 'Jupinder Parmar', 'Pritam Gundecha', 'Brandon Norick', 'Joseph Jennings', 'Shrimai Prabhumoye', 'Syeda Nahida Akter', 'Mostofa Patwary', 'Abhinav Khattar', 'Deepak Narayanan', 'Roger Waleffe', 'Jimmy Zhang', 'Bor-Yiing Su', 'Guyue Huang', 'Terry Kong', 'Parth Chadha', 'Sahil Jain', 'Christine Harvey', 'Elad Segal', 'Jining Huang', 'Sergey Kashirsky', 'Robert McQueen', 'Izzy Putterman', 'George Lam', 'Arun Venkatesan', 'Sherry Wu', 'Vinh Nguyen', 'Manoj Kilaru', 'Andrew Wang', 'Anna Warno', 'Abhilash Somasamudramath', 'Sandip Bhaskar', 'Maka Dong', 'Nave Assaf', 'Shahar Mor', 'Omer Ullman Argov', 'Scot Junkin', 'Oleksandr Romanenko', 'Pedro Larroy', 'Monika Katariya', 'Marco Rovinelli', 'Viji Balas', 'Nicholas Edelman', 'Anahita Bhiwandiwalla', 'Muthu Subramaniam', 'Smita Ithape', 'Karthik Ramamoorthy', 'Yuting Wu', 'Suguna Varshini Velury', 'Omri Almog', 'Joyjit Daw', 'Denys Fridman', 'Erick Galinkin', 'Michael Evans', 'Katherine Luna', 'Leon Derczynski', 'Nikki Pope', 'Eileen Long', 'Seth Schneider', 'Guillermo Siman', 'Tomasz Grzegorzek', 'Pablo Ribalta', 'Monika Katariya', 'Joey Conway', 'Trisha Saar', 'Ann Guan', 'Krzysztof Pawelec', 'Shyamala Prayaga', 'Oleksii Kuchaiev', 'Boris Ginsburg', 'Oluwatobi Olabiyi', 'Kari Briski', 'Jonathan Cohen', 'Bryan Catanzaro', 'Jonah Alben', 'Yonatan Geifman', 'Eric Chung'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00949.jpg', 'data': {'categories': ['#agi', '#training', '#rl', '#open_source', '#architecture', '#dataset', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Llama-Nemotron - —Å–µ–º–µ–π—Å—Ç–≤–æ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –ú–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ç—Ä–µ—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö (8B, 49B, 253B) –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –û–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫, –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ —ç—Ç–∞–ø –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏ –æ–±—ã—á–Ω–æ–≥–æ —á–∞—Ç–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Efficiency', 'desc': 'The Llama-Nemotron series introduces a new family of reasoning models designed for efficient inference and strong reasoning capabilities. These models come in three sizes, allowing flexibility for different applications while maintaining competitive performance against leading models. The training process involves advanced techniques like neural architecture search, knowledge distillation, and reinforcement learning to enhance reasoning abilities. Additionally, these models are open-source, providing resources for further research and development in the machine learning community.'}, 'zh': {'title': 'ÂºÄÊîæÊé®ÁêÜÊ®°ÂûãÔºåÊèêÂçáÊé®ÁêÜÊïàÁéáÔºÅ', 'desc': 'Llama-NemotronÁ≥ªÂàóÊ®°ÂûãÊòØ‰∏ÄÁßçÂºÄÊîæÁöÑÂºÇÊûÑÊé®ÁêÜÊ®°ÂûãÔºåÂÖ∑ÊúâÂçìË∂äÁöÑÊé®ÁêÜËÉΩÂäõÂíåÈ´òÊïàÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇËØ•Á≥ªÂàóÂåÖÊã¨‰∏âÁßç‰∏çÂêåËßÑÊ®°ÁöÑÊ®°ÂûãÔºöNanoÔºà8BÔºâ„ÄÅSuperÔºà49BÔºâÂíåUltraÔºà253BÔºâÔºåÂú®Êé®ÁêÜÈÄüÂ∫¶ÂíåÂÜÖÂ≠òÊïàÁéá‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°Âûã„ÄÇÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ãÈááÁî®‰∫ÜÁ•ûÁªèÊû∂ÊûÑÊêúÁ¥¢„ÄÅÁü•ËØÜËí∏È¶èÂíåÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºåÊúÄÂêéÈÄöËøáÁõëÁù£ÂæÆË∞ÉÂíåÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ËøõË°åÊé®ÁêÜ‰∏ìÊ≥®ÁöÑÂêéËÆ≠ÁªÉÈò∂ÊÆµ„ÄÇLlama-NemotronÊ®°ÂûãÊòØÈ¶ñ‰∏™ÊîØÊåÅÂä®ÊÄÅÊé®ÁêÜÂàáÊç¢ÁöÑÂºÄÊ∫êÊ®°ÂûãÔºåÁî®Êà∑ÂèØ‰ª•Âú®Êé®ÁêÜËøáÁ®ã‰∏≠Âú®Ê†áÂáÜËÅäÂ§©Ê®°ÂºèÂíåÊé®ÁêÜÊ®°Âºè‰πãÈó¥ÂàáÊç¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00174', 'title': 'Real-World Gaps in AI Governance Research', 'url': 'https://huggingface.co/papers/2505.00174', 'abstract': 'Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.', 'score': 8, 'issue_id': 3582, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '7618edbafcee6b13', 'authors': ['Ilan Strauss', 'Isobel Moure', "Tim O'Reilly", 'Sruly Rosenblat'], 'affiliations': ['AI Disclosures Project, Social Science Research Council', 'Institute for Innovation and Public Purpose, University College London', 'OReilly Media'], 'pdf_title_img': 'assets/pdf/title_img/2505.00174.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#alignment', '#healthcare', '#hallucinations', '#data'], 'emoji': 'üîç', 'ru': {'title': '–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ò–ò: –ø—Ä–æ–±–µ–ª—ã –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç 1178 —Ä–∞–±–æ—Ç –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏–∑ 9439 —Å—Ç–∞—Ç–µ–π –ø–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –ò–ò –∑–∞ –ø–µ—Ä–∏–æ–¥ —Å —è–Ω–≤–∞—Ä—è 2020 –ø–æ –º–∞—Ä—Ç 2025 –≥–æ–¥–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–¥—É—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ò–ò –≤—Å–µ –±–æ–ª—å—à–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–∏, –≤–∫–ª—é—á–∞—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –ø—Ä–æ–±–ª–µ–º–∞–º —ç—Ç–∞–ø–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–º –∫–∞–∫ —Å–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ—Å–ª–∞–±–µ–≤–∞–µ—Ç. –ê–≤—Ç–æ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –∫ –¥–∞–Ω–Ω—ã–º —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –ò–ò –Ω–∞ —Ä—ã–Ω–∫–µ.'}, 'en': {'title': 'Bridging the Gap: Enhancing AI Safety in Deployment', 'desc': 'This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps.'}, 'zh': {'title': 'ÂÖ≥Ê≥®‰∫∫Â∑•Êô∫ËÉΩÈÉ®ÁΩ≤Èò∂ÊÆµÁöÑÁ†îÁ©∂Áº∫Âè£', 'desc': 'Êú¨Á†îÁ©∂ÂàÜÊûê‰∫Ü1178ÁØáÂÆâÂÖ®ÊÄßÂíåÂèØÈù†ÊÄßËÆ∫Êñá‰∏é9439ÁØáÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩËÆ∫ÊñáÔºåÊØîËæÉ‰∫Ü‰∏ªË¶Å‰∫∫Â∑•Êô∫ËÉΩÂÖ¨Âè∏ÂíåÂ§ßÂ≠¶ÁöÑÁ†îÁ©∂ÊàêÊûú„ÄÇÁ†îÁ©∂ÂèëÁé∞Ôºå‰ºÅ‰∏öÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Ë∂äÊù•Ë∂äÈõÜ‰∏≠Âú®Ê®°ÂûãÂØπÈΩêÂíåÊµãËØïËØÑ‰º∞Á≠âÈ¢ÑÈÉ®ÁΩ≤È¢ÜÂüüÔºåËÄåÂØπÈÉ®ÁΩ≤Èò∂ÊÆµÈóÆÈ¢òÂ¶ÇÊ®°ÂûãÂÅèËßÅÁöÑÂÖ≥Ê≥®ÊúâÊâÄÂáèÂ∞ë„ÄÇÈ´òÈ£éÈô©ÈÉ®ÁΩ≤È¢ÜÂüüÔºàÂ¶ÇÂåªÁñó„ÄÅÈáëËûç„ÄÅËôöÂÅá‰ø°ÊÅØÁ≠âÔºâÂ≠òÂú®ÊòæËëóÁöÑÁ†îÁ©∂Á©∫ÁôΩ„ÄÇ‰∏∫‰∫ÜÊîπÂñÑÂØπÂ∑≤ÈÉ®ÁΩ≤‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèØËßÇÂØüÊÄßÔºåÂª∫ËÆÆÊâ©Â§ßÂ§ñÈÉ®Á†îÁ©∂‰∫∫ÂëòÂØπÈÉ®ÁΩ≤Êï∞ÊçÆÁöÑËÆøÈóÆÔºåÂπ∂Á≥ªÁªüÂåñÂ∏ÇÂú∫‰∏≠‰∫∫Â∑•Êô∫ËÉΩË°å‰∏∫ÁöÑÂèØËßÇÂØüÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00023', 'title': 'CORG: Generating Answers from Complex, Interrelated Contexts', 'url': 'https://huggingface.co/papers/2505.00023', 'abstract': 'In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.', 'score': 5, 'issue_id': 3582, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 –∞–ø—Ä–µ–ª—è', 'en': 'April 25', 'zh': '4Êúà25Êó•'}, 'hash': '46da290a5c894311', 'authors': ['Hyunji Lee', 'Franck Dernoncourt', 'Trung Bui', 'Seunghyun Yoon'], 'affiliations': ['Adobe Research', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.00023.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#graphs', '#architecture', '#data'], 'emoji': 'üß†', 'ru': {'title': 'CORG: –£–º–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Context Organizer (CORG) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –≤ –∫–æ—Ä–ø—É—Å–∞—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞. CORG –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –≤ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–µ –≥—Ä—É–ø–ø—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –≤—Å–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞ –≥—Ä–∞—Ñ–æ–≤, —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CORG —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.'}, 'en': {'title': 'Organizing Contexts for Better Language Understanding', 'desc': 'This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods.'}, 'zh': {'title': '‰∏ä‰∏ãÊñáÁªÑÁªáÔºåÊèêÂçáÊ®°ÂûãÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'Âú®Áé∞ÂÆû‰∏ñÁïåÁöÑËØ≠ÊñôÂ∫ì‰∏≠ÔºåÁü•ËØÜÁªèÂ∏∏Âú®ÊñáÊ°£‰∏≠ÈáçÂ§çÂá∫Áé∞Ôºå‰ΩÜÁî±‰∫éÂëΩÂêçÊ®°Á≥ä„ÄÅ‰ø°ÊÅØËøáÊó∂ÊàñÈîôËØØÔºåÂØºËá¥‰∏ä‰∏ãÊñá‰πãÈó¥Â≠òÂú®Â§çÊùÇÁöÑÁõ∏‰∫íÂÖ≥Á≥ª„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜËøô‰∫õÂ§çÊùÇÊÄßÊó∂ÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®Âçï‰∏ÄÂõ†Á¥†„ÄÇÊàë‰ª¨Â∞ÜËøô‰∫õÂÖ≥Á≥ªÂàÜ‰∏∫ÂõõÁßçÁ±ªÂûãÔºöÂπ≤Êâ∞„ÄÅÊ®°Á≥ä„ÄÅÂèç‰∫ãÂÆûÂíåÈáçÂ§ç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ä‰∏ãÊñáÁªÑÁªáÂô®ÔºàCORGÔºâÔºåÂÆÉÂ∞ÜÂ§ö‰∏™‰∏ä‰∏ãÊñáÁªÑÁªáÊàêÁã¨Á´ãÂ§ÑÁêÜÁöÑÁªÑÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00562', 'title': 'TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching', 'url': 'https://huggingface.co/papers/2505.00562', 'abstract': "Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': 'bf5b246f5848fa6e', 'authors': ['Yue Meng', 'Chuchu Fan'], 'affiliations': ['Department of Aeronautics and Astronautics, MIT, Cambridge, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00562.jpg', 'data': {'categories': ['#dataset', '#inference', '#agents', '#robotics', '#graphs', '#optimization'], 'emoji': '‚è±Ô∏è', 'ru': {'title': '–ì—Ä–∞—Ñ–æ–≤—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TeLoGraF - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á —Å —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π —Å–∏–≥–Ω–∞–ª–æ–≤ (STL). –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥—Ä–∞—Ñ–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ —Ç–µ—Ö–Ω–∏–∫—É flow-matching –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º –Ω–∞–±–æ—Ä–µ STL-—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –ø—è—Ç–∏ —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö, –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö 2D-–º–æ–¥–µ–ª–µ–π –¥–æ —Å–ª–æ–∂–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ TeLoGraF –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.'}, 'en': {'title': 'TeLoGraF: Fast and Robust Solutions for Complex Temporal Logic Tasks', 'desc': 'This paper introduces TeLoGraF, a novel approach that leverages Graph Neural Networks (GNN) to effectively learn solutions for complex tasks defined by signal temporal logic (STL) specifications. The authors address the limitations of previous methods that relied on fixed STL templates by creating a diverse dataset of 200,000 STL specifications paired with demonstrations. Through extensive experiments across various simulation environments, TeLoGraF demonstrates superior performance in STL satisfaction rates and significantly faster inference times compared to traditional STL planning algorithms. Additionally, the graph-encoding technique shows robustness in handling complex and out-of-distribution STL specifications, making it a versatile tool for real-world applications.'}, 'zh': {'title': 'TeLoGraFÔºöÈ´òÊïàËß£ÂÜ≥Â§çÊùÇÊó∂Â∫èÈÄªËæë‰ªªÂä°ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïTeLoGraFÔºåÁî®‰∫éËß£ÂÜ≥Â§çÊùÇ‰ªªÂä°ÁöÑ‰ø°Âè∑Êó∂Â∫èÈÄªËæëÔºàSTLÔºâËßÑËåÉ„ÄÇÊàë‰ª¨Âà©Áî®ÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÁºñÁ†ÅÂô®ÂíåÊµÅÂåπÈÖçÊäÄÊúØÔºåÂ≠¶‰π†ÈÄöÁî®STLËßÑËåÉÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÈÄöËøáÊî∂ÈõÜ20‰∏á‰∏™ÈÖçÂØπÁ§∫‰æãÔºåÊàë‰ª¨Âú®Â§ö‰∏™‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜÂπøÊ≥õÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïÂú®STLÊª°Ë∂≥Áéá‰∏ä‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫ø„ÄÇ‰∏é‰º†ÁªüÁöÑSTLËßÑÂàíÁÆóÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Êé®ÁêÜÈÄüÂ∫¶‰∏äÂø´10Âà∞100ÂÄçÔºåÂπ∂‰∏îËÉΩÂ§üÈÄÇÂ∫î‰ªª‰ΩïÁ≥ªÁªüÂä®ÊÄÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20859', 'title': 'X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2504.20859', 'abstract': "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': '2102f697cfc2375e', 'authors': ['Guy Hadad', 'Haggai Roitman', 'Yotam Eshel', 'Bracha Shapira', 'Lior Rokach'], 'affiliations': ['Ben-Gurion University of the Negev Beer Sheva, Israel', 'eBay Netanya, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.20859.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#low_resource', '#training', '#multimodal'], 'emoji': 'üîÄ', 'ru': {'title': 'X-Cross: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∫—Ä–æ—Å—Å-–¥–æ–º–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±–µ–∑ –æ–±—à–∏—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å X-Cross –¥–ª—è –∫—Ä–æ—Å—Å-–¥–æ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–º–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ (LoRA). X-Cross –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∏—Å—Ö–æ–¥–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –∑–Ω–∞–Ω–∏—è –∏–∑ –≤—Å–µ—Ö –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –¥–∞–Ω–Ω—ã—Ö Amazon –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ X-Cross –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –º–æ–¥–µ–ª—å—é, –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π —Å LoRA, –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å 25% –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫—Ä–æ—Å—Å-–¥–æ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—è –Ω–∞ 50-75% –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∏.'}, 'en': {'title': 'X-Cross: Efficient Cross-Domain Recommendations with Minimal Data', 'desc': "The paper introduces 'X-Cross', a new model designed for cross-domain sequential recommendations that can quickly adapt to new product categories without extensive retraining. It utilizes multiple domain-specific language models, each fine-tuned with low-rank adapters (LoRA), to enhance the recommendation process. By refining the representations of these models layer by layer, X-Cross effectively integrates knowledge from different domains while maintaining their unique characteristics. The model shows strong performance on Amazon datasets, requiring significantly less fine-tuning data and parameters compared to traditional methods, making it efficient for data-limited scenarios."}, 'zh': {'title': 'X-CrossÔºöÈ´òÊïàÁöÑË∑®È¢ÜÂüüÊé®ËçêËß£ÂÜ≥ÊñπÊ°à', 'desc': 'ÈöèÁùÄÊñ∞‰∫ßÂìÅÁöÑ‰∏çÊñ≠Ê∂åÁé∞ÔºåÊé®ËçêÁ≥ªÁªüÈúÄË¶ÅÂø´ÈÄüÈÄÇÂ∫îÊñ∞È¢ÜÂüüÔºåËÄåÊó†ÈúÄÂ§ßÈáèÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‚ÄúX-Cross‚ÄùÊ®°ÂûãÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑË∑®È¢ÜÂüüÂ∫èÂàóÊé®ËçêÊ®°ÂûãÔºåÈÄöËøáÊï¥ÂêàÂ§ö‰∏™ÁâπÂÆöÈ¢ÜÂüüÁöÑËØ≠Ë®ÄÊ®°ÂûãÊù•Êé®ËçêÊñ∞È¢ÜÂüüÁöÑ‰∫ßÂìÅ„ÄÇX-CrossÈÄöËøáÈÄêÂ±ÇÊìç‰ΩúÂä®ÊÄÅÂú∞‰ºòÂåñÊØè‰∏™Ê∫êËØ≠Ë®ÄÊ®°ÂûãÁöÑË°®Á§∫ÔºåÁ°Æ‰øùÂú®Ë∑®È¢ÜÂüüÈÄÇÂ∫îÊó∂‰øùÁïôÈ¢ÜÂüüÁâπÊúâÁöÑÁªÜÂæÆÂ∑ÆÂà´„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåX-CrossÂú®Ë∑®È¢ÜÂüü‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÊâÄÈúÄÁöÑÂæÆË∞ÉÊï∞ÊçÆÈáèÊòæËëó‰Ωé‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02707', 'title': 'Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play', 'url': 'https://huggingface.co/papers/2505.02707', 'abstract': "A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.", 'score': 56, 'issue_id': 3604, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '1cf06df6011df348', 'authors': ['Yemin Shi', 'Yu Shu', 'Siwei Dong', 'Guangyi Liu', 'Jaward Sesay', 'Jingwen Li', 'Zhiting Hu'], 'affiliations': ['MBZUAI', 'Maitrix.org', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.02707.jpg', 'data': {'categories': ['#open_source', '#agi', '#multimodal', '#audio', '#architecture', '#agents', '#reasoning', '#multilingual', '#machine_translation'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'Voila: –ò–ò-—Å–æ–±–µ—Å–µ–¥–Ω–∏–∫ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –≥–æ–ª–æ—Å–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Voila - —Å–µ–º–µ–π—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤–µ—Å—Ç–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ —Å —á–µ–ª–æ–≤–µ–∫–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫–≤–æ–∑–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º. Voila –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –æ—Ç–≤–µ—Ç–∞ –≤—Å–µ–≥–æ 195 –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥, —á—Ç–æ –±—ã—Å—Ç—Ä–µ–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Ä–µ–∞–∫—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –∏ –º–æ–∂–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –≥–æ–ª–æ—Å–∞ –ø–æ –∫–æ—Ä–æ—Ç–∫–∏–º –∞—É–¥–∏–æ—Å—ç–º–ø–ª–∞–º.'}, 'en': {'title': 'Voila: Revolutionizing Voice AI for Natural Conversations', 'desc': 'This paper presents Voila, an advanced voice AI agent designed for seamless interaction in daily life. Voila utilizes a novel end-to-end architecture that allows for real-time, emotionally expressive conversations, achieving a response time of just 195 milliseconds. It combines large language models with sophisticated acoustic modeling to generate natural and persona-aware voice outputs, enabling users to customize voice characteristics easily. Additionally, Voila supports various voice-based applications, including automatic speech recognition and multilingual translation, and is fully open-sourced to promote further research in human-machine communication.'}, 'zh': {'title': 'VoilaÔºöÂÆûÁé∞Ëá™ÁÑ∂ÊÉÖÊÑü‰∫íÂä®ÁöÑËØ≠Èü≥AI‰ª£ÁêÜ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫ÜVoilaÔºå‰∏Ä‰∏™Êñ∞ÂûãÁöÑËØ≠Èü≥AI‰ª£ÁêÜÔºåÊó®Âú®ÂÆûÁé∞‰∏é‰∫∫Á±ªÁöÑËá™ÁÑ∂‰∫íÂä®„ÄÇVoilaÈááÁî®Á´ØÂà∞Á´ØÊû∂ÊûÑÔºåÊîØÊåÅÂÖ®ÂèåÂ∑•„ÄÅ‰ΩéÂª∂ËøüÁöÑÂØπËØùÔºåËÉΩÂ§üÊçïÊçâËØ≠Èü≥‰∏≠ÁöÑÊÉÖÊÑüÂíåÁªÜÂæÆÂ∑ÆÂà´„ÄÇÂÆÉÁªìÂêà‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂº∫Â§ßÁöÑÂ£∞Â≠¶Âª∫Ê®°ÔºåÂÖÅËÆ∏Áî®Êà∑ÈÄöËøáÁÆÄÂçïÁöÑÊñáÊú¨Êåá‰ª§ÂÆö‰πâËØ¥ËØùËÄÖÁöÑË∫´‰ªΩÂíåËØ≠Ë∞É„ÄÇVoilaËøòÊîØÊåÅË∂ÖËøá‰∏ÄÁôæ‰∏áÁßçÈ¢ÑÊûÑÂª∫ÁöÑÂ£∞Èü≥ÔºåÂπ∂ËÉΩ‰ªéÁü≠Ëá≥10ÁßíÁöÑÈü≥È¢ëÊ†∑Êú¨‰∏≠È´òÊïàÂÆöÂà∂Êñ∞Â£∞Èü≥ÔºåÊé®Âä®‰∫∫Êú∫‰∫§‰∫íÁöÑËøõÊ≠•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02387', 'title': 'RM-R1: Reward Modeling as Reasoning', 'url': 'https://huggingface.co/papers/2505.02387', 'abstract': "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.", 'score': 43, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '6cac1dc82fc6bcdc', 'authors': ['Xiusi Chen', 'Gaotang Li', 'Ziqi Wang', 'Bowen Jin', 'Cheng Qian', 'Yu Wang', 'Hongru Wang', 'Yu Zhang', 'Denghui Zhang', 'Tong Zhang', 'Hanghang Tong', 'Heng Ji'], 'affiliations': ['Stevens Institute of Technology', 'Texas A&M University', 'University of California, San Diego', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02387.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#training', '#benchmark', '#rlhf', '#alignment', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è - Reasoning Reward Models (ReasRMs), –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∫–∞–∫ –∑–∞–¥–∞—á—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏. –ú–æ–¥–µ–ª–∏ ReasRM —É–ª—É—á—à–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –¥–ª—è —á–∞—Ç–∞ —Ä—É–±—Ä–∏–∫–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–µ–ª–∏—á–∏–Ω—É –¥–æ 13.8%.'}, 'en': {'title': 'Enhancing Reward Models with Reasoning for Better Interpretability', 'desc': 'This paper discusses the importance of reward modeling in aligning large language models (LLMs) with human preferences using reinforcement learning from human feedback (RLHF). It highlights the limitations of existing reward models, which often lack interpretability and struggle to incorporate natural language critiques. The authors introduce Reasoning Reward Models (ReasRMs), which enhance interpretability by treating reward modeling as a reasoning task. Their approach includes a two-stage training process that improves performance and achieves state-of-the-art results on various benchmarks, outperforming larger models.'}, 'zh': {'title': 'Êé®ÁêÜÂ•ñÂä±Ê®°ÂûãÔºöÊèêÂçáÂèØËß£ÈáäÊÄßÁöÑÂÖ≥ÈîÆ', 'desc': 'Â•ñÂä±Âª∫Ê®°Âú®Â∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩê‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÈÄöËøá‰∫∫Á±ªÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâ„ÄÇÁé∞ÊúâÁöÑÂ•ñÂä±Ê®°ÂûãÔºàRMÔºâÂæÄÂæÄÁîüÊàê‰∏çÈÄèÊòéÁöÑÊ†áÈáèÂàÜÊï∞ÊàñÁõ¥Êé•È¢ÑÊµãÂÅèÂ•ΩÁöÑÁ≠îÊ°àÔºåÁº∫‰πèÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁîüÊàêÂ•ñÂä±Ê®°Âûã‚Äî‚ÄîÊé®ÁêÜÂ•ñÂä±Ê®°ÂûãÔºàReasRMsÔºâÔºåÂ∞ÜÂ•ñÂä±Âª∫Ê®°ËßÜ‰∏∫Êé®ÁêÜ‰ªªÂä°Ôºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÊÄßËÉΩ„ÄÇÈÄöËøáÈ´òË¥®ÈáèÊé®ÁêÜÈìæÁöÑËí∏È¶èÂíåÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Â§ö‰∏™Â•ñÂä±Ê®°ÂûãÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20752', 'title': 'Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers', 'url': 'https://huggingface.co/papers/2504.20752', 'abstract': 'Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio phi_r of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing phi_r drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.', 'score': 41, 'issue_id': 3604, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': '46858ed83065e6ae', 'authors': ['Roman Abramov', 'Felix Steinbauer', 'Gjergji Kasneci'], 'affiliations': ['School of Computation, Information and Technology, Technical University of Munich, Munich, Germany', 'School of Social Sciences and Technology, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.20752.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#dataset', '#reasoning', '#synthetic', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': '–ì—Ä–æ–∫–∏–Ω–≥ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–≤–µ—Ä—å –∫ –Ω–∞–¥–µ–∂–Ω–æ–º—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–≥–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –≥—Ä–æ–∫–∏–Ω–≥–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—à–∏—Ä—è—é—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–æ–∫–∏–Ω–≥–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –¥–æ–ø–æ–ª–Ω—è—è –≥—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –¥–ª—è –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è. –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ –¥–∞–∂–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–≤–µ—Ä–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç —É—Å–∏–ª–∏—Ç—å —Å—Ö–µ–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∑–∞—Å—Ç–∞–≤–ª—è—è –º–æ–¥–µ–ª—å –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 95-100% –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ 2WikiMultiHopQA, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.'}, 'en': {'title': 'Unlocking Reasoning in Transformers with Grokking and Data Augmentation', 'desc': "This paper explores how Transformers, a type of neural network, can improve their ability to reason through complex factual information. It introduces the concept of 'grokking' to real-world data by enhancing knowledge graphs with synthetic data, which helps the model learn logical patterns instead of just memorizing facts. Interestingly, the study finds that even incorrect synthetic data can aid in developing reasoning capabilities by emphasizing relational structures. The results show significant improvements in multi-hop reasoning tasks, achieving high accuracy and suggesting that this method can enhance the reasoning abilities of large language models."}, 'zh': {'title': 'Âà©Áî®ÂêàÊàêÊï∞ÊçÆÊèêÂçáÂ§öÊ≠•Êé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂèòÊç¢Âô®Âú®Â§öÊ≠•‰∫ãÂÆûÊé®ÁêÜ‰∏≠ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁúüÂÆû‰∏ñÁïåÁü•ËØÜÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊàë‰ª¨È¶ñÊ¨°Â∞ÜgrokkingÊâ©Â±ïÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑ‰∫ãÂÆûÊï∞ÊçÆÔºåÂπ∂ÈÄöËøáÂ¢ûÂº∫Áü•ËØÜÂõæË∞±Êù•Ëß£ÂÜ≥Êï∞ÊçÆÈõÜÁ®ÄÁñèÊÄßÁöÑÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂç≥‰ΩøÊòØ‰∫ãÂÆû‰∏çÊ≠£Á°ÆÁöÑÂêàÊàêÊï∞ÊçÆ‰πüËÉΩÂ¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÄå‰∏çÊòØÈôç‰ΩéÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§öË∑≥Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫Ü95-100%ÁöÑÂáÜÁ°ÆÁéáÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02222', 'title': 'Practical Efficiency of Muon for Pretraining', 'url': 'https://huggingface.co/papers/2505.02222', 'abstract': 'We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.', 'score': 26, 'issue_id': 3608, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': '3b2cca9779803131', 'authors': ['Essential AI', ':', 'Ishaan Shah', 'Anthony M. Polloreno', 'Karl Stratos', 'Philip Monk', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Ashish Tanwer', 'Darsh J Shah', 'Khoi Nguyen', 'Kurt Smith', 'Michael Callahan', 'Michael Pust', 'Mohit Parmar', 'Peter Rushton', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Somanshu Singla', 'Tim Romanski', 'Yash Vanjani', 'Ashish Vaswani'], 'affiliations': ['Essential AI, San Francisco, CA'], 'pdf_title_img': 'assets/pdf/title_img/2505.02222.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'Muon: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Muon, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç AdamW –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è. Muon —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –±–∞—Ç—á–∞, –ø–æ–∑–≤–æ–ª—è—è –ø—Ä–æ–≤–æ–¥–∏—Ç—å –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é Muon –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π (muP) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã –æ–±—à–∏—Ä–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–æ 4 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Muon: A New Era of Efficient Optimization in Machine Learning', 'desc': 'This paper introduces Muon, a second-order optimizer that improves upon AdamW by expanding the Pareto frontier in terms of compute-time efficiency. Muon demonstrates superior data efficiency at large batch sizes, even exceeding the critical batch size, while maintaining computational efficiency for cost-effective training. The authors also explore the integration of Muon with the maximal update parameterization (muP) to enhance hyperparameter transfer efficiency. Their extensive experiments, involving models with up to four billion parameters, validate the effectiveness of Muon across various data distributions and architectures.'}, 'zh': {'title': 'Muon‰ºòÂåñÂô®ÔºöË∂ÖË∂äAdamWÁöÑÈ´òÊïàËÆ≠ÁªÉ', 'desc': 'Êú¨ÊñáÂ±ïÁ§∫‰∫ÜMuon‰Ωú‰∏∫‰∏ÄÁßç‰∫åÈò∂‰ºòÂåñÂô®ÁöÑÁÆÄÂçïÂÆûÁé∞ÔºåËÉΩÂ§üÂú®ËÆ°ÁÆóÊó∂Èó¥ÁöÑÊùÉË°°‰∏äÊòæËëóÊâ©Â±ïParetoÂâçÊ≤øÔºåÁõ∏ÊØî‰∫éAdamWÊõ¥ÂÖ∑‰ºòÂäø„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåMuonÂú®Â§ßÊâπÈáèËÆ≠ÁªÉ‰∏≠‰øùÊåÅÊï∞ÊçÆÊïàÁéáÔºåË∂ÖË∂ä‰∫ÜÊâÄË∞ìÁöÑ‰∏¥ÁïåÊâπÈáèÂ§ßÂ∞èÔºåÂêåÊó∂‰ªçÁÑ∂‰øùÊåÅËÆ°ÁÆóÊïàÁéáÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÁªèÊµéÁöÑËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ËøòÁ†îÁ©∂‰∫ÜMuon‰∏éÊúÄÂ§ßÊõ¥Êñ∞ÂèÇÊï∞ÂåñÔºàmuPÔºâÁöÑÁªìÂêàÔºå‰ª•ÂÆûÁé∞È´òÊïàÁöÑË∂ÖÂèÇÊï∞ËΩ¨ÁßªÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑÈÄíÂΩíÁÆóÊ≥ïÔºåËÉΩÂ§üËÄÉËôëmuP‰∏≠ÁöÑÊâÄÊúâËØØÂ∑ÆÊù•Ê∫êÔºåÂêåÊó∂‰ªÖÂºïÂÖ•ÈÄÇÂ∫¶ÁöÑËµÑÊ∫êÂºÄÈîÄ„ÄÇÈÄöËøáÂØπÊ®°ÂûãËßÑÊ®°ËææÂà∞ÂõõÂçÅ‰∫øÂèÇÊï∞ÁöÑÂπøÊ≥õÂÆûÈ™å‰ª•ÂèäÂØπÊï∞ÊçÆÂàÜÂ∏ÉÂíåÊû∂ÊûÑÁöÑÊ∂àËûçÁ†îÁ©∂ÔºåÊàë‰ª¨È™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÂèëÁé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02735', 'title': 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.02735', 'abstract': 'Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.', 'score': 20, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '1ffa5567eb2f4acc', 'authors': ['Zhouliang Yu', 'Ruotian Peng', 'Keyi Ding', 'Yizhe Li', 'Zhongyuan Peng', 'Minghao Liu', 'Yifan Zhang', 'Zheng Yuan', 'Huajian Xin', 'Wenhao Huang', 'Yandong Wen', 'Ge Zhang', 'Weiyang Liu'], 'affiliations': ['M-A-P', 'Max Planck Institute for Intelligent Systems, T√ºbingen', 'Numina', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02735.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark'], 'emoji': 'üßÆ', 'ru': {'title': 'FormalMATH: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –¥–ª—è –ò–ò', 'desc': 'FormalMATH - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –Ω–∞ Lean4, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 5560 —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ—Ç –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ –¥–æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏. –û—Ü–µ–Ω–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM-–¥–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Ç–µ–æ—Ä–µ–º –ø–æ–∫–∞–∑–∞–ª–∞ –∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å, —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —É—Å–ø–µ—Ö–æ–º –≤ 16.46% –ø—Ä–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –≤—ã—è–≤–∏–ª –æ–±—Ä–∞—Ç–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É –Ω–∞–ª–∏—á–∏–µ–º —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –∏ —É—Å–ø–µ—Ö–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–æ —Ü–µ–ø–æ—á–∫–µ –º—ã—Å–ª–µ–π.'}, 'en': {'title': 'FormalMATH: Advancing AI in Formal Mathematical Reasoning', 'desc': 'The paper introduces FormalMATH, a comprehensive benchmark designed to enhance formal mathematical reasoning in AI. It consists of 5,560 verified problems that cover a wide range of mathematical topics, from high school to undergraduate level. The authors propose a human-in-the-loop autoformalization pipeline that utilizes large language models (LLMs) for automating the formalization process, significantly reducing the need for expert input. The study also highlights the limitations of current LLM-based theorem provers, revealing their low success rates and biases across different mathematical domains.'}, 'zh': {'title': 'FormalMATHÔºöÊé®Âä®Ê≠£ÂºèÊï∞Â≠¶Êé®ÁêÜÁöÑÂü∫ÂáÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫ÜFormalMATHÔºåËøôÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑLean4Âü∫ÂáÜÔºåÂåÖÂê´5560‰∏™ÁªèËøáÊ≠£ÂºèÈ™åËØÅÁöÑÊï∞Â≠¶ÈóÆÈ¢òÔºåÊ∂µÁõñ‰ªéÈ´ò‰∏≠Â••ÊûóÂåπÂÖãÊåëÊàòÂà∞Êú¨ÁßëÂÆöÁêÜÁöÑÂ§ö‰∏™È¢ÜÂüü„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ÊâãÂä®ÂΩ¢ÂºèÂåñÁöÑ‰ΩéÊïàÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∫∫Êú∫Âçè‰ΩúËá™Âä®ÂΩ¢ÂºèÂåñÊµÅÁ®ãÔºåÁªìÂêà‰∫Ü‰∏ìÈó®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°åËØ≠Âè•Ëá™Âä®ÂΩ¢ÂºèÂåñ„ÄÅÂ§öLLMËØ≠‰πâÈ™åËØÅÂíåÂü∫‰∫éÂê¶ÂÆöÁöÑÂèçÈ©≥ËøáÊª§Á≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑLLMÂÆöÁêÜËØÅÊòéÂô®Âú®ÂÆûÈôÖÈááÊ†∑È¢ÑÁÆó‰∏ãÁöÑÊàêÂäüÁéá‰ªÖ‰∏∫16.46%ÔºåÂπ∂‰∏îÂú®‰∏çÂêåÈ¢ÜÂüüË°®Áé∞Âá∫ÊòéÊòæÁöÑÂÅèÂ∑Æ„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåFormalMATH‰∏∫Ê≠£ÂºèÊï∞Â≠¶Êé®ÁêÜÊèê‰æõ‰∫Ü‰∏Ä‰∏™Âº∫ÊúâÂäõÁöÑÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02819', 'title': 'ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations', 'url': 'https://huggingface.co/papers/2505.02819', 'abstract': "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.", 'score': 19, 'issue_id': 3608, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '7f169d4491ab6864', 'authors': ['Dmitriy Shopkhoev', 'Ammar Ali', 'Magauiya Zhussip', 'Valentin Malykh', 'Stamatios Lefkimmiatis', 'Nikos Komodakis', 'Sergey Zagoruyko'], 'affiliations': ['Archimedes Athena RC', 'IACM-Forth', 'IITU', 'ITMO University', 'MTS AI', 'Polynome', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2505.02819.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#inference', '#optimization'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': 'ReplaceMe - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—Ä–µ–∑–∫–∏ –≥–ª—É–±–∏–Ω—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∑–∞–º–µ–Ω—è–µ—Ç –±–ª–æ–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –ª–∏–Ω–µ–π–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –Ω–∏–∑–∫–∏—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞—Ö —Å–∂–∞—Ç–∏—è. –ú–µ—Ç–æ–¥ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. ReplaceMe –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –∏ –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ä–µ–∑–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Effortless Depth Pruning with ReplaceMe', 'desc': 'ReplaceMe is a novel method for depth pruning in transformer models that does not require additional training. Instead of retraining the model after pruning, it uses a small calibration dataset to create a linear transformation that approximates the pruned blocks. This allows for significant compression of the model while maintaining high performance, achieving up to 25% pruning with around 90% of the original performance. The method is efficient and can be easily integrated into existing architectures without adding extra parameters, making it a competitive alternative to traditional pruning techniques.'}, 'zh': {'title': 'ReplaceMeÔºöÈ´òÊïàÁöÑÊó†ËÆ≠ÁªÉÊ∑±Â∫¶Ââ™ÊûùÊñπÊ≥ï', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ReplaceMeÁöÑÊ∑±Â∫¶Ââ™ÊûùÊñπÊ≥ïÔºåÂÆÉ‰∏çÈúÄË¶ÅËÆ≠ÁªÉÂ∞±ËÉΩÊúâÊïàÂú∞Â∞ÜÂèòÊç¢Âô®ÂùóÊõøÊç¢‰∏∫Á∫øÊÄßÊìç‰ΩúÔºåÂêåÊó∂Âú®‰ΩéÂéãÁº©ÊØî‰∏ã‰øùÊåÅÈ´òÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑÂâ™ÊûùÊñπÊ≥ï‰∏çÂêåÔºåReplaceMeÂè™ÈúÄ‰∏Ä‰∏™Â∞èÁöÑÊ†°ÂáÜÊï∞ÊçÆÈõÜÊù•‰º∞ËÆ°Á∫øÊÄßÂèòÊç¢Ôºå‰ªéËÄåËøë‰ººÂâ™ÊûùÂêéÁöÑÂùó„ÄÇËøô‰∏™‰º∞ËÆ°ÁöÑÁ∫øÊÄßÊò†Â∞ÑÂèØ‰ª•‰∏éÂâ©‰ΩôÁöÑÂèòÊç¢Âô®ÂùóÊó†ÁºùÂêàÂπ∂ÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÁöÑÁΩëÁªúÂèÇÊï∞ÈúÄÊ±Ç„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåReplaceMeÂú®Â§ö‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏äË°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÂÆûÁé∞È´òËææ25%ÁöÑÂâ™ÊûùÔºåÂêåÊó∂‰øùÁïôÁ∫¶90%ÁöÑÂéüÂßãÊ®°ÂûãÊÄßËÉΩÔºå‰∏îÊó†ÈúÄ‰ªª‰ΩïËÆ≠ÁªÉÊàñË∞ÉÊï¥Ê≠•È™§„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02391', 'title': 'Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL', 'url': 'https://huggingface.co/papers/2505.02391', 'abstract': 'Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.', 'score': 19, 'issue_id': 3605, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '48b57ad8bd358e5d', 'authors': ['Jiarui Yao', 'Yifan Hao', 'Hanning Zhang', 'Hanze Dong', 'Wei Xiong', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02391.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GVM-RAFT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π (CoT) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç 2-4-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º –º–µ—Ç–æ–¥–æ–º RAFT.'}, 'en': {'title': 'Dynamic Resource Allocation for Enhanced Reasoning in LLMs', 'desc': 'This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains.'}, 'zh': {'title': 'Âä®ÊÄÅÊ†∑Êú¨ÂàÜÈÖçÔºåÊèêÂçáÊé®ÁêÜÊïàÁéáÔºÅ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈìæÂºèÊé®ÁêÜÔºàCoTÔºâÂ¶Ç‰ΩïË¢´ÂΩ¢ÂºèÂåñ‰∏∫ÊΩúÂèòÈáèÈóÆÈ¢òÔºåÊ®°ÂûãÈúÄË¶ÅÁîüÊàê‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§„ÄÇ‰ª•ÂæÄÁöÑÊñπÊ≥ïÂ¶ÇËø≠‰ª£Â•ñÂä±ÊéíÂêçÂæÆË∞ÉÔºàRAFTÔºâÂú®Â§ÑÁêÜ‰∏çÂêåÈöæÂ∫¶ÁöÑÊèêÁ§∫Êó∂ÔºåÈÄöÂ∏∏ÈááÁî®Áªü‰∏ÄÁöÑÊé®ÁêÜÈ¢ÑÁÆóÔºåËøôÂØºËá¥‰∫ÜÊïàÁéá‰Ωé‰∏ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜGVM-RAFTÔºåËøôÊòØ‰∏ÄÁßçÈíàÂØπÁâπÂÆöÊèêÁ§∫ÁöÑÂä®ÊÄÅÊ†∑Êú¨ÂàÜÈÖçÁ≠ñÁï•ÔºåÊó®Âú®Âú®ËÆ°ÁÆóÈ¢ÑÁÆóÈôêÂà∂‰∏ãÊúÄÂ∞èÂåñÈöèÊú∫Ê¢ØÂ∫¶ÊñπÂ∑Æ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGVM-RAFTÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü2-4ÂÄçÁöÑÂä†ÈÄüÂíåÊòæËëóÁöÑÂáÜÁ°ÆÊÄßÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01658', 'title': 'A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency', 'url': 'https://huggingface.co/papers/2505.01658', 'abstract': 'Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine', 'score': 19, 'issue_id': 3604, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 –º–∞—è', 'en': 'May 3', 'zh': '5Êúà3Êó•'}, 'hash': '56f8dc116dbd737d', 'authors': ['Sihyeong Park', 'Sungryeol Jeon', 'Chaelyn Lee', 'Seokhun Jeon', 'Byung-Soo Kim', 'Jemin Lee'], 'affiliations': ['Electronics and Telecommunications Research Institute, South Korea', 'Korea Electronics Technology Institute, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01658.jpg', 'data': {'categories': ['#inference', '#open_source', '#optimization', '#survey'], 'emoji': 'üöÄ', 'ru': {'title': '–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ LLM: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –±—É–¥—É—â–µ–µ', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 25 –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –≤–∫–ª—é—á–∞—è –æ—Ç–∫—Ä—ã—Ç—ã–µ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –∫–∞–∂–¥—ã–π –¥–≤–∏–∂–æ–∫ –ø–æ —Ä—è–¥—É –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∫–∞–∂–¥—ã–º –¥–≤–∏–∂–∫–æ–º, –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –∑—Ä–µ–ª–æ—Å—Ç—å —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ—à–µ–Ω–∏–π. –°—Ç–∞—Ç—å—è –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –æ–±–∑–æ—Ä–æ–º –±—É–¥—É—â–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –¥–≤–∏–∂–∫–æ–≤ –≤—ã–≤–æ–¥–∞ –¥–ª—è LLM –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π.'}, 'en': {'title': 'Optimizing Inference: A Deep Dive into LLM Engines', 'desc': 'This paper evaluates 25 open-source and commercial inference engines designed for large language models (LLMs). It focuses on key factors such as ease-of-use, deployment, scalability, and performance in terms of throughput and latency. The study also investigates the optimization techniques supported by each engine and assesses the maturity of the open-source ecosystem compared to commercial solutions. Finally, it outlines future research directions to enhance LLM services, hardware support, and security, providing valuable insights for researchers and developers.'}, 'zh': {'title': '‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÂºïÊìéÁöÑÁ†îÁ©∂‰∏éËØÑ‰º∞', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅ‰ª£Á†ÅÁîüÊàêÂô®ÂíåÊêúÁ¥¢ÂºïÊìé‰∏≠ÂæóÂà∞‰∫ÜÂπøÊ≥õÂ∫îÁî®„ÄÇÁî±‰∫éÈìæÂºèÊÄùÁª¥„ÄÅÂ§çÊùÇÊé®ÁêÜÂíå‰ª£ÁêÜÊúçÂä°Á≠âÂ∑•‰ΩúË¥üËΩΩÁöÑÂ¢ûÂä†ÔºåÊé®ÁêÜÊàêÊú¨ÊòæËëó‰∏äÂçá„ÄÇËôΩÁÑ∂Â∑≤ÁªèÈááÁî®Âπ∂Ë°å„ÄÅÂéãÁº©ÂíåÁºìÂ≠òÁ≠â‰ºòÂåñÊñπÊ≥ïÊù•Èôç‰ΩéÊàêÊú¨Ôºå‰ΩÜÂ§öÊ†∑ÂåñÁöÑÊúçÂä°ÈúÄÊ±Ç‰ΩøÂæóÈÄâÊã©ÂêàÈÄÇÁöÑÊñπÊ≥ïÂèòÂæóÂõ∞Èöæ„ÄÇÊú¨ÊñáÂØπ25‰∏™ÂºÄÊ∫êÂíåÂïÜ‰∏öÊé®ÁêÜÂºïÊìéËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÊé¢ËÆ®‰∫ÜÂÆÉ‰ª¨ÁöÑÊòìÁî®ÊÄß„ÄÅÂèØÈÉ®ÁΩ≤ÊÄß„ÄÅÈÄöÁî®ÊîØÊåÅ„ÄÅÂèØÊâ©Â±ïÊÄß‰ª•ÂèäÈÄÇÂêàÂêûÂêêÈáèÂíåÂª∂ËøüÊÑüÁü•ËÆ°ÁÆóÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02835', 'title': 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.02835', 'abstract': "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.", 'score': 17, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '5134d59b0389ade9', 'authors': ['Yi-Fan Zhang', 'Xingyu Lu', 'Xiao Hu', 'Chaoyou Fu', 'Bin Wen', 'Tianke Zhang', 'Changyi Liu', 'Kaiyu Jiang', 'Kaibing Chen', 'Kaiyu Tang', 'Haojie Ding', 'Jiankang Chen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Liang Wang'], 'affiliations': ['CASIA', 'KuaiShou', 'NJU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2505.02835.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#multimodal', '#rl', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (MRM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º StableReinforce, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏, –≤–æ–∑–Ω–∏–∫–∞—é—â–∏–µ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö RL-–∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –û–Ω–∏ —Å–æ–±—Ä–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 200 —Ç—ã—Å—è—á –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è MRM. –ò—Ö –º–æ–¥–µ–ª—å R1-Reward, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é StableReinforce, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Stable Reinforcement for Enhanced Multimodal Reward Modeling', 'desc': 'This paper investigates the use of Reinforcement Learning (RL) to enhance Multimodal Reward Models (MRMs) for Multimodal Large Language Models (MLLMs). It identifies challenges in applying traditional RL algorithms to reward modeling, which can lead to instability during training. To overcome these issues, the authors introduce the StableReinforce algorithm, which improves training dynamics through better loss refinement and reward design. The results show that their reward model, R1-Reward, outperforms existing state-of-the-art models on multimodal benchmarks, demonstrating the effectiveness of their approach.'}, 'zh': {'title': 'Á®≥ÂÆöÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÊÄßËÉΩ', 'desc': 'Â§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºàMRMsÔºâÂú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÊÄßËÉΩ‰∏≠Ëµ∑ÁùÄÈáçË¶Å‰ΩúÁî®„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊù•ÊîπÂñÑÂ•ñÂä±Âª∫Ê®°ÔºåÊèêÂá∫Â∞ÜÂ•ñÂä±Âª∫Ê®°ÈóÆÈ¢òÈáçÊñ∞Ë°®Ëø∞‰∏∫Âü∫‰∫éËßÑÂàôÁöÑRL‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑStableReinforceÁÆóÊ≥ïÈÄöËøá‰ºòÂåñËÆ≠ÁªÉÊçüÂ§±„ÄÅ‰ºòÂäø‰º∞ËÆ°Á≠ñÁï•ÂíåÂ•ñÂä±ËÆæËÆ°ÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâRLÁÆóÊ≥ïÂú®Â•ñÂä±Âª∫Ê®°‰∏≠ÂØºËá¥ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇÁªèËøáStableReinforceÁÆóÊ≥ïËÆ≠ÁªÉÁöÑÂ•ñÂä±Ê®°ÂûãR1-RewardÂú®Â§öÊ®°ÊÄÅÂ•ñÂä±Âª∫Ê®°Âü∫ÂáÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜRLÁÆóÊ≥ïÂú®‰ºòÂåñMRMs‰∏≠ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01441', 'title': 'Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.01441', 'abstract': 'Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.', 'score': 17, 'issue_id': 3603, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 –∞–ø—Ä–µ–ª—è', 'en': 'April 28', 'zh': '4Êúà28Êó•'}, 'hash': '4c0a590eccbb1960', 'authors': ['Joykirat Singh', 'Raghav Magazine', 'Yash Pandya', 'Akshay Nambi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01441.jpg', 'data': {'categories': ['#agents', '#rl', '#reasoning', '#training', '#benchmark', '#optimization', '#rlhf'], 'emoji': 'ü§ñ', 'ru': {'title': 'ARTIST: –ê–≥–µ–Ω—Ç–Ω—ã–π –ò–ò —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ARTIST - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≥–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). ARTIST –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ—à–∞—Ç—å, –∫–æ–≥–¥–∞ –∏ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ARTIST –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ LLM.'}, 'en': {'title': 'Empowering LLMs with Dynamic Reasoning and Tool Integration', 'desc': "This paper presents ARTIST, a new framework that enhances large language models (LLMs) by integrating agentic reasoning, reinforcement learning (RL), and tool usage. Unlike traditional LLMs that rely on static knowledge, ARTIST allows models to dynamically decide when and how to use external tools during complex reasoning tasks. The framework employs outcome-based RL to improve the model's strategies for interacting with tools and environments, enabling it to learn without needing detailed step-by-step guidance. Experimental results demonstrate that ARTIST significantly outperforms existing models, achieving better reasoning and tool utilization in challenging scenarios."}, 'zh': {'title': 'ARTISTÔºöÊô∫ËÉΩÊé®ÁêÜ‰∏éÂ∑•ÂÖ∑ÈõÜÊàêÁöÑÊñ∞ÂâçÊ≤ø', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜ‰ªçÁÑ∂ÂèóÂà∞ÈùôÊÄÅÂÜÖÈÉ®Áü•ËØÜÂíå‰ªÖÂü∫‰∫éÊñáÊú¨Êé®ÁêÜÁöÑÈôêÂà∂„ÄÇÁé∞ÂÆû‰∏ñÁïåÁöÑÈóÆÈ¢òËß£ÂÜ≥ÈÄöÂ∏∏ÈúÄË¶ÅÂä®ÊÄÅÁöÑÂ§öÊ≠•È™§Êé®ÁêÜ„ÄÅÈÄÇÂ∫îÊÄßÂÜ≥Á≠ñ‰ª•Âèä‰∏éÂ§ñÈÉ®Â∑•ÂÖ∑ÂíåÁéØÂ¢ÉÁöÑ‰∫§‰∫íËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜARTISTÔºàËá™ÊàëÊîπËøõÂèòÊç¢Âô®‰∏≠ÁöÑ‰ª£ÁêÜÊé®ÁêÜÂíåÂ∑•ÂÖ∑ÈõÜÊàêÔºâÔºåËøôÊòØ‰∏Ä‰∏™Â∞Ü‰ª£ÁêÜÊé®ÁêÜ„ÄÅÂº∫ÂåñÂ≠¶‰π†ÂíåÂ∑•ÂÖ∑ÈõÜÊàêÁ¥ßÂØÜÁªìÂêàÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåARTISTÂú®Êï∞Â≠¶Êé®ÁêÜÂíåÂ§öËΩÆÂáΩÊï∞Ë∞ÉÁî®Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÂü∫Á°ÄÊ®°ÂûãÊúâÈ´òËææ22%ÁöÑÁªùÂØπÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02156', 'title': 'Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents', 'url': 'https://huggingface.co/papers/2505.02156', 'abstract': "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction rightarrow deep contemplation) based on real-time context. Our framework's core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach", 'score': 16, 'issue_id': 3602, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': '13e5cf390c136aeb', 'authors': ['Minzheng Wang', 'Yongbin Li', 'Haobo Wang', 'Xinghua Zhang', 'Nan Xu', 'Bingli Wu', 'Fei Huang', 'Haiyang Yu', 'Wenji Mao'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.02156.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞–º –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Adaptive Mode Learning (AML) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. AML –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Adaptive Mode Policy Optimization (AMPO), –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–¥–∏–Ω –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö —Ä–µ–∂–∏–º–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ç–æ–∫–µ–Ω–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ AML –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–∞ 15.6% –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –Ω–∞ 32.8% –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Dynamic Reasoning for Smarter Social Agents', 'desc': 'This paper introduces Adaptive Mode Learning (AML), a new approach for simulating social intelligence in language agents. Unlike existing methods that either lack dynamic reasoning or use a one-size-fits-all strategy, AML allows agents to switch between four different thinking modes based on the context of the interaction. The core of this framework is the Adaptive Mode Policy Optimization (AMPO) algorithm, which enhances reasoning efficiency by adapting the depth of thought to the situation. Experimental results show that AML significantly improves task performance and reduces the length of reasoning chains compared to current state-of-the-art methods.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊ®°ÂºèÂ≠¶‰π†ÔºöÊèêÂçáÁ§æ‰∫§Êô∫ËÉΩÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫Ëá™ÈÄÇÂ∫îÊ®°ÂºèÂ≠¶‰π†ÔºàAMLÔºâÔºåÊó®Âú®ÊèêÈ´òÁ§æ‰∫§Êô∫ËÉΩÊ®°ÊãüÁöÑÊúâÊïàÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êé®ÁêÜÊ∑±Â∫¶‰∏äÁº∫‰πèÁÅµÊ¥ªÊÄßÔºåÂØºËá¥Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊé®ÁêÜ‰∏çÂ§üÈ´òÊïà„ÄÇAMLÈÄöËøáÂÆûÊó∂‰∏ä‰∏ãÊñáÂä®ÊÄÅÈÄâÊã©ÂõõÁßçÊÄùÁª¥Ê®°ÂºèÔºå‰ªéÁõ¥ËßâÂèçÂ∫îÂà∞Ê∑±Â∫¶ÊÄùËÄÉÔºå‰ºòÂåñ‰∫ÜÊé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAMLÂú®Á§æ‰∫§Êô∫ËÉΩ‰ªªÂä°‰∏≠ÊØîÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÊèêÈ´ò‰∫Ü15.6%ÁöÑÊÄßËÉΩÔºåÂêåÊó∂Êé®ÁêÜÈìæÈïøÂ∫¶ÂáèÂ∞ë‰∫Ü32.8%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02094', 'title': 'SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations', 'url': 'https://huggingface.co/papers/2505.02094', 'abstract': 'We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.', 'score': 14, 'issue_id': 3604, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': '1ae5a69dab5de633', 'authors': ['Runyi Yu', 'Yinhuai Wang', 'Qihan Zhao', 'Hok Wai Tsui', 'Jingbo Wang', 'Ping Tan', 'Qifeng Chen'], 'affiliations': ['HKUST Hong Kong, China', 'Shanghai AI Laboratory Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02094.jpg', 'data': {'categories': ['#games', '#training', '#rl', '#optimization', '#data'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (RLID). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —à—É–º–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π: Stitched Trajectory Graph (STG) –∏ State Transition Field (STF). –û–Ω–∏ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Adaptive Trajectory Sampling (ATS) –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—á–µ–±–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –º–µ—Ö–∞–Ω–∏–∑–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞–≤—ã–∫–∞–º, –∑–∞–≤–∏—Å—è—â–∏–º –æ—Ç –ø–∞–º—è—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –ø–ª–∞–Ω–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging Skills: Enhancing Reinforcement Learning with Augmented Trajectories', 'desc': 'This paper tackles the issue of noise and limited coverage in Reinforcement Learning from Interaction Demonstration (RLID). It highlights that even with noisy and sparse data, there are countless feasible trajectories that can connect different demonstrated skills. The authors introduce two innovative techniques: the Stitched Trajectory Graph (STG) for identifying transitions between skills, and the State Transition Field (STF) for linking arbitrary states. Their method, combined with Adaptive Trajectory Sampling (ATS) and a historical encoding mechanism, enhances skill learning and generalization beyond the initial demonstrations, showing significant improvements in various tasks.'}, 'zh': {'title': 'ÂÖãÊúçÊºîÁ§∫Âô™Â£∞ÔºåÂÆûÁé∞ÊäÄËÉΩÁöÑÈ≤ÅÊ£íÂ≠¶‰π†', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ªé‰∫§‰∫íÊºîÁ§∫‰∏≠ËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºàRLIDÔºâÊó∂Èù¢‰∏¥ÁöÑÊåëÊàòÔºåÁâπÂà´ÊòØÊºîÁ§∫Âô™Â£∞ÂíåË¶ÜÁõñÈôêÂà∂„ÄÇÁé∞ÊúâÁöÑÊï∞ÊçÆÊî∂ÈõÜÊñπÊ≥ïËôΩÁÑ∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑ‰∫§‰∫íÊºîÁ§∫Ôºå‰ΩÜÂæÄÂæÄÂØºËá¥Á®ÄÁñè„ÄÅÊñ≠Ë£ÇÂíåÂô™Â£∞ÁöÑËΩ®ËøπÔºåÊó†Ê≥ïÂÖ®Èù¢ÊçïÊçâÊäÄËÉΩÂèòÂåñÂíåËøáÊ∏°ÁöÑÂÖ®Ë≤å„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÂÖ≥ÈîÆËßÅËß£ÊòØÔºåÂ∞ΩÁÆ°ÊºîÁ§∫Â≠òÂú®Âô™Â£∞ÂíåÁ®ÄÁñèÊÄßÔºå‰ΩÜ‰ªçÁÑ∂Â≠òÂú®Êó†ÈôêÁöÑÁâ©ÁêÜÂèØË°åËΩ®ËøπÔºåÂèØ‰ª•Ëá™ÁÑ∂Âú∞ËøûÊé•ÊºîÁ§∫ÊäÄËÉΩÊàñ‰ªéÂÖ∂ÈÇªËøëÁä∂ÊÄÅ‰∏≠‰∫ßÁîü„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÔºåÊó®Âú®ÊèêÈ´òÊäÄËÉΩËé∑ÂèñÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02471', 'title': 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction', 'url': 'https://huggingface.co/papers/2505.02471', 'abstract': 'We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'fa1cacd261a625e3', 'authors': ['Biao Gong', 'Cheng Zou', 'Dandan Zheng', 'Hu Yu', 'Jingdong Chen', 'Jianxin Sun', 'Junbo Zhao', 'Jun Zhou', 'Kaixiang Ji', 'Lixiang Ru', 'Libin Wang', 'Qingpei Guo', 'Rui Liu', 'Weilong Chai', 'Xinyu Xiao', 'Ziyuan Huang'], 'affiliations': ['Ant Group', 'Ming-Lite-Uni'], 'pdf_title_img': 'assets/pdf/title_img/2505.02471.jpg', 'data': {'categories': ['#agi', '#multimodal', '#open_source', '#cv'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤ –µ–¥–∏–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ', 'desc': 'Ming-Lite-Uni - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (MLLM) –∏ –æ–±—É—á–∞–µ–º—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–µ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –æ–±—É—á–∞–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Ming-Lite-Uni –∏ –≤–ø–µ—á–∞—Ç–ª—è—é—â—É—é –≥–∏–±–∫–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞.'}, 'en': {'title': 'Unifying Vision and Language for Advanced AI', 'desc': "Ming-Lite-Uni is an innovative open-source framework designed to integrate vision and language through a unified visual generator and a multimodal autoregressive model. It introduces advanced techniques like multi-scale learnable tokens and representation alignment to enhance the interaction between text and images. By utilizing a fixed MLLM alongside a learnable diffusion model, it supports both text-to-image generation and image editing based on instructions. The framework's strong performance and interactive capabilities highlight its potential impact on the development of advanced AI systems, contributing to the journey towards artificial general intelligence (AGI)."}, 'zh': {'title': 'Ming-Lite-UniÔºöÁªü‰∏ÄËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÂ§öÊ®°ÊÄÅÊ°ÜÊû∂', 'desc': 'Ming-Lite-UniÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§öÊ®°ÊÄÅÊ°ÜÊû∂ÔºåÊó®Âú®Áªü‰∏ÄËßÜËßâÂíåËØ≠Ë®Ä„ÄÇÂÆÉÂºïÂÖ•‰∫ÜÊñ∞ÁöÑËßÜËßâÁîüÊàêÂô®ÂíåÂ§öÊ®°ÊÄÅËá™ÂõûÂΩíÊ®°ÂûãÔºåÊîØÊåÅÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÂíåÂõæÂÉèÁºñËæë‰ªªÂä°„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∫ÜÂ§öÂ∞∫Â∫¶ÂèØÂ≠¶‰π†Ê†áËÆ∞ÂíåÂ§öÂ∞∫Â∫¶Ë°®Á§∫ÂØπÈΩêÁ≠ñÁï•ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÊâÄÊúâ‰ª£Á†ÅÂíåÊ®°ÂûãÊùÉÈáçÈÉΩÊòØÂºÄÊ∫êÁöÑÔºåÈºìÂä±Á§æÂå∫Ëøõ‰∏ÄÊ≠•Êé¢Á¥¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02370', 'title': 'SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing', 'url': 'https://huggingface.co/papers/2505.02370', 'abstract': 'Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '5833f2e06b661a76', 'authors': ['Ming Li', 'Xin Gu', 'Fan Chen', 'Xiaoying Xing', 'Longyin Wen', 'Chen Chen', 'Sijie Zhu'], 'affiliations': ['ByteDance Intelligent Creation (USA)', 'Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2505.02370.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#training', '#cv'], 'emoji': '‚úèÔ∏è', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø–∞—Ä–∞–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –¥–æ—Å—Ç–∏–≥–∞—è 9.19% —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Real-Edit –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ 30 —Ä–∞–∑ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤ 13 —Ä–∞–∑ –º–µ–Ω—å—à–µ–º —Ä–∞–∑–º–µ—Ä–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Enhancing Image Editing with Accurate Instructions and Contrastive Supervision', 'desc': 'This paper addresses the problem of noisy supervision in image editing tasks caused by mismatched editing instructions and image pairs. The authors propose a novel approach that involves creating more accurate editing instructions that align better with the original and edited images. They introduce contrastive editing instructions to enhance the effectiveness of these instructions and utilize a unified guide for vision-language models to rectify them. Their method improves supervision signals without relying on pre-training or VLM modules, achieving significant performance gains on benchmarks with less training data and smaller model sizes.'}, 'zh': {'title': 'ÊèêÂçáÂõæÂÉèÁºñËæëÊïàÊûúÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑÂõæÂÉèÁºñËæëÊ®°ÂûãÁöÑÁõëÁù£‰ø°Âè∑„ÄÇÊàë‰ª¨ÈÄöËøáÊûÑÂª∫Êõ¥ÊúâÊïàÁöÑÁºñËæëÊåá‰ª§Ôºå‰ΩøÂÖ∂‰∏éÂéüÂßãÂíåÁºñËæëÂêéÁöÑÂõæÂÉèÂØπÊõ¥Â•ΩÂú∞ÂØπÈΩêÔºåÂπ∂ÂºïÂÖ•ÂØπÊØîÁºñËæëÊåá‰ª§Êù•Â¢ûÂº∫ÊïàÊûú„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁºñËæëÊ®°ÂûãÂú®‰∏çÂêåÊé®ÁêÜÊ≠•È™§‰∏≠Ë°®Áé∞Âá∫ÁâπÂÆöÁöÑÁîüÊàêÂ±ûÊÄßÔºåËøô‰∫õÂ±ûÊÄß‰∏éÊñáÊú¨Êó†ÂÖ≥„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏ç‰æùËµñ‰∫é‰πãÂâçÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊàñÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥Áõ¥Êé•ÂíåÈ´òÊïàÁöÑÁõëÁù£‰ø°Âè∑ÔºåÊòæËëóÊèêÂçá‰∫ÜÂõæÂÉèÁºñËæëÁöÑÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01043', 'title': 'Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities', 'url': 'https://huggingface.co/papers/2505.01043', 'abstract': 'Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 –º–∞—è', 'en': 'May 2', 'zh': '5Êúà2Êó•'}, 'hash': 'b67e3ec75756e896', 'authors': ['Zhiwei Hao', 'Jianyuan Guo', 'Li Shen', 'Yong Luo', 'Han Hu', 'Guoxia Wang', 'Dianhai Yu', 'Yonggang Wen', 'Dacheng Tao'], 'affiliations': ['Baidu Inc., Beijing 100000, China', 'College of Computing and Data Science, Nanyang Technological University, 639798, Singapore', 'Computer Science, City University of Hong Kong, Hong Kong 999077, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China', 'School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China', 'School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01043.jpg', 'data': {'categories': ['#optimization', '#inference', '#survey', '#training'], 'emoji': 'üî¨', 'ru': {'title': '–°–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã, —Ä–∞–∑–¥–µ–ª—è—è –∏—Ö –Ω–∞ —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–æ—á–∫–∏ –∏ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∏—Å–µ–ª —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –∏ –º–µ—Ç–æ–¥—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤. –í —Ä–∞–±–æ—Ç–µ —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—É—á–µ–Ω–∏—é —Å —É—á–µ—Ç–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ –Ω–∞–º–µ—á–∞—é—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –û–±–∑–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –µ–¥–∏–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ –ª–∞–Ω–¥—à–∞—Ñ—Ç–µ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.'}, 'en': {'title': 'Optimizing Large Language Models with Low-Precision Training', 'desc': 'This paper reviews low-precision training techniques for large language models (LLMs), which help improve training efficiency while reducing hardware resource demands. It categorizes these methods into three main groups based on numerical formats: fixed-point and integer-based, floating-point-based, and customized format-based methods. The survey also addresses quantization-aware training, which is closely related to low-precision training during model inference. By organizing the existing research, the paper aims to provide clarity and direction for future advancements in low-precision training.'}, 'zh': {'title': '‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÔºöÊèêÂçáÊïàÁéáÁöÑÂÖ≥ÈîÆ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ö‰∏™È¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóÁöÑË°®Áé∞Ôºå‰ΩÜÂÖ∂ËÆ≠ÁªÉÊâÄÈúÄÁöÑÁ°¨‰ª∂ËµÑÊ∫êÊûÅÂ§ßÔºåÊàê‰∏∫ÊïàÁéáÂíåÂèØÊâ©Â±ïÊÄßÁöÑÈöúÁ¢ç„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÊåëÊàòÔºå‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÊäÄÊúØË¢´ÂπøÊ≥õÈááÁî®ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§Ôºå‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÊ∂âÂèäÂ§ö‰∏™ÁªÑ‰ª∂ÔºåÂ¶ÇÊùÉÈáç„ÄÅÊøÄÊ¥ªÂíåÊ¢ØÂ∫¶ÔºåÊØè‰∏™ÁªÑ‰ª∂ÂèØ‰ª•Áî®‰∏çÂêåÁöÑÊï∞ÂÄºÊ†ºÂºèË°®Á§∫ÔºåÂØºËá¥Á†îÁ©∂È¢ÜÂüüÁöÑÁ¢éÁâáÂåñ„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜÁé∞ÊúâÁöÑ‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÊñπÊ≥ïÔºåÂπ∂Ê†πÊçÆÊï∞ÂÄºÊ†ºÂºèÂ∞ÜÂÖ∂Á≥ªÁªüÂú∞ÂàÜÁ±ª‰∏∫‰∏âÂ§ßÁ±ªÔºå‰ª•‰æø‰∫éÁ†îÁ©∂ËÄÖÁêÜËß£ÂíåÂèÇËÄÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02625', 'title': 'LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis', 'url': 'https://huggingface.co/papers/2505.02625', 'abstract': 'Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.', 'score': 7, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '2f8233e0d3083036', 'authors': ['Qingkai Fang', 'Yan Zhou', 'Shoutao Guo', 'Shaolei Zhang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02625.jpg', 'data': {'categories': ['#audio', '#small_models', '#benchmark'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–ü—Ä–æ—Ä—ã–≤ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–º –ò–ò: –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å LLaMA-Omni 2 - —Å–µ—Ä–∏—è —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Qwen2.5 –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∞ —Ä–µ—á–µ–≤—ã–º —ç–Ω–∫–æ–¥–µ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–µ—Ä–æ–º. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, LLaMA-Omni 2 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∞–Ω–∞–ª–æ–≥–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–º –æ–±—ä–µ–º–µ —Ä–µ—á–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Revolutionizing Speech Interaction with LLaMA-Omni 2', 'desc': 'This paper presents LLaMA-Omni 2, a new series of speech language models designed for real-time speech interaction. These models, ranging from 0.5B to 14B parameters, utilize a speech encoder and an autoregressive streaming speech decoder to facilitate intelligent spoken chatbots. Remarkably, LLaMA-Omni 2 achieves high performance despite being trained on only 200K multi-turn speech dialogue samples. It outperforms previous models like GLM-4-Voice, which were trained on significantly larger datasets, showcasing the efficiency of the new architecture.'}, 'zh': {'title': 'ÂÆûÊó∂Êô∫ËÉΩËØ≠Èü≥‰∫§‰∫íÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜLLaMA-Omni 2ÔºåËøôÊòØ‰∏Ä‰∏™Á≥ªÂàóÁöÑËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÔºåÂèÇÊï∞ËåÉÂõ¥‰ªé0.5‰∫øÂà∞14‰∫øÔºåËÉΩÂ§üÂÆûÁé∞È´òË¥®ÈáèÁöÑÂÆûÊó∂ËØ≠Èü≥‰∫§‰∫í„ÄÇËØ•Ê®°ÂûãÂü∫‰∫éQwen2.5Á≥ªÂàóÔºåÁªìÂêà‰∫ÜËØ≠Èü≥ÁºñÁ†ÅÂô®ÂíåËá™ÂõûÂΩíÊµÅÂºèËØ≠Èü≥Ëß£Á†ÅÂô®„ÄÇÂ∞ΩÁÆ°‰ªÖÂú®20‰∏áÂ§öËΩÆËØ≠Èü≥ÂØπËØùÊ†∑Êú¨‰∏äËøõË°åËÆ≠ÁªÉÔºåLLaMA-Omni 2Âú®Â§ö‰∏™ËØ≠Èü≥ÈóÆÁ≠îÂíåËØ≠Èü≥Êåá‰ª§Ë∑üÈöèÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄÂÖàËøõÊ®°ÂûãGLM-4-Voice„ÄÇËøôË°®ÊòéÔºå‰ΩøÁî®ËæÉÂ∞ëÁöÑÊï∞ÊçÆ‰πüËÉΩËÆ≠ÁªÉÂá∫È´òÊïàÁöÑÊô∫ËÉΩËØ≠Èü≥ËÅäÂ§©Êú∫Âô®‰∫∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01583', 'title': 'TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action', 'url': 'https://huggingface.co/papers/2505.01583', 'abstract': 'Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.', 'score': 6, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 –º–∞—è', 'en': 'May 2', 'zh': '5Êúà2Êó•'}, 'hash': 'd29565337415d11f', 'authors': ['Jen-Hao Cheng', 'Vivian Wang', 'Huayu Wang', 'Huapeng Zhou', 'Yi-Hao Peng', 'Hou-I Liu', 'Hsiang-Wei Huang', 'Kuang-Ming Chen', 'Cheng-Yen Yang', 'Wenhao Chai', 'Yi-Ling Chen', 'Vibhav Vineet', 'Qin Cai', 'Jenq-Neng Hwang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.01583.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#video', '#benchmark'], 'emoji': 'üé¨', 'ru': {'title': 'TEMPURA: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π', 'desc': 'TEMPURA - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –≤ –≤–∏–¥–µ–æ. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—à–∞–≥–æ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π. –ó–∞—Ç–µ–º TEMPURA —É—á–∏—Ç—Å—è —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö VER, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 1 –º–ª–Ω –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ 500 —Ç—ã—Å. –≤–∏–¥–µ–æ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ —Å–æ–±—ã—Ç–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TEMPURA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'TEMPURA: Enhancing Video Understanding through Causal Reasoning and Temporal Segmentation', 'desc': 'This paper introduces TEMPURA, a novel framework designed to improve how vision-language models understand the timing and relationships of events in videos. It addresses the limitations of existing methods that either lose important details by compressing video data or treat videos as continuous streams without clear boundaries. TEMPURA employs a two-stage training process that first predicts missing events to create causal explanations and then segments videos into distinct events with precise descriptions. The framework is trained on a large dataset, VER, and shows significant improvements in tasks like temporal grounding and highlight detection compared to existing models.'}, 'zh': {'title': 'TEMPURAÔºöÊèêÂçáËßÜÈ¢ëÁêÜËß£ÁöÑÂõ†ÊûúÊé®ÁêÜ‰∏éÊó∂Èó¥ÂàÜÂâ≤ÁªìÂêà', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫TEMPURAÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÊèêÈ´òËßÜÈ¢ëÁöÑÊó∂Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇTEMPURAÈÄöËøáÊé©ËîΩ‰∫ã‰ª∂È¢ÑÊµãÊé®ÁêÜÔºåÈáçÂª∫Áº∫Â§±‰∫ã‰ª∂Âπ∂ÁîüÊàêÈÄêÊ≠•ÁöÑÂõ†ÊûúËß£ÈáäÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£ËßÜÈ¢ë‰∏≠ÁöÑ‰∫ã‰ª∂ÂÖ≥Á≥ª„ÄÇÊé•ÁùÄÔºåÂÆÉÂ≠¶‰π†ËßÜÈ¢ëÂàÜÂâ≤ÂíåÂØÜÈõÜÊ†áÊ≥®ÔºåÂ∞ÜËßÜÈ¢ëÂàÜËß£‰∏∫‰∏çÈáçÂè†ÁöÑ‰∫ã‰ª∂ÔºåÂπ∂Êèê‰æõËØ¶ÁªÜÁöÑÊó∂Èó¥Êà≥ÂØπÈΩêÊèèËø∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTEMPURAÂú®Êó∂Èó¥ÂÆö‰ΩçÂíåÈ´ò‰∫ÆÊ£ÄÊµãÂü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫øÊ®°ÂûãÔºåÈ™åËØÅ‰∫ÜÂõ†ÊûúÊé®ÁêÜ‰∏éÁªÜÁ≤íÂ∫¶Êó∂Èó¥ÂàÜÂâ≤ÁöÑÁªìÂêàËÉΩÂ§üÊèêÂçáËßÜÈ¢ëÁêÜËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02823', 'title': 'MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing', 'url': 'https://huggingface.co/papers/2505.02823', 'abstract': 'Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.', 'score': 3, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'da9d48f7aea6c0bd', 'authors': ['Zinan Guo', 'Pengze Zhang', 'Yanze Wu', 'Chong Mou', 'Songtao Zhao', 'Qian He'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2505.02823.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#multimodal', '#data'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'MUSAR: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—Å—É–±—ä–µ–∫—Ç–Ω–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': 'MUSAR - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ—Å—É–±—ä–µ–∫—Ç–Ω–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–ø–ª–µ—Ç–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –º–µ–∂–¥—É —Å—É–±—ä–µ–∫—Ç–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∏–ø—Ç–∏—Ö–∞—Ö –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏—è. MUSAR –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—É–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å—É–±—ä–µ–∫—Ç–∞–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ MUSAR –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—É–±—ä–µ–∫—Ç–æ–≤ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.'}, 'en': {'title': 'MUSAR: Unlocking Multi-Subject Customization with Single-Subject Data', 'desc': 'The paper presents MUSAR, a novel framework designed to enhance multi-subject customization in machine learning. It addresses two main challenges: the scarcity of diverse multi-subject training data and the issue of attribute entanglement among different subjects. MUSAR employs debiased diptych learning to create training pairs from single-subject images, correcting biases through static attention routing and dual-branch LoRA. Additionally, it utilizes a dynamic attention routing mechanism to establish clear mappings between generated images and their corresponding subjects, leading to improved image quality and consistency across subjects.'}, 'zh': {'title': 'MUSARÔºöÂçï‰∏Ä‰∏ªÈ¢òÊï∞ÊçÆ‰∏ãÁöÑÂ§ö‰∏ªÈ¢òÂÆöÂà∂Êñ∞ÊñπÊ≥ï', 'desc': 'ÂΩìÂâçÁöÑÂ§ö‰∏ªÈ¢òÂÆöÂà∂ÊñπÊ≥ïÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöËé∑ÂèñÂ§öÊ†∑ÂåñÁöÑÂ§ö‰∏ªÈ¢òËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂõ∞ÈöæÔºå‰ª•Âèä‰∏çÂêå‰∏ªÈ¢ò‰πãÈó¥Â±ûÊÄßÁöÑÁ∫†Áº†„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMUSARÊ°ÜÊû∂ÔºåÂÆÉËÉΩÂ§üÂú®‰ªÖÈúÄÂçï‰∏Ä‰∏ªÈ¢òËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞Á®≥ÂÅ•ÁöÑÂ§ö‰∏ªÈ¢òÂÆöÂà∂„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂéªÂÅèÂ∑ÆÁöÑÂèåËÅîÂ≠¶‰π†ÔºåÈÄöËøá‰ªéÂçï‰∏Ä‰∏ªÈ¢òÂõæÂÉèÊûÑÂª∫ÂèåËÅîËÆ≠ÁªÉÂØπÔºå‰øÉËøõÂ§ö‰∏ªÈ¢òÂ≠¶‰π†ÔºåÂπ∂ÈÄöËøáÈùôÊÄÅÊ≥®ÊÑèÂäõË∑ØÁî±ÂíåÂèåÂàÜÊîØLoRA‰∏ªÂä®Á∫†Ê≠£ÂèåËÅîÊûÑÂª∫ÂºïÂÖ•ÁöÑÂàÜÂ∏ÉÂÅèÂ∑Æ„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂä®ÊÄÅÊ≥®ÊÑèÂäõË∑ØÁî±Êú∫Âà∂ÔºåÈÄÇÂ∫îÊÄßÂú∞Âª∫Á´ãÁîüÊàêÂõæÂÉè‰∏éÊù°‰ª∂‰∏ªÈ¢ò‰πãÈó¥ÁöÑÂèåÂ∞ÑÊò†Â∞ÑÔºå‰ªéËÄåÊ∂àÈô§Ë∑®‰∏ªÈ¢òÁöÑÁ∫†Áº†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02005', 'title': 'Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields', 'url': 'https://huggingface.co/papers/2505.02005', 'abstract': 'Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.', 'score': 3, 'issue_id': 3611, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': 'b0fc91d25884f885', 'authors': ['Zhenxing Mi', 'Ping Yin', 'Xue Xiao', 'Dan Xu'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR', 'Inspur Cloud Information Technology Co, Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2505.02005.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset'], 'emoji': 'üèôÔ∏è', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π NeRF –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Å—Ü–µ–Ω', 'desc': 'Switch-NeRF++ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π –∏–∑–ª—É—á–µ–Ω–∏—è (NeRF). –≠—Ç–∞ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—É—é —Å–º–µ—Å—å —Ö–µ—à-—ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (HMoHE) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è —Å—Ü–µ–Ω—ã –∏ –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö NeRF-—ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ö–ª—é—á–µ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è —Å–µ—Ç—å –≥–µ–π—Ç–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ —Å—Ü–µ–Ω—ã –∏ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–µ —Ö–µ—à-—ç–∫—Å–ø–µ—Ä—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. Switch-NeRF++ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –¥–ª—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —Å—Ü–µ–Ω.'}, 'en': {'title': 'Efficient Scene Decomposition for Scalable NeRFs', 'desc': 'This paper presents Switch-NeRF++, a novel approach to improve the scalability and efficiency of Neural Radiance Fields (NeRF) for large-scale scenes. It introduces a Heterogeneous Mixture of Hash Experts (HMoHE) network that learns to decompose scenes into specialized components, allowing for better handling of scene heterogeneity. The framework utilizes a gating network that allocates 3D points to different NeRF experts, optimizing the learning process in an end-to-end manner. The results show significant improvements in both training and rendering speeds, achieving state-of-the-art accuracy on large-scale datasets.'}, 'zh': {'title': 'È´òÊïàÂ§ßËßÑÊ®°Âú∫ÊôØÂª∫Ê®°ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Switch-NeRF++ÁöÑÁΩëÁªúÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßËßÑÊ®°Âú∫ÊôØ‰∏≠ÁöÑÂú∫ÊôØÂàÜËß£„ÄÅÂª∫Ê®°ÂºÇË¥®ÊÄßÂíåÂª∫Ê®°ÊïàÁéáÁ≠âÈóÆÈ¢ò„ÄÇËØ•ÁΩëÁªúÈááÁî®‰∫ÜÂºÇË¥®Ê∑∑ÂêàÂìàÂ∏å‰∏ìÂÆ∂ÔºàHMoHEÔºâÊû∂ÊûÑÔºåËÉΩÂ§üÈ´òÊïàÂú∞Â≠¶‰π†ÂºÇË¥®ÂàÜËß£ÂíåÂºÇË¥®NeRF„ÄÇÈÄöËøá‰∏Ä‰∏™Èó®ÊéßÁΩëÁªúÔºåÁ≥ªÁªüËÉΩÂ§üÂ∞Ü3DÁÇπÂàÜÈÖçÁªô‰∏ìÈó®ÁöÑNeRF‰∏ìÂÆ∂Ôºå‰ªéËÄåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ßËßÑÊ®°Âú∫ÊôØÂª∫Ê®°‰∏≠ÂÖ∑Êúâ‰ºòË∂äÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåËÆ≠ÁªÉÈÄüÂ∫¶ÊèêÂçá8ÂÄçÔºåÊ∏≤ÊüìÈÄüÂ∫¶ÊèêÂçá16ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01456', 'title': 'Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation', 'url': 'https://huggingface.co/papers/2505.01456', 'abstract': 'LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.', 'score': 2, 'issue_id': 3608, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '0201cbbcc6e52005', 'authors': ['Vaidehi Patil', 'Yi-Lin Sung', 'Peter Hase', 'Jie Peng', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science University of North Carolina at Chapel Hill', 'School of Artificial Intelligence and Data Science University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01456.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#interpretability', '#security', '#multimodal', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ó–∞–±—ã–≤–∞–Ω–∏–µ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ UnLOK-VQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —É–¥–∞–ª–µ–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π –∏–∑ MLLM. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞—Ç–∞–∫ –∏ –∑–∞—â–∏—Ç—ã, –≤–∫–ª—é—á–∞—é—â–∏–π —à–µ—Å—Ç—å —Ü–µ–ª–µ–π –∑–∞—â–∏—Ç—ã –∏ —Å–µ–º—å —Ç–∏–ø–æ–≤ –∞—Ç–∞–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∞—Ç–∞–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –æ–¥–Ω–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö, –∞ –Ω–∞–∏–ª—É—á—à–∞—è –∑–∞—â–∏—Ç–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —É–¥–∞–ª–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ—Ç–≤–µ—Ç–µ –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Enhancing Safety in Multimodal LLMs through Targeted Unlearning', 'desc': 'This paper discusses the risks associated with large language models (LLMs) that are trained on extensive datasets, particularly the unintentional acquisition of sensitive information. It highlights the challenges of multimodal LLMs, which combine text and images, making them more vulnerable to adversarial attacks that can extract private details. The authors introduce a new benchmark called UnLOK-VQA for evaluating targeted unlearning in multimodal contexts, along with a framework for assessing methods to remove specific knowledge from these models. Their findings indicate that multimodal attacks are more effective than those targeting only text or images, and that larger models tend to be more robust against such attacks, emphasizing the need for improved unlearning techniques in multimodal settings.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïèÊÑü‰ø°ÊÅØÈÅóÂøòÊåëÊàò', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜÊïèÊÑü‰ø°ÊÅØÊó∂ÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂõæÂÉèÂíåÊñáÊú¨ÁªìÂêàÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊîªÂáªËÄÖÂèØ‰ª•Âà©Áî®Â§öÊ®°ÊÄÅÊèêÁ§∫Êù•ÊèêÂèñËøô‰∫õÊïèÊÑü‰ø°ÊÅØÔºåÂõ†Ê≠§ÈúÄË¶ÅÊúâÊïàÁöÑÁõÆÊ†áÊÄßÈÅóÂøòÊú∫Âà∂„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïUnLOK-VQAÔºåÂπ∂Âª∫Á´ã‰∫Ü‰∏Ä‰∏™ÊîªÂáª‰∏éÈò≤Âæ°Ê°ÜÊû∂Ôºå‰ª•ËØÑ‰º∞‰ªéMLLMs‰∏≠Âà†Èô§ÁâπÂÆöÂ§öÊ®°ÊÄÅÁü•ËØÜÁöÑÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ§öÊ®°ÊÄÅÊîªÂáªÁöÑÊïàÊûú‰ºò‰∫éÂçï‰∏ÄÊñáÊú¨ÊàñÂõæÂÉèÊîªÂáªÔºåËÄåÊõ¥Â§ßÁöÑÊ®°ÂûãÂú®ÂêéÊúüÁºñËæëÊó∂Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåË°®ÊòéÊ®°ÂûãËßÑÊ®°ÂØπÂÆâÂÖ®ÊÄßÊúâÁßØÊûÅÂΩ±Âìç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02130', 'title': 'Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data', 'url': 'https://huggingface.co/papers/2505.02130', 'abstract': "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: https://github.com/millioniron/LLM_exploration{LLM4Exploration}", 'score': 1, 'issue_id': 3616, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 –º–∞—è', 'en': 'May 4', 'zh': '5Êúà4Êó•'}, 'hash': 'a07adea642e1877d', 'authors': ['Zhong Guan', 'Likang Wu', 'Hongke Zhao', 'Ming He', 'Jianpin Fan'], 'affiliations': ['AI Lab at Lenovo', 'College of Management and Economics, Tianjin University, Tianjin, China', 'Laboratory of Computation and Analytics of Complex Management Systems, Tianjin University, Tianjin, China', 'ai-deepcube'], 'pdf_title_img': 'assets/pdf/title_img/2505.02130.jpg', 'data': {'categories': ['#graphs', '#training', '#architecture', '#interpretability'], 'emoji': 'üï∏Ô∏è', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤–Ω–∏–º–∞–Ω–∏—è –≤ LLM –¥–ª—è –≥—Ä–∞—Ñ–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç, –∫–∞–∫ –º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (LLM) –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –≥—Ä–∞—Ñ–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ LLM –º–æ–≥—É—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≥—Ä–∞—Ñ–æ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —É–∑–ª–∞–º–∏. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è LLM –ø–æ —É–∑–ª–∞–º –≥—Ä–∞—Ñ–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–¥–µ–∞–ª—å–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –æ–∫–Ω–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è LLM –Ω–∞ –≥—Ä–∞—Ñ–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Unlocking LLMs: Enhancing Graph Understanding with Attention', 'desc': 'This paper investigates how large language models (LLMs) utilize attention mechanisms when processing graph-structured data. It highlights that while LLMs can identify interactions between text and graph nodes, they struggle with understanding the relationships between nodes due to their architectural limitations. The study reveals that the attention distribution in LLMs does not effectively reflect the underlying graph structure, leading to suboptimal performance. To address these issues, the authors propose using intermediate-state attention windows to enhance training and improve the transition to fully connected attention during inference.'}, 'zh': {'title': 'Êé¢Á¥¢Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÂõæÊï∞ÊçÆ‰∏≠ÁöÑË°®Áé∞', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ§ÑÁêÜÂõæÁªìÊûÑÊï∞ÊçÆÊó∂ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°LLMsËÉΩÂ§üËØÜÂà´ÂõæÊï∞ÊçÆÂπ∂ÊçïÊçâÊñáÊú¨‰∏éËäÇÁÇπ‰πãÈó¥ÁöÑ‰∫§‰∫íÔºå‰ΩÜÂú®Âª∫Ê®°ËäÇÁÇπÈó¥ÂÖ≥Á≥ªÊó∂Â≠òÂú®Âõ∞Èöæ„ÄÇÊ≥®ÊÑèÂäõÂàÜÂ∏ÉÊú™ËÉΩ‰∏éÁêÜÊÉ≥ÁöÑÁªìÊûÑÊ®°ÂºèÂØπÈΩêÔºåÊòæÁ§∫Âá∫ÂØπÂõæÊãìÊâëÁöÑÈÄÇÂ∫îÊÄß‰∏çË∂≥„ÄÇÈÄöËøáÂºïÂÖ•‰∏≠Èó¥Áä∂ÊÄÅÁöÑÊ≥®ÊÑèÂäõÁ™óÂè£ÔºåÁ†îÁ©∂Ë°®ÊòéÂèØ‰ª•ÊèêÈ´òLLMsÁöÑËÆ≠ÁªÉÊÄßËÉΩÔºåÂπ∂Âú®Êé®ÁêÜÊó∂Êó†ÁºùËøáÊ∏°Âà∞ÂÆåÂÖ®ËøûÊé•ÁöÑÁ™óÂè£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01548', 'title': 'Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional\n  Motion-enhanced Event Representation', 'url': 'https://huggingface.co/papers/2505.01548', 'abstract': 'Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.', 'score': 1, 'issue_id': 3619, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 –º–∞—è', 'en': 'May 2', 'zh': '5Êúà2Êó•'}, 'hash': '07a36a1d377e5cc7', 'authors': ['Zhen Yao', 'Xiaowen Ying', 'Mooi Choo Chuah'], 'affiliations': ['Lehigh University', 'Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01548.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': 'üé•', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Å–ª–∏—è–Ω–∏—è RGB –∏ —Å–æ–±—ã—Ç–∏–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö —Å RGB-–∫–∞–º–µ—Ä –∏ —Å–æ–±—ã—Ç–∏–π–Ω—ã—Ö –∫–∞–º–µ—Ä –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π - Motion-enhanced Event Tensor (MET), –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –≤–æ–∫—Å–µ–ª–∏ —Å–æ–±—ã—Ç–∏–π –≤ –ø–ª–æ—Ç–Ω—É—é –∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É. –¢–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç—Å—è –º–æ–¥—É–ª–∏ BFAM –∏ TFM –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏ –º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É RGB –∏ —Å–æ–±—ã—Ç–∏–π–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Enhancing RGB-Event Fusion with Motion-Enhanced Event Tensor', 'desc': 'This paper addresses the challenges of combining RGB images with event camera data in computer vision. It identifies three main types of misalignments: temporal, spatial, and modal, which hinder effective integration. To overcome these issues, the authors introduce the Motion-enhanced Event Tensor (MET), which creates a dense and coherent representation of events. Additionally, they propose two modules, BFAM and TFM, to further align the data and improve performance in semantic segmentation tasks, achieving superior results on benchmark datasets.'}, 'zh': {'title': 'ËøêÂä®Â¢ûÂº∫‰∫ã‰ª∂Âº†ÈáèÔºöËß£ÂÜ≥RGB-‰∫ã‰ª∂ËûçÂêàÊåëÊàòÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∫ã‰ª∂Ë°®Á§∫ÊñπÊ≥ïÔºåÁß∞‰∏∫ËøêÂä®Â¢ûÂº∫‰∫ã‰ª∂Âº†ÈáèÔºàMETÔºâÔºåÊó®Âú®Ëß£ÂÜ≥RGB-‰∫ã‰ª∂ËûçÂêà‰∏≠ÁöÑ‰∏âÁßçÂÜÖÂú®‰∏çÂØπÈΩêÈóÆÈ¢òÔºöÊó∂Èó¥„ÄÅÁ©∫Èó¥ÂíåÊ®°ÊÄÅ‰∏çÂØπÈΩê„ÄÇMETÈÄöËøáÂà©Áî®ÂØÜÈõÜÂÖâÊµÅÂíå‰∫ã‰ª∂Êó∂Èó¥ÁâπÂæÅÔºåÂ∞ÜÁ®ÄÁñè‰∫ã‰ª∂‰ΩìÁ¥†ËΩ¨Âåñ‰∏∫ÂØÜÈõÜ‰∏îÊó∂Èó¥‰∏ÄËá¥ÁöÑÂΩ¢ÂºèÔºå‰ªéËÄåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÈ¢ëÁéáÊÑüÁü•ÂèåÂêëÊµÅËÅöÂêàÊ®°ÂùóÔºàBFAMÔºâÂíåÊó∂Èó¥ËûçÂêàÊ®°ÂùóÔºàTFMÔºâÔºå‰ª•Ëß£ÂÜ≥Ê®°ÊÄÅÂíåÊó∂Á©∫‰∏çÂØπÈΩêÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®‰∏§‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑRGB-‰∫ã‰ª∂ËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02550', 'title': 'Bielik v3 Small: Technical Report', 'url': 'https://huggingface.co/papers/2505.02550', 'abstract': 'We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.', 'score': 53, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'a7c9d183be6447dd', 'authors': ['Krzysztof Ociepa', '≈Åukasz Flis', 'Remigiusz Kinas', 'Krzysztof Wr√≥bel', 'Adrian Gwo≈∫dziej'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02550.jpg', 'data': {'categories': ['#small_models', '#plp', '#multilingual', '#low_resource', '#dataset', '#benchmark'], 'emoji': 'üáµüá±', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—é—Ç –ò–ò –Ω–∞ –ø–æ–ª—å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–æ—Å—Ç—É–ø–Ω–µ–µ', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è Bielik v3 - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–µ–Ω—å—à–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–ª—å—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–∞ –¥–ª—è –º–µ–Ω–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤.'}, 'en': {'title': 'Efficient Polish Language Models: Big Performance from Small Sizes', 'desc': 'Bielik v3 introduces efficient generative text models specifically designed for the Polish language, with sizes of 1.5B and 4.5B parameters. These models show that smaller architectures can perform as well as larger ones while using less computational power. Key innovations include a custom tokenizer for better token efficiency, a specialized loss function to balance learning, and an adaptive learning rate that adjusts during training. With training on a vast dataset, these models set new standards for language processing in Polish, making advanced AI more accessible for various applications.'}, 'zh': {'title': 'È´òÊïàÊ≥¢ÂÖ∞ËØ≠ÁîüÊàêÊ®°ÂûãÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜBielik v3ÔºåËøôÊòØ‰∏ÄÁ≥ªÂàóÈíàÂØπÊ≥¢ÂÖ∞ËØ≠Â§ÑÁêÜÁöÑÈ´òÊïàÁîüÊàêÊñáÊú¨Ê®°ÂûãÔºà1.5BÂíå4.5BÂèÇÊï∞Ôºâ„ÄÇËøô‰∫õÊ®°ÂûãË°®ÊòéÔºåËæÉÂ∞è‰∏îÁªèËøá‰ºòÂåñÁöÑÊû∂ÊûÑÂèØ‰ª•Âú®ËÆ°ÁÆóËµÑÊ∫êÂ§ßÂπÖÂáèÂ∞ëÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∏éÊõ¥Â§ßÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÂàõÊñ∞ÂåÖÊã¨ÂÆöÂà∂ÁöÑÊ≥¢ÂÖ∞ËØ≠ÂàÜËØçÂô®ÔºàAPT4ÔºâÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ†áËÆ∞ÊïàÁéáÔºå‰ª•ÂèäÂä†ÊùÉÊåá‰ª§‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÂπ≥Ë°°‰∏çÂêåÊåá‰ª§Á±ªÂûãÁöÑÂ≠¶‰π†„ÄÇÊ≠§Â§ñÔºåÂä®ÊÄÅË∞ÉÊï¥ÁöÑÂ≠¶‰π†ÁéáÊ†πÊçÆËÆ≠ÁªÉËøõÂ∫¶ËøõË°åË∞ÉÊï¥Ôºå‰ΩøÂæóÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02410', 'title': 'Bielik 11B v2 Technical Report', 'url': 'https://huggingface.co/papers/2505.02410', 'abstract': "We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.", 'score': 44, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'e9cb82cbeaac24ed', 'authors': ['Krzysztof Ociepa', '≈Åukasz Flis', 'Krzysztof Wr√≥bel', 'Adrian Gwo≈∫dziej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02410.jpg', 'data': {'categories': ['#inference', '#multilingual', '#low_resource', '#training', '#benchmark', '#architecture', '#optimization', '#reasoning'], 'emoji': 'üáµüá±', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: Bielik 11B v2 —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Bielik 11B v2 - —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª—å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤. –û—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mistral 7B v0.2 –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ 11 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –ø–æ–ª—å—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –í –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏: –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. Bielik 11B v2 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.'}, 'en': {'title': 'Revolutionizing Polish Language Processing with Bielik 11B v2', 'desc': 'Bielik 11B v2 is a cutting-edge language model specifically designed for processing Polish text. It utilizes the Mistral 7B v0.2 architecture and has been enhanced to 11 billion parameters, achieving remarkable results on Polish language tasks while also performing well in cross-lingual scenarios. The model introduces innovative techniques like Weighted Instruction Cross-Entropy Loss for better learning from diverse instructions and an Adaptive Learning Rate that adjusts based on the context length. Its efficiency and quantization options allow it to run on various hardware, making it a significant advancement in AI for Polish language applications.'}, 'zh': {'title': 'Ê≥¢ÂÖ∞ËØ≠Â§ÑÁêÜÁöÑÊñ∞Ê†áÊùÜÔºöBielik 11B v2', 'desc': 'Bielik 11B v2 ÊòØ‰∏Ä‰∏™ÈíàÂØπÊ≥¢ÂÖ∞ËØ≠ÊñáÊú¨Â§ÑÁêÜÁöÑÂÖàËøõËØ≠Ë®ÄÊ®°ÂûãÔºåÂü∫‰∫é Mistral 7B v0.2 Êû∂ÊûÑÔºåÂèÇÊï∞ËßÑÊ®°ËææÂà∞ 11B„ÄÇËØ•Ê®°ÂûãÂú®Ê≥¢ÂÖ∞ËØ≠Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂêåÊó∂ÂÖ∑Â§áÂº∫Â§ßÁöÑË∑®ËØ≠Ë®ÄËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§È°πÂÖ≥ÈîÆÊäÄÊúØÂàõÊñ∞ÔºöÂä†ÊùÉÊåá‰ª§‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÈÄöËøá‰∏∫ËÆ≠ÁªÉÊ†∑Êú¨ÂàÜÈÖçÂü∫‰∫éË¥®ÈáèÁöÑÊùÉÈáçÊù•‰ºòÂåñ‰∏çÂêåÊåá‰ª§Á±ªÂûãÁöÑÂ≠¶‰π†Ôºå‰ª•ÂèäËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÔºåÊ†πÊçÆ‰∏ä‰∏ãÊñáÈïøÂ∫¶Âä®ÊÄÅË∞ÉÊï¥„ÄÇÁªºÂêàËØÑ‰º∞ÊòæÁ§∫ÔºåBielik 11B v2 Ë∂ÖË∂ä‰∫ÜËÆ∏Â§öÊõ¥Â§ßÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂Âú®ËØ≠Ë®ÄÁêÜËß£ÂíåÂ§çÊùÇÊé®ÁêÜÁ≠â‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÂÖ∂‰ªñ‰∏ìÈó®ÁöÑÊ≥¢ÂÖ∞ËØ≠Ë®ÄÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06111', 'title': 'UniVLA: Learning to Act Anywhere with Task-centric Latent Actions', 'url': 'https://huggingface.co/papers/2505.06111', 'abstract': "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.", 'score': 17, 'issue_id': 3704, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': 'bf19981dd100b8fb', 'authors': ['Qingwen Bu', 'Yanting Yang', 'Jisong Cai', 'Shenyuan Gao', 'Guanghui Ren', 'Maoqing Yao', 'Ping Luo', 'Hongyang Li'], 'affiliations': ['AgiBot', 'OpenDriveLab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.06111.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#benchmark', '#agents', '#transfer_learning'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ –∏ —è–∑—ã–∫', 'desc': 'UniVLA - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ä–æ–±–æ—Ç–∞ —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑—Ä–µ–Ω–∏—è, —è–∑—ã–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–π. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑ –≤–∏–¥–µ–æ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–æ–ø–ª–æ—â–µ–Ω–∏–π –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. UniVLA –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å OpenVLA, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏ —ç—Ç–æ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'UniVLA: Empowering Robots with Cross-Embodiment Learning', 'desc': 'The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches.'}, 'zh': {'title': 'UniVLAÔºöÊèêÂçáÈÄöÁî®Êú∫Âô®‰∫∫Â≠¶‰π†ÊïàÁéáÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂UniVLAÔºåÁî®‰∫éÂ≠¶‰π†Ë∑®‰ΩìÁé∞ÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òÈÄöÁî®Êú∫Âô®‰∫∫Âú®‰∏çÂêåÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÈÄöËøáËßÜÈ¢ë‰∏≠ÁöÑÊΩúÂú®Âä®‰ΩúÊ®°ÂûãÊèêÂèñ‰ª•‰ªªÂä°‰∏∫‰∏≠ÂøÉÁöÑÂä®‰ΩúË°®Á§∫Ôºå‰ªéËÄåÂà©Áî®ÂπøÊ≥õÁöÑÂ§öÊ†∑ÂåñÊï∞ÊçÆ„ÄÇ‰∏∫‰∫ÜÂáèÂ∞ë‰∏é‰ªªÂä°Êó†ÂÖ≥ÁöÑÂä®ÊÄÅÂΩ±ÂìçÔºåÊàë‰ª¨ÁªìÂêà‰∫ÜËØ≠Ë®ÄÊåá‰ª§ÔºåÂπ∂Âú®DINOÁâπÂæÅÁ©∫Èó¥‰∏≠Âª∫Á´ã‰∫ÜÊΩúÂú®Âä®‰ΩúÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniVLAÂú®Â§ö‰∏™Êìç‰ΩúÂíåÂØºËà™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÂú®È¢ÑËÆ≠ÁªÉËÆ°ÁÆóÂíå‰∏ãÊ∏∏Êï∞ÊçÆÊñπÈù¢ÁöÑÈúÄÊ±ÇÊòæËëó‰Ωé‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05026', 'title': 'G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness', 'url': 'https://huggingface.co/papers/2505.05026', 'abstract': 'Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.', 'score': 11, 'issue_id': 3705, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '41e61eccd430ea55', 'authors': ['Jaehyun Jeon', 'Jang Han Yoon', 'Min Soo Kim', 'Sumin Shim', 'Yejin Choi', 'Hanbin Kim', 'Youngjae Yu'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05026.jpg', 'data': {'categories': ['#reasoning', '#cv', '#benchmark', '#optimization', '#inference'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ UI –±–µ–∑ A/B-—Ç–µ—Å—Ç–æ–≤', 'desc': '–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏—Ö —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∏ –≤–≤–æ–¥—è—Ç –±–µ–Ω—á–º–∞—Ä–∫ WiserUI-Bench –¥–ª—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –¥–∏–∑–∞–π–Ω–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 300 –ø–∞—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö UI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ A/B-—Ç–µ—Å—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é G-FOCUS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ G-FOCUS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤.'}, 'en': {'title': 'Revolutionizing UI Evaluation with G-FOCUS and WiserUI-Bench', 'desc': 'This paper discusses the importance of evaluating user interface (UI) design not just for its visual appeal but for its ability to influence user behavior, a concept known as Design Persuasiveness. The authors highlight the limitations of traditional A/B testing, which is often expensive and slow, and propose a new benchmark called WiserUI-Bench for assessing UI design effectiveness through pairwise comparisons. They introduce G-FOCUS, an innovative reasoning strategy that improves the accuracy of Vision-Language Models (VLMs) in evaluating UI persuasiveness by minimizing biases. The results demonstrate that G-FOCUS outperforms existing methods, paving the way for more efficient and scalable UI design optimization.'}, 'zh': {'title': 'ÊèêÂçáÁî®Êà∑ÁïåÈù¢ËÆæËÆ°ÁöÑËØ¥ÊúçÂäõËØÑ‰º∞', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁî®Êà∑ÁïåÈù¢ÔºàUIÔºâËÆæËÆ°ÁöÑÊúâÊïàÊÄßËØÑ‰º∞ÔºåÂº∫Ë∞ÉËÆæËÆ°ÁöÑËØ¥ÊúçÂäõÂØπÁî®Êà∑Ë°å‰∏∫ÁöÑÂΩ±Âìç„ÄÇ‰º†ÁªüÁöÑA/BÊµãËØïÊñπÊ≥ïËôΩÁÑ∂Â∏∏Áî®Ôºå‰ΩÜÊàêÊú¨È´ò‰∏îËÄóÊó∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜWiserUI-BenchÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÊàêÂØπUIËÆæËÆ°ËØ¥ÊúçÂäõËØÑ‰º∞ÁöÑÂü∫ÂáÜÔºåÂåÖÂê´300ÂØπÁúüÂÆûÁöÑUIÂõæÂÉèÂèäÂÖ∂A/BÊµãËØïÁªìÊûúÂíå‰∏ìÂÆ∂ÁêÜÁî±„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜG-FOCUSÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊé®ÁêÜÁ≠ñÁï•ÔºåËÉΩÂ§üÊèêÈ´òÂü∫‰∫éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑËØ¥ÊúçÂäõËØÑ‰º∞ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02686', 'title': 'Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2505.02686', 'abstract': 'Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.', 'score': 11, 'issue_id': 3706, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '22b290e68229e62f', 'authors': ['Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02686.jpg', 'data': {'categories': ['#survey', '#training', '#rlhf', '#alignment', '#benchmark', '#reasoning', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º: –∫–ª—é—á –∫ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã –Ω–∞ —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è, –≤—ã–≤–æ–¥–∞ –∏ –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏. –û–±—Å—É–∂–¥–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç LLM –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –æ—Ç –ø–∞—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∫ –∞–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. –í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏ –±—É–¥—É—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Harnessing Rewards: The Future of Learning in LLMs', 'desc': 'This paper discusses the evolution of Large Language Models (LLMs) focusing on the shift from pre-training to learning from rewards. It highlights how reward signals guide LLM behavior through techniques like reinforcement learning, reward-guided decoding, and post-hoc correction. The authors categorize various strategies used in training, inference, and post-inference stages, emphasizing the importance of dynamic feedback for improving model alignment and reasoning. Additionally, the paper addresses benchmarks for reward models and outlines future challenges and directions in this area.'}, 'zh': {'title': '‰ªéÂ•ñÂä±‰∏≠Â≠¶‰π†ÔºåËµãËÉΩÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'desc': 'ÊúÄËøëÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ï‰ªéÈ¢ÑËÆ≠ÁªÉÊâ©Â±ïÂà∞ÂêéËÆ≠ÁªÉÂíåÊµãËØïÊó∂Êâ©Â±ï„ÄÇ‰∏Ä‰∏™ÂÖ≥ÈîÆÁöÑÁªü‰∏ÄËåÉÂºèÂá∫Áé∞‰∫ÜÔºö‰ªéÂ•ñÂä±‰∏≠Â≠¶‰π†ÔºåÂÖ∂‰∏≠Â•ñÂä±‰ø°Âè∑‰Ωú‰∏∫ÊåáÂØºÊòüÔºåÂºïÂØºLLMÁöÑË°å‰∏∫„ÄÇËøô‰∏™ËåÉÂºèÊîØÊåÅ‰∫ÜËÆ∏Â§öÊµÅË°åÁöÑÊäÄÊúØÔºåÂ¶ÇÂº∫ÂåñÂ≠¶‰π†ÔºàÂú®RLHF„ÄÅDPOÂíåGRPO‰∏≠Ôºâ„ÄÅÂ•ñÂä±ÂºïÂØºËß£Á†ÅÂíå‰∫ãÂêé‰øÆÊ≠£„ÄÇÈÄöËøáËøô‰∏™ËåÉÂºèÔºåLLMsËÉΩÂ§ü‰ªéÈùôÊÄÅÊï∞ÊçÆÁöÑË¢´Âä®Â≠¶‰π†ËΩ¨Âêë‰ªéÂä®ÊÄÅÂèçÈ¶àÁöÑ‰∏ªÂä®Â≠¶‰π†ÔºåËµã‰∫àÂÆÉ‰ª¨ÂØπÈΩêÁöÑÂÅèÂ•ΩÂíåÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06046', 'title': 'Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information', 'url': 'https://huggingface.co/papers/2505.06046', 'abstract': "As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.", 'score': 10, 'issue_id': 3707, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': 'c671de3a9e8ff4de', 'authors': ['Joshua Harris', 'Fan Grayson', 'Felix Feldman', 'Timothy Laurence', 'Toby Nonnenmacher', 'Oliver Higgins', 'Leo Loman', 'Selina Patel', 'Thomas Finnie', 'Samuel Collins', 'Michael Borowitz'], 'affiliations': ['UK Health Security Agency (UKHSA)'], 'pdf_title_img': 'assets/pdf/title_img/2505.06046.jpg', 'data': {'categories': ['#alignment', '#science', '#dataset', '#healthcare', '#benchmark'], 'emoji': 'üè•', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π LLM –≤ —Å—Ñ–µ—Ä–µ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PubHealthBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 8000 –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –∏ —Å–≤–æ–±–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–æ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –Ω–æ–≤–µ–π—à–∏–µ —á–∞—Å—Ç–Ω—ã–µ LLM –¥–æ—Å—Ç–∏–≥–∞—é—Ç –±–æ–ª–µ–µ 90% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ª—é–¥–µ–π —Å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º. –û–¥–Ω–∞–∫–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–æ —Å–≤–æ–±–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–æ–π –æ—Ç–≤–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∏–∂–µ, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ LLM –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏.'}, 'en': {'title': 'Evaluating LLMs for Public Health: Promising Yet Cautious', 'desc': "This paper investigates the knowledge of Large Language Models (LLMs) in the domain of UK public health information. It introduces a benchmark called PubHealthBench, which consists of over 8000 questions designed to evaluate LLMs' performance in Multiple Choice Question Answering (MCQA) and free form responses. The study assesses 24 different LLMs, revealing that the latest models perform well in MCQA tasks, achieving over 90% accuracy, but struggle with free form responses, none scoring above 75%. The findings suggest that while LLMs show promise as reliable sources of public health information, caution is needed when interpreting their free form outputs."}, 'zh': {'title': 'ÊèêÂçáÂÖ¨ÂÖ±Âç´Áîü‰ø°ÊÅØÁöÑÂáÜÁ°ÆÊÄß', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂπøÊ≥õÂ∫îÁî®Ôºå‰∫ÜËß£ÂÆÉ‰ª¨Âú®ÁâπÂÆöÈ¢ÜÂüüÁöÑÁü•ËØÜÂèòÂæóËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÖ¨ÂÖ±Âç´ÁîüÈ¢ÜÂüü„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïPubHealthBenchÔºåÂåÖÂê´Ë∂ÖËøá8000‰∏™ÈóÆÈ¢òÔºåÁî®‰∫éËØÑ‰º∞LLMsÂú®ÂÖ¨ÂÖ±Âç´ÁîüÊü•ËØ¢‰∏≠ÁöÑÂ§öÈ°πÈÄâÊã©ÈóÆÁ≠îÔºàMCQAÔºâÂíåËá™Áî±ÂΩ¢ÂºèÂõûÁ≠îÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊúÄÊñ∞ÁöÑÁßÅÊúâLLMsÔºàÂ¶ÇGPT-4.5ÂíåGPT-4.1ÔºâÂú®MCQAÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂáÜÁ°ÆÁéáË∂ÖËøá90%ÔºåÁîöËá≥Ë∂ÖË∂ä‰∫Ü‰∫∫Á±ªÁöÑÊêúÁ¥¢ÂºïÊìé‰ΩøÁî®„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá™Áî±ÂΩ¢ÂºèÂõûÁ≠î‰∏≠ÔºåÊ®°ÂûãÁöÑË°®Áé∞ËæÉ‰ΩéÔºåÊ≤°Êúâ‰∏Ä‰∏™Ê®°ÂûãÂæóÂàÜË∂ÖËøá75%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05621', 'title': 'A Preliminary Study for GPT-4o on Image Restoration', 'url': 'https://huggingface.co/papers/2505.05621', 'abstract': "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.", 'score': 4, 'issue_id': 3709, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '4fd37a23cd5db52f', 'authors': ['Hao Yang', 'Yan Yang', 'Ruikun Zhang', 'Liyuan Pan'], 'affiliations': ['Australian National University', 'Beijing Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.05621.jpg', 'data': {'categories': ['#optimization', '#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'GPT-4o: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–ú–æ–¥–µ–ª—å GPT-4o –æ—Ç OpenAI –ø–æ–∫–∞–∑–∞–ª–∞ –±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É GPT-4o –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –•–æ—Ç—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∏–∑—É–∞–ª—å–Ω–æ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã, –æ–Ω–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ GPT-4o –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –º–æ—â–Ω—ã–º–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∞–º–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–µ—Ç–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—ã–º–∫–∏, –¥–æ–∂–¥—è –∏ —É–ª—É—á—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –æ—Å–≤–µ—â–µ–Ω–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Harnessing GPT-4o for Enhanced Image Restoration', 'desc': "The paper explores the capabilities of OpenAI's GPT-4o model in the field of image restoration, highlighting its ability to generate visually appealing images. Despite its impressive outputs, the model struggles with maintaining pixel-level accuracy, leading to issues like incorrect object positioning and altered image proportions. The authors demonstrate that GPT-4o can enhance existing image restoration techniques, particularly in tasks like dehazing and deraining, by providing valuable visual priors. This work aims to establish a foundation for integrating GPT-4o into future restoration workflows and encourages further research in image generation."}, 'zh': {'title': 'GPT-4oÔºöÂõæÂÉè‰øÆÂ§çÁöÑÊñ∞Âä®Âäõ', 'desc': 'OpenAIÁöÑGPT-4oÊ®°ÂûãÁªìÂêà‰∫ÜÂ§öÊ®°ÊÄÅËæìÂÖ•ÂíåËæìÂá∫ÔºåÂ±ïÁé∞‰∫ÜÂú®ÂõæÂÉèÁîüÊàêÊñπÈù¢ÁöÑÂçìË∂äÊÄßËÉΩ„ÄÇÊú¨ÊñáÁ≥ªÁªüËØÑ‰º∞‰∫ÜGPT-4oÂú®ÂõæÂÉè‰øÆÂ§ç‰ªªÂä°‰∏≠ÁöÑÊΩúÂú®ÂΩ±ÂìçÔºåÂ∞ΩÁÆ°ÂÖ∂ÁîüÊàêÁöÑ‰øÆÂ§çÂõæÂÉèÂú®ËßÜËßâ‰∏äÂê∏Âºï‰∫∫Ôºå‰ΩÜÂú®ÂÉèÁ¥†Á∫ßÁªìÊûÑ‰øùÁúüÂ∫¶‰∏ä‰∏éÁúüÂÆûÂõæÂÉèÁõ∏ÊØîÂ≠òÂú®ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøáÂõæÂÉèÂéªÈõæ„ÄÅÂéªÈõ®Âíå‰ΩéÂÖâÂ¢ûÂº∫Á≠âÊ°à‰æãÁ†îÁ©∂ÔºåÂ±ïÁ§∫‰∫ÜGPT-4oÁöÑËæìÂá∫ÂèØ‰ª•‰Ωú‰∏∫Âº∫Â§ßÁöÑËßÜËßâÂÖàÈ™åÔºåÊòæËëóÊèêÂçáÁé∞ÊúâÂéªÈõæÁΩëÁªúÁöÑÊÄßËÉΩ„ÄÇÂ∏åÊúõÊú¨Á†îÁ©∂ËÉΩÂä†ÈÄüÂõæÂÉèÁîüÊàêÈ¢ÜÂüüÁöÑÂàõÊñ∞ÔºåÂπ∂Â∞ÜÂèëÂ∏ÉÊù•Ëá™10‰∏™ÂπøÊ≥õ‰ΩøÁî®ÁöÑÂõæÂÉè‰øÆÂ§çÊï∞ÊçÆÈõÜÁöÑGPT-4o‰øÆÂ§çÂõæÂÉè‰ª•ÊîØÊåÅËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21467', 'title': 'Multiview Point Cloud Registration via Optimization in an Autoencoder\n  Latent Space', 'url': 'https://huggingface.co/papers/2504.21467', 'abstract': 'Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.', 'score': 0, 'issue_id': 3717, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '0bb5c596b88819cc', 'authors': ['Luc Vedrenne', 'Sylvain Faisan', 'Denis Fortun'], 'affiliations': ['ICube Laboratory, IMAGeS team, UMR 7357, CNRS, University of Strasbourg, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.21467.jpg', 'data': {'categories': ['#3d', '#cv', '#synthetic', '#optimization', '#open_source'], 'emoji': 'üåÄ', 'ru': {'title': 'POLAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫', 'desc': 'POLAR - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫, —Å–ø–æ—Å–æ–±–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∫—É—Ä—Å–æ–≤. –û–Ω –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –∑–∞–¥–∞—á—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. POLAR —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã—Å–æ–∫–æ–º—É —É—Ä–æ–≤–Ω—é –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–π –∏ –±–æ–ª—å—à–∏–º –Ω–∞—á–∞–ª—å–Ω—ã–º —É–≥–ª–∞–º –º–µ–∂–¥—É –æ–±–ª–∞–∫–∞–º–∏ —Ç–æ—á–µ–∫. –ú–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'POLAR: Efficient and Robust Multiview Point Cloud Registration', 'desc': 'This paper presents POLAR, a new method for aligning multiple 3D point clouds, which is crucial in computer vision. Unlike traditional pairwise registration methods that struggle with scalability, POLAR efficiently handles many views and is robust against significant data degradation. The approach utilizes a pretrained autoencoder to transform the registration task into a latent space, allowing for better optimization. By incorporating a specialized loss function and a multistart optimization strategy, POLAR outperforms existing techniques on both synthetic and real datasets.'}, 'zh': {'title': 'POLARÔºöÈ´òÊïàÁöÑÂ§öËßÜËßíÁÇπ‰∫ëÈÖçÂáÜÊñπÊ≥ï', 'desc': 'ÁÇπ‰∫ëÂàöÊÄßÈÖçÂáÜÊòØ3DËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠ÁöÑ‰∏Ä‰∏™Âü∫Êú¨ÈóÆÈ¢ò„ÄÇÂú®Â§öËßÜËßíÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÊâæÂà∞‰∏ÄÁªÑ6DÂßøÊÄÅÊù•ÂØπÈΩê‰∏ÄÁªÑÁâ©‰Ωì„ÄÇ‰º†ÁªüÁöÑÂü∫‰∫éÊàêÂØπÈÖçÂáÜÁöÑÊñπÊ≥ïÂú®ËßÜËßíÊï∞ÈáèÂ¢ûÂä†Êó∂Êâ©Â±ïÊÄßËæÉÂ∑ÆÔºåËÄåÁîüÊàêÊñπÊ≥ïËôΩÁÑ∂ÂÖãÊúç‰∫ÜËøô‰∏ÄÈôêÂà∂Ôºå‰ΩÜ‰∏çÈÄÇÂêàÂ§ÑÁêÜÂ§ßÂèòÊç¢ÂíåÈ´òÈôçÁ∫ßÊ∞¥Âπ≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜPOLARÔºàÁÇπ‰∫ëÊΩúÂú®ÈÖçÂáÜÔºâÔºåÂÆÉËÉΩÂ§üÈ´òÊïàÂ§ÑÁêÜÂ§ßÈáèËßÜËßíÔºåÂêåÊó∂ÂØπÈ´òÈôçÁ∫ßÂíåÂ§ßÂàùÂßãËßíÂ∫¶ÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21635', 'title': 'Sadeed: Advancing Arabic Diacritization Through Small Language Model', 'url': 'https://huggingface.co/papers/2504.21635', 'abstract': "Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.", 'score': 44, 'issue_id': 3530, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': 'af5a1b038b3ccab3', 'authors': ['Zeina Aldallal', 'Sara Chrouf', 'Khalil Hennara', 'Mohamed Motaism Hamed', 'Muhammad Hreden', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21635.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#multilingual', '#benchmark', '#data', '#machine_translation'], 'emoji': 'üî†', 'ru': {'title': 'Sadeed: –ü—Ä–æ—Ä—ã–≤ –≤ –¥–∏–∞–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–ø–∞–∫—Ç–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏', 'desc': 'Sadeed - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∞–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–æ–¥–µ–ª–∏ Kuwain 1.5B. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, Sadeed –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SadeedDiac-25 –¥–ª—è –±–æ–ª–µ–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–∏–∞–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∞—Ä–∞–±—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Sadeed: Advancing Arabic Diacritization with Efficiency and Precision', 'desc': 'This paper presents Sadeed, a new method for Arabic text diacritization using a fine-tuned decoder-only language model based on Kuwain 1.5B. Sadeed is specifically trained on high-quality diacritized datasets, which were created through a thorough data-cleaning process. The model demonstrates competitive performance against larger proprietary models while requiring less computational power. Additionally, the authors introduce SadeedDiac-25, a new benchmark for evaluating Arabic diacritization, aiming to improve assessment practices in the field.'}, 'zh': {'title': 'SadeedÔºöÈòøÊãâ‰ºØËØ≠Ê†áËÆ∞ÂåñÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈòøÊãâ‰ºØËØ≠ÊñáÊú¨Ê†áËÆ∞ÂåñÊñπÊ≥ïÔºåÂêç‰∏∫Sadeed„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éÁªèËøáÂæÆË∞ÉÁöÑËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ìÈó®ÈíàÂØπÈòøÊãâ‰ºØËØ≠ÁöÑ‰∏∞ÂØåÂΩ¢ÊÄÅÁâπÂæÅËøõË°å‰ºòÂåñ„ÄÇSadeedÂú®È´òË¥®ÈáèÁöÑÊ†áËÆ∞ÂåñÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºåÂ∞ΩÁÆ°ËÆ°ÁÆóËµÑÊ∫êÊúâÈôêÔºå‰ΩÜÂÖ∂ÊÄßËÉΩ‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÂΩìÔºå‰∏î‰ºò‰∫é‰º†ÁªüÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèêÂá∫‰∫ÜSadeedDiac-25Âü∫ÂáÜÔºå‰ª•‰øÉËøõÂØπÈòøÊãâ‰ºØËØ≠Ê†áËÆ∞ÂåñÁöÑÂÖ¨Âπ≥ËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21776', 'title': 'WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability', 'url': 'https://huggingface.co/papers/2504.21776', 'abstract': 'Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.', 'score': 26, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '61ce82abe42f584a', 'authors': ['Xiaoxi Li', 'Jiajie Jin', 'Guanting Dong', 'Hongjin Qian', 'Yutao Zhu', 'Yongkang Wu', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21776.jpg', 'data': {'categories': ['#rl', '#science', '#agents', '#rlhf', '#optimization', '#reasoning', '#benchmark'], 'emoji': 'üï∏Ô∏è', 'ru': {'title': 'WebThinker: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π', 'desc': 'WebThinker - —ç—Ç–æ –≥–ª—É–±–æ–∫–∞—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å Deep Web Explorer –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –∞ —Ç–∞–∫–∂–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è, –ø–æ–∏—Å–∫–∞ –∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–æ–≤. WebThinker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (DPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ WebThinker –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤.'}, 'en': {'title': 'Empowering LRMs with Real-Time Web Research Capabilities', 'desc': "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."}, 'zh': {'title': 'WebThinkerÔºöËÆ©Êé®ÁêÜÊ®°ÂûãÊõ¥Êô∫ËÉΩÁöÑÁ†îÁ©∂Âä©Êâã', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂ¶ÇOpenAI-o1ÂíåDeepSeek-R1Âú®ÈïøÊó∂Èó¥Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨‰æùËµñÈùôÊÄÅÂÜÖÈÉ®Áü•ËØÜÔºåÈôêÂà∂‰∫ÜÂú®Â§çÊùÇÁü•ËØÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜWebThinkerÔºå‰∏Ä‰∏™Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÔºåËÉΩÂ§üËÆ©LRMsËá™‰∏ªÊêúÁ¥¢ÁΩëÁªú„ÄÅÊµèËßàÁΩëÈ°µÂπ∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠Êí∞ÂÜôÁ†îÁ©∂Êä•Âëä„ÄÇWebThinkerÈõÜÊàê‰∫ÜÊ∑±ÁΩëÊé¢Á¥¢Ê®°ÂùóÔºå‰ΩøLRMsÂú®ÈÅáÂà∞Áü•ËØÜÁ©∫ÁôΩÊó∂ËÉΩÂ§üÂä®ÊÄÅÊêúÁ¥¢ÂíåÊèêÂèñ‰ø°ÊÅØ„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéWebThinkerÂú®Â§çÊùÇÊé®ÁêÜÂü∫ÂáÜÂíåÁßëÂ≠¶Êä•ÂëäÁîüÊàê‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21850', 'title': 'COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning', 'url': 'https://huggingface.co/papers/2504.21850', 'abstract': 'Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.', 'score': 20, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '3db97f245360deb4', 'authors': ['Xindi Wu', 'Hee Seung Hwang', 'Polina Kirichenko', 'Olga Russakovsky'], 'affiliations': ['Meta AI', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21850.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#data', '#optimization', '#benchmark'], 'emoji': 'üß©', 'ru': {'title': 'COMPACT: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ MLLM —Å–ª–æ–∂–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º COMPACT. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤. COMPACT –ø–æ–∑–≤–æ–ª—è–µ—Ç MLLM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –æ–±—É—á–∞—Ç—å—Å—è —Å–ª–æ–∂–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –∫–æ–º–±–∏–Ω–∏—Ä—É—è –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å LLaVA-665k, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ–Ω–µ–µ 10% –¥–∞–Ω–Ω—ã—Ö, –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'Unlocking Complex Tasks with Efficient Compositional Training', 'desc': "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."}, 'zh': {'title': 'ÊèêÂçáÂ§çÊùÇËßÜËßâËØ≠Ë®Ä‰ªªÂä°ÁöÑËÉΩÂäõ', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÁÆÄÂçïÁöÑËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶ÅÂ§öÁßçËÉΩÂäõÁöÑÂ§çÊùÇ‰ªªÂä°‰∏≠Âç¥Èù¢‰∏¥ÊåëÊàò„ÄÇ‰º†ÁªüÁöÑËßÜËßâÊåá‰ª§Ë∞É‰ºòÔºàVITÔºâ‰∏ªË¶ÅÂÖ≥Ê≥®Êï∞ÊçÆÈáèÁöÑÊâ©Â§ßÔºåËÄåÂøΩËßÜ‰∫ÜËÆ≠ÁªÉÁ§∫‰æãÁöÑÁªÑÂêàÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCOMPACTÔºàÁªÑÂêàÂéüÂ≠êÂà∞Â§çÊùÇËßÜËßâËÉΩÂäõË∞É‰ºòÔºâÔºåÂÆÉÁîüÊàê‰∏Ä‰∏™ÊòéÁ°ÆÊéßÂà∂ËÆ≠ÁªÉÁ§∫‰æãÁªÑÂêàÂ§çÊùÇÊÄßÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ„ÄÇCOMPACT‰ΩøÂæóMLLMsËÉΩÂ§üÊõ¥È´òÊïàÂú∞Â≠¶‰π†Â§çÊùÇËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∂âÂèäÂ§çÊùÇÂ§öËÉΩÂäõ‰ªªÂä°Êó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21233', 'title': 'Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math', 'url': 'https://huggingface.co/papers/2504.21233', 'abstract': 'Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.', 'score': 19, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '0b800a9195884db4', 'authors': ['Haoran Xu', 'Baolin Peng', 'Hany Awadalla', 'Dongdong Chen', 'Yen-Chun Chen', 'Mei Gao', 'Young Jin Kim', 'Yunsheng Li', 'Liliang Ren', 'Yelong Shen', 'Shuohang Wang', 'Weijian Xu', 'Jianfeng Gao', 'Weizhu Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21233.jpg', 'data': {'categories': ['#training', '#rl', '#transfer_learning', '#small_models', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (SLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —á–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–∏–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö, –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä—è–º–æ–≥–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∫ –º–æ–¥–µ–ª–∏ Phi-4-Mini (3.8 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–∞–∂–µ –≤ —Ä–µ—Å—É—Ä—Å–Ω–æ-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –º–∞–ª—ã—Ö –º–æ–¥–µ–ª—è—Ö.'}, 'en': {'title': 'Unlocking Reasoning Power in Small Models', 'desc': 'This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning.'}, 'zh': {'title': 'Â∞èÊ®°Âûã‰πüËÉΩÂº∫Êé®ÁêÜÔºÅ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊù•ÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÁîüÊàê‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂ∞èÂûãÊ®°ÂûãÁî±‰∫éÂÆπÈáèÈôêÂà∂ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøá‰ªéLLMÁîüÊàêÁöÑÂêàÊàêÊï∞ÊçÆËøõË°åËí∏È¶èÔºåÂèØ‰ª•ÊòæËëóÊîπÂñÑSLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑËÆ≠ÁªÉÊñπÊ°àÔºåÂåÖÊã¨Âõõ‰∏™Ê≠•È™§ÔºåÊúÄÁªàÂú®Phi-4-MiniÊ®°Âûã‰∏äÂÆûÁé∞‰∫ÜË∂ÖË∂äÊõ¥Â§ßÊ®°ÂûãÁöÑÊé®ÁêÜË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20708', 'title': 'Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think', 'url': 'https://huggingface.co/papers/2504.20708', 'abstract': "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.", 'score': 17, 'issue_id': 3532, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': 'b26e58cf1cee464f', 'authors': ['Hasan Abed Al Kader Hammoud', 'Hani Itani', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2504.20708.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#interpretability', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ü–æ–¥–º—ã—Å–ª–∏ –≤ LLM: –ø—É—Ç—å –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–ø–æ–¥–º—ã—Å–ª–µ–π) –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –ø–æ–¥–º—ã—Å–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π –∏–∑ –∫–∞–∂–¥–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–π —Ç–æ—á–∫–∏. –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–º—ã—Å–ª–µ–π, —á–∞—Å—Ç–æ –¥–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, –¥–æ—Å—Ç–∏–≥–∞—é—â–µ–µ 13%.'}, 'en': {'title': 'Unlocking Better Answers Through Subthoughts in LLMs', 'desc': 'This paper investigates the reasoning process of Large Language Models (LLMs) by focusing on intermediate reasoning steps, called subthoughts, rather than just the final answer. It questions whether the final answer is the best conclusion and explores if different reasoning paths can lead to varied results. The authors propose a method that segments reasoning into subthoughts and generates multiple potential answers from these segments, aggregating them to find the most frequent answer for improved accuracy. Their experiments demonstrate that this approach can enhance the accuracy of LLMs by up to 13% on challenging mathematical reasoning tasks.'}, 'zh': {'title': '‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÔºåÊèêÂçáÊ®°ÂûãÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÊó∂ÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§‰∏™ÈóÆÈ¢òÔºöÊúÄÁªàÁ≠îÊ°àÊòØÂê¶ÂèØÈù†Âú∞‰ª£Ë°®Ê®°ÂûãÁöÑÊúÄ‰Ω≥ÁªìËÆ∫Ôºü‰∏çÂêåÁöÑÊé®ÁêÜË∑ØÂæÑÊòØÂê¶‰ºö‰∫ßÁîü‰∏çÂêåÁöÑÁªìÊûúÔºü‰∏∫‰∫ÜËß£Á≠îËøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂàÜÊûê‰∫Ü‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÁß∞‰∏∫Â≠êÊÄùÁª¥ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËøô‰∫õÂèëÁé∞ÁöÑÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáÈÄâÊã©ÊúÄÈ¢ëÁπÅÁöÑÁ≠îÊ°àÔºà‰ºóÊï∞ÔºâÊù•ËÅöÂêà‰∏çÂêåÂ≠êÊÄùÁª¥ÁöÑÁ≠îÊ°àÔºåÂáÜÁ°ÆÊÄßÊòæËëóÊèêÈ´òÔºåÊúÄÈ´òÂèØËææ13%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21318', 'title': 'Phi-4-reasoning Technical Report', 'url': 'https://huggingface.co/papers/2504.21318', 'abstract': 'We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.', 'score': 14, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '7004ae060f674e9a', 'authors': ['Marah Abdin', 'Sahaj Agarwal', 'Ahmed Awadallah', 'Vidhisha Balachandran', 'Harkirat Behl', 'Lingjiao Chen', 'Gustavo de Rosa', 'Suriya Gunasekar', 'Mojan Javaheripi', 'Neel Joshi', 'Piero Kauffmann', 'Yash Lara', 'Caio C√©sar Teodoro Mendes', 'Arindam Mitra', 'Besmira Nushi', 'Dimitris Papailiopoulos', 'Olli Saarikivi', 'Shital Shah', 'Vaishnavi Shrivastava', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi', 'Guoqing Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21318.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#math', '#transfer_learning', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Phi-4-reasoning - –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—É—é –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. Phi-4-reasoning –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, –Ω–∞—É—á–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ.'}, 'en': {'title': 'Unlocking Complex Reasoning with Phi-4-Reasoning', 'desc': 'The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models.'}, 'zh': {'title': 'Êé®ÁêÜÊ®°ÂûãÁöÑÊñ∞Á™ÅÁ†¥ÔºöPhi-4-reasoning', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜPhi-4-reasoningÔºåËøôÊòØ‰∏Ä‰∏™Êã•Êúâ140‰∫øÂèÇÊï∞ÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂØπÁ≤æÂøÉÊåëÈÄâÁöÑ‚ÄúÂèØÊïô‚ÄùÊèêÁ§∫ËøõË°åÁõëÁù£ÂæÆË∞ÉËÆ≠ÁªÉÔºåÁîüÊàêËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÔºåÊúâÊïàÂà©Áî®Êé®ÁêÜÊó∂ÁöÑËÆ°ÁÆóËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫ÜPhi-4-reasoning-plusÔºåÈÄöËøáÂü∫‰∫éÁªìÊûúÁöÑÂº∫ÂåñÂ≠¶‰π†Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫ÔºåËÉΩÂ§üÁîüÊàêÊõ¥ÈïøÁöÑÊé®ÁêÜËΩ®ËøπÔºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩ„ÄÇÁªºÂêàËØÑ‰º∞ÊòæÁ§∫ÔºåËøô‰∏§‰∏™Ê®°ÂûãÂú®Êï∞Â≠¶„ÄÅÁßëÂ≠¶Êé®ÁêÜ„ÄÅÁºñÁ†ÅÁ≠âÂ§ö‰∏™‰ªªÂä°‰∏äÂùá‰ºò‰∫éÊõ¥Â§ßÁöÑÂºÄÊîæÊùÉÈáçÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.20966', 'title': 'Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax', 'url': 'https://huggingface.co/papers/2504.20966', 'abstract': 'We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.', 'score': 14, 'issue_id': 3526, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 –∞–ø—Ä–µ–ª—è', 'en': 'April 29', 'zh': '4Êúà29Êó•'}, 'hash': 'cb610c1427bdf307', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#architecture', '#inference', '#optimization', '#training'], 'emoji': 'üîç', 'ru': {'title': 'Softpick: —É–ª—É—á—à–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç softpick - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö, –∑–∞–º–µ–Ω—è—é—â–∏–π softmax. Softpick —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É 'attention sink' –∏ —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ softmax. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ softpick —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –º–µ–Ω—å—à–∏–º —ç–∫—Å—Ü–µ—Å—Å–æ–º. –ú–æ–¥–µ–ª–∏ —Å softpick –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç softmax –ø—Ä–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –±–∏—Ç–æ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏."}, 'en': {'title': 'Softpick: A Smarter Alternative to Softmax for Transformers', 'desc': 'This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability.'}, 'zh': {'title': 'softpickÔºöÊèêÂçáTransformerÊ≥®ÊÑèÂäõÁöÑÊñ∞ÈÄâÊã©', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫softpickÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂèØ‰ª•Êõø‰ª£transformerÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÁöÑsoftmax„ÄÇsoftpick‰∏çÈúÄË¶ÅÂ∞ÜÊùÉÈáçÂΩí‰∏ÄÂåñ‰∏∫1ÔºåËÉΩÂ§üÊ∂àÈô§Ê≥®ÊÑèÂäõÊ≤âÊ≤°ÂíåÂ§ßËßÑÊ®°ÊøÄÊ¥ª„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®softpickÁöÑÊ®°ÂûãÂú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠‰∏ésoftmaxË°®Áé∞Áõ∏ÂΩìÔºå‰ΩÜÊ≥®ÊÑèÂäõÊ≤âÊ≤°Áéá‰∏∫0%ÔºåÂπ∂‰∏îÁîüÊàêÁöÑÈöêËóèÁä∂ÊÄÅÂÖ∑ÊúâÊõ¥‰ΩéÁöÑÂ≥∞Â∫¶„ÄÇsoftpickÂú®ÈáèÂåñÂíå‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉ‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÂ∞§ÂÖ∂Âú®ËæÉ‰Ωé‰ΩçÊï∞Á≤æÂ∫¶‰∏ãÂÖ∑ÊúâÊòéÊòæ‰ºòÂäøÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Á®ÄÁñèÊÄß‰ºòÂåñÂíåÂèØËß£ÈáäÊÄßÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.19720', 'title': 'Taming the Titans: A Survey of Efficient LLM Inference Serving', 'url': 'https://huggingface.co/papers/2504.19720', 'abstract': 'Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.', 'score': 9, 'issue_id': 3526, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 –∞–ø—Ä–µ–ª—è', 'en': 'April 28', 'zh': '4Êúà28Êó•'}, 'hash': 'e74f8b7af65e09fd', 'authors': ['Ranran Zhen', 'Juntao Li', 'Yixin Ji', 'Zhenlin Yang', 'Tong Liu', 'Qingrong Xia', 'Xinyu Duan', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang'], 'affiliations': ['Huawei Cloud', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2504.19720.jpg', 'data': {'categories': ['#survey', '#inference', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞–º–∏. –¢–∞–∫–∂–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–ª–∞—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –Ω–æ–≤—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Optimizing LLMs: Balancing Performance and Efficiency in Generative AI', 'desc': 'This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services.'}, 'zh': {'title': 'Êé®Âä®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊúçÂä°ÁöÑÁ†îÁ©∂ËøõÂ±ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂Â∫ûÂ§ßÁöÑÂèÇÊï∞ÈáèÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÈ´òËÆ°ÁÆóÈúÄÊ±ÇÂØºËá¥‰∫ÜÂÜÖÂ≠òÂºÄÈîÄÂ§ßÔºåÂΩ±Âìç‰∫ÜÊé®ÁêÜÊúçÂä°ÁöÑ‰ΩéÂª∂ËøüÂíåÈ´òÂêûÂêêÈáè„ÄÇÊú¨ÊñáÂÖ®Èù¢Ë∞ÉÊü•‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÁöÑÊñπÊ≥ïÔºåÂåÖÊã¨ÂÆû‰æãÁ∫ßÂíåÈõÜÁæ§Á∫ßÁöÑÁ≠ñÁï•Ôºå‰ª•ÂèäÊñ∞ÂÖ¥Âú∫ÊôØÁöÑÊñπÂêë„ÄÇÊàë‰ª¨ËÆ®ËÆ∫‰∫ÜÊ®°ÂûãÈÉ®ÁΩ≤„ÄÅËØ∑Ê±ÇË∞ÉÂ∫¶„ÄÅËß£Á†ÅÈïøÂ∫¶È¢ÑÊµãÁ≠âÂÆû‰æãÁ∫ßÊñπÊ≥ïÔºå‰ª•ÂèäGPUÈõÜÁæ§ÈÉ®ÁΩ≤ÂíåÂ§öÂÆû‰æãË¥üËΩΩÂùáË°°Á≠âÈõÜÁæ§Á∫ßÁ≠ñÁï•„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÊΩúÂú®ÊñπÂêëÔºå‰ª•Êé®Âä®LLMÊé®ÁêÜÊúçÂä°ÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21855', 'title': 'ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction', 'url': 'https://huggingface.co/papers/2504.21855', 'abstract': 'In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '5d8989ce0c77aa23', 'authors': ['Qihao Liu', 'Ju He', 'Qihang Yu', 'Liang-Chieh Chen', 'Alan Yuille'], 'affiliations': ['Independent Researcher', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21855.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games', '#video', '#small_models'], 'emoji': 'üé•', 'ru': {'title': 'ReVision: —Ñ–∏–∑–∏–∫–∞ –≤ –ø–æ–º–æ—â—å –ò–ò –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ', 'desc': 'ReVision - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö —ç—Ç–∞–ø–æ–≤: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä—É–±–æ–≥–æ –≤–∏–¥–µ–æ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ 2D –∏ 3D –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏. ReVision –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'ReVision: Enhancing Video Generation with 3D Physical Knowledge', 'desc': 'The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation.'}, 'zh': {'title': 'ÈÄöËøá3DÁâ©ÁêÜÁü•ËØÜÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÁúüÂÆûÊÑü‰∏éÂèØÊéßÊÄß', 'desc': 'ËøëÂπ¥Êù•ÔºåËßÜÈ¢ëÁîüÊàêÊäÄÊúØÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÁîüÊàêÂ§çÊùÇÂä®‰ΩúÂíå‰∫§‰∫íÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜReVisionÔºåËøôÊòØ‰∏Ä‰∏™ÂèØÊèíÊãîÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÂ∞ÜÂèÇÊï∞ÂåñÁöÑ‰∏âÁª¥Áâ©ÁêÜÁü•ËØÜÈõÜÊàêÂà∞È¢ÑËÆ≠ÁªÉÁöÑÊù°‰ª∂ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠Ôºå‰ªéËÄåÊòæËëóÊèêÂçáÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÁöÑËÉΩÂäõ„ÄÇReVisionÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖà‰ΩøÁî®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁîüÊàêÁ≤óÁï•ËßÜÈ¢ëÔºåÁÑ∂ÂêéÊèêÂèñ2DÂíå3DÁâπÂæÅÊûÑÂª∫‰∏âÁª¥Áâ©‰Ωì‰∏≠ÂøÉË°®Á§∫ÔºåÊúÄÂêéÈÄöËøáÂèÇÊï∞ÂåñÁâ©ÁêÜÂÖàÈ™åÊ®°ÂûãÁ≤æÁÇºËøêÂä®Â∫èÂàóÔºåÂèçÈ¶àÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏≠‰ª•ÁîüÊàê‰∏ÄËá¥ÁöÑËøêÂä®ËßÜÈ¢ë„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåReVisionÂú®Â§çÊùÇËßÜÈ¢ëÁîüÊàê‰∏äË°®Áé∞‰ºòÂºÇÔºåÁîöËá≥‰ª•ËæÉÂ∞ëÁöÑÂèÇÊï∞Ë∂ÖË∂ä‰∫ÜÂ§ßÂûãÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.19056', 'title': 'Generative AI for Character Animation: A Comprehensive Survey of\n  Techniques, Applications, and Future Directions', 'url': 'https://huggingface.co/papers/2504.19056', 'abstract': 'Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.', 'score': 7, 'issue_id': 3535, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 –∞–ø—Ä–µ–ª—è', 'en': 'April 27', 'zh': '4Êúà27Êó•'}, 'hash': 'deba947a8e69f6ee', 'authors': ['Mohammad Mahdi Abootorabi', 'Omid Ghahroodi', 'Pardis Sadat Zahraei', 'Hossein Behzadasl', 'Alireza Mirrokni', 'Mobina Salimipanah', 'Arash Rasouli', 'Bahar Behzadipour', 'Sara Azarnoush', 'Benyamin Maleki', 'Erfan Sadraiye', 'Kiarash Kiani Feriz', 'Mahdi Teymouri Nahad', 'Ali Moghadasi', 'Abolfazl Eshagh Abianeh', 'Nizi Nazar', 'Hamid R. Rabiee', 'Mahdieh Soleymani Baghshah', 'Meisam Ahmadi', 'Ehsaneddin Asgari'], 'affiliations': ['Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Iran University of Science and Technology', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2504.19056.jpg', 'data': {'categories': ['#survey', '#diffusion', '#cv', '#games', '#multimodal', '#video'], 'emoji': 'ü§ñ', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ò–ò —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –∞–Ω–∏–º–∞—Ü–∏—é –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π', 'desc': '–≠—Ç–æ –æ–±–∑–æ—Ä–Ω–∞—è —Å—Ç–∞—Ç—å—è –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ü–µ–≤–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏, —Å–∏–Ω—Ç–µ–∑–∞ –¥–≤–∏–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–∏—è –∞–≤–∞—Ç–∞—Ä–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤. –í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±–∑–æ—Ä –≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –≤–∫–ª—é—á–∞—è –≤–µ–¥—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö. –¢–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è —Å–ø—Ä–∞–≤–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ –º–µ—Ç—Ä–∏–∫–∞—Ö –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤ –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Revolutionizing Character Animation with Generative AI', 'desc': 'This paper reviews the recent advancements in generative AI technologies specifically for character animation, which includes facial animation, gesture modeling, and motion synthesis. It highlights how foundation and diffusion models have significantly lowered the costs and time required for creating animated content. The survey provides a comprehensive overview of the state-of-the-art techniques, practical applications, and datasets used in the field, making it a valuable resource for newcomers. Additionally, it discusses ongoing challenges and suggests future research directions to enhance AI-driven character animation.'}, 'zh': {'title': 'ÁîüÊàêÊÄßAIÔºöÈáçÂ°ëËßíËâ≤Âä®ÁîªÁöÑÊú™Êù•', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÂú®ËßíËâ≤Âä®ÁîªÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®Èù¢ÈÉ®Âä®Áîª„ÄÅË°®ÊÉÖÊ∏≤ÊüìÂíåÂä®‰ΩúÂêàÊàêÁ≠âÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÈÄöËøáÊï¥Âêà‰∏çÂêåÁöÑÁîüÊàêÊ®°ÂûãÂíåÊâ©Êï£Ê®°ÂûãÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÊòæËëóÈôç‰Ωé‰∫ÜÂä®ÁîªÂÜÖÂÆπÁöÑÂà∂‰ΩúÊó∂Èó¥ÂíåÊàêÊú¨„ÄÇËÆ∫ÊñáËøòÊèê‰æõ‰∫ÜÂØπÂΩìÂâçÁ†îÁ©∂„ÄÅÂÆûÈôÖÂ∫îÁî®„ÄÅÂ∏∏Áî®Êï∞ÊçÆÈõÜÂíåÊñ∞ÂÖ¥Ë∂ãÂäøÁöÑÂÖ®Èù¢ÂõûÈ°æÔºåÂ∏ÆÂä©Êñ∞Êâã‰∫ÜËß£Âü∫Á°ÄÊ®°ÂûãÂíåËØÑ‰º∞ÊåáÊ†á„ÄÇÊúÄÂêéÔºå‰ΩúËÄÖËÆ®ËÆ∫‰∫ÜËØ•È¢ÜÂüüÈù¢‰∏¥ÁöÑÊåëÊàòÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.18904', 'title': 'RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning', 'url': 'https://huggingface.co/papers/2504.18904', 'abstract': 'Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 –∞–ø—Ä–µ–ª—è', 'en': 'April 26', 'zh': '4Êúà26Êó•'}, 'hash': 'c724dcb5ceb5df7b', 'authors': ['Haoran Geng', 'Feishi Wang', 'Songlin Wei', 'Yuyang Li', 'Bangjun Wang', 'Boshi An', 'Charlie Tianyue Cheng', 'Haozhe Lou', 'Peihao Li', 'Yen-Jen Wang', 'Yutong Liang', 'Dylan Goetting', 'Chaoyi Xu', 'Haozhe Chen', 'Yuxi Qian', 'Yiran Geng', 'Jiageng Mao', 'Weikang Wan', 'Mingtong Zhang', 'Jiangran Lyu', 'Siheng Zhao', 'Jiazhao Zhang', 'Jialiang Zhang', 'Chengyang Zhao', 'Haoran Lu', 'Yufei Ding', 'Ran Gong', 'Yuran Wang', 'Yuxuan Kuang', 'Ruihai Wu', 'Baoxiong Jia', 'Carlo Sferrazza', 'Hao Dong', 'Siyuan Huang', 'Yue Wang', 'Jitendra Malik', 'Pieter Abbeel'], 'affiliations': ['BIGAI', 'CMU', 'PKU', 'Stanford', 'UC Berkeley', 'UCLA', 'UIUC', 'UMich', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2504.18904.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#synthetic', '#optimization', '#transfer_learning', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': 'RoboVerse: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤', 'desc': 'RoboVerse - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –≤–∫–ª—é—á–∞—é—â–∞—è —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—É—é —Å—Ä–µ–¥—É, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–∏–º—É–ª—è—Ç–æ—Ä–æ–≤ –∏ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø–ª–æ—â–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ä–µ–¥–∞–º–∏. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ñ–∏–∑–∏–∫–∏ –∏ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º. RoboVerse –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –æ–±–æ–±—â–µ–Ω–∏—è.'}, 'en': {'title': 'RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks', 'desc': 'This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field.'}, 'zh': {'title': 'RoboVerseÔºöÊé®Âä®Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÂº∫Â§ßÊ°ÜÊû∂', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜRoboVerseÔºåËøôÊòØ‰∏Ä‰∏™ÁªºÂêàÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫È¢ÜÂüüÊï∞ÊçÆÊî∂ÈõÜÂíåËØÑ‰º∞ÁöÑÊåëÊàò„ÄÇRoboVerseÂåÖÊã¨‰∏Ä‰∏™Ê®°ÊãüÂπ≥Âè∞„ÄÅ‰∏Ä‰∏™ÂêàÊàêÊï∞ÊçÆÈõÜÂíåÁªü‰∏ÄÁöÑÂü∫ÂáÜÊµãËØïÔºåÊîØÊåÅÂ§öÁßçÊ®°ÊãüÂô®ÂíåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÇÈÄöËøáÈ´ò‰øùÁúüÁâ©ÁêÜÂíåÈÄºÁúüÁöÑÊ∏≤ÊüìÔºåÂêàÊàêÊï∞ÊçÆÈõÜÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÁöÑÊï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRoboVerseÊòæËëóÊèêÂçá‰∫ÜÊ®°‰ªøÂ≠¶‰π†„ÄÅÂº∫ÂåñÂ≠¶‰π†Âíå‰ªéÊ®°ÊãüÂà∞Áé∞ÂÆûÁöÑËΩ¨ÁßªÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÁöÑÂèØÈù†ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21039', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report', 'url': 'https://huggingface.co/papers/2504.21039', 'abstract': 'As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.', 'score': 5, 'issue_id': 3528, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 –∞–ø—Ä–µ–ª—è', 'en': 'April 28', 'zh': '4Êúà28Êó•'}, 'hash': 'a66fbdd0c4cc7250', 'authors': ['Paul Kassianik', 'Baturay Saglam', 'Alexander Chen', 'Blaine Nelson', 'Anu Vellore', 'Massimo Aufiero', 'Fraser Burch', 'Dhruv Kedia', 'Avi Zohary', 'Sajana Weerawardhena', 'Aman Priyanshu', 'Adam Swanda', 'Amy Chang', 'Hyrum Anderson', 'Kojin Oshiba', 'Omar Santos', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Foundation AI Cisco Systems Inc.', 'Security & Trust Organization Cisco Systems Inc.', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21039.jpg', 'data': {'categories': ['#architecture', '#data', '#open_source', '#training', '#dataset', '#benchmark', '#security'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Foundation-Sec-8B - —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Llama 3.1 –∏ –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. Foundation-Sec-8B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å Llama 3.1-70B –∏ GPT-4o-mini –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø—É–±–ª–∏–∫—É—é—Ç –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –ò–ò-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Å—Ñ–µ—Ä–µ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Empowering Cybersecurity with Foundation-Sec-8B', 'desc': 'This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors.'}, 'zh': {'title': 'Êé®Âä®ÁΩëÁªúÂÆâÂÖ®ÁöÑAIÂ∑•ÂÖ∑ËøõÊ≠•', 'desc': 'ÈöèÁùÄÂü∫‰∫éÂèòÊç¢Âô®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Á§æ‰ºö‰∏≠ÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåÂÆÉ‰ª¨Âú®ËΩØ‰ª∂Â∑•Á®ã„ÄÅÂàõÊÑèÂÜô‰ΩúÂíåÊï∞Â≠óËâ∫ÊúØÁ≠âÈ¢ÜÂüüÂ∏¶Êù•‰∫ÜÈù©ÂëΩÊÄßÁöÑÂèòÂåñ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁº∫‰πè‰∏ì‰∏öÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåË°®Á§∫ÁΩëÁªúÂÆâÂÖ®ÁâπÂÆöÁü•ËØÜÁöÑÂ§çÊùÇÊÄßÔºåÂÆÉ‰ª¨Âú®ÁΩëÁªúÂÆâÂÖ®È¢ÜÂüüÁöÑÂ∫îÁî®‰ªçÁÑ∂ÊúâÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFoundation-Sec-8BÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÁΩëÁªúÂÆâÂÖ®ÁöÑLLMÔºåÂü∫‰∫éLlama 3.1Êû∂ÊûÑÔºåÂπ∂ÈÄöËøáÂú®Á≤æÂøÉÁ≠ñÂàíÁöÑÁΩëÁªúÂÆâÂÖ®ËØ≠ÊñôÂ∫ì‰∏äËøõË°åÊåÅÁª≠È¢ÑËÆ≠ÁªÉÊù•Â¢ûÂº∫„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™ÁΩëÁªúÂÆâÂÖ®Âü∫ÂáÜÊµãËØï‰∏≠ËØÑ‰º∞‰∫ÜFoundation-Sec-8BÔºåÁªìÊûúÊòæÁ§∫ÂÆÉÂú®Êüê‰∫õÁΩëÁªúÂÆâÂÖ®ÁâπÂÆö‰ªªÂä°‰∏ä‰∏éLlama 3.1-70BÂíåGPT-4o-miniÁõ∏ÂåπÈÖç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21336', 'title': 'UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation', 'url': 'https://huggingface.co/papers/2504.21336', 'abstract': 'Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.', 'score': 2, 'issue_id': 3532, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': 'b4a0872fb3eb8547', 'authors': ['Linshan Wu', 'Yuxiang Nie', 'Sunan He', 'Jiaxin Zhuang', 'Hao Chen'], 'affiliations': ['Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China', 'Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China', 'State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21336.jpg', 'data': {'categories': ['#optimization', '#cv', '#science', '#multimodal', '#dataset', '#healthcare', '#interpretability', '#agi'], 'emoji': 'üß¨', 'ru': {'title': 'UniBiomed: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': 'UniBiomed - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 27 –º–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ 10 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. UniBiomed –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Ä–µ–≥–∏–æ–Ω–∞–º, –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤. –ú–æ–¥–µ–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏.'}, 'en': {'title': 'UniBiomed: Revolutionizing Biomedical Image Interpretation', 'desc': "This paper presents UniBiomed, a groundbreaking universal foundation model designed for interpreting biomedical images by integrating Multi-modal Large Language Models (MLLM) and segmentation techniques. Unlike traditional AI methods that treat text generation and image segmentation separately, UniBiomed combines these processes to provide a cohesive understanding of biomedical data. It utilizes a large-scale dataset of over 27 million image-text pairs across various imaging modalities, enabling it to perform multiple tasks such as segmentation, disease recognition, and report generation. The model's ability to automate grounded interpretation marks a significant advancement in clinical workflows, enhancing diagnostic efficiency and accuracy."}, 'zh': {'title': 'UniBiomedÔºöÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Â§öÊ®°ÊÄÅÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèÁöÑËß£Èáä‰∏∫ÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÂºÄËæü‰∫ÜÊñ∞ÁöÑÊú∫‰ºö„ÄÇ‰º†ÁªüÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂàÜÁ¶ªÁöÑËÆ≠ÁªÉÔºåÂØºËá¥Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Áº∫‰πèÁÅµÊ¥ªÊÄßÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®Êï¥‰ΩìÁîüÁâ©ÂåªÂ≠¶‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜUniBiomedÔºåËøôÊòØÈ¶ñ‰∏™Áî®‰∫éÁîüÁâ©ÂåªÂ≠¶ÂõæÂÉèËß£ÈáäÁöÑÈÄöÁî®Âü∫Á°ÄÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÂàÜÂâ≤Ê®°ÂûãÔºåËÉΩÂ§üÁªü‰∏ÄÁîüÊàê‰∏¥Â∫äÊñáÊú¨ÂíåÁõ∏Â∫îÁîüÁâ©ÂåªÂ≠¶ÂØπË±°ÁöÑÂàÜÂâ≤„ÄÇUniBiomedÂú®Â§ö‰∏™ÁîüÁâ©ÂåªÂ≠¶‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËØäÊñ≠ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.19043', 'title': 'Selecting Optimal Candidate Profiles in Adversarial Environments Using\n  Conjoint Analysis and Machine Learning', 'url': 'https://huggingface.co/papers/2504.19043', 'abstract': 'Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.', 'score': 2, 'issue_id': 3538, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 –∞–ø—Ä–µ–ª—è', 'en': 'April 26', 'zh': '4Êúà26Êó•'}, 'hash': 'c7a7b2771c1e5c1f', 'authors': ['Connor T. Jerzak', 'Priyanshi Chandra', 'Rishi Hazra'], 'affiliations': ['Department of Government, University of Texas at Austin', 'Department of Statistics, Harvard College', 'Faculty of Informatics, Universit√† della Svizzera Italiana'], 'pdf_title_img': 'assets/pdf/title_img/2504.19043.jpg', 'data': {'categories': [], 'emoji': 'üó≥Ô∏è', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é, —Ç–∞–∫ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–µ–∂–¥—É –¥–≤—É–º—è –ø–∞—Ä—Ç–∏—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–µ—Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–æ–º.'}, 'en': {'title': 'Optimizing Political Candidate Selection through Adversarial Conjoint Analysis', 'desc': 'This paper explores the use of conjoint analysis in political candidate selection, focusing on how to identify optimal candidate profiles. It highlights the challenge of having too many possible candidate features compared to the limited number of observations in typical experiments. To solve this, the authors propose a stochastic intervention that generates a probability distribution of candidate attributes to maximize favorable outcomes. The study shows that considering adversarial dynamics between political parties leads to more accurate predictions of candidate success compared to traditional non-adversarial methods.'}, 'zh': {'title': 'ÂØπÊäóÊÄßÂä®ÊÄÅÊèêÂçáËÅîÂêàÂàÜÊûêÁöÑÊ¥ûÂØüÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËÅîÂêàÂàÜÊûêÂú®ÊîøÊ≤ªÂÄôÈÄâ‰∫∫ÈÄâÊã©‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂ¶Ç‰ΩïËØÜÂà´ÊúÄ‰Ω≥ÂÄôÈÄâ‰∫∫ÁâπÂæÅÁªÑÂêà„ÄÇÁî±‰∫éÁâπÂæÅÁªÑÂêàÁöÑÊï∞ÈáèËøúË∂ÖËßÇÂØüÊ†∑Êú¨ÔºåÊó†Ê≥ïÁ≤æÁ°ÆÁ°ÆÂÆöÊúÄ‰Ω≥ÈÖçÁΩÆ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúÄ‰ºòÈöèÊú∫Âπ≤È¢ÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÊ¶ÇÁéáÂàÜÂ∏ÉÂÆûÁé∞ÊúÄÊúâÂà©ÁöÑÂπ≥ÂùáÁªìÊûú„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ÂØπÊäóÊÄßÁéØÂ¢É‰∏≠ÔºåÊâÄÂª∫ËÆÆÁöÑÁ≠ñÁï•Êõ¥ÂèØËÉΩ‰∏éÂÆûÈôÖÂÄôÈÄâ‰∫∫ÂåπÈÖçÔºåÊè≠Á§∫‰∫ÜÂØπÊäóÂä®ÊÄÅÂú®Á§æ‰ºöÁßëÂ≠¶ÂÆûÈ™åÊï∞ÊçÆÂàÜÊûê‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11820', 'title': 'Chain-of-Model Learning for Language Model', 'url': 'https://huggingface.co/papers/2505.11820', 'abstract': 'In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.', 'score': 67, 'issue_id': 3848, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '2e8115f0fe78856b', 'authors': ['Kaitao Song', 'Xiaohua Wang', 'Xu Tan', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yongliang Shen', 'Cen LU', 'Zihao Li', 'Zifan Song', 'Caihua Shan', 'Yansen Wang', 'Kan Ren', 'Xiaoqing Zheng', 'Tao Qin', 'Yuqing Yang', 'Dongsheng Li', 'Lili Qiu'], 'affiliations': ['Fudan University', 'Microsoft Research', 'ShanghaiTech University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11820.jpg', 'data': {'categories': ['#training', '#open_source', '#inference', '#agi', '#architecture', '#optimization'], 'emoji': 'üîó', 'ru': {'title': '–¶–µ–ø–Ω–∞—è —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –≥–∏–±–∫–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Chain-of-Model (CoM), –∫–æ—Ç–æ—Ä–∞—è –≤–Ω–µ–¥—Ä—è–µ—Ç –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –≤ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ —Ü–µ–ø–æ—á–∫–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é Chain-of-Representation (CoR), —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—â—É—é —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ –∫–∞–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–¥-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∫—Ä—ã—Ç—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Chain-of-Language-Model (CoLM), –∫–æ—Ç–æ—Ä–∞—è –≤–Ω–µ–¥—Ä—è–µ—Ç –∏–¥–µ—é CoM –≤ –∫–∞–∂–¥—ã–π —Å–ª–æ–π Transformer. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π CoLM –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º Transformer, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª—å—à—É—é –≥–∏–±–∫–æ—Å—Ç—å –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–∏.'}, 'en': {'title': 'Scaling Language Models with Chain-of-Model Efficiency', 'desc': 'This paper introduces a new learning approach called Chain-of-Model (CoM), which enhances model training efficiency by incorporating causal relationships into the hidden states of each layer. It presents the Chain-of-Representation (CoR) concept, where hidden states are formed from multiple sub-representations, allowing each layer to only access its preceding chains. The CoM framework enables models to scale up by adding more chains, providing flexibility in deploying various sub-models of different sizes. The Chain-of-Language-Model (CoLM) and its variant CoLM-Air further optimize Transformer architectures by sharing key-value pairs across chains, resulting in improved performance and adaptability for language models.'}, 'zh': {'title': 'ÈìæÂºèÊ®°ÂûãÔºöÁÅµÊ¥ªÈ´òÊïàÁöÑËØ≠Ë®ÄÊ®°ÂûãÊñ∞ËåÉÂºè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ≠¶‰π†ËåÉÂºèÔºåÁß∞‰∏∫ÈìæÂºèÊ®°ÂûãÔºàCoMÔºâÔºåÂÆÉÂ∞ÜÂõ†ÊûúÂÖ≥Á≥ªËûçÂÖ•ÊØè‰∏ÄÂ±ÇÁöÑÈöêËóèÁä∂ÊÄÅÔºå‰ª•ÈìæÂºèÁªìÊûÑÊèêÈ´òÊ®°ÂûãËÆ≠ÁªÉÁöÑÊïàÁéáÂíåÊé®ÁêÜÁöÑÁÅµÊ¥ªÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈìæÂºèË°®Á§∫ÔºàCoRÔºâÁöÑÊ¶ÇÂøµÔºåÂ∞ÜÊØè‰∏ÄÂ±ÇÁöÑÈöêËóèÁä∂ÊÄÅË°®Á§∫‰∏∫Â§ö‰∏™Â≠êË°®Á§∫ÁöÑÁªÑÂêàÔºàÂç≥ÈìæÔºâ„ÄÇÂú®ÊØè‰∏ÄÂ±Ç‰∏≠ÔºåËæìÂá∫Ë°®Á§∫ÁöÑÊØè‰∏™ÈìæÂè™ËÉΩÊü•ÁúãËæìÂÖ•Ë°®Á§∫‰∏≠ÊâÄÊúâÂâçÈù¢ÁöÑÈìæÔºå‰ªéËÄå‰ΩøÂæóÂü∫‰∫éCoMÊ°ÜÊû∂ÊûÑÂª∫ÁöÑÊ®°ÂûãËÉΩÂ§üÈÄöËøáÂ¢ûÂä†ÈìæÁöÑÊï∞ÈáèÈÄêÊ≠•Êâ©Â§ßÊ®°ÂûãËßÑÊ®°ÔºåÂπ∂Êèê‰æõ‰∏çÂêåÂ§ßÂ∞èÁöÑÂ≠êÊ®°Âûã‰ª•ÂÆûÁé∞ÁÅµÊ¥ªÊé®ÁêÜ„ÄÇÂü∫‰∫éËøô‰∏ÄÂéüÁêÜÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÈìæÂºèËØ≠Ë®ÄÊ®°ÂûãÔºàCoLMÔºâÔºåÂπ∂Ëøõ‰∏ÄÊ≠•ÂºïÂÖ•‰∫ÜCoLM-AirÔºåÈÄöËøáÂºïÂÖ•ÈîÆÂÄºÂÖ±‰∫´Êú∫Âà∂ÔºåËÆ°ÁÆóÁ¨¨‰∏Ä‰∏™Èìæ‰∏≠ÁöÑÊâÄÊúâÈîÆÂíåÂÄºÔºåÁÑ∂ÂêéÂú®ÊâÄÊúâÈìæ‰πãÈó¥ÂÖ±‰∫´Ôºå‰ªéËÄåÂ±ïÁ§∫‰∫ÜÈ¢ùÂ§ñÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13417', 'title': 'AdaptThink: Reasoning Models Can Learn When to Think', 'url': 'https://huggingface.co/papers/2505.13417', 'abstract': 'Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.', 'score': 56, 'issue_id': 3845, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'edd33223d8d833a7', 'authors': ['Jiajie Zhang', 'Nianyi Lin', 'Lei Hou', 'Ling Feng', 'Juanzi Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13417.jpg', 'data': {'categories': ['#math', '#reasoning', '#inference', '#training', '#optimization', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º AdaptThink, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏. –ê–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ AdaptThink –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –ø–æ–≤—ã—à–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –ù–∞ —Ç—Ä–µ—Ö –Ω–∞–±–æ—Ä–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º —Å–æ–∫—Ä–∞—Ç–∏–ª —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ DeepSeek-R1-Distill-Qwen-1.5B –Ω–∞ 53% –∏ –ø–æ–≤—ã—Å–∏–ª –µ–µ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ 2.4%.'}, 'en': {'title': 'Optimize Reasoning with Adaptive Thinking Modes!', 'desc': 'This paper introduces AdaptThink, a reinforcement learning algorithm designed to optimize reasoning models by allowing them to choose between two thinking modes: NoThinking and traditional thinking. NoThinking enables models to skip lengthy reasoning processes for simpler tasks, improving efficiency without sacrificing performance. AdaptThink employs a constrained optimization objective to encourage the use of NoThinking while maintaining overall accuracy, and it uses importance sampling to balance training between both modes. The results show that AdaptThink significantly reduces inference costs and enhances performance on math tasks, demonstrating the effectiveness of adaptive thinking-mode selection.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊÄùËÄÉÊ®°ÂºèÈÄâÊã©ÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá‰∏éË¥®Èáè', 'desc': 'ÊúÄËøëÔºåÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®ÂêÑÁßç‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ÂÜóÈïøÁöÑÊÄùËÄÉËøáÁ®ãÊòæËëóÂ¢ûÂä†‰∫ÜÊé®ÁêÜÂºÄÈîÄÔºåÂØºËá¥ÊïàÁéáÊàê‰∏∫Áì∂È¢à„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫NoThinkingÁöÑÊñπÊ≥ïÔºåÈºìÂä±Êé®ÁêÜÊ®°ÂûãË∑≥ËøáÊÄùËÄÉÔºåÁõ¥Êé•ÁîüÊàêÊúÄÁªàËß£ÂÜ≥ÊñπÊ°àÔºåÈÄÇÁî®‰∫éÁõ∏ÂØπÁÆÄÂçïÁöÑ‰ªªÂä°„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜAdaptThinkÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®Ê†πÊçÆÈóÆÈ¢òÈöæÂ∫¶Ëá™ÈÄÇÂ∫îÈÄâÊã©ÊúÄ‰Ω≥ÊÄùËÄÉÊ®°Âºè„ÄÇÂÆûÈ™åË°®ÊòéÔºåAdaptThinkÊòæËëóÈôç‰Ωé‰∫ÜÊé®ÁêÜÊàêÊú¨ÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11896', 'title': 'AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.11896', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.', 'score': 43, 'issue_id': 3846, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': 'bdc79864df7cbd51', 'authors': ['Chenwei Lou', 'Zewei Sun', 'Xinnian Liang', 'Meng Qu', 'Wei Shen', 'Wenqi Wang', 'Yuntao Li', 'Qingping Yang', 'Shuangzhi Wu'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2505.11896.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#rl', '#training'], 'emoji': 'üß†', 'ru': {'title': 'AdaCoT: –£–º–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'AdaCoT - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∫—Ä—É–ø–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–µ—à–∞—Ç—å, –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought, CoT). –ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ Proximal Policy Optimization (PPO), AdaCoT –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º CoT. –ö–ª—é—á–µ–≤—ã–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –≤–∫–ª–∞–¥–æ–º —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ç–æ–¥ Selective Loss Masking (SLM), –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—â–∏–π –∫–æ–ª–ª–∞–ø—Å –≥—Ä–∞–Ω–∏—Ü—ã –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ AdaCoT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CoT –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Language Models', 'desc': 'This paper presents AdaCoT, a new framework that improves the efficiency of Large Language Models (LLMs) by adaptively deciding when to use Chain-of-Thought (CoT) prompting. Traditional CoT prompting can be computationally expensive, especially for simpler queries, but AdaCoT optimizes this by framing the decision to use CoT as a Pareto optimization problem. The authors employ reinforcement learning, specifically Proximal Policy Optimization (PPO), to dynamically adjust when CoT is triggered based on the complexity of the input. Their approach includes a technique called Selective Loss Masking (SLM) to ensure stable training, resulting in significant reductions in CoT usage while maintaining high performance on complex tasks.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÈìæÂºèÊé®ÁêÜÔºåÊèêÂçáÊïàÁéá‰∏éÊÄßËÉΩ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°Êó∂Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÈù¢‰∏¥ÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜAdaCoTÔºàËá™ÈÄÇÂ∫îÈìæÂºèÊé®ÁêÜÔºâÔºåÂÆÉÂÖÅËÆ∏Ê®°ÂûãÊ†πÊçÆËæìÂÖ•ÁöÑÂ§çÊùÇÊÄßËá™ÈÄÇÂ∫îÂú∞ÂÜ≥ÂÆöÊòØÂê¶‰ΩøÁî®ÈìæÂºèÊé®ÁêÜ„ÄÇÊàë‰ª¨Â∞ÜËá™ÈÄÇÂ∫îÊé®ÁêÜËßÜ‰∏∫‰∏Ä‰∏™Â∏ïÁ¥ØÊâò‰ºòÂåñÈóÆÈ¢òÔºåÊó®Âú®Âπ≥Ë°°Ê®°ÂûãÊÄßËÉΩ‰∏éÈìæÂºèÊé®ÁêÜÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAdaCoTÂú®‰∏çÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÁöÑÊü•ËØ¢‰∏≠ÊòæËëóÂáèÂ∞ë‰∫ÜÈìæÂºèÊé®ÁêÜÁöÑ‰ΩøÁî®ÔºåÊèêÂçá‰∫ÜÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11254', 'title': 'Delta Attention: Fast and Accurate Sparse Attention Inference by Delta\n  Correction', 'url': 'https://huggingface.co/papers/2505.11254', 'abstract': 'The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.', 'score': 35, 'issue_id': 3847, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '2aba31b686859e82', 'authors': ['Jeffrey Willette', 'Heejun Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.11254.jpg', 'data': {'categories': ['#optimization', '#long_context', '#architecture', '#inference', '#benchmark'], 'emoji': 'üîç', 'ru': {'title': '–ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–¥–≤–∏–≥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —ç—Ç–æ—Ç —Å–¥–≤–∏–≥, –ø—Ä–∏–±–ª–∏–∂–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–º—É. –ú–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –ø–æ–≤–µ—Ä—Ö –ª—é–±–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏.'}, 'en': {'title': 'Boosting Sparse Attention: Aligning Outputs for Enhanced Performance', 'desc': 'This paper addresses the inefficiencies of the attention mechanism in transformers, which typically has a quadratic complexity that increases inference costs for long sequences. It highlights that while sparse attention methods can reduce computation, they often lead to performance degradation due to a distributional shift in attention outputs. The authors propose a novel procedure to correct this shift, aligning sparse attention outputs more closely with those of traditional quadratic attention. Their method significantly improves performance, achieving an average increase of 36 percentage points while maintaining high sparsity and speed, making it much faster than existing methods.'}, 'zh': {'title': 'Á®ÄÁñèÊ≥®ÊÑèÂäõÁöÑÂàÜÂ∏É‰øÆÊ≠£ÔºåÊèêÂçáÊÄßËÉΩ‰∏éÊïàÁéá', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂèòÊç¢Âô®ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Âú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÈóÆÈ¢òÔºåÂØºËá¥Êé®ÁêÜÊàêÊú¨È´òÂíåÂª∂ËøüÂ§ß„ÄÇÂ∞ΩÁÆ°Ê≥®ÊÑèÂäõÁü©ÈòµÈÄöÂ∏∏ÊòØÁ®ÄÁñèÁöÑÔºå‰ΩÜÁ®ÄÁñèÊ≥®ÊÑèÂäõÊé®ÁêÜÊñπÊ≥ïÂú®ÂáèÂ∞ëËÆ°ÁÆóË¥üÊãÖÁöÑÂêåÊó∂ÔºåÂèØËÉΩ‰ºöÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÊÄßËÉΩ‰∏ãÈôçÁöÑ‰∏Ä‰∏™ÂéüÂõ†ÊòØÁ®ÄÁñèËÆ°ÁÆóÂºïËµ∑‰∫ÜÊ≥®ÊÑèÂäõËæìÂá∫ÁöÑÂàÜÂ∏ÉÂÅèÁßªÔºåËøô‰ΩøÂæóËß£Á†ÅÊó∂ÁöÑÊü•ËØ¢‰∏éÈ¢ÑÂ°´Èò∂ÊÆµÁöÑÈîÆÂØπÈΩê‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑ‰øÆÊ≠£ÊñπÊ≥ïÔºå‰ΩøÁ®ÄÁñèÊ≥®ÊÑèÂäõËæìÂá∫ÁöÑÂàÜÂ∏ÉÊõ¥Êé•Ëøë‰∫é‰∫åÊ¨°Ê≥®ÊÑèÂäõÔºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13227', 'title': 'Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis', 'url': 'https://huggingface.co/papers/2505.13227', 'abstract': 'Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.', 'score': 34, 'issue_id': 3849, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'fe01e0daca57b031', 'authors': ['Tianbao Xie', 'Jiaqi Deng', 'Xiaochuan Li', 'Junlin Yang', 'Haoyuan Wu', 'Jixuan Chen', 'Wenjing Hu', 'Xinyuan Wang', 'Yuhui Xu', 'Zekun Wang', 'Yiheng Xu', 'Junli Wang', 'Doyen Sahoo', 'Tao Yu', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13227.jpg', 'data': {'categories': ['#data', '#dataset', '#graphs', '#agents', '#benchmark', '#open_source'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò —Ä–∞–±–æ—Ç–µ —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OSWorld-G –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç Jedi, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 4 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ Jedi, –ø—Ä–µ–≤–∑–æ—à–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≤–∫–ª—é—á–∞—è OSWorld-G. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏.'}, 'en': {'title': 'Enhancing GUI Grounding with Comprehensive Datasets and Models', 'desc': 'This paper addresses the challenge of GUI grounding, which is the process of translating natural language commands into actions on graphical user interfaces. Current benchmarks are limited as they only focus on simple tasks, neglecting the complexities of real-world interactions that require understanding of software context and layout. The authors introduce OSWorld-G, a new benchmark with 564 detailed samples and the Jedi dataset, which contains 4 million examples to improve grounding tasks. Their findings show that using the Jedi dataset significantly enhances the performance of multi-scale models in executing complex computer tasks, demonstrating the importance of specialized data for effective grounding.'}, 'zh': {'title': 'ÊèêÂçáËÆ°ÁÆóÊú∫‰ΩøÁî®‰ª£ÁêÜÁöÑÂü∫Á°ÄËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâÂü∫Á°ÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Êò†Â∞ÑÈóÆÈ¢òÔºåÊåáÂá∫Áé∞ÊúâÂü∫ÂáÜÊµãËØïËøá‰∫éÁÆÄÂåñÔºåÊó†Ê≥ïÂèçÊò†ÁúüÂÆû‰∏ñÁïåÁöÑÂ§çÊùÇ‰∫§‰∫í„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜOSWorld-GÂü∫ÂáÜÔºåÂåÖÂê´564‰∏™Á≤æÁªÜÊ≥®ÈáäÁöÑÊ†∑Êú¨ÔºåÊ∂µÁõñÊñáÊú¨ÂåπÈÖç„ÄÅÂÖÉÁ¥†ËØÜÂà´„ÄÅÂ∏ÉÂ±ÄÁêÜËß£ÂíåÁ≤æÁ°ÆÊìç‰ΩúÁ≠âÂ§öÁßç‰ªªÂä°Á±ªÂûã„ÄÇÊ≠§Â§ñÔºå‰ΩúËÄÖÂêàÊàêÂπ∂ÂèëÂ∏É‰∫ÜÊúÄÂ§ßÁöÑËÆ°ÁÆóÊú∫‰ΩøÁî®Âü∫Á°ÄÊï∞ÊçÆÈõÜJediÔºåÂåÖÂê´400‰∏á‰∏™Á§∫‰æãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇËÆ°ÁÆóÊú∫‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÈÄöËøáËØ¶ÁªÜÁöÑÊ∂àËûçÁ†îÁ©∂ÔºåËÆ∫ÊñáËøòËØÜÂà´‰∫ÜÂΩ±ÂìçÂü∫Á°ÄÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåÂπ∂È™åËØÅ‰∫Ü‰∏çÂêåÁïåÈù¢ÂÖÉÁ¥†ÁöÑ‰∏ìÈó®Êï∞ÊçÆÁªìÂêàËÉΩÂ§üÂÆûÁé∞ÂØπÊñ∞ÁïåÈù¢ÁöÑÁªÑÂêàÊ≥õÂåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13379', 'title': 'Thinkless: LLM Learns When to Think', 'url': 'https://huggingface.co/papers/2505.13379', 'abstract': "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless", 'score': 25, 'issue_id': 3846, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'd41117eabc11e5c3', 'authors': ['Gongfan Fang', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.13379.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#benchmark', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–∏–º –∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Thinkless - –æ–±—É—á–∞–µ–º—É—é —Å–∏—Å—Ç–µ–º—É, –ø–æ–∑–≤–æ–ª—è—é—â—É—é —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–∏–º –∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –¥–≤–∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Ç–æ–∫–µ–Ω–∞: <short> –¥–ª—è –∫—Ä–∞—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ <think> –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ –ª–µ–∂–∏—Ç –∞–ª–≥–æ—Ä–∏—Ç–º DeGRPO, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Ü–µ–ª—å –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Thinkless —Å–ø–æ—Å–æ–±–µ–Ω —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ 50-90%, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Thinkless: Smart Reasoning for Efficient Language Models', 'desc': "This paper introduces Thinkless, a framework designed to enhance the efficiency of Reasoning Language Models (RLMs) by enabling them to choose between short-form and long-form reasoning based on task complexity. The framework utilizes reinforcement learning and two control tokens, <short> for brief answers and <think> for detailed reasoning, to guide the model's response strategy. A novel algorithm called Decoupled Group Relative Policy Optimization (DeGRPO) is employed to separate the learning objectives, allowing for better control over reasoning mode selection and response accuracy. The results show that Thinkless can significantly reduce the reliance on long-chain reasoning, improving computational efficiency while maintaining performance on various benchmarks."}, 'zh': {'title': 'ËÆ©Ê®°ÂûãÂ≠¶‰ºö‰ΩïÊó∂ÊÄùËÄÉ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ThinklessÁöÑÂèØÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊïàÁéá„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆ‰ªªÂä°Â§çÊùÇÊÄßÂíåËá™Ë∫´ËÉΩÂäõËá™ÈÄÇÂ∫îÈÄâÊã©Áü≠ÊúüÊàñÈïøÊúüÊé®ÁêÜ„ÄÇThinkless‰ΩøÁî®‰∏§‰∏™ÊéßÂà∂Ê†áËÆ∞<short>Âíå<think>Êù•ÂàÜÂà´Ë°®Á§∫ÁÆÄÊ¥ÅÂõûÁ≠îÂíåËØ¶ÁªÜÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåThinklessËÉΩÂ§üÂ∞ÜÈïøÊúüÊé®ÁêÜÁöÑ‰ΩøÁî®ÂáèÂ∞ë50%Ëá≥90%ÔºåÊòæËëóÊèêÂçáÊé®ÁêÜËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13389', 'title': 'Faster Video Diffusion with Trainable Sparse Attention', 'url': 'https://huggingface.co/papers/2505.13389', 'abstract': 'Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53times with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6times and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.', 'score': 24, 'issue_id': 3850, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '33a48c202961951b', 'authors': ['Peiyuan Zhang', 'Haofeng Huang', 'Yongqi Chen', 'Will Lin', 'Zhengzhong Liu', 'Ion Stoica', 'Eric P. Xing', 'Hao Zhang'], 'affiliations': ['MBZUAI', 'UC Berkeley', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.13389.jpg', 'data': {'categories': ['#optimization', '#architecture', '#diffusion', '#training', '#open_source', '#video'], 'emoji': 'üé•', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (VSA) –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. VSA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –≥—Ä—É–±—ã–π —ç—Ç–∞–ø –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ç–æ–Ω–∫–∏–π —ç—Ç–∞–ø –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π. –ú–µ—Ç–æ–¥ –æ–±—É—á–∞–µ—Ç—Å—è –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 85% —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VSA —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –≤ 2,53 —Ä–∞–∑–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞, –∞ —Ç–∞–∫–∂–µ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ 6 —Ä–∞–∑ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Efficient Attention for Scalable Video Diffusion', 'desc': 'This paper introduces VSA, a novel trainable sparse attention mechanism designed to improve the efficiency of video diffusion transformers (DiTs). By focusing on a small subset of critical tokens, VSA reduces the computational burden associated with traditional quadratic attention. The method consists of a coarse stage that pools tokens and identifies important ones, followed by a fine stage that computes attention only within these selected tokens. The results demonstrate that VSA significantly decreases training FLOPS while maintaining performance, making it a viable alternative for scaling video diffusion models.'}, 'zh': {'title': 'ÂèØËÆ≠ÁªÉÁ®ÄÁñèÊ≥®ÊÑèÂäõÔºöËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊñ∞ÈÄâÊã©', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫VSAÁöÑÂèØËÆ≠ÁªÉÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊó®Âú®Ëß£ÂÜ≥ËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâÂú®Â§ÑÁêÜ3DÊ≥®ÊÑèÂäõÊó∂ÁöÑËÆ°ÁÆóÈôêÂà∂„ÄÇVSAÈÄöËøáÂ∞ÜÊ≥®ÊÑèÂäõËÆ°ÁÆóÂàÜ‰∏∫Á≤óÁï•Èò∂ÊÆµÂíåÁ≤æÁªÜÈò∂ÊÆµÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈ´òÊïàÁöÑËÆ≠ÁªÉÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVSAÂú®‰∏çÈôç‰ΩéÊâ©Êï£ÊçüÂ§±ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞ÜËÆ≠ÁªÉÁöÑFLOPSÂáèÂ∞ë‰∫Ü2.53ÂÄçÔºåÂπ∂‰∏îÂú®ÂºÄÊ∫êÊ®°ÂûãWan-2.1‰∏äÂÆûÁé∞‰∫Ü6ÂÄçÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆóÂä†ÈÄü„ÄÇËØ•Á†îÁ©∂Ë°®ÊòéÔºåÂèØËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊòØÂÖ®Ê≥®ÊÑèÂäõÁöÑÊúâÊïàÊõø‰ª£ÊñπÊ°àÔºåÂπ∂‰∏∫ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑËøõ‰∏ÄÊ≠•Êâ©Â±ïÊèê‰æõ‰∫ÜÂÖ≥ÈîÆÊîØÊåÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13308', 'title': 'Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space', 'url': 'https://huggingface.co/papers/2505.13308', 'abstract': "Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.", 'score': 23, 'issue_id': 3851, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'bd9e494dc68db18c', 'authors': ['Hengli Li', 'Chenxi Li', 'Tong Wu', 'Xuekai Zhu', 'Yuxuan Wang', 'Zhaoxin Yu', 'Eric Hanchen Jiang', 'Song-Chun Zhu', 'Zixia Jia', 'Ying Nian Wu', 'Zilong Zheng'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'Institute of Automation, Chinese Academy of Sciences', 'NLCo Lab, Beijing Institute for General Artificial Intelligence', 'Shanghai Jiao Tong University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2505.13308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#reasoning', '#training', '#agi'], 'emoji': 'üß†', 'ru': {'title': 'LatentSeek: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å —É –ò–ò —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∞—Ü–∏—é –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LatentSeek - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏. LatentSeek –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É—è—Å—å —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–∏–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ª–∏–Ω–∏–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –æ–±—ã—á–Ω–æ —Å—Ö–æ–¥—è—Å—å –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π. LatentSeek –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–µ, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM.'}, 'en': {'title': 'Enhancing LLM Reasoning with LatentSeek', 'desc': "This paper addresses the challenges of reasoning in Large Language Models (LLMs) as they strive for Artificial General Intelligence (AGI). It introduces LatentSeek, a framework that improves reasoning by adapting the model's latent space during test time, rather than updating parameters. By using policy gradient methods, LatentSeek iteratively refines latent representations based on self-generated rewards, leading to enhanced performance on reasoning tasks. The results show that LatentSeek outperforms existing methods and is efficient, converging quickly while still benefiting from additional iterations."}, 'zh': {'title': 'LatentSeekÔºöÊèêÂçáLLMÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ËøΩÊ±ÇÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩÔºàAGIÔºâÊó∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂LatentSeekÔºåÈÄöËøáÂú®Ê®°ÂûãÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊµãËØïÊó∂ÂÆû‰æãÁ∫ßÈÄÇÂ∫îÔºàTTIAÔºâÔºåÊù•Â¢ûÂº∫LLMsÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ï‰∏çÂêåÔºåLatentSeekÂà©Áî®Á≠ñÁï•Ê¢ØÂ∫¶Ëø≠‰ª£Êõ¥Êñ∞ÊΩúÂú®Ë°®Á§∫ÔºåÂπ∂ÈÄöËøáËá™ÁîüÊàêÁöÑÂ•ñÂä±‰ø°Âè∑ËøõË°åÊåáÂØº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLatentSeekÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊµãËØïÊó∂Êâ©Â±ïÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12082', 'title': 'Model Merging in Pre-training of Large Language Models', 'url': 'https://huggingface.co/papers/2505.12082', 'abstract': 'Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.', 'score': 23, 'issue_id': 3854, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '7f25885def33b040', 'authors': ['Yunshui Li', 'Yiyuan Ma', 'Shen Yan', 'Chaoyi Zhang', 'Jing Liu', 'Jianqiao Lu', 'Ziwen Xu', 'Mengzhao Chen', 'Minrui Wang', 'Shiyi Zhan', 'Jin Ma', 'Xunhao Lai', 'Yao Luo', 'Xingyan Bin', 'Hongbin Ren', 'Mingji Han', 'Wenhao Hao', 'Bairen Yi', 'LingJun Liu', 'Bole Ma', 'Xiaoying Jia', 'Zhou Xun', 'Liang Xiang', 'Yonghui Wu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.12082.jpg', 'data': {'categories': ['#optimization', '#open_source', '#architecture', '#training'], 'emoji': 'üîÄ', 'ru': {'title': '–°–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ Mixture-of-Experts, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–ª–∏—è–Ω–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫, –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∏–¥–µ–∏ –æ –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞.'}, 'en': {'title': 'Enhancing Language Models through Effective Model Merging', 'desc': 'This paper explores the technique of model merging to improve large language models during their pre-training phase. The authors conduct extensive experiments on various architectures, including dense models and Mixture-of-Experts (MoE), to assess the impact of merging checkpoints. They find that using constant learning rates during merging not only enhances model performance but also allows for better predictions of training behavior. The study provides valuable insights and practical guidelines for the open-source community to implement effective model merging strategies, ultimately reducing training costs and improving efficiency.'}, 'zh': {'title': 'Ê®°ÂûãÂêàÂπ∂ÔºöÊèêÂçáÈ¢ÑËÆ≠ÁªÉÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Ê®°ÂûãÂêàÂπ∂ÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊäÄÊúØÔºåÂèØ‰ª•Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩÜÂú®Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÂ∫îÁî®‰ªçÁÑ∂Áõ∏ÂØπÊú™Ë¢´Êé¢Á¥¢„ÄÇÊú¨ÊñáÂÖ®Èù¢Á†îÁ©∂‰∫ÜÂú®È¢ÑËÆ≠ÁªÉËøáÁ®ã‰∏≠‰ΩøÁî®ÁöÑÊ®°ÂûãÂêàÂπ∂ÊäÄÊúØ„ÄÇÈÄöËøáÂØπÊï∞Áôæ‰∏áÂà∞Ë∂ÖËøá1000‰∫øÂèÇÊï∞ÁöÑÂØÜÈõÜÂíåÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫Ü‰ΩøÁî®ÊÅíÂÆöÂ≠¶‰π†ÁéáËÆ≠ÁªÉÁöÑÊ£ÄÊü•ÁÇπÂêàÂπ∂‰∏ç‰ªÖÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåËøòËÉΩÂáÜÁ°ÆÈ¢ÑÊµãÈÄÄÁÅ´Ë°å‰∏∫„ÄÇËøô‰∫õÊîπËøõ‰ΩøÂæóÊ®°ÂûãÂºÄÂèëÊõ¥Âä†È´òÊïàÔºåÂπ∂ÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13427', 'title': 'MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision', 'url': 'https://huggingface.co/papers/2505.13427', 'abstract': 'While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://github.com/ModalMinds/MM-PRM.', 'score': 20, 'issue_id': 3847, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '22362149c9b7b5ae', 'authors': ['Lingxiao Du', 'Fanqing Meng', 'Zongkai Liu', 'Zhixiang Zhou', 'Ping Luo', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13427.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#multimodal', '#math', '#training', '#open_source', '#benchmark', '#data', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—à–∞–≥–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å MM-PRM, –æ–±—É—á–∞—é—â–∞—è—Å—è –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç MM-K12 —Å 10 000 –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ 700 —Ç—ã—Å—è—á –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —à–∞–≥–æ–≤ —Ä–µ—à–µ–Ω–∏—è –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ MM-PRM –≤ —Å—Ö–µ–º–µ –≤—ã–≤–æ–¥–∞ Best-of-N –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ MM-K12, —Ç–∞–∫ –∏ –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º—è–≥–∫–∏—Ö –º–µ—Ç–æ–∫, –º–µ–Ω—å—à–∏—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—É—Ç–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Process Supervision', 'desc': 'This paper introduces MM-PRM, a novel process reward model designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs) in solving complex math problems. The authors highlight that MLLMs often struggle with multi-step reasoning due to insufficient supervision of intermediate steps. To overcome this, they create a strong multimodal model called MM-Policy and a new dataset, MM-K12, containing 10,000 multimodal math problems. By employing a Monte Carlo Tree Search method, they generate over 700,000 annotations to train the PRM, which significantly enhances the logical consistency of reasoning in various benchmarks.'}, 'zh': {'title': 'ËøáÁ®ãÁõëÁù£ÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÈÄªËæëÁ®≥ÂÅ•ÊÄß', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàMM-PRMÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÂ§öÊ≠•È™§Êé®ÁêÜ‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Áº∫‰πèÁªÜÁ≤íÂ∫¶ÁöÑÁõëÁù£ÔºåÂØºËá¥ÈÄªËæë‰∏ç‰∏ÄËá¥ÊàñÈÉ®ÂàÜÊ≠£Á°ÆÁöÑÁªìÊûú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãMM-PolicyÔºåÂπ∂ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´10,000‰∏™ÂèØÈ™åËØÅÁ≠îÊ°àÁöÑÂ§öÊ®°ÊÄÅÊï∞Â≠¶ÈóÆÈ¢òÊï∞ÊçÆÈõÜMM-K12„ÄÇÈÄöËøáÊó†‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊñπÂºèÁîüÊàêË∂ÖËøá70‰∏áÊù°Ê≠•È™§Á∫ßÊ≥®ÈáäÔºåMM-PRMÂú®Êé®ÁêÜË∑ØÂæÑËØÑÂàÜ‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåËØÅÊòé‰∫ÜËøáÁ®ãÁõëÁù£Âú®Â¢ûÂº∫Â§öÊ®°ÊÄÅÊé®ÁêÜÁ≥ªÁªüÈÄªËæëÁ®≥ÂÅ•ÊÄßÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13215', 'title': 'Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation', 'url': 'https://huggingface.co/papers/2505.13215', 'abstract': 'Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.', 'score': 20, 'issue_id': 3846, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '0ab00a261298ad44', 'authors': ['Seungjun Oh', 'Younggeun Lee', 'Hyejin Jeon', 'Eunbyung Park'], 'affiliations': ['Department of Artificial Intelligence, Sungkyunkwan University', 'Department of Artificial Intelligence, Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13215.jpg', 'data': {'categories': ['#3d'], 'emoji': 'üé•', 'ru': {'title': '–ì–∏–±—Ä–∏–¥–Ω—ã–π 3D-4D –ø–æ–¥—Ö–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ 3D-4DGS –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω. –û–Ω –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç 3D –≥–∞—É—Å—Å–∏–∞–Ω—ã –¥–ª—è —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ 4D –≥–∞—É—Å—Å–∏–∞–Ω—ã –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –ø–∞–º—è—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é 4D –ø–æ–¥—Ö–æ–¥–æ–º. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–ª–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Efficient 3D-4D Scene Reconstruction with Hybrid Gaussian Splatting', 'desc': 'This paper presents a new method called hybrid 3D-4D Gaussian Splatting (3D-4DGS) for dynamic 3D scene reconstruction. It combines 3D and 4D Gaussian representations to efficiently model static and dynamic elements in a scene. By converting static regions to 3D Gaussians, the method reduces computational load and memory usage while preserving the quality of dynamic elements with 4D Gaussians. The results show that 3D-4DGS achieves faster training times and improved visual quality compared to traditional 4D Gaussian Splatting techniques.'}, 'zh': {'title': 'È´òÊïàÁöÑÂä®ÊÄÅ3DÂú∫ÊôØÈáçÂª∫Êñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøëÂä®ÊÄÅ3DÂú∫ÊôØÈáçÂª∫ÁöÑËøõÂ±ïÊòæÁ§∫Âá∫ËâØÂ•ΩÁöÑÊïàÊûúÔºåËÉΩÂ§üÂÆûÁé∞È´ò‰øùÁúüÂ∫¶ÁöÑ3DÊñ∞ËßÜÂõæÂêàÊàêÔºåÂπ∂ÊèêÈ´òÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÂú®Ëøô‰∫õÊñπÊ≥ï‰∏≠Ôºå4DÈ´òÊñØÁÇπ‰∫ëÔºà4DGSÔºâÂõ†ÂÖ∂ËÉΩÂ§üÂª∫Ê®°È´ò‰øùÁúüÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥ÂèòÂåñËÄåÂèóÂà∞ÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÈùôÊÄÅÂå∫ÂüüÂÜó‰ΩôÂàÜÈÖç4DÈ´òÊñØÊó∂ÔºåÂØºËá¥‰∫ÜÊòæËëóÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÔºåÂπ∂ÂèØËÉΩÈôç‰ΩéÂõæÂÉèË¥®Èáè„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑Âêà3D-4DÈ´òÊñØÁÇπ‰∫ëÔºà3D-4DGSÔºâÊ°ÜÊû∂ÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Áî®3DÈ´òÊñØË°®Á§∫ÈùôÊÄÅÂå∫ÂüüÔºåÂêåÊó∂‰∏∫Âä®ÊÄÅÂÖÉÁ¥†‰øùÁïô4DÈ´òÊñØÔºå‰ªéËÄåÊòæËëóÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12805', 'title': 'FedSVD: Adaptive Orthogonalization for Private Federated Learning with\n  LoRA', 'url': 'https://huggingface.co/papers/2505.12805', 'abstract': 'Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update (BA) intensifies this effect. Freezing one matrix (e.g., A) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the B matrix and transmits it to the server. The server aggregates the B matrices, computes the product BA using the previous A, and refactorizes the result via SVD. This yields a new adaptive A composed of the orthonormal right singular vectors of BA, and an updated B containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing A to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of A bounds the gradient norms of B and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.', 'score': 20, 'issue_id': 3848, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '41ef4fd84db0c7fb', 'authors': ['Seanie Lee', 'Sangwoo Park', 'Dong Bok Lee', 'Dominik Wagner', 'Haebin Seong', 'Tobias Bocklet', 'Juho Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'Technische Hochschule N√ºrnberg Georg Simon Ohm'], 'pdf_title_img': 'assets/pdf/title_img/2505.12805.jpg', 'data': {'categories': ['#training', '#data', '#benchmark', '#security', '#optimization'], 'emoji': 'üîí', 'ru': {'title': 'FedSVD: –ó–∞—â–∏—â–µ–Ω–Ω–æ–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ FedSVD –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é. FedSVD —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É—Å–∏–ª–µ–Ω–∏—è —à—É–º–∞ –≤ –º–µ—Ç–æ–¥–µ Low-Rank Adaptation (LoRA) –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ DP-SGD. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ (SVD) –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ä–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —É—Å–∏–ª–µ–Ω–∏—è —à—É–º–∞. FedSVD –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Federated Learning with FedSVD: A Noise-Resilient Approach', 'desc': 'This paper presents FedSVD, a novel method that enhances the fine-tuning of language models in federated learning while addressing the challenges posed by noise amplification in differentially private stochastic gradient descent (DP-SGD). By utilizing singular value decomposition (SVD), FedSVD allows clients to optimize only one matrix (B) and send it to a central server, which then aggregates these updates to improve model adaptation. This approach mitigates the noise amplification that occurs when combining LoRA with DP-SGD, ensuring that the model remains expressive without compromising privacy. The results demonstrate that FedSVD improves both stability and performance across various privacy settings, outperforming existing methods.'}, 'zh': {'title': 'FedSVDÔºö‰ºòÂåñËÅîÈÇ¶Â≠¶‰π†‰∏≠ÁöÑ‰ΩéÁß©ÈÄÇÂ∫î', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫FedSVDÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÂú®ËÅîÈÇ¶Â≠¶‰π†‰∏≠‰∏éÂ∑ÆÂàÜÈöêÁßÅÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÔºàDP-SGDÔºâÁªìÂêàÊó∂ÁöÑÂô™Â£∞ÊîæÂ§ßÈóÆÈ¢ò„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂ•áÂºÇÂÄºÂàÜËß£ÔºàSVDÔºâÁöÑÂÖ®Â±ÄÈáçÂèÇÊï∞ÂåñÔºåFedSVDÂÖÅËÆ∏ÊØè‰∏™ÂÆ¢Êà∑Á´Ø‰ªÖ‰ºòÂåñBÁü©ÈòµÂπ∂Â∞ÜÂÖ∂‰º†ËæìÂà∞ÊúçÂä°Âô®„ÄÇÊúçÂä°Âô®ËÅöÂêàBÁü©ÈòµÔºåËÆ°ÁÆóBAÁöÑ‰πòÁßØÔºåÂπ∂ÈÄöËøáSVDÈáçÊñ∞Âõ†ÂºèÂàÜËß£Ôºå‰ªéËÄåÁîüÊàêÊñ∞ÁöÑÈÄÇÂ∫îÊÄßAÂíåÊõ¥Êñ∞ÂêéÁöÑB„ÄÇËØ•ÊñπÊ≥ïÊúâÊïàÂáèÂ∞ë‰∫ÜÂô™Â£∞ÊîæÂ§ßÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩÔºåÂ∞§ÂÖ∂Âú®ÈöêÁßÅËÆæÁΩÆ‰∏ãË°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12504', 'title': 'CPGD: Toward Stable Rule-based Reinforcement Learning for Language\n  Models', 'url': 'https://huggingface.co/papers/2505.12504', 'abstract': 'Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA.', 'score': 20, 'issue_id': 3847, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': 'f8b07da7e5e43f1e', 'authors': ['Zongkai Liu', 'Fanqing Meng', 'Lingxiao Du', 'Zhixiang Zhou', 'Chao Yu', 'Wenqi Shao', 'Qiaosheng Zhang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12504.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º CPGD –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. CPGD –≤–≤–æ–¥–∏—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä–µ–π—Ñ –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –æ—Ç—Å–µ—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–æ–ª–∏—Ç–∏–∫–∏. –ê–≤—Ç–æ—Ä—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—é—Ç CPGD –∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –æ–Ω —É–º–µ–Ω—å—à–∞–µ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –Ω–∞–±–ª—é–¥–∞–µ–º—É—é –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with CPGD', 'desc': 'This paper introduces Clipped Policy Gradient Optimization with Policy Drift (CPGD), a new algorithm aimed at improving the stability of reinforcement learning in language models. Traditional methods often face issues like training collapse due to large policy updates and improper clipping. CPGD addresses these challenges by using a KL divergence-based policy drift constraint to regulate updates and a clipping mechanism to limit excessive changes. The authors provide theoretical support for CPGD and demonstrate its effectiveness in enhancing performance while ensuring stable training.'}, 'zh': {'title': 'Á®≥ÂÆöÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ', 'desc': 'ÊúÄËøëÔºåÂü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁé∞ÊúâÁöÑRLÊñπÊ≥ïÂ¶ÇGRPO„ÄÅREINFORCE++ÂíåRLOOÂ∏∏Â∏∏Èù¢‰∏¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁÆóÊ≥ï‚Äî‚ÄîÂ∏¶ÊúâÁ≠ñÁï•ÊºÇÁßªÁöÑÂâ™ÂàáÁ≠ñÁï•Ê¢ØÂ∫¶‰ºòÂåñÔºàCPGDÔºâÔºåÊó®Âú®Á®≥ÂÆöËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁ≠ñÁï•Â≠¶‰π†„ÄÇCPGDÈÄöËøáÂü∫‰∫éKLÊï£Â∫¶ÁöÑÁ≠ñÁï•ÊºÇÁßªÁ∫¶ÊùüÂä®ÊÄÅÂú∞ËßÑËåÉÁ≠ñÁï•Êõ¥Êñ∞ÔºåÂπ∂Âà©Áî®ÂØπÊï∞ÊØîÁéáÁöÑÂâ™ÂàáÊú∫Âà∂Èò≤Ê≠¢ËøáÂ§ßÁöÑÁ≠ñÁï•Êõ¥Êñ∞„ÄÇÊàë‰ª¨ÁöÑÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûËØÅÁªìÊûúË°®ÊòéÔºåCPGD‰∏ç‰ªÖÁºìËß£‰∫Ü‰πãÂâçÊñπÊ≥ïÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÔºåËøòÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12992', 'title': 'Fractured Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2505.12992', 'abstract': 'Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.', 'score': 16, 'issue_id': 3849, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '464c0b3fac842217', 'authors': ['Baohao Liao', 'Hanze Dong', 'Yuhui Xu', 'Doyen Sahoo', 'Christof Monz', 'Junnan Li', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2505.12992.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Fractured Sampling –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–¥–µ–µ —É—Å–µ—á–µ–Ω–∏—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought, CoT) –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥—É –ø–æ–ª–Ω—ã–º CoT –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ —Ç—Ä–µ–º –æ—Å—è–º: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –∏ –≥–ª—É–±–∏–Ω–∞ —É—Å–µ—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Fractured Sampling –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –ø—è—Ç–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –ø—Ä–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Efficient Reasoning with Fractured Sampling', 'desc': 'This paper presents a new method called Fractured Sampling that enhances the reasoning abilities of large language models (LLMs) during inference without the need for retraining. It builds on the concept of Chain-of-Thought (CoT) prompting, which improves accuracy by generating intermediate reasoning steps, but often at a high token cost. The authors demonstrate that a truncated version of CoT can achieve similar results with significantly fewer tokens. By exploring different ways to balance reasoning depth and the number of solutions, Fractured Sampling offers a more efficient approach that improves accuracy while reducing computational costs.'}, 'zh': {'title': 'È´òÊïàÊé®ÁêÜÔºöFractured SamplingÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÊäÄÊúØÔºåÁß∞‰∏∫Fractured SamplingÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÊà™Êñ≠ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÔºåËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÊúÄÁªàÁ≠îÊ°àÊó∂ÂáèÂ∞ë‰∫ÜÊâÄÈúÄÁöÑtokenÊï∞ÈáèÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éÂÆåÊï¥CoTÁõ∏‰ººÁöÑÂáÜÁ°ÆÊÄß„ÄÇFractured SamplingÂú®Êé®ÁêÜËΩ®ËøπÊï∞Èáè„ÄÅÊØèÊù°ËΩ®ËøπÁöÑÊúÄÁªàËß£ÂÜ≥ÊñπÊ°àÊï∞ÈáèÂíåÊé®ÁêÜÊ∑±Â∫¶Á≠â‰∏â‰∏™Áª¥Â∫¶‰∏äËøõË°åÊèíÂÄºÔºå‰ªéËÄå‰ºòÂåñ‰∫ÜÂáÜÁ°ÆÊÄß‰∏éÊàêÊú¨ÁöÑÂπ≥Ë°°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥È´òÊïàÂíåÂèØÊâ©Â±ïÁöÑLLMÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13444', 'title': 'ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.13444', 'abstract': 'Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.', 'score': 15, 'issue_id': 3848, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '7bd36c8068fb640c', 'authors': ['Liyan Tang', 'Grace Kim', 'Xinyu Zhao', 'Thom Lake', 'Wenxuan Ding', 'Fangcong Yin', 'Prasann Singhal', 'Manya Wadhwa', 'Zeyu Leo Liu', 'Zayne Sprague', 'Ramya Namuduri', 'Bodun Hu', 'Juan Diego Rodriguez', 'Puyuan Peng', 'Greg Durrett'], 'affiliations': ['The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.13444.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#cv', '#benchmark', '#synthetic', '#reasoning', '#dataset'], 'emoji': 'üìä', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∏–∞–≥—Ä–∞–º–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ChartMuseum –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º –º–æ–¥–µ–ª—è–º–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –ª—é–¥—è–º –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–≥—Ä–∞–º–º. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á. ChartMuseum —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –º–æ–¥–µ–ª–µ–π –∏ –ª—é–¥–µ–π –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging the Gap in Chart Understanding: Visual vs. Textual Reasoning', 'desc': 'This paper addresses the challenges faced by large vision-language models (LVLMs) in understanding charts, highlighting their struggle with visual reasoning compared to textual reasoning. The authors present a case study using a synthetic dataset that shows a significant drop in model performance as visual complexity increases, while human performance remains stable. They introduce ChartMuseum, a new benchmark for Chart Question Answering (QA) that includes 1,162 expert-annotated questions designed to test both visual and textual reasoning. The results reveal a substantial performance gap between humans and models, with the best model achieving only 63% accuracy compared to 93% for humans, particularly struggling with questions that require visual reasoning.'}, 'zh': {'title': 'ÂõæË°®ÁêÜËß£Ôºö‰∫∫Á±ª‰∏éÊ®°ÂûãÁöÑÂ∑ÆË∑ù', 'desc': 'ÂõæË°®ÁêÜËß£ÂØπÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÊèêÂá∫‰∫ÜÁã¨ÁâπÁöÑÊåëÊàòÔºåÂõ†‰∏∫ÂÆÉÈúÄË¶ÅÂ§çÊùÇÁöÑÊñáÊú¨ÂíåËßÜËßâÊé®ÁêÜËÉΩÂäõÁöÑÁªìÂêà„ÄÇÂΩìÂâçÁöÑLVLMÂú®Ëøô‰∫õÊäÄËÉΩ‰πãÈó¥Â≠òÂú®ÊòæËëóÁöÑ‰∏çÂπ≥Ë°°ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâÊé®ÁêÜÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÊàë‰ª¨ÈÄöËøá‰∏Ä‰∏™ÂêàÊàêÊï∞ÊçÆÈõÜËøõË°åÊ°à‰æãÁ†îÁ©∂ÔºåÂèëÁé∞ÈöèÁùÄËßÜËßâÂ§çÊùÇÊÄßÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºåËÄå‰∫∫Á±ªÁöÑË°®Áé∞Âàô‰øùÊåÅÁ®≥ÂÆö„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜChartMuseumÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂõæË°®ÈóÆÁ≠îÂü∫ÂáÜÔºåÂåÖÂê´1162‰∏™‰∏ìÂÆ∂Ê≥®ÈáäÁöÑÈóÆÈ¢òÔºåÊó®Âú®ËØÑ‰º∞Â§çÊùÇÁöÑËßÜËßâÂíåÊñáÊú¨Êé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11932', 'title': 'Neuro-Symbolic Query Compiler', 'url': 'https://huggingface.co/papers/2505.11932', 'abstract': "Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar G[q] to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.", 'score': 14, 'issue_id': 3846, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '9445be4eff7e4edc', 'authors': ['Yuyao Zhang', 'Zhicheng Dou', 'Xiaoxi Li', 'Jiajie Jin', 'Yongkang Wu', 'Zhonghua Li', 'Qi Ye', 'Ji-Rong Wen'], 'affiliations': ['Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.11932.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–º–ø–∏–ª—è—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö', 'desc': 'QCompiler - —ç—Ç–æ –Ω–µ–π—Ä–æ-—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≥—Ä–∞–º–º–∞—Ç–∏–∫—É BNF –¥–ª—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –∏—Ö –≤ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –¥–µ—Ä–µ–≤—å—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏. QCompiler –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∑–∞–ø—Ä–æ—Å–æ–≤, –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä.'}, 'en': {'title': 'Enhancing Query Understanding in RAG Systems with QCompiler', 'desc': 'This paper introduces QCompiler, a neuro-symbolic framework designed to enhance the understanding of complex search queries in Retrieval-Augmented Generation (RAG) systems. It utilizes a specially designed Backus-Naur Form (BNF) grammar to formalize these queries, ensuring that they are both complete and free of unnecessary complexity. QCompiler operates through a series of components, including a Query Expression Translator and a Lexical Syntax Parser, which work together to convert queries into Abstract Syntax Trees (ASTs). By focusing on the atomicity of sub-queries, QCompiler improves the accuracy of document retrieval and response generation for intricate queries.'}, 'zh': {'title': 'ÊèêÂçáRAGÁ≥ªÁªüÁöÑÂ§çÊùÇÊü•ËØ¢ËØÜÂà´ËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫QCompilerÁöÑÁ•ûÁªèÁ¨¶Âè∑Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÂØπÂ§çÊùÇÊü•ËØ¢ÁöÑËØÜÂà´ËÉΩÂäõ„ÄÇQCompilerÂü∫‰∫éËØ≠Ë®ÄËØ≠Ê≥ïËßÑÂàôÂíåÁºñËØëÂô®ËÆæËÆ°ÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÊúÄÂ∞è‰ΩÜË∂≥Â§üÁöÑÂ∑¥ÁßëÊñØ-ËØ∫Â∞îÂΩ¢ÂºèÔºàBNFÔºâËØ≠Ê≥ïG[q]Ôºå‰ª•ÂΩ¢ÂºèÂåñÂ§çÊùÇÊü•ËØ¢„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ï‰∏çÂêåÔºåËøôÁßçËØ≠Ê≥ïÂú®‰øùÊåÅÂÆåÊï¥ÊÄßÁöÑÂêåÊó∂ÔºåÂáèÂ∞ë‰∫ÜÂÜó‰Ωô„ÄÇÈÄöËøáÂ∞ÜÊü•ËØ¢ÁºñËØëÊàêÊäΩË±°ËØ≠Ê≥ïÊ†ëÔºàASTÔºâÔºåQCompilerËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞Ê£ÄÁ¥¢ÊñáÊ°£Âπ∂ÁîüÊàêÂìçÂ∫îÔºå‰ªéËÄåÊòæËëóÊèêÂçáRAGÁ≥ªÁªüÂ§ÑÁêÜÂ§çÊùÇÊü•ËØ¢ÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12346', 'title': 'SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2505.12346', 'abstract': "Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.", 'score': 13, 'issue_id': 3857, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': 'fbe6291cd68e8efb', 'authors': ['Minghan Chen', 'Guikun Chen', 'Wenguan Wang', 'Yi Yang'], 'affiliations': ['ReLER Lab, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12346.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#math', '#benchmark', '#rl', '#hallucinations'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò: —É—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SEED-GRPO. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –∏–∑–º–µ—Ä—è—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. SEED-GRPO –º–æ–¥—É–ª–∏—Ä—É–µ—Ç –≤–µ–ª–∏—á–∏–Ω—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –ø—è—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ SEED-GRPO –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è —Ç–æ—á–Ω–æ—Å—Ç–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã.'}, 'en': {'title': 'Enhancing LLM Training with Uncertainty Awareness', 'desc': "This paper introduces SEED-GRPO, an advanced method for training large language models (LLMs) that takes into account the model's uncertainty regarding input prompts. Traditional Group Relative Policy Optimization (GRPO) treats all prompts the same, which can overlook important variations in the model's confidence. SEED-GRPO incorporates semantic entropy, a measure of the diversity of answers generated for a prompt, to adjust how much the model learns from different questions. By applying this uncertainty-aware approach, the model can make more cautious updates for prompts it finds challenging, leading to improved performance on mathematical reasoning tasks."}, 'zh': {'title': 'Âü∫‰∫é‰∏çÁ°ÆÂÆöÊÄßÁöÑÁ≠ñÁï•‰ºòÂåñÊñ∞ÊñπÊ≥ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜËæìÂÖ•ÊèêÁ§∫Êó∂Ë°®Áé∞Âá∫‰∏çÂêåÁöÑ‰ø°ÂøÉÊ∞¥Âπ≥ÔºåÊúâ‰∫õÊèêÁ§∫‰∫ßÁîü‰∏ÄËá¥‰∏îËØ≠‰πâÁõ∏‰ººÁöÑÁ≠îÊ°àÔºåËÄåÂè¶‰∏Ä‰∫õÂàô‰∫ßÁîüÂ§öÊ†∑ÊàñÁüõÁõæÁöÑËæìÂá∫„ÄÇËøôÁßçÂèòÂåñÂèçÊò†‰∫ÜÊ®°ÂûãÂØπËæìÂÖ•ÊèêÁ§∫ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåË°®ÊòéÊ®°ÂûãÂØπÁâπÂÆöÈóÆÈ¢òÁöÑÁêÜËß£Á®ãÂ∫¶„ÄÇ‰º†ÁªüÁöÑÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÂú®Á≠ñÁï•Êõ¥Êñ∞Êó∂ÂØπÊâÄÊúâÊèêÁ§∫‰∏ÄËßÜÂêå‰ªÅÔºåÂøΩÁï•‰∫ÜÊ®°ÂûãÁü•ËØÜËæπÁïåÁöÑÈáçË¶Å‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSEED-GRPOÔºàËØ≠‰πâÁÜµÂ¢ûÂº∫ÁöÑGRPOÔºâÔºåÂÆÉÈÄöËøáÊµãÈáèËæìÂÖ•ÊèêÁ§∫ÁöÑËØ≠‰πâÁÜµÊù•ÊòæÂºèËÄÉËôëLLMsÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÂä®ÊÄÅË∞ÉÊï¥Á≠ñÁï•Êõ¥Êñ∞ÁöÑÂπÖÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12081', 'title': 'VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.12081', 'abstract': 'Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).', 'score': 13, 'issue_id': 3845, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '9b7953f88ae7653d', 'authors': ['Yuqi Liu', 'Tianyuan Qu', 'Zhisheng Zhong', 'Bohao Peng', 'Shu Liu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2505.12081.jpg', 'data': {'categories': ['#multimodal', '#cv', '#benchmark', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω VisionReasoner - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. VisionReasoner –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –≤—ã–¥–∞—á–µ–π –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ VisionReasoner –Ω–∞–¥ Qwen2.5VL –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ø–æ–¥—Å—á–µ—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤.'}, 'en': {'title': 'VisionReasoner: Unifying Visual Perception with Advanced Reasoning', 'desc': 'This paper presents VisionReasoner, a unified framework designed to enhance visual perception tasks through advanced reasoning capabilities. It employs innovative multi-object cognitive learning strategies and reformulates tasks systematically to improve its performance across various visual challenges. The model processes visual inputs in a structured manner, allowing it to effectively respond to user queries. Evaluation results demonstrate that VisionReasoner significantly outperforms existing models in detection, segmentation, and counting tasks, showcasing its effectiveness as a comprehensive solution for visual perception.'}, 'zh': {'title': 'Áªü‰∏ÄËßÜËßâÊÑüÁü•ÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫VisionReasonerÁöÑÁªü‰∏ÄÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®ÂÖ±‰∫´Ê®°Âûã‰∏≠Â§ÑÁêÜÂ§öÁßçËßÜËßâÊÑüÁü•‰ªªÂä°„ÄÇÈÄöËøáËÆæËÆ°Êñ∞È¢ñÁöÑÂ§öÂØπË±°ËÆ§Áü•Â≠¶‰π†Á≠ñÁï•ÂíåÁ≥ªÁªüÁöÑ‰ªªÂä°ÈáçÊûÑÔºåVisionReasonerÂ¢ûÂº∫‰∫ÜÂÖ∂Êé®ÁêÜËÉΩÂäõÔºå‰ª•ÂàÜÊûêËßÜËßâËæìÂÖ•Âπ∂Ëß£ÂÜ≥Â§öÊ†∑ÁöÑÊÑüÁü•‰ªªÂä°„ÄÇËØ•Ê®°ÂûãÂú®ÁîüÊàêÊâÄÈúÄËæìÂá∫‰πãÂâçÔºå‰ºöÂÖàËøõË°åÁªìÊûÑÂåñÁöÑÊé®ÁêÜËøáÁ®ãÔºå‰ª•ÂìçÂ∫îÁî®Êà∑Êü•ËØ¢„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVisionReasonerÂú®Ê£ÄÊµã„ÄÅÂàÜÂâ≤ÂíåËÆ°Êï∞Á≠â‰∏â‰∏™ÂÖ≥ÈîÆÈ¢ÜÂüüÁöÑÂçÅ‰∏™‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜQwen2.5VL„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07704', 'title': 'Through the Looking Glass: Common Sense Consistency Evaluation of Weird\n  Images', 'url': 'https://huggingface.co/papers/2505.07704', 'abstract': 'Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.', 'score': 13, 'issue_id': 3862, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9fc123da51142c1e', 'authors': ['Elisei Rykov', 'Kseniia Petrushina', 'Kseniia Titova', 'Anton Razzhigaev', 'Alexander Panchenko', 'Vasily Konovalov'], 'affiliations': ['AIRI', 'MTS AI', 'Moscow Institute of Physics and Technology', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.07704.jpg', 'data': {'categories': ['#training', '#dataset', '#interpretability', '#cv', '#architecture', '#optimization'], 'emoji': 'üîç', 'ru': {'title': '–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Through the Looking Glass (TLG). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä—É–ø–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LVLMs) –∏ —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ó–∞—Ç–µ–º —ç—Ç–∏ —Ñ–∞–∫—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å –ø—É–ª–∏–Ω–≥–æ–º –≤–Ω–∏–º–∞–Ω–∏—è. TLG –¥–æ—Å—Ç–∏–≥ –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö WHOOPS! –∏ WEIRD.'}, 'en': {'title': 'Assessing Image Realism with Through the Looking Glass (TLG)', 'desc': 'This paper presents a new method called Through the Looking Glass (TLG) for evaluating the common sense consistency of images using Large Vision-Language Models (LVLMs). The approach involves extracting atomic facts from images with a Transformer-based encoder, which helps in understanding the context of the images better. By fine-tuning a compact attention-pooling classifier on these encoded facts, TLG improves the accuracy of image assessments. The method has set a new benchmark in performance on the WHOOPS! and WEIRD datasets, demonstrating its effectiveness in measuring image realism.'}, 'zh': {'title': 'ÈÄèËøáÈïúÂ≠êÔºöÂõæÂÉèÂ∏∏ËØÜ‰∏ÄËá¥ÊÄßÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Âú®‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂‰∏≠ÔºåËØÑ‰º∞ÁúüÂÆûÂõæÂÉèÁöÑÂ§ñËßÇÊòØ‰∏ÄÈ°πÂ§çÊùÇÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫ÈÄèËøáÈïúÂ≠êÔºàTLGÔºâÔºåÁî®‰∫éËØÑ‰º∞ÂõæÂÉèÁöÑ‰∏ÄËá¥ÊÄß‰∏éÂ∏∏ËØÜ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÊèêÂèñÂõæÂÉè‰∏≠ÁöÑÂü∫Êú¨‰∫ãÂÆûÔºåÂπ∂ÈÄöËøáÂØπËøô‰∫õ‰∫ãÂÆûËøõË°åÁºñÁ†ÅÊù•Ëé∑ÂæóÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÁöÑTLGÂú®WHOOPS!ÂíåWEIRDÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊÄßËÉΩÔºåÂêåÊó∂‰ΩøÁî®‰∫ÜÁ¥ßÂáëÁöÑÂæÆË∞ÉÁªÑ‰ª∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13180', 'title': 'ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.13180', 'abstract': 'Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning.', 'score': 11, 'issue_id': 3851, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '333ba599d7e29ff8', 'authors': ['Matteo Merler', 'Nicola Dainese', 'Minttu Alakuijala', 'Giovanni Bonetta', 'Pietro Ferrazzi', 'Yu Tian', 'Bernardo Magnini', 'Pekka Marttinen'], 'affiliations': ['Department of Computer Science, Aalto University', 'Department of Mathematics, Universit√† degli Studi di Padova', 'Fondazione Bruno Kessler'], 'pdf_title_img': 'assets/pdf/title_img/2505.13180.jpg', 'data': {'categories': ['#benchmark', '#video', '#cv', '#games', '#reasoning', '#agents', '#open_source'], 'emoji': 'ü§ñ', 'ru': {'title': 'ViPlan: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ –∏ –ø—Ä—è–º–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ViPlan - –ø–µ—Ä–≤—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–∏–∫–∞—Ç–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è (VLM). –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ VLM –∏ –ø—Ä—è–º–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é VLM –≤ –¥–≤—É—Ö –¥–æ–º–µ–Ω–∞—Ö: –≤–∏–∑—É–∞–ª—å–Ω–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏ Blocksworld –∏ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ –¥–æ–º–∞—à–Ω–µ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–∏–º–≤–æ–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä—è–º–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ VLM –≤ Blocksworld, –≥–¥–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ —Ç–æ—á–Ω–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≤ –∑–∞–¥–∞—á–∞—Ö –¥–æ–º–∞—à–Ω–µ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Chain-of-Thought –Ω–µ –¥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤.'}, 'en': {'title': 'ViPlan: Bridging Symbolic Planning and Vision-Language Models for Better Visual Reasoning', 'desc': 'This paper discusses the integration of Large Language Models (LLMs) with symbolic planners to create more reliable and understandable plans in visual tasks. It introduces ViPlan, an open-source benchmark designed to evaluate various planning methods using Vision-Language Models (VLMs) in two distinct environments: Blocksworld and household robotics. The study compares the performance of symbolic planning against direct VLM planning, revealing that symbolic methods excel in scenarios requiring precise image grounding, while VLMs perform better in tasks that demand commonsense reasoning. Additionally, the findings indicate that current VLMs do not significantly benefit from Chain-of-Thought prompting, highlighting ongoing challenges in visual reasoning capabilities.'}, 'zh': {'title': 'ËßÜËßâËßÑÂàíÁöÑÊñ∞Âü∫ÂáÜÔºöViPlan', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜViPlanÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éËßÜËßâËßÑÂàíÁöÑÂºÄÊ∫êÂü∫ÂáÜÔºåÁªìÂêà‰∫ÜÁ¨¶Âè∑ËßÑÂàíÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÈÄêÊ∏êÂ¢ûÂä†ÈöæÂ∫¶ÁöÑ‰ªªÂä°ÔºåÂåÖÊã¨ÁªèÂÖ∏ÁöÑBlocksworldËßÑÂàíÈóÆÈ¢òÂíåÊ®°ÊãüÂÆ∂Â∫≠Êú∫Âô®‰∫∫ÁéØÂ¢É„ÄÇÈÄöËøáÂØπ‰πù‰∏™ÂºÄÊ∫êVLMÊ®°ÂûãËøõË°åÂü∫ÂáÜÊµãËØïÔºåÊàë‰ª¨ÂèëÁé∞Á¨¶Âè∑ËßÑÂàíÂú®Blocksworld‰∏≠Ë°®Áé∞‰ºò‰∫éÁõ¥Êé•‰ΩøÁî®VLMËøõË°åËßÑÂàíÔºåËÄåÂú®ÂÆ∂Â∫≠Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂàôÁõ∏Âèç„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑVLMÂú®ËßÜËßâÊé®ÁêÜÊñπÈù¢‰ªçÁÑ∂Â≠òÂú®Âõ∞ÈöæÔºå‰ΩøÁî®Chain-of-ThoughtÊèêÁ§∫Âπ∂Ê≤°ÊúâÊòæËëóÊèêÈ´òÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11855', 'title': 'When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification\n  of Scientific Research', 'url': 'https://huggingface.co/papers/2505.11855', 'abstract': 'Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.', 'score': 8, 'issue_id': 3849, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '8432e529923dacc5', 'authors': ['Guijin Son', 'Jiwoo Hong', 'Honglu Fan', 'Heejeong Nam', 'Hyunwoo Ko', 'Seungwon Lim', 'Jinyeop Song', 'Jinha Choi', 'Gon√ßalo Paulo', 'Youngjae Yu', 'Stella Biderman'], 'affiliations': ['Boeing Korea', 'EleutherAI', 'KAIST', 'MIT', 'OneLineAI', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11855.jpg', 'data': {'categories': ['#data', '#science', '#dataset', '#benchmark'], 'emoji': 'üîç', 'ru': {'title': '–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤—ã –±—ã—Ç—å –Ω–∞—É—á–Ω—ã–º–∏ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SPOT - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 83 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç —Å 91 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –æ—à–∏–±–∫–æ–π, –ø—Ä–∏–≤–µ–¥—à–µ–π –∫ –æ–ø–µ—á–∞—Ç–∫–∞–º –∏–ª–∏ –æ—Ç–∑—ã–≤—É. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å —ç—Ç–∏ –æ—à–∏–±–∫–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞—É—á–Ω—ã—Ö —Ä—É–∫–æ–ø–∏—Å–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª–∏—à—å 21.1% –ø–æ–ª–Ω–æ—Ç—ã –∏ 6.1% —Ç–æ—á–Ω–æ—Å—Ç–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –Ω–µ–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –Ω–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞–ø–æ–º–∏–Ω–∞—é—Ç —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–µ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏—è, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ LLM –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∫ –Ω–∞–¥–µ–∂–Ω–æ–π –ò–ò-assisted –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç.'}, 'en': {'title': 'Bridging the Gap: LLMs as Verifiers in Scientific Discovery', 'desc': 'This paper investigates the use of large language models (LLMs) as tools for verifying scientific manuscripts, rather than just generating content. The authors introduce a dataset called SPOT, which includes published papers with significant errors that could lead to errata or retraction. They evaluate various state-of-the-art LLMs on this dataset and find that their performance is lacking, with low recall and precision rates. The study concludes that current LLMs are not yet reliable enough for academic verification, as they often make errors similar to those of novice students.'}, 'zh': {'title': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â≠¶ÊúØÈ™åËØÅ‰∏≠ÁöÑÊåëÊàò', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â≠¶ÊúØÈ™åËØÅ‰∏≠ÁöÑÂ∫îÁî®ÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫SPOTÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´83ÁØáÂ∑≤ÂèëË°®ËÆ∫ÊñáÂíå91‰∏™ÊòæËëóÈîôËØØ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑLLMsÂú®ËØÜÂà´ÈîôËØØÊñπÈù¢ÁöÑË°®Áé∞‰∏ç‰Ω≥ÔºåÊúÄÈ´òÂè¨ÂõûÁéá‰ªÖ‰∏∫21.1%ÔºåÁ≤æÁ°ÆÁéá‰∏∫6.1%„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÁöÑÁΩÆ‰ø°Â∫¶ÊôÆÈÅçËæÉ‰ΩéÔºå‰∏îÂú®Â§öÊ¨°Áã¨Á´ãÊµãËØï‰∏≠ÔºåÊ®°ÂûãÂæàÂ∞ëËÉΩÈáçÊñ∞ÂèëÁé∞Áõ∏ÂêåÁöÑÈîôËØØ„ÄÇÈÄöËøá‰∏éÈ¢ÜÂüü‰∏ìÂÆ∂ÁöÑÂÆöÊÄßÂàÜÊûêÔºåÂèëÁé∞Âç≥‰ΩøÊòØÊúÄÂº∫ÁöÑÊ®°Âûã‰πü‰ºöÁäØÁ±ª‰ººÂ≠¶ÁîüÁ∫ßÂà´ÁöÑËØØËß£ÈîôËØØÔºåÊòæÁ§∫Âá∫ÂΩìÂâçLLMsÂú®ÂèØÈù†ÁöÑÂ≠¶ÊúØÈ™åËØÅ‰∏≠Â≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12849', 'title': 'Accelerate TarFlow Sampling with GS-Jacobi Iteration', 'url': 'https://huggingface.co/papers/2505.12849', 'abstract': 'Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow\'s sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is "simple" (converges in few iterations) or "tough" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow', 'score': 7, 'issue_id': 3846, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '191f5a409cc6b32e', 'authors': ['Ben Liu', 'Zhen Qin'], 'affiliations': ['TapTap, Shanghai, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.12849.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#cv', '#training'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ TarFlow —Å –ø–æ–º–æ—â—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª–∏ TarFlow –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ì–∞—É—Å—Å–∞-–ó–µ–π–¥–µ–ª—è-–Ø–∫–æ–±–∏ –∏ –≤–≤–æ–¥—è—Ç –¥–≤–µ –º–µ—Ç—Ä–∏–∫–∏: Convergence Ranking Metric (CRM) –∏ Initial Guessing Metric (IGM). CRM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –±–ª–æ–∫–æ–≤ TarFlow, –∞ IGM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–¥–æ 5.32 —Ä–∞–∑) –±–µ–∑ —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Accelerating TarFlow: Faster Sampling without Quality Loss', 'desc': 'This paper presents an optimization strategy for the TarFlow model, which combines transformer architecture with Normalizing Flow for image generation. The authors introduce the Gauss-Seidel-Jacobi (GS-Jacobi) iteration method to accelerate the slow sampling process of TarFlow. They identify that certain blocks within the model are more critical for image generation and propose metrics to evaluate their performance: the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM). Experimental results show that the GS-Jacobi method significantly improves sampling speed while preserving image quality, achieving notable speed-ups across various benchmarks.'}, 'zh': {'title': 'Âä†ÈÄüTarFlowÈááÊ†∑ÔºåÊèêÂçáÂõæÂÉèÁîüÊàêÊïàÁéá', 'desc': 'ÂõæÂÉèÁîüÊàêÊ®°ÂûãÂú®Â§ö‰∏™Â∫îÁî®‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇTarFlowÊ®°ÂûãÁªìÂêà‰∫ÜÂèòÊç¢Âô®Êû∂ÊûÑÂíåÂΩí‰∏ÄÂåñÊµÅÊ®°ÂûãÔºåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂõ†ÊûúÊ≥®ÊÑèÂäõÁöÑÈ°∫Â∫èËÆ°ÁÆóÔºåTarFlowÁöÑÈááÊ†∑ËøáÁ®ãÈùûÂ∏∏ÁºìÊÖ¢„ÄÇÊú¨ÊñáÈÄöËøá‰ºòÂåñÁ≠ñÁï•ÔºåÂà©Áî®È´òÊñØ-ËµõÂæ∑Â∞î-ÈõÖÂèØÊØîËø≠‰ª£ÊñπÊ≥ïÊòæËëóÂä†ÈÄü‰∫ÜTarFlowÁöÑÈááÊ†∑ËøáÁ®ãÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêÂõæÂÉèÁöÑË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13388', 'title': 'R3: Robust Rubric-Agnostic Reward Models', 'url': 'https://huggingface.co/papers/2505.13388', 'abstract': 'Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3', 'score': 6, 'issue_id': 3860, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '2589c2eb838a62f9', 'authors': ['David Anugraha', 'Zilu Tang', 'Lester James V. Miranda', 'Hanyang Zhao', 'Mohammad Rifqi Farhansyah', 'Garry Kuwanto', 'Derry Wijaya', 'Genta Indra Winata'], 'affiliations': ['Allen Institute for AI', 'Boston University', 'Capital One', 'Columbia University', 'Institut Teknologi Bandung', 'Monash Indonesia', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13388.jpg', 'data': {'categories': ['#open_source', '#data', '#optimization', '#benchmark', '#alignment', '#interpretability', '#rlhf'], 'emoji': 'üéØ', 'ru': {'title': 'R3: –ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ –∏ –≥–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'R3 - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. R3 –∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∞ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ä—É–±—Ä–∏–∫–∞–º –∏ –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ü–µ–Ω–∫–∏. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ –≥–∏–±–∫–æ—Å—Ç—å –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'R3: A New Era in Reward Modeling for Language Alignment', 'desc': 'This paper presents R3, a new framework for reward modeling that aims to improve the alignment of language model outputs with human preferences. Unlike traditional reward models that focus on narrow objectives, R3 is designed to be rubric-agnostic and generalizable, allowing it to adapt to various evaluation criteria. It also enhances interpretability by providing reasoned score assignments, making it easier to understand how scores are derived. Overall, R3 promotes a more transparent and flexible approach to evaluating language models, ensuring they align better with diverse human values.'}, 'zh': {'title': 'R3ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂØπ‰∫∫Á±ª‰ª∑ÂÄºÁöÑÂØπÈΩêËÉΩÂäõ', 'desc': 'Â•ñÂä±Ê®°ÂûãÂú®Â∞ÜËØ≠Ë®ÄÊ®°ÂûãÁöÑËæìÂá∫‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩê‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÁº∫‰πèÂèØÊéßÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÈíàÂØπÁã≠Á™ÑÁöÑÁõÆÊ†áËøõË°å‰ºòÂåñÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êõ¥ÂπøÊ≥õ‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÈÄöÁî®ÊÄß„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ª¨ÁöÑÊ†áÈáèËæìÂá∫Âú®Ê≤°Êúâ‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊÉÖÂÜµ‰∏ãÈöæ‰ª•Ëß£Èáä„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜR3ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ•ñÂä±Âª∫Ê®°Ê°ÜÊû∂ÔºåÂÖ∑Êúâ‰∏éËØÑÂàÜÊ†áÂáÜÊó†ÂÖ≥„ÄÅË∑®ËØÑ‰º∞Áª¥Â∫¶ÁöÑÂèØÊé®ÂπøÊÄßÔºåÂπ∂Êèê‰æõÂèØËß£ÈáäÁöÑÊé®ÁêÜËØÑÂàÜÂàÜÈÖç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12058', 'title': 'Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset\n  Generation & Smoke-Tests for Continuous LLM Evaluation', 'url': 'https://huggingface.co/papers/2505.12058', 'abstract': 'Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.', 'score': 6, 'issue_id': 3853, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': 'db0c871b64bd6d89', 'authors': ['Vincent Koc'], 'affiliations': ['Comet ML, Inc. New York, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.12058.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#synthetic', '#open_source', '#benchmark'], 'emoji': 'üöÄ', 'ru': {'title': '–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—Å–µ—Ö', 'desc': 'TQB++ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –∏–∑ 52 –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö. TQB++ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, –Ω–µ —Ç—Ä–µ–±—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –≠—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.'}, 'en': {'title': 'Quick and Efficient Testing for Language Models', 'desc': 'Tiny QA Benchmark++ (TQB++) is a lightweight, multilingual testing suite designed for large-language-model (LLM) pipelines, providing a quick and cost-effective way to ensure model quality. It includes a small set of gold-standard tests and a synthetic data generator that allows users to create custom test packs in various languages and domains. TQB++ integrates easily with existing tools, enabling developers to incorporate micro-benchmarks into their workflows without significant resource expenditure. This framework aims to enhance continuous quality assurance in generative AI by quickly identifying issues like prompt errors and tokenizer drift before more extensive testing is conducted.'}, 'zh': {'title': 'ËΩªÈáèÁ∫ßÂ§öËØ≠Ë®ÄÊµãËØïÔºåÊèêÂçáAIË¥®Èáè‰øùÈöú', 'desc': 'Tiny QA Benchmark++ÔºàTQB++ÔºâÊòØ‰∏Ä‰∏™Ë∂ÖËΩªÈáèÁ∫ßÁöÑÂ§öËØ≠Ë®ÄÊµãËØïÂ•ó‰ª∂ÔºåÊó®Âú®‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊèê‰æõÂø´ÈÄüÁöÑÂçïÂÖÉÊµãËØïÊï∞ÊçÆÈõÜÔºåËøêË°åÊó∂Èó¥Áü≠‰∏îÊàêÊú¨‰Ωé„ÄÇÂÆÉÁªìÂêà‰∫Ü‰∏Ä‰∏™52È°πÁöÑËã±ËØ≠ÈáëÊ†áÂáÜÈõÜÂíå‰∏Ä‰∏™Âü∫‰∫éLiteLLMÁöÑÂêàÊàêÊï∞ÊçÆÁîüÊàêÂô®ÔºåÂÖÅËÆ∏Áî®Êà∑ÁîüÊàêÈÄÇÂêà‰ªª‰ΩïËØ≠Ë®Ä„ÄÅÈ¢ÜÂüüÊàñÈöæÂ∫¶ÁöÑÂ∞èÊï∞ÊçÆÂåÖ„ÄÇTQB++Êèê‰æõ‰∫ÜÂçÅ‰∏™Áé∞ÊàêÁöÑÊï∞ÊçÆÂåÖÔºåË¶ÜÁõñÂ§öÁßçËØ≠Ë®ÄÔºåÂπ∂‰∏îÊØè‰∏™Êï∞ÊçÆÈõÜÈÉΩÈôÑÂ∏¶‰∫ÜÂÖÉÊï∞ÊçÆÂíåÂèØÁõ¥Êé•‰ΩøÁî®ÁöÑÊñá‰ª∂ÔºåÊñπ‰æøÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÂºÄÂèëÊµÅÁ®ã‰∏≠„ÄÇËøô‰∏™Ê°ÜÊû∂ÁöÑËÆæËÆ°Êó®Âú®Âä†ÈÄüÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÁîüÊÄÅÁ≥ªÁªü‰∏≠ÁöÑÊåÅÁª≠„ÄÅËµÑÊ∫êÈ´òÊïàÁöÑË¥®Èáè‰øùËØÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13437', 'title': 'FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance', 'url': 'https://huggingface.co/papers/2505.13437', 'abstract': 'Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as "switch leap with 0.5 turn" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys\'s ability to generate more natural and plausible fine-grained human actions.', 'score': 4, 'issue_id': 3847, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '28a08dbfb09c6639', 'authors': ['Dian Shao', 'Mingfei Shi', 'Shengda Xu', 'Haodong Chen', 'Yongle Huang', 'Binglu Wang'], 'affiliations': ['School of Astronautics, Northwestern Polytechnical University, Xian, China', 'School of Automation, Northwestern Polytechnical University, Xian, China', 'School of Software, Northwestern Polytechnical University, Xian, China', 'Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.13437.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion', '#video'], 'emoji': 'ü§∏', 'ru': {'title': '–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FinePhys - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ—á–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –°–∏—Å—Ç–µ–º–∞ —Å–Ω–∞—á–∞–ª–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç 2D –ø–æ–∑—ã, –∑–∞—Ç–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ 3D —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –î–∞–ª–µ–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≠–π–ª–µ—Ä–∞-–õ–∞–≥—Ä–∞–Ω–∂–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D –ø–æ–∑. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–æ–∑—ã –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç—Å—è —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö 2D —Ç–µ–ø–ª–æ–≤—ã—Ö –∫–∞—Ä—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏.'}, 'en': {'title': 'Bridging Physics and Data for Realistic Human Action Generation', 'desc': 'This paper presents FinePhys, a framework designed to improve the generation of fine-grained human actions in videos by integrating physics-based modeling. It addresses the challenges of synthesizing complex movements, such as gymnastics routines, by first estimating 2D poses and then converting them to 3D using in-context learning. To enhance the stability and interpretability of the generated poses, FinePhys employs a motion re-estimation module based on Euler-Lagrange equations, which calculates joint accelerations through bidirectional temporal updates. The combination of physics-based predictions with data-driven approaches results in more realistic and coherent human actions, as demonstrated by superior performance on benchmark datasets.'}, 'zh': {'title': 'FinePhysÔºöÁâ©ÁêÜÈ©±Âä®ÁöÑÁªÜÁ≤íÂ∫¶‰∫∫Á±ªÂä®‰ΩúÁîüÊàêÊ°ÜÊû∂', 'desc': 'Â∞ΩÁÆ°ËßÜÈ¢ëÁîüÊàêÊäÄÊúØÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂêàÊàêÁâ©ÁêÜ‰∏äÂêàÁêÜÁöÑ‰∫∫Á±ªÂä®‰Ωú‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊåÅÁª≠ÁöÑÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Âª∫Ê®°ÁªÜÁ≤íÂ∫¶ËØ≠‰πâÂíåÂ§çÊùÇÊó∂Èó¥Âä®ÊÄÅÊñπÈù¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜFinePhysÔºå‰∏Ä‰∏™ÁªÜÁ≤íÂ∫¶‰∫∫Á±ªÂä®‰ΩúÁîüÊàêÊ°ÜÊû∂ÔºåÁªìÂêàÁâ©ÁêÜÂ≠¶‰ª•Ëé∑ÂæóÊúâÊïàÁöÑÈ™®È™ºÊåáÂØº„ÄÇFinePhysÈ¶ñÂÖà‰ª•Âú®Á∫øÊñπÂºè‰º∞ËÆ°2DÂßøÂäøÔºåÁÑ∂ÂêéÈÄöËøá‰∏ä‰∏ãÊñáÂ≠¶‰π†ËøõË°å2DÂà∞3DÁöÑÁª¥Â∫¶ÊèêÂçá„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÁâ©ÁêÜÁöÑËøêÂä®ÈáçÊñ∞‰º∞ËÆ°Ê®°ÂùóÔºåFinePhysËÉΩÂ§üÁîüÊàêÊõ¥Ëá™ÁÑ∂ÂíåÂêàÁêÜÁöÑÁªÜÁ≤íÂ∫¶‰∫∫Á±ªÂä®‰Ωú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10238', 'title': 'MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation', 'url': 'https://huggingface.co/papers/2505.10238', 'abstract': 'Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.', 'score': 4, 'issue_id': 3851, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '5d8979fc79f9bd4e', 'authors': ['Yanbo Ding', 'Xirui Hu', 'Zhizhi Guo', 'Yali Wang'], 'affiliations': ['chinatelecom.cn', 'siat.ac.cn', 'xjtu.edu.cn'], 'pdf_title_img': 'assets/pdf/title_img/2505.10238.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': 'üï∫', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞: –æ—Ç 2D –∫ 4D –¥–≤–∏–∂–µ–Ω–∏—é', 'desc': 'MTVCrafter - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∏–º–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é 4D –¥–≤–∏–∂–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∑. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç 4DMoT –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è 3D –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–≤–∏–∂–µ–Ω–∏—è –≤ —Ç–æ–∫–µ–Ω—ã –∏ MV-DiT –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–Ω–∏–º–∞—Ü–∏–∏. MTVCrafter –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –º–µ—Ç—Ä–∏–∫–µ FID-VID –Ω–∞ 65% –∏ —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –º–∏—Ä–µ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–æ–∑–æ–π —á–µ–ª–æ–≤–µ–∫–∞.'}, 'en': {'title': 'Revolutionizing Human Animation with 4D Motion Tokens', 'desc': 'This paper presents MTVCrafter, a novel framework for human image animation that utilizes raw 3D motion sequences instead of traditional 2D-rendered pose images. By introducing 4DMoT, the framework quantizes 3D motion into 4D motion tokens, which provide enhanced spatio-temporal cues and greater flexibility in animation control. Additionally, MV-DiT employs motion-aware attention mechanisms to effectively utilize these motion tokens, allowing for more expressive human image generation in complex 3D environments. The results demonstrate that MTVCrafter achieves state-of-the-art performance, significantly improving generalization across various character types and styles.'}, 'zh': {'title': 'ÂºÄÂàõ4DËøêÂä®Ê†áËÆ∞ÁöÑ‰∫∫ÂÉèÂä®ÁîªÊñ∞ÊñπÂêë', 'desc': '‰∫∫ÂÉèÂä®ÁîªÊäÄÊúØÂú®Êï∞Â≠ó‰∫∫Á±ªÂ∫îÁî®‰∏≠Ë∂äÊù•Ë∂äÂèóÂà∞ÂÖ≥Ê≥®Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫é2DÊ∏≤ÊüìÁöÑÂßøÊÄÅÂõæÂÉèÔºåÈôêÂà∂‰∫ÜÂÖ∂Ê≥õÂåñËÉΩÂäõÂπ∂‰∏¢Â§±‰∫ÜÈáçË¶ÅÁöÑ3D‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMTVCrafterÊ°ÜÊû∂ÔºåÂÆÉÁõ¥Êé•Âª∫Ê®°ÂéüÂßãÁöÑ3DËøêÂä®Â∫èÂàóÔºàÂç≥4DËøêÂä®ÔºâÔºåÂπ∂ÂºïÂÖ•‰∫Ü4DMoTÔºà4DËøêÂä®Ê†áËÆ∞Âô®ÔºâÂ∞Ü3DËøêÂä®Â∫èÂàóÈáèÂåñ‰∏∫4DËøêÂä®Ê†áËÆ∞„ÄÇ‰∏é2DÊ∏≤ÊüìÁöÑÂßøÊÄÅÂõæÂÉèÁõ∏ÊØîÔºå4DËøêÂä®Ê†áËÆ∞Êèê‰æõ‰∫ÜÊõ¥Âº∫ÁöÑÊó∂Á©∫Á∫øÁ¥¢ÔºåÈÅøÂÖç‰∫ÜÂßøÊÄÅÂõæÂÉè‰∏éËßíËâ≤‰πãÈó¥‰∏•Ê†ºÁöÑÂÉèÁ¥†Á∫ßÂØπÈΩêÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊõ¥ÁÅµÊ¥ªÂíåËß£ËÄ¶ÁöÑÊéßÂà∂„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåMTVCrafterÂú®Â§öÁßçÈ£éÊ†ºÂíåÂú∫ÊôØ‰∏ãÂØπ‰∏çÂêåÁöÑÂºÄÊîæ‰∏ñÁïåËßíËâ≤ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12996', 'title': 'ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.12996', 'abstract': 'In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.', 'score': 3, 'issue_id': 3850, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '56ff8af5ab05144f', 'authors': ['Jiaan Wang', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.12996.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#rl', '#low_resource', '#machine_translation'], 'emoji': 'üåê', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ: –æ—Ç –æ–¥–Ω–æ—è–∑—ã—á–Ω–æ–≥–æ –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º—É —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —Å–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ. –ú–µ—Ç–æ–¥ —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è 11 —è–∑—ã–∫–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ 90 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö –ø–µ—Ä–µ–≤–æ–¥–∞.'}, 'en': {'title': 'Revolutionizing Multilingual Translation with Advanced Reward Modeling', 'desc': 'This paper discusses advancements in large reasoning models (LRMs) for neural machine translation (MT), particularly focusing on improving translation capabilities through reinforcement learning (RL). The authors introduce a novel reward modeling method that evaluates translation quality by comparing outputs from a policy MT model against a powerful LRM, DeepSeek-R1-671B. Their approach not only enhances performance in literary translation but also extends to 11 languages, achieving state-of-the-art results and surpassing existing LRMs. The lightweight reward modeling allows for effective transfer of translation skills across multiple languages, demonstrating significant improvements in multilingual MT performance.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÂ§öËØ≠Ë®ÄÁøªËØëÊñ∞Á™ÅÁ†¥', 'desc': 'ËøëÂπ¥Êù•ÔºåÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂ¶ÇOpenAI-o1ÂíåDeepSeek-R1Âú®Â§çÊùÇÈóÆÈ¢ò‰∏äÂ±ïÁé∞‰∫ÜÂá∫Ëâ≤ÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞Â≠¶ÂíåÁºñÁ®ãÊñπÈù¢„ÄÇ‰∏Ä‰∫õÂºÄÂàõÊÄßÁ†îÁ©∂Â∞ùËØïÂ∞ÜLRMsÁöÑÊàêÂäüÂ∫îÁî®‰∫éÁ•ûÁªèÊú∫Âô®ÁøªËØëÔºàMTÔºâÔºåÂπ∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊûÑÂª∫ÂÖ∑ÊúâÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõÁöÑMTÊ®°Âûã„ÄÇÂ∞ΩÁÆ°ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜËøô‰∫õÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®È´òËµÑÊ∫êËØ≠Ë®Ä‰∏äÔºåÂ¶ÇËã±ËØ≠Âíå‰∏≠ÊñáÔºåÂÖ∂‰ªñËØ≠Ë®ÄÁöÑË°®Áé∞‰ªç‰∏çÊòéÁ°Æ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ•ñÂä±Âª∫Ê®°ÊñπÊ≥ïÔºåÈÄöËøá‰∏éÂº∫Â§ßÁöÑLRMÔºàÂ¶ÇDeepSeek-R1-671BÔºâÊØîËæÉÁøªËØëÁªìÊûúÔºå‰∏∫MTÊ®°ÂûãÊèê‰æõÂ•ñÂä±Ôºå‰ªéËÄåÂÖÖÂàÜÂèëÊå•Âº∫ÂåñÂ≠¶‰π†ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12120', 'title': 'HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for\n  Computational Pathology', 'url': 'https://huggingface.co/papers/2505.12120', 'abstract': 'Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack sufficient scale, tissue diversity, and comprehensive clinical metadata, limiting the robustness and generalizability of AI models. In response, we introduce the HISTAI dataset, a large, multimodal, open-access WSI collection comprising over 60,000 slides from various tissue types. Each case in the HISTAI dataset is accompanied by extensive clinical metadata, including diagnosis, demographic information, detailed pathological annotations, and standardized diagnostic coding. The dataset aims to fill gaps identified in existing resources, promoting innovation, reproducibility, and the development of clinically relevant computational pathology solutions. The dataset can be accessed at https://github.com/HistAI/HISTAI.', 'score': 3, 'issue_id': 3855, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '4774ecdbd93d4a7b', 'authors': ['Dmitry Nechaev', 'Alexey Pchelnikov', 'Ekaterina Ivanova'], 'affiliations': ['HistAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.12120.jpg', 'data': {'categories': ['#multimodal', '#healthcare', '#dataset', '#data'], 'emoji': 'üî¨', 'ru': {'title': 'HISTAI: –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö HISTAI –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 60 000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ä–µ–∑–æ–≤ —Ç–∫–∞–Ω–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏. –ö–∞–∂–¥—ã–π —Å–ª—É—á–∞–π —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –¥–∏–∞–≥–Ω–æ–∑–æ–º, –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö HISTAI –ø—Ä–∏–∑–≤–∞–Ω —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–±–µ–ª—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö –∏ —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏.'}, 'en': {'title': 'Empowering AI in Pathology with the HISTAI Dataset', 'desc': "This paper presents the HISTAI dataset, a significant advancement in Digital Pathology that addresses the limitations of existing Whole Slide Image (WSI) datasets. It comprises over 60,000 slides from diverse tissue types, ensuring a large-scale and multimodal resource for training AI models. Each slide is enriched with comprehensive clinical metadata, including diagnosis and detailed annotations, which enhances the dataset's utility for research and clinical applications. By providing this open-access resource, the authors aim to foster innovation and improve the robustness of AI solutions in computational pathology."}, 'zh': {'title': 'HISTAIÊï∞ÊçÆÈõÜÔºöÊé®Âä®Êï∞Â≠óÁóÖÁêÜÂ≠¶ÁöÑÂàõÊñ∞‰∏éÂèëÂ±ï', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜHISTAIÊï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™Â§ßÂûãÁöÑÂ§öÊ®°ÊÄÅÂºÄÊîæËé∑ÂèñÁöÑÂÖ®ÂπªÁÅØÁâáÂõæÂÉèÔºàWSIÔºâÈõÜÂêàÔºåÂåÖÂê´Ë∂ÖËøá60,000Âº†Êù•Ëá™‰∏çÂêåÁªÑÁªáÁ±ªÂûãÁöÑÂπªÁÅØÁâá„ÄÇËØ•Êï∞ÊçÆÈõÜÈÖçÂ§á‰∫Ü‰∏∞ÂØåÁöÑ‰∏¥Â∫äÂÖÉÊï∞ÊçÆÔºåÂåÖÊã¨ËØäÊñ≠„ÄÅ‰∫∫Âè£ÁªüËÆ°‰ø°ÊÅØ„ÄÅËØ¶ÁªÜÁöÑÁóÖÁêÜÊ≥®ÈáäÂíåÊ†áÂáÜÂåñÁöÑËØäÊñ≠ÁºñÁ†Å„ÄÇHISTAIÊï∞ÊçÆÈõÜÊó®Âú®Â°´Ë°•Áé∞ÊúâËµÑÊ∫ê‰∏≠ÁöÑÁ©∫ÁôΩÔºå‰øÉËøõÂàõÊñ∞„ÄÅÂèØÈáçÂ§çÊÄß‰ª•Âèä‰∏¥Â∫äÁõ∏ÂÖ≥ËÆ°ÁÆóÁóÖÁêÜËß£ÂÜ≥ÊñπÊ°àÁöÑÂèëÂ±ï„ÄÇÈÄöËøáÊèê‰æõÂ§öÊ†∑ÂåñÂíå‰∏∞ÂØåÊ≥®ÈáäÁöÑÊï∞ÊçÆÔºåHISTAIÂ∞ÜÂ¢ûÂº∫‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊôÆÈÄÇÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11497', 'title': 'QVGen: Pushing the Limit of Quantized Video Generative Models', 'url': 'https://huggingface.co/papers/2505.11497', 'abstract': 'Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench.', 'score': 3, 'issue_id': 3851, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '48866ba1d6ad838b', 'authors': ['Yushi Huang', 'Ruihao Gong', 'Jing Liu', 'Yifu Ding', 'Chengtao Lv', 'Haotong Qin', 'Jun Zhang'], 'affiliations': ['Beihang University', 'ETH Z√ºrich', 'Hong Kong University of Science and Technology', 'Monash University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11497.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#inference', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'QVGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏ –≤—ã–≤–æ–¥–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –Ω–∏–∑–∫–æ–±–∏—Ç–Ω–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥–∏—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–Ω–≥–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ QVGen –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–ª–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ 4-–±–∏—Ç–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã.'}, 'en': {'title': 'Efficient Video Synthesis with Low-Bit Quantization', 'desc': 'This paper introduces QVGen, a new framework for quantization-aware training (QAT) specifically designed for video diffusion models (DMs). It addresses the challenges of high computational and memory requirements by enabling efficient video synthesis at extremely low-bit quantization levels, such as 4-bit. The authors demonstrate that reducing the gradient norm is crucial for effective convergence in QAT and propose auxiliary modules to minimize quantization errors. Additionally, they implement a rank-decay strategy to eliminate inference overhead while maintaining performance, achieving significant improvements over existing methods in video quality metrics.'}, 'zh': {'title': 'ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºåÊèêÂçáËßÜÈ¢ëÂêàÊàêÊïàÁéáÔºÅ', 'desc': 'ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºàDMsÔºâÂú®È´òË¥®ÈáèËßÜÈ¢ëÂêàÊàêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂ËÆ°ÁÆóÂíåÂÜÖÂ≠òÈúÄÊ±ÇÈ´òÔºåÈôêÂà∂‰∫ÜÂÆûÈôÖÂ∫îÁî®„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÊ°ÜÊû∂QVGenÔºåÊó®Âú®Âú®ÊûÅ‰Ωé‰ΩçÈáèÂåñÔºàÂ¶Ç4‰ΩçÊàñÊõ¥‰ΩéÔºâ‰∏ãÂÆûÁé∞È´òÊÄßËÉΩÂíåÈ´òÊïàÊé®ÁêÜ„ÄÇÊàë‰ª¨ÈÄöËøáÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåÈôç‰ΩéÊ¢ØÂ∫¶ËåÉÊï∞ÂØπQATÁöÑÊî∂ÊïõËá≥ÂÖ≥ÈáçË¶ÅÔºåÂπ∂ÂºïÂÖ•ËæÖÂä©Ê®°ÂùóÔºàPhiÔºâÊù•ÂáèÂ∞èÈáèÂåñËØØÂ∑ÆÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÊî∂ÊïõÊÄß„ÄÇÈÄöËøáÈÄêÊ≠•Ê∂àÈô§PhiÁöÑÊé®ÁêÜÂºÄÈîÄÔºåÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéQVGenÂú®4‰ΩçËÆæÁΩÆ‰∏ãÈ¶ñÊ¨°ÂÆûÁé∞‰∫Ü‰∏éÂÖ®Á≤æÂ∫¶Áõ∏ÂΩìÁöÑË¥®ÈáèÔºåÂπ∂ÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11484', 'title': 'SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2505.11484', 'abstract': "Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.", 'score': 3, 'issue_id': 3851, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '4a502c14d07f6e6c', 'authors': ['Yige Xu', 'Xu Guo', 'Zhiwei Zeng', 'Chunyan Miao'], 'affiliations': ['Alibaba-NTU Global e-Sustainability CorpLab (ANGEL)', 'College of Computing and Data Science, Nanyang Technological University, Singapore', 'Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly', 'KTH Royal Institute of Technology, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2505.11484.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –º—ã—Å–ª–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SoftCoT++, —É–ª—É—á—à–∞—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, SoftCoT++ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—É—Ç–µ–º –≤–æ–∑–º—É—â–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –º—è–≥–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º—ã—Å–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SoftCoT++ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–π SoftCoT –∏ —Ö–æ—Ä–æ—à–æ —Å–æ—á–µ—Ç–∞–µ—Ç—Å—è —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Enhancing Reasoning with Diverse Latent Thoughts', 'desc': 'This paper introduces SoftCoT++, an advanced method for Test-Time Scaling (TTS) that enhances reasoning performance in machine learning models. Unlike traditional TTS methods that work with discrete tokens, SoftCoT++ leverages continuous latent space to improve the quality of reasoning by allowing for diverse exploration of thought paths. By perturbing latent thoughts with specialized initial tokens and using contrastive learning, the method promotes diversity in soft thought representations. Experimental results show that SoftCoT++ significantly outperforms previous methods, demonstrating its effectiveness across various reasoning benchmarks and compatibility with existing scaling techniques.'}, 'zh': {'title': 'Â§öÊ†∑ÂåñÊÄùÁª¥Ë∑ØÂæÑÁöÑÊé¢Á¥¢Êñ∞ÊñπÊ≥ï', 'desc': 'ÊµãËØïÊó∂Êâ©Â±ïÔºàTTSÔºâÊòØ‰∏ÄÁßçÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈÄöËøáÂàÜÈÖçÈ¢ùÂ§ñËÆ°ÁÆóÊù•ÊèêÈ´òÊé®ÁêÜÊÄßËÉΩÁöÑÊñπÊ≥ïÔºåËÄå‰∏çÊîπÂèòÊ®°ÂûãÂèÇÊï∞„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ËøûÁª≠ÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊÄùËÄÉÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊé®ÁêÜÊÄßËÉΩ„ÄÇ‰∏éÁ¶ªÊï£Ëß£Á†Å‰∏çÂêåÔºåËøûÁª≠Á©∫Èó¥‰∏≠ÁöÑÊΩúÂú®Ë°®Á§∫ÊòØÂõ∫ÂÆöÁöÑÔºåËøôÈôêÂà∂‰∫ÜÂ§öÊ†∑ÂåñÁöÑÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSoftCoT++ÔºåÈÄöËøáÊâ∞Âä®ÊΩúÂú®ÊÄùÁª¥Âπ∂Â∫îÁî®ÂØπÊØîÂ≠¶‰π†Êù•‰øÉËøõÊÄùÁª¥Ë∑ØÂæÑÁöÑÂ§öÊ†∑ÊÄßÔºå‰ªéËÄåÊâ©Â±ï‰∫ÜSoftCoTÂú®ÊµãËØïÊó∂Êâ©Â±ïËåÉÂºè‰∏≠ÁöÑÂ∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12872', 'title': 'From Grunts to Grammar: Emergent Language from Cooperative Foraging', 'url': 'https://huggingface.co/papers/2505.12872', 'abstract': 'Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains a challenge. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly.', 'score': 2, 'issue_id': 3853, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'cb62284e65b8c471', 'authors': ['Maytus Piriyajitakonkij', 'Rujikorn Charakorn', 'Weicheng Tao', 'Wei Pan', 'Mingfei Sun', 'Cheston Tan', 'Mengmi Zhang'], 'affiliations': ['Centre for Frontier AI Research (CFAR), ASTAR, Singapore', 'College of Computing and Data Science, Nanyang Technological University, Singapore', 'Department of Computer Science, The University of Manchester, United Kingdom', 'Institute for Infocomm Research (I2R), ASTAR, Singapore', 'Sakana AI, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.12872.jpg', 'data': {'categories': ['#rl', '#rlhf', '#agents', '#reasoning', '#open_source', '#multimodal', '#games'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–≠–≤–æ–ª—é—Ü–∏—è —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é —è–∑—ã–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö –∏–≥—Ä –ø–æ –¥–æ–±—ã—á–µ —Ä–µ—Å—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –º–æ–¥–µ–ª–∏—Ä—É—é—Ç —Å—Ä–µ–¥—É, –æ—Ç—Ä–∞–∂–∞—é—â—É—é –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –∏ —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –ø–æ–≤–ª–∏—è–≤—à–∏–µ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —É —Ä–∞–Ω–Ω–∏—Ö –ª—é–¥–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç—ã —Ä–∞–∑–≤–∏–≤–∞—é—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ–±—â–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞: –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ—Å—Ç—å, –≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è–µ–º–æ—Å—Ç—å, —Å–º–µ—â–µ–Ω–∏–µ, –∫—É–ª—å—Ç—É—Ä–Ω—É—é –ø–µ—Ä–µ–¥–∞—á—É –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è —ç–≤–æ–ª—é—Ü–∏–∏ —è–∑—ã–∫–∞ –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–∞—Å—Ç–∏—á–Ω–æ–π –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏, –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ü–µ–ª–µ–π –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö.'}, 'en': {'title': 'Evolving Language Through Cooperative Learning in Multi-Agent Systems', 'desc': 'This paper explores how language can emerge in multi-agent systems through the lens of Foraging Games, which simulate the ecological and social conditions of early human cooperation. Using end-to-end deep reinforcement learning, agents learn to communicate and coordinate their actions in a shared environment with limited information. The study reveals that these agents develop communication protocols that exhibit characteristics similar to natural language, such as arbitrariness and compositionality. By analyzing how factors like population size and temporal dependencies influence language features, the research provides insights into the evolutionary processes of communication.'}, 'zh': {'title': 'ËØ≠Ë®ÄÁöÑÊºîÂåñÔºö‰ªéÂêà‰ΩúÂà∞Ê≤üÈÄöÁöÑÊóÖÁ®ã', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÂ¶Ç‰ΩïÂú®Â§öÊô∫ËÉΩ‰ΩìËßÖÈ£üÊ∏∏Êàè‰∏≠Âá∫Áé∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊô∫ËÉΩ‰ΩìÂú®ÂÖ±‰∫´ÁöÑÁéØÂ¢É‰∏≠ÔºåÈÄöËøáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Â≠¶‰π†Ë°åÂä®ÂíåÊ≤üÈÄöÁ≠ñÁï•ÔºåÈÄêÊ∏êÂèëÂ±ïÂá∫ÂÖ∑ÊúâËá™ÁÑ∂ËØ≠Ë®ÄÁâπÂæÅÁöÑÊ≤üÈÄöÂçèËÆÆ„ÄÇËÆ∫ÊñáÈáèÂåñ‰∫ÜËØ≠Ë®ÄÁöÑ‰∏çÂêåÁâπÊÄßÔºåÂ¶Ç‰ªªÊÑèÊÄß„ÄÅÂèØ‰∫íÊç¢ÊÄßÂíåÊñáÂåñ‰º†ÈÄíÁ≠âÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∫∫Âè£ËßÑÊ®°ÂíåÊó∂Èó¥‰æùËµñÊÄßÁ≠âÂõ†Á¥†Â¶Ç‰ΩïÂΩ±ÂìçËØ≠Ë®ÄÁöÑÊºîÂèò„ÄÇËØ•Ê°ÜÊû∂‰∏∫Á†îÁ©∂ËØ≠Ë®ÄÂ¶Ç‰ΩïÂú®Âêà‰ΩúÁõÆÊ†áÂíåÈÉ®ÂàÜÂèØËßÇÂØüÊÄß‰∏≠ÊºîÂåñÊèê‰æõ‰∫ÜÂπ≥Âè∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11733', 'title': 'MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports', 'url': 'https://huggingface.co/papers/2505.11733', 'abstract': 'Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.', 'score': 2, 'issue_id': 3864, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '0eaea70de66597a0', 'authors': ['Kevin Wu', 'Eric Wu', 'Rahul Thapa', 'Kevin Wei', 'Angela Zhang', 'Arvind Suresh', 'Jacqueline J. Tao', 'Min Woo Sun', 'Alejandro Lozano', 'James Zou'], 'affiliations': ['Stanford University', 'University of California, San Francisco', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.11733.jpg', 'data': {'categories': ['#open_source', '#dataset', '#training', '#benchmark', '#alignment', '#data', '#healthcare', '#reasoning'], 'emoji': 'ü©∫', 'ru': {'title': '–ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç MedCaseReasoning –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –≤—Ä–∞—á–µ–π –ø—Ä–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –¥–∏–∞–≥–Ω–æ–∑–æ–≤. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 14 —Ç—ã—Å—è—á –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É—á–∞–µ–≤ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–∞—Ö. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –Ω–∞ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –≤ –∏—Ö –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –û–¥–Ω–∞–∫–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö MedCaseReasoning –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ —Ç–æ—á–Ω–æ—Å—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ø–æ–ª–Ω–æ—Ç—É –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Medical Diagnosis with Reasoning Evaluation', 'desc': 'This paper addresses the limitations of current medical benchmarks that only evaluate the final accuracy of Large Language Models (LLMs) in clinical diagnosis. It introduces MedCaseReasoning, a new dataset designed to assess both the accuracy of diagnoses and the quality of reasoning behind them, featuring 14,489 cases with clinician-authored explanations. The authors evaluate existing LLMs and find that they struggle with both diagnostic accuracy and recalling reasoning statements. By fine-tuning these models on the new dataset, they achieve significant improvements in both diagnostic accuracy and reasoning recall, demonstrating the importance of evaluating the reasoning process in medical AI applications.'}, 'zh': {'title': 'ÊèêÂçáÂåªÂ≠¶ËØäÊñ≠ÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜMedCaseReasoningÊï∞ÊçÆÈõÜÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂåªÂ≠¶ËØäÊñ≠‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÂåªÂ≠¶Âü∫ÂáÜ‰∏çÂêåÔºåËØ•Êï∞ÊçÆÈõÜ‰∏ç‰ªÖÂÖ≥Ê≥®ÊúÄÁªàÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÈáçËßÜ‰∏¥Â∫äÊé®ÁêÜËøáÁ®ãÁöÑË¥®ÈáèÂíåÂèØ‰ø°Â∫¶„ÄÇÊàë‰ª¨ÂèëÁé∞ÂΩìÂâçÊúÄÂÖàËøõÁöÑLLMsÂú®ËØäÊñ≠ÂíåÊé®ÁêÜÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÂõûÂøÜÁéá‰∏ä„ÄÇÈÄöËøáÂØπLLMsËøõË°åÂæÆË∞ÉÔºå‰ΩøÁî®MedCaseReasoningÁöÑÊï∞ÊçÆÔºåËØäÊñ≠ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÂõûÂøÜÁéáÂàÜÂà´ÊèêÈ´ò‰∫Ü29%Âíå41%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11475', 'title': 'HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages', 'url': 'https://huggingface.co/papers/2505.11475', 'abstract': 'Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference', 'score': 2, 'issue_id': 3854, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '82282826ff02b787', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Hoo-Chang Shin', 'Felipe Soares', 'Alexander Bukharin', 'Ellie Evans', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.11475.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#multilingual', '#open_source', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': 'HelpSteer3-Preference: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RLHF', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HelpSteer3-Preference - –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF). –ù–∞–±–æ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 40 000 –æ–±—Ä–∞–∑—Ü–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –≤–∫–ª—é—á–∞—è –∑–∞–¥–∞—á–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ STEM, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –ò—Å–ø–æ–ª—å–∑—É—è HelpSteer3-Preference, –∞–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (Reward Models), –¥–æ—Å—Ç–∏–≥—à–∏–µ –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö RM-Bench –∏ JudgeBench. –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–ª–∏—Ç–∏–∫ —Å –ø–æ–º–æ—â—å—é RLHF.'}, 'en': {'title': 'Enhancing Language Models with High-Quality Preference Data', 'desc': 'This paper presents HelpSteer3-Preference, a new dataset designed to improve the training of language models using Reinforcement Learning from Human Feedback (RLHF). It contains over 40,000 high-quality, human-annotated preference samples that cover a wide range of real-world applications, including STEM and coding tasks. The dataset has been shown to significantly enhance the performance of Reward Models (RMs), achieving top scores on benchmark tests. Additionally, the paper discusses how this dataset can be utilized to train Generative RMs and align policy models with RLHF techniques.'}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÅèÂ•ΩÊï∞ÊçÆÈõÜ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜHelpSteer3-PreferenceÔºåËøôÊòØ‰∏Ä‰∏™È´òË¥®ÈáèÁöÑ‰∫∫Á±ªÊ†áÊ≥®ÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ë∂ÖËøá40,000‰∏™Ê†∑Êú¨ÔºåÊó®Âú®ÊèêÂçáÈÄöÁî®È¢ÜÂüüÁöÑËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ„ÄÇËØ•Êï∞ÊçÆÈõÜÊ∂µÁõñ‰∫ÜÂ§öÁßçÁúüÂÆû‰∏ñÁïåÂ∫îÁî®ÔºåÂåÖÊã¨STEM„ÄÅÁºñÁ®ãÂíåÂ§öËØ≠Ë®ÄÂú∫ÊôØÔºåÂÖ∑ÊúâÂ§öÊ†∑ÊÄßÂíåÈ´òË¥®Èáè„ÄÇÈÄöËøá‰ΩøÁî®HelpSteer3-PreferenceÔºåÊàë‰ª¨ËÆ≠ÁªÉÁöÑÂ•ñÂä±Ê®°ÂûãÂú®RM-BenchÂíåJudgeBench‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑË°®Áé∞ÔºåÊòæËëóÊèêÈ´ò‰∫Ü‰πãÂâçÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ËØ•Êï∞ÊçÆÈõÜËÆ≠ÁªÉÁîüÊàêÊÄßÂ•ñÂä±Ê®°ÂûãÔºåÂπ∂Â∞ÜÁ≠ñÁï•Ê®°Âûã‰∏é‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÂØπÈΩê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12781', 'title': 'A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation\n  through Low-Rank Clone', 'url': 'https://huggingface.co/papers/2505.12781', 'abstract': 'Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.', 'score': 1, 'issue_id': 3860, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '1ba09e10a601b724', 'authors': ['Jitai Hao', 'Qiang Huang', 'Hao Liu', 'Xinyan Xiao', 'Zhaochun Ren', 'Jun Yu'], 'affiliations': ['Baidu Inc.', 'Harbin Institute of Technology, Shenzhen', 'Leiden University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12781.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#training', '#small_models'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Low-Rank Clone (LRC). LRC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –≤–µ—Å–æ–≤ —É—á–∏—Ç–µ–ª—è –∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –≤–∫–ª—é—á–∞—è —Å–∏–≥–Ω–∞–ª—ã –∏–∑ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Å —Å–∏–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏-—É—á–∏—Ç–µ–ª—è–º–∏, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è –ø–µ—Ä–µ–¥–∞—á—É –∑–Ω–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LRC –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ —á–µ–º –≤ 1000 —Ä–∞–∑.'}, 'en': {'title': 'Efficient Training of Small Language Models with Low-Rank Clone', 'desc': 'This paper presents a new method called Low-Rank Clone (LRC) for training Small Language Models (SLMs) efficiently. LRC addresses three main challenges in model training: reducing information loss from hard pruning, improving the alignment of representations, and better utilizing activations from Feed-Forward Networks. By using low-rank projection matrices, LRC allows for soft pruning and aligns student activations with those of a teacher model, enhancing knowledge transfer. The results show that LRC achieves high performance comparable to larger models while significantly reducing the amount of training data needed.'}, 'zh': {'title': '‰ΩéÁß©ÂÖãÈöÜÔºöÈ´òÊïàËÆ≠ÁªÉÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰ΩéÁß©ÂÖãÈöÜÔºàLow-Rank Clone, LRCÔºâÁöÑÈ´òÊïàÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ËÆ≠ÁªÉÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÊó∂Èù¢‰∏¥ÁöÑ‰ø°ÊÅØÊçüÂ§±„ÄÅË°®Á§∫ÂØπÈΩêÊïàÁéá‰ΩéÂíåÊøÄÊ¥ªÂà©Áî®‰∏çË∂≥Á≠âÊåëÊàò„ÄÇLRCÈÄöËøáÊûÑÂª∫‰ΩéÁß©ÊäïÂΩ±Áü©ÈòµÔºåÂÆûÁé∞‰∫ÜÊïôÂ∏àÊ®°ÂûãÊùÉÈáçÁöÑËΩØ‰øÆÂâ™ÂíåÂ≠¶ÁîüÊ®°ÂûãÊøÄÊ¥ªÁöÑÂØπÈΩêÔºåÁâπÂà´ÊòØÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÁöÑ‰ø°Âè∑„ÄÇËØ•ÊñπÊ≥ïÊúÄÂ§ßÂåñ‰∫ÜÁü•ËØÜËΩ¨ÁßªÁöÑÊïàÁéáÔºåÊ∂àÈô§‰∫ÜÂØπÊòæÂºèÂØπÈΩêÊ®°ÂùóÁöÑÈúÄÊ±Ç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLRCÂú®‰ΩøÁî®‰ªÖ20BÊ†áËÆ∞ÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂåπÈÖçÊàñË∂ÖË∂äÂú®‰∏á‰∫øÊ†áËÆ∞‰∏äËÆ≠ÁªÉÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåËææÂà∞‰∫ÜË∂ÖËøá1000ÂÄçÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12257', 'title': 'LLM Context Conditioning and PWP Prompting for Multimodal Validation of\n  Chemical Formulas', 'url': 'https://huggingface.co/papers/2505.12257', 'abstract': "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.", 'score': 1, 'issue_id': 3849, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': '74901d316cc1d6cb', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2505.12257.jpg', 'data': {'categories': ['#science', '#multimodal', '#interpretability', '#inference'], 'emoji': 'üîç', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ LLM –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤—ã—è–≤–ª—è—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (PWP) –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö LLM –¥–ª—è –∑–∞–¥–∞—á —Ç–æ—á–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π PWP-–ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∏–ª –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ—à–∏–±–æ–∫ –∏ –¥–∞–∂–µ –ø–æ–∑–≤–æ–ª–∏–ª –º–æ–¥–µ–ª–∏ Gemini 2.5 Pro –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —Å–∫—Ä—ã—Ç—É—é –æ—à–∏–±–∫—É –≤ —Ñ–æ—Ä–º—É–ª–µ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ –Ω–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.'}, 'en': {'title': 'Enhancing LLM Accuracy in Scientific Error Detection with PWP', 'desc': 'This paper explores how to improve Large Language Models (LLMs) in identifying subtle errors in complex scientific documents, especially those with images and formulas. It introduces a method called Persistent Workflow Prompting (PWP) to better condition LLMs during their analysis, enhancing their ability to detect inaccuracies. The study specifically tests this approach on Gemini 2.5 Pro and ChatGPT Plus o3, showing that PWP can significantly improve error detection compared to basic prompting strategies. The findings suggest that this method could lead to more reliable LLMs for validating technical content, although further research is needed to confirm its effectiveness across different contexts.'}, 'zh': {'title': 'ÊèêÂçáLLMÂú®ÁßëÂ≠¶ÊñáÊ°£‰∏≠ÁöÑÈîôËØØËØÜÂà´ËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ÁªìÊûÑÂåñÁöÑ‰∏ä‰∏ãÊñáÊù°‰ª∂Êù•ÊîπÂñÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÁßëÂ≠¶ÂíåÊäÄÊúØÊñáÊ°£‰∏≠ÁöÑÈîôËØØËØÜÂà´ËÉΩÂäõ„ÄÇÁ†îÁ©∂ÈááÁî®‰∫ÜÊåÅ‰πÖÂ∑•‰ΩúÊèêÁ§∫ÔºàPWPÔºâÂéüÂàôÔºåÊó®Âú®ÊèêÈ´òLLMsÂú®Êé®ÁêÜÊó∂ÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®È™åËØÅÂåñÂ≠¶ÂÖ¨ÂºèÊó∂„ÄÇÈÄöËøáÂØπ‰∏çÂêåÊèêÁ§∫Á≠ñÁï•ÁöÑËØÑ‰º∞ÔºåÂèëÁé∞ÈÄÇÂ∫îPWPÁªìÊûÑÁöÑÊèêÁ§∫ËÉΩÂ§üÊúâÊïàÊèêÈ´òÊñáÊú¨ÈîôËØØÁöÑËØÜÂà´ÁéáÔºåÂπ∂ÊàêÂäüÂèëÁé∞‰∫Ü‰πãÂâçÊú™Ë¢´ÊâãÂä®ÂÆ°Êü•ËØÜÂà´ÁöÑÂõæÂÉèÂÖ¨ÂºèÈîôËØØ„ÄÇËØ•ÊñπÊ≥ï‰∏∫ÂºÄÂèëÊõ¥Âº∫Â§ßÁöÑLLMÈ©±Âä®ÂàÜÊûêÂ∑•‰ΩúÊµÅÊèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÊäÄÊúØÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÁªÜËá¥ÈîôËØØÊ£ÄÊµãÁöÑ‰ªªÂä°‰∏≠„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11988', 'title': 'TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text', 'url': 'https://huggingface.co/papers/2505.11988', 'abstract': 'Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.', 'score': 1, 'issue_id': 3850, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '95b404534e69c826', 'authors': ['Ahmed Lekssays', 'Utsav Shukla', 'Husrev Taha Sencar', 'Md Rizwan Parvez'], 'affiliations': ['Independent Researcher', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2505.11988.jpg', 'data': {'categories': ['#security', '#data', '#hallucinations', '#rag', '#benchmark'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–¢–æ—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫ –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–æ–≤ —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö', 'desc': 'TechniqueRAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ—Ö–Ω–∏–∫ –ø—Ä–æ—Ç–∏–≤–Ω–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–∞—Ö –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤. TechniqueRAG –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.'}, 'en': {'title': 'Enhancing Cyber Defense with TechniqueRAG: Precision without Excessive Resources', 'desc': 'This paper introduces TechniqueRAG, a novel framework designed to improve the identification of adversarial techniques in cybersecurity texts. It combines retrieval-augmented generation (RAG) with instruction-tuned large language models (LLMs) to enhance domain specificity while minimizing the need for extensive labeled datasets. By fine-tuning only the generation component and employing zero-shot LLM re-ranking, TechniqueRAG improves the quality of retrieved candidates, ensuring they are more relevant to the specific domain of adversarial techniques. The results show that TechniqueRAG outperforms existing methods, achieving high accuracy without the heavy resource demands typically associated with task-specific optimizations.'}, 'zh': {'title': 'ÊèêÂçáÂÆâÂÖ®ÊñáÊú¨ÂØπÊäóÊäÄÊúØËØÜÂà´ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫TechniqueRAGÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂØπÂÆâÂÖ®ÊñáÊú¨‰∏≠ÂØπÊäóÊÄßÊäÄÊúØÁöÑËØÜÂà´ËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÁé∞ÊàêÁöÑÊ£ÄÁ¥¢Âô®„ÄÅÁªèËøáÊåá‰ª§Ë∞É‰ºòÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåÊúÄÂ∞èÁöÑÊñáÊú¨-ÊäÄÊúØÂØπÔºåËß£ÂÜ≥‰∫ÜÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøá‰ªÖÂØπÁîüÊàêÁªÑ‰ª∂ËøõË°åÂæÆË∞ÉÔºåTechniqueRAGÈÅøÂÖç‰∫ÜÂØπËµÑÊ∫êÂØÜÈõÜÂûãÊ£ÄÁ¥¢ËÆ≠ÁªÉÁöÑ‰æùËµñÔºåÂêåÊó∂ÈÄöËøáÈõ∂-shot LLMÈáçÊéíÂ∫èÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢Ë¥®ÈáèÂíåÈ¢ÜÂüüÁâπÂºÇÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTechniqueRAGÂú®Â§ö‰∏™ÂÆâÂÖ®Âü∫ÂáÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊó†ÈúÄÂ§ßÈáèÁâπÂÆö‰ªªÂä°ÁöÑ‰ºòÂåñÊàñÊ†áËÆ∞Êï∞ÊçÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10420', 'title': 'Learned Lightweight Smartphone ISP with Unpaired Data', 'url': 'https://huggingface.co/papers/2505.10420', 'abstract': 'The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .', 'score': 1, 'issue_id': 3855, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '0ba36884ee806c6f', 'authors': ['Andrei Arhire', 'Radu Timofte'], 'affiliations': ['Computer Vision Lab, CAIDAS & IFI University of Wurzburg', 'Faculty of Computer Science Alexandru Ioan Cuza University of Iasi'], 'pdf_title_img': 'assets/pdf/title_img/2505.10420.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': 'üì±', 'ru': {'title': '–£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è Image Signal Processor (ISP) –¥–ª—è —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞–º–∏ –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ü–≤–µ—Ç—É –∏ —Ç–µ–∫—Å—Ç—É—Ä–µ. –ú–µ—Ç–æ–¥ –±—ã–ª –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö Zurich RAW to RGB –∏ Fujifilm UltraISP —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–µ–≥–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Revolutionizing Smartphone Imaging with Unpaired Learning!', 'desc': 'This paper presents a new method for training an Image Signal Processor (ISP) using unpaired data, which means it does not require exact matches between raw images and high-quality reference images. The authors utilize a multi-term loss function and adversarial training with multiple discriminators to effectively learn color and texture characteristics while preserving content structure. Their approach is designed for lightweight neural networks, making it suitable for mobile devices. The results demonstrate that this unpaired learning strategy outperforms traditional paired methods in terms of image quality and fidelity.'}, 'zh': {'title': 'Êó†ÈÖçÂØπÊï∞ÊçÆÁöÑÂ≠¶‰π†ÂûãÂõæÂÉè‰ø°Âè∑Â§ÑÁêÜÂô®', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁî®‰∫éÂ≠¶‰π†ÂõæÂÉè‰ø°Âè∑Â§ÑÁêÜÂô®ÔºàISPÔºâÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÊñπÊ≥ï‰∏≠ÈúÄË¶ÅÊàêÂØπÁöÑÂéüÂßãÂõæÂÉèÂíåÈ´òË¥®ÈáèÂèÇËÄÉÂõæÂÉèÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈááÁî®Êó†ÈÖçÂØπÁöÑÂ≠¶‰π†Á≠ñÁï•ÔºåÈÄöËøáÂØπÊäóËÆ≠ÁªÉÂíåÂ§öÈáçÂà§Âà´Âô®Êù•‰øùÊåÅÂÜÖÂÆπÁªìÊûÑÔºåÂêåÊó∂Â≠¶‰π†ÁõÆÊ†áRGBÊï∞ÊçÆÈõÜÁöÑÈ¢úËâ≤ÂíåÁ∫πÁêÜÁâπÂæÅ„ÄÇ‰ΩøÁî®ÈÄÇÂêàÁßªÂä®ËÆæÂ§áÁöÑËΩªÈáèÁ∫ßÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÔºåÊàë‰ª¨Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÔºåÁªìÊûúÊòæÁ§∫Âá∫‰∏éÊàêÂØπËÆ≠ÁªÉÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑ‰øùÁúüÂ∫¶ÂíåÊΩúÂäõ„ÄÇ‰ª£Á†ÅÂíåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂèØÂú®ÊåáÂÆöÈìæÊé•Ëé∑Âèñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03332', 'title': 'AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning', 'url': 'https://huggingface.co/papers/2505.03332', 'abstract': 'Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.', 'score': 1, 'issue_id': 3849, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '9c52936c2b9a7443', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2505.03332.jpg', 'data': {'categories': ['#reasoning', '#data', '#science', '#multimodal', '#interpretability', '#architecture'], 'emoji': 'üî¨', 'ru': {'title': 'PWP: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç —Å –ø–æ–º–æ—â—å—é LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Persistent Workflow Prompting (PWP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Ä—É–∫–æ–ø–∏—Å–µ–π. PWP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∞–Ω–∞–ª–∏–∑–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏. –ú–µ—Ç–æ–¥ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Å –ø–æ–º–æ—â—å—é –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–∞-–ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –∏ –º–µ—Ç–∞-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏—è. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PWP-—É–ø—Ä–∞–≤–ª—è–µ–º–∞—è LLM —Å–ø–æ—Å–æ–±–Ω–∞ –≤—ã—è–≤–ª—è—Ç—å —Å–µ—Ä—å–µ–∑–Ω—ã–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏–∑–∞.'}, 'en': {'title': 'Empowering LLMs for Expert Scientific Review with Persistent Workflow Prompting', 'desc': 'This paper addresses the challenges faced by Large Language Models (LLMs) in performing critical peer reviews of scientific manuscripts, particularly due to data limitations and the intricacies of expert reasoning. It introduces a new methodology called Persistent Workflow Prompting (PWP), which allows users to create structured prompts that guide LLMs through detailed analysis workflows without needing coding skills. The PWP framework is designed to help LLMs systematically evaluate scientific content by integrating various forms of data, such as text and images, to identify flaws and assess claims. The authors demonstrate the effectiveness of PWP in analyzing experimental chemistry manuscripts, showcasing its ability to enhance LLM performance in complex scientific evaluations.'}, 'zh': {'title': 'ÊåÅ‰πÖÂ∑•‰ΩúÊµÅÊèêÁ§∫ÔºöÊèêÂçáÁßëÂ≠¶ËØÑÂÆ°ÁöÑÊô∫ËÉΩÂåñ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊèêÁ§∫Â∑•Á®ãÊñπÊ≥ïÔºåÁß∞‰∏∫ÊåÅ‰πÖÂ∑•‰ΩúÊµÅÊèêÁ§∫ÔºàPWPÔºâÔºåÊó®Âú®Â∏ÆÂä©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°åÁßëÂ≠¶ÊâãÁ®øÁöÑÊâπÂà§ÊÄßÂêåË°åËØÑÂÆ°„ÄÇPWPÈÄöËøáÊ†áÂáÜÁöÑLLMËÅäÂ§©ÁïåÈù¢Ôºå‰ΩøÁî®ÂàÜÂ±ÇÊ®°ÂùóÂåñÁöÑÊû∂ÊûÑÔºåÂÆö‰πâËØ¶ÁªÜÁöÑÂàÜÊûêÂ∑•‰ΩúÊµÅÁ®ãÔºåËÉΩÂ§üÁ≥ªÁªüÂåñÂú∞ÁºñÁ†Å‰∏ìÂÆ∂ËØÑÂÆ°ÁöÑÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÈÄöËøáËø≠‰ª£ÁöÑÂÖÉÊèêÁ§∫ÊäÄÊúØÂíåÂÖÉÊé®ÁêÜÔºåPWPËÉΩÂ§üÂºïÂØºLLMËøõË°åÂ§öÊ®°ÊÄÅËØÑ‰º∞ÔºåËØÜÂà´ÂÆûÈ™åÂåñÂ≠¶ÊâãÁ®ø‰∏≠ÁöÑ‰∏ªË¶ÅÊñπÊ≥ïËÆ∫Áº∫Èô∑„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰ªÖÊèê‰æõ‰∫ÜÂÖ∑‰ΩìÂ∫îÁî®ÁöÑÁ§∫‰æãÔºåËøòÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Áé∞ÊúâÁöÑLLMËøõË°åÂ§çÊùÇÁßëÂ≠¶‰ªªÂä°ÁöÑÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12973', 'title': 'Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models', 'url': 'https://huggingface.co/papers/2505.12973', 'abstract': 'Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.', 'score': 0, 'issue_id': 3854, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'e144d477e8eb1cab', 'authors': ['Mahta Fetrat Qharabagh', 'Zahra Dehghanian', 'Hamid R. Rabiee'], 'affiliations': ['Dep. of Computer Engineering, Sharif University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.12973.jpg', 'data': {'categories': ['#data', '#healthcare', '#low_resource', '#dataset'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ–º–æ–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è G2P –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ –≤ –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –æ–º–æ–≥—Ä–∞—Ñ–æ–≤ –≤ –∑–∞–¥–∞—á–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥—Ä–∞—Ñ–µ–º –≤ —Ñ–æ–Ω–µ–º—ã (G2P) –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –æ–º–æ–≥—Ä–∞—Ñ–∞–º–∏ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –¥–∞—Ç–∞—Å–µ—Ç HomoRich. –û–Ω–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞, –ø—Ä–∏–º–µ–Ω—è—è –µ–≥–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã G2P –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä—Å–∏–¥—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –±—ã—Å—Ç—Ä—É—é –≤–µ—Ä—Å–∏—é –∏–∑–≤–µ—Å—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã eSpeak —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–º–æ–≥—Ä–∞—Ñ–æ–≤ - HomoFast eSpeak, –ø–æ–∫–∞–∑—ã–≤–∞—è —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –æ–º–æ–≥—Ä–∞—Ñ–æ–≤ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 30%.'}, 'en': {'title': 'Enhancing G2P with Efficient Homograph Disambiguation', 'desc': 'This paper tackles the problem of homograph disambiguation in grapheme-to-phoneme (G2P) conversion, particularly for low-resource languages. It introduces a semi-automated method to create homograph datasets, exemplified by the new HomoRich dataset, which enhances a deep learning G2P system for Persian. Additionally, the authors propose a shift towards using comprehensive offline datasets to develop efficient, rule-based G2P methods that are suitable for real-time applications. The improved eSpeak system, named HomoFast eSpeak, demonstrates a significant increase in disambiguation accuracy, making it more effective for accessibility tools.'}, 'zh': {'title': 'ÊèêÂçáÂêåÂΩ¢ÂºÇ‰πâËØçÊ∂àÊ≠ßÁöÑÊô∫ËÉΩËß£ÂÜ≥ÊñπÊ°à', 'desc': 'ÂêåÂΩ¢ÂºÇ‰πâËØçÊ∂àÊ≠ßÂú®ÂõæÂΩ¢Âà∞Èü≥Á¥†ÔºàG2PÔºâËΩ¨Êç¢‰∏≠‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂåÆ‰πèÁöÑËØ≠Ë®Ä‰∏≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂçäËá™Âä®ÂåñÁöÑÊµÅÁ®ãÊù•ÊûÑÂª∫ÂêåÂΩ¢ÂºÇ‰πâËØçÊï∞ÊçÆÈõÜÔºåÂπ∂ÁîüÊàê‰∫ÜHomoRichÊï∞ÊçÆÈõÜÔºå‰ª•Â¢ûÂº∫Ê≥¢ÊñØËØ≠ÁöÑÊ∑±Â∫¶Â≠¶‰π†G2PÁ≥ªÁªü„ÄÇÊàë‰ª¨ËøòÂÄ°ÂØºÂà©Áî®‰∏∞ÂØåÁöÑÁ¶ªÁ∫øÊï∞ÊçÆÈõÜÊù•ÂºÄÂèëÈÄÇÂêàÂÆûÊó∂Â∫îÁî®ÁöÑÂø´ÈÄüËßÑÂàôÂü∫Á°ÄÊñπÊ≥ï„ÄÇÈÄöËøáÊîπËøõËëóÂêçÁöÑËßÑÂàôÂü∫Á°ÄG2PÁ≥ªÁªüeSpeakÔºåÊàë‰ª¨ÁöÑHomoFast eSpeakÁâàÊú¨Âú®ÂêåÂΩ¢ÂºÇ‰πâËØçÊ∂àÊ≠ßÂáÜÁ°ÆÊÄß‰∏äÊèêÈ´ò‰∫ÜÁ∫¶30%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10831', 'title': 'Creating General User Models from Computer Use', 'url': 'https://huggingface.co/papers/2505.10831', 'abstract': "Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.", 'score': 0, 'issue_id': 3858, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'b70f4a83c98a9326', 'authors': ['Omar Shaikh', 'Shardul Sapkota', 'Shan Rizvi', 'Eric Horvitz', 'Joon Sung Park', 'Diyi Yang', 'Michael S. Bernstein'], 'affiliations': ['Independent', 'Microsoft Research', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10831.jpg', 'data': {'categories': ['#interpretability', '#agents', '#multimodal', '#agi', '#architecture', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'GUM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –±—É–¥—É—â–µ–≥–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ–±—â–µ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (GUM), –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–∞, –Ω–∞–±–ª—é–¥–∞—è –∑–∞ –µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º. GUM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –∑–Ω–∞–Ω–∏—è—Ö –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –≠—Ç–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∫ —Å–≤–∞–¥—å–±–µ –∏–ª–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è—Ö –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –Ω–∞–¥ —Ç–µ–∫—Å—Ç–æ–º. GUM –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —É–º–Ω—ã–µ —á–∞—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã, —É–ø—Ä–∞–≤–ª—è—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è–º–∏ –û–° –∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∞–¥–∞–ø—Ç–∏—Ä—É—é—â–∏—Ö—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Empowering Technology to Anticipate User Needs', 'desc': 'This paper introduces a General User Model (GUM) that learns about users by observing their interactions with computers, using unstructured data like device screenshots. GUMs create confidence-weighted propositions to capture user preferences and behaviors, allowing for flexible reasoning about user needs. The architecture can infer context from multimodal observations, enabling proactive assistants that can suggest actions without explicit user requests. Overall, GUMs enhance human-computer interaction by providing a more comprehensive understanding of user behavior and preferences across different applications.'}, 'zh': {'title': 'ÈÄöÁî®Áî®Êà∑Ê®°ÂûãÔºöÊô∫ËÉΩ‰∫∫Êú∫‰∫§‰∫íÁöÑÊñ∞Êú™Êù•', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®Áî®Êà∑Ê®°ÂûãÔºàGUMÔºâÔºåÊó®Âú®ÈÄöËøáËßÇÂØüÁî®Êà∑‰∏éËÆ°ÁÆóÊú∫ÁöÑ‰∫íÂä®Êù•Â≠¶‰π†Áî®Êà∑ÁöÑÂÅèÂ•ΩÂíå‰π†ÊÉØ„ÄÇGUMËÉΩÂ§üÂ§ÑÁêÜÈùûÁªìÊûÑÂåñÁöÑÁî®Êà∑ËßÇÂØüÊï∞ÊçÆÔºàÂ¶ÇËÆæÂ§áÊà™ÂõæÔºâÔºåÂπ∂ÊûÑÂª∫Âá∫ÂèçÊò†Áî®Êà∑Áü•ËØÜÂíåÂÅèÂ•ΩÁöÑÁΩÆ‰ø°Âä†ÊùÉÂëΩÈ¢ò„ÄÇÈÄöËøáÂ§öÊ®°ÊÄÅËßÇÂØüÔºåGUMÂèØ‰ª•Êé®Êñ≠Áî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÂπ∂Ê†πÊçÆ‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÂëΩÈ¢òÔºåÊåÅÁª≠‰øÆÊ≠£Â∑≤ÊúâÂëΩÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂü∫‰∫éGUMÁöÑÂä©ÊâãËÉΩÂ§ü‰∏ªÂä®ËØÜÂà´Áî®Êà∑ÈúÄÊ±ÇÂπ∂ÊâßË°åÊúâÁî®ÁöÑÂª∫ËÆÆÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09388', 'title': 'Qwen3 Technical Report', 'url': 'https://huggingface.co/papers/2505.09388', 'abstract': 'In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.', 'score': 108, 'issue_id': 3823, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '69a0f87bb5460e8d', 'authors': ['An Yang', 'Anfeng Li', 'Baosong Yang', 'Beichen Zhang', 'Binyuan Hui', 'Bo Zheng', 'Bowen Yu', 'Chang Gao', 'Chengen Huang', 'Chenxu Lv', 'Chujie Zheng', 'Dayiheng Liu', 'Fan Zhou', 'Fei Huang', 'Feng Hu', 'Hao Ge', 'Haoran Wei', 'Huan Lin', 'Jialong Tang', 'Jian Yang', 'Jianhong Tu', 'Jianwei Zhang', 'Jianxin Yang', 'Jiaxi Yang', 'Jing Zhou', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Keqin Bao', 'Kexin Yang', 'Le Yu', 'Lianghao Deng', 'Mei Li', 'Mingfeng Xue', 'Mingze Li', 'Pei Zhang', 'Peng Wang', 'Qin Zhu', 'Rui Men', 'Ruize Gao', 'Shixuan Liu', 'Shuang Luo', 'Tianhao Li', 'Tianyi Tang', 'Wenbiao Yin', 'Xingzhang Ren', 'Xinyu Wang', 'Xinyu Zhang', 'Xuancheng Ren', 'Yang Fan', 'Yang Su', 'Yichang Zhang', 'Yinger Zhang', 'Yu Wan', 'Yuqiong Liu', 'Zekun Wang', 'Zeyu Cui', 'Zhenru Zhang', 'Zhipeng Zhou', 'Zihan Qiu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.09388.jpg', 'data': {'categories': ['#low_resource', '#agi', '#reasoning', '#multilingual', '#benchmark', '#architecture', '#training', '#open_source'], 'emoji': 'üß†', 'ru': {'title': 'Qwen3: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º—ã—à–ª–µ–Ω–∏—è –∏ –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤', 'desc': 'Qwen3 - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π Qwen3 —è–≤–ª—è–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ –º—ã—à–ª–µ–Ω–∏—è –∏ —Ä–µ–∂–∏–º–∞ –±–µ–∑ –º—ã—à–ª–µ–Ω–∏—è –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –Ω–∏–º–∏. –ú–æ–¥–µ–ª—å –≤–≤–æ–¥–∏—Ç –º–µ—Ö–∞–Ω–∏–∑–º –±—é–¥–∂–µ—Ç–∞ –º—ã—à–ª–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. Qwen3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 119 —è–∑—ã–∫–æ–≤ –∏ –¥–∏–∞–ª–µ–∫—Ç–æ–≤.'}, 'en': {'title': 'Qwen3: Unifying Thinking and Efficiency in Language Models', 'desc': 'Qwen3 is the latest version of the Qwen model family, featuring large language models that enhance performance, efficiency, and multilingual capabilities. It includes both dense and Mixture-of-Expert (MoE) architectures with a wide range of parameters, from 0.6 to 235 billion. A notable innovation is the integration of thinking and non-thinking modes, allowing for seamless dynamic switching based on user needs, which improves response times and reasoning capabilities. Additionally, Qwen3 supports 119 languages, significantly increasing its accessibility and effectiveness in diverse applications, while also providing a thinking budget mechanism for optimized resource allocation during inference.'}, 'zh': {'title': 'Qwen3ÔºöÁªü‰∏ÄÊÄùÁª¥‰∏éÂìçÂ∫îÁöÑÊô∫ËÉΩËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜQwen3ÔºåËøôÊòØQwenÊ®°ÂûãÁ≥ªÂàóÁöÑÊúÄÊñ∞ÁâàÊú¨„ÄÇQwen3ÂåÖÂê´‰∏ÄÁ≥ªÂàóÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÊÄßËÉΩ„ÄÅÊïàÁéáÂíåÂ§öËØ≠Ë®ÄËÉΩÂäõ„ÄÇÂÖ∂ÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂ∞ÜÊÄùÁª¥Ê®°ÂºèÂíåÈùûÊÄùÁª¥Ê®°ÂºèÊï¥ÂêàÂà∞‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂‰∏≠ÔºåÂÆûÁé∞Âä®ÊÄÅÊ®°ÂºèÂàáÊç¢ÔºåÈÄÇÂ∫îÁî®Êà∑Êü•ËØ¢„ÄÇQwen3ËøòÂºïÂÖ•‰∫ÜÊÄùÁª¥È¢ÑÁÆóÊú∫Âà∂ÔºåÂÖÅËÆ∏Áî®Êà∑Âú®Êé®ÁêÜËøáÁ®ã‰∏≠Ëá™ÈÄÇÂ∫îÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫êÔºå‰ªéËÄåÂú®‰ªªÂä°Â§çÊùÇÊÄßÂü∫Á°Ä‰∏äÂπ≥Ë°°Âª∂ËøüÂíåÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11049', 'title': 'GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning', 'url': 'https://huggingface.co/papers/2505.11049', 'abstract': "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/", 'score': 41, 'issue_id': 3829, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'bedca054f1392a71', 'authors': ['Yue Liu', 'Shengfang Zhai', 'Mingzhe Du', 'Yulin Chen', 'Tri Cao', 'Hongcheng Gao', 'Cheng Wang', 'Xinfeng Li', 'Kun Wang', 'Junfeng Fang', 'Jiaheng Zhang', 'Bryan Hooi'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.11049.jpg', 'data': {'categories': ['#multimodal', '#rl', '#dataset', '#reasoning', '#open_source', '#training'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–∞—é—â–∏–π —Å—Ç—Ä–∞–∂ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GuardReasoner-VL - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å-–æ—Ö—Ä–∞–Ω–Ω–∏–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM), –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ GuardReasoner-VLTrain, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 123 —Ç—ã—Å—è—á–∏ –æ–±—Ä–∞–∑—Ü–æ–≤ —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –æ—Ç–±–æ—Ä –æ–±—Ä–∞–∑—Ü–æ–≤, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GuardReasoner-VL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏, —É–ª—É—á—à–∞—è F1-–º–µ—Ä—É –Ω–∞ 19.27% –≤ —Å—Ä–µ–¥–Ω–µ–º.'}, 'en': {'title': 'GuardReasoner-VL: Enhancing VLM Safety through Reasoning and Reinforcement Learning', 'desc': 'This paper presents GuardReasoner-VL, a new model designed to improve the safety of Vision-Language Models (VLMs) by incorporating reasoning into moderation decisions. The model is trained using a large reasoning corpus that includes diverse text and image inputs, allowing it to learn from 123K samples and 631K reasoning steps. To enhance its reasoning capabilities, the model employs supervised fine-tuning (SFT) and online reinforcement learning (RL), which helps it adapt and improve over time. The results show that GuardReasoner-VL significantly outperforms existing models, achieving a 19.27% higher F1 score on average, demonstrating its effectiveness in ensuring safer VLM operations.'}, 'zh': {'title': 'Êé®ÁêÜÈ©±Âä®ÁöÑÂÆâÂÖ®‰øùÊä§ÔºöGuardReasoner-VL', 'desc': '‰∏∫‰∫ÜÊèêÈ´òËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂÆâÂÖ®ÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫‰∫éÊé®ÁêÜÁöÑVLM‰øùÊä§Ê®°ÂûãÔºåÁß∞‰∏∫GuardReasoner-VL„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊøÄÂä±‰øùÊä§Ê®°ÂûãÂú®ÂÅöÂá∫ÂÆ°Êü•ÂÜ≥Á≠ñ‰πãÂâçËøõË°åÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´123KÊ†∑Êú¨Âíå631KÊé®ÁêÜÊ≠•È™§ÁöÑÊé®ÁêÜËØ≠ÊñôÂ∫ìÔºåÂπ∂ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊù•ÂÜ∑ÂêØÂä®Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈÄöËøáÂú®Á∫øRLËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÂÆ°Êü•Êé®ÁêÜÁöÑËÉΩÂäõÔºåÊúÄÁªàÂÆûÈ™åÁªìÊûúÊòæÁ§∫ËØ•Ê®°ÂûãÂú®F1ÂàÜÊï∞‰∏äÂπ≥ÂùáË∂ÖË∂ä‰∫ÜÁ¨¨‰∫åÂêç19.27%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10610', 'title': 'MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly', 'url': 'https://huggingface.co/papers/2505.10610', 'abstract': "The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.", 'score': 39, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '565f788b384ce66a', 'authors': ['Zhaowei Wang', 'Wenhao Yu', 'Xiyu Ren', 'Jipeng Zhang', 'Yu Zhao', 'Rohit Saxena', 'Liang Cheng', 'Ginny Wong', 'Simon See', 'Pasquale Minervini', 'Yangqiu Song', 'Mark Steedman'], 'affiliations': ['CSE Department, HKUST', 'Miniml.AI', 'NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA', 'Tencent AI Seattle Lab', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10610.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#long_context'], 'emoji': 'üìè', 'ru': {'title': 'MMLongBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MMLongBench - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (LCVLM). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 13,331 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–¥–∞—á, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –æ—Ç 8K –¥–æ 128K —Ç–æ–∫–µ–Ω–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 46 –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤.'}, 'en': {'title': 'MMLongBench: Advancing Long-Context Vision-Language Models', 'desc': 'This paper introduces MMLongBench, a new benchmark designed to evaluate long-context vision-language models (LCVLMs) that can process large amounts of images and text simultaneously. It includes 13,331 examples across five task categories, ensuring a diverse assessment of model performance. The benchmark tests models at various input lengths, allowing for a detailed analysis of their capabilities in handling long-context tasks. The findings reveal that current models struggle with long-context challenges, highlighting the need for improvements and suggesting that models with better reasoning skills perform better in these scenarios.'}, 'zh': {'title': 'MMLongBenchÔºöÈïø‰∏ä‰∏ãÊñáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑËØÑ‰º∞Êñ∞Âü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜMMLongBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÈíàÂØπÈïø‰∏ä‰∏ãÊñáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLCVLMsÔºâÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇMMLongBenchÂåÖÂê´13331‰∏™Á§∫‰æãÔºåÊ∂µÁõñ‰∫îÁßç‰∏çÂêåÁ±ªÂà´ÁöÑ‰∏ãÊ∏∏‰ªªÂä°ÔºåÂπ∂Êèê‰æõÂ§öÁßçËá™ÁÑ∂ÂíåÂêàÊàêÂõæÂÉèÁ±ªÂûã„ÄÇÈÄöËøáÂØπ46‰∏™Èó≠Ê∫êÂíåÂºÄÊ∫êLCVLMÁöÑÊ∑±ÂÖ•ËØÑ‰º∞ÔºåÁ†îÁ©∂ÂèëÁé∞Âçï‰∏Ä‰ªªÂä°ÁöÑË°®Áé∞Âπ∂‰∏çËÉΩÊúâÊïàÂèçÊò†Êï¥‰ΩìÁöÑÈïø‰∏ä‰∏ãÊñáËÉΩÂäõ„ÄÇÁªìÊûúË°®ÊòéÔºåÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõË∂äÂº∫ÔºåÈïø‰∏ä‰∏ãÊñáË°®Áé∞Ë∂äÂ•ΩÔºåÊú™Êù•Âú®Ëøô‰∏ÄÈ¢ÜÂüü‰ªçÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11409', 'title': "Visual Planning: Let's Think Only with Images", 'url': 'https://huggingface.co/papers/2505.11409', 'abstract': 'Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.', 'score': 30, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '67875b7838a7b7ea', 'authors': ['Yi Xu', 'Chengzu Li', 'Han Zhou', 'Xingchen Wan', 'Caiqi Zhang', 'Anna Korhonen', 'Ivan Vuliƒá'], 'affiliations': ['Google', 'Language Technology Lab, University of Cambridge', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.11409.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#training', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—à–∏–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –±–µ–∑ —Å–ª–æ–≤', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —á–∏—Å—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º VPRL, —É—Å–∏–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥–æ–º GRPO –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–≥–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –≤—ã–≤–æ–¥–∞."}, 'en': {'title': 'Visual Planning: Reasoning Beyond Text', 'desc': 'This paper introduces a new approach called Visual Planning, which focuses on using visual representations for reasoning instead of relying solely on text. The authors argue that for tasks involving spatial and geometrical information, visual reasoning can be more effective. They present a reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), which enhances planning capabilities in visual navigation tasks. The results show that this visual approach outperforms traditional text-based reasoning methods, suggesting a shift towards image-based inference in machine learning applications.'}, 'zh': {'title': 'ËßÜËßâËßÑÂàíÔºöË∂ÖË∂äÊñáÊú¨ÁöÑÊé®ÁêÜÊñ∞ËåÉÂºè', 'desc': 'ÊúÄËøëÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§öÊ®°ÊÄÅÊâ©Â±ïÔºàMLLMsÔºâÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÊú∫Âô®Êé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°Âûã‰∏ªË¶Å‰æùËµñÁ∫ØÊñáÊú¨Êù•Ë°®ËææÂíåÊûÑÂª∫Êé®ÁêÜÔºåÂç≥‰ΩøÂú®Â≠òÂú®ËßÜËßâ‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËåÉÂºèÔºåÁß∞‰∏∫ËßÜËßâËßÑÂàíÔºåÂÖÅËÆ∏ÈÄöËøáÁ∫ØËßÜËßâË°®Á§∫ËøõË°åËßÑÂàíÔºåËÄå‰∏ç‰æùËµñÊñáÊú¨„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËßÜËßâËßÑÂàíÂú®Â§ÑÁêÜÁ©∫Èó¥ÂíåÂá†‰Ωï‰ø°ÊÅØÁöÑ‰ªªÂä°‰∏≠ÔºåÊØîÂü∫‰∫éËØ≠Ë®ÄÁöÑÊé®ÁêÜÊõ¥ÊúâÊïà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11107', 'title': 'Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity', 'url': 'https://huggingface.co/papers/2505.11107', 'abstract': "Recent advances in large language models (LLMs) have demonstrated the power of reasoning through self-generated chains of thought. Multiple reasoning agents can collaborate to raise joint reasoning quality above individual outcomes. However, such agents typically interact in a turn-based manner, trading increased latency for improved quality. In this paper, we propose Group Think--a single LLM that acts as multiple concurrent reasoning agents, or thinkers. With shared visibility into each other's partial generation progress, Group Think introduces a new concurrent-reasoning paradigm in which multiple reasoning trajectories adapt dynamically to one another at the token level. For example, a reasoning thread may shift its generation mid-sentence upon detecting that another thread is better positioned to continue. This fine-grained, token-level collaboration enables Group Think to reduce redundant reasoning and improve quality while achieving significantly lower latency. Moreover, its concurrent nature allows for efficient utilization of idle computational resources, making it especially suitable for edge inference, where very small batch size often underutilizes local~GPUs. We give a simple and generalizable modification that enables any existing LLM to perform Group Think on a local GPU. We also present an evaluation strategy to benchmark reasoning latency and empirically demonstrate latency improvements using open-source LLMs that were not explicitly trained for Group Think. We hope this work paves the way for future LLMs to exhibit more sophisticated and more efficient collaborative behavior for higher quality generation.", 'score': 15, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'b2ea7382367e75ba', 'authors': ['Chan-Jan Hsu', 'Davide Buffelli', 'Jamie McGowan', 'Feng-Ting Liao', 'Yi-Chang Chen', 'Sattar Vakili', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11107.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#agents', '#training', '#open_source', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –æ–¥–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Group Think, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤. –≠—Ç–∏ –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ–π –∑–∞–¥–µ—Ä–∂–∫–µ. –ú–µ—Ç–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –≥–¥–µ —á–∞—Å—Ç–æ –Ω–µ–¥–æ–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω—ã–µ GPU. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –ª—é–±–æ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π LLM –≤—ã–ø–æ–ª–Ω—è—Ç—å Group Think –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º GPU.'}, 'en': {'title': 'Group Think: Collaborative Reasoning for Faster, Smarter LLMs', 'desc': "This paper introduces Group Think, a novel approach that allows a single large language model (LLM) to function as multiple reasoning agents working together simultaneously. By enabling these agents to share visibility into each other's progress, they can dynamically adjust their reasoning paths at the token level, enhancing the overall quality of the output. This concurrent reasoning reduces redundancy and improves efficiency, leading to lower latency compared to traditional turn-based interactions. The authors also provide a method to adapt existing LLMs for Group Think, demonstrating its effectiveness through empirical evaluations."}, 'zh': {'title': 'Group ThinkÔºöÊèêÂçáÊé®ÁêÜË¥®ÈáèÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Group ThinkÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Âçï‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫Â§ö‰∏™Âπ∂ÂèëÊé®ÁêÜ‰ª£ÁêÜ„ÄÇÈÄöËøáÂÖ±‰∫´ÂΩºÊ≠§ÁöÑÁîüÊàêËøõÂ∫¶ÔºåËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏Êé®ÁêÜÁ∫øÁ®ãÂú®ÁîüÊàêËøáÁ®ã‰∏≠Âä®ÊÄÅÈÄÇÂ∫îÔºå‰ªéËÄåÂáèÂ∞ëÂÜó‰ΩôÊé®ÁêÜÂπ∂ÊèêÈ´òÁîüÊàêË¥®Èáè„ÄÇGroup ThinkÁöÑÂπ∂ÂèëÁâπÊÄß‰ΩøÂæóËÆ°ÁÆóËµÑÊ∫êÂæóÂà∞Êõ¥ÊúâÊïàÁöÑÂà©Áî®ÔºåÁâπÂà´ÈÄÇÂêàËæπÁºòÊé®ÁêÜÂú∫ÊôØ„ÄÇÊàë‰ª¨ËøòÊèê‰æõ‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑ‰øÆÊîπÊñπÊ≥ïÔºå‰ΩøÁé∞ÊúâÁöÑLLMËÉΩÂ§üÂú®Êú¨Âú∞GPU‰∏äÂÆûÁé∞Group Think„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07675', 'title': 'Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization', 'url': 'https://huggingface.co/papers/2505.07675', 'abstract': 'Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose texttt{D}ual-texttt{H}ead texttt{O}ptimization (texttt{DHO}) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that DHO mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that DHO consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.', 'score': 13, 'issue_id': 3828, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '73f4f4dd13e67a25', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07675.jpg', 'data': {'categories': ['#small_models', '#optimization', '#transfer_learning', '#training', '#cv', '#data'], 'emoji': 'üß†', 'ru': {'title': 'DHO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Dual-Head Optimization (DHO) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π –æ—Ç –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∫ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. DHO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≥–æ–ª–æ–≤—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö —É—á–∏—Ç–µ–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É —Å–∏–≥–Ω–∞–ª–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –±–µ–∑ –Ω–µ–≥–æ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DHO –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–æ—Å—Ç–∏–≥–∞—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ ImageNet –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Streamlining Knowledge Distillation with Dual-Head Optimization', 'desc': 'This paper introduces a new framework called Dual-Head Optimization (DHO) for knowledge distillation from vision-language models (VLMs) to smaller, task-specific models. DHO uses two prediction heads that learn from both labeled data and the predictions of a larger teacher model, which helps to reduce conflicts in learning signals. The method simplifies the distillation process, avoiding the complexity of multi-stage training while still improving feature learning. Experiments show that DHO outperforms existing methods, achieving better accuracy on datasets like ImageNet with fewer parameters.'}, 'zh': {'title': 'DHOÔºöÈ´òÊïàÁöÑÁü•ËØÜËí∏È¶èÊ°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DHOÁöÑÁü•ËØÜËí∏È¶èÊ°ÜÊû∂ÔºåÊó®Âú®Â∞ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞Á¥ßÂáëÁöÑ‰ªªÂä°ÁâπÂÆöÊ®°Âûã‰∏≠„ÄÇDHOÈÄöËøáÂºïÂÖ•ÂèåÈ¢ÑÊµãÂ§¥ÔºåÂàÜÂà´‰ªéÊ†áËÆ∞Êï∞ÊçÆÂíåÊïôÂ∏àÈ¢ÑÊµã‰∏≠Áã¨Á´ãÂ≠¶‰π†ÔºåÂπ∂Âú®Êé®ÁêÜÊó∂Á∫øÊÄßÁªÑÂêàÂÆÉ‰ª¨ÁöÑËæìÂá∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDHOÊúâÊïàÁºìËß£‰∫ÜÁõëÁù£‰ø°Âè∑ÂíåËí∏È¶è‰ø°Âè∑‰πãÈó¥ÁöÑÊ¢ØÂ∫¶ÂÜ≤Á™ÅÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊØîÂçïÂ§¥Áü•ËØÜËí∏È¶èÂü∫Á∫øÊõ¥ÊúâÊïàÁöÑÁâπÂæÅÂ≠¶‰π†„ÄÇÊúÄÁªàÔºåDHOÂú®Â§ö‰∏™È¢ÜÂüüÂíåÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÈõÜ‰∏äÂùáË°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ImageNet‰∏äÔºå‰ΩøÁî®Êõ¥Â∞ëÁöÑÂèÇÊï∞ÂÆûÁé∞‰∫Ü3%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11427', 'title': 'Mergenetic: a Simple Evolutionary Model Merging Library', 'url': 'https://huggingface.co/papers/2505.11427', 'abstract': 'Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.', 'score': 10, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '759eb3fbdec85844', 'authors': ['Adrian Robert Minut', 'Tommaso Mencattini', 'Andrea Santilli', 'Donato Crisostomi', 'Emanuele Rodol√†'], 'affiliations': ['Ecole Polytechnique F√©d√©rale de Lausanne', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2505.11427.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#training'], 'emoji': 'üß¨', 'ru': {'title': '–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É Mergenetic –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã. Mergenetic –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏ –≤–∫–ª—é—á–∞–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —è–∑—ã–∫–∞—Ö –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–∫—Ä–æ–º–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.'}, 'en': {'title': 'Mergenetic: Evolving Better Models Through Merging', 'desc': 'This paper presents Mergenetic, an open-source library designed for evolutionary model merging in machine learning. Model merging allows the combination of existing models into a new one without the need for additional training, making it cost-effective and efficient. Mergenetic enhances this process by integrating evolutionary algorithms, which can improve model performance. The library also includes lightweight fitness estimators to minimize evaluation costs, demonstrating competitive results across various tasks and languages using standard hardware.'}, 'zh': {'title': 'MergeneticÔºöËøõÂåñÊ®°ÂûãÂêàÂπ∂ÁöÑÊñ∞ÈÄâÊã©', 'desc': 'Ê®°ÂûãÂêàÂπ∂ÊòØ‰∏ÄÁßçÂ∞ÜÁé∞ÊúâÊ®°ÂûãÁöÑËÉΩÂäõÁªìÂêàÊàêÊñ∞Ê®°ÂûãÁöÑÊñπÊ≥ïÔºåÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉ„ÄÇËøôÁßçÊñπÊ≥ïÂõ†ÂÖ∂‰ΩéÊàêÊú¨ÂíåÊîØÊåÅÊ∂àË¥πËÄÖGPUÁöÑÂ∫ìËÄåË∂äÊù•Ë∂äÂèóÊ¨¢Ëøé„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ÜÂêàÂπ∂‰∏éËøõÂåñÁÆóÊ≥ïÁªìÂêàÂèØ‰ª•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜÁõÆÂâçÊ≤°ÊúâÊ°ÜÊû∂ÊîØÊåÅÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁÅµÊ¥ªÂÆûÈ™åËøô‰∫õÁ≠ñÁï•„ÄÇÊàë‰ª¨‰ªãÁªç‰∫ÜMergeneticÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÂ∫ìÔºåÁî®‰∫éËøõÂåñÊ®°ÂûãÂêàÂπ∂ÔºåËÉΩÂ§üËΩªÊùæÁªÑÂêàÂêàÂπ∂ÊñπÊ≥ïÂíåËøõÂåñÁÆóÊ≥ïÔºåÂêåÊó∂ÂºïÂÖ•ËΩªÈáèÁ∫ßÁöÑÈÄÇÂ∫îÂ∫¶ËØÑ‰º∞Âô®‰ª•Èôç‰ΩéËØÑ‰º∞ÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10518', 'title': 'Multi-Token Prediction Needs Registers', 'url': 'https://huggingface.co/papers/2505.10518', 'abstract': 'Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.', 'score': 9, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '10aef9838701ab6e', 'authors': ['Anastasios Gerontopoulos', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ['Archimedes, Athena Research Center', 'IACM-Forth', 'University of Crete', 'valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2505.10518.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization'], 'emoji': 'üîÆ', 'ru': {'title': 'MuToR: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ–º–∞—Ä–∫–µ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ–º–∞—Ä–∫–µ—Ä–Ω–æ–º—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MuToR. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—É—á–∞–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã-—Ä–µ–≥–∏—Å—Ç—Ä—ã –≤ –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Ü–µ–ª–µ–π. MuToR –∏–º–µ–µ—Ç —Ä—è–¥ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å MuToR –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è.'}, 'en': {'title': 'MuToR: Enhancing Multi-Token Prediction for Language Models', 'desc': "This paper introduces MuToR, a novel approach to multi-token prediction that enhances language model pretraining. MuToR integrates learnable register tokens into the input sequence, allowing the model to predict multiple future tokens effectively. It maintains compatibility with existing pretrained models without requiring architectural changes and adds minimal parameters. The authors demonstrate MuToR's effectiveness across various tasks in both language and vision, showcasing its versatility in supervised and parameter-efficient fine-tuning."}, 'zh': {'title': 'MuToRÔºöÈ´òÊïàÁöÑÂ§öÊ†áËÆ∞È¢ÑÊµãÊñπÊ≥ï', 'desc': 'Â§öÊ†áËÆ∞È¢ÑÊµãÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÁõÆÊ†áÔºåÁî®‰∫éÊîπÂñÑËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºå‰ΩÜÂÖ∂‰ºòÂäøÂú®ÂÖ∂‰ªñËÆæÁΩÆÔºàÂ¶ÇÂæÆË∞ÉÔºâ‰∏≠Âπ∂‰∏çÊÄªÊòØÊúâÊïà„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂ§öÊ†áËÆ∞È¢ÑÊµãÊñπÊ≥ïMuToRÔºåÂÆÉÂ∞ÜÂèØÂ≠¶‰π†ÁöÑÊ≥®ÂÜåÊ†áËÆ∞‰∫§ÈîôÂà∞ËæìÂÖ•Â∫èÂàó‰∏≠ÔºåÊØè‰∏™Ê†áËÆ∞Ë¥üË¥£È¢ÑÊµãÊú™Êù•ÁöÑÁõÆÊ†á„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåMuToRÂÖ∑ÊúâÂá†‰∏™ÂÖ≥ÈîÆ‰ºòÂäøÔºö‰ªÖÂºïÂÖ•ÊûÅÂ∞ëÁöÑÈ¢ùÂ§ñÂèÇÊï∞Ôºå‰∏çÈúÄË¶ÅÊû∂ÊûÑÊõ¥ÊîπÔºåÁ°Æ‰øù‰∏éÁé∞ÊàêÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÂÖºÂÆπÔºåÂπ∂‰∏î‰∏é‰∏ã‰∏Ä‰∏™Ê†áËÆ∞ÁöÑÈ¢ÑËÆ≠ÁªÉÁõÆÊ†á‰øùÊåÅ‰∏ÄËá¥ÔºåÁâπÂà´ÈÄÇÂêàÁõëÁù£ÂæÆË∞É„ÄÇÊ≠§Â§ñÔºåÂÆÉËá™ÁÑ∂ÊîØÊåÅÂèØÊâ©Â±ïÁöÑÈ¢ÑÊµãËåÉÂõ¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10962', 'title': 'MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation', 'url': 'https://huggingface.co/papers/2505.10962', 'abstract': 'Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.', 'score': 7, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '07990204af30ff71', 'authors': ['Zhenwen Liang', 'Linfeng Song', 'Yang Li', 'Tao Yang', 'Feng Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'Tencent LLM Department'], 'pdf_title_img': 'assets/pdf/title_img/2505.10962.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#reasoning', '#data', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ú–Ω–æ–≥–æ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Ç–µ–æ—Ä–µ–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MPS-Prover. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∏—Å–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞–º–∏. MPS-Prover –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ MPS-Prover –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Multi-Perspective Search', 'desc': 'This paper presents the Multi-Perspective Search Prover (MPS-Prover), a new system for Automated Theorem Proving (ATP) that addresses inefficiencies in existing stepwise provers. MPS-Prover utilizes a post-training data curation strategy to eliminate redundant training data, improving performance without loss of quality. It also features a multi-perspective tree search that combines a learned critic model with heuristic rules to enhance search diversity and prevent unproductive paths. The results show that MPS-Prover not only achieves state-of-the-art performance on various benchmarks but also produces shorter and more diverse proofs than previous models.'}, 'zh': {'title': 'Â§öËßÜËßíÊêúÁ¥¢ÔºåÊèêÂçáÂÆöÁêÜËØÅÊòéÊïàÁéá', 'desc': 'Ëá™Âä®ÂÆöÁêÜËØÅÊòéÔºàATPÔºâÂú®ÂΩ¢ÂºèËØ≠Ë®Ä‰∏≠‰ªçÁÑ∂ÊòØ‰∫∫Â∑•Êô∫ËÉΩ‰∏≠ÁöÑ‰∏ÄÂ§ßÊåëÊàòÔºåÈúÄË¶Å‰∏•Ê†ºÁöÑÈÄªËæëÊé®ÁêÜÂíåÂπøÊ≥õÁöÑÊêúÁ¥¢Á©∫Èó¥„ÄÇËôΩÁÑ∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩÔºå‰ΩÜÁé∞ÊúâÁöÑÈÄêÊ≠•ËØÅÊòéÂô®Â∏∏Â∏∏ÂèóÂà∞ÂÅèËßÅÊêúÁ¥¢ÊåáÂØºÁöÑÂΩ±ÂìçÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÂíåÊ¨°‰ºòÁöÑËØÅÊòéÁ≠ñÁï•„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈÄêÊ≠•ATPÁ≥ªÁªü‚Äî‚ÄîÂ§öËßÜËßíÊêúÁ¥¢ËØÅÊòéÂô®ÔºàMPS-ProverÔºâÔºåÊó®Âú®ÂÖãÊúçËøô‰∫õÂ±ÄÈôêÊÄß„ÄÇMPS-ProverÁªìÂêà‰∫ÜÈ´òÊïàÁöÑÂêéËÆ≠ÁªÉÊï∞ÊçÆÊï¥ÁêÜÁ≠ñÁï•ÂíåÂ§öËßÜËßíÊ†ëÊêúÁ¥¢Êú∫Âà∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËØÅÊòéÁöÑÊïàÁéáÂíåÂ§öÊ†∑ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11140', 'title': 'Scaling Reasoning can Improve Factuality in Large Language Models', 'url': 'https://huggingface.co/papers/2505.11140', 'abstract': 'Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.', 'score': 5, 'issue_id': 3827, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '29d6b0a8040db2ff', 'authors': ['Mike Zhang', 'Johannes Bjerva', 'Russa Biswas'], 'affiliations': ['Department of Computer Science Aalborg University Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2505.11140.jpg', 'data': {'categories': ['#reasoning', '#graphs', '#dataset', '#benchmark', '#inference', '#training'], 'emoji': 'üß†', 'ru': {'title': '–î–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –≤–ª–∏—è–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±–æ–≥–∞—â–∞—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ–Ω—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∑–∞–º–µ—Ç–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ 2-8%.'}, 'en': {'title': 'Enhancing LLM Reasoning with Knowledge and Compute', 'desc': 'This paper investigates the reasoning capabilities of large language models (LLMs) in open-domain question-answering tasks. It analyzes how longer reasoning processes and additional computational resources can impact factual accuracy, especially beyond mathematical reasoning. The authors fine-tune various models and incorporate knowledge graph information to enhance reasoning traces. Their experiments reveal that smaller models can achieve better factual accuracy than larger, instruction-tuned models, and that increasing computational resources during inference can further improve performance.'}, 'zh': {'title': 'ÊèêÂçáÊé®ÁêÜÂáÜÁ°ÆÊÄßÁöÑÂÖ≥ÈîÆÂú®‰∫éÊ®°Âûã‰∏éËµÑÊ∫êÁöÑÁªìÂêà', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Â§çÊùÇÂºÄÊîæÈ¢ÜÂüüÈóÆÁ≠îÔºàQAÔºâÂú∫ÊôØ‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨‰ªéÂÖàËøõÁöÑÊé®ÁêÜÊ®°Âûã‰∏≠ÊèêÂèñÊé®ÁêÜËΩ®ËøπÔºåÂπ∂ÂØπÂ§öÁßçÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•ÊèêÈ´òÂÖ∂Êé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáÂºïÂÖ•Áü•ËØÜÂõæË∞±‰∏≠ÁöÑ‰∫ãÂÆû‰ø°ÊÅØÔºåÊàë‰ª¨‰∏∞ÂØå‰∫ÜÊé®ÁêÜËΩ®ËøπÔºåÂπ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåËæÉÂ∞èÁöÑÊé®ÁêÜÊ®°ÂûãÂú®‰∫ãÂÆûÂáÜÁ°ÆÊÄß‰∏äÊúâÊòæËëóÊèêÂçáÔºåËÄåÂú®ÊµãËØïÊó∂Â¢ûÂä†ËÆ°ÁÆóËµÑÊ∫êÂíå‰ª§ÁâåÈ¢ÑÁÆó‰πüËÉΩËøõ‰∏ÄÊ≠•ÊèêÈ´òÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11011', 'title': 'Humans expect rationality and cooperation from LLM opponents in\n  strategic games', 'url': 'https://huggingface.co/papers/2505.11011', 'abstract': "As Large Language Models (LLMs) integrate into our social and economic interactions, we need to deepen our understanding of how humans respond to LLMs opponents in strategic settings. We present the results of the first controlled monetarily-incentivised laboratory experiment looking at differences in human behaviour in a multi-player p-beauty contest against other humans and LLMs. We use a within-subject design in order to compare behaviour at the individual level. We show that, in this environment, human subjects choose significantly lower numbers when playing against LLMs than humans, which is mainly driven by the increased prevalence of `zero' Nash-equilibrium choices. This shift is mainly driven by subjects with high strategic reasoning ability. Subjects who play the zero Nash-equilibrium choice motivate their strategy by appealing to perceived LLM's reasoning ability and, unexpectedly, propensity towards cooperation. Our findings provide foundational insights into the multi-player human-LLM interaction in simultaneous choice games, uncover heterogeneities in both subjects' behaviour and beliefs about LLM's play when playing against them, and suggest important implications for mechanism design in mixed human-LLM systems.", 'score': 4, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '0f12554b6b3b2b83', 'authors': ['Darija Barak', 'Miguel Costa-Gomes'], 'affiliations': ['School of Economics University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.11011.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#games'], 'emoji': 'ü§ñ', 'ru': {'title': '–õ—é–¥–∏ vs –ò–ò: –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –∏–≥—Ä–∞—Ö —Å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ª—é–¥–µ–π –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–π –∏–≥—Ä–µ p-beauty contest –ø—Ä–æ—Ç–∏–≤ –¥—Ä—É–≥–∏—Ö –ª—é–¥–µ–π –∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —É—á–∞—Å—Ç–Ω–∏–∫–∏ –≤—ã–±–∏—Ä–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏–µ —á–∏—Å–ª–∞ –ø—Ä–∏ –∏–≥—Ä–µ –ø—Ä–æ—Ç–∏–≤ LLM, —á–µ–º –ø—Ä–æ—Ç–∏–≤ –ª—é–¥–µ–π, —á—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç—Å—è –±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º –≤—ã–±–æ—Ä–æ–º —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—è –ù—ç—à–∞ '–Ω–æ–ª—å'. –¢–∞–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∞—é—Ç –≤–∞–∂–Ω—ã–µ insights –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –∏ LLM –≤ –∏–≥—Ä–∞—Ö —Å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ-LLM —Å–∏—Å—Ç–µ–º–∞—Ö."}, 'en': {'title': 'Understanding Human Behavior in Games Against LLMs', 'desc': "This paper investigates how humans behave when competing against Large Language Models (LLMs) in strategic games, specifically in a multi-player p-beauty contest. The study reveals that participants tend to choose lower numbers when playing against LLMs compared to human opponents, influenced by the perception of LLMs' reasoning capabilities. The results indicate that individuals with strong strategic reasoning are more likely to adopt a 'zero' Nash-equilibrium strategy, believing it aligns with the LLM's cooperative tendencies. These findings highlight the complexities of human-LLM interactions and their implications for designing effective systems that integrate both human and LLM participants."}, 'zh': {'title': '‰∫∫Á±ª‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊàòÁï•‰∫íÂä®Êñ∞ËßÜËßí', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰∫∫Á±ªÂú®‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°åÊàòÁï•‰∫íÂä®Êó∂ÁöÑË°å‰∏∫Â∑ÆÂºÇ„ÄÇÈÄöËøá‰∏Ä‰∏™ÂèóÊéßÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞‰∫∫Á±ªÂú®‰∏éLLMsÂØπÊàòÊó∂ÈÄâÊã©ÁöÑÊï∞Â≠óÊòæËëó‰Ωé‰∫é‰∏éÂÖ∂‰ªñ‰∫∫Á±ªÂØπÊàòÊó∂ÁöÑÈÄâÊã©„ÄÇËøôÁßçÁé∞Ë±°‰∏ªË¶ÅÊòØÁî±‰∫éÈ´òÊàòÁï•Êé®ÁêÜËÉΩÂäõÁöÑÂèÇ‰∏éËÄÖÊõ¥ÂÄæÂêë‰∫éÈÄâÊã©Èõ∂Á∫≥‰ªÄÂùáË°°Á≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑÂèëÁé∞‰∏∫‰∫∫Á±ª‰∏éLLMsÂú®Â§öÁé©ÂÆ∂ÂêåÊó∂ÈÄâÊã©Ê∏∏Êàè‰∏≠ÁöÑ‰∫íÂä®Êèê‰æõ‰∫ÜÂü∫Á°ÄÊÄßËßÅËß£ÔºåÂπ∂Êè≠Á§∫‰∫ÜÂèÇ‰∏éËÄÖË°å‰∏∫ÂíåÂØπLLMsÊ∏∏ÊàèÊñπÂºèÁöÑ‰ø°ÂøµÂ∑ÆÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10852', 'title': 'MatTools: Benchmarking Large Language Models for Materials Science Tools', 'url': 'https://huggingface.co/papers/2505.10852', 'abstract': 'Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.', 'score': 4, 'issue_id': 3831, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'a29f8a9973b15514', 'authors': ['Siyu Liu', 'Jiamin Xu', 'Beilin Ye', 'Bo Hu', 'David J. Srolovitz', 'Tongqi Wen'], 'affiliations': ['Center for Structural Materials, Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.10852.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#science'], 'emoji': 'üß™', 'ru': {'title': 'MatTools: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MatTools - –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏—è –ø—É—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–∫–µ—Ç–æ–≤ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏—è. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –Ω–∞–±–æ—Ä–∞ –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –Ω–∞–±–æ—Ä–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏—è. –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM —Å –ø–æ–º–æ—â—å—é MatTools –≤—ã—è–≤–∏–ª–∞ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–∞: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –ò–ò —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç—Å—è –≤ –ò–ò, –∏ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç.'}, 'en': {'title': 'Evaluating LLMs in Materials Science: Generalists Win!', 'desc': 'This paper introduces MatTools, a benchmark designed to evaluate how well large language models (LLMs) can handle materials science tasks. It combines a question-answer (QA) benchmark with a real-world tool-usage benchmark, assessing LLMs on their ability to generate and execute code for materials property calculations. The QA benchmark includes over 69,000 question-answer pairs derived from existing materials science resources, while the real-world benchmark consists of 49 tasks that require functional Python code generation. The findings suggest that generalist models perform better than specialized ones, that AI can effectively leverage other AI tools, and that simpler approaches yield better results.'}, 'zh': {'title': 'ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÊùêÊñôÁßëÂ≠¶‰∏≠ÁöÑÂ∫îÁî®ËÉΩÂäõ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊùêÊñôÁßëÂ≠¶È¢ÜÂüüÁöÑÂ∫îÁî®Ë∂äÊù•Ë∂äÂπøÊ≥õÔºåÂåÖÊã¨ÊñáÁåÆÁêÜËß£„ÄÅÂ±ûÊÄßÈ¢ÑÊµã„ÄÅÊùêÊñôÂèëÁé∞ÂíåÂêàÈáëËÆæËÆ°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫ÂáÜÂ∫îÁî®ÔºåËØÑ‰º∞LLMsÂú®ÊùêÊñôÁßëÂ≠¶ÈóÆÈ¢ò‰∏äÁöÑËÉΩÂäõÔºåÁâπÂà´ÊòØÈÄöËøáÁîüÊàêÂíåÂÆâÂÖ®ÊâßË°åÂü∫‰∫éÁâ©ÁêÜÁöÑËÆ°ÁÆóÊùêÊñôÁßëÂ≠¶ËΩØ‰ª∂ÂåÖÁöÑ‰ª£Á†Å„ÄÇMatToolsÁî±‰∏§‰∏™‰∫íË°•ÁªÑ‰ª∂ÊûÑÊàêÔºöÊùêÊñôÊ®°ÊãüÂ∑•ÂÖ∑ÈóÆÁ≠îÂü∫ÂáÜÂíåÁúüÂÆûÂ∑•ÂÖ∑‰ΩøÁî®Âü∫ÂáÜ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÈÄöÁî®Ê®°Âûã‰ºò‰∫é‰∏ì‰∏öÊ®°ÂûãÔºåAIËÉΩÂ§üÁêÜËß£ÂÖ∂‰ªñAIÁöÑËÉΩÂäõÔºåÂπ∂‰∏îÁÆÄÂçïÁöÑÊñπÊ≥ïÊõ¥ÊúâÊïà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11152', 'title': 'Learning Dense Hand Contact Estimation from Imbalanced Data', 'url': 'https://huggingface.co/papers/2505.11152', 'abstract': 'Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.', 'score': 2, 'issue_id': 3822, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'caa702fa71c24606', 'authors': ['Daniel Sungho Jung', 'Kyoung Mu Lee'], 'affiliations': ['IPAI, Dept. of ECE & ASRI, Seoul National University, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.11152.jpg', 'data': {'categories': ['#training', '#dataset', '#data'], 'emoji': 'üñêÔ∏è', 'ru': {'title': '–¢–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Ä—É–∫: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –¥–∞–Ω–Ω—ã—Ö', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –ø–ª–æ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞ —Ä—É–∫ —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å –º–∏—Ä–æ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ HACO –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∞—Å—Å–æ–≤–æ–≥–æ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–Ω—Ç–∞–∫—Ç–∞—Ö —Ä—É–∫. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å VCB, —É—á–∏—Ç—ã–≤–∞—é—â—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ —Ä—É–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–ª–æ—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Ä—É–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing Hand Contact Estimation with Balanced Learning Techniques', 'desc': 'This paper addresses the challenge of estimating dense hand contact in various interactions, which is crucial for understanding hand functionality. It identifies two main issues: class imbalance, where most samples do not involve contact, and spatial imbalance, where contact is primarily at the fingertips. To overcome these challenges, the authors propose a framework called HACO that utilizes balanced contact sampling to ensure diverse representation of contact data. Additionally, they introduce a vertex-level class-balanced loss to adjust the learning process based on the frequency of contact across different hand regions, leading to improved predictions in dense hand contact estimation.'}, 'zh': {'title': 'ÊèêÂçáÊâãÈÉ®Êé•Ëß¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊâãÈÉ®Êé•Ëß¶‰º∞ËÆ°ÁöÑÈáçË¶ÅÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏éÁâ©‰Ωì„ÄÅÂÖ∂‰ªñÊâã„ÄÅÂú∫ÊôØÂíåË∫´‰ΩìÁöÑ‰∫íÂä®‰∏≠„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâÂ§ßÈáèÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÔºå‰ΩÜÂ¶Ç‰ΩïÊúâÊïàÂ≠¶‰π†ÂØÜÈõÜÁöÑÊâãÈÉ®Êé•Ëß¶‰º∞ËÆ°‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ë¢´ÂÖÖÂàÜÁ†îÁ©∂ÁöÑÈóÆÈ¢ò„ÄÇËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫HACOÔºåÊó®Âú®Ëß£ÂÜ≥Á±ª‰∏çÂπ≥Ë°°ÂíåÁ©∫Èó¥‰∏çÂπ≥Ë°°ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂºïÂÖ•Âπ≥Ë°°Êé•Ëß¶ÈááÊ†∑ÂíåÈ°∂ÁÇπÁ∫ßÁ±ªÂπ≥Ë°°ÊçüÂ§±ÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÊàêÂäüÂú∞ÊèêÈ´ò‰∫ÜÊâãÈÉ®Êé•Ëß¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10496', 'title': 'CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs', 'url': 'https://huggingface.co/papers/2505.10496', 'abstract': 'We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': 'ef303e066da24351', 'authors': ['Raman Dutt', 'Pedro Sanchez', 'Yongchen Yao', 'Steven McDonagh', 'Sotirios A. Tsaftaris', 'Timothy Hospedales'], 'affiliations': ['Samsung AI Center, Cambridge', 'Sinkove', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10496.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#cv', '#open_source', '#benchmark'], 'emoji': '\U0001fa7b', 'ru': {'title': '–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –∫–∞—á–µ—Å—Ç–≤–æ, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', 'desc': 'CheXGenBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å, —Ä–∏—Å–∫–∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫—É—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. CheXGenBench –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 20 –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è 11 –≤–µ–¥—É—â–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤—ã–ø—É—Å—Ç–∏–ª–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç SynthCheX-75K –∏–∑ 75 000 —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≥—Ä–∞–º–º, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é –≤ –∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–µ.'}, 'en': {'title': 'Setting New Standards for Synthetic Chest Radiograph Evaluation', 'desc': 'CheXGenBench is a comprehensive evaluation framework designed to assess the generation of synthetic chest radiographs using advanced text-to-image models. It addresses previous challenges in medical image evaluation by providing standardized metrics that measure fidelity, privacy risks, and clinical utility. The framework includes over 20 quantitative metrics and evaluates 11 leading generative architectures, revealing inefficiencies in current evaluation methods. Additionally, it introduces a high-quality synthetic dataset, SynthCheX-75K, to support ongoing research in medical AI.'}, 'zh': {'title': 'Âª∫Á´ãÂåªÂ≠¶AIÁöÑÊñ∞Ê†áÂáÜËØÑ‰º∞Ê°ÜÊû∂', 'desc': 'CheXGenBenchÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éÂêàÊàêËÉ∏ÈÉ®XÂÖâÂõæÂÉèÁöÑÁîüÊàêÔºåËØÑ‰º∞ÁîüÊàêÁöÑÁúüÂÆûÊÄß„ÄÅÈöêÁßÅÈ£éÈô©Âíå‰∏¥Â∫äÂÆûÁî®ÊÄß„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫Ü‰ª•ÂæÄÂåªÂ≠¶È¢ÜÂüüËØÑ‰º∞‰∏≠ÁöÑÊñπÊ≥ï‰∏ç‰∏ÄËá¥„ÄÅÊû∂ÊûÑÊØîËæÉËøáÊó∂ÂíåËØÑ‰º∞Ê†áÂáÜËÑ±ËäÇÁ≠âÈóÆÈ¢ò„ÄÇÈÄöËøáÊ†áÂáÜÂåñÁöÑÊï∞ÊçÆÂàíÂàÜÂíåÁªü‰∏ÄÁöÑËØÑ‰º∞ÂçèËÆÆÔºåCheXGenBench‰ΩøÁî®Ë∂ÖËøá20‰∏™ÂÆöÈáèÊåáÊ†áÁ≥ªÁªüÂàÜÊûêÁîüÊàêË¥®ÈáèÂíåÊΩúÂú®ÈöêÁßÅÊºèÊ¥û„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÂêàÊàêÊï∞ÊçÆÈõÜSynthCheX-75KÔºåÂåÖÂê´75,000Âº†Áî±ÊúÄ‰Ω≥Ê®°ÂûãÁîüÊàêÁöÑXÂÖâÂõæÂÉèÔºå‰ª•ÊîØÊåÅËØ•È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09924', 'title': 'From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models', 'url': 'https://huggingface.co/papers/2505.09924', 'abstract': 'The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark{https://github.com/redwyd/SymMark}.', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '17456d03961632a9', 'authors': ['Yidan Wang', 'Yubing Ren', 'Yanan Cao', 'Binxing Fang'], 'affiliations': ['Hainan Province Fang Binxing Academician Workstation, Hainan, China', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China', 'School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09924.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#security', '#open_source', '#benchmark', '#architecture'], 'emoji': 'üîê', 'ru': {'title': '–°–∏–º–±–∏–æ—Ç–∏—á–µ—Å–∫–∞—è –∑–∞—â–∏—Ç–∞: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ–¥—è–Ω—ã–º –∑–Ω–∞–∫–∞–º –≤ —Ç–µ–∫—Å—Ç–∞—Ö –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç—ã, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–∏—Ç–æ–≤ –∏ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º–æ—Å—Ç—å—é, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é, –∫–∞—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –ª–∏–Ω–∏–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ —Å–≤–æ–µ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Synergizing Watermarking Techniques for Robust AI Text Security', 'desc': 'This paper addresses the challenges of watermarking Large Language Models (LLMs) to prevent misuse of AI-generated text. It introduces a new framework that combines logits-based and sampling-based watermarking techniques to enhance robustness and text quality while maintaining security. The proposed hybrid approach uses token and semantic entropy to adaptively embed watermarks, optimizing the balance between detectability and performance. Experimental results demonstrate that this method surpasses existing watermarking techniques, achieving state-of-the-art results across various datasets and models.'}, 'zh': {'title': 'ÂÖ±ÁîüÊ∞¥Âç∞Ê°ÜÊû∂Ôºö‰ºòÂåñAIÊñáÊú¨ÂÆâÂÖ®ÊÄß‰∏éË¥®ÈáèÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂÖ¥Ëµ∑ÔºåÂÖ≥‰∫éAIÁîüÊàêÊñáÊú¨Êª•Áî®ÁöÑÊãÖÂøßÂä†ÂâßÔºåÂõ†Ê≠§Ê∞¥Âç∞ÊäÄÊúØÊàê‰∏∫‰∏ÄÁßçÊúâÂâçÊôØÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÁé∞ÊúâÁöÑÊ∞¥Âç∞ÊñπÊ°à‰∏ªË¶ÅÂàÜ‰∏∫Âü∫‰∫élogitsÂíåÂü∫‰∫éÈááÊ†∑ÁöÑ‰∏§Á±ªÔºå‰ΩÜËøô‰∫õÊñπÊ°àÂú®È≤ÅÊ£íÊÄß„ÄÅÊñáÊú¨Ë¥®ÈáèÂíåÂÆâÂÖ®ÊÄß‰πãÈó¥Â≠òÂú®ÊùÉË°°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÂäüËÉΩÁöÑÂÖ±ÁîüÊ∞¥Âç∞Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËøô‰∏§ÁßçÊñπÊ°àÁöÑ‰ºòÁÇπÔºåÈááÁî®‰∏≤Ë°å„ÄÅÂπ∂Ë°åÂíåÊ∑∑Âêà‰∏âÁßçÁ≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåÊ®°Âûã‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫ÂáÜÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11480', 'title': 'Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.11480', 'abstract': 'Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.', 'score': 1, 'issue_id': 3837, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '553b664dc4913a2e', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Huanmi Tan', 'Yinglun Xu', 'Gagandeep Singh', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Carnegie Mellon University', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11480.jpg', 'data': {'categories': ['#rl', '#plp', '#training', '#benchmark', '#rlhf', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'LLM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç gcc –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞—Å—Å–µ–º–±–ª–µ—Ä–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞—Å—Å–µ–º–±–ª–µ—Ä–Ω–æ–≥–æ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –∞–ª–≥–æ—Ä–∏—Ç–º Proximal Policy Optimization (PPO) –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ LLM. –ú–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ gcc -O3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Qwen2.5-Coder-7B-PPO –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 96% –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤ –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 1.47 —Ä–∞–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–µ–π gcc -O3.'}, 'en': {'title': 'Unlocking LLMs for Assembly Code Optimization', 'desc': 'This paper explores the ability of large language models (LLMs) to optimize assembly code, which allows for precise control over execution. The authors develop a reinforcement learning framework using Proximal Policy Optimization (PPO) to train LLMs, focusing on both correctness and performance improvements. They introduce a benchmark of 8,072 real-world programs to evaluate their model, Qwen2.5-Coder-7B-PPO, which achieves a high test pass rate and significant speedup compared to the standard gcc -O3 compiler. The findings suggest that LLMs, when trained with reinforcement learning, can effectively enhance the performance of assembly code.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñÊ±áÁºñ‰ª£Á†Å', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁºñÁ®ã‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®‰ª£Á†Å‰ºòÂåñÊñπÈù¢ÁöÑÊΩúÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫ÜLLMsÊòØÂê¶ËÉΩÂ§ü‰ºòÂåñÊ±áÁºñ‰ª£Á†ÅÁöÑÊÄßËÉΩÔºåÂõ†‰∏∫Ê±áÁºñËØ≠Ë®ÄÊèê‰æõ‰∫ÜÂØπÊâßË°åÁöÑÁªÜÁ≤íÂ∫¶ÊéßÂà∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ΩøÁî®ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâËÆ≠ÁªÉLLMsÔºåÂπ∂ÈÄöËøáËÄÉËôëÂäüËÉΩÊ≠£Á°ÆÊÄßÂíåÊâßË°åÊÄßËÉΩÁöÑÂ•ñÂä±ÂáΩÊï∞ËøõË°åÊåáÂØº„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãQwen2.5-Coder-7B-PPOÂú®ÊµãËØï‰∏≠ËææÂà∞‰∫Ü96.0%ÁöÑÈÄöËøáÁéáÔºåÂπ∂‰∏îÂú®ÈÄüÂ∫¶‰∏äÊØîË°å‰∏öÊ†áÂáÜÁºñËØëÂô®gcc -O3Âø´‰∫Ü1.47ÂÄçÔºåÊòæÁ§∫Âá∫Âº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•ÊúâÊïàÊèêÂçáLLMsÂú®Ê±áÁºñ‰ª£Á†Å‰ºòÂåñ‰∏≠ÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11493', 'title': 'GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2505.11493', 'abstract': 'Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing.', 'score': 0, 'issue_id': 3838, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'd5f4f14c60b051ec', 'authors': ['Yusu Qian', 'Jiasen Lu', 'Tsu-Jui Fu', 'Xinze Wang', 'Chen Chen', 'Yinfei Yang', 'Wenze Hu', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2505.11493.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'GIE-Bench: –¢–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–∏–∑–º–µ–Ω—è–µ–º—ã—Ö —á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 1000 –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –∏ –º–∞—Å–∫–∞–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–æ–¥–µ–ª—å GPT-Image-1 –ª–∏–¥–∏—Ä—É–µ—Ç –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –Ω–æ —á–∞—Å—Ç–æ –∏–∑–º–µ–Ω—è–µ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing Evaluation of Text-Guided Image Editing with GIE-Bench', 'desc': 'This paper addresses the challenges in evaluating models that edit images based on natural language instructions. It introduces a new benchmark called GIE-Bench, which assesses models on functional correctness and image content preservation. The benchmark includes a large dataset of editing examples with detailed instructions and evaluation metrics. The study compares the performance of GPT-Image-1 with other models, revealing strengths in instruction-following but weaknesses in preserving non-targeted image areas.'}, 'zh': {'title': 'ÊèêÂçáÊñáÊú¨ÂºïÂØºÂõæÂÉèÁºñËæëÁöÑËØÑ‰º∞ÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®Êõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞ÊñáÊú¨ÂºïÂØºÁöÑÂõæÂÉèÁºñËæëÊ®°Âûã„ÄÇÊàë‰ª¨ÈÄöËøáËá™Âä®ÁîüÊàêÁöÑÂ§öÈ°πÈÄâÊã©È¢òÊù•ËØÑ‰º∞ÂäüËÉΩÊ≠£Á°ÆÊÄßÔºåÂπ∂‰ΩøÁî®ÂØπË±°ÊÑüÁü•Êé©ËÜúÊäÄÊúØÁ°Æ‰øùÂõæÂÉèÂÜÖÂÆπÁöÑ‰øùÁïô„ÄÇÂü∫ÂáÜÂåÖÂê´Ë∂ÖËøá1000‰∏™È´òË¥®ÈáèÁöÑÁºñËæëÁ§∫‰æãÔºåÊ∂µÁõñ20‰∏™‰∏çÂêåÁöÑÂÜÖÂÆπÁ±ªÂà´ÔºåÂπ∂ÈôÑÊúâËØ¶ÁªÜÁöÑÁºñËæëÊåá‰ª§ÂíåËØÑ‰º∞ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°GPT-Image-1Âú®ÈÅµÂæ™Êåá‰ª§ÁöÑÂáÜÁ°ÆÊÄß‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂú®Â§ÑÁêÜÊó†ÂÖ≥ÂõæÂÉèÂå∫ÂüüÊó∂Â∏∏Â∏∏ËøáÂ∫¶‰øÆÊîπÔºåÁ™ÅÊòæ‰∫ÜÂΩìÂâçÊ®°ÂûãË°å‰∏∫‰∏≠ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÊùÉË°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11315', 'title': 'Improving Inference-Time Optimisation for Vocal Effects Style Transfer\n  with a Gaussian Prior', 'url': 'https://huggingface.co/papers/2505.11315', 'abstract': "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach for transferring the applied effects of a reference audio to a raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including a blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our method's superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems.", 'score': 0, 'issue_id': 3836, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'a8b0aeed9083fba7', 'authors': ['Chin-Yun Yu', 'Marco A. Mart√≠nez-Ram√≠rez', 'Junghyun Koo', 'Wei-Hsiang Liao', 'Yuki Mitsufuji', 'Gy√∂rgy Fazekas'], 'affiliations': ['Centre for Digital Music, Queen Mary University of London, London, UK', 'Sony AI, Tokyo, Japan', 'Sony Group Corporation, Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.11315.jpg', 'data': {'categories': ['#optimization', '#inference', '#audio', '#dataset'], 'emoji': 'üéµ', 'ru': {'title': '–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–∞–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –∞—É–¥–∏–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä–µ–Ω–æ—Å—É –∞—É–¥–∏–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ ST-ITO —Å –≥–∞—É—Å—Å–æ–≤—ã–º –∞–ø—Ä–∏–æ—Ä–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –°—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ 16 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing Audio Effects Transfer with Prior Knowledge', 'desc': 'This paper presents a new method called Style Transfer with Inference-Time Optimisation (ST-ITO) for applying audio effects from a reference track to a raw audio track. The approach improves upon previous methods by introducing a Gaussian prior based on a vocal preset dataset, which helps to guide the optimisation of effect parameters. This results in more realistic audio effects by reducing the mean squared error of the parameters and better matching the reference style. The method shows significant improvements in performance metrics and subjective evaluations, especially when data is limited, highlighting the importance of incorporating prior knowledge in audio processing.'}, 'zh': {'title': 'ÂºïÂÖ•ÂÖàÈ™åÁü•ËØÜÔºåÊèêÂçáÈü≥È¢ëÈ£éÊ†ºËΩ¨ÁßªÊïàÊûú', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈü≥È¢ëÈ£éÊ†ºËΩ¨ÁßªÊñπÊ≥ïÔºåÁß∞‰∏∫Êé®ÁêÜÊó∂‰ºòÂåñÔºàST-ITOÔºâÔºåÊó®Âú®Â∞ÜÂèÇËÄÉÈü≥È¢ëÁöÑÊïàÊûúÂ∫îÁî®‰∫éÂéüÂßãÈü≥È¢ëËΩ®ÈÅì„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰ºòÂåñÊïàÊûúÂèÇÊï∞ÔºåÊúÄÂ∞èÂåñÂ§ÑÁêÜÂêéÈü≥È¢ë‰∏éÂèÇËÄÉÈü≥È¢ëÁöÑÈ£éÊ†ºÂµåÂÖ•‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÊñπÊ≥ïÂØπÊâÄÊúâÈÖçÁΩÆÁöÑÂ§ÑÁêÜÊòØÂπ≥Á≠âÁöÑÔºåÂèØËÉΩÂØºËá¥‰∏çÁúüÂÆûÊàñÊúâÂÅèÂ∑ÆÁöÑÁªìÊûú„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂ£∞‰πêÈ¢ÑËÆæÊï∞ÊçÆÈõÜDiffVoxÁöÑÈ´òÊñØÂÖàÈ™åÔºåÊîπËøõ‰∫ÜÂèÇÊï∞Á©∫Èó¥ÁöÑ‰ºòÂåñÔºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜÈü≥È¢ëÊïàÊûúËΩ¨ÁßªÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05678', 'title': 'InstanceGen: Image Generation with Instance-level Instructions', 'url': 'https://huggingface.co/papers/2505.05678', 'abstract': 'Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible fine-grained structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances.', 'score': 0, 'issue_id': 3840, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '201865e7cbd49316', 'authors': ['Etai Sella', 'Yanir Kleiman', 'Hadar Averbuch-Elor'], 'affiliations': ['Cornell University, USA', 'Meta AI, UK', 'Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.05678.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': 'üé®', 'ru': {'title': '–°—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Å–ª–æ–∂–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Å–æ—á–µ—Ç–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—Å–µ–º –∞—Å–ø–µ–∫—Ç–∞–º —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –≤–∫–ª—é—á–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤, –∞—Ç—Ä–∏–±—É—Ç—ã —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏.'}, 'en': {'title': 'Enhancing Image Generation with Structural Guidance and LLM Instructions', 'desc': "This paper addresses the limitations of current text-to-image generative models in understanding complex prompts that involve multiple objects and their attributes. The authors propose a novel approach that combines structural guidance from existing image generation models with instructions from large language models (LLMs). By doing so, they enhance the model's ability to generate images that accurately reflect the details specified in the prompts, such as the number of objects and their spatial relationships. This technique aims to improve the fidelity and coherence of generated images in response to intricate textual descriptions."}, 'zh': {'title': 'ÁªìÊûÑÊåáÂØº‰∏éÂÆû‰æãÊåá‰ª§ÁªìÂêàÁöÑÂõæÂÉèÁîüÊàêÊñ∞ÊñπÊ≥ï', 'desc': 'Â∞ΩÁÆ°ÁîüÊàêÊ®°ÂûãÁöÑËÉΩÂäõËøÖÈÄüÊèêÂçáÔºå‰ΩÜÈ¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÊèêÁ§∫Êó∂‰ªçÁÑ∂Â≠òÂú®Âõ∞ÈöæÔºåÂ∞§ÂÖ∂ÊòØÊ∂âÂèäÂ§ö‰∏™ÂØπË±°ÂíåÂÆû‰æãÂ±ûÊÄßÊó∂„ÄÇÂõ†Ê≠§ÔºåË∂äÊù•Ë∂äÂ§öÁöÑÁ†îÁ©∂ËÄÖÂºÄÂßãÂÖ≥Ê≥®ÈÄöËøáÊ∑ªÂä†ÁªìÊûÑÁ∫¶ÊùüÔºàÈÄöÂ∏∏ÊòØÁ≤óÁï•ÁöÑËæπÁïåÊ°ÜÔºâÊù•Êõ¥Â•ΩÂú∞ÊåáÂØºÁîüÊàêËøáÁ®ã„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÂèëÂ±ï‰∫ÜÁªìÊûÑÊåáÂØºÁöÑÁêÜÂøµÔºåËßÇÂØüÂà∞Áé∞‰ª£ÂõæÂÉèÁîüÊàêÊ®°ÂûãÂèØ‰ª•Áõ¥Êé•Êèê‰æõÂêàÁêÜÁöÑÁªÜÁ≤íÂ∫¶ÁªìÊûÑÂàùÂßãÂåñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊäÄÊúØÔºåÂ∞ÜÂü∫‰∫éÂõæÂÉèÁöÑÁªìÊûÑÊåáÂØº‰∏éÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆû‰æãÁ∫ßÊåá‰ª§Áõ∏ÁªìÂêàÔºå‰ªéËÄåÁîüÊàêÁ¨¶ÂêàÊñáÊú¨ÊèêÁ§∫ÊâÄÊúâÈÉ®ÂàÜÁöÑËæìÂá∫ÂõæÂÉèÔºåÂåÖÊã¨ÂØπË±°Êï∞Èáè„ÄÅÂÆû‰æãÂ±ûÊÄßÂíåÂÆû‰æã‰πãÈó¥ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09568', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset', 'url': 'https://huggingface.co/papers/2505.09568', 'abstract': 'Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.', 'score': 46, 'issue_id': 3768, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '822f8dd79d39211b', 'authors': ['Jiuhai Chen', 'Zhiyang Xu', 'Xichen Pan', 'Yushi Hu', 'Can Qin', 'Tom Goldstein', 'Lifu Huang', 'Tianyi Zhou', 'Saining Xie', 'Silvio Savarese', 'Le Xue', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['New York University', 'Salesforce Research', 'UC Davis', 'University of Maryland', 'University of Washington', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.09568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#open_source', '#multimodal', '#training', '#architecture'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã—Ö CLIP-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö VAE-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è: —Å–Ω–∞—á–∞–ª–∞ –Ω–∞ –∑–∞–¥–∞—á–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∑–∞—Ç–µ–º –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å BLIP3-o, –¥–æ—Å—Ç–∏–≥–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Unifying Image Understanding and Generation with BLIP3-o', 'desc': 'This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks.'}, 'zh': {'title': 'Áªü‰∏ÄÂõæÂÉèÁêÜËß£‰∏éÁîüÊàêÁöÑÂàõÊñ∞Ê®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂõæÂÉèÁêÜËß£‰∏éÁîüÊàêÁöÑÁªü‰∏ÄÊ®°ÂûãÔºåÂº∫Ë∞É‰∫ÜËá™ÂõûÂΩíÂíåÊâ©Êï£Ê®°ÂûãÂú®È´òË¥®ÈáèÁîüÊàê‰∏≠ÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºå‰ΩøÁî®Êâ©Êï£ÂèòÊç¢Âô®ÁîüÊàêËØ≠‰πâ‰∏∞ÂØåÁöÑCLIPÂõæÂÉèÁâπÂæÅÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÊïàÁéáÂíåÁîüÊàêË¥®Èáè„ÄÇÈÄöËøáÂÖàËøõË°åÂõæÂÉèÁêÜËß£ËÆ≠ÁªÉÔºåÂÜçËøõË°åÂõæÂÉèÁîüÊàêËÆ≠ÁªÉÁöÑÈ°∫Â∫èÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰øùÊåÅ‰∫ÜÂõæÂÉèÁêÜËß£ËÉΩÂäõÁöÑÂêåÊó∂Â¢ûÂº∫‰∫ÜÁîüÊàêËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂàõÂª∫‰∫ÜÈ´òË¥®ÈáèÁöÑÊåá‰ª§Ë∞É‰ºòÊï∞ÊçÆÈõÜBLIP3o-60kÔºå‰ª•ÊîØÊåÅÂõæÂÉèÁîüÊàê‰ªªÂä°ÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04410', 'title': 'DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception', 'url': 'https://huggingface.co/papers/2505.04410', 'abstract': "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.", 'score': 35, 'issue_id': 3774, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '24fee436fe24f861', 'authors': ['Junjie Wang', 'Bin Chen', 'Yulin Li', 'Bin Kang', 'Yichi Chen', 'Zhuotao Tian'], 'affiliations': ['International Research Institute for Artificial Intelligence, HIT, Shenzhen', 'School of Computer Science and Technology, HIT, Shenzhen', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.04410.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#cv', '#optimization', '#open_source'], 'emoji': 'üîç', 'ru': {'title': 'DeCLIP: –ù–æ–≤—ã–π —à–∞–≥ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ DeCLIP –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å –º–æ–¥—É–ª—å —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è CLIP –Ω–∞ '—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ' –∏ '–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ' –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑–ª–∏—á–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. DeCLIP –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è."}, 'en': {'title': 'Enhancing Dense Visual Predictions with DeCLIP', 'desc': "This paper introduces DeCLIP, a new framework designed to improve dense visual prediction tasks by enhancing the capabilities of Vision-Language Models (VLMs) like CLIP. The authors identify that CLIP's image tokens fail to effectively gather information from related regions, leading to poor local feature representation. DeCLIP addresses this by separating the self-attention mechanism into 'content' and 'context' features, which improves local discriminability and maintains spatial relationships. The results show that DeCLIP outperforms existing methods in various open-vocabulary tasks such as object detection and semantic segmentation."}, 'zh': {'title': 'DeCLIPÔºöÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØÜÈõÜÈ¢ÑÊµãËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂DeCLIPÔºåÊó®Âú®ÊîπÂñÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãCLIPÂú®ÂØÜÈõÜËßÜËßâÈ¢ÑÊµã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÂèëÁé∞CLIPÁöÑÂõæÂÉèÊ†áËÆ∞Âú®ËÅöÂêàÁ©∫Èó¥ÊàñËØ≠‰πâÁõ∏ÂÖ≥Âå∫ÂüüÁöÑ‰ø°ÊÅØÊó∂Â≠òÂú®Âõ∞ÈöæÔºåÂØºËá¥ÁâπÂæÅÁº∫‰πèÂ±ÄÈÉ®ÂèØÂå∫ÂàÜÊÄßÂíåÁ©∫Èó¥‰∏ÄËá¥ÊÄß„ÄÇDeCLIPÈÄöËøáËß£ËÄ¶Ëá™Ê≥®ÊÑèÂäõÊ®°ÂùóÔºåÂàÜÂà´ÊèêÂèñ‚ÄúÂÜÖÂÆπ‚ÄùÂíå‚Äú‰∏ä‰∏ãÊñá‚ÄùÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÂ±ÄÈÉ®ÂèØÂå∫ÂàÜÊÄßÂπ∂‰øùÊåÅÁ©∫Èó¥Áõ∏ÂÖ≥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDeCLIPÂú®Â§ö‰∏™ÂºÄÊîæËØçÊ±áÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09343', 'title': 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures', 'url': 'https://huggingface.co/papers/2505.09343', 'abstract': "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.", 'score': 24, 'issue_id': 3773, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '3c249078ec32a334', 'authors': ['Chenggang Zhao', 'Chengqi Deng', 'Chong Ruan', 'Damai Dai', 'Huazuo Gao', 'Jiashi Li', 'Liyue Zhang', 'Panpan Huang', 'Shangyan Zhou', 'Shirong Ma', 'Wenfeng Liang', 'Ying He', 'Yuqing Wang', 'Yuxuan Liu', 'Y. X. Wei'], 'affiliations': ['DeepSeek-AI Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09343.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ DeepSeek-V3/R1 –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –ò–ò, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è Multi-head Latent Attention (MLA) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Mixture of Experts (MoE) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏ –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π. –í —Ä–∞–±–æ—Ç–µ —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ FP8 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–Ω–æ–≥–æ–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π —Å–µ—Ç–µ–≤–æ–π —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–∞. –ù–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ DeepSeek-V3 –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–ª—è –ò–ò.'}, 'en': {'title': 'Innovating AI: Bridging Hardware and Model Design for Scalable Solutions', 'desc': 'This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design.'}, 'zh': {'title': 'Á°¨‰ª∂‰∏éÊ®°ÂûãÂÖ±ÂêåËÆæËÆ°ÔºåÊé®Âä®AIÂàõÊñ∞', 'desc': 'ËøôÁØáËÆ∫ÊñáËÆ®ËÆ∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Á°¨‰ª∂Êû∂ÊûÑ‰∏äÁöÑÈôêÂà∂ÔºåÂåÖÊã¨ÂÜÖÂ≠òÂÆπÈáè„ÄÅËÆ°ÁÆóÊïàÁéáÂíå‰∫íËøûÂ∏¶ÂÆΩÁ≠âÈóÆÈ¢ò„ÄÇDeepSeek-V3Ê®°ÂûãÂú®2048‰∏™NVIDIA H800 GPU‰∏äËÆ≠ÁªÉÔºåÂ±ïÁ§∫‰∫ÜÁ°¨‰ª∂ÊÑüÁü•Ê®°ÂûãÂÖ±ÂêåËÆæËÆ°Â¶Ç‰ΩïÊúâÊïàËß£ÂÜ≥Ëøô‰∫õÊåëÊàò„ÄÇËÆ∫ÊñáÂàÜÊûê‰∫ÜDeepSeek-V3/R1Ê®°ÂûãÊû∂ÊûÑÂèäÂÖ∂AIÂü∫Á°ÄËÆæÊñΩÔºå‰ªãÁªç‰∫ÜÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõÔºàMLAÔºâ„ÄÅ‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊû∂ÊûÑÂíåFP8Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÁ≠âÂàõÊñ∞„ÄÇÊúÄÂêéÔºå‰ΩúËÄÖ‰∏éÂ≠¶ÊúØÁïåÂíåÂ∑•‰∏öÁïåÂêåË°åÊé¢ËÆ®‰∫ÜÊú™Êù•Á°¨‰ª∂ÁöÑÂèëÂ±ïÊñπÂêëÔºåÂº∫Ë∞É‰∫ÜÁ°¨‰ª∂‰∏éÊ®°ÂûãÂÖ±ÂêåËÆæËÆ°Âú®Êª°Ë∂≥AIÂ∑•‰ΩúË¥üËΩΩÈúÄÊ±Ç‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09358', 'title': 'Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\n  Image Analysis', 'url': 'https://huggingface.co/papers/2505.09358', 'abstract': "The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io", 'score': 13, 'issue_id': 3777, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '88afa5d56831ceb4', 'authors': ['Bingxin Ke', 'Kevin Qu', 'Tianfu Wang', 'Nando Metzger', 'Shengyu Huang', 'Bo Li', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2505.09358.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#synthetic', '#transfer_learning', '#training'], 'emoji': 'üåº', 'ru': {'title': 'Marigold: –†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Marigold - —Å–µ–º–µ–π—Å—Ç–≤–æ —É—Å–ª–æ–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–æ—Ç–æ–∫–æ–ª –¥–æ–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç –∑–Ω–∞–Ω–∏—è –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ Stable Diffusion. Marigold –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —ç—Ç–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á –ø–ª–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –æ—Ü–µ–Ω–∫—É –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–µ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é. –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–¥–Ω–æ–º GPU –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π. Marigold –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤ —Ä–µ–∂–∏–º–µ zero-shot.'}, 'en': {'title': 'Unlocking Image Analysis with Pretrained Generative Models', 'desc': 'This paper introduces Marigold, a set of conditional generative models designed to leverage pretrained latent diffusion models for various dense image analysis tasks. By fine-tuning these models, Marigold can effectively adapt to tasks like monocular depth estimation and surface normals prediction with minimal changes to the original architecture. The approach allows for training on small synthetic datasets, making it efficient and accessible for users with limited resources. Notably, Marigold achieves impressive zero-shot generalization, showcasing its potential in data-scarce environments.'}, 'zh': {'title': 'MarigoldÔºöÈ´òÊïàÁöÑÂõæÂÉèÂàÜÊûêÁîüÊàêÊ®°Âûã', 'desc': 'Ê∑±Â∫¶Â≠¶‰π†Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÁöÑÊàêÂäü‰æùËµñ‰∫éÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÈõÜÂíåÂº∫Â§ßÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÂú®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåËøô‰∫õÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑË¥®ÈáèÂØπÊúâÊïàÁöÑËøÅÁßªÂ≠¶‰π†Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊúÄËøëÔºåÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÁîüÊàêÊ®°ÂûãÔºåÁâπÂà´ÊòØ‰ΩøÁî®ÂéªÂô™Êâ©Êï£ÁöÑÊΩúÂú®Á©∫Èó¥Ê®°ÂûãÔºåÂºÄÂàõ‰∫Ü‰∏ÄÁ±ªÊñ∞ÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ßÈáèÂ∏¶Ê≥®ÈáäÁöÑÂõæÂÉèÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜMarigoldÔºå‰∏Ä‰∏™Êù°‰ª∂ÁîüÊàêÊ®°ÂûãÁöÑÂÆ∂ÊóèÂèäÂÖ∂ÂæÆË∞ÉÂçèËÆÆÔºåËÉΩÂ§üÊèêÂèñÈ¢ÑËÆ≠ÁªÉÊΩúÂú®Êâ©Êï£Ê®°ÂûãÁöÑÁü•ËØÜÔºåÂπ∂Â∞ÜÂÖ∂ÈÄÇÂ∫î‰∫éÂØÜÈõÜÂõæÂÉèÂàÜÊûê‰ªªÂä°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08787', 'title': 'UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations', 'url': 'https://huggingface.co/papers/2505.08787', 'abstract': 'Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.', 'score': 12, 'issue_id': 3779, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '385e98801f0b0c4a', 'authors': ['Hanjung Kim', 'Jaehyun Kang', 'Hyolim Kang', 'Meedeum Cho', 'Seon Joo Kim', 'Youngwoon Lee'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08787.jpg', 'data': {'categories': ['#video', '#transfer_learning', '#dataset', '#robotics', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': 'UniSkill: –û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –Ω–∞–≤—ã–∫–∞–º –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω UniSkill - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞–≤—ã–∫–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏. –û–Ω —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤, –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –æ—Ç –≤–æ–ø–ª–æ—â–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞—Å—à—Ç–∞–±–Ω—ã–µ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏. UniSkill –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –Ω–∞–≤—ã–∫–∏, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏, –Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–æ–±–æ—Ç–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —É—Å–ø–µ—à–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è —Ä–æ–±–æ—Ç–æ–≤ –¥–∞–∂–µ –¥–ª—è –Ω–æ–≤—ã—Ö –≤–∏–¥–µ–æ–ø—Ä–æ–º–ø—Ç–æ–≤.'}, 'en': {'title': 'Bridging Human-Robot Learning with UniSkill', 'desc': 'This paper introduces UniSkill, a framework designed to help robots learn skills by observing human actions without needing labeled data. It addresses the challenge of differences in appearance and capabilities between humans and robots by using large-scale video data that captures both. UniSkill creates skill representations that are not tied to any specific embodiment, allowing robots to apply learned skills from human videos to their own tasks. The results demonstrate that robots can effectively choose actions based on human video prompts, even when those prompts are new to them.'}, 'zh': {'title': 'Ë∑®‰ΩìÁé∞ÊäÄËÉΩÂ≠¶‰π†ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Ê®°‰ªøÊòØ‰∫∫Á±ªÂ≠¶‰π†Êñ∞‰ªªÂä°ÁöÑÂü∫Êú¨Êú∫Âà∂ÔºåÈÄöËøáËßÇÂØüÂíåÊ®°‰ªø‰∏ìÂÆ∂Êù•Â≠¶‰π†„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜËøôÁßçËÉΩÂäõÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Èù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºåÂõ†‰∏∫‰∫∫Á±ªÂíåÊú∫Âô®‰∫∫ÁöÑÂ§ñËßÇÂíåÁâ©ÁêÜËÉΩÂäõÂ≠òÂú®Âõ∫ÊúâÂ∑ÆÂºÇ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜUniSkillÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•‰ªéÂ§ßËßÑÊ®°ÁöÑË∑®‰ΩìÁé∞ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠Â≠¶‰π†‰∏é‰ΩìÁé∞Êó†ÂÖ≥ÁöÑÊäÄËÉΩË°®Á§∫ÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïÊ†áÁ≠æÔºå‰ªéËÄå‰Ωø‰ªé‰∫∫Á±ªËßÜÈ¢ëÊèêÁ§∫‰∏≠ÊèêÂèñÁöÑÊäÄËÉΩËÉΩÂ§üÊúâÊïàËΩ¨ÁßªÂà∞‰ªÖÂú®Êú∫Âô®‰∫∫Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÊú∫Âô®‰∫∫Á≠ñÁï•‰∏≠„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËøô‰∫õË∑®‰ΩìÁé∞ÊäÄËÉΩËÉΩÂ§üÊàêÂäüÊåáÂØºÊú∫Âô®‰∫∫ÈÄâÊã©ÂêàÈÄÇÁöÑÂä®‰ΩúÔºåÂç≥‰ΩøÂú®Èù¢ÂØπÊú™ËßÅËøáÁöÑËßÜÈ¢ëÊèêÁ§∫Êó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07849', 'title': 'SweRank: Software Issue Localization with Code Ranking', 'url': 'https://huggingface.co/papers/2505.07849', 'abstract': "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community.", 'score': 6, 'issue_id': 3777, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': 'b9c609d9756c1056', 'authors': ['Revanth Gangi Reddy', 'Tarun Suresh', 'JaeHyeok Doo', 'Ye Liu', 'Xuan Phi Nguyen', 'Yingbo Zhou', 'Semih Yavuz', 'Caiming Xiong', 'Heng Ji', 'Shafiq Joty'], 'affiliations': ['KAIST AI', 'Salesforce Research', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07849.jpg', 'data': {'categories': ['#data', '#benchmark', '#optimization', '#dataset', '#survey', '#agents'], 'emoji': 'üîç', 'ru': {'title': 'SweRank: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º –≤ –∫–æ–¥–µ —Å –ø–æ–º–æ—â—å—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'SweRank - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–±–ª–µ–º –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –ø–æ–¥—Ö–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç SweLoc, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ–ø–∏—Å–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ GitHub. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SweRank –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–∞–∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫ –∏ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≥–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –∑–∞–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. SweLoc —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–≤–æ—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∑–∞–¥–∞—á–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–±–ª–µ–º.'}, 'en': {'title': 'SweRank: Efficient Software Issue Localization with SweLoc Dataset', 'desc': 'This paper presents SweRank, a new framework designed to improve software issue localization by efficiently retrieving and re-ranking code relevant to natural language issue descriptions. Unlike traditional models that struggle with the complexity of verbose queries, SweRank leverages a large-scale dataset called SweLoc, which contains real-world issue descriptions and their corresponding code changes. The empirical results indicate that SweRank outperforms existing models, including those based on costly closed-source large language models, in terms of accuracy and efficiency. This work not only introduces a novel approach to issue localization but also provides a valuable dataset for future research in the field.'}, 'zh': {'title': 'È´òÊïàËΩØ‰ª∂ÈóÆÈ¢òÂÆö‰ΩçÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ËΩØ‰ª∂ÈóÆÈ¢òÂÆö‰ΩçÊòØËØÜÂà´‰∏éËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢òÊèèËø∞ÔºàÂ¶ÇÈîôËØØÊä•Âëä„ÄÅÂäüËÉΩËØ∑Ê±ÇÔºâÁõ∏ÂÖ≥ÁöÑ‰ª£Á†Å‰ΩçÁΩÆÔºàÊñá‰ª∂„ÄÅÁ±ªÊàñÂáΩÊï∞ÔºâÁöÑ‰ªªÂä°ÔºåËøôÂú®ËΩØ‰ª∂ÂºÄÂèë‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å‰ΩÜËÄóÊó∂„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑ‰ª£ÁêÜÊñπÊ≥ïÊòæÁ§∫Âá∫ÊΩúÂäõÔºå‰ΩÜÁî±‰∫éÂ§çÊùÇÁöÑÂ§öÊ≠•È™§Êé®ÁêÜÂíå‰æùËµñ‰∫éÂ∞ÅÈó≠Ê∫êLLMÔºåÂæÄÂæÄ‰ºöÂØºËá¥ÊòæËëóÁöÑÂª∂ËøüÂíåÊàêÊú¨„ÄÇ‰º†ÁªüÁöÑ‰ª£Á†ÅÊéíÂêçÊ®°ÂûãÈÄöÂ∏∏ÈíàÂØπÊü•ËØ¢Âà∞‰ª£Á†ÅÊàñ‰ª£Á†ÅÂà∞‰ª£Á†ÅÁöÑÊ£ÄÁ¥¢ËøõË°å‰ºòÂåñÔºå‰ΩÜÂú®Â§ÑÁêÜÂÜóÈïøÂíåÊèèËø∞ÊÄßÂ§±Ë¥•ÁöÑÂÆö‰ΩçÊü•ËØ¢Êó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSweRankÔºå‰∏Ä‰∏™È´òÊïà‰∏îÊúâÊïàÁöÑËΩØ‰ª∂ÈóÆÈ¢òÂÆö‰ΩçÊ£ÄÁ¥¢‰∏éÈáçÊéíÂêçÊ°ÜÊû∂ÔºåÂπ∂ÊûÑÂª∫‰∫ÜSweLocÔºå‰∏Ä‰∏™Êù•Ëá™ÂÖ¨ÂÖ±GitHubÂ∫ìÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂåÖÂê´ÁúüÂÆûÈóÆÈ¢òÊèèËø∞ÂèäÁõ∏Â∫îÁöÑ‰ª£Á†Å‰øÆÊîπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09558', 'title': 'WavReward: Spoken Dialogue Models With Generalist Reward Evaluators', 'url': 'https://huggingface.co/papers/2505.09558', 'abstract': "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by a margin of 83%. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.", 'score': 5, 'issue_id': 3783, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '74f9831e9fa216f6', 'authors': ['Shengpeng Ji', 'Tianle Liang', 'Yangzhuo Li', 'Jialong Zuo', 'Minghui Fang', 'Jinzheng He', 'Yifu Chen', 'Zhengqing Liu', 'Ziyue Jiang', 'Xize Cheng', 'Siqi Zheng', 'Jin Xu', 'Junyang Lin', 'Zhou Zhao'], 'affiliations': ['Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.09558.jpg', 'data': {'categories': ['#audio', '#reasoning', '#open_source', '#rlhf', '#dataset', '#benchmark'], 'emoji': 'üéôÔ∏è', 'ru': {'title': 'WavReward: –ø—Ä–æ—Ä—ã–≤ –≤ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WavReward - –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ. –≠—Ç–∞ –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å —Ä–µ—á–µ–≤—ã–º –≤–≤–æ–¥–æ–º. WavReward –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—É–¥–∏–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç ChatReward-30K, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –∏ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏.'}, 'en': {'title': 'WavReward: Revolutionizing Evaluation for Spoken Dialogue Systems', 'desc': 'This paper introduces WavReward, a novel evaluation model designed specifically for spoken dialogue systems. Unlike traditional text-based models, WavReward assesses both the intelligence (IQ) and emotional understanding (EQ) of dialogue systems using audio inputs. It employs a reinforcement learning approach with multi-sample feedback to enhance its evaluation capabilities. The model demonstrates significant improvements in accuracy and user preference over existing evaluation methods, showcasing its effectiveness in various dialogue scenarios.'}, 'zh': {'title': 'WavRewardÔºöÊèêÂçáÂØπËØùÁ≥ªÁªüËØÑ‰º∞ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞Ê®°ÂûãWavRewardÔºåÁî®‰∫éËØÑ‰º∞Âü∫‰∫éÈü≥È¢ëÁöÑÂØπËØùÁ≥ªÁªüÁöÑË°®Áé∞„ÄÇWavRewardÁªìÂêà‰∫ÜÊ∑±Â∫¶Êé®ÁêÜËøáÁ®ãÂíåÈùûÁ∫øÊÄßÂ•ñÂä±Êú∫Âà∂ÔºåËÉΩÂ§üÂêåÊó∂ËØÑ‰º∞ÂØπËØùÁ≥ªÁªüÁöÑÊô∫ÂïÜÂíåÊÉÖÂïÜ„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜChatReward-30KÊï∞ÊçÆÈõÜÔºåÁî®‰∫éËÆ≠ÁªÉWavRewardÔºåÊ∂µÁõñ‰∫ÜÂØπËØùÊ®°ÂûãÁöÑÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWavRewardÂú®Â§ö‰∏™ÂØπËØùÂú∫ÊôØ‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑËØÑ‰º∞Ê®°ÂûãÔºåÊèêÂçá‰∫ÜÂÆ¢ËßÇÂáÜÁ°ÆÁéáÂíå‰∏ªËßÇÊµãËØïÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12894', 'title': 'CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image', 'url': 'https://huggingface.co/papers/2502.12894', 'abstract': "Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.", 'score': 5, 'issue_id': 3779, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'aeb3d023043e8a18', 'authors': ['Kaixin Yao', 'Longwen Zhang', 'Xinhao Yan', 'Yan Zeng', 'Qixuan Zhang', 'Wei Yang', 'Lan Xu', 'Jiayuan Gu', 'Jingyi Yu'], 'affiliations': ['Deemos Technology Co., Ltd., China', 'Huazhong University of Science and Technology, China', 'ShanghaiTech University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12894.jpg', 'data': {'categories': ['#3d', '#graphs', '#robotics', '#optimization'], 'emoji': 'üèôÔ∏è', 'ru': {'title': '–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 3D-—Å—Ü–µ–Ω —Å —É—á–µ—Ç–æ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ —Ñ–∏–∑–∏–∫–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è', 'desc': 'CAST - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é GPT –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ–ª–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞. CAST –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å—Ü–µ–Ω–µ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Ö –ø–æ–∑–∏—Ü–∏–π. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–∫–∫–ª—é–∑–∏–π, –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –≤ —Å—Ü–µ–Ω–µ.'}, 'en': {'title': 'CAST: Transforming 2D Images into Coherent 3D Scenes', 'desc': "The paper presents CAST, a new method for reconstructing high-quality 3D scenes from a single RGB image. It begins by extracting 2D segmentations and depth information, then uses a GPT-based model to analyze how objects relate spatially within the scene. CAST generates each object's geometry while addressing occlusions and partial data through a large-scale 3D generation model. Finally, it ensures physical consistency in the scene by optimizing object poses with a physics-aware correction step, making it useful for applications in robotics."}, 'zh': {'title': 'CASTÔºö‰ªéÂçïÂº†RGBÂõæÂÉèÈáçÂª∫È´òË¥®Èáè3DÂú∫ÊôØÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CASTÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫é‰ªéÂçïÂº†RGBÂõæÂÉè‰∏≠ÈáçÂª∫È´òË¥®ÈáèÁöÑ3DÂú∫ÊôØ„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÊèêÂèñÂØπË±°Á∫ßÁöÑ2DÂàÜÂâ≤ÂíåÁõ∏ÂØπÊ∑±Â∫¶‰ø°ÊÅØÔºåÁÑ∂ÂêéÂà©Áî®Âü∫‰∫éGPTÁöÑÊ®°ÂûãÂàÜÊûêÂØπË±°‰πãÈó¥ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÔºå‰ª•ÂÆûÁé∞Êõ¥ËøûË¥ØÁöÑÈáçÂª∫„ÄÇCASTËøòÈááÁî®‰∫Ü‰∏Ä‰∏™ËÄÉËôëÈÅÆÊå°ÁöÑÂ§ßËßÑÊ®°3DÁîüÊàêÊ®°ÂûãÔºåÁã¨Á´ãÁîüÊàêÊØè‰∏™ÂØπË±°ÁöÑÂÆåÊï¥Âá†‰ΩïÂΩ¢Áä∂ÔºåÂπ∂ÈÄöËøáMAEÂíåÁÇπ‰∫ëÊù°‰ª∂Êù•ÂáèËΩªÈÅÆÊå°ÂíåÈÉ®ÂàÜÂØπË±°‰ø°ÊÅØÁöÑÂΩ±Âìç„ÄÇÊúÄÂêéÔºåCASTÈÄöËøáÁâ©ÁêÜÊÑüÁü•ÁöÑÊ†°Ê≠£Ê≠•È™§ÔºåÁ°Æ‰øùÁîüÊàêÁöÑÂú∫ÊôØÂú®Áâ©ÁêÜ‰∏ä‰øùÊåÅ‰∏ÄËá¥ÔºåÈÄÇÁî®‰∫éÊú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09439', 'title': 'Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?', 'url': 'https://huggingface.co/papers/2505.09439', 'abstract': 'We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.', 'score': 4, 'issue_id': 3778, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '95b580ea2dcf9967', 'authors': ['Andrew Rouditchenko', 'Saurabhchand Bhati', 'Edson Araujo', 'Samuel Thomas', 'Hilde Kuehne', 'Rogerio Feris', 'James Glass'], 'affiliations': ['Goethe University of Frankfurt', 'IBM Research AI', 'MIT CSAIL', 'MIT-IBM Watson AI Lab', 'Tuebingen AI Center/University of Tuebingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.09439.jpg', 'data': {'categories': ['#training', '#multimodal', '#rl', '#reasoning', '#optimization', '#benchmark', '#rag'], 'emoji': 'üéß', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—É–¥–∏–æ-–ò–ò: Omni-R1 –ø–æ–∫–æ—Ä—è–µ—Ç –Ω–æ–≤—ã–µ –≤–µ—Ä—à–∏–Ω—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Omni-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, —É–ª—É—á—à–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ MMAU –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é –∑–≤—É–∫–æ–≤, –º—É–∑—ã–∫–∏ –∏ —Ä–µ—á–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–≤—è–∑–∞–Ω–æ —Å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –∞—É–¥–∏–æ —Ç–∞–∫–∂–µ –ø–æ–≤—ã—Å–∏–ª–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∞—É–¥–∏–æ–∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Enhancing Audio Understanding through Text-Based Reasoning', 'desc': "The paper introduces Omni-R1, a model that enhances the Qwen2.5-Omni multi-modal large language model (LLM) by fine-tuning it on an audio question answering dataset using the GRPO reinforcement learning method. This approach achieves state-of-the-art results on the MMAU benchmark, excelling in categories such as sounds, music, and speech. The authors found that the performance gains were largely due to improved text-based reasoning capabilities, even when audio data was not used during fine-tuning. Interestingly, they discovered that training on a text-only dataset could still boost the model's performance on audio tasks."}, 'zh': {'title': 'Omni-R1ÔºöÈü≥È¢ëÈóÆÁ≠îÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜOmni-R1ÔºåÂÆÉÂú®Èü≥È¢ëÈóÆÁ≠îÊï∞ÊçÆÈõÜ‰∏äÂØπÊúÄÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãQwen2.5-OmniËøõË°å‰∫ÜÂæÆË∞ÉÔºåÈááÁî®‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïGRPO„ÄÇËøô‰ΩøÂæóOmni-R1Âú®ÊúÄËøëÁöÑMMAUÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊÄßËÉΩ„ÄÇOmni-R1Âú®Â£∞Èü≥„ÄÅÈü≥‰πê„ÄÅËØ≠Èü≥ÂíåÊï¥‰ΩìÂπ≥ÂùáÁ±ªÂà´‰∏äÈÉΩÂèñÂæó‰∫ÜÊúÄÈ´òÁöÑÂáÜÁ°ÆÁéáÔºåÊó†ËÆ∫ÊòØÂú®Test-miniËøòÊòØTest-fullÂàÜÂâ≤‰∏ä„ÄÇÊàë‰ª¨ËøòÂèëÁé∞ÔºåÂú®Ê≤°ÊúâÈü≥È¢ëÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÂæÆË∞ÉÔºåÁ´üÁÑ∂‰πüËÉΩÊúâÊïàÊèêÈ´òÈü≥È¢ëÁõ∏ÂÖ≥ÁöÑÊÄßËÉΩÔºåËøôË°®ÊòéÊñáÊú¨Êé®ÁêÜËÉΩÂäõÁöÑÊèêÂçáÂØπÊï¥‰ΩìË°®Áé∞ÊúâÈáçË¶ÅÂΩ±Âìç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08455', 'title': 'VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models', 'url': 'https://huggingface.co/papers/2505.08455', 'abstract': 'Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.', 'score': 4, 'issue_id': 3773, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': 'b7bc69bb40029690', 'authors': ['Pritam Sarkar', 'Ali Etemad'], 'affiliations': ['Queens University, Canada', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.08455.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#long_context', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VCRBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ë–æ–ª—å—à–∏—Ö –í–∏–¥–µ–æ-–Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥–µ–ª–µ–π (LVLM) –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. VCRBench –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π —Å –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è. –û—Ü–µ–Ω–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LVLM –Ω–∞ VCRBench –ø–æ–∫–∞–∑–∞–ª–∞ –∏—Ö —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–æ–¥—É–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ Recognition-Reasoning Decomposition (RRD), –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏–ª —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ VCRBench.'}, 'en': {'title': 'Enhancing Video Causal Reasoning with RRD', 'desc': 'This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëÂõ†ÊûúÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Â∞ΩÁÆ°ËßÜÈ¢ëÁêÜËß£ÊäÄÊúØÊúâÊâÄËøõÊ≠•Ôºå‰ΩÜÂ§ßÂûãËßÜÈ¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®ËßÜÈ¢ëÂü∫Á°ÄÁöÑÂõ†ÊûúÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÂêç‰∏∫ËßÜÈ¢ëÂü∫Á°ÄÁöÑÈïøÂΩ¢ÂºèÂõ†ÊûúÊé®ÁêÜÔºàVCRBenchÔºâÔºåÈÄöËøáÂØπÊó•Â∏∏Ê¥ªÂä®ÁöÑËßÜÈ¢ëËøõË°åÂ§ÑÁêÜÔºåÊµãËØïLVLMsËÉΩÂê¶ËØÜÂà´„ÄÅÊé®ÁêÜÂπ∂Ê≠£Á°ÆÊéíÂ∫èÂÆûÁé∞ÁâπÂÆöÁõÆÊ†áÊâÄÈúÄÁöÑ‰∫ã‰ª∂„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ®°ÂùóÂåñÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ËØÜÂà´-Êé®ÁêÜÂàÜËß£ÔºàRRDÔºâÔºåÂ∞ÜËßÜÈ¢ëÂü∫Á°ÄÁöÑÂõ†ÊûúÊé®ÁêÜÂàÜ‰∏∫ËßÜÈ¢ëËØÜÂà´ÂíåÂõ†ÊûúÊé®ÁêÜ‰∏§‰∏™Â≠ê‰ªªÂä°Ôºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåLVLMsÂú®Â§çÊùÇÁöÑÈïøÂΩ¢ÂºèÂõ†ÊûúÊé®ÁêÜ‰ªªÂä°‰∏≠‰∏ªË¶Å‰æùËµñËØ≠Ë®ÄÁü•ËØÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09608', 'title': 'LightLab: Controlling Light Sources in Images with Diffusion Models', 'url': 'https://huggingface.co/papers/2505.09608', 'abstract': 'We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.', 'score': 3, 'issue_id': 3785, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': 'dbbc9a09c87e94d0', 'authors': ['Nadav Magar', 'Amir Hertz', 'Eric Tabellion', 'Yael Pritch', 'Alex Rav-Acha', 'Ariel Shamir', 'Yedid Hoshen'], 'affiliations': ['Google, Israel', 'Google, United States of America', 'Hebrew University of Jerusalem and Google, Israel', 'Reichman University and Google, Israel', 'Tel Aviv University and Google, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.09608.jpg', 'data': {'categories': ['#cv', '#diffusion', '#synthetic', '#data', '#dataset', '#training'], 'emoji': 'üí°', 'ru': {'title': '–ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ–º —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–≤–µ—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä–∞—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –∏ —Ü–≤–µ—Ç –æ—Å–≤–µ—â–µ–Ω–∏—è. –ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫, –æ–Ω –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Å–≤–µ—â–µ–Ω–∏—è.'}, 'en': {'title': 'Mastering Light: Fine-Grained Control with Diffusion Models', 'desc': 'This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques.'}, 'zh': {'title': 'Á≤æÁ°ÆÊéßÂà∂ÂÖâÊ∫êÁöÑÊâ©Êï£ÈáçÂÖâÁÖßÊñπÊ≥ï', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂØπÂõæÂÉè‰∏≠ÁöÑÂÖâÊ∫êËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÂèÇÊï∞ÊéßÂà∂„ÄÇÁé∞ÊúâÁöÑÈáçÂÖâÁÖßÊñπÊ≥ïË¶Å‰πà‰æùËµñÂ§ö‰∏™ËæìÂÖ•ËßÜÂõæËøõË°åÈÄÜÂêëÊ∏≤ÊüìÔºåË¶Å‰πàÊó†Ê≥ïÊèê‰æõÂØπÂÖâÂèòÂåñÁöÑÊòéÁ°ÆÊéßÂà∂„ÄÇÊàë‰ª¨ÈÄöËøáÂú®‰∏ÄÂ∞èÁªÑÁúüÂÆûÂéüÂßãÁÖßÁâáÂØπ‰∏äÂæÆË∞ÉÊâ©Êï£Ê®°ÂûãÔºåÂπ∂ÁªìÂêàÂ§ßËßÑÊ®°ÂêàÊàêÊ∏≤ÊüìÂõæÂÉèÔºåÊù•ÂºïÂØºÂÖ∂Âú®ÈáçÂÖâÁÖßÊñπÈù¢ÁöÑÁúüÂÆûÊÑüÂÖàÈ™å„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜËØ•ÊñπÊ≥ïÂú®ÂÖâÁºñËæëÁªìÊûú‰∏äÁöÑ‰ºòË∂äÊÄßÔºåË∂ÖË∂ä‰∫ÜÂü∫‰∫éÁî®Êà∑ÂÅèÂ•ΩÁöÑÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04793', 'title': 'DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\n  Recognition', 'url': 'https://huggingface.co/papers/2505.04793', 'abstract': 'Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/', 'score': 2, 'issue_id': 3778, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '76d34246ac682dfd', 'authors': ['Kailash A. Hambarde', 'Nzakiese Mbongo', 'Pavan Kumar MP', 'Satish Mekewad', 'Carolina Fernandes', 'G√∂khan Silahtaroƒülu', 'Alice Nithya', 'Pawan Wasnik', 'MD. Rashidunnabi', 'Pranita Samale', 'Hugo Proen√ßa'], 'affiliations': ['Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal', 'Istanbul Medipol University, Istanbul, Turkey', 'J.N.N. College of Engineering, Shivamogga, Karnataka, India', 'SRM Institute of Science and Technology, Kattankulathur, India', 'School of Computational Sciences, SRTM University, Nanded, India'], 'pdf_title_img': 'assets/pdf/title_img/2505.04793.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic'], 'emoji': 'üéØ', 'ru': {'title': 'DetReIDX: –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –¥–ª—è —Ä–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DetReIDX - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π –≤ –≤–æ–∑–¥—É—à–Ω–æ-–Ω–∞–∑–µ–º–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 13 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ –¥–ª—è 509 –ª–∏—á–Ω–æ—Å—Ç–µ–π, —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤ —Å–µ–º–∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏—Ö –∫–∞–º–ø—É—Å–∞—Ö –Ω–∞ —Ç—Ä–µ—Ö –∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç–∞—Ö, —Å –≤—ã—Å–æ—Ç–æ–π —Å—ä–µ–º–∫–∏ –æ—Ç 5,8 –¥–æ 120 –º–µ—Ç—Ä–æ–≤. DetReIDX —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Ä–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –≤–∫–ª—é—á–∞—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ–¥–µ–∂–¥—ã, –æ—Å–≤–µ—â–µ–Ω–∏—è –∏ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ —Ä–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Ö—É–¥—à–∞—é—Ç —Å–≤–æ—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏ DetReIDX.'}, 'en': {'title': 'DetReIDX: A Game-Changer for Real-World Person ReID Challenges', 'desc': 'This paper presents DetReIDX, a new dataset designed to improve person reidentification (ReID) technology in real-world scenarios. It addresses the limitations of existing datasets by incorporating significant variability factors such as changes in clothing, viewpoint, and environmental conditions. DetReIDX includes over 13 million bounding boxes from 509 identities, collected across multiple sessions in diverse locations, making it a comprehensive resource for evaluating ReID systems. The authors demonstrate that current state-of-the-art methods struggle significantly under the conditions presented by DetReIDX, highlighting the need for robust solutions in person reidentification.'}, 'zh': {'title': 'DetReIDXÔºöÁúüÂÆû‰∏ñÁïåË°å‰∫∫ÈáçËØÜÂà´ÁöÑÊñ∞ÊåëÊàò', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫DetReIDXÁöÑÂ§ßËßÑÊ®°Á©∫‰∏≠-Âú∞Èù¢Ë°å‰∫∫ÈáçËØÜÂà´ÔºàReIDÔºâÊï∞ÊçÆÈõÜÔºåÊó®Âú®ÊµãËØïReIDÊäÄÊúØÂú®ÁúüÂÆû‰∏ñÁïåÊù°‰ª∂‰∏ãÁöÑË°®Áé∞„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´Êù•Ëá™509‰∏™Ë∫´‰ªΩÁöÑ1300Â§ö‰∏á‰∏™ËæπÁïåÊ°ÜÔºåÊï∞ÊçÆÊî∂ÈõÜËá™‰∏â‰∏™Â§ßÊ¥≤ÁöÑ‰∏É‰∏™Â§ßÂ≠¶Ê†°Âõ≠ÔºåÊ∂µÁõñ‰∫Ü‰∏çÂêåÁöÑÊúçË£Ö„ÄÅÂÖâÁÖßÂíå‰ΩçÁΩÆÂèòÂåñ„ÄÇDetReIDXÁöÑÁã¨Áâπ‰πãÂ§ÑÂú®‰∫éÔºåÂÆÉËÆ∞ÂΩï‰∫ÜËá≥Â∞ë‰∏§‰∏™‰∏çÂêåÊó•ÊúüÁöÑ‰ºöËØùÔºåËÉΩÂ§üÊúâÊïàËØÑ‰º∞ÈïøÊúüË°å‰∫∫ÈáçËØÜÂà´ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂú®DetReIDXÊù°‰ª∂‰∏ãÁöÑË°®Áé∞ÊòæËëó‰∏ãÈôçÔºåÊ£ÄÊµãÂáÜÁ°ÆÁéá‰∏ãÈôçÈ´òËææ80%ÔºåRank-1 ReID‰∏ãÈôçË∂ÖËøá70%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08084', 'title': 'Visually Interpretable Subtask Reasoning for Visual Question Answering', 'url': 'https://huggingface.co/papers/2505.08084', 'abstract': "Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.", 'score': 1, 'issue_id': 3779, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '3bcc5517eeabe122', 'authors': ['Yu Cheng', 'Arushi Goel', 'Hakan Bilen'], 'affiliations': ['NVIDIA', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.08084.jpg', 'data': {'categories': ['#training', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': 'üß†', 'ru': {'title': 'VISTAR: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VISTAR - –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–∑–∞–¥–∞—á –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). VISTAR —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ MLLM. –ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Subtask-of-Thought. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VISTAR –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å.'}, 'en': {'title': 'Enhancing Visual Question Answering with VISTAR', 'desc': "This paper presents VISTAR, a new framework designed to improve the reasoning capabilities of multimodal large language models (MLLMs) when answering complex visual questions. VISTAR enhances interpretability by breaking down tasks into smaller subtasks and generating structured reasoning sequences, known as Subtask-of-Thought rationales. Unlike previous methods that relied on external models, VISTAR fine-tunes MLLMs directly, leading to better adaptation and accuracy on target data. Experimental results demonstrate that VISTAR not only boosts reasoning accuracy but also maintains a high level of interpretability in the model's outputs."}, 'zh': {'title': 'VISTARÔºöÊèêÂçáËßÜËßâÊé®ÁêÜÁöÑÂ≠ê‰ªªÂä°È©±Âä®Ê®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫VISTARÁöÑÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§çÊùÇËßÜËßâÈóÆÈ¢ò‰∏äÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇVISTARÈÄöËøáÁîüÊàêÊñáÊú¨ÂíåËßÜËßâËß£ÈáäÔºåÈááÁî®Â≠ê‰ªªÂä°È©±Âä®ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂ¢ûÂº∫‰∫ÜËß£ÈáäÊÄßÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏é‰æùËµñÂ§ñÈÉ®Ê®°ÂûãÁöÑÊñπÊ≥ï‰∏çÂêåÔºåVISTARÂØπMLLMsËøõË°åÂæÆË∞ÉÔºå‰ª•ÁîüÊàêÁªìÊûÑÂåñÁöÑÊÄùÁª¥Â≠ê‰ªªÂä°Êé®ÁêÜÂ∫èÂàó„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVISTARÂú®‰∏§‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂùáÊèêÈ´ò‰∫ÜÊé®ÁêÜÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËß£ÈáäÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06356', 'title': 'Understanding and Mitigating Toxicity in Image-Text Pretraining\n  Datasets: A Case Study on LLaVA', 'url': 'https://huggingface.co/papers/2505.06356', 'abstract': 'Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.', 'score': 1, 'issue_id': 3784, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': '159eecb990673138', 'authors': ['Karthik Reddy Kanjula', 'Surya Guthikonda', 'Nahid Alam', 'Shayekh Bin Islam'], 'affiliations': ['Bangladesh University of Engineering and Technology', 'Cisco Meraki', 'Cohere for AI Community', 'Indiana University Bloomington'], 'pdf_title_img': 'assets/pdf/title_img/2505.06356.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#ethics', '#open_source', '#data'], 'emoji': 'üßπ', 'ru': {'title': '–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–∏—á–Ω–æ–≥–æ –ò–ò', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö LLaVA –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ –µ–≥–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ—á–∏—â–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ —É–¥–∞–ª–µ–Ω–æ –±–æ–ª–µ–µ 7500 —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç. –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Building Safer Multimodal Models by Mitigating Toxicity', 'desc': 'This paper addresses the issue of toxic content in pretraining datasets used for multimodal models, specifically focusing on the LLaVA image-text dataset. The authors analyze various categories of toxicity and how they appear in both images and text. They propose strategies to reduce this harmful content, resulting in a refined dataset that eliminates over 7,500 toxic image-text pairs. The study emphasizes the importance of filtering out toxic elements to create fairer and more responsible AI systems, and the new dataset is made available for further research.'}, 'zh': {'title': 'ÊûÑÂª∫Êõ¥ÂÆâÂÖ®ÁöÑÂ§öÊ®°ÊÄÅÊ®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜLLaVAÂõæÂÉè-ÊñáÊú¨È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠ÊúâÊØíÂÜÖÂÆπÁöÑÊôÆÈÅçÊÄßÔºåÂàÜÊûê‰∫ÜÊúâÂÆ≥ÂÜÖÂÆπÂú®‰∏çÂêåÊ®°ÊÄÅ‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ËØÜÂà´‰∫ÜÂ∏∏ËßÅÁöÑÊúâÊØíÁ±ªÂà´ÔºåÂπ∂ÊèêÂá∫‰∫ÜÈíàÂØπÊÄßÁöÑÂáèËΩªÁ≠ñÁï•ÔºåÊúÄÁªàÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁªèËøáÊîπËøõÁöÑÂéªÊØíÊÄßÊï∞ÊçÆÈõÜ„ÄÇËØ•Êï∞ÊçÆÈõÜÁßªÈô§‰∫Ü7531ÂØπÊúâÊØíÁöÑÂõæÂÉè-ÊñáÊú¨ÈÖçÂØπÔºåÊèê‰æõ‰∫ÜÂÆûÊñΩÂº∫Â§ßÊØíÊÄßÊ£ÄÊµãÁÆ°ÈÅìÁöÑÊåáÂçó„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Âº∫Ë∞É‰∫Ü‰∏ªÂä®ËØÜÂà´ÂíåËøáÊª§ÊúâÊØíÂÜÖÂÆπÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÊûÑÂª∫Êõ¥Ë¥üË¥£‰ªªÂíåÂÖ¨Âπ≥ÁöÑÂ§öÊ®°ÊÄÅÁ≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05587', 'title': 'Steepest Descent Density Control for Compact 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2505.05587', 'abstract': '3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.', 'score': 1, 'issue_id': 3784, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'c689f8e4175cc211', 'authors': ['Peihao Wang', 'Yuehao Wang', 'Dilin Wang', 'Sreyas Mohan', 'Zhiwen Fan', 'Lemeng Wu', 'Ruisi Cai', 'Yu-Ying Yeh', 'Zhangyang Wang', 'Qiang Liu', 'Rakesh Ranjan'], 'affiliations': ['Meta Reality Labs', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.05587.jpg', 'data': {'categories': ['#inference', '#3d', '#optimization'], 'emoji': 'üîç', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è 3D Gaussian Splatting: –º–µ–Ω—å—à–µ —Ç–æ—á–µ–∫, –≤—ã—à–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ç–æ–¥–µ 3D Gaussian Splatting (3DGS). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ—Ü–µ—Å—Å —É–ø–ª–æ—Ç–Ω–µ–Ω–∏—è –≤ 3DGS –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —É—Å–ª–æ–≤–∏–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ SteepGS, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å 50% —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–∞—É—Å—Å–æ–≤—ã—Ö —Ç–æ—á–µ–∫ –±–µ–∑ —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å 3DGS, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.'}, 'en': {'title': 'Optimizing 3D Gaussian Splatting for Efficient Real-Time Rendering', 'desc': 'This paper presents 3D Gaussian Splatting (3DGS), a technique for creating high-quality 3D views in real-time by using Gaussian primitives. It addresses the issue of excessive memory usage and slow performance caused by redundant point clouds during the densification process. The authors propose a theoretical framework to improve density control, which includes optimizing the number of Gaussian points and their opacity. The new method, SteepGS, effectively reduces the number of Gaussian points by about 50% while preserving rendering quality, making it more efficient for use on devices with limited resources.'}, 'zh': {'title': 'È´òÊïàÁ¥ßÂáëÁöÑ3DÈ´òÊñØÁÇπ‰∫ëÊéßÂà∂', 'desc': '3DÈ´òÊñØÁÇπ‰∫ëÔºà3DGSÔºâÊòØ‰∏ÄÁßçÁî®‰∫éÂÆûÊó∂È´òÂàÜËæ®ÁéáÊñ∞ËßÜËßíÂêàÊàêÁöÑÂº∫Â§ßÊäÄÊúØ„ÄÇÂÆÉÈÄöËøáÂ∞ÜÂú∫ÊôØË°®Á§∫‰∏∫È´òÊñØÂéüËØ≠ÁöÑÊ∑∑Âêà‰ΩìÔºåÂà©Áî®GPUÂÖâÊ†ÖÂåñÁÆ°ÈÅìÂÆûÁé∞È´òÊïàÊ∏≤ÊüìÂíåÈáçÂª∫„ÄÇ‰∏∫‰∫Ü‰ºòÂåñÂú∫ÊôØË¶ÜÁõñÂíåÊçïÊçâÁªÜËäÇÔºå3DGSÈááÁî®‰∫ÜÂØÜÂ∫¶ÂåñÁÆóÊ≥ïÁîüÊàêÈ¢ùÂ§ñÁÇπÔºå‰ΩÜËøô‰ºöÂØºËá¥ÂÜó‰ΩôÁÇπ‰∫ëÔºåÂ¢ûÂä†ÂÜÖÂ≠ò‰ΩøÁî®ÂíåÂ≠òÂÇ®ÈúÄÊ±Ç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁêÜËÆ∫Ê°ÜÊû∂ÔºåÊîπËøõ‰∫Ü3DGS‰∏≠ÁöÑÂØÜÂ∫¶ÊéßÂà∂ÔºåÊèêÂá∫‰∫ÜSteepGSÊñπÊ≥ïÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÈ´òÊñØÁÇπÊï∞ÈáèÔºåÂêåÊó∂‰øùÊåÅÊ∏≤ÊüìË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08910', 'title': 'Behind Maya: Building a Multilingual Vision Language Model', 'url': 'https://huggingface.co/papers/2505.08910', 'abstract': 'In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.', 'score': 0, 'issue_id': 3784, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '617c510dcc5eefc7', 'authors': ['Nahid Alam', 'Karthik Reddy Kanjula', 'Surya Guthikonda', 'Timothy Chung', 'Bala Krishna S Vegesna', 'Abhipsha Das', 'Anthony Susevski', 'Ryan Sze-Yin Chan', 'S M Iftekhar Uddin', 'Shayekh Bin Islam', 'Roshan Santhosh', 'Snegha A', 'Drishti Sharma', 'Chen Liu', 'Isha Chaturvedi', 'Genta Indra Winata', 'Ashvanth. S', 'Snehanshu Mukherjee', 'Alham Fikri Aji'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08910.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#dataset', '#cv', '#open_source'], 'emoji': 'üåç', 'ru': {'title': 'Maya: –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è VLM –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Maya - –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM). –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º –≤–æ—Å–µ–º—å —è–∑—ã–∫–æ–≤, —á—Ç–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –µ—ë –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. Maya —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ –∑—Ä–µ–Ω–∏–µ–º –∏ —è–∑—ã–∫–æ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ –∫–æ–¥—É –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫.'}, 'en': {'title': 'Empowering Multilingual Understanding in Vision-Language Models', 'desc': 'This paper presents Maya, a new open-source Multilingual Vision-Language Model (VLM) designed to improve performance in low-resource languages and diverse cultural contexts. It introduces a multilingual image-text pretraining dataset that includes eight languages, expanding upon the existing LLaVA dataset. The model aims to enhance understanding in vision-language tasks by incorporating cultural and linguistic nuances. By providing this resource, the authors hope to bridge the gap in VLM capabilities across different languages and cultures.'}, 'zh': {'title': 'MayaÔºöÊèêÂçáÂ§öËØ≠Ë®ÄËßÜËßâÁêÜËß£ÁöÑÂºÄÊ∫êÊ®°Âûã', 'desc': 'ËøëÂπ¥Êù•ÔºåÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËøÖÈÄüÂèëÂ±ïÔºåÂèñÂæó‰∫Ü‰ª§‰∫∫Áû©ÁõÆÁöÑÂ≠¶ÊúØÊàêÁª©„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂíå‰∏çÂêåÊñáÂåñËÉåÊôØ‰∏ãÁöÑË°®Áé∞‰ªçÁÑ∂‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜMayaÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§öËØ≠Ë®ÄËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÂåÖÊã¨ÔºöÂü∫‰∫éLLaVAÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂÖ´ÁßçËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÂõæÂÉè-ÊñáÊú¨È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäÊîØÊåÅËøô‰∫õËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÂõæÂÉè-ÊñáÊú¨Ê®°ÂûãÔºåÊèêÂçá‰∫ÜËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠ÁöÑÊñáÂåñÂíåËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04921', 'title': 'Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2505.04921', 'abstract': "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.", 'score': 102, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'a07d92b81581eea3', 'authors': ['Yunxin Li', 'Zhenyu Liu', 'Zitao Li', 'Xuanyu Zhang', 'Zhenran Xu', 'Xinyu Chen', 'Haoyuan Shi', 'Shenyuan Jiang', 'Xintong Wang', 'Jifang Wang', 'Shouzheng Huang', 'Xinping Zhao', 'Borui Jiang', 'Lanqing Hong', 'Longyue Wang', 'Zhuotao Tian', 'Baoxing Huai', 'Wenhan Luo', 'Weihua Luo', 'Zheng Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.04921.jpg', 'data': {'categories': ['#rl', '#multimodal', '#benchmark', '#survey', '#agents', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç –º–æ–¥—É–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LMRMs). –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç —ç–≤–æ–ª—é—Ü–∏—é –ø–æ–¥—Ö–æ–¥–æ–≤ –æ—Ç –º–æ–¥—É–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–º –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –º–µ–∂–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (MCoT) –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –°—Ç–∞—Ç—å—è –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–∞—Ç–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (N-LMRMs) –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö.'}, 'en': {'title': 'Empowering AI with Multimodal Reasoning for Real-World Intelligence', 'desc': 'This paper discusses the importance of reasoning in artificial intelligence, especially in complex environments with multiple types of data like text, images, and audio. It introduces Large Multimodal Reasoning Models (LMRMs) that combine these different data types to enhance reasoning capabilities. The authors provide a structured survey of the evolution of multimodal reasoning, from early task-specific models to more integrated language-centric frameworks. They also highlight ongoing challenges in generalization and reasoning depth, while proposing a roadmap for future research in developing more adaptive and intelligent systems.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊé®ÁêÜÔºöÊô∫ËÉΩÁöÑÊú™Êù•', 'desc': 'Êé®ÁêÜÊòØÊô∫ËÉΩÁöÑÊ†∏ÂøÉÔºåÂΩ±ÂìçÂÜ≥Á≠ñ„ÄÅÁªìËÆ∫ÂíåË∑®È¢ÜÂüüÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇÂú®‰∫∫Â∑•Êô∫ËÉΩ‰∏≠ÔºåÈöèÁùÄÁ≥ªÁªüÂú®ÂºÄÊîæ„ÄÅ‰∏çÁ°ÆÂÆöÂíåÂ§öÊ®°ÊÄÅÁéØÂ¢É‰∏≠ËøêË°åÔºåÊé®ÁêÜÂèòÂæóËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ª•ÂÆûÁé∞Á®≥ÂÅ•ÂíåÈÄÇÂ∫îÊÄßÁöÑË°å‰∏∫„ÄÇÂ§ßÂûãÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºàLMRMsÔºâÊï¥ÂêàÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅÔºåÊîØÊåÅÂ§çÊùÇÁöÑÊé®ÁêÜËÉΩÂäõÔºåÊó®Âú®ÂÆûÁé∞ÂÖ®Èù¢ÊÑüÁü•„ÄÅÁ≤æÁ°ÆÁêÜËß£ÂíåÊ∑±Â∫¶Êé®ÁêÜ„ÄÇÊú¨ÊñáÂØπÂ§öÊ®°ÊÄÅÊé®ÁêÜÁ†îÁ©∂ËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑË∞ÉÊü•ÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂõõÈò∂ÊÆµÁöÑÂèëÂ±ïË∑ØÁ∫øÂõæÔºåÂèçÊò†‰∫ÜËØ•È¢ÜÂüüËÆæËÆ°ÁêÜÂøµÁöÑÂèòÂåñÂíåÊñ∞ÂÖ¥ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04620', 'title': 'On Path to Multimodal Generalist: General-Level and General-Bench', 'url': 'https://huggingface.co/papers/2505.04620', 'abstract': 'The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/', 'score': 62, 'issue_id': 3671, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '57991e528141671e', 'authors': ['Hao Fei', 'Yuan Zhou', 'Juncheng Li', 'Xiangtai Li', 'Qingshan Xu', 'Bobo Li', 'Shengqiong Wu', 'Yaoting Wang', 'Junbao Zhou', 'Jiahao Meng', 'Qingyu Shi', 'Zhiyuan Zhou', 'Liangtao Shi', 'Minghe Gao', 'Daoan Zhang', 'Zhiqi Ge', 'Weiming Wu', 'Siliang Tang', 'Kaihang Pan', 'Yaobo Ye', 'Haobo Yuan', 'Tao Zhang', 'Tianjie Ju', 'Zixiang Meng', 'Shilin Xu', 'Liyu Jia', 'Wentao Hu', 'Meng Luo', 'Jiebo Luo', 'Tat-Seng Chua', 'Shuicheng Yan', 'Hanwang Zhang'], 'affiliations': ['HFUT', 'KAUST', 'NJU', 'NTU', 'NUS', 'PKU', 'SJTU', 'UR', 'WHU', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2505.04620.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agi'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º –Ω–∞ –ø—É—Ç–∏ –∫ AGI', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º General-Level. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç 5 —É—Ä–æ–≤–Ω–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ MLLM, –ø—Ä–µ–¥–ª–∞–≥–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º. –ö–ª—é—á–µ–≤—ã–º –ø–æ–Ω—è—Ç–∏–µ–º –≤ —ç—Ç–æ–π —Å–∏—Å—Ç–µ–º–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –°–∏–Ω–µ—Ä–≥–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤ —Ä–∞–±–æ—Ç–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —ç—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç General-Bench - –Ω–∞–±–æ—Ä –∏–∑ –±–æ–ª–µ–µ —á–µ–º 700 –∑–∞–¥–∞—á –∏ 325 800 –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –Ω–∞–≤—ã–∫–æ–≤, –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤.'}, 'en': {'title': 'Towards Multimodal Generalists: Evaluating MLLM Progress', 'desc': 'The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI).'}, 'zh': {'title': 'ËøàÂêëÁúüÊ≠£ÁöÑÂ§öÊ®°ÊÄÅÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩ', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÊ≠£Âú®Âø´ÈÄüÂèëÂ±ïÔºåÂæóÁõä‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂÖàËøõËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑMLLMÊ≠£ÊúùÁùÄÂ§öÊ®°ÊÄÅÈÄöÁî®‰∏ª‰πâËÄÖÁöÑÊñπÂêëÊºîÂèòÔºå‰∏ç‰ªÖËÉΩÂ§üÁêÜËß£Â§öÁßçÊ®°ÊÄÅÔºåËøòËÉΩÂú®‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁîüÊàêÂÜÖÂÆπ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞Ê°ÜÊû∂‚Äî‚ÄîGeneral-LevelÔºåÂÆö‰πâ‰∫ÜMLLMÊÄßËÉΩÂíåÈÄöÁî®ÊÄßÁöÑ‰∫î‰∏™Á≠âÁ∫ßÔºå‰ª•‰æøÊØîËæÉ‰∏çÂêåÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÈÄöËøáGeneral-BenchÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™Êõ¥ÂπøÊ≥õÁöÑÊäÄËÉΩÂíå‰ªªÂä°ËØÑ‰º∞ÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÂ§öÊ®°ÊÄÅÈÄöÁî®Ê®°ÂûãÂú®ÂÆûÁé∞ÁúüÊ≠£‰∫∫Â∑•Êô∫ËÉΩÊñπÈù¢ÁöÑÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05470', 'title': 'Flow-GRPO: Training Flow Matching Models via Online RL', 'url': 'https://huggingface.co/papers/2505.05470', 'abstract': "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.", 'score': 44, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '8db85b6df75e7479', 'authors': ['Jie Liu', 'Gongye Liu', 'Jiajun Liang', 'Yangguang Li', 'Jiaheng Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wanli Ouyang'], 'affiliations': ['CUHK MMLab', 'Kuaishou Technology', 'Nanjing University', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05470.jpg', 'data': {'categories': ['#rl', '#cv', '#rlhf', '#alignment', '#games', '#multimodal'], 'emoji': 'üé®', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: RL –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏', 'desc': 'Flow-GRPO ‚Äì —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –û–î–£ –≤ –°–î–£ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏—è—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ —Ç–µ–∫—Å—Ç–∞. Flow-GRPO —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–∏ —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞ –±–µ–∑ —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–ª–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Flow-GRPO', 'desc': 'Flow-GRPO is a novel method that combines online reinforcement learning with flow matching models to enhance performance in text-to-image tasks. It employs an ODE-to-SDE conversion to facilitate statistical sampling, allowing for better exploration in reinforcement learning. Additionally, the Denoising Reduction strategy optimizes training efficiency by minimizing unnecessary denoising steps while maintaining inference quality. The results show significant improvements in accuracy and human preference alignment, with minimal risk of reward hacking, ensuring high-quality and diverse outputs.'}, 'zh': {'title': 'Flow-GRPOÔºöÂº∫ÂåñÂ≠¶‰π†‰∏éÊµÅÂåπÈÖçÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜFlow-GRPOÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Â∞ÜÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÈõÜÊàêÂà∞ÊµÅÂåπÈÖçÊ®°Âûã‰∏≠ÁöÑÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∫Ü‰∏§‰∏™ÂÖ≥ÈîÆÁ≠ñÁï•ÔºöÈ¶ñÂÖàÔºåÈÄöËøáÂ∞ÜÁ°ÆÂÆöÊÄßÂ∏∏ÂæÆÂàÜÊñπÁ®ãÔºàODEÔºâËΩ¨Êç¢‰∏∫Á≠âÊïàÁöÑÈöèÊú∫ÂæÆÂàÜÊñπÁ®ãÔºàSDEÔºâÔºåÂÆûÁé∞‰∫ÜÂú®ÊâÄÊúâÊó∂Èó¥Ê≠•Èïø‰∏ä‰∏éÂéüÂßãÊ®°ÂûãÁöÑËæπÈôÖÂàÜÂ∏ÉÂåπÈÖçÔºå‰ªéËÄåÊîØÊåÅRLÊé¢Á¥¢ÁöÑÁªüËÆ°ÈááÊ†∑ÔºõÂÖ∂Ê¨°ÔºåÈááÁî®ÂéªÂô™ÂáèÂ∞ëÁ≠ñÁï•ÔºåÂú®‰øùÊåÅÂéüÂßãÊé®ÁêÜÊó∂Èó¥Ê≠•Êï∞ÁöÑÂêåÊó∂ÂáèÂ∞ëËÆ≠ÁªÉÂéªÂô™Ê≠•È™§ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈááÊ†∑ÊïàÁéáËÄå‰∏çÈôç‰ΩéÊÄßËÉΩ„ÄÇÂÆûÈ™åËØÅÊòéÔºåFlow-GRPOÂú®Â§ö‰∏™ÊñáÊú¨Âà∞ÂõæÂÉèÁöÑ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÁªÑÂêà‰∏≠ÔºåRLË∞É‰ºòÁöÑSD3.5Âá†‰πéÂÆåÁæéÂú∞ÁîüÊàê‰∫ÜÂØπË±°Êï∞Èáè„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ªÂíåÁªÜÁ≤íÂ∫¶Â±ûÊÄßÔºåÊòæËëóÊèêÈ´ò‰∫ÜGenEvalÁöÑÂáÜÁ°ÆÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02847', 'title': 'Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.02847', 'abstract': "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.", 'score': 22, 'issue_id': 3671, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '9204f0ca97eb8bc7', 'authors': ['Bang Zhang', 'Ruotian Ma', 'Qingxuan Jiang', 'Peisong Wang', 'Jiaqi Chen', 'Zheng Xie', 'Xingyu Chen', 'Yue Wang', 'Fanghua Ye', 'Jian Li', 'Yifan Yang', 'Zhaopeng Tu', 'Xiaolong Li'], 'affiliations': ['Hunyuan AI Digital Human, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.02847.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#alignment', '#agents', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'SAGE: –ò–∑–º–µ—Ä–µ–Ω–∏–µ —ç–º–ø–∞—Ç–∏–∏ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SAGE - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–∞, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç. SAGE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç '–†–∞–∑—É–º–Ω–æ–≥–æ –ê–≥–µ–Ω—Ç–∞', –∫–æ—Ç–æ—Ä—ã–π —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —ç–º–æ—Ü–∏–∏ –∏ –º—ã—Å–ª–∏ –≤–æ –≤—Ä–µ–º—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ–π –º–æ–¥–µ–ª–∏ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ SAGE –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—É–±–ª–∏—á–Ω–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞ LLM, –≤—ã—è–≤–ª—è—é—â–µ–≥–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –∏ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏."}, 'en': {'title': 'Measuring Empathy in AI: The SAGE Framework', 'desc': 'The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems.'}, 'zh': {'title': 'ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÉÖÊÑüÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SAGEÁöÑËá™Âä®ËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éÊµãÈáèÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂØπ‰∫∫Á±ªÊÉÖÊÑüÂíåÁ§æ‰∫§ËÆ§Áü•ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇSAGEÈÄöËøáÊ®°Êãü‰∫∫Á±ªÊÉÖÊÑüÂèòÂåñÂíåÂÜÖÂøÉÊÉ≥Ê≥ïÔºåÊèê‰æõ‰∫ÜÊõ¥ÁúüÂÆûÁöÑÂ§öËΩÆÂØπËØùËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSAGEÁöÑÊÉÖÊÑüËØÑÂàÜ‰∏éÂøÉÁêÜÂ≠¶ËØÑ‰º∞Â∑•ÂÖ∑ÁöÑËØÑÂàÜÈ´òÂ∫¶Áõ∏ÂÖ≥ÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÂøÉÁêÜÂ≠¶ÁöÑÁúüÂÆûÊÄß„ÄÇËØ•Ê°ÜÊû∂ËøòÂª∫Á´ã‰∫Ü‰∏Ä‰∏™ÂÖ¨ÂºÄÁöÑSentientÊéíË°åÊ¶úÔºåÊè≠Á§∫‰∫Ü‰∏çÂêåÊ®°Âûã‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ùÔºåÊé®Âä®‰∫ÜÂØπÊõ¥ÂÖ∑ÂêåÁêÜÂøÉÂíåÁ§æ‰∫§ËÉΩÂäõÁöÑËØ≠Ë®Ä‰ª£ÁêÜÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05315', 'title': 'Scalable Chain of Thoughts via Elastic Reasoning', 'url': 'https://huggingface.co/papers/2505.05315', 'abstract': 'Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.', 'score': 18, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '0ce3be6057da3ed2', 'authors': ['Yuhui Xu', 'Hanze Dong', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Caiming Xiong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05315.jpg', 'data': {'categories': ['#plp', '#training', '#reasoning', '#optimization', '#benchmark', '#math'], 'emoji': 'üß†', 'ru': {'title': '–≠–ª–∞—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤', 'desc': "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–≠–ª–∞—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ' –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM). –ú–µ—Ç–æ–¥ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–≤–µ —Ñ–∞–∑—ã - –º—ã—à–ª–µ–Ω–∏–µ –∏ —Ä–µ—à–µ–Ω–∏–µ - —Å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –±—é–¥–∂–µ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –±—é–¥–∂–µ—Ç–æ–º, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤ GRPO, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –ø—Ä–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ '–≠–ª–∞—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ' —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ –ø—Ä–∏ —Å—Ç—Ä–æ–≥–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –±—é–¥–∂–µ—Ç–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä–∞—Ç–∫–∏–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–∞–∂–µ –≤ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö."}, 'en': {'title': 'Elastic Reasoning: Scalable and Efficient Thought Processes for ML Models', 'desc': 'This paper introduces Elastic Reasoning, a framework designed to enhance the performance of large reasoning models (LRMs) under strict resource constraints. It separates the reasoning process into two distinct phases: thinking and solution, allowing for independent budget allocation for each phase. The framework employs a budget-constrained rollout strategy that helps models adaptively reason even when the thinking phase is limited, ensuring reliability in various scenarios. Empirical results show that Elastic Reasoning not only meets budget constraints effectively but also reduces training costs while improving the efficiency of reasoning outputs.'}, 'zh': {'title': 'ÂºπÊÄßÊé®ÁêÜÔºöÂèØÊéßÊé®ÁêÜÁöÑÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®Â§çÊùÇ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂ËæìÂá∫ÈïøÂ∫¶‰∏çÂèóÊéßÂà∂ÔºåÁªôÂÆûÈôÖÂ∫îÁî®Â∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂºπÊÄßÊé®ÁêÜÁöÑÊñ∞Ê°ÜÊû∂ÔºåÂ∞ÜÊé®ÁêÜËøáÁ®ãÂàÜ‰∏∫ÊÄùËÄÉÂíåËß£ÂÜ≥‰∏§‰∏™Èò∂ÊÆµÔºåÂπ∂‰∏∫ÊØè‰∏™Èò∂ÊÆµÂàÜÈÖçÁã¨Á´ãÁöÑÈ¢ÑÁÆó„ÄÇÂú®ÊµãËØïÊó∂ÔºåÂºπÊÄßÊé®ÁêÜ‰ºòÂÖàËÄÉËôëËß£ÂÜ≥ÊñπÊ°àÁöÑÂÆåÊï¥ÊÄßÔºå‰ªéËÄåÂú®ËµÑÊ∫êÁ¥ßÂº†ÁöÑÊÉÖÂÜµ‰∏ãÊòæËëóÊèêÈ´òÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åËØÅÊòéÔºåÂºπÊÄßÊé®ÁêÜÂú®‰∏•Ê†ºÁöÑÈ¢ÑÁÆóÈôêÂà∂‰∏ãË°®Áé∞Âá∫Ëâ≤ÔºåÂêåÊó∂ËÆ≠ÁªÉÊàêÊú¨ÊòæËëó‰Ωé‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05469', 'title': 'Generating Physically Stable and Buildable LEGO Designs from Text', 'url': 'https://huggingface.co/papers/2505.05469', 'abstract': 'We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.', 'score': 17, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '20752e033ce6b40a', 'authors': ['Ava Pun', 'Kangle Deng', 'Ruixuan Liu', 'Deva Ramanan', 'Changliu Liu', 'Jun-Yan Zhu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05469.jpg', 'data': {'categories': ['#robotics', '#dataset', '#multimodal', '#3d', '#open_source'], 'emoji': 'üß±', 'ru': {'title': 'LegoGPT: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º LEGO', 'desc': 'LegoGPT - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –∫—É–±–∏–∫–æ–≤ LEGO –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π LEGO —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –∏ –æ–±—É—á–∏–ª–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –∫—É–±–∏–∫ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–∫–∞—Ç —Å —É—á–µ—Ç–æ–º —Ñ–∏–∑–∏–∫–∏ –≤–æ –≤—Ä–µ–º—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LegoGPT —Å–æ–∑–¥–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —ç—Å—Ç–µ—Ç–∏—á–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ LEGO, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º.'}, 'en': {'title': 'Building LEGO Dreams with AI!', 'desc': 'LegoGPT is a novel machine learning model designed to generate stable LEGO brick structures from textual descriptions. It utilizes a large dataset of LEGO designs paired with captions to train an autoregressive language model that predicts the next brick to add based on the input prompt. To ensure the generated designs are physically stable, the model incorporates a validity check and physics-aware rollback mechanism during the generation process. The results demonstrate that LegoGPT can create diverse and visually appealing LEGO models that can be assembled by both humans and robotic systems.'}, 'zh': {'title': '‰πêÈ´òËÆæËÆ°ÁöÑÊô∫ËÉΩÁîüÊàê‰∏éÁ®≥ÂÆöÊÄß‰øùÈöú', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜLegoGPTÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰ªéÊñáÊú¨ÊèêÁ§∫ÁîüÊàêÁâ©ÁêÜÁ®≥ÂÆöÁöÑ‰πêÈ´òÁ†ñÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÁâ©ÁêÜÁ®≥ÂÆö‰πêÈ´òËÆæËÆ°Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Ëá™ÂõûÂΩíÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊù•È¢ÑÊµã‰∏ã‰∏Ä‰∏™Ë¶ÅÊ∑ªÂä†ÁöÑÁ†ñÂùó„ÄÇ‰∏∫‰∫ÜÊèêÈ´òËÆæËÆ°ÁöÑÁ®≥ÂÆöÊÄßÔºåÊàë‰ª¨Âú®Ëá™ÂõûÂΩíÊé®ÁêÜËøáÁ®ã‰∏≠ÈááÁî®‰∫ÜÊúâÊïàÁöÑÊúâÊïàÊÄßÊ£ÄÊü•ÂíåÁâ©ÁêÜÊÑüÁü•ÂõûÊªöÔºåÂà©Áî®Áâ©ÁêÜÊ≥ïÂàôÂíåÁªÑË£ÖÁ∫¶ÊùüÊù•‰øÆÂâ™‰∏çÂèØË°åÁöÑÈ¢ÑÊµã„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåLegoGPTÁîüÊàêÁöÑ‰πêÈ´òËÆæËÆ°Á®≥ÂÆö„ÄÅÂ§öÊ†∑‰∏îÁæéËßÇÔºå‰∏éËæìÂÖ•ÁöÑÊñáÊú¨ÊèêÁ§∫Á¥ßÂØÜÂØπÈΩê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05071', 'title': 'FG-CLIP: Fine-Grained Visual and Textual Alignment', 'url': 'https://huggingface.co/papers/2505.05071', 'abstract': "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.", 'score': 15, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '4251cc9ddf64d2b8', 'authors': ['Chunyu Xie', 'Bin Wang', 'Fanjing Kong', 'Jincheng Li', 'Dawei Liang', 'Gengshen Zhang', 'Dawei Leng', 'Yuhui Yin'], 'affiliations': ['360 AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05071.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#multimodal', '#training', '#dataset'], 'emoji': 'üîç', 'ru': {'title': 'FG-CLIP: –¢–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Fine-Grained CLIP (FG-CLIP) - —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ CLIP –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. FG-CLIP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 1,6 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 12 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å 40 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏. FG-CLIP –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π CLIP –∏ –¥—Ä—É–≥–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏.'}, 'en': {'title': 'Unlocking Fine-Grained Understanding with FG-CLIP', 'desc': 'The paper introduces Fine-Grained CLIP (FG-CLIP), an enhancement of the original CLIP model aimed at improving fine-grained understanding in multimodal tasks. FG-CLIP achieves this by generating a massive dataset of 1.6 billion long caption-image pairs, which helps capture detailed semantic information. Additionally, it constructs a high-quality dataset with 12 million images and 40 million bounding boxes, ensuring that the model learns from context-rich representations. By incorporating 10 million hard negative samples, FG-CLIP enhances its ability to differentiate subtle semantic differences, leading to superior performance in various tasks compared to the original CLIP and other leading models.'}, 'zh': {'title': 'ÁªÜÁ≤íÂ∫¶ÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥ÔºöFG-CLIP', 'desc': 'ÂØπÊØîËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºàCLIPÔºâÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÁªÜÁ≤íÂ∫¶ÁêÜËß£ÊñπÈù¢Â≠òÂú®Âõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁªÜÁ≤íÂ∫¶CLIPÔºàFG-CLIPÔºâÔºåÈÄöËøá‰∏âÈ°πÂÖ≥ÈîÆÂàõÊñ∞Êù•Â¢ûÂº∫ÁªÜÁ≤íÂ∫¶ÁêÜËß£„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨Âà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁîüÊàê16‰∫øÂØπÈïøÊ†áÈ¢ò-ÂõæÂÉèÂØπÔºå‰ª•ÊçïÊçâÂÖ®Â±ÄËØ≠‰πâÁªÜËäÇ„ÄÇÂÖ∂Ê¨°ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´1200‰∏áÂº†ÂõæÂÉèÂíå4000‰∏á‰∏™Âå∫ÂüüÁâπÂÆöÁöÑËæπÁïåÊ°ÜÔºåÁ°Æ‰øùÁ≤æÁ°Æ‰∏î‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñáË°®Á§∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05467', 'title': 'StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant', 'url': 'https://huggingface.co/papers/2505.05467', 'abstract': 'We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.', 'score': 13, 'issue_id': 3674, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'c876dfdb1d290930', 'authors': ['Haibo Wang', 'Bo Feng', 'Zhengfeng Lai', 'Mingze Xu', 'Shiyu Li', 'Weifeng Ge', 'Afshin Dehghan', 'Meng Cao', 'Ping Huang'], 'affiliations': ['Apple', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05467.jpg', 'data': {'categories': ['#benchmark', '#video', '#dataset', '#long_context'], 'emoji': 'üé•', 'ru': {'title': 'StreamBridge: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ', 'desc': 'StreamBridge - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –æ—Ñ–ª–∞–π–Ω-–º–æ–¥–µ–ª–∏ Video-LLM –≤ –ø–æ—Ç–æ–∫–æ–≤—ã–µ. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±—É—Ñ–µ—Ä –ø–∞–º—è—Ç–∏ —Å –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π –∏ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –º–æ–¥–µ–ª—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ Video-LLM. –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ StreamBridge —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç Stream-IT, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Transforming Video-LLMs for Real-Time Streaming Success', 'desc': 'StreamBridge is a framework designed to enhance offline Video-LLMs for real-time streaming applications. It tackles two main issues: the need for effective multi-turn interactions and the ability to provide proactive responses. By using a memory buffer with round-decayed compression, it allows models to handle longer contexts in conversations. Additionally, StreamBridge introduces a lightweight activation model that integrates easily with existing Video-LLMs, and it is supported by the Stream-IT dataset, which is specifically created for streaming video understanding tasks.'}, 'zh': {'title': 'StreamBridgeÔºöÊµÅÂ™í‰ΩìËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'StreamBridgeÊòØ‰∏Ä‰∏™ÁÆÄÂçïËÄåÊúâÊïàÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•Â∞ÜÁ¶ªÁ∫øËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàVideo-LLMsÔºâËΩ¨Âèò‰∏∫ÊîØÊåÅÊµÅÂ™í‰ΩìÁöÑÊ®°Âûã„ÄÇÂÆÉËß£ÂÜ≥‰∫ÜÂú®Âú®Á∫øÂú∫ÊôØ‰∏≠ÈÄÇÂ∫îÁé∞ÊúâÊ®°ÂûãÁöÑ‰∏§‰∏™Âü∫Êú¨ÊåëÊàòÔºöÂ§öËΩÆÂÆûÊó∂ÁêÜËß£ËÉΩÂäõÊúâÈôêÂíåÁº∫‰πè‰∏ªÂä®ÂìçÂ∫îÊú∫Âà∂„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåStreamBridgeÁªìÂêà‰∫ÜÂÜÖÂ≠òÁºìÂÜ≤Âå∫ÂíåÈÄêËΩÆË°∞ÂáèÂéãÁº©Á≠ñÁï•ÔºåÊîØÊåÅÈïø‰∏ä‰∏ãÊñáÁöÑÂ§öËΩÆ‰∫§‰∫íÔºåÂπ∂‰∏îÈááÁî®‰∫ÜËΩªÈáèÁ∫ßÁöÑËß£ËÄ¶ÊøÄÊ¥ªÊ®°ÂûãÔºåËÉΩÂ§üËΩªÊùæÈõÜÊàêÂà∞Áé∞ÊúâÁöÑËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÔºåÂÆûÁé∞ÊåÅÁª≠ÁöÑ‰∏ªÂä®ÂìçÂ∫î„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜStream-ITÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπÊµÅÂ™í‰ΩìËßÜÈ¢ëÁêÜËß£ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂåÖÂê´‰∫§ÈîôÁöÑËßÜÈ¢ë-ÊñáÊú¨Â∫èÂàóÂíåÂ§öÊ†∑ÁöÑÊåá‰ª§Ê†ºÂºè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05474', 'title': '3D Scene Generation: A Survey', 'url': 'https://huggingface.co/papers/2505.05474', 'abstract': '3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.', 'score': 12, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '06bda1a6228b8f26', 'authors': ['Beichen Wen', 'Haozhe Xie', 'Zhaoxi Chen', 'Fangzhou Hong', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore 637335'], 'pdf_title_img': 'assets/pdf/title_img/2505.05474.jpg', 'data': {'categories': ['#3d', '#robotics', '#multimodal', '#synthetic', '#survey'], 'emoji': 'üåê', 'ru': {'title': '–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω: –æ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∫ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç –ø–æ–¥—Ö–æ–¥—ã –ø–æ —á–µ—Ç—ã—Ä–µ–º –ø–∞—Ä–∞–¥–∏–≥–º–∞–º: –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –Ω–µ–π—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ 3D, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã, –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, –∞ —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ü–µ–Ω–∫–∏ –∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã–µ –∑–∞–¥–∞—á–∏. –°—Ç–∞—Ç—å—è –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω, –≤–∫–ª—é—á–∞—è –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.'}, 'en': {'title': 'Advancing 3D Scene Generation with Deep Learning', 'desc': 'This paper reviews the latest techniques in 3D scene generation, which aims to create realistic and meaningful environments for various applications. It highlights the evolution from early procedural methods to modern deep generative models like GANs and diffusion models, which enhance the quality and diversity of generated scenes. The authors categorize these methods into four main paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation, analyzing their strengths and weaknesses. The paper also discusses ongoing challenges and future directions in the field, such as improving fidelity and integrating physics into scene generation.'}, 'zh': {'title': '3DÂú∫ÊôØÁîüÊàêÁöÑÊú™Êù•ÊñπÂêë‰∏éÊåëÊàò', 'desc': '3DÂú∫ÊôØÁîüÊàêÊó®Âú®ÂêàÊàêÂÖ∑ÊúâÁ©∫Èó¥ÁªìÊûÑ„ÄÅËØ≠‰πâÊÑè‰πâÂíåÁÖßÁâáÁúüÂÆûÊÑüÁöÑÁéØÂ¢ÉÔºåÂπøÊ≥õÂ∫îÁî®‰∫éÊ≤âÊµ∏ÂºèÂ™í‰Ωì„ÄÅÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂ÂíåÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁ≠âÈ¢ÜÂüü„ÄÇÊó©ÊúüÂü∫‰∫éÁ®ãÂ∫èËßÑÂàôÁöÑÊñπÊ≥ïËôΩÁÑ∂ÂÖ∑ÊúâÂèØÊâ©Â±ïÊÄßÔºå‰ΩÜÂ§öÊ†∑ÊÄßÊúâÈôê„ÄÇËøëÂπ¥Êù•ÔºåÊ∑±Â∫¶ÁîüÊàêÊ®°ÂûãÔºàÂ¶ÇGANÂíåÊâ©Êï£Ê®°ÂûãÔºâ‰ª•Âèä3DË°®Á§∫ÔºàÂ¶ÇNeRFÂíå3DÈ´òÊñØÔºâÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩøÂæóËÉΩÂ§üÂ≠¶‰π†ÁúüÂÆû‰∏ñÁïåÂú∫ÊôØÁöÑÂàÜÂ∏ÉÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÁîüÊàêÁöÑÁúüÂÆûÊÑü„ÄÅÂ§öÊ†∑ÊÄßÂíåËßÜÂõæ‰∏ÄËá¥ÊÄß„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜÊúÄÊñ∞ÁöÑ3DÂú∫ÊôØÁîüÊàêÊñπÊ≥ïÔºåÂàÜÊûê‰∫ÜÂÖ∂ÊäÄÊúØÂü∫Á°Ä„ÄÅÊùÉË°°Âíå‰ª£Ë°®ÊÄßÁªìÊûúÔºåÂπ∂ËÆ®ËÆ∫‰∫ÜÁîüÊàêËÉΩÂäõ„ÄÅ3DË°®Á§∫„ÄÅÊï∞ÊçÆÂíåËØÑ‰º∞Á≠âÂÖ≥ÈîÆÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04842', 'title': 'Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers', 'url': 'https://huggingface.co/papers/2505.04842', 'abstract': "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL^V that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL^V boosts MATH accuracy by over 20\\% with parallel sampling and enables 8-32times efficient test-time compute scaling compared to the base RL method. RL^V also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL^V achieves 1.2-1.6times higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.", 'score': 12, 'issue_id': 3688, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': 'ae650f18905f196e', 'authors': ['Kusha Sareen', 'Morgane M Moss', 'Alessandro Sordoni', 'Rishabh Agarwal', 'Arian Hosseini'], 'affiliations': ['Google DeepMind, Mila', 'Microsoft Research, Mila', 'Mila, McGill University', 'Mila, Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2505.04842.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#optimization', '#rl', '#math'], 'emoji': 'üß†', 'ru': {'title': 'RL^V: –£–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫–µ', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ RL^V, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. RL^V –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. RL^V —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Enhancing LLMs with Value-Driven Reinforcement Learning', 'desc': 'This paper introduces RL^V, a novel reinforcement learning method that enhances large language models (LLMs) by integrating value functions into the training process. Unlike traditional methods that discard learned value functions, RL^V allows LLMs to act as both reasoners and generative verifiers, improving their ability to verify outputs. The approach significantly increases accuracy in mathematical tasks and improves computational efficiency during testing, allowing for faster processing. Additionally, RL^V demonstrates strong performance across various task difficulties and domains, showcasing its versatility and effectiveness in real-world applications.'}, 'zh': {'title': 'RL^VÔºöÊèêÂçáÊé®ÁêÜ‰∏éÈ™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïRL^VÔºåÁî®‰∫é‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåRL^VÂêåÊó∂ËÆ≠ÁªÉLLM‰Ωú‰∏∫Êé®ÁêÜËÄÖÂíåÁîüÊàêÈ™åËØÅËÄÖÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÁîüÊàêÁöÑÊï∞ÊçÆÊù•Â¢ûÂº∫È™åËØÅËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRL^VÂú®MATH‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜË∂ÖËøá20%ÔºåÂπ∂‰∏îÂú®ÊµãËØïÊó∂ËÆ°ÁÆóÊïàÁéá‰∏äÊèêÂçá‰∫Ü8Âà∞32ÂÄç„ÄÇËØ•ÊñπÊ≥ïËøòÂ±ïÁ§∫‰∫ÜÂú®‰∏çÂêåÈöæÂ∫¶ÂíåÈ¢ÜÂüü‰ªªÂä°‰∏äÁöÑÂº∫Ê≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05327', 'title': 'ICon: In-Context Contribution for Automatic Data Selection', 'url': 'https://huggingface.co/papers/2505.05327', 'abstract': 'Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.', 'score': 11, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'b64bd2ecee9bb211', 'authors': ['Yixin Yang', 'Qingxiu Dong', 'Linli Yao', 'Fangwei Zhu', 'Zhifang Sui'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05327.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#data'], 'emoji': 'üß†', 'ru': {'title': 'ICon: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ICon. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (in-context learning) –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤–∫–ª–∞–¥–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Ä—É—á–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫. ICon –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ —Ç—Ä–µ—Ö LLM –∏ 12 –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø–æ–∑–≤–æ–ª–∏–≤ –º–æ–¥–µ–ª—è–º, –æ–±—É—á–µ–Ω–Ω—ã–º –Ω–∞ 15% –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –ø–æ–ª–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö ICon –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –æ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –∏–º–µ—é—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Efficient Data Selection for Better Language Model Training', 'desc': 'This paper introduces a new method called In-context Learning for Contribution Measurement (ICon) that helps select the best data for training Large Language Models (LLMs) without using complex gradient calculations. ICon leverages the concept of in-context learning to evaluate how much each data sample contributes to model performance, making it more efficient than traditional methods. The approach reduces reliance on human-designed heuristics, which can introduce bias, and instead focuses on the intrinsic qualities of the data. Experiments show that models trained with ICon-selected data perform better than those trained on full datasets or those selected by existing methods.'}, 'zh': {'title': 'È´òÊïàÊï∞ÊçÆÈÄâÊã©ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÈÄâÊã©ÊñπÊ≥ïÔºåÁß∞‰∏∫IConÔºàIn-context Learning for Contribution MeasurementÔºâÔºåÁî®‰∫éÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊÄßËÉΩÂπ∂Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇIConÊòØ‰∏ÄÁßçÊó†Ê¢ØÂ∫¶ÁöÑÊñπÊ≥ïÔºåÂà©Áî®‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÈöêÂºèÂæÆË∞ÉÁâπÊÄßÊù•ËØÑ‰º∞Ê†∑Êú¨ÁöÑË¥°ÁåÆÔºåËÄåÊó†ÈúÄËÆ°ÁÆóÊ¢ØÂ∫¶ÊàñËÆæËÆ°ÊâãÂä®ÊåáÊ†á„ÄÇÈÄöËøáÂú®‰∏âÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåIConÊòæÁ§∫Âá∫ÂÖ∂Âú®ÈÄâÊã©È´òË¥°ÁåÆÊï∞ÊçÆÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåËÉΩÂ§üÂú®ÂáèÂ∞ëËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®IConÈÄâÊã©ÁöÑ15%Êï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äË∂ÖËøá‰∫Ü‰ΩøÁî®ÂÆåÊï¥Êï∞ÊçÆÈõÜÁöÑÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03981', 'title': 'X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains', 'url': 'https://huggingface.co/papers/2505.03981', 'abstract': "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.", 'score': 11, 'issue_id': 3672, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '0e6c2f37e1536f9f', 'authors': ['Qianchu Liu', 'Sheng Zhang', 'Guanghui Qin', 'Timothy Ossowski', 'Yu Gu', 'Ying Jin', 'Sid Kiblawi', 'Sam Preston', 'Mu Wei', 'Paul Vozila', 'Tristan Naumann', 'Hoifung Poon'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.03981.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#transfer_learning', '#healthcare'], 'emoji': 'üß†', 'ru': {'title': '–û–±–æ–±—â–∞–µ–º—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–º–µ–Ω–∞–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–æ–±—â–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥–æ–º–µ–Ω—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç X-Reasoner - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω—É—é —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—â–µ–≥–æ –¥–æ–º–µ–Ω–∞ –¥–ª—è –æ–±–æ–±—â–∞–µ–º—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ X-Reasoner —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∏ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ - X-Reasoner-Med, –¥–æ—Å—Ç–∏–≥–∞—é—â–∞—è –Ω–æ–≤—ã—Ö —Ä–µ–∫–æ—Ä–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä—è–¥–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.'}, 'en': {'title': 'Unlocking Generalizable Reasoning Across Modalities with X-Reasoner', 'desc': 'This paper investigates whether reasoning abilities can be generalized across different types of data, specifically from text to other modalities like images. The authors present X-Reasoner, a vision-language model that is post-trained on general-domain text to enhance its reasoning capabilities. They employ a two-stage training process that includes supervised fine-tuning with detailed reasoning steps and reinforcement learning with measurable rewards. The results demonstrate that X-Reasoner not only excels in multimodal tasks but also improves in specialized fields, leading to the creation of X-Reasoner-Med, which sets new benchmarks in medical reasoning tasks.'}, 'zh': {'title': 'Êé®ÁêÜËÉΩÂäõÁöÑË∑®Ê®°ÊÄÅÊé®Âπø', 'desc': 'ÊúÄËøëÁöÑ‰∏ìÊúâÊ®°ÂûãÔºàÂ¶Ço3ÔºâÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∞Áé∞ÊúâÁöÑÂºÄÊ∫êÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËÆ≠ÁªÉ‰ªÖÂü∫‰∫éÊñáÊú¨ÁöÑÊé®ÁêÜÊ®°ÂûãÔºåËØÑ‰º∞‰πü‰∏ªË¶ÅÈôê‰∫éÊï∞Â≠¶Âíå‰∏ÄËà¨È¢ÜÂüü‰ªªÂä°„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜÊé®ÁêÜËÉΩÂäõÊâ©Â±ïÂà∞ÊñáÊú¨ËæìÂÖ•Âíå‰∏ÄËà¨È¢ÜÂüü‰πãÂ§ñ‰ªçÁÑ∂‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏Ä‰∏™Âü∫Êú¨ÁöÑÁ†îÁ©∂ÈóÆÈ¢òÔºöÊé®ÁêÜÊòØÂê¶ÂèØ‰ª•Ë∑®Ê®°ÊÄÅÂíåÈ¢ÜÂüüËøõË°åÊé®ÂπøÔºü'}}}, {'id': 'https://huggingface.co/papers/2505.05408', 'title': 'Crosslingual Reasoning through Test-Time Scaling', 'url': 'https://huggingface.co/papers/2505.05408', 'abstract': "Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.", 'score': 8, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'a6c39be653cbaefe', 'authors': ['Zheng-Xin Yong', 'M. Farid Adilazuarda', 'Jonibek Mansurov', 'Ruochen Zhang', 'Niklas Muennighoff', 'Carsten Eickhoff', 'Genta Indra Winata', 'Julia Kreutzer', 'Stephen H. Bach', 'Alham Fikri Aji'], 'affiliations': ['Brown University', 'Capital One', 'Cohere Labs', 'MBZUAI', 'Stanford University', 'University of T√ºbingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.05408.jpg', 'data': {'categories': ['#math', '#reasoning', '#low_resource', '#multilingual'], 'emoji': 'üåê', 'ru': {'title': '–ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —É–ª—É—á—à–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –¥–∞–∂–µ –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ú–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–∞—Ç—Ç–µ—Ä–Ω '—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ-–∏-—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ' –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –Ω–µ–∞–Ω–≥–ª–∏–π—Å–∫–∏–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ–Ω—Ç—Ä–æ–ª—è —è–∑—ã–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≤—ã—è–≤–∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –æ–±–æ–±—â–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–Ω–µ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏."}, 'en': {'title': 'Unlocking Multilingual Reasoning with English-Centric Models', 'desc': 'This paper explores how reasoning abilities of large language models, which are often trained in English, can be applied to other languages. The authors find that increasing computational resources for English-based reasoning models enhances their performance in multilingual mathematical reasoning, even in languages with fewer resources. They also identify a pattern where these models can effectively reason about non-English inputs by quoting and thinking in English. However, the study highlights challenges in transferring reasoning skills from STEM topics to cultural knowledge, indicating that while there is potential for cross-lingual reasoning, improvements are needed for low-resource languages and diverse contexts.'}, 'zh': {'title': 'ÊèêÂçáÂ§öËØ≠Ë®ÄÊé®ÁêÜËÉΩÂäõÁöÑÊΩúÂäõ‰∏éÊåëÊàò', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠Êé®ÁêÜÁöÑÂæÆË∞ÉÂ¶Ç‰ΩïÂú®ÂÖ∂‰ªñËØ≠Ë®Ä‰∏≠Êé®Âπø„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÈÄöËøáÂ¢ûÂä†Êé®ÁêÜËÆ°ÁÆóËÉΩÂäõÔºåËã±ËØ≠‰∏≠ÂøÉÁöÑÊé®ÁêÜËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÁßçËØ≠Ë®ÄÔºàÂåÖÊã¨‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºâ‰∏≠ÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçá„ÄÇÁ†îÁ©∂ËøòË°®ÊòéÔºåÂ∞ΩÁÆ°Ëã±ËØ≠‰∏≠ÂøÉÁöÑÊé®ÁêÜÈìæ‰∏ªË¶ÅÊòØËã±ËØ≠Ôºå‰ΩÜÂÆÉ‰ª¨Âú®Â§ÑÁêÜÈùûËã±ËØ≠ËæìÂÖ•Êó∂‰ªçËÉΩÈÅµÂæ™ÁâπÂÆöÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂèëÁé∞Ê®°ÂûãÂú®È´òËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÁöÑÊé®ÁêÜÊïàÊûúÊõ¥Â•ΩÔºå‰ΩÜÂú®Ë∑®È¢ÜÂüüÊé®ÁêÜÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ªéSTEMÈ¢ÜÂüüÂà∞ÊñáÂåñÂ∏∏ËØÜÁöÑËøÅÁßª‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05288', 'title': 'PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes', 'url': 'https://huggingface.co/papers/2505.05288', 'abstract': "We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.", 'score': 7, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '7ffec9bccc965d17', 'authors': ['Ahmed Abdelreheem', 'Filippo Aleotti', 'Jamie Watson', 'Zawar Qureshi', 'Abdelrahman Eldesokey', 'Peter Wonka', 'Gabriel Brostow', 'Sara Vicente', 'Guillermo Garcia-Hernando'], 'affiliations': ['KAUST', 'Niantic Spatial', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.05288.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#3d', '#survey', '#dataset'], 'emoji': 'üßä', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫ 3D-—Å—Ü–µ–Ω—ã, 3D-–º–æ–¥–µ–ª—å –æ–±—ä–µ–∫—Ç–∞ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è. –ó–∞–¥–∞—á–∞ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ 3D-–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö –∏ —Å–≤–æ–±–æ–¥–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —á—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç –µ–µ –æ—Ç –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ 3D. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫, –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏, –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ—à–µ–Ω–∏—è.'}, 'en': {'title': 'Placing Objects with Words in 3D Spaces!', 'desc': 'This paper presents a new task called Language-Guided Object Placement in Real 3D Scenes, which involves placing a 3D object based on a textual description. The model must analyze a point cloud of a 3D scene and determine a suitable location for the object that aligns with the given prompt. This task is particularly challenging due to the ambiguity of multiple valid placements and the need for understanding 3D spatial relationships. The authors introduce a benchmark, evaluation protocol, and a dataset to train 3D language models, aiming to enhance the evaluation of generalist 3D models.'}, 'zh': {'title': 'ËØ≠Ë®ÄÂºïÂØºÁöÑ3DÁâ©‰ΩìÊîæÁΩÆÊñ∞ÊåëÊàò', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÈ°πÊñ∞‰ªªÂä°ÔºöÂú®ÁúüÂÆû3DÂú∫ÊôØ‰∏≠ËøõË°åËØ≠Ë®ÄÂºïÂØºÁöÑÁâ©‰ΩìÊîæÁΩÆ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÊé•Êî∂‰∏Ä‰∏™3DÂú∫ÊôØÁöÑÁÇπ‰∫ë„ÄÅ‰∏Ä‰∏™3DËµÑ‰∫ßÂíå‰∏Ä‰∏™ÊñáÊú¨ÊèêÁ§∫Ôºå‰ªªÂä°ÊòØÊâæÂà∞‰∏Ä‰∏™Á¨¶ÂêàÊèêÁ§∫ÁöÑÊúâÊïàÊîæÁΩÆ‰ΩçÁΩÆ„ÄÇ‰∏éÂÖ∂‰ªñËØ≠Ë®ÄÂºïÂØºÁöÑ3DÂú∫ÊôØÂÆö‰Ωç‰ªªÂä°Áõ∏ÊØîÔºåËøôÈ°π‰ªªÂä°ÂÖ∑ÊúâÁâπÂÆöÁöÑÊåëÊàòÊÄßÔºåÂõ†‰∏∫ÂÆÉÂ≠òÂú®Â§ö‰∏™ÊúâÊïàËß£ÔºåÂπ∂‰∏îÈúÄË¶ÅÊé®ÁêÜ3DÂá†‰ΩïÂÖ≥Á≥ªÂíåËá™Áî±Á©∫Èó¥„ÄÇÊàë‰ª¨ÈÄöËøáÊèêÂá∫Êñ∞ÁöÑÂü∫ÂáÜÂíåËØÑ‰º∞ÂçèËÆÆÊù•ÂºÄÂêØËøô‰∏Ä‰ªªÂä°ÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÊçÆÈõÜÁî®‰∫éËÆ≠ÁªÉ3DÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ª•ÂèäÁ¨¨‰∏Ä‰∏™ÈùûÂπ≥Âá°ÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04769', 'title': 'Vision-Language-Action Models: Concepts, Progress, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2505.04769', 'abstract': 'Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models', 'score': 7, 'issue_id': 3682, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '75960b9512a0a05b', 'authors': ['Ranjan Sapkota', 'Yang Cao', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Biological & Environmental Engineering, Ithaca, New York, USA', 'The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong', 'University of the Peloponnese, Department of Informatics and Telecommunications, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.04769.jpg', 'data': {'categories': ['#agi', '#robotics', '#ethics', '#architecture', '#agents', '#multimodal', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—è, —è–∑—ã–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏—è: –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –µ–¥–∏–Ω—É—é –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ VLA –ø–æ –ø—è—Ç–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –û–±—Å—É–∂–¥–∞—é—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤–∞—Ö, –º–µ–¥–∏—Ü–∏–Ω–µ –∏ –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.'}, 'en': {'title': 'Unifying Vision, Language, and Action in AI', 'desc': 'Vision-Language-Action (VLA) models represent a significant step in AI by combining visual perception, language understanding, and physical actions into one system. This review organizes recent developments in VLA models into five key themes, highlighting their evolution from basic cross-modal learning to sophisticated generalist agents. It covers over 80 models from the last three years, focusing on innovations in architecture, training efficiency, and real-time performance. The paper also discusses applications in various fields and proposes solutions to challenges like real-time control and ethical deployment, aiming to guide future advancements in intelligent robotics and AI.'}, 'zh': {'title': 'Áªü‰∏ÄÊÑüÁü•‰∏éË°åÂä®ÁöÑÊô∫ËÉΩÊ®°Âûã', 'desc': 'ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊ®°ÂûãÊòØ‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑ‰∏ÄÈ°πÈáçË¶ÅËøõÂ±ïÔºåÊó®Âú®Â∞ÜÊÑüÁü•„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ÂíåÂÖ∑‰ΩìË°åÂä®Áªü‰∏ÄÂú®‰∏Ä‰∏™ËÆ°ÁÆóÊ°ÜÊû∂ÂÜÖ„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜVLAÊ®°ÂûãÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÁ≥ªÁªüÂú∞ÁªÑÁªá‰∫Ü‰∫î‰∏™‰∏ªÈ¢òÊîØÊü±ÔºåÊèèÁªò‰∫ÜËøô‰∏ÄÂø´ÈÄüÂèëÂ±ïÁöÑÈ¢ÜÂüü„ÄÇÊàë‰ª¨ÂõûÈ°æ‰∫ÜVLAÁ≥ªÁªüÁöÑÊ¶ÇÂøµÂü∫Á°ÄÔºåËøΩÊ∫ØÂÖ∂‰ªéË∑®Ê®°ÊÄÅÂ≠¶‰π†Êû∂ÊûÑÂà∞Á¥ßÂØÜÈõÜÊàêËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÅË°åÂä®ËßÑÂàíÂô®ÂíåÂ±ÇÊ¨°ÊéßÂà∂Âô®ÁöÑÊºîÂèò„ÄÇÊñáÁ´†ËøòÊé¢ËÆ®‰∫ÜVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂåªÁñó„ÄÅÂÜú‰∏öÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÂ§ö‰∏™Â∫îÁî®È¢ÜÂüüÁöÑÊåëÊàò‰∏éËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03422', 'title': 'LiftFeat: 3D Geometry-Aware Local Feature Matching', 'url': 'https://huggingface.co/papers/2505.03422', 'abstract': 'Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.', 'score': 7, 'issue_id': 3678, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '5be3a3b002db41cc', 'authors': ['Yepeng Liu', 'Wenpeng Lai', 'Zhou Zhao', 'Yuxuan Xiong', 'Jinchi Zhu', 'Jun Cheng', 'Yongchao Xu'], 'affiliations': ['Institute for Infocomm Research, A*STAR, Singapore', 'SF Technology, Shenzhen, China', 'School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03422.jpg', 'data': {'categories': ['#cv', '#robotics', '#3d'], 'emoji': 'ü§ñ', 'ru': {'title': 'LiftFeat: –ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é 3D –≥–µ–æ–º–µ—Ç—Ä–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å LiftFeat –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. LiftFeat –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤ –ø—É—Ç–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ 3D –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã. –°–µ—Ç—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π 3D –≥–µ–æ–º–µ—Ç—Ä–∏—é, –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–æ—Ä–º–∞–ª–µ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ 2D –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ LiftFeat –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ü–µ–Ω–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–∑—ã, –æ—Ü–µ–Ω–∫–∏ –≥–æ–º–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'LiftFeat: Enhancing Feature Matching with 3D Geometry', 'desc': 'This paper introduces LiftFeat, a lightweight neural network designed to improve local feature matching in challenging visual conditions. It addresses issues like drastic lighting changes and low texture areas by incorporating 3D geometric features into the feature extraction process. The network uses a pre-trained monocular depth estimation model to create pseudo surface normal labels, which guide the extraction of 3D features. Experimental results show that LiftFeat significantly enhances the performance of visual localization and pose estimation tasks compared to existing lightweight methods.'}, 'zh': {'title': 'ÊèêÂçáËßÜËßâÁâπÂæÅÂåπÈÖçÁöÑÈ≤ÅÊ£íÊÄß‰∏éÊïàÁéá', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËΩªÈáèÁ∫ßÁΩëÁªúLiftFeatÔºåÊó®Âú®ÊèêÈ´òÂú®ÊûÅÁ´ØÊù°‰ª∂‰∏ãÁöÑËßÜËßâÁâπÂæÅÂåπÈÖçËÉΩÂäõ„ÄÇÈÄöËøáËÅöÂêà‰∏âÁª¥Âá†‰ΩïÁâπÂæÅÔºåLiftFeatÂ¢ûÂº∫‰∫ÜÂéüÂßãÊèèËø∞Á¨¶ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁîüÊàê‰º™Ë°®Èù¢Ê≥ïÁ∫øÊ†áÁ≠æÔºå‰ª•ÊåáÂØº‰∏âÁª¥Âá†‰ΩïÁâπÂæÅÁöÑÊèêÂèñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLiftFeatÂú®Áõ∏ÂØπÂßøÊÄÅ‰º∞ËÆ°„ÄÅÂçïÂ∫îÊÄß‰º∞ËÆ°ÂíåËßÜËßâÂÆö‰Ωç‰ªªÂä°‰∏≠‰ºò‰∫é‰∏Ä‰∫õËΩªÈáèÁ∫ßÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05064', 'title': 'WaterDrum: Watermarking for Data-centric Unlearning Metric', 'url': 'https://huggingface.co/papers/2505.05064', 'abstract': 'Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.', 'score': 6, 'issue_id': 3684, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '8d04facba381d919', 'authors': ['Xinyang Lu', 'Xinyuan Niu', 'Gregory Kang Ruey Lau', 'Bui Thi Cam Nhung', 'Rachael Hwee Ling Sim', 'Fanyu Wen', 'Chuan-Sheng Foo', 'See-Kiong Ng', 'Bryan Kian Hsiang Low'], 'affiliations': ['A*STAR, Way, Create', 'CNRS@CREATE, 1 Singapore #08Tower, Computer Singapore (CFAR), Create', 'Centre for Singapore Science, National University', 'Department of of Singapore', 'Frontier AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05064.jpg', 'data': {'categories': ['#leakage', '#dataset', '#benchmark', '#data', '#open_source'], 'emoji': 'üßΩ', 'ru': {'title': 'WaterDrum: –¢–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—É—é WaterDrum. –≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è LLM, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. WaterDrum –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –≥–¥–µ –∑–∞–±—ã–≤–∞–µ–º—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–º–∏.'}, 'en': {'title': 'WaterDrum: A New Era in LLM Unlearning Metrics', 'desc': 'This paper addresses the challenge of unlearning in large language models (LLMs), which is essential for removing sensitive or harmful data. Current metrics for evaluating unlearning focus on model performance but may not reflect true unlearning effectiveness, especially when data is semantically similar. The authors propose a new data-centric unlearning metric called WaterDrum, which utilizes robust text watermarking to better assess unlearning in practical scenarios. Additionally, they introduce benchmark datasets designed to test unlearning algorithms under various conditions, enhancing the evaluation process for LLM unlearning.'}, 'zh': {'title': 'ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÅóÂøòËÉΩÂäõ', 'desc': 'Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÈÅóÂøòËÉΩÂäõÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÊúâÊïàÂéªÈô§Áî®Êà∑ÁöÑÁßÅ‰∫∫„ÄÅÁâàÊùÉÊàñÊúâÂÆ≥Êï∞ÊçÆÂΩ±ÂìçÊó∂„ÄÇÁé∞ÊúâÁöÑ‰ª•ÊïàÁî®‰∏∫‰∏≠ÂøÉÁöÑÈÅóÂøòÂ∫¶ÈáèÂèØËÉΩÊó†Ê≥ïÂáÜÁ°ÆËØÑ‰º∞Âú®ËØ≠‰πâÁõ∏‰ººÂÜÖÂÆπÁöÑÊÉÖÂÜµ‰∏ãÁöÑÈÅóÂøòÁ®ãÂ∫¶„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆ‰∏≠ÂøÉÈÅóÂøòÂ∫¶ÈáèÊñπÊ≥ïÔºåÁß∞‰∏∫WaterDrumÔºåÂà©Áî®Á®≥ÂÅ•ÁöÑÊñáÊú¨Ê∞¥Âç∞ÊäÄÊúØÊù•ÂÖãÊúçËøô‰∫õÈôêÂà∂„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÁî®‰∫éËØÑ‰º∞LLMÁöÑÈÅóÂøòÁÆóÊ≥ïÔºåÁ°Æ‰øùÂèØ‰ª•‰∏•Ê†ºÊµãËØïÈÅóÂøòÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02363', 'title': 'SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in\n  Language Model Preference Learning', 'url': 'https://huggingface.co/papers/2505.02363', 'abstract': 'Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.', 'score': 6, 'issue_id': 3687, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'da8e6360c88cd0ed', 'authors': ['Tianjian Li', 'Daniel Khashabi'], 'affiliations': ['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, US'], 'pdf_title_img': 'assets/pdf/title_img/2505.02363.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#dataset'], 'emoji': 'üîÄ', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ on-policy –∏ off-policy –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –≤–∑–∞–∏–º–æ–¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ SIMPLEMIX, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º –∏—Ö –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–º–µ—à–∏–≤–∞–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SIMPLEMIX –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã.'}, 'en': {'title': 'SIMPLEMIX: Uniting On-Policy and Off-Policy for Better Language Model Alignment', 'desc': 'This paper explores how to align language models with human preferences using two types of data: on-policy and off-policy. On-policy data is shown to excel in tasks requiring reasoning, like math and coding, while off-policy data is better for creative and open-ended tasks. The authors propose a new method called SIMPLEMIX, which effectively combines these two data sources to leverage their strengths. Their experiments demonstrate that SIMPLEMIX significantly enhances the performance of language models compared to existing methods, achieving notable improvements in alignment metrics.'}, 'zh': {'title': 'ÁªìÂêàÂú®Á∫ø‰∏éÁ¶ªÁ∫øÊï∞ÊçÆÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÊïàÊûú', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®ÂÅèÂ•ΩÂ≠¶‰π†‰∏≠ÔºåÂú®Á∫øÊï∞ÊçÆÂíåÁ¶ªÁ∫øÊï∞ÊçÆÁöÑ‰∫íË°•‰ºòÂäø„ÄÇÂú®Á∫øÊï∞ÊçÆÂú®Êé®ÁêÜ‰ªªÂä°ÔºàÂ¶ÇÊï∞Â≠¶ÂíåÁºñÁ®ãÔºâ‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËÄåÁ¶ªÁ∫øÊï∞ÊçÆÂú®ÂºÄÊîæÂºè‰ªªÂä°ÔºàÂ¶ÇÂàõÊÑèÂÜô‰ΩúÂíå‰∏™‰∫∫Êé®ËçêÔºâ‰∏≠Êõ¥ÂÖ∑‰ºòÂäø„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SIMPLEMIXÁöÑÊñπÊ≥ïÔºåÈÄöËøáÁÆÄÂçïÂú∞Ê∑∑ÂêàËøô‰∏§ÁßçÊï∞ÊçÆÊ∫êÔºåÁªìÂêàÂÆÉ‰ª¨ÁöÑ‰ºòÁÇπ„ÄÇÂÆûÈ™åËØÅÊòéÔºåSIMPLEMIXÂú®Â§öÁßç‰ªªÂä°ÂíåÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÈΩêÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04955', 'title': 'Chain-of-Thought Tokens are Computer Program Variables', 'url': 'https://huggingface.co/papers/2505.04955', 'abstract': 'Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically study the role of CoT tokens in LLMs on two compositional tasks: multi-digit multiplication and dynamic programming. While CoT is essential for solving these problems, we find that preserving only tokens that store intermediate results would achieve comparable performance. Furthermore, we observe that storing intermediate results in an alternative latent form will not affect model performance. We also randomly intervene some values in CoT, and notice that subsequent CoT tokens and the final answer would change correspondingly. These findings suggest that CoT tokens may function like variables in computer programs but with potential drawbacks like unintended shortcuts and computational complexity limits between tokens. The code and data are available at https://github.com/solitaryzero/CoTs_are_Variables.', 'score': 5, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'a8bb5e6b1e09e5dd', 'authors': ['Fangwei Zhu', 'Peiyi Wang', 'Zhifang Sui'], 'affiliations': ['School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04955.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#architecture', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–¢–æ–∫–µ–Ω—ã CoT –≤ LLM: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π (Chain-of-Thought, CoT) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Ä–æ–ª—å —Ç–æ–∫–µ–Ω–æ–≤ CoT –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –∑–∞–¥–∞—á –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã CoT —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É—é—Ç –ø–æ–¥–æ–±–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –º–µ—Ç–æ–¥–∞ CoT, —Ç–∞–∫–∏–µ –∫–∞–∫ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —É–ø—Ä–æ—â–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏.'}, 'en': {'title': 'Unlocking the Power of Chain-of-Thought Tokens in LLMs', 'desc': "This paper investigates the role of chain-of-thought (CoT) tokens in large language models (LLMs) when solving complex reasoning tasks. The authors find that while CoT is important for tasks like multi-digit multiplication and dynamic programming, retaining only tokens that represent intermediate results can yield similar performance. They also discover that storing these results in a different latent form does not impact the model's effectiveness. Additionally, the study reveals that altering CoT values affects subsequent tokens and final answers, indicating that CoT tokens may act like variables in programming, albeit with some limitations."}, 'zh': {'title': 'ÈìæÂºèÊé®ÁêÜÔºöÂèòÈáèÁöÑÂäõÈáè‰∏éÊåëÊàò', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÈìæÂºèÊé®ÁêÜÔºàCoTÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑ‰ΩúÁî®ÔºåÁâπÂà´ÊòØÂú®Â§ö‰ΩçÊï∞‰πòÊ≥ïÂíåÂä®ÊÄÅËßÑÂàíËøô‰∏§‰∏™Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËôΩÁÑ∂CoTÂØπ‰∫éËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜ‰ªÖ‰øùÁïôÂ≠òÂÇ®‰∏≠Èó¥ÁªìÊûúÁöÑÊ†áËÆ∞‰πüËÉΩËææÂà∞Áõ∏‰ººÁöÑÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÂ∞Ü‰∏≠Èó¥ÁªìÊûú‰ª•Âè¶‰∏ÄÁßçÊΩúÂú®ÂΩ¢ÂºèÂ≠òÂÇ®‰∏ç‰ºöÂΩ±ÂìçÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÊúÄÂêéÔºåÈöèÊú∫Âπ≤È¢ÑCoT‰∏≠ÁöÑÊüê‰∫õÂÄº‰ºöÂØºËá¥ÂêéÁª≠ÁöÑCoTÊ†áËÆ∞ÂíåÊúÄÁªàÁ≠îÊ°àÁõ∏Â∫îÂèòÂåñÔºåËøôË°®ÊòéCoTÊ†áËÆ∞ÂèØËÉΩÂÉèËÆ°ÁÆóÊú∫Á®ãÂ∫è‰∏≠ÁöÑÂèòÈáèÔºå‰ΩÜ‰πüÂ≠òÂú®‰∏Ä‰∫õÊΩúÂú®ÁöÑÁº∫Èô∑ÔºåÂ¶ÇÊÑèÂ§ñÁöÑÊç∑ÂæÑÂíåÊ†áËÆ∞‰πãÈó¥ÁöÑËÆ°ÁÆóÂ§çÊùÇÊÄßÈôêÂà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.19314', 'title': 'BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese', 'url': 'https://huggingface.co/papers/2504.19314', 'abstract': "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.", 'score': 4, 'issue_id': 3673, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 –∞–ø—Ä–µ–ª—è', 'en': 'April 27', 'zh': '4Êúà27Êó•'}, 'hash': '06aff0f566bd3817', 'authors': ['Peilin Zhou', 'Bruce Leon', 'Xiang Ying', 'Can Zhang', 'Yifan Shao', 'Qichen Ye', 'Dading Chong', 'Zhiling Jin', 'Chenxuan Xie', 'Meng Cao', 'Yuxin Gu', 'Sixin Hong', 'Jing Ren', 'Jian Chen', 'Chao Liu', 'Yining Hua'], 'affiliations': ['Alibaba Group', 'HSBC', 'Harvard T.H. Chan School of Public Health', 'Hong Kong University of Science and Technology (Guangzhou)', 'MBZUAI', 'Mindverse AI', 'NIO', 'Peking University', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.19314.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#dataset', '#benchmark', '#low_resource'], 'emoji': 'üåê', 'ru': {'title': 'BrowseComp-ZH: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BrowseComp-ZH - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–∏—Ç–∞–π—Å–∫–∏–º –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 289 —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ 11 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–≤–æ–∏ —Å–∏–ª—å–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –∏ –ø–æ–∏—Å–∫–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ø–ú –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —ç—Ç–æ–º —Ç–µ—Å—Ç–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–ª—è —É—Å–ø–µ—Ö–∞ –≤ BrowseComp-ZH —Ç—Ä–µ–±—É—é—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞, –Ω–æ –∏ —Å–ª–æ–∂–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.'}, 'en': {'title': 'Evaluating LLMs: The Challenge of Chinese Web Browsing', 'desc': 'This paper introduces BrowseComp-ZH, a benchmark designed to evaluate large language models (LLMs) on their ability to browse and retrieve information from the Chinese web. It consists of 289 challenging multi-hop questions across various domains, focusing on high difficulty and unique answers. The study reveals that most state-of-the-art models perform poorly, with accuracy rates often below 10%, highlighting the complexity of reasoning and retrieval in non-English contexts. The results indicate that current LLMs still face significant challenges in mastering the necessary skills for effective information retrieval and reasoning in diverse linguistic environments.'}, 'zh': {'title': '‰∏≠ÊñáÁΩëÁªúÊô∫ËÉΩ‰ΩìËØÑ‰º∞Êñ∞Âü∫ÂáÜÔºöBrowseComp-ZH', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄêÊ∏êÊºîÂèò‰∏∫‰ΩøÁî®Â∑•ÂÖ∑ÁöÑÊô∫ËÉΩ‰ΩìÔºåÂÆûÊó∂ÊµèËßàÁΩëÁªúÁöÑËÉΩÂäõÊàê‰∏∫Ë°°ÈáèÂÖ∂Êé®ÁêÜÂíåÊ£ÄÁ¥¢ËÉΩÂäõÁöÑÈáçË¶ÅÊ†áÂáÜ„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÂ¶ÇBrowseComp‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëã±ËØ≠ÔºåÂøΩËßÜ‰∫ÜÂÖ∂‰ªñ‰∏ªË¶Å‰ø°ÊÅØÁîüÊÄÅÁ≥ªÁªüÔºàÂ∞§ÂÖ∂ÊòØ‰∏≠ÊñáÔºâÂú®ËØ≠Ë®Ä„ÄÅÂü∫Á°ÄËÆæÊñΩÂíåÂÆ°Êü•ÊñπÈù¢ÁöÑÂ§çÊùÇÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜBrowseComp-ZHÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®‰∏∫ÂÖ®Èù¢ËØÑ‰º∞‰∏≠ÊñáÁΩëÁªú‰∏äÁöÑLLMÊô∫ËÉΩ‰ΩìËÄåËÆæËÆ°ÁöÑÈ´òÈöæÂ∫¶Âü∫ÂáÜÊµãËØï„ÄÇËØ•Âü∫ÂáÜÊµãËØïÂåÖÂê´289‰∏™Ë∑®Ë∂ä11‰∏™‰∏çÂêåÈ¢ÜÂüüÁöÑÂ§öË∑≥ÈóÆÈ¢òÔºåÊó®Âú®ËÄÉÂØüÊ®°ÂûãÁöÑÊ£ÄÁ¥¢Á≠ñÁï•„ÄÅÊé®ÁêÜËÉΩÂäõÂíå‰ø°ÊÅØÊï¥ÂêàËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07916', 'title': 'MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder', 'url': 'https://huggingface.co/papers/2505.07916', 'abstract': 'We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.', 'score': 80, 'issue_id': 3752, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '30175415a859995c', 'authors': ['Bowen Zhang', 'Congchao Guo', 'Geng Yang', 'Hang Yu', 'Haozhe Zhang', 'Heidi Lei', 'Jialong Mai', 'Junjie Yan', 'Kaiyue Yang', 'Mingqi Yang', 'Peikai Huang', 'Ruiyang Jin', 'Sitan Jiang', 'Weihua Cheng', 'Yawei Li', 'Yichen Xiao', 'Yiying Zhou', 'Yongmao Zhang', 'Yuan Lu', 'Yucen He'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07916.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#games', '#audio'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ —Ä–µ—á–∏: MiniMax-Speech - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∫–ª–æ–Ω', 'desc': 'MiniMax-Speech - —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π —è–≤–ª—è–µ—Ç—Å—è –æ–±—É—á–∞–µ–º—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–µ–º–±—Ä–∞ –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –µ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 32 —è–∑—ã–∫–∞ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–Ω–æ–≥–∏–º –º–µ—Ç—Ä–∏–∫–∞–º, –¥–æ—Å—Ç–∏–≥–∞—è –ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –≥–æ–ª–æ—Å–∞. MiniMax-Speech —Ç–∞–∫–∂–µ –æ–±–ª–∞–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å—é, –ø–æ–∑–≤–æ–ª—è—é—â–µ–π —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏—è–º–∏ –≥–æ–ª–æ—Å–∞ –∏ —Å–∏–Ω—Ç–µ–∑ —Ç–µ–º–±—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Speech Synthesis with MiniMax-Speech', 'desc': 'MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.'}, 'zh': {'title': 'MiniMax-SpeechÔºöÈ´òË¥®ÈáèËØ≠Èü≥ÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'MiniMax-SpeechÊòØ‰∏ÄÁßçÂü∫‰∫éËá™ÂõûÂΩíTransformerÁöÑÊñáÊú¨Âà∞ËØ≠Èü≥ÔºàTTSÔºâÊ®°ÂûãÔºåËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑËØ≠Èü≥„ÄÇÂÖ∂ÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂèØÂ≠¶‰π†ÁöÑËØ¥ËØù‰∫∫ÁºñÁ†ÅÂô®ÔºåÂèØ‰ª•‰ªéÂèÇËÄÉÈü≥È¢ë‰∏≠ÊèêÂèñÈü≥Ëâ≤ÁâπÂæÅÔºåËÄåÊó†ÈúÄÂÖ∂ËΩ¨ÂΩï„ÄÇËØ•Ê®°ÂûãÊîØÊåÅÈõ∂Ê†∑Êú¨ÁîüÊàêÂÖ∑ÊúâÂèÇËÄÉÈü≥Ëâ≤ÁöÑË°®ËææÊÄßËØ≠Èü≥ÔºåÂπ∂‰∏îÂú®ÂçïÊ†∑Êú¨ËØ≠Èü≥ÂÖãÈöÜ‰∏≠Ë°®Áé∞Âá∫ÊûÅÈ´òÁöÑÁõ∏‰ººÂ∫¶„ÄÇÊ≠§Â§ñÔºåMiniMax-SpeechÂú®Â§ö‰∏™ÂÆ¢ËßÇÂíå‰∏ªËßÇËØÑ‰º∞ÊåáÊ†á‰∏äË°®Áé∞‰ºòÂºÇÔºåÊîØÊåÅ32ÁßçËØ≠Ë®ÄÔºåÂπ∂Âú®ËØ≠Èü≥ÂÖãÈöÜÁöÑÂÆ¢ËßÇÊåáÊ†á‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07591', 'title': 'A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models', 'url': 'https://huggingface.co/papers/2505.07591', 'abstract': "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.", 'score': 7, 'issue_id': 3750, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'ca7c47ccc0066e55', 'authors': ['Junjie Ye', 'Caishuang Huang', 'Zhuohan Chen', 'Wenjie Fu', 'Chenyuan Yang', 'Leyi Yang', 'Yilong Wu', 'Peng Wang', 'Meng Zhou', 'Xiaolong Yang', 'Tao Gui', 'Qi Zhang', 'Zhongchao Shi', 'Jianping Fan', 'Xuanjing Huang'], 'affiliations': ['Institute of Modern Languages and Linguistics, Fudan University', 'Lenovo Research', 'School of Computer Science, Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.07591.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#optimization', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': '–ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—é—â—É—é —Ç—Ä–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞, —á–µ—Ç—ã—Ä–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —á–µ—Ç—ã—Ä–µ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã –±—ã–ª —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª–∏–ª —Å–æ–∑–¥–∞—Ç—å 1200 –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 19 LLM –ø–æ–∫–∞–∑–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, –ø—Ä–∏—á–µ–º —Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–∞–ª–∞—Å—å —Å 77,67% –Ω–∞ —É—Ä–æ–≤–Ω–µ I –¥–æ 32,96% –Ω–∞ —É—Ä–æ–≤–Ω–µ IV.'}, 'en': {'title': 'Enhancing LLMs with Multi-Dimensional Constraints', 'desc': 'This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters.'}, 'zh': {'title': 'Â§öÁª¥Á∫¶ÊùüÊ°ÜÊû∂ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈÅµÂæ™Áî®Êà∑ÂÆö‰πâÁ∫¶ÊùüÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÈÄöÂ∏∏‰æùËµñ‰∫éÊ®°ÊùøÂåñÁöÑÁ∫¶ÊùüÊèêÁ§∫ÔºåÁº∫‰πèÁúüÂÆû‰∏ñÁïå‰ΩøÁî®ÁöÑÂ§öÊ†∑ÊÄßÔºåÈôêÂà∂‰∫ÜÁªÜËá¥ÁöÑÊÄßËÉΩËØÑ‰º∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÁª¥Á∫¶ÊùüÊ°ÜÊû∂ÔºåÊ∂µÁõñ‰∏âÁßçÁ∫¶ÊùüÊ®°Âºè„ÄÅÂõõÁßçÁ∫¶ÊùüÁ±ªÂà´ÂíåÂõõ‰∏™ÈöæÂ∫¶Á∫ßÂà´„ÄÇÈÄöËøáËøô‰∏™Ê°ÜÊû∂ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Ëá™Âä®ÂåñÊåá‰ª§ÁîüÊàêÁÆ°ÈÅìÔºåÁîüÊàê‰∫Ü1200‰∏™ÂèØÈ™åËØÅÁöÑÊåá‰ª§ÈÅµÂæ™ÊµãËØïÊ†∑Êú¨ÔºåÂπ∂ËØÑ‰º∞‰∫Ü19‰∏™LLMÂú®‰∏çÂêåÁ∫¶ÊùüÂΩ¢Âºè‰∏ãÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07215', 'title': 'Measuring General Intelligence with Generated Games', 'url': 'https://huggingface.co/papers/2505.07215', 'abstract': 'We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.', 'score': 5, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'ed99ec3875dd9a95', 'authors': ['Vivek Verma', 'David Huang', 'William Chen', 'Dan Klein', 'Nicholas Tomlin'], 'affiliations': ['Computer Science Division, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.07215.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#rl', '#games', '#synthetic'], 'emoji': 'üéÆ', 'ru': {'title': 'gg-bench: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–≥—Ä–æ–≤—ã–µ —Å—Ä–µ–¥—ã', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω gg-bench - –Ω–∞–±–æ—Ä –∏–≥—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –æ–±—â–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. gg-bench –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –∏–≥—Ä—ã —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM), –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∏ –∫–æ–¥ –∏–≥—Ä. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–≥—Ä–∞—é—Ç –ø—Ä–æ—Ç–∏–≤ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∏–∑–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (7-9% –ø–æ–±–µ–¥), –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–æ—Å—Ç–∏–≥–∞—é—Ç 31-36% –ø–æ–±–µ–¥.'}, 'en': {'title': 'Dynamic Game Environments for Evaluating AI Reasoning', 'desc': 'The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI.'}, 'zh': {'title': 'gg-benchÔºöËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ügg-benchÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞ËØ≠Ë®ÄÊ®°Âûã‰∏ÄËà¨Êé®ÁêÜËÉΩÂäõÁöÑÊ∏∏ÊàèÁéØÂ¢ÉÈõÜÂêà„ÄÇ‰∏éÂ§ßÂ§öÊï∞ÈùôÊÄÅÂü∫ÂáÜÊµãËØï‰∏çÂêåÔºågg-benchÊòØ‰∏Ä‰∏™Êï∞ÊçÆÁîüÊàêËøáÁ®ãÔºåÂèØ‰ª•ÈöèÊó∂ÁîüÊàêÊñ∞ÁöÑËØÑ‰º∞ÂÆû‰æã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºågg-benchÈÄöËøá‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊñ∞Ê∏∏ÊàèÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÔºåÂπ∂Â∞ÜÊØè‰∏™Ê∏∏ÊàèÂÆûÁé∞‰∏∫GymÁéØÂ¢ÉÔºåÊúÄÂêéÈÄöËøáËá™ÊàëÂØπÂºàËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜ„ÄÇÊàë‰ª¨ÈÄöËøáÊ®°ÂûãÂú®Ëøô‰∫õÊ∏∏Êàè‰∏≠‰∏éÂº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜÁöÑËÉúÁéáÊù•ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÔºåÂèëÁé∞ÂΩìÂâçÊúÄÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®gg-bench‰∏äÁöÑËÉúÁéá‰ªÖ‰∏∫7-9%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.05464', 'title': 'Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging', 'url': 'https://huggingface.co/papers/2505.05464', 'abstract': 'Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.', 'score': 4, 'issue_id': 3760, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': 'd0d13229ec81018d', 'authors': ['Shiqi Chen', 'Jinghan Zhang', 'Tongyao Zhu', 'Wei Liu', 'Siyang Gao', 'Miao Xiong', 'Manling Li', 'Junxian He'], 'affiliations': ['City University of Hong Kong', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05464.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#architecture', '#reasoning', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—è –∏ –º—ã—à–ª–µ–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –æ—Ç —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è –≤ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ—è—Ö –º–æ–¥–µ–ª–∏, –∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –≤ —Å—Ä–µ–¥–Ω–∏—Ö –∏ –ø–æ–∑–¥–Ω–∏—Ö. –ü–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è –≤—Å–µ —Å–ª–æ–∏ –Ω–∞—á–∏–Ω–∞—é—Ç —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º.'}, 'en': {'title': 'Merging Models: Bridging Vision and Reasoning', 'desc': 'This paper investigates how to combine the visual understanding of Vision-Language Models (VLMs) with the reasoning skills of Large Language Models (LLMs) through a process called model merging. The authors propose a novel approach that merges models from different modalities, allowing VLMs to gain reasoning capabilities without the need for additional training. Their experiments reveal that while perception is mainly captured in the early layers of the model, reasoning is enhanced across all layers after merging. This study highlights the effectiveness of model merging for improving multimodal integration and understanding the interplay between perception and reasoning.'}, 'zh': {'title': 'Ê®°ÂûãÂêàÂπ∂ÔºöË∑®Ê®°ÊÄÅÁöÑÊé®ÁêÜ‰∏éÊÑüÁü•ÁªìÂêà', 'desc': 'ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂ∞ÜËßÜËßâÊÑüÁü•‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõÁªìÂêàÂú®‰∏ÄËµ∑„ÄÇÁÑ∂ËÄåÔºåËøô‰∏§ÁßçËÉΩÂäõÂ¶Ç‰ΩïÁªìÂêàÂπ∂Áõ∏‰∫íË¥°ÁåÆ‰ªçÁÑ∂‰∏çÂ§™Ê∏ÖÊ•ö„ÄÇÊàë‰ª¨ÈÄöËøáÊ®°ÂûãÂêàÂπ∂ÁöÑÊñπÊ≥ïÊù•Êé¢Á¥¢ÊÑüÁü•‰∏éÊé®ÁêÜÁöÑÁªÑÂêàÔºåËøûÊé•‰∏çÂêåÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊ®°ÂûãÂêàÂπ∂ËÉΩÂ§üÊàêÂäüÂú∞Â∞ÜLLMsÁöÑÊé®ÁêÜËÉΩÂäõËΩ¨ÁßªÂà∞VLMsÔºåÂπ∂‰∏îÊúâÂä©‰∫éÁêÜËß£ÊÑüÁü•‰∏éÊé®ÁêÜÁöÑÂÜÖÈÉ®Êú∫Âà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08638', 'title': 'TRAIL: Trace Reasoning and Agentic Issue Localization', 'url': 'https://huggingface.co/papers/2505.08638', 'abstract': 'The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '823a40ec2bb1f793', 'authors': ['Darshan Deshpande', 'Varun Gangal', 'Hersh Mehta', 'Jitin Krishnan', 'Anand Kannappan', 'Rebecca Qian'], 'affiliations': ['Patronus AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.08638.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#agents', '#open_source'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ç–ª–∞–¥–∫–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –æ—Ç —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏ –æ—à–∏–±–æ–∫ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö TRAIL –∏–∑ 148 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞—Å—Å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ—Ç–ª–∞–¥–∫–æ–π —Ç—Ä–∞—Å—Å, –¥–∞–∂–µ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å Gemini-2.5-pro –¥–æ—Å—Ç–∏–≥–ª–∞ –≤—Å–µ–≥–æ 11% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ TRAIL. –î–∞—Ç–∞—Å–µ—Ç –∏ –∫–æ–¥ –æ—Ç–∫—Ä—ã—Ç—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.'}, 'en': {'title': 'Revolutionizing Evaluation for Agentic Workflows', 'desc': 'This paper addresses the challenges of evaluating agentic workflows, which are increasingly used in various fields. Current methods rely on manual analysis of complex workflow traces, making them inefficient as the volume of data grows. The authors propose a new framework that includes a taxonomy of error types specific to agentic systems and introduce a dataset of 148 annotated traces for evaluation. Their findings indicate that existing large language models struggle with debugging these traces, highlighting the need for improved evaluation techniques in this area.'}, 'zh': {'title': 'Êô∫ËÉΩÂ∑•‰ΩúÊµÅËØÑ‰º∞ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöèÁùÄÊô∫ËÉΩÂ∑•‰ΩúÊµÅÂú®ÂêÑ‰∏™È¢ÜÂüüÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåÁ≥ªÁªüÂú∞ËØÑ‰º∞Ëøô‰∫õÁ≥ªÁªüÁîüÊàêÁöÑÂ§çÊùÇËΩ®ËøπÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁõÆÂâçÁöÑËØÑ‰º∞ÊñπÊ≥ï‰æùËµñ‰∫é‰∫∫Â∑•„ÄÅÁâπÂÆöÈ¢ÜÂüüÁöÑÂàÜÊûêÔºåËøôÁßçÊñπÊ≥ïÊó†Ê≥ïÈÄÇÂ∫îÊó•ÁõäÂ§çÊùÇÂíåÂ∫ûÂ§ßÁöÑÊô∫ËÉΩËæìÂá∫„ÄÇÈîôËØØÂàÜÊûêÂú®Ëøô‰∫õÁéØÂ¢É‰∏≠ÂèòÂæóÊõ¥Âä†Â§çÊùÇÔºåÂõ†‰∏∫Â§ñÈÉ®Â∑•ÂÖ∑ËæìÂá∫‰∏éËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®Ôºå‰ΩøÂæóË∞ÉËØïÂèòÂæóÊõ¥Âä†Âõ∞Èöæ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈíàÂØπÊô∫ËÉΩÂ∑•‰ΩúÊµÅËΩ®ËøπÁöÑÂä®ÊÄÅËØÑ‰º∞ÊñπÊ≥ïÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÈîôËØØÁ±ªÂûãÁöÑÊ≠£ÂºèÂàÜÁ±ªÊ≥ïÔºåÂêåÊó∂ÊûÑÂª∫‰∫Ü148‰∏™Â§ßÂûã‰∫∫Á±ªÊ†áÊ≥®ÁöÑËΩ®ËøπÊï∞ÊçÆÈõÜÔºå‰ª•ÊîØÊåÅÊú™Êù•Âú®Êô∫ËÉΩÂ∑•‰ΩúÊµÅËØÑ‰º∞ÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08175', 'title': 'Fast Text-to-Audio Generation with Adversarial Post-Training', 'url': 'https://huggingface.co/papers/2505.08175', 'abstract': 'Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating approx12s of 44.1kHz stereo audio in approx75ms on an H100, and approx7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '845d8b64408c2d62', 'authors': ['Zachary Novack', 'Zach Evans', 'Zack Zukowski', 'Josiah Taylor', 'CJ Carr', 'Julian Parker', 'Adnan Al-Sinan', 'Gian Marco Iodice', 'Julian McAuley', 'Taylor Berg-Kirkpatrick', 'Jordi Pons'], 'affiliations': ['Arm', 'Stability AI', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.08175.jpg', 'data': {'categories': ['#audio', '#inference', '#optimization', '#diffusion', '#open_source'], 'emoji': 'üöÄ', 'ru': {'title': '–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –ø–æ —Ç–µ–∫—Å—Ç—É —Å –ø–æ–º–æ—â—å—é ARC –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –ø–æ —Ç–µ–∫—Å—Ç—É - Adversarial Relativistic-Contrastive (ARC) –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ. –≠—Ç–æ –ø–µ—Ä–≤—ã–π —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —É—Å–∫–æ—Ä–µ–Ω–∏—è, –Ω–µ –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏. ARC —Å–æ—á–µ—Ç–∞–µ—Ç —Ä–µ–ª—è—Ç–∏–≤–∏—Å—Ç—Å–∫–∏–π —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –Ω–æ–≤–æ–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–æ–º–ø—Ç—É. –í —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ Stable Audio Open –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 12 —Å–µ–∫—É–Ω–¥ —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ 44.1–∫–ì—Ü –∑–∞ 75 –º—Å –Ω–∞ H100 –∏ 7 —Å–µ–∫—É–Ω–¥ –Ω–∞ –º–æ–±–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ.'}, 'en': {'title': 'Accelerating Text-to-Audio: The Fastest Model Yet!', 'desc': "This paper introduces Adversarial Relativistic-Contrastive (ARC) post-training, a novel method designed to speed up text-to-audio systems without relying on distillation. ARC post-training enhances diffusion and flow models by applying a relativistic adversarial approach combined with a contrastive discriminator objective, which improves the model's ability to follow prompts accurately. The authors demonstrate that their method significantly reduces inference time, achieving audio generation in approximately 75 milliseconds on high-performance hardware and 7 seconds on mobile devices. This advancement positions ARC post-training as the fastest text-to-audio model currently available, making it more practical for creative applications."}, 'zh': {'title': 'Âä†ÈÄüÊñáÊú¨Âà∞Èü≥È¢ëÁîüÊàêÁöÑÈù©ÂëΩÊÄßÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÁß∞‰∏∫ÂØπÊäóÁõ∏ÂØπÂØπÊØîÔºàARCÔºâÔºåÊó®Âú®Âä†ÈÄüÊñáÊú¨Âà∞Èü≥È¢ëÁ≥ªÁªüÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇARCÂêéËÆ≠ÁªÉÁÆóÊ≥ï‰∏ç‰æùËµñ‰∫éËí∏È¶èÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÊâ©Êï£/ÊµÅÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÁªìÂêàÁõ∏ÂØπÂØπÊäóÂΩ¢ÂºèÂíåÂØπÊØîÂà§Âà´Âô®ÁõÆÊ†áÔºåARCÈºìÂä±Ê®°ÂûãÊõ¥Â•ΩÂú∞ÈÅµÂæ™ÊèêÁ§∫„ÄÇÁªèËøá‰ºòÂåñÂêéÔºåËØ•Ê®°ÂûãÂú®H100‰∏äËÉΩÂú®Á∫¶75ÊØ´ÁßíÂÜÖÁîüÊàê12ÁßíÁöÑ44.1kHzÁ´ã‰ΩìÂ£∞Èü≥È¢ëÔºåÊàê‰∏∫ÁõÆÂâçÂ∑≤Áü•ÁöÑÊúÄÂø´ÊñáÊú¨Âà∞Èü≥È¢ëÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08727', 'title': 'Memorization-Compression Cycles Improve Generalization', 'url': 'https://huggingface.co/papers/2505.08727', 'abstract': 'We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.', 'score': 2, 'issue_id': 3760, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '07a8c687bc82dfeb', 'authors': ['Fangyuan Yu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08727.jpg', 'data': {'categories': ['#optimization', '#data', '#training', '#math'], 'emoji': 'üß†', 'ru': {'title': '–°–∂–∞—Ç–∏–µ –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Information Bottleneck Language Modeling (IBLM). –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –∏ –ø—É—Ç–µ–º —Å–∂–∞—Ç–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –í —Ä–∞–±–æ—Ç–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–∞–∑ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ —Å–∂–∞—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º Gated Phase Transition (GAPT), –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ —Å–∂–∞—Ç–∏—è, —É–ª—É—á—à–∞—è –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Å–Ω–∏–∂–∞—è —ç—Ñ—Ñ–µ–∫—Ç –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Compressing Knowledge for Better Generalization', 'desc': 'This paper demonstrates that improving generalization in machine learning models can be achieved not only by increasing the amount of data but also by compressing the internal representations of the model. The authors introduce the Information Bottleneck Language Modeling (IBLM) objective, which focuses on minimizing the complexity of representations while maintaining effective prediction capabilities. They observe a cycle of memorization and compression during the pretraining of large language models, which aligns with the principles of IBLM and reflects biological processes of learning and memory consolidation. To leverage this insight, they propose the Gated Phase Transition (GAPT) algorithm, which alternates between phases of memorization and compression, leading to significant improvements in model performance and generalization.'}, 'zh': {'title': 'ÂéãÁº©‰∏éËÆ∞ÂøÜÁöÑÂπ≥Ë°°ÊèêÂçáÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ', 'desc': 'Êú¨ÊñáÁêÜËÆ∫‰∏äËØÅÊòéÔºåÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ‰∏ç‰ªÖÈÄöËøáÂ¢ûÂä†Êï∞ÊçÆÈáèÊù•ÊèêÈ´òÔºåËøòÂèØ‰ª•ÈÄöËøáÂéãÁº©ÂÜÖÈÉ®Ë°®Á§∫Êù•ÂÆûÁé∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰ø°ÊÅØÁì∂È¢àËØ≠Ë®ÄÂª∫Ê®°ÔºàIBLMÔºâÁõÆÊ†áÔºåÂ∞ÜËØ≠Ë®ÄÂª∫Ê®°ÈáçÊñ∞ÊûÑÂª∫‰∏∫‰∏Ä‰∏™Á∫¶Êùü‰ºòÂåñÈóÆÈ¢òÔºöÂú®ÊúÄ‰ºòÈ¢ÑÊµãÊÄßËÉΩÁöÑÊù°‰ª∂‰∏ãÔºåÊúÄÂ∞èÂåñË°®Á§∫ÁÜµ„ÄÇÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåÂú®Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂá∫Áé∞‰∫ÜËÆ∞ÂøÜ‰∏éÂéãÁº©ÁöÑÂæ™ÁéØÁé∞Ë±°ÔºåËøôÁßçÁé∞Ë±°‰∏éIBLMÊâÄÊèèËø∞ÁöÑÈ¢ÑÊµã-ÂéãÁº©ÊùÉË°°ÂØÜÂàáÁõ∏ÂÖ≥„ÄÇÂü∫‰∫éËøô‰∏ÄËßÇÂØüÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈó®ÊéßÁõ∏ÂèòÔºàGAPTÔºâËÆ≠ÁªÉÁÆóÊ≥ïÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Âú®ËÆ∞ÂøÜÂíåÂéãÁº©Èò∂ÊÆµ‰πãÈó¥ÂàáÊç¢Ôºå‰ªéËÄåÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08665', 'title': 'SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation', 'url': 'https://huggingface.co/papers/2505.08665', 'abstract': 'Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.', 'score': 2, 'issue_id': 3750, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '8cbf16dc2ec90273', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Free University of Bozen-Bolzano Bozen-Bolzano, IT'], 'pdf_title_img': 'assets/pdf/title_img/2505.08665.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training'], 'emoji': 'üé•', 'ru': {'title': 'SkillFormer: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞ –ø–æ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ–º—É –≤–∏–¥–µ–æ', 'desc': 'SkillFormer - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Ä–æ–≤–Ω—è –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞ –≤ —Å–ª–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å CrossViewFusion –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∞–º–æ–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–µ—Ö–Ω–∏–∫–µ Low-Rank Adaptation, SkillFormer –¥–æ–æ–±—É—á–∞–µ—Ç –ª–∏—à—å –Ω–µ–±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ EgoExo4D –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ 4,5 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–Ω–∞–ª–æ–≥–∞–º–∏.'}, 'en': {'title': 'SkillFormer: Efficient Multi-View Skill Assessment', 'desc': 'This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation.'}, 'zh': {'title': 'SkillFormerÔºöÈ´òÊïàÁöÑÂ§öËßÜËßíÊäÄËÉΩËØÑ‰º∞Êû∂ÊûÑ', 'desc': 'ËØÑ‰º∞‰∫∫Á±ªÂú®Â§çÊùÇÊ¥ªÂä®‰∏≠ÁöÑÊäÄËÉΩÊ∞¥Âπ≥ÊòØ‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°ÔºåÂπøÊ≥õÂ∫îÁî®‰∫é‰ΩìËÇ≤„ÄÅÂ∫∑Â§çÂíåÂüπËÆ≠È¢ÜÂüü„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜSkillFormerÔºåËøôÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÊû∂ÊûÑÔºåËÉΩÂ§ü‰ªéËá™Êàë‰∏≠ÂøÉÂíåÂ§ñÈÉ®ËßÜËßíÁöÑËßÜÈ¢ë‰∏≠Áªü‰∏Ä‰º∞ËÆ°ÊäÄËÉΩÊ∞¥Âπ≥„ÄÇSkillFormerÂü∫‰∫éTimeSformerÈ™®Âπ≤ÁΩëÔºåÂºïÂÖ•‰∫ÜCrossViewFusionÊ®°ÂùóÔºåÈÄöËøáÂ§öÂ§¥‰∫§ÂèâÊ≥®ÊÑèÂäõ„ÄÅÂèØÂ≠¶‰π†ÁöÑÈó®ÊéßÂíåËá™ÈÄÇÂ∫îËá™Ê†°ÂáÜÊù•ËûçÂêàËßÜËßíÁâπÂæÅ„ÄÇÈÄöËøá‰ΩéÁß©ÈÄÇÂ∫îÊäÄÊúØÔºåÊàë‰ª¨‰ªÖÂæÆË∞ÉÂ∞ëÈáèÂèÇÊï∞ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÂπ∂Âú®EgoExo4DÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÂ§öËßÜËßíËÆæÁΩÆ‰∏ãÁöÑÊúÄÂÖàËøõÂáÜÁ°ÆÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08311', 'title': 'AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale', 'url': 'https://huggingface.co/papers/2505.08311', 'abstract': 'We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.', 'score': 2, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '492d10424bc0d97d', 'authors': ['Yunjie Ji', 'Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.08311.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#open_source', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç–∞—è 32B-–º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—É—é –ø–ª–∞–Ω–∫—É –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö', 'desc': 'AM-Thinking-v1 - —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 32 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç DeepSeek-R1 –∏ –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å –≤–µ–¥—É—â–∏–º–∏ MoE-–º–æ–¥–µ–ª—è–º–∏, –ø–æ–∫–∞–∑—ã–≤–∞—è –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∫–æ–¥–∏–Ω–≥–æ–≤—ã—Ö —Ç–µ—Å—Ç–∞—Ö. AM-Thinking-v1 –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–∏ Qwen2.5-32B —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–µ–≥–æ supervised fine-tuning –∏ reinforcement learning. –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å 32 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —É–¥–æ–±—Å—Ç–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Innovation', 'desc': 'AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.'}, 'zh': {'title': 'ÂºÄÊ∫êÂàõÊñ∞ÔºåÊé®ÁêÜÊñ∞È´òÂ∫¶', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜAM-Thinking-v1ÔºåËøôÊòØ‰∏Ä‰∏™32BÁöÑÂØÜÈõÜËØ≠Ë®ÄÊ®°ÂûãÔºåÊé®Âä®‰∫ÜÊé®ÁêÜÁöÑÂâçÊ≤øÔºå‰ΩìÁé∞‰∫ÜÂºÄÊ∫êÂàõÊñ∞ÁöÑÂêà‰ΩúÁ≤æÁ•û„ÄÇÂÆÉÂú®AIME 2024„ÄÅAIME 2025ÂíåLiveCodeBenchÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫ÜDeepSeek-R1ÔºåÂπ∂‰∏éÈ¢ÜÂÖàÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÂ¶ÇQwen3-235B-A22BÂíåSeed1.5-ThinkingÁõ∏Â™≤Áæé„ÄÇAM-Thinking-v1ÂÆåÂÖ®Âü∫‰∫éÂºÄÊ∫êÁöÑQwen2.5-32BÂü∫Á°ÄÊ®°ÂûãÔºåÁªìÂêàÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÂ±ïÁé∞‰∫ÜÂçìË∂äÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰ΩúËØÅÊòé‰∫ÜÂºÄÊ∫êÁ§æÂå∫ÂèØ‰ª•Âú®32BËßÑÊ®°‰∏äÂÆûÁé∞È´òÊÄßËÉΩÔºåÂπ≥Ë°°È°∂Á∫ßÊÄßËÉΩ‰∏éÂÆûÈôÖÂèØÁî®ÊÄßÔºåÊøÄÂä±Êõ¥Â§öÂêà‰ΩúÂä™Âäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2504.21475', 'title': 'Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines', 'url': 'https://huggingface.co/papers/2504.21475', 'abstract': 'This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.', 'score': 2, 'issue_id': 3756, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': '86e4cc7ad4a84ece', 'authors': ['Serry Sibaee', 'Samar Ahmed', 'Abdullah Al Harbi', 'Omer Nacar', 'Adel Ammar', 'Yasser Habashi', 'Wadii Boulila'], 'affiliations': ['College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia', 'Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia', 'Independent Researcher, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21475.jpg', 'data': {'categories': ['#training', '#data', '#machine_translation', '#dataset', '#architecture', '#multilingual', '#low_resource'], 'emoji': 'üìö', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—Ä–∞–±—Å–∫–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–∞ —Å–ª—É–∂–±–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –∞—Ä–∞–±—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –ø–æ–∑–≤–æ–ª—è—é—â–µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–ª–æ–≤–∞ –ø–æ –∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è–º –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ø–æ–ª—É—ç–Ω–∫–æ–¥–µ—Ä–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏–π –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –∞—Ä–∞–±—Å–∫–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∞—Ä–∞–±—Å–∫–∏—Ö –ª–µ–∫—Å–∏–∫–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∞—Ä–∞–±–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –ø—Ä–∏—á–µ–º ARBERTv2 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–µ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è.'}, 'en': {'title': 'Bridging the Gap in Arabic Language Processing with a Reverse Dictionary', 'desc': 'This paper presents a new system for Arabic natural language processing called the Arabic Reverse Dictionary (RD), which helps users find words based on their meanings. The authors introduce a transformer-based model with a unique semi-encoder architecture that improves performance on Arabic RD tasks. They also create a detailed dataset and establish quality standards for Arabic definitions, showing that specialized Arabic models outperform general multilingual ones. Additionally, the study offers a theoretical framework for reverse dictionaries and introduces a Python library for easy implementation and training.'}, 'zh': {'title': 'ÈòøÊãâ‰ºØËØ≠ÂèçÂêëËØçÂÖ∏ÔºöÊèêÂçáËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÂà©Âô®', 'desc': 'Êú¨Á†îÁ©∂Ëß£ÂÜ≥‰∫ÜÈòøÊãâ‰ºØËØ≠Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÂÖ≥ÈîÆÈóÆÈ¢òÔºåÂºÄÂèë‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑÈòøÊãâ‰ºØËØ≠ÂèçÂêëËØçÂÖ∏Á≥ªÁªüÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆÊèèËø∞ÊàñÂê´‰πâÊâæÂà∞ÂçïËØç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÂèòÊç¢Âô®ÁöÑÂçäÁºñÁ†ÅÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÔºåÂÖ∑ÊúâÂá†‰ΩïÈÄíÂáèÂ±ÇÔºåËææÂà∞‰∫ÜÈòøÊãâ‰ºØËØ≠ÂèçÂêëËØçÂÖ∏‰ªªÂä°ÁöÑÊúÄÂÖàËøõÁªìÊûú„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊñπÊ≥ïÂåÖÊã¨ÂÖ®Èù¢ÁöÑÊï∞ÊçÆÈõÜÊûÑÂª∫ËøáÁ®ãÔºåÂπ∂Âª∫Á´ã‰∫ÜÈòøÊãâ‰ºØËØ≠ËØçÂÖ∏ÂÆö‰πâÁöÑÊ≠£ÂºèË¥®ÈáèÊ†áÂáÜ„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰∏ìÈó®ÈíàÂØπÈòøÊãâ‰ºØËØ≠ÁöÑÊ®°ÂûãÊòæËëó‰ºò‰∫éÈÄöÁî®Â§öËØ≠Ë®ÄÂµåÂÖ•ÔºåARBERTv2Ê®°ÂûãËé∑Âæó‰∫ÜÊúÄ‰Ω≥ÊéíÂêçÂàÜÊï∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08751', 'title': 'Aya Vision: Advancing the Frontier of Multilingual Multimodality', 'url': 'https://huggingface.co/papers/2505.08751', 'abstract': 'Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.', 'score': 1, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': 'c063c125e504fa88', 'authors': ['Saurabh Dash', 'Yiyang Nan', 'John Dang', 'Arash Ahmadian', 'Shivalika Singh', 'Madeline Smith', 'Bharat Venkitesh', 'Vlad Shmyhlo', 'Viraat Aryabumi', 'Walter Beller-Morales', 'Jeremy Pekmez', 'Jason Ozuzu', 'Pierre Richemond', 'Acyr Locatelli', 'Nick Frosst', 'Phil Blunsom', 'Aidan Gomez', 'Ivan Zhang', 'Marzieh Fadaee', 'Manoj Govindassamy', 'Sudip Roy', 'Matthias Gall√©', 'Beyza Ermis', 'Ahmet √úst√ºn', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.08751.jpg', 'data': {'categories': ['#synthetic', '#data', '#low_resource', '#multimodal', '#training', '#multilingual'], 'emoji': 'üåê', 'ru': {'title': '–ü—Ä–æ—Ä—ã–≤ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Ç–µ—Ö–Ω–∏–∫—É –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª–∏ Aya-Vision, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Advancing Multimodal Language Models for Multilingual Mastery', 'desc': 'This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ™ÅÁ†¥ÊÄßËøõÂ±ï', 'desc': 'ÊûÑÂª∫Â§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÈù¢‰∏¥ËÆ∏Â§öÊåëÊàòÔºåÂåÖÊã¨ËßÜËßâÂíåËØ≠Ë®ÄÊ®°ÊÄÅÁöÑÂØπÈΩê„ÄÅÈ´òË¥®ÈáèÊåá‰ª§Êï∞ÊçÆÁöÑÊï¥ÁêÜÔºå‰ª•ÂèäÂú®ÂºïÂÖ•ËßÜËßâÂêéÈÅøÂÖçÊñáÊú¨ËÉΩÂäõÁöÑÈÄÄÂåñ„ÄÇÂú®Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÔºåËøô‰∫õÂõ∞ÈöæÊõ¥Âä†Á™ÅÂá∫ÔºåÂõ†‰∏∫‰∏çÂêåËØ≠Ë®ÄÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈúÄÊ±ÇÂä†Ââß‰∫ÜÊï∞ÊçÆÁ®ÄÁº∫ÔºåÊú∫Âô®ÁøªËØëÂ∏∏Â∏∏Êâ≠Êõ≤ÊÑè‰πâÔºåÂπ∂‰∏îÁÅæÈöæÊÄßÈÅóÂøòÁé∞Ë±°Êõ¥Âä†ÊòéÊòæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊñ∞ÁöÑÊäÄÊúØÔºåÂåÖÊã¨‰∏Ä‰∏™ÂêàÊàêÊ≥®ÈáäÊ°ÜÊû∂ÔºåÁî®‰∫éÊï¥ÁêÜÈ´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÁöÑÂ§öËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÊåá‰ª§Êï∞ÊçÆÔºå‰ΩøAya VisionÊ®°ÂûãËÉΩÂ§üÂú®Â§öÁßçËØ≠Ë®Ä‰∏≠ÂØπÂ§öÊ®°ÊÄÅËæìÂÖ•ÁîüÊàêËá™ÁÑ∂„ÄÅÁ¨¶Âêà‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂìçÂ∫î„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçË∑®Ê®°ÊÄÅÊ®°ÂûãÂêàÂπ∂ÊäÄÊúØÔºåÊúâÊïàÂáèËΩªÁÅæÈöæÊÄßÈÅóÂøòÔºåÂêåÊó∂Â¢ûÂº∫Â§öÊ®°ÊÄÅÁîüÊàêÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08712', 'title': 'NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance', 'url': 'https://huggingface.co/papers/2505.08712', 'abstract': "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.", 'score': 1, 'issue_id': 3752, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': 'a68cc40de86ece61', 'authors': ['Wenzhe Cai', 'Jiaqi Peng', 'Yuqiang Yang', 'Yujian Zhang', 'Meng Wei', 'Hanqing Wang', 'Yilun Chen', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.08712.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#diffusion', '#robotics', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': 'NavDP: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è —Ä–æ–±–æ—Ç–æ–≤ –∏–∑ —Å–∏–º—É–ª—è—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NavDP - —Å–∏—Å—Ç–µ–º—É –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π –æ—Ç–∫—Ä—ã—Ç–æ–π —Å—Ä–µ–¥–µ, –æ–±—É—á–µ–Ω–Ω—É—é –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Å–∏–º—É–ª—è—Ü–∏—è—Ö. NavDP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ä–æ–±–æ—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ. –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤ —Å–∏–º—É–ª—è—Ü–∏–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–±–æ—Ä–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–æ–±–æ—Ç–æ–≤ –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö.'}, 'en': {'title': 'Efficient Robot Navigation with Simulation-Driven Learning', 'desc': 'This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.'}, 'zh': {'title': 'Êú∫Âô®‰∫∫ÂØºËà™ÁöÑÊñ∞Á™ÅÁ†¥ÔºöÂØºËà™Êâ©Êï£Á≠ñÁï•', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂØºËà™Êâ©Êï£Á≠ñÁï•ÔºàNavDPÔºâÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®Â∏ÆÂä©Êú∫Âô®‰∫∫Âú®Âä®ÊÄÅÂºÄÊîæ‰∏ñÁïåÁéØÂ¢É‰∏≠ËøõË°åÂØºËà™„ÄÇËØ•ÊñπÊ≥ïÂÆåÂÖ®Âú®Ê®°ÊãüÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÔºåËÉΩÂ§üÈõ∂-shotËøÅÁßªÂà∞‰∏çÂêåÁöÑÂÆûÈôÖÂ∫îÁî®‰∏≠„ÄÇNavDPÁªìÂêà‰∫ÜÂü∫‰∫éÊâ©Êï£ÁöÑËΩ®ËøπÁîüÊàêÂíåÁî®‰∫éËΩ®ËøπÈÄâÊã©ÁöÑËØÑ‰ª∑ÂáΩÊï∞ÔºåÂà©Áî®ÂÖ±‰∫´Á≠ñÁï•ÂèòÊç¢Âô®ÁºñÁ†ÅÁöÑÂ±ÄÈÉ®ËßÇÂØü‰ø°ÊÅØËøõË°åÊù°‰ª∂Âåñ„ÄÇÈÄöËøáÂú®Ê®°Êãü‰∏≠Ëé∑ÂèñÂÖ®ÁêÉÁéØÂ¢ÉÁöÑÁâπÊùÉ‰ø°ÊÅØÔºåÊàë‰ª¨ÁîüÊàê‰∫ÜÈ´òË¥®ÈáèÁöÑÊºîÁ§∫Êï∞ÊçÆÔºåÂπ∂Âú®Â§öÁßçÂÆ§ÂÜÖÂ§ñÁéØÂ¢É‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂØºËà™ÊÄßËÉΩÂíåÂá∫Ëâ≤ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08445', 'title': 'Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency', 'url': 'https://huggingface.co/papers/2505.08445', 'abstract': 'Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.', 'score': 1, 'issue_id': 3756, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '7b4d8b3daa6efb6d', 'authors': ['Adel Ammar', 'Anis Koubaa', 'Omer Nacar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, P.O. Box 50927, Riyadh 11533, Saudi Arabia', 'Prince Sultan University, Rafha Street, P.O. Box 66833, Riyadh 11586, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.08445.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#healthcare', '#hallucinations', '#rag'], 'emoji': 'üîç', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è RAG: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é', 'desc': '–°—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º (RAG). –ò—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ Chroma –∏ Faiss, —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–æ–º –∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —à–µ—Å—Ç—å –º–µ—Ç—Ä–∏–∫, –≤–∫–ª—é—á–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ RAG-—Å–∏—Å—Ç–µ–º—ã –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏—á—å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ–¥–±–æ—Ä–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Optimizing RAG: Balancing Speed and Accuracy for Better AI Responses', 'desc': 'This paper explores how hyperparameters affect the performance of Retrieval-Augmented Generation (RAG) systems, which combine language generation with external information retrieval. It evaluates different vector stores, chunking strategies, and re-ranking methods to find the best balance between speed and accuracy. The findings indicate that while Chroma is faster, Faiss offers better retrieval precision, highlighting a trade-off between these two factors. The study also shows that with optimal hyperparameter tuning, RAG systems can achieve very high retrieval accuracy, which is crucial for applications like healthcare decision support.'}, 'zh': {'title': '‰ºòÂåñRAGÁ≥ªÁªüÔºåÂÆûÁé∞È´òÊïàÂáÜÁ°ÆÁöÑÊ£ÄÁ¥¢', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰ªªÂä°Ë°®Áé∞‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂ∏∏Â∏∏‰ºöÂá∫Áé∞ÂπªËßâÊàñ‰æùËµñËøáÊó∂Áü•ËØÜ„ÄÇÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÈÄöËøáÂ∞ÜÁîüÊàê‰∏éÂ§ñÈÉ®ÊêúÁ¥¢ÁªìÂêàÔºåËß£ÂÜ≥‰∫ÜËøô‰∫õÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜË∂ÖÂèÇÊï∞Â¶Ç‰ΩïÂΩ±ÂìçRAGÁ≥ªÁªüÁöÑÈÄüÂ∫¶ÂíåË¥®ÈáèÔºåÂåÖÊã¨ÂêëÈáèÂ≠òÂÇ®„ÄÅÂàÜÂùóÁ≠ñÁï•„ÄÅ‰∫§ÂèâÁºñÁ†ÅÈáçÊéíÂ∫èÂíåÊ∏©Â∫¶Á≠âÔºåÂπ∂ËØÑ‰º∞‰∫ÜÂÖ≠‰∏™ÊåáÊ†á„ÄÇÁ†îÁ©∂ÁªìÊûúÂ∏ÆÂä©‰ªé‰∏öËÄÖÂú®Ë∞ÉÊï¥RAGÁ≥ªÁªüÊó∂Âπ≥Ë°°ËÆ°ÁÆóÊàêÊú¨ÂíåÂáÜÁ°ÆÊÄßÔºå‰ª•ÂÆûÁé∞ÈÄèÊòé‰∏îÊúÄÊñ∞ÁöÑÂìçÂ∫î„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07416', 'title': 'ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation', 'url': 'https://huggingface.co/papers/2505.07416', 'abstract': 'Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP', 'score': 1, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '4f126d39b5476454', 'authors': ['Truc Mai-Thanh Nguyen', 'Dat Minh Nguyen', 'Son T. Luu', 'Kiet Van Nguyen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07416.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#multilingual', '#low_resource', '#open_source'], 'emoji': 'üáªüá≥', 'ru': {'title': '–ò–ò —É—Å–∫–æ—Ä—è–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –≤—å–µ—Ç–Ω–∞–º—Å–∫–æ–º —è–∑—ã–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ViMRHP –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –≤—å–µ—Ç–Ω–∞–º—Å–∫–æ–º —è–∑—ã–∫–µ. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è –ø–æ–º–æ—â–∏ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞–º, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏–ª–æ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ —Å–Ω–∏–∑–∏–ª–æ –∑–∞—Ç—Ä–∞—Ç—ã. –î–∞—Ç–∞—Å–µ—Ç –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 4 –¥–æ–º–µ–Ω–∞, –≤–∫–ª—é—á–∞—è 2000 –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ 46000 –æ—Ç–∑—ã–≤–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò.'}, 'en': {'title': 'Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction', 'desc': 'This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks.'}, 'zh': {'title': 'ÊèêÂçáË∂äÂçóËØ≠ËØÑËÆ∫ÊúâÁî®ÊÄßÁöÑÊô∫ËÉΩËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Â§öÊ®°ÊÄÅËØÑËÆ∫ÊúâÁî®ÊÄßÈ¢ÑÊµãÔºàMRHPÔºâÊòØÊé®ËçêÁ≥ªÁªü‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶Å‰ªªÂä°ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁîµÂ≠êÂïÜÂä°Âπ≥Âè∞‰∏ä„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜViMRHPÔºàË∂äÂçóËØ≠Â§öÊ®°ÊÄÅËØÑËÆ∫ÊúâÁî®ÊÄßÈ¢ÑÊµãÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∂äÂçóËØ≠ÁöÑMRHP‰ªªÂä°ÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÂõõ‰∏™È¢ÜÂüüÔºåÂåÖÊã¨2000‰∏™‰∫ßÂìÅÂíå46000Êù°ËØÑËÆ∫„ÄÇ‰∏∫‰∫Ü‰ºòÂåñÊ≥®ÈáäËøáÁ®ãÔºåÊàë‰ª¨Âà©Áî®‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©Ê≥®ÈáäËÄÖÊûÑÂª∫ViMRHPÊï∞ÊçÆÈõÜÔºå‰ªéËÄåÊòæËëóÂáèÂ∞ë‰∫ÜÊ≥®ÈáäÊó∂Èó¥ÂíåÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊï∞ÊçÆË¥®Èáè„ÄÇÂ∞ΩÁÆ°AIÁîüÊàêÁöÑÊ≥®ÈáäÂú®Â§çÊùÇ‰ªªÂä°‰∏≠‰ªçÂ≠òÂú®Â±ÄÈôêÊÄßÔºå‰ΩÜÊàë‰ª¨ÈÄöËøáËØ¶ÁªÜÁöÑÊÄßËÉΩÂàÜÊûêËøõ‰∏ÄÊ≠•Êé¢ËÆ®‰∫ÜËøô‰∫õÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02567', 'title': 'Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities', 'url': 'https://huggingface.co/papers/2505.02567', 'abstract': "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).", 'score': 51, 'issue_id': 3655, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '0d49b4c41b7654a0', 'authors': ['Xinjie Zhang', 'Jintao Guo', 'Shanshan Zhao', 'Minghao Fu', 'Lunhao Duan', 'Guo-Hua Wang', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba Group', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02567.jpg', 'data': {'categories': ['#dataset', '#architecture', '#survey', '#multimodal', '#diffusion', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –ò–ò', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ä–∞–¥–∏–≥–º—ã: –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã. –í —Ä–∞–±–æ—Ç–µ —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–±—Å—É–∂–¥–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏, –≤–∫–ª—é—á–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –¥–∞–Ω–Ω—ã–µ.'}, 'en': {'title': 'Bridging the Gap: Unifying Multimodal Understanding and Image Generation', 'desc': 'This paper surveys the integration of multimodal understanding and image generation models, which have traditionally developed separately. It highlights the architectural differences between autoregressive and diffusion-based models, emphasizing the challenges in unifying these approaches. The authors categorize existing unified models into three paradigms: diffusion-based, autoregressive-based, and hybrid methods. They also provide resources such as datasets and benchmarks to support future research in this emerging field.'}, 'zh': {'title': 'Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊú™Êù•Êé¢Á¥¢', 'desc': 'ËøëÂπ¥Êù•ÔºåÂ§öÊ®°ÊÄÅÁêÜËß£Ê®°ÂûãÂíåÂõæÂÉèÁîüÊàêÊ®°ÂûãÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜËøô‰∏§‰∏™È¢ÜÂüüÁöÑÂèëÂ±ïÁõ∏ÂØπÁã¨Á´ãÔºåÂØºËá¥‰∫Ü‰∏çÂêåÁöÑÊû∂ÊûÑËåÉÂºè„ÄÇËá™ÂõûÂΩíÊû∂ÊûÑÂú®Â§öÊ®°ÊÄÅÁêÜËß£‰∏≠Âç†‰∏ªÂØºÂú∞‰ΩçÔºåËÄåÊâ©Êï£Ê®°ÂûãÂàôÊàê‰∏∫ÂõæÂÉèÁîüÊàêÁöÑÂü∫Áü≥„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜÂΩìÂâçÁªü‰∏ÄÊ°ÜÊû∂ÁöÑÂä™ÂäõÔºå‰ªãÁªç‰∫ÜÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÂü∫Á°ÄÊ¶ÇÂøµÂèäÊúÄÊñ∞ËøõÂ±ïÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∏âÁßç‰∏ªË¶ÅÁöÑÁªü‰∏ÄÊ®°ÂûãÊû∂ÊûÑ„ÄÇÊàë‰ª¨ËøòËÆ®ËÆ∫‰∫ÜËøô‰∏ÄÊñ∞ÂÖ¥È¢ÜÂüüÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÊåëÊàòÔºåÂπ∂Êèê‰æõ‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂèÇËÄÉËµÑÊ∫ê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04588', 'title': 'ZeroSearch: Incentivize the Search Capability of LLMs without Searching', 'url': 'https://huggingface.co/papers/2505.04588', 'abstract': "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", 'score': 29, 'issue_id': 3647, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '24edc7c3c5e5e23d', 'authors': ['Hao Sun', 'Zile Qiao', 'Jiayan Guo', 'Xuanbo Fan', 'Yingyan Hou', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Yan Zhang'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.04588.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#reasoning', '#rl'], 'emoji': 'üîç', 'ru': {'title': 'ZeroSearch: –æ–±—É—á–µ–Ω–∏–µ LLM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –ø–æ–∏—Å–∫—É –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ZeroSearch - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, ZeroSearch –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, —á—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –≤—ã—Å–æ–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ API. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥—É–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ZeroSearch —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM, –ø—Ä–∏—á–µ–º –º–æ–¥–µ–ª–∏ —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'ZeroSearch: Enhancing LLM Search Without Real Engines', 'desc': 'This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines.'}, 'zh': {'title': 'ÊèêÂçáLLMsÊêúÁ¥¢ËÉΩÂäõÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'ÊúâÊïàÁöÑ‰ø°ÊÅØÊêúÁ¥¢ÂØπ‰∫éÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜÂíåÁîüÊàêËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ZeroSearchÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òLLMsÁöÑÊêúÁ¥¢ËÉΩÂäõÔºåËÄåÊó†ÈúÄ‰∏éÁúüÂÆûÊêúÁ¥¢ÂºïÊìé‰∫íÂä®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËΩªÈáèÁ∫ßÁöÑÁõëÁù£ÂæÆË∞ÉÔºåÂ∞ÜLLMËΩ¨Âèò‰∏∫‰∏Ä‰∏™Ê£ÄÁ¥¢Ê®°ÂùóÔºåÂπ∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈÄêÊ≠•Èôç‰ΩéÁîüÊàêÊñáÊ°£ÁöÑË¥®ÈáèÔºå‰ª•ÊøÄÂèëÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåZeroSearchËÉΩÂ§üÊúâÊïàÊèêÂçáLLMsÁöÑÊêúÁ¥¢ËÉΩÂäõÔºåÂπ∂Âú®‰∏çÂêåÂèÇÊï∞ËßÑÊ®°ÁöÑÊ®°Âûã‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03821', 'title': 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models', 'url': 'https://huggingface.co/papers/2505.03821', 'abstract': "We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.", 'score': 18, 'issue_id': 3655, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 –º–∞—è', 'en': 'May 3', 'zh': '5Êúà3Êó•'}, 'hash': 'abede452b390c7de', 'authors': ['Gracjan G√≥ral', 'Alicja Ziarko', 'Piotr Mi≈Ço≈õ', 'Micha≈Ç Nauman', 'Maciej Wo≈Çczyk', 'Micha≈Ç Kosi≈Ñski'], 'affiliations': ['Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, S. Banacha 2, 02-097 Warsaw, PL', 'Graduate School of Business, Stanford University, Stanford, CA 94305, USA', 'IDEAS NCBR, Chmielna 69, 00-801 Warsaw, PL', 'Institute of Mathematics, Polish Academy of Sciences, J. & J. Sniadeckich 8, 00-656 Warsaw, PL', 'Robot Learning Lab, University of California, Berkeley, CA 94720, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.03821.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#training'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': 'VLM –º–æ–¥–µ–ª–∏: –æ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–±–æ—Ä –∏–∑ 144 –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ü–µ–Ω—ã —Å –º–∏–Ω–∏–∞—Ç—é—Ä–Ω–æ–π —Ñ–∏–≥—É—Ä–∫–æ–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ –æ–±—ä–µ–∫—Ç–æ–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4-Turbo –∏ Claude Sonnet, —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —Å—Ü–µ–Ω, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç –∑–∞–¥–∞—á–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —è–≤–Ω—ã—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è –≤ –±—É–¥—É—â–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞—Ö VLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Spatial Reasoning and Perspective Taking', 'desc': "This paper explores how well Vision Language Models (VLMs) can understand visual perspectives through a series of unique tasks. The tasks involve a humanoid figure and an object in various spatial arrangements, designed to test scene understanding, spatial reasoning, and visual perspective taking. The study evaluates several advanced models, finding that while they perform well in recognizing scenes, they struggle with more complex reasoning tasks. The results highlight a significant gap in the models' abilities, suggesting that future developments should focus on incorporating geometric representations and specialized training methods."}, 'zh': {'title': 'ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ËßÜËßâËßÜËßíÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰ΩøÁî®‰∫Ü‰∏ÄÁªÑÊñ∞È¢ñÁöÑËßÜËßâ‰ªªÂä°ÔºåËøô‰∫õ‰ªªÂä°ÁÅµÊÑüÊù•Ê∫ê‰∫é‰∫∫Á±ªÁöÑÁªèÂÖ∏ÊµãËØï„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü144‰∏™Áã¨ÁâπÁöÑËßÜËßâ‰ªªÂä°ÔºåÈÄöËøáÁ≥ªÁªüÂú∞ÊîπÂèòÁ©∫Èó¥ÈÖçÁΩÆÔºåÂ¶ÇÁâ©‰ΩìÁõ∏ÂØπ‰∫é‰∫∫ÂΩ¢Â∞è‰∫∫ÂÅ∂ÁöÑ‰ΩçÁΩÆÂíåÊñπÂêëÔºåÊù•ËØÑ‰º∞Ê®°ÂûãÁöÑË°®Áé∞„ÄÇÊØè‰∏™ËßÜËßâ‰ªªÂä°ÈÖçÊúâ7‰∏™ËØäÊñ≠ÈóÆÈ¢òÔºåÊó®Âú®ËØÑ‰º∞Âú∫ÊôØÁêÜËß£„ÄÅÁ©∫Èó¥Êé®ÁêÜÂíåËßÜËßâËßÜËßíÁêÜËß£‰∏â‰∏™Â±ÇÊ¨°ÁöÑËßÜËßâËÆ§Áü•„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Ëøô‰∫õÂÖàËøõÊ®°ÂûãÂú®Âú∫ÊôØÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Á©∫Èó¥Êé®ÁêÜÂíåËßÜËßíÁêÜËß£ÊñπÈù¢ÁöÑË°®Áé∞ÊòæËëó‰∏ãÈôçÔºåË°®ÊòéÂú®Â§çÊùÇËßÜËßâ‰ªªÂä°‰∏≠ÔºåË°®Èù¢Áâ©‰ΩìËØÜÂà´‰∏éÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÁ©∫Èó¥ÂíåËßÜËßíÊé®ÁêÜ‰πãÈó¥Â≠òÂú®Â∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00358', 'title': 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training', 'url': 'https://huggingface.co/papers/2505.00358', 'abstract': "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.", 'score': 14, 'issue_id': 3652, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '74b251baea8510bd', 'authors': ['Albert Ge', 'Tzu-Heng Huang', 'John Cooper', 'Avi Trost', 'Ziyi Chu', 'Satya Sai Srinath Namburi GNVV', 'Ziyang Cai', 'Kendall Park', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.00358.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#data', '#multimodal'], 'emoji': 'üîÄ', 'ru': {'title': 'R&B: –£–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ R&B –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. R&B –ø–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–∞–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞—Ç—Ä–∏—Ü—É –ì—Ä–∞–º–∞, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–æ–º–µ–Ω–æ–≤. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å R&B –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–µ–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –∫ —Å–º–µ—à–∏–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'R&B: Smarter Data Mixing for Language Models', 'desc': 'This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods.'}, 'zh': {'title': 'R&BÔºöÈ´òÊïàÁöÑÊï∞ÊçÆÊ∑∑ÂêàÊñ∞Á≠ñÁï•', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÊ∑∑ÂêàÁ≠ñÁï•R&BÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïÁöÑ‰∏§‰∏™‰∏ªË¶ÅÁº∫Èô∑„ÄÇÈ¶ñÂÖàÔºåR&BÈÄöËøáËØ≠‰πâÁõ∏‰ººÊÄßÈáçÊñ∞ÂàíÂàÜËÆ≠ÁªÉÊï∞ÊçÆÔºåÂàõÂª∫Êõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÂüüÔºå‰ªéËÄåÊçïÊçâÂà∞ÈáçË¶ÅÁöÑËØ≠‰πâÁªÜËäÇ„ÄÇÂÖ∂Ê¨°ÔºåËØ•Ê°ÜÊû∂ÈÄöËøáÂà©Áî®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ëé∑ÂæóÁöÑÈ¢ÜÂüüÊ¢ØÂ∫¶ÁöÑGramÁü©ÈòµÔºå‰ºòÂåñÊï∞ÊçÆÁªÑÂêàÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåR&BÂú®Â§öÁßçÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§ü‰ª•ÊûÅÂ∞èÁöÑËÆ°ÁÆóÊàêÊú¨Ë∂ÖË∂äÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊï∞ÊçÆÊ∑∑ÂêàÁ≠ñÁï•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04622', 'title': 'PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer', 'url': 'https://huggingface.co/papers/2505.04622', 'abstract': 'Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '8205883cc18835a6', 'authors': ['Jingwen Ye', 'Yuze He', 'Yanning Zhou', 'Yiqin Zhu', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Wei Yang', 'Xiao Han'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04622.jpg', 'data': {'categories': ['#optimization', '#3d', '#cv', '#games'], 'emoji': 'üßä', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è 3D-—Ñ–æ—Ä–º —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PrimitiveAnything - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ 3D-—Ñ–æ—Ä–º —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–π, –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–±–æ—Ä–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤. PrimitiveAnything –ø—Ä–∏–º–µ–Ω—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö —Ñ–æ—Ä–º –∏ –∏–º–µ–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ –∏–≥—Ä–∞—Ö –∏ –¥—Ä—É–≥–∏—Ö 3D-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Revolutionizing 3D Shape Understanding with PrimitiveAnything', 'desc': 'This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games.'}, 'zh': {'title': 'ÂΩ¢Áä∂ÊäΩË±°ÁöÑÊñ∞Á™ÅÁ†¥ÔºöPrimitiveAnything', 'desc': 'ÂΩ¢Áä∂ÂéüÂßãÊäΩË±°ÊòØÂ∞ÜÂ§çÊùÇÁöÑ3DÂΩ¢Áä∂ÂàÜËß£‰∏∫ÁÆÄÂçïÂá†‰ΩïÂÖÉÁ¥†ÁöÑËøáÁ®ãÔºåËøôÂØπ‰∫∫Á±ªËßÜËßâËÆ§Áü•Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂπ∂Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÂíåÂõæÂΩ¢Â≠¶‰∏≠ÊúâÂπøÊ≥õÂ∫îÁî®„ÄÇÁé∞ÊúâÁöÑÂéüÂßãÊäΩË±°ÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂá†‰Ωï‰ºòÂåñÔºåÁº∫‰πèËØ≠‰πâÁêÜËß£ÔºåÊàñËÄÖ‰ªÖ‰ªéÂ∞èËßÑÊ®°„ÄÅÁâπÂÆöÁ±ªÂà´ÁöÑÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÔºåÈöæ‰ª•Âú®Â§öÊ†∑ÁöÑÂΩ¢Áä∂Á±ªÂà´‰∏≠ËøõË°åÊ≥õÂåñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPrimitiveAnythingÔºå‰∏Ä‰∏™Â∞ÜÂΩ¢Áä∂ÂéüÂßãÊäΩË±°ÈáçÊñ∞ÂÆö‰πâ‰∏∫ÂéüÂßãÁªÑË£ÖÁîüÊàê‰ªªÂä°ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂ§ßËßÑÊ®°‰∫∫Á±ªÂàõ‰ΩúÁöÑÊäΩË±°Â≠¶‰π†ÂéüÂßãÁªÑË£ÖËøáÁ®ãÔºå‰ªéËÄåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∫∫Á±ªÂ¶Ç‰ΩïÂ∞ÜÂ§çÊùÇÂΩ¢Áä∂ÂàÜËß£‰∏∫ÂéüÂßãÂÖÉÁ¥†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04512', 'title': 'HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation', 'url': 'https://huggingface.co/papers/2505.04512', 'abstract': 'Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '82e5839ef846d9d8', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Sen Liang', 'Yuan Zhou', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.04512.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal'], 'emoji': 'üé¨', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏', 'desc': 'HunyuanCustom - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è —É—Å–ª–æ–≤–∏—è –≤ –≤–∏–¥–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞—É–¥–∏–æ, –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–∞. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å —Å–ª–∏—è–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLaVA –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –º–æ–¥—É–ª—å —É—Å–∏–ª–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ—Å—Ç–∏ –≤ –∫–∞–¥—Ä–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∞—É–¥–∏–æ- –∏ –≤–∏–¥–µ–æ—É—Å–ª–æ–≤–∏–π, —Ç–∞–∫–∏–µ –∫–∞–∫ AudioNet –∏ —Å–µ—Ç—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—á–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ HunyuanCustom –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é —Ç–µ–∫—Å—Ç–∞ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'HunyuanCustom: Consistent and Multi-Modal Video Generation', 'desc': 'This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÂÆöÂà∂ËßÜÈ¢ëÁîüÊàêÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'ÂÆöÂà∂ËßÜÈ¢ëÁîüÊàêÊó®Âú®Ê†πÊçÆÁî®Êà∑ÂÆö‰πâÁöÑÊù°‰ª∂ÁîüÊàêÁâπÂÆö‰∏ªÈ¢òÁöÑËßÜÈ¢ëÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®Ë∫´‰ªΩ‰∏ÄËá¥ÊÄßÂíåËæìÂÖ•Ê®°ÊÄÅÊñπÈù¢Â∏∏Â∏∏Èù¢‰∏¥ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜHunyuanCustomÔºå‰∏Ä‰∏™Â§öÊ®°ÊÄÅÂÆöÂà∂ËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåÂº∫Ë∞É‰∏ªÈ¢ò‰∏ÄËá¥ÊÄßÔºåÂπ∂ÊîØÊåÅÂõæÂÉè„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ëÂíåÊñáÊú¨Êù°‰ª∂„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÈÄöËøáÂºïÂÖ•Âü∫‰∫éLLaVAÁöÑÊñáÊú¨-ÂõæÂÉèËûçÂêàÊ®°ÂùóÂíåÂõæÂÉèIDÂ¢ûÂº∫Ê®°ÂùóÔºåËß£ÂÜ≥‰∫ÜÂõæÂÉè-ÊñáÊú¨Êù°‰ª∂ÁîüÊàê‰ªªÂä°Ôºå‰ªéËÄåÂ¢ûÂº∫Â§öÊ®°ÊÄÅÁêÜËß£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHunyuanCustomÂú®Ë∫´‰ªΩ‰∏ÄËá¥ÊÄß„ÄÅÁúüÂÆûÊÑüÂíåÊñáÊú¨-ËßÜÈ¢ëÂØπÈΩêÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂ§öÊ®°ÊÄÅÊù°‰ª∂ÂíåË∫´‰ªΩ‰øùÊåÅÁ≠ñÁï•Âú®ÂèØÊéßËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04364', 'title': "Benchmarking LLMs' Swarm intelligence", 'url': 'https://huggingface.co/papers/2505.04364', 'abstract': 'Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.', 'score': 12, 'issue_id': 3648, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '4b0575d2194aee20', 'authors': ['Kai Ruan', 'Mowen Huang', 'Ji-Rong Wen', 'Hao Sun'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04364.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#open_source', '#multimodal'], 'emoji': 'üêù', 'ru': {'title': 'SwarmBench: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–æ–µ–≤–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SwarmBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–æ–µ–≤–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. SwarmBench –≤–∫–ª—é—á–∞–µ—Ç –ø—è—Ç—å –∑–∞–¥–∞—á –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –≤ 2D-—Å–µ—Ç–∫–µ, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏, –≤—ã—è–≤–ª—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.'}, 'en': {'title': 'Unlocking Swarm Intelligence in Language Models', 'desc': "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."}, 'zh': {'title': 'Êé¢Á¥¢Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁæ§‰ΩìÊô∫ËÉΩÊΩúÂäõ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊé®ÁêÜÊñπÈù¢ÊòæÁ§∫Âá∫ÊΩúÂäõÔºå‰ΩÜÂÆÉ‰ª¨Âú®Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºàMASÔºâ‰∏≠Âú®‰∏•Ê†ºÁ∫¶Êùü‰∏ãÁöÑÂçèË∞ÉËÉΩÂäõ‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÔºåÂ∞§ÂÖ∂ÊòØÂú®Áæ§‰ΩìÊô∫ËÉΩÁöÑÁªÜÂæÆÂ∑ÆÂà´ÊñπÈù¢„ÄÇÁé∞ÊúâÂü∫ÂáÜÊµãËØïÂæÄÂæÄÊó†Ê≥ïÂÆåÂÖ®ÊçïÊçâÂà∞Âú®‰∏çÂÆåÊï¥Êó∂Á©∫‰ø°ÊÅØ‰∏ãÔºåÊô∫ËÉΩ‰ΩìËøõË°åÂéª‰∏≠ÂøÉÂåñÂçèË∞ÉÊâÄÈù¢‰∏¥ÁöÑÁã¨ÁâπÊåëÊàò„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜSwarmBenchÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂü∫ÂáÜÔºåÊó®Âú®Á≥ªÁªüËØÑ‰º∞LLMs‰Ωú‰∏∫Âéª‰∏≠ÂøÉÂåñÊô∫ËÉΩ‰ΩìÁöÑÁæ§‰ΩìÊô∫ËÉΩËÉΩÂäõ„ÄÇÈÄöËøáËØÑ‰º∞Â§ö‰∏™È¢ÜÂÖàÁöÑLLMsÔºåÊàë‰ª¨ÂèëÁé∞ÂÆÉ‰ª¨Âú®‰ªªÂä°‰∏≠ÁöÑË°®Áé∞Â∑ÆÂºÇÊòæËëóÔºåÁ™ÅÊòæ‰∫ÜÂú®Â±ÄÈÉ®‰ø°ÊÅØÈôêÂà∂‰∏ãÁöÑÂçèË∞ÉÂõ∞Èöæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04528', 'title': 'Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving', 'url': 'https://huggingface.co/papers/2505.04528', 'abstract': 'As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.', 'score': 7, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '0e9e0d509e4b4624', 'authors': ['Qi Liu', 'Xinhao Zheng', 'Renqiu Xia', 'Xingzhi Qi', 'Qinxiang Cao', 'Junchi Yan'], 'affiliations': ['Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04528.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#alignment', '#interpretability', '#reasoning', '#agents'], 'emoji': 'üß†', 'ru': {'title': '–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∫–∞–∫ –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ FPS (Formal Problem-Solving), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å—Ä–µ–¥—ã —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω D-FPS (Deductive FPS), —Ä–∞–∑–¥–µ–ª—è—é—â–∏–π —Ä–µ—à–µ–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –ø–æ–¥—Ö–æ–¥—É. –°–æ–∑–¥–∞–Ω—ã —Ç—Ä–∏ –Ω–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ RPE –¥–ª—è —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Revolutionizing Problem-Solving with Formal Frameworks', 'desc': 'This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods.'}, 'zh': {'title': 'ÂΩ¢ÂºèÂåñÈóÆÈ¢òËß£ÂÜ≥ÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÈóÆÈ¢òËß£ÂÜ≥ÁöÑÂΩ¢ÂºèÂåñÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∞ÜÈóÆÈ¢òËß£ÂÜ≥ËßÜ‰∏∫Á°ÆÂÆöÊÄßÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÁöÑÊ°ÜÊû∂„ÄÇ‰ΩúËÄÖ‰ªãÁªç‰∫ÜFPSÔºàÊ≠£ÂºèÈóÆÈ¢òËß£ÂÜ≥ÔºâÊ°ÜÊû∂ÔºåÂà©Áî®Áé∞ÊúâÁöÑÊ≠£ÂºèÂÆöÁêÜËØÅÊòéÁéØÂ¢ÉËøõË°åËøáÁ®ãÈ™åËØÅÁöÑÈóÆÈ¢òËß£ÂÜ≥„ÄÇ‰∏∫‰∫ÜÊèêÈ´ò‰∫∫Á±ªÂØπÈΩêÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜD-FPSÔºàÊºîÁªéFPSÔºâÔºåÂ∞ÜÊ±ÇËß£‰∏éÁ≠îÊ°àÈ™åËØÅËß£ËÄ¶„ÄÇÊúÄÂêéÔºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏â‰∏™Âü∫ÂáÜÊµãËØïÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ¨¶Âè∑ÊñπÊ≥ïRPEÊù•ËØÑ‰º∞Á≠îÊ°àÁöÑÊ≠£Á°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03912', 'title': 'OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2505.03912', 'abstract': 'Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.', 'score': 6, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': 'f7347c1b093f9488', 'authors': ['Can Cui', 'Pengxiang Ding', 'Wenxuan Song', 'Shuanghao Bai', 'Xinyang Tong', 'Zirui Ge', 'Runze Suo', 'Wanqi Zhou', 'Yang Liu', 'Bofang Jia', 'Han Zhao', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['HKUST(GZ)', 'Westlake University', 'Xian Jiaotong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03912.jpg', 'data': {'categories': ['#architecture', '#optimization', '#agents', '#open_source', '#multimodal'], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–≤—É—Ö—Å–∏—Å—Ç–µ–º–Ω—ã—Ö VLA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –¥–≤—É—Ö—Å–∏—Å—Ç–µ–º–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º VLA (Vision-Language-Action) –≤ –æ–±–ª–∞—Å—Ç–∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –ø—Ä–æ–≤–æ–¥—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —ç–º–ø–∏—Ä–∏—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –¶–µ–ª—å—é —Ä–∞–±–æ—Ç—ã —è–≤–ª—è–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –ü—Ä–æ–µ–∫—Ç –ø–ª–∞–Ω–∏—Ä—É–µ—Ç —Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –Ω–æ–≤—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Empowering Embodied Intelligence with Open-Source VLA Models', 'desc': 'This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers.'}, 'zh': {'title': 'Êé®Âä®ÂèåÁ≥ªÁªüVLAÊû∂ÊûÑÁöÑÂºÄÊ∫êÊé¢Á¥¢', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂèåÁ≥ªÁªüËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊû∂ÊûÑÂú®ÂÖ∑Ë∫´Êô∫ËÉΩÁ†îÁ©∂‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÊåáÂá∫ÁõÆÂâçÁº∫‰πèË∂≥Â§üÁöÑÂºÄÊ∫êÂ∑•‰ΩúÊù•ËøõË°åÊÄßËÉΩÂàÜÊûêÂíå‰ºòÂåñ„ÄÇ‰ΩúËÄÖÊÄªÁªìÂπ∂ÊØîËæÉ‰∫ÜÁé∞ÊúâÂèåÁ≥ªÁªüÊû∂ÊûÑÁöÑÁªìÊûÑËÆæËÆ°ÔºåÂπ∂ÂØπÂÖ∂Ê†∏ÂøÉËÆæËÆ°ÂÖÉÁ¥†ËøõË°å‰∫ÜÁ≥ªÁªüÁöÑÂÆûËØÅËØÑ‰º∞„ÄÇÊúÄÁªàÔºåÊú¨ÊñáÂ∞ÜÊèê‰æõ‰∏Ä‰∏™‰ΩéÊàêÊú¨ÁöÑÂºÄÊ∫êÊ®°ÂûãÔºå‰ª•‰æøËøõ‰∏ÄÊ≠•Êé¢Á¥¢ÂíåÁ†îÁ©∂„ÄÇËØ•È°πÁõÆÂ∞ÜÊåÅÁª≠Êõ¥Êñ∞ÔºåÊèê‰æõÊõ¥Â§öÂÆûÈ™åÁªìËÆ∫ÂíåÊÄßËÉΩÊîπËøõÁöÑÂºÄÊ∫êÊ®°Âûã‰æõÂ§ßÂÆ∂ÈÄâÊã©„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04606', 'title': 'OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution', 'url': 'https://huggingface.co/papers/2505.04606', 'abstract': "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", 'score': 5, 'issue_id': 3657, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '25e97f182730fc25', 'authors': ['Lianghong Guo', 'Wei Tao', 'Runhan Jiang', 'Yanlin Wang', 'Jiachi Chen', 'Xilin Liu', 'Yuchi Ma', 'Mingzhi Mao', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University, China', 'Huawei Cloud Computing Technologies Co., Ltd., China', 'Independent Researcher, China', 'Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04606.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset', '#long_context', '#multimodal'], 'emoji': 'üêô', 'ru': {'title': 'OmniGIRL: –í—ã–∑–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á GitHub', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniGIRL - –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∏ –º—É–ª—å—Ç–∏–¥–æ–º–µ–Ω–Ω—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –Ω–∞ GitHub. OmniGIRL –≤–∫–ª—é—á–∞–µ—Ç 959 –∑–∞–¥–∞—á –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–æ—Å—å–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ OmniGIRL, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω –Ω–µ—É–¥–∞—á LLM –Ω–∞ OmniGIRL –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç insights –¥–ª—è –±—É–¥—É—â–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π.'}, 'en': {'title': 'OmniGIRL: A Comprehensive Benchmark for GitHub Issue Resolution', 'desc': 'This paper introduces OmniGIRL, a new benchmark for automatically resolving GitHub issues using large language models (LLMs). Unlike existing benchmarks, OmniGIRL is designed to be multilingual, multimodal, and multi-domain, addressing the limitations of focusing on a single programming language and a narrow range of issues. The benchmark includes 959 instances from four programming languages and eight domains, highlighting the diversity of real-world problems. Evaluation results show that current LLMs perform poorly on this benchmark, particularly in resolving issues that require understanding images, indicating a need for further advancements in model capabilities.'}, 'zh': {'title': 'OmniGIRLÔºöÂ§öËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÁöÑGitHubÈóÆÈ¢òËß£ÂÜ≥Âü∫ÂáÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫ÜOmniGIRLÔºå‰∏Ä‰∏™Â§öËØ≠Ë®Ä„ÄÅÂ§öÊ®°ÊÄÅÂíåÂ§öÈ¢ÜÂüüÁöÑGitHubÈóÆÈ¢òËß£ÂÜ≥Âü∫ÂáÜ„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÂ≠òÂú®‰∏â‰∏™‰∏ªË¶ÅÈôêÂà∂ÔºöÂè™ÂÖ≥Ê≥®Âçï‰∏ÄÁºñÁ®ãËØ≠Ë®Ä„ÄÅË¶ÜÁõñÈ¢ÜÂüüÁã≠Á™Ñ‰ª•Âèä‰ªÖ‰æùËµñÊñáÊú¨‰ø°ÊÅØ„ÄÇOmniGIRLÂåÖÂê´Êù•Ëá™ÂõõÁßçÁºñÁ®ãËØ≠Ë®ÄÂíåÂÖ´‰∏™‰∏çÂêåÈ¢ÜÂüüÁöÑ959‰∏™‰ªªÂä°ÂÆû‰æãÔºåÊó®Âú®Êõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂΩìÂâçÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®OmniGIRL‰∏äÁöÑË°®Áé∞ÊúâÈôêÔºåÂ∞§ÂÖ∂Âú®Â§ÑÁêÜÈúÄË¶ÅÁêÜËß£ÂõæÂÉèÁöÑÈóÆÈ¢òÊó∂Ë°®Áé∞Êõ¥Â∑Æ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03570', 'title': 'OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents', 'url': 'https://huggingface.co/papers/2505.03570', 'abstract': 'In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.', 'score': 4, 'issue_id': 3654, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': 'e87199c8805bce4f', 'authors': ['Mariya Davydova', 'Daniel Jeffries', 'Patrick Barker', 'Arturo M√°rquez Flores', 'Sin√©ad Ryan'], 'affiliations': ['Kentauros AI Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.03570.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#optimization', '#benchmark'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': 'OSUniverse: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω OSUniverse - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, –Ω–∞–≤–∏–≥–∏—Ä—É—é—â–∏—Ö –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∫–ª–∏–∫–æ–≤ –¥–æ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–µ –±–æ–ª–µ–µ 50% —É—Å–ø–µ—Ö–∞, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ –æ—Ñ–∏—Å–Ω—ã–µ —Ä–∞–±–æ—Ç–Ω–∏–∫–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å–æ –≤—Å–µ–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –∏–º–µ–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å—é –º–µ–Ω–µ–µ 2%.'}, 'en': {'title': 'OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks', 'desc': 'This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.'}, 'zh': {'title': 'OSUniverseÔºöGUIÂØºËà™AIÁöÑÂÖ®Êñ∞Âü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜOSUniverseÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπÈ´òÁ∫ßGUIÂØºËà™AI‰ª£ÁêÜÁöÑÂ§çÊùÇÂ§öÊ®°ÊÄÅÊ°åÈù¢‰ªªÂä°Âü∫ÂáÜÔºåÊó®Âú®ÊòìÁî®ÊÄß„ÄÅÂèØÊâ©Â±ïÊÄß„ÄÅÂÖ®Èù¢Ë¶ÜÁõñÊµãËØïÊ°à‰æãÂíåËá™Âä®È™åËØÅÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨Â∞Ü‰ªªÂä°ÂàÜ‰∏∫‰∏çÂêåÂ§çÊùÇÂ∫¶ÁöÑÁ∫ßÂà´Ôºå‰ªéÂü∫Êú¨ÁöÑÁ≤æÁ°ÆÁÇπÂáªÂà∞ÈúÄË¶ÅÁÅµÊ¥ªÊÄß„ÄÅÁ≤æÁ°ÆÊÄßÂíåÊ∏ÖÊô∞ÊÄùÁª¥ÁöÑÂ§öÊ≠•È™§„ÄÅÂ§öÂ∫îÁî®Á®ãÂ∫èÊµãËØï„ÄÇÂú®Âü∫ÂáÜÁöÑÁ¨¨‰∏ÄÁâà‰∏≠ÔºåÊàë‰ª¨Ë∞ÉÊï¥‰∫ÜÊµãËØïÊ°à‰æãÁöÑÂ§çÊùÇÊÄßÔºå‰ª•Á°Æ‰øùÂΩìÊó∂ÁöÑÊúÄÂÖàËøõÔºàSOTAÔºâ‰ª£ÁêÜÁöÑÁªìÊûú‰∏çË∂ÖËøá50%ÔºåËÄåÊôÆÈÄöÁôΩÈ¢ÜÂ∑•‰∫∫ÂèØ‰ª•ÂÆåÁæéÂÆåÊàêÊâÄÊúâËøô‰∫õ‰ªªÂä°„ÄÇËØ•Âü∫ÂáÜÂèØ‰ª•ÊâãÂä®ËØÑÂàÜÔºåÂêåÊó∂Êàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âπ≥ÂùáÈîôËØØÁéá‰Ωé‰∫é2%ÁöÑËá™Âä®È™åËØÅÊú∫Âà∂Ôºå‰∏∫ÂÖ®Èù¢Ëá™Âä®ÂåñÊµãÈáèGUIÂØºËà™AI‰ª£ÁêÜÁöÑËøõÂ±ï„ÄÅËÉΩÂäõÂíåÊúâÊïàÊÄßÊèê‰æõ‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03418', 'title': 'Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey', 'url': 'https://huggingface.co/papers/2505.03418', 'abstract': 'Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.', 'score': 4, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '8417799a01a2ecc2', 'authors': ['Da Zheng', 'Lun Du', 'Junwei Su', 'Yuchen Tian', 'Yuqi Zhu', 'Jintian Zhang', 'Lanning Wei', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['Ant Group, China', 'The University of Hong Kong, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03418.jpg', 'data': {'categories': ['#rl', '#survey', '#math', '#training', '#reasoning', '#science', '#data'], 'emoji': 'üß†', 'ru': {'title': 'LLM: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ç–∞–∫–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏, –∫–∞–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ —Ü–µ–ø–æ—á–∫–µ –º—ã—Å–ª–µ–π (Chain-of-Thought), —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –í —Å—Ç–∞—Ç—å–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞, –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –¢–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∏ –±—É–¥—É—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.'}, 'en': {'title': 'Unlocking Complex Problem-Solving with Large Language Models', 'desc': 'This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving.'}, 'zh': {'title': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºöÂ§çÊùÇÈóÆÈ¢òËß£ÂÜ≥ÁöÑÊñ∞Â∑•ÂÖ∑', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÈóÆÈ¢òËß£ÂÜ≥‰∏≠ÁöÑËÉΩÂäõÂíåÂ±ÄÈôêÊÄß„ÄÇ‰∏é‰º†ÁªüËÆ°ÁÆóÁ≥ªÁªü‰∏çÂêåÔºåLLMsÁªìÂêà‰∫ÜÂº∫Â§ßÁöÑËÆ°ÁÆóËÉΩÂäõÂíå‰∫∫Á±ªÊé®ÁêÜÁöÑËøë‰ººÔºåËÉΩÂ§üÁîüÊàêËß£ÂÜ≥ÊñπÊ°àÂíåËøõË°åÊé®ÁêÜ„ÄÇÂ∞ΩÁÆ°LLMsÂú®Â§öÊ≠•È™§Êé®ÁêÜ„ÄÅÈ¢ÜÂüüÁü•ËØÜÊï¥ÂêàÂíåÁªìÊûúÈ™åËØÅÊñπÈù¢Èù¢‰∏¥ÊåëÊàòÔºå‰ΩÜÂÆÉ‰ª¨Âú®ËΩØ‰ª∂Â∑•Á®ã„ÄÅÊï∞Â≠¶Êé®ÁêÜ„ÄÅÊï∞ÊçÆÂàÜÊûêÂíåÁßëÂ≠¶Á†îÁ©∂Á≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®ÊΩúÂäõÂ∑®Â§ß„ÄÇÊú¨ÊñáËøòËÆ®ËÆ∫‰∫ÜÂΩìÂâçLLMËß£ÂÜ≥ÊñπÊ°àÁöÑÂü∫Êú¨Â±ÄÈôêÊÄß‰ª•ÂèäÊú™Êù•Âú®Â§çÊùÇÈóÆÈ¢òËß£ÂÜ≥‰∏≠ÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04601', 'title': 'OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning', 'url': 'https://huggingface.co/papers/2505.04601', 'abstract': "OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.", 'score': 2, 'issue_id': 3666, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '0b9c03d9680cfe04', 'authors': ['Xianhang Li', 'Yanqing Liu', 'Haoqin Tu', 'Hongru Zhu', 'Cihang Xie'], 'affiliations': ['University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.04601.jpg', 'data': {'categories': ['#small_models', '#dataset', '#open_source', '#multimodal', '#architecture'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': 'OpenVision: –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'OpenVision –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–µ–º–µ–π—Å—Ç–≤–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å CLIP –æ—Ç OpenAI –ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –≠–Ω–∫–æ–¥–µ—Ä—ã OpenVision –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ CLIPS –∏ Recap-DataComp-1B, –Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –∏–¥–µ–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —ç–Ω–∫–æ–¥–µ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ (–æ—Ç 5,9 –¥–æ 632,1 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. OpenVision –∑–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–±–µ–ª –≤ –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'OpenVision: Open and Efficient Vision Encoders for Multimodal Models', 'desc': "This paper introduces OpenVision, a new family of vision encoders that are fully open and cost-effective, designed to compete with OpenAI's CLIP. OpenVision not only matches but can also surpass CLIP's performance when used in multimodal frameworks like LLaVA. The authors leverage existing methodologies and datasets to improve encoder quality and provide insights into enhancing multimodal models. By offering a range of models with varying parameters, OpenVision allows users to choose between high performance and efficiency for their specific applications."}, 'zh': {'title': 'ÂºÄÊîæËßÜËßâÁºñÁ†ÅÂô®ÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÊÄßËÉΩ', 'desc': 'OpenVisionÊòØ‰∏Ä‰∏™ÂÆåÂÖ®ÂºÄÊîæÁöÑËßÜËßâÁºñÁ†ÅÂô®ÂÆ∂ÊóèÔºåÊó®Âú®‰∏éOpenAIÁöÑCLIPÁõ∏Â™≤ÁæéÊàñË∂ÖË∂äÂÖ∂ÊÄßËÉΩ„ÄÇËØ•ËÆ∫ÊñáÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Áé∞ÊúâÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÂíåÊï∞ÊçÆÈõÜÔºåÊèêÂçáÁºñÁ†ÅÂô®ÁöÑË¥®ÈáèÔºåÂπ∂‰∏∫Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂèëÂ±ïÊèê‰æõÂÆûÁî®ÁöÑÂ•ΩÂ§Ñ„ÄÇOpenVisionÊèê‰æõ‰∫Ü‰ªé590‰∏áÂà∞6.32‰∫øÂèÇÊï∞ÁöÑÂ§öÁßçÈÄâÊã©Ôºå‰ΩøÂæóÂºÄÂèëËÄÖÂèØ‰ª•Âú®Ê®°ÂûãÂÆπÈáèÂíåÊïàÁéá‰πãÈó¥ÁÅµÊ¥ªÊùÉË°°„ÄÇÈÄöËøáËøô‰∫õÂºÄÊîæÁöÑËµÑÊ∫êÔºåOpenVision‰∏∫Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊûÑÂª∫Êèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04253', 'title': 'LLM-Independent Adaptive RAG: Let the Question Speak for Itself', 'url': 'https://huggingface.co/papers/2505.04253', 'abstract': 'Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '7ce9a465202a9c3c', 'authors': ['Maria Marina', 'Nikolay Ivanov', 'Sergey Pletenev', 'Mikhail Salnikov', 'Daria Galimzianova', 'Nikita Krayko', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.04253.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rag', '#benchmark', '#hallucinations'], 'emoji': 'üîç', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–µ –∑–∞–≤–∏—Å—è—â–∏–µ –æ—Ç —Å–∞–º–∏—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∏–ª–∏ 27 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ 7 –≥—Ä—É–ø–ø, –∏ –∏—Ö –≥–∏–±—Ä–∏–¥–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥—ã –±—ã–ª–∏ –æ—Ü–µ–Ω–µ–Ω—ã –Ω–∞ 6 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º –ø–æ–≤—ã—à–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Efficient Adaptive Retrieval: Enhancing QA with External Information', 'desc': 'This paper addresses the issue of hallucinations in Large Language Models (LLMs) and presents a solution through Retrieval-Augmented Generation (RAG). The authors propose lightweight, LLM-independent adaptive retrieval methods that utilize external information, aiming to improve efficiency while maintaining performance. They analyze 27 features across 7 groups to create effective hybrid combinations for retrieval. The results indicate that their approach can achieve comparable performance to complex LLM-based methods but with significant efficiency improvements.'}, 'zh': {'title': 'ËΩªÈáèÁ∫ßËá™ÈÄÇÂ∫îÊ£ÄÁ¥¢ÔºåÊèêÂçáÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂÆπÊòìÂá∫Áé∞ÂπªËßâÔºåËÄåÂ¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÔºàRAGÔºâÂèØ‰ª•Â∏ÆÂä©ÂáèËΩªËøô‰∏ÄÈóÆÈ¢òÔºå‰ΩÜ‰ª£‰ª∑È´òÊòÇ‰∏îÂèØËÉΩÂØºËá¥ÈîôËØØ‰ø°ÊÅØ„ÄÇËá™ÈÄÇÂ∫îÊ£ÄÁ¥¢Êó®Âú®‰ªÖÂú®ÂøÖË¶ÅÊó∂ËøõË°åÊ£ÄÁ¥¢Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂü∫‰∫éLLMÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÔºåÊïàÁéá‰Ωé‰∏ã‰∏î‰∏çÂàáÂÆûÈôÖ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§ñÈÉ®‰ø°ÊÅØÁöÑËΩªÈáèÁ∫ßLLMÁã¨Á´ãËá™ÈÄÇÂ∫îÊ£ÄÁ¥¢ÊñπÊ≥ïÔºåÁ†îÁ©∂‰∫Ü27‰∏™ÁâπÂæÅÂπ∂Â∞ÜÂÖ∂ÁªÑÁªáÊàê7‰∏™ÁªÑÂèäÂÖ∂Ê∑∑ÂêàÁªÑÂêà„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®6‰∏™ÈóÆÁ≠îÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞‰∏éÂ§çÊùÇÁöÑLLMÊñπÊ≥ïÁõ∏ÂΩìÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊïàÁéáÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂ§ñÈÉ®‰ø°ÊÅØÂú®Ëá™ÈÄÇÂ∫îÊ£ÄÁ¥¢‰∏≠ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03538', 'title': 'RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT', 'url': 'https://huggingface.co/papers/2505.03538', 'abstract': 'Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '9e7cb17dec2eda27', 'authors': ['Chuyu Zhao', 'Hao Huang', 'Jiashuo Guo', 'Ziyu Shen', 'Zhongwei Zhou', 'Jie Liu', 'Zekuan Yu'], 'affiliations': ['Academy for Engineering and Technology, Fudan University, Shanghai 200433, China', 'Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China', 'School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03538.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#transfer_learning', '#training'], 'emoji': 'ü¶∑', 'ru': {'title': '–£–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑—É–±–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑—É–±–æ–≤ –Ω–∞ –ö–õ–ö–¢-—Å–Ω–∏–º–∫–∞—Ö, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Region-Aware Instructive Learning (RAIL). RAIL –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö–≥—Ä—É–ø–ø–æ–≤—É—é –¥–≤—É—Ö—Å—Ç—É–¥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –≥–¥–µ –∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–∞ —Å—Ç—É–¥–µ–Ω—Ç–∞, —É–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –æ–±—â–∏–º —É—á–∏—Ç–µ–ª–µ–º. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç –¥–≤–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: Disagreement-Focused Supervision (DFS) Controller –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ Confidence-Aware Learning (CAL) Modulator –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RAIL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing 3D Tooth Segmentation with RAIL: A Smart Learning Approach', 'desc': 'This paper presents a new method called Region-Aware Instructive Learning (RAIL) for improving 3D tooth segmentation from CBCT scans using semi-supervised learning. RAIL addresses challenges like limited supervision in ambiguous areas and the unreliability of pseudo-labels by employing a dual-group, dual-student framework with a shared teacher network. It introduces two key mechanisms: Disagreement-Focused Supervision (DFS) to enhance learning in uncertain regions, and Confidence-Aware Learning (CAL) to stabilize predictions by focusing on high-confidence areas. Experimental results demonstrate that RAIL outperforms existing methods, making it a significant advancement in the field of medical image segmentation.'}, 'zh': {'title': 'Âå∫ÂüüÊÑüÁü•ÊåáÂØºÂ≠¶‰π†ÔºöÊèêÂçá3DÁâôÈΩøÂàÜÂâ≤ÁöÑÂçäÁõëÁù£ÊñπÊ≥ï', 'desc': 'ÂçäÁõëÁù£Â≠¶‰π†Âú®CBCTÊâ´ÊèèÁöÑ3DÁâôÈΩøÂàÜÂâ≤‰∏≠ÂèòÂæóË∂äÊù•Ë∂äÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê†áÊ≥®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöÂú®ÁªìÊûÑÊ®°Á≥äÊàñÊ†áÊ≥®ÈîôËØØÁöÑÂå∫ÂüüÔºåÁõëÁù£ËÆ≠ÁªÉ‰∏≠ÁöÑÁ∫†Ê≠£ÁõëÁù£ÊúâÈôêÔºõ‰ª•ÂèäÂú®Êú™Ê†áÊ≥®Êï∞ÊçÆ‰∏äÔºåÁî±‰∫é‰∏çÂèØÈù†ÁöÑ‰º™Ê†áÁ≠æÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂå∫ÂüüÊÑüÁü•ÊåáÂØºÂ≠¶‰π†ÔºàRAILÔºâÔºåËøôÊòØ‰∏ÄÁßçÂèåÁªÑÂèåÂ≠¶ÁîüÁöÑÂçäÁõëÁù£Ê°ÜÊû∂ÔºåÈÄöËøá‰∫§ÊõøËÆ≠ÁªÉ‰øÉËøõÁªÑÈó¥Áü•ËØÜËΩ¨ÁßªÂíåÂçè‰ΩúÊåáÂØºÔºåÂêåÊó∂ÂáèÂ∞ëÂØπÂçï‰∏ÄÊ®°ÂûãÁâπÂæÅÁöÑËøáÊãüÂêà„ÄÇRAILÂºïÂÖ•‰∫Ü‰∏§ÁßçÊåáÂØºÊú∫Âà∂ÔºåÂàÜÂà´ÊòØÂÖ≥Ê≥®ÂàÜÊ≠ßÁöÑÁõëÁù£ÊéßÂà∂Âô®ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÂ≠¶‰π†Ë∞ÉËäÇÂô®Ôºå‰ª•ÊèêÈ´òÊ®°ÂûãÂú®‰∏çÁ°ÆÂÆöÂå∫ÂüüÁöÑÂ≠¶‰π†ÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03105', 'title': 'Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation', 'url': 'https://huggingface.co/papers/2505.03105', 'abstract': 'Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI\'s evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.', 'score': 1, 'issue_id': 3657, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '24cdaf99b9b04dad', 'authors': ['Xule Lin'], 'affiliations': ['Department of Management and Entrepreneurship, Imperial College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.03105.jpg', 'data': {'categories': ['#agents', '#healthcare', '#science', '#ethics', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': 'Cognitio Emergens: –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –∫–æ—ç–≤–æ–ª—é—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –ò–ò –≤ –Ω–∞—É–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é Cognitio Emergens (CE) - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. CE –æ–ø–∏—Å—ã–≤–∞–µ—Ç, –∫–∞–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Ä–æ–ª–∏ –º–µ–∂–¥—É –ª—é–¥—å–º–∏ –∏ –ò–ò, –∫–∞–∫–∏–µ —ç–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∏ –∫–∞–∫–∏–µ —Å–∏–ª—ã –≤–ª–∏—è—é—Ç –Ω–∞ —ç–≤–æ–ª—é—Ü–∏—é —ç—Ç–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–Ω—ã–π –∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–µ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –ò–ò. CE –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Ä–æ–ª—å –ò–ò –≤ –Ω–∞—É–∫–µ, –∏–∑–±–µ–≥–∞—è –∫–∞–∫ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–º–∞, —Ç–∞–∫ –∏ –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ö–æ–≤.'}, 'en': {'title': 'Transforming Scientific Collaboration: Humans and AI as Co-Evolutionary Partners', 'desc': 'This paper discusses how the relationship between humans and AI in scientific research is changing from a simple tool-user dynamic to a more collaborative partnership. It introduces a new framework called Cognitio Emergens (CE) that addresses the limitations of existing models by focusing on the evolving roles and interactions between humans and AI over time. CE includes three main components: Agency Configurations that describe how authority is shared, Epistemic Dimensions that outline capabilities developed through collaboration, and Partnership Dynamics that explore how these relationships change. By viewing human-AI collaboration as a co-evolutionary process, the framework aims to enhance scientific understanding while ensuring that human input remains significant in the face of advancing AI capabilities.'}, 'zh': {'title': '‰∫∫Á±ª‰∏éAIÁöÑÂÖ±ÂêåËøõÂåñÔºöÁü•ËØÜÂàõÈÄ†ÁöÑÊñ∞ËßÜËßí', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∫∫Á±ª‰∏é‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâ‰πãÈó¥ÁöÑÂêà‰ΩúÂÖ≥Á≥ªÂ¶Ç‰Ωï‰ªéÁÆÄÂçïÁöÑÂ∑•ÂÖ∑‰ΩøÁî®ËÄÖËΩ¨Âèò‰∏∫ÂÖ±ÂêåËøõÂåñÁöÑÁü•ËØÜ‰ºô‰º¥„ÄÇÊñáÁ´†‰ªãÁªç‰∫ÜCognitio EmergensÔºàCEÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂº∫Ë∞ÉÁßëÂ≠¶ÁêÜËß£ÊòØÂ¶Ç‰ΩïÈÄöËøá‰∫∫Á±ª‰∏éAIÁöÑ‰∫íÂä®ÈÄêÊ≠•ÂΩ¢ÊàêÁöÑ„ÄÇCEÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºö‰ª£ÁêÜÈÖçÁΩÆ„ÄÅËÆ§Áü•Áª¥Â∫¶Âíå‰ºô‰º¥Âä®ÊÄÅÔºåÂ∏ÆÂä©ÊèèËø∞‰∫∫Á±ª‰∏éAI‰πãÈó¥ÁöÑÊùÉÂäõÂàÜÈÖç„ÄÅÂêà‰ΩúËÉΩÂäõÂíåÂÖ≥Á≥ªÊºîÂèò„ÄÇÈÄöËøáÈáçÊñ∞ÂÆö‰πâ‰∫∫Á±ª‰∏éAIÁöÑÁßëÂ≠¶Âêà‰ΩúÔºåCEÊèê‰æõ‰∫Ü‰øÉËøõÊúâÊÑè‰πâÁöÑ‰∫∫Á±ªÂèÇ‰∏éÂíåÁßëÂ≠¶Á™ÅÁ†¥ÁöÑÊ¶ÇÂøµÂ∑•ÂÖ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02820', 'title': 'AutoLibra: Agent Metric Induction from Open-Ended Feedback', 'url': 'https://huggingface.co/papers/2505.02820', 'abstract': 'Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don\'t click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent\'s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra\'s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.', 'score': 1, 'issue_id': 3661, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': '52333e4c868e25f8', 'authors': ['Hao Zhu', 'Phil Cuvin', 'Xinkai Yu', 'Charlotte Ka Yee Yan', 'Jason Zhang', 'Diyi Yang'], 'affiliations': ['stanford.edu', 'upenn.edu', 'utoronto.ca'], 'pdf_title_img': 'assets/pdf/title_img/2505.02820.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rlhf', '#alignment', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': 'AutoLibra: –£–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π', 'desc': "AutoLibra - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ –æ—Ç–∑—ã–≤—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Å—Ö–æ–∂–∏—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–µ—Ç–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏. AutoLibra —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–∞-–º–µ—Ç—Ä–∏–∫–∏ '–ø–æ–∫—Ä—ã—Ç–∏–µ' –∏ '–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å' –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–∞–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∏ AutoLibra —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö."}, 'en': {'title': 'Transforming Feedback into Fine-Grained Agent Evaluation Metrics', 'desc': "The paper introduces AutoLibra, a novel framework for evaluating agents using fine-grained metrics derived from open-ended human feedback. It transforms qualitative feedback into quantitative metrics by clustering behaviors and defining clear examples, allowing for a more nuanced assessment of agent performance. AutoLibra also proposes two meta-metrics, 'coverage' and 'redundancy', to ensure that the evaluation metrics align well with the feedback provided. Experimental results show that AutoLibra improves agent performance significantly compared to traditional success metrics, making it a valuable tool for enhancing language agents."}, 'zh': {'title': 'AutoLibraÔºöÊô∫ËÉΩ‰ΩìËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AutoLibraÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞Êô∫ËÉΩ‰ΩìÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂ∞ÜÂºÄÊîæÂºè‰∫∫Á±ªÂèçÈ¶àËΩ¨Âåñ‰∏∫ÁªÜÁ≤íÂ∫¶ÁöÑËØÑ‰º∞ÊåáÊ†áÔºåÊù•Êõ¥Â•ΩÂú∞Ë°°ÈáèÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫„ÄÇAutoLibraÈÄöËøáÂØπÂèçÈ¶àËøõË°åÂΩíÁ±ªÂíåÂÆö‰πâÔºåÁîüÊàêÂÖ∑‰ΩìÁöÑËØÑ‰º∞Ê†áÂáÜÔºåÂπ∂Âà©Áî®Ëøô‰∫õÊ†áÂáÜÊù•‰ºòÂåñÊô∫ËÉΩ‰ΩìÁöÑË°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAutoLibraËÉΩÂ§üÁîüÊàêÊØîÁé∞ÊúâÂü∫ÂáÜÊõ¥ÊúâÊïàÁöÑËØÑ‰º∞ÊåáÊ†áÔºåÂπ∂Âú®Â§öÁßç‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçáÊô∫ËÉΩ‰ΩìÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.02393', 'title': 'Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection', 'url': 'https://huggingface.co/papers/2505.02393', 'abstract': 'Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.', 'score': 1, 'issue_id': 3651, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 –º–∞—è', 'en': 'May 5', 'zh': '5Êúà5Êó•'}, 'hash': 'b5c708abbb25e1ce', 'authors': ['Sungheon Jeong', 'Jihong Park', 'Mohsen Imani'], 'affiliations': ['MOLOCO', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.02393.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#synthetic'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ —Å–æ–±—ã—Ç–∏–π –∏–∑ RGB –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ–∞–Ω–æ–º–∞–ª–∏–π', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ IEF-VAD –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç RGB-–∫–∞–¥—Ä—ã —Å —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–æ–±—ã—Ç–∏–π–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏. –°–∏—Å—Ç–µ–º–∞ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —à—É–º –¥–∞—Ç—á–∏–∫–∞, –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–∫–∞–¥—Ä–æ–≤—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ —Å—Ç–∏–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—è–µ—Ç —Å–ª–∏—Ç–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ. IEF-VAD –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π. –ú–µ—Ç–æ–¥ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Video Anomaly Detection with Image-Event Fusion', 'desc': 'The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data.'}, 'zh': {'title': 'ÂõæÂÉè‰∏é‰∫ã‰ª∂ËûçÂêàÔºåÊèêÂçáËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÁöÑÂáÜÁ°ÆÊÄß', 'desc': 'Áé∞ÊúâÁöÑËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÂô®‰∏ªË¶Å‰æùËµñRGBÂ∏ßÔºå‰ΩÜËøô‰∫õÂ∏ßÁº∫‰πèÊçïÊçâÁ™ÅÂèëÊàñÁû¨ÊÄÅËøêÂä®Á∫øÁ¥¢ÁöÑÊó∂Èó¥ÂàÜËæ®Áéá„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂõæÂÉè-‰∫ã‰ª∂ËûçÂêàÁöÑËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÊ°ÜÊû∂ÔºàIEF-VADÔºâÔºåËØ•Ê°ÜÊû∂Áõ¥Êé•‰ªéRGBËßÜÈ¢ëÂêàÊàê‰∫ã‰ª∂Ë°®Á§∫ÔºåÂπ∂ÈÄöËøá‰∏ÄÁßçÂü∫‰∫é‰∏çÁ°ÆÂÆöÊÄßÁöÑËøáÁ®ãÂ∞ÜÂÖ∂‰∏éÂõæÂÉèÁâπÂæÅËûçÂêà„ÄÇËØ•Á≥ªÁªüÈÄöËøáÊãâÊôÆÊãâÊñØËøë‰ººÂª∫Ê®°ÈáçÂ∞æ‰º†ÊÑüÂô®Âô™Â£∞ÔºåÂ∫îÁî®Âç°Â∞îÊõºÈ£éÊ†ºÁöÑÈÄêÂ∏ßÊõ¥Êñ∞Êù•Âπ≥Ë°°Êó∂Èó¥‰∏äÁöÑÊ®°ÊÄÅÔºåÂπ∂Ëø≠‰ª£‰ºòÂåñËûçÂêàÁöÑÊΩúÂú®Áä∂ÊÄÅ‰ª•Ê∂àÈô§ÊÆã‰ΩôÁöÑË∑®Ê®°ÊÄÅÂô™Â£∞„ÄÇIEF-VADÂú®Â§ö‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÂºÇÂ∏∏Ê£ÄÊµãÂü∫ÂáÜ‰∏äËÆæÂÆö‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÂ±ïÁ§∫‰∫ÜÂêàÊàê‰∫ã‰ª∂Ë°®Á§∫Âú®Âº∫Ë∞ÉRGBÂ∏ß‰∏≠Â∏∏Ë¢´‰Ωé‰º∞ÁöÑËøêÂä®Á∫øÁ¥¢ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.01449', 'title': 'COSMOS: Predictable and Cost-Effective Adaptation of LLMs', 'url': 'https://huggingface.co/papers/2505.01449', 'abstract': 'Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.', 'score': 1, 'issue_id': 3665, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 –∞–ø—Ä–µ–ª—è', 'en': 'April 30', 'zh': '4Êúà30Êó•'}, 'hash': 'cc6993ec1bb06272', 'authors': ['Jiayu Wang', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.01449.jpg', 'data': {'categories': ['#optimization', '#data', '#inference', '#training', '#benchmark'], 'emoji': 'üîÆ', 'ru': {'title': 'COSMOS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç COSMOS - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏-–º–æ–¥–µ–ª–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –∑–∞–∫–æ–Ω—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ fine-tuning –∏ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –°–∏—Å—Ç–µ–º–∞ COSMOS –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –≤–æ—Å—å–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 92.72%.'}, 'en': {'title': 'COSMOS: Smart Predictions for Efficient LLM Adaptation', 'desc': 'This paper addresses the challenge of selecting the best model and adaptation strategy for large language models (LLMs) while managing resource constraints. It introduces COSMOS, a unified prediction framework designed to estimate the performance and cost of different adaptation strategies without the need for extensive trials. The framework utilizes lightweight proxy models and low-sample scaling laws to predict outcomes efficiently. The results indicate that COSMOS can significantly reduce computational costs while maintaining high prediction accuracy across various benchmarks.'}, 'zh': {'title': 'COSMOSÔºöÈ´òÊïàÈ¢ÑÊµãÈÄÇÂ∫îÁªìÊûúÁöÑÊ°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ËµÑÊ∫êÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÈÄâÊã©ÊúÄ‰Ω≥Ê®°ÂûãÂíåÈÄÇÂ∫îÁ≠ñÁï•ÈùûÂ∏∏ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCOSMOSÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÈ¢ÑÊµãÊ°ÜÊû∂ÔºåÂèØ‰ª•Âú®ÊúÄ‰ΩéÊàêÊú¨‰∏ãÈ´òÊïà‰º∞ËÆ°ÈÄÇÂ∫îÁªìÊûú„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ËΩªÈáèÁ∫ß‰ª£ÁêÜÊ®°ÂûãÂíå‰ΩéÊ†∑Êú¨Êâ©Â±ïÊ≥ïÂàôÊù•È¢ÑÊµãÂæÆË∞ÉÊÄßËÉΩÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåCOSMOSÂú®ÂÖ´‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂Âπ≥ÂùáÂáèÂ∞ë‰∫Ü92.72%ÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14683', 'title': 'Emerging Properties in Unified Multimodal Pretraining', 'url': 'https://huggingface.co/papers/2505.14683', 'abstract': 'Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/', 'score': 80, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '57522649bb8f8010', 'authors': ['Chaorui Deng', 'Deyao Zhu', 'Kunchang Li', 'Chenhui Gou', 'Feng Li', 'Zeyu Wang', 'Shu Zhong', 'Weihao Yu', 'Xiaonan Nie', 'Ziang Song', 'Guang Shi', 'Haoqi Fan'], 'affiliations': ['ByteDance Seed', 'Hong Kong University of Science and Technology', 'Monash University', 'Shenzhen Institutes of Advanced Technology', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.14683.jpg', 'data': {'categories': ['#3d', '#benchmark', '#reasoning', '#open_source', '#multimodal', '#dataset'], 'emoji': 'ü•Ø', 'ru': {'title': 'BAGEL: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–∏', 'desc': 'BAGEL - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö. BAGEL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏, –≤–∫–ª—é—á–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö –∫–∞–¥—Ä–æ–≤ –∏ 3D-–º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏.'}, 'en': {'title': 'BAGEL: Unifying Multimodal AI for Enhanced Understanding and Generation', 'desc': 'This paper presents BAGEL, an open-source foundational model designed for multimodal understanding and generation. BAGEL is a decoder-only model that has been pretrained on a vast dataset comprising text, images, videos, and web content. By leveraging this diverse multimodal data, BAGEL demonstrates advanced capabilities in complex reasoning tasks, outperforming existing open-source models. The authors aim to promote further research in multimodal AI by sharing their findings, pretraining methods, and code with the community.'}, 'zh': {'title': 'BAGELÔºöÂºÄÊ∫êÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàêÁöÑÁªü‰∏ÄÊ®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫BAGELÁöÑÂºÄÊ∫êÂü∫Á°ÄÊ®°ÂûãÔºåÂÆÉÊîØÊåÅÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàê„ÄÇBAGELÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑËß£Á†ÅÂô®Ê®°ÂûãÔºåÁªèËøáÂú®Â§ßÈáèÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅËßÜÈ¢ëÂíåÁΩëÁªúÊï∞ÊçÆ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇÈÄöËøá‰ΩøÁî®Â§öÊ†∑ÂåñÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÔºåBAGELÂú®Â§çÊùÇÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊñπÈù¢Â±ïÁé∞Âá∫Êñ∞ÁöÑËÉΩÂäõÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂºÄÊ∫êÁªü‰∏ÄÊ®°Âûã„ÄÇÊàë‰ª¨Â∏åÊúõÈÄöËøáÂàÜ‰∫´ÂÖ≥ÈîÆÂèëÁé∞„ÄÅÈ¢ÑËÆ≠ÁªÉÁªÜËäÇÂíåÊï∞ÊçÆÂàõÂª∫ÂçèËÆÆÔºå‰øÉËøõÂ§öÊ®°ÊÄÅÁ†îÁ©∂ÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11594', 'title': 'SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training', 'url': 'https://huggingface.co/papers/2505.11594', 'abstract': 'The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.', 'score': 45, 'issue_id': 3869, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '33309444d442b40c', 'authors': ['Jintao Zhang', 'Jia Wei', 'Pengle Zhang', 'Xiaoming Xu', 'Haofeng Huang', 'Haoxu Wang', 'Kai Jiang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.11594.jpg', 'data': {'categories': ['#inference', '#architecture', '#training', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è: –æ—Ç FP4 –¥–æ 8-–±–∏—Ç', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –í–æ-–ø–µ—Ä–≤—ã—Ö, –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–æ–≤—ã–µ —Ç–µ–Ω–∑–æ—Ä–Ω—ã–µ —è–¥—Ä–∞ FP4 –≤ GPU Blackwell –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—è 5-–∫—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç 8-–±–∏—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Revolutionizing Attention: Fast and Efficient for Training and Inference', 'desc': 'This paper addresses the inefficiency of attention mechanisms in machine learning, which typically have a quadratic time complexity. The authors introduce enhancements using FP4 Tensor Cores in Blackwell GPUs, achieving a significant speedup in attention computation, reaching 1038 TOPS on the RTX5090. Additionally, they explore low-bit attention for training tasks, proposing an 8-bit attention method that maintains performance during fine-tuning while showing slower convergence during pretraining. This work not only improves inference speed but also expands the application of low-bit attention to training, making it a versatile solution for large model training.'}, 'zh': {'title': 'ÊèêÂçáÊ≥®ÊÑèÂäõÊú∫Âà∂ÊïàÁéáÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊïàÁéáÈóÆÈ¢òÔºå‰∏ªË¶ÅÁî±‰∫éÂÖ∂‰∫åÊ¨°Êó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇÊàë‰ª¨ÈÄöËøáÂà©Áî®Blackwell GPU‰∏≠ÁöÑÊñ∞FP4 Tensor CoresÊù•Âä†ÈÄüÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÂÆûÁé∞‰∫ÜÂú®RTX5090‰∏äËææÂà∞1038 TOPSÁöÑÊÄßËÉΩÔºåÁõ∏ÊØî‰∫éÊúÄÂø´ÁöÑFlashAttentionÊèêÂçá‰∫Ü5ÂÄç„ÄÇÊàë‰ª¨ÁöÑFP4Ê≥®ÊÑèÂäõÂèØ‰ª•‰ª•Âç≥ÊèíÂç≥Áî®ÁöÑÊñπÂºèÂä†ÈÄüÂêÑÁßçÊ®°ÂûãÁöÑÊé®ÁêÜ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÈ¶ñÊ¨°Â∞Ü‰Ωé‰ΩçÊ≥®ÊÑèÂäõÂ∫îÁî®‰∫éËÆ≠ÁªÉ‰ªªÂä°ÔºåËÆæËÆ°‰∫ÜÈ´òÊïàÁöÑ8‰ΩçÊ≥®ÊÑèÂäõÔºåÂÆûÈ™åË°®ÊòéÂú®ÂæÆË∞É‰ªªÂä°‰∏≠Ë°®Áé∞Êó†ÊçüÔºå‰ΩÜÂú®È¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏≠Êî∂ÊïõÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13438', 'title': 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2505.13438', 'abstract': 'Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.', 'score': 27, 'issue_id': 3874, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '34ff8235b7a27562', 'authors': ['Penghui Qi', 'Zichen Liu', 'Tianyu Pang', 'Chao Du', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.13438.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning', '#math'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –õ–Ø–ú –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ AnytimeReasoner –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–õ–Ø–ú) –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –º—ã—à–ª–µ–Ω–∏—è –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±—é–¥–∂–µ—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤, –∑–∞—Å—Ç–∞–≤–ª—è—è –º–æ–¥–µ–ª—å —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Å–µ—á–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Ç–µ—Ö–Ω–∏–∫—É —Å–Ω–∏–∂–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ Budget Relative Policy Optimization (BRPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ GRPO –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –±—é–¥–∂–µ—Ç–æ–≤.'}, 'en': {'title': 'Optimizing Reasoning Efficiency with AnytimeReasoner', 'desc': 'This paper introduces AnytimeReasoner, a framework designed to improve the reasoning capabilities of large language models (LLMs) by optimizing their performance under varying token budgets. Unlike traditional reinforcement learning methods that focus solely on final outcomes, AnytimeReasoner allows for flexible reasoning by truncating the thinking process and summarizing answers based on sampled token budgets. This approach incorporates verifiable dense rewards, which aids in better credit assignment during the reinforcement learning optimization. The authors also propose a new technique called Budget Relative Policy Optimization (BRPO) to enhance the robustness and efficiency of the learning process, leading to superior performance in mathematical reasoning tasks compared to existing methods.'}, 'zh': {'title': '‰ºòÂåñÊé®ÁêÜÊÄßËÉΩÔºåÊèêÂçáÊïàÁéá‰∏éÁÅµÊ¥ªÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂêç‰∏∫AnytimeReasonerÔºåÊó®Âú®‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåAnytimeReasonerÈÄöËøáÂú®‰∏çÂêåÁöÑ‰ª§ÁâåÈ¢ÑÁÆóÁ∫¶Êùü‰∏ãËøõË°åÊé®ÁêÜÔºåÊèêÂçá‰∫Ü‰ª§ÁâåÁöÑ‰ΩøÁî®ÊïàÁéáÂíåÁÅµÊ¥ªÊÄß„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÂèØÈ™åËØÅÁöÑÂØÜÈõÜÂ•ñÂä±Ôºå‰ΩøÂæóÂú®Âº∫ÂåñÂ≠¶‰π†‰ºòÂåñ‰∏≠ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËøõË°å‰ø°Áî®ÂàÜÈÖç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÔºåAnytimeReasonerÂú®ÂêÑÁßçÈ¢ÑÁÆóÊù°‰ª∂‰∏ãÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÂíå‰ª§ÁâåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14460', 'title': 'VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank', 'url': 'https://huggingface.co/papers/2505.14460', 'abstract': 'DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.', 'score': 24, 'issue_id': 3876, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '98e34275d69f6e41', 'authors': ['Tianhe Wu', 'Jian Zou', 'Jie Liang', 'Lei Zhang', 'Kede Ma'], 'affiliations': ['City University of Hong Kong', 'OPPO Research Institute', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14460.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#rl', '#training', '#multimodal'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'VisualQuality-R1: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å VisualQuality-R1 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–∞, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ü–µ–Ω–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞—Ä–µ, –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä—É–ø–ø–æ–≤—É—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏. VisualQuality-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞, –ø–æ–Ω—è—Ç–Ω—ã–µ —á–µ–ª–æ–≤–µ–∫—É. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–π —à–∫–∞–ª—ã.'}, 'en': {'title': 'Revolutionizing Image Quality Assessment with Reasoning and Reinforcement Learning', 'desc': "This paper presents VisualQuality-R1, a novel no-reference image quality assessment (NR-IQA) model that leverages reasoning and reinforcement learning to evaluate image quality. By using group relative policy optimization, the model generates multiple quality scores for image pairs, allowing for a nuanced comparison of visual quality. The rewards for these scores are based on continuous fidelity measures, enhancing the model's ability to provide accurate quality assessments. Experimental results show that VisualQuality-R1 outperforms existing deep learning-based NR-IQA models and can generate detailed quality descriptions, making it effective for various image processing applications."}, 'zh': {'title': 'Êé®ÁêÜÈ©±Âä®ÁöÑÂõæÂÉèË¥®ÈáèËØÑ‰º∞Êñ∞Ê®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÂèÇËÄÉÂõæÂÉèË¥®ÈáèËØÑ‰º∞Ê®°ÂûãVisualQuality-R1ÔºåËØ•Ê®°ÂûãÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Êù•ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇVisualQuality-R1Âà©Áî®ÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºå‰∏∫ÊØèÂØπÂõæÂÉèÁîüÊàêÂ§ö‰∏™Ë¥®ÈáèËØÑÂàÜÔºåÂπ∂ËÆ°ÁÆóÂõæÂÉè‰πãÈó¥ÁöÑÊØîËæÉÊ¶ÇÁéá„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫åÂÖÉÊ†áÁ≠æ‰∏çÂêåÔºåÂ•ñÂä±ÊòØÂü∫‰∫éËøûÁª≠ÁöÑ‰øùÁúüÂ∫¶Â∫¶ÈáèÊù•ÂÆö‰πâÁöÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVisualQuality-R1Âú®ÂõæÂÉèË¥®ÈáèËØÑ‰º∞‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºåÂπ∂ËÉΩÂ§üÁîüÊàê‰∏∞ÂØåÁöÑË¥®ÈáèÊèèËø∞ÔºåÈÄÇÁî®‰∫éÂ§öÊï∞ÊçÆÈõÜËÆ≠ÁªÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14246', 'title': 'Visual Agentic Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.14246', 'abstract': "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.", 'score': 24, 'issue_id': 3873, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '163cdaefde9d9174', 'authors': ['Ziyu Liu', 'Yuhang Zang', 'Yushan Zou', 'Zijian Liang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14246.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#benchmark', '#multimodal', '#agents', '#open_source'], 'emoji': 'ü§ñ', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. Visual-ARFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –≤–µ–±-—Å–∞–π—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–∏—Å–∞—Ç—å –∫–æ–¥ –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ Multi-modal Agentic Tool Bench (MAT) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LVLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Visual-ARFT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ MAT –∏ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.'}, 'en': {'title': 'Empowering Vision-Language Models with Visual-ARFT for Enhanced Reasoning', 'desc': "This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT), a method that enhances Large Vision-Language Models (LVLMs) by enabling them to think with images and use external tools effectively. The research highlights the development of multi-modal agentic capabilities, allowing LVLMs to browse the web for information and manipulate images through coding. The authors present a new evaluation framework called the Multi-modal Agentic Tool Bench (MAT), which assesses the models' abilities in searching and coding tasks. Experimental results show that Visual-ARFT significantly improves performance on various benchmarks, indicating its potential for creating more capable multimodal agents."}, 'zh': {'title': 'ËßÜËßâ‰ª£ÁêÜÂº∫ÂåñÂæÆË∞ÉÔºöÂ§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÊú™Êù•', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑËøõÂ±ïÔºåÁâπÂà´ÊòØÈÄöËøáËßÜËßâ‰ª£ÁêÜÂº∫ÂåñÂæÆË∞ÉÔºàVisual-ARFTÔºâÊäÄÊúØ„ÄÇËØ•ÊäÄÊúØ‰ΩøÂæóLVLMsËÉΩÂ§üÁÅµÊ¥ªÂú∞‰ΩøÁî®Â§ñÈÉ®Â∑•ÂÖ∑ÔºåÂ¶ÇÊµèËßàÂô®Âíå‰ª£Á†ÅÊâßË°åÔºåËøõË°åÂÆûÊó∂‰ø°ÊÅØÊõ¥Êñ∞ÂíåÂõæÂÉèÂ§ÑÁêÜ„ÄÇÁ†îÁ©∂ËøòÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÊ®°ÊÄÅ‰ª£ÁêÜÂ∑•ÂÖ∑Âü∫ÂáÜÔºàMATÔºâÔºåÁî®‰∫éËØÑ‰º∞LVLMsÁöÑÊêúÁ¥¢ÂíåÁºñÁ†ÅËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVisual-ARFTÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫é‰º†ÁªüÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13138', 'title': 'Neurosymbolic Diffusion Models', 'url': 'https://huggingface.co/papers/2505.13138', 'abstract': 'Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration.', 'score': 24, 'issue_id': 3876, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '56639003a63a4eb4', 'authors': ['Emile van Krieken', 'Pasquale Minervini', 'Edoardo Ponti', 'Antonio Vergari'], 'affiliations': ['Miniml.AI', 'School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.13138.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#agents', '#benchmark', '#diffusion', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–ù–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —É—Å–ª–æ–≤–Ω–æ–π –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏', 'desc': '–ù–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (NeSyDMs) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç —É—Å–ª–æ–≤–Ω—É—é –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, NeSyDMs —Å–ø–æ—Å–æ–±–Ω—ã –ª—É—á—à–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —É—á–µ—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏. –ù–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏ –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª, NeSyDMs –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥–∏ –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∫—É.'}, 'en': {'title': 'Enhancing Symbolic Reasoning with Dependency Modeling in Neurosymbolic Predictors', 'desc': 'This paper presents neurosymbolic diffusion models (NeSyDMs), which enhance traditional neurosymbolic predictors by addressing the limitations of assuming conditional independence between symbols. By employing discrete diffusion processes, NeSyDMs effectively model the interactions and dependencies among symbols, leading to improved predictions. This method allows for scalable learning while also quantifying uncertainty, which is crucial for tasks like visual reasoning and autonomous driving. The results show that NeSyDMs outperform existing neurosymbolic approaches in accuracy and calibration on various benchmarks.'}, 'zh': {'title': 'Á™ÅÁ†¥Áã¨Á´ãÂÅáËÆæÔºåÊèêÂçáÁ¨¶Âè∑‰æùËµñÊÄßÂª∫Ê®°', 'desc': 'Á•ûÁªèÁ¨¶Âè∑È¢ÑÊµãÂô®ÔºàNeSyÔºâÁªìÂêà‰∫ÜÁ•ûÁªèÊÑüÁü•ÂíåÁ¨¶Âè∑Êé®ÁêÜÔºåÁî®‰∫éËß£ÂÜ≥ËßÜËßâÊé®ÁêÜÁ≠â‰ªªÂä°„ÄÇ‰º†ÁªüÁöÑNeSyÈ¢ÑÊµãÂô®ÂÅáËÆæÊèêÂèñÁöÑÁ¨¶Âè∑‰πãÈó¥ÊòØÊù°‰ª∂Áã¨Á´ãÁöÑÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âª∫Ê®°‰∫§‰∫íÂíå‰∏çÁ°ÆÂÆöÊÄßÁöÑËÉΩÂäõÔºåÂ∏∏Â∏∏ÂØºËá¥Ëøá‰∫éËá™‰ø°ÁöÑÈ¢ÑÊµãÂíåËæÉÂ∑ÆÁöÑÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Áã¨Á´ãÊÄßÂÅáËÆæÁöÑÂ±ÄÈôêÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁ•ûÁªèÁ¨¶Âè∑Êâ©Êï£Ê®°ÂûãÔºàNeSyDMsÔºâÔºåËøôÊòØ‰∏ÄÁ±ªÊñ∞ÁöÑNeSyÈ¢ÑÊµãÂô®ÔºåÂà©Áî®Á¶ªÊï£Êâ©Êï£Êù•Âª∫Ê®°Á¨¶Âè∑‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Âú®ÂêàÊàêÂíåÁúüÂÆû‰∏ñÁïåÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂåÖÊã¨È´òÁª¥ËßÜËßâË∑ØÂæÑËßÑÂàíÂíåÂü∫‰∫éËßÑÂàôÁöÑËá™Âä®È©æÈ©∂ÔºåÂ±ïÁ§∫‰∫ÜNeSyDMsÂú®NeSyÈ¢ÑÊµãÂô®‰∏≠ÁöÑÊúÄÂÖàËøõÂáÜÁ°ÆÊÄßÂíåÂº∫Â§ßÁöÑÊ†°ÂáÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04388', 'title': 'The Aloe Family Recipe for Open and Specialized Healthcare LLMs', 'url': 'https://huggingface.co/papers/2505.04388', 'abstract': 'Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.', 'score': 19, 'issue_id': 3874, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': '12792ceffb601d5a', 'authors': ['Dario Garcia-Gasulla', 'Jordi Bayarri-Planas', 'Ashwin Kumar Gururajan', 'Enrique Lopez-Cuena', 'Adrian Tormos', 'Daniel Hinjos', 'Pablo Bernabeu-Perez', 'Anna Arias-Duart', 'Pablo Agustin Martin-Torres', 'Marta Gonzalez-Mallo', 'Sergio Alvarez-Napagao', 'Eduard Ayguad√©-Parra', 'Ulises Cort√©s'], 'affiliations': ['Barcelona Supercomputing Center (BSC-CNS), Spain', 'Universitat Polit`ecnica de Catalunya - Barcelona Tech (UPC), Spain'], 'pdf_title_img': 'assets/pdf/title_img/2505.04388.jpg', 'data': {'categories': ['#alignment', '#data', '#healthcare', '#training', '#open_source', '#ethics', '#benchmark', '#rlhf', '#rag'], 'emoji': 'ü©∫', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ò–ò-–º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Aloe Beta - –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Å –ª—É—á—à–∏–º–∏ –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —ç—Ç–∞–ø—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è, —É–ª—É—á—à–∏–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é DPO –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ RAG. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—é—â–∞—è —á–µ—Ç—ã—Ä–µ —Ç–∏–ø–∞ —Ç–µ—Å—Ç–æ–≤, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –ú–æ–¥–µ–ª–∏ Aloe Beta –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∞—Ç–∞–∫–∞–º –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Empowering Healthcare with Open-Source LLMs: Safety Meets Efficacy', 'desc': 'This paper discusses the development of open-source Large Language Models (LLMs) for healthcare, focusing on optimizing data preprocessing and training methods. It introduces Direct Preference Optimization (DPO) to enhance model safety and Retrieval-Augmented Generation (RAG) to improve efficacy. The evaluation methodology includes various tests to establish a new standard for assessing model performance in medical contexts. The resulting Aloe Beta models demonstrate competitive capabilities against private models while adhering to ethical guidelines and safety measures.'}, 'zh': {'title': 'Êé®Âä®ÂºÄÊîæÂåªÁñóÊ®°ÂûãÔºå‰øùÈöúÂÖ¨‰ºóÂà©Áõä', 'desc': 'Êú¨Á†îÁ©∂Êó®Âú®Êé®Âä®ÂºÄÊîæÊ∫ê‰ª£Á†ÅÂåªÁñóÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÔºå‰ª•‰øùÊä§ÂÖ¨‰ºóÂà©Áõä„ÄÇÈÄöËøá‰ºòÂåñÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂíåËÆ≠ÁªÉÁöÑÂÖ≥ÈîÆÈò∂ÊÆµÔºåÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÊèêÈ´òÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÔºå‰ª•ÂèäÈÄöËøáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊèêÂçáÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇËØÑ‰º∞ÊñπÊ≥ïÂåÖÊã¨ÂõõÁßç‰∏çÂêåÁ±ªÂûãÁöÑÊµãËØïÔºå‰∏∫ËØ•È¢ÜÂüüËÆæÂÆö‰∫ÜÊñ∞ÁöÑÊ†áÂáÜ„ÄÇÊúÄÁªàÂèëÂ∏ÉÁöÑÊ®°ÂûãÂú®ÂåªÁñóÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÂú®ÂÆâÂÖ®ÊÄßÂíå‰º¶ÁêÜÊñπÈù¢‰πüÊúâÊòæËëóÊîπÂñÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14513', 'title': 'Latent Flow Transformer', 'url': 'https://huggingface.co/papers/2505.14513', 'abstract': 'Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.', 'score': 18, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '3683bab427c47086', 'authors': ['Yen-Chen Wu', 'Feng-Ting Liao', 'Meng-Hsi Chen', 'Pei-Chen Ho', 'Farhang Nabiei', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.14513.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training'], 'emoji': 'üåä', 'ru': {'title': '–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –≤–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–µ–≤: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Latent Flow Transformer (LFT), –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. LFT –∑–∞–º–µ–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–µ–≤ –æ–¥–Ω–∏–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º –ø–µ—Ä–µ–Ω–æ—Å–∞, –æ–±—É—á–µ–Ω–Ω—ã–º —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Flow Walking –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª–∏ Pythia-410M –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∞—Ç—å –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–ª–∏ –¥–∞–∂–µ —É–ª—É—á—à–∞—è –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Efficient Layer Compression with Latent Flow Transformers', 'desc': 'This paper introduces the Latent Flow Transformer (LFT), a new architecture for large language models that replaces multiple discrete layers with a single learned transport operator. By utilizing flow matching, LFT achieves significant model compression while still being compatible with traditional transformer designs. The authors also present the Flow Walking (FW) algorithm to enhance the coupling preservation in flow-based methods. Experimental results show that LFT can effectively reduce the number of layers while improving performance metrics, bridging the gap between autoregressive and flow-based generation techniques.'}, 'zh': {'title': 'ÊΩúÂú®ÊµÅÂèòÊç¢Âô®ÔºöÈ´òÊïàÂéãÁº©Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ®°Âûã‚Äî‚ÄîÊΩúÂú®ÊµÅÂèòÊç¢Âô®ÔºàLatent Flow Transformer, LFTÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïàÁéá„ÄÇLFTÈÄöËøá‰ΩøÁî®Â≠¶‰π†ÁöÑ‰º†ËæìÁÆóÂ≠êÊõø‰ª£Â§ö‰∏™Á¶ªÊï£Â±ÇÔºå‰ªéËÄåÂÆûÁé∞ÊòæËëóÁöÑÂéãÁº©ÔºåÂêåÊó∂‰øùÊåÅ‰∏éÂéüÂßãÊû∂ÊûÑÁöÑÂÖºÂÆπÊÄß„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÊµÅÊ≠•Ë°åÔºàFlow Walking, FWÔºâÁÆóÊ≥ïÔºå‰ª•Ëß£ÂÜ≥Áé∞ÊúâÊµÅÂü∫ÊñπÊ≥ïÂú®‰øùÊåÅËÄ¶ÂêàÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLFTÂú®ÂéãÁº©Â±ÇÊï∞ÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰º†ÁªüÁöÑÂ±ÇË∑≥ËøáÊñπÊ≥ïÔºåÁº©Â∞èËá™ÂõûÂΩíÂíåÊµÅÁîüÊàêËåÉÂºè‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14674', 'title': 'Reward Reasoning Model', 'url': 'https://huggingface.co/papers/2505.14674', 'abstract': 'Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.', 'score': 14, 'issue_id': 3871, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'b51747905eeda5db', 'authors': ['Jiaxin Guo', 'Zewen Chi', 'Li Dong', 'Qingxiu Dong', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14674.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#reasoning', '#rl', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è: –Ω–æ–≤—ã–π —à–∞–≥ –∫ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Reward Reasoning Models (RRMs). RRMs –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ú–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é reinforcement learning –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RRMs –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∏ –º–æ–≥—É—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Reward Models with Adaptive Reasoning', 'desc': 'This paper introduces Reward Reasoning Models (RRMs), which enhance the performance of reward models in large language models by incorporating a structured reasoning process. RRMs utilize additional computational resources during testing to tackle complex queries where the correct rewards are not obvious. The authors employ a reinforcement learning framework that allows these models to develop their reasoning capabilities autonomously, without needing specific reasoning examples in the training data. Experimental results indicate that RRMs outperform existing reward modeling methods across various benchmarks, demonstrating their ability to adaptively use test-time compute for improved reward accuracy.'}, 'zh': {'title': 'ÊèêÂçáÂ•ñÂä±Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Â•ñÂä±Ê®°ÂûãÂú®ÂºïÂØºÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁ¨¶Âêà‰∫∫Á±ªÊúüÊúõÁöÑËæìÂá∫‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÁÑ∂ËÄåÔºåÂú®ÊúâÊïàÂà©Áî®ÊµãËØïÊó∂ËÆ°ÁÆó‰ª•ÊèêÂçáÂ•ñÂä±Ê®°ÂûãÊÄßËÉΩÊñπÈù¢‰ªçÁÑ∂Â≠òÂú®ÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ•ñÂä±Êé®ÁêÜÊ®°ÂûãÔºàRRMsÔºâÔºåÂÆÉ‰ª¨‰∏ìÈó®ËÆæËÆ°Áî®‰∫éÂú®ÁîüÊàêÊúÄÁªàÂ•ñÂä±‰πãÂâçÊâßË°åÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRRMsÂú®ÂêÑ‰∏™È¢ÜÂüüÁöÑÂ•ñÂä±Âª∫Ê®°Âü∫ÂáÜ‰∏äË°®Áé∞‰ºòË∂äÔºåÂπ∂ËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Âà©Áî®ÊµãËØïÊó∂ËÆ°ÁÆóËøõ‰∏ÄÊ≠•ÊèêÈ´òÂ•ñÂä±ÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14489', 'title': 'Reasoning Models Better Express Their Confidence', 'url': 'https://huggingface.co/papers/2505.14489', 'abstract': 'Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.', 'score': 13, 'issue_id': 3871, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'c945246738ceba22', 'authors': ['Dongkeun Yoon', 'Seungone Kim', 'Sohee Yang', 'Sunkyoung Kim', 'Soyeon Kim', 'Yongil Kim', 'Eunbi Choi', 'Yireun Kim', 'Minjoon Seo'], 'affiliations': ['CMU', 'KAIST', 'LG AI Research', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.14489.jpg', 'data': {'categories': ['#training', '#reasoning', '#interpretability', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ú–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É –ò–ò', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (reasoning models) –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ª—É—á—à—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º–∏ LLM. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç '–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è' - –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –£–¥–∞–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ '–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è' –∏–∑ CoT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏."}, 'en': {'title': 'Boosting Confidence Calibration in Language Models through Reasoning', 'desc': 'This paper explores how large language models (LLMs) can improve their confidence calibration through reasoning techniques. It shows that LLMs that use chain-of-thought (CoT) reasoning not only solve problems better but also express their confidence more accurately. The study benchmarks six reasoning models and finds that they outperform non-reasoning models in confidence calibration across most scenarios. The authors conclude that the slow thinking behaviors inherent in reasoning models allow them to dynamically adjust their confidence, leading to better performance as the reasoning process unfolds.'}, 'zh': {'title': 'Êé®ÁêÜÊ®°ÂûãÊèêÂçáËá™‰ø°Â∫¶Ê†°ÂáÜÁöÑÁßòÂØÜ', 'desc': 'Â∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂÖ∑ÊúâÂæàÂº∫ÁöÑËÉΩÂäõÔºå‰ΩÜÂÆÉ‰ª¨Âú®Ë°®ËææËá™‰ø°Â∫¶ÊñπÈù¢Â∏∏Â∏∏‰∏çÂáÜÁ°ÆÔºåËøô‰ΩøÂæóËØÑ‰º∞ÂÖ∂ÈîôËØØÁöÑÂèØËÉΩÊÄßÂèòÂæóÂõ∞ÈöæÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂèØÈù†ÊÄß„ÄÇÊú¨ÊñáÂ±ïÁ§∫‰∫ÜÊé®ÁêÜÊ®°ÂûãÔºåÂç≥ËøõË°åÊâ©Â±ïÊÄùÁª¥ÈìæÔºàCoTÔºâÊé®ÁêÜÁöÑLLMsÔºå‰∏ç‰ªÖÂú®Ëß£ÂÜ≥ÈóÆÈ¢òÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåËÄå‰∏îÂú®ÂáÜÁ°ÆË°®ËææËá™‰ø°Â∫¶ÊñπÈù¢‰πüË°®Áé∞Êõ¥‰Ω≥„ÄÇÊàë‰ª¨ÂØπÂÖ≠‰∏™Êé®ÁêÜÊ®°ÂûãÂú®ÂÖ≠‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØïÔºåÂèëÁé∞ÂÆÉ‰ª¨Âú®36ÁßçËÆæÁΩÆ‰∏≠Êúâ33ÁßçÊÉÖÂÜµ‰∏ãÁöÑËá™‰ø°Â∫¶Ê†°ÂáÜÊòéÊòæ‰ºò‰∫éÈùûÊé®ÁêÜÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåËøôÁßçÊ†°ÂáÜÁöÑÊèêÂçáÊ∫ê‰∫éÊé®ÁêÜÊ®°ÂûãÁöÑÊÖ¢ÊÄùÁª¥Ë°å‰∏∫ÔºåÂ¶ÇÊé¢Á¥¢Êõø‰ª£ÊñπÊ≥ïÂíåÂõûÊ∫ØÔºå‰ΩøÂÆÉ‰ª¨ËÉΩÂ§üÂú®ÊÄùÁª¥Èìæ‰∏≠Âä®ÊÄÅË∞ÉÊï¥Ëá™‰ø°Â∫¶Ôºå‰ªéËÄåÈÄêÊ≠•ÊèêÈ´òÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14652', 'title': 'General-Reasoner: Advancing LLM Reasoning Across All Domains', 'url': 'https://huggingface.co/papers/2505.14652', 'abstract': 'Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.', 'score': 12, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '494fe90709dc6c63', 'authors': ['Xueguang Ma', 'Qian Liu', 'Dongfu Jiang', 'Ge Zhang', 'Zejun Ma', 'Wenhu Chen'], 'affiliations': ['M-A-P', 'Singapore', 'TikTok', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.14652.jpg', 'data': {'categories': ['#dataset', '#rl', '#benchmark', '#math', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∞—Å—Å—É–∂–¥–∞—Ç–µ–ª—å: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM –≤ –º–Ω–æ–≥–æ–¥–æ–º–µ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ General-Reasoner, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω. –û–Ω–∏ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—É—é –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ General-Reasoner –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö.'}, 'en': {'title': 'Empowering LLMs with General-Reasoner for Diverse Reasoning', 'desc': "This paper introduces General-Reasoner, a new approach to improve the reasoning abilities of large language models (LLMs) across various fields. It leverages a large-scale dataset of questions with verifiable answers, which is created through web crawling, to train LLMs without the need for prior supervised fine-tuning. Additionally, it employs a generative model-based answer verifier that enhances the model's ability to understand context and think through problems. The results show that General-Reasoner significantly outperforms existing methods in reasoning tasks, especially in mathematics, while also being effective in other domains like physics and finance."}, 'zh': {'title': 'ÊèêÂçáLLMÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ËåÉÂºè', 'desc': 'Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊΩúÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉËåÉÂºè‚Äî‚ÄîGeneral-ReasonerÔºåÊó®Âú®Â¢ûÂº∫LLMÂú®Â§öÈ¢ÜÂüüÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°„ÄÅÈ´òË¥®ÈáèÁöÑÈóÆÈ¢òÊï∞ÊçÆÈõÜÔºåÂπ∂ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁîüÊàêÊ®°ÂûãÁöÑÁ≠îÊ°àÈ™åËØÅÂô®ÔºåÂèñ‰ª£‰∫Ü‰º†ÁªüÁöÑÂü∫‰∫éËßÑÂàôÁöÑÈ™åËØÅÊñπÊ≥ï„ÄÇÈÄöËøáÂú®Â§ö‰∏™È¢ÜÂüüÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åËØÑ‰º∞ÔºåGeneral-ReasonerÂú®Êé®ÁêÜÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13866', 'title': 'Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.13866', 'abstract': 'Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60times compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.', 'score': 12, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '72f6460e348e135a', 'authors': ['Jiwon Song', 'Dongwon Jo', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Seoul National University', 'Sungkyunkwan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13866.jpg', 'data': {'categories': ['#inference', '#optimization', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∂–∞—Ç–∏–µ –ø—É—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –°–∂–∞—Ç–∏—è –ü—É—Ç–∏ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (RPC) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π —è–∑—ã–∫–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. RPC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –ø—É—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–∂–∏–º–∞—è KV-–∫—ç—à –ø—É—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RPC —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ QwQ-32B –¥–æ 1.60 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –≤—ã–≤–æ–¥–æ–º —Å –ø–æ–ª–Ω—ã–º KV-–∫—ç—à–µ–º. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–ª–µ–¥–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è, –ø—Ä–µ–¥–ª–∞–≥–∞—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Efficient Inference with Reasoning Path Compression', 'desc': 'This paper introduces Reasoning Path Compression (RPC), a method designed to enhance the efficiency of reasoning-focused language models during inference. By utilizing the concept of semantic sparsity, RPC compresses the key-value (KV) cache, retaining only the most important elements based on recent queries. This approach significantly increases the throughput of token generation while only slightly affecting accuracy. The results indicate that RPC can improve the performance of large models like QwQ-32B, making them more practical for real-world applications.'}, 'zh': {'title': 'Êé®ÁêÜË∑ØÂæÑÂéãÁº©ÔºöÈ´òÊïàÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøë‰∏ìÊ≥®‰∫éÊé®ÁêÜÁöÑËØ≠Ë®ÄÊ®°ÂûãÈÄöËøáÁîüÊàêËæÉÈïøÁöÑ‰∏≠Èó¥Êé®ÁêÜË∑ØÂæÑÊù•ÂÆûÁé∞È´òÂáÜÁ°ÆÁéá„ÄÇËøôÁßçÊñπÊ≥ïÂú®Ëß£ÂÜ≥ÈúÄË¶ÅÈÄªËæëÊÄùÁª¥ÁöÑÈóÆÈ¢òÊó∂ÈùûÂ∏∏ÊúâÊïàÔºå‰ΩÜÈïøÊé®ÁêÜË∑ØÂæÑÊòæËëóÂ¢ûÂä†‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®Âíå‰ª§ÁâåÁîüÊàêÁöÑÂêûÂêêÈáèÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Êé®ÁêÜË∑ØÂæÑÂéãÁº©ÔºàRPCÔºâÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂà©Áî®Êé®ÁêÜË∑ØÂæÑÁöÑËØ≠‰πâÁ®ÄÁñèÊÄßÊù•Âä†ÈÄüÊé®ÁêÜ„ÄÇÂÆûÈ™åË°®ÊòéÔºåRPCÂú®AIME 2024Âü∫ÂáÜÊµãËØï‰∏≠Áõ∏ÊØî‰∫éÂÆåÊï¥KVÁºìÂ≠òÔºåÊèêÂçá‰∫ÜQwQ-32BÁöÑÁîüÊàêÂêûÂêêÈáèÔºåÂáÜÁ°ÆÁéá‰ªÖ‰∏ãÈôç1.2%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13547', 'title': 'Exploring Federated Pruning for Large Language Models', 'url': 'https://huggingface.co/papers/2505.13547', 'abstract': 'LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.', 'score': 12, 'issue_id': 3872, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '436f0f2e8c3f8481', 'authors': ['Pengxin Guo', 'Yinong Wang', 'Wei Li', 'Mengting Liu', 'Ming Li', 'Jinkai Zheng', 'Liangqiong Qu'], 'affiliations': ['Guangming Laboratory', 'Hangzhou Dianzi University', 'Southern University of Science and Technology', 'Sun Yat-sen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13547.jpg', 'data': {'categories': ['#inference', '#security', '#optimization', '#open_source', '#training'], 'emoji': 'üîí', 'ru': {'title': '–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö', 'desc': 'FedPrLLM - —ç—Ç–æ –Ω–æ–≤–∞—è —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –≤—ã—á–∏—Å–ª—è—Ç—å –º–∞—Å–∫–∏ –ø—Ä—É–Ω–∏–Ω–≥–∞ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–µ–ª–∏—Ç—å—Å—è –∏–º–∏ —Å —Å–µ—Ä–≤–µ—Ä–æ–º –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π –ø—Ä—É–Ω–∏–Ω–≥ —Å –ø–æ—Å–¥–æ–π–Ω—ã–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤ –¥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –≤ —Å–∂–∞—Ç–∏–∏ LLM –¥–ª—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.'}, 'en': {'title': 'Privacy-Preserving Compression of LLMs with FedPrLLM', 'desc': 'This paper presents FedPrLLM, a federated pruning framework aimed at compressing large language models (LLMs) while ensuring data privacy. Unlike traditional methods that require public calibration samples, FedPrLLM allows clients to generate pruning masks using their local data, which are then shared with a central server. This collaborative approach enables the global model to be pruned without exposing sensitive local data. The authors conducted experiments to identify the best pruning strategies, concluding that one-shot pruning with layer comparison and no weight scaling is the most effective method within their framework.'}, 'zh': {'title': 'ÈöêÁßÅ‰øùÊä§‰∏ãÁöÑLLMÂâ™ÊûùÊñ∞ÊñπÊ≥ï', 'desc': 'LLMÂâ™ÊûùÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊäÄÊúØÔºåÂèØ‰ª•ÂéãÁº©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®ËµÑÊ∫êÊúâÈôêÁöÑËÆæÂ§á‰∏äËøêË°å„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂÖ¨ÂÖ±Ê†°ÂáÜÊ†∑Êú¨ÔºåËøôÂú®ÈöêÁßÅÊïèÊÑüÁöÑÈ¢ÜÂüü‰∏≠ÂæàÈöæËé∑Âæó„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFedPrLLMÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËÅîÈÇ¶Ââ™ÊûùÊ°ÜÊû∂ÔºåÊó®Âú®‰øùÊä§ÈöêÁßÅÁöÑÂêåÊó∂ÂéãÁº©LLM„ÄÇÂú®FedPrLLM‰∏≠ÔºåÊØè‰∏™ÂÆ¢Êà∑Á´ØÂè™ÈúÄÊ†πÊçÆÊú¨Âú∞Ê†°ÂáÜÊï∞ÊçÆËÆ°ÁÆóÂâ™ÊûùÊé©Á†ÅÁü©ÈòµÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÊúçÂä°Âô®ÂÖ±‰∫´Ôºå‰ªéËÄåÂØπÂÖ®Â±ÄÊ®°ÂûãËøõË°åÂâ™Êûù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14677', 'title': 'Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.14677', 'abstract': 'Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks.', 'score': 11, 'issue_id': 3873, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '032b4d528d6984fd', 'authors': ['Jiaer Xia', 'Yuhang Zang', 'Peng Gao', 'Yixuan Li', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University', 'Shanghai AI Lab', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.14677.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#cv', '#multimodal', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —è–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ü–µ–ø–æ—á–∫–µ –º—ã—Å–ª–µ–π', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø–∞—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–æ—è–≤–ª–µ–Ω–∏—é —É –º–æ–¥–µ–ª–∏ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö 'shortcuts'. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ '–ø–æ–¥–ø–∏—Å—å-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ-–æ—Ç–≤–µ—Ç', –∫–æ—Ç–æ—Ä—ã–π –ø–æ–±—É–∂–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Visionary-R1 –ø—Ä–µ–≤–∑–æ—à–ª–∞ –¥—Ä—É–≥–∏–µ —Å–∏–ª—å–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."}, 'en': {'title': 'Enhancing Visual Reasoning with Captions First!', 'desc': 'This paper addresses the challenge of enhancing reasoning capabilities in visual language models (VLMs) using reinforcement learning. The authors propose a novel training approach that emphasizes generating detailed captions for images before reasoning, which helps prevent shortcut learning. By training their model, Visionary-R1, on a large dataset of visual question-answer pairs without explicit chain-of-thought supervision, they achieve superior performance compared to existing multimodal models. The results suggest that focusing on image interpretation prior to reasoning can significantly improve generalization across diverse data distributions.'}, 'zh': {'title': 'ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÊù•ËøõË°åÂõæÂÉèÊï∞ÊçÆÁöÑÊé®ÁêÜ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÊòæÂºèÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÁõëÁù£ÔºåËÄåÊòØÈÄöËøáËßÜËßâÈóÆÁ≠îÂØπÊù•ÂÆûÁé∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁÆÄÂçïÂú∞Â∫îÁî®Âº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•ÂØºËá¥Ê®°ÂûãÂú®ÂõûÁ≠î‰πãÂâçÁîüÊàêÊé®ÁêÜÈìæÔºå‰ΩÜËøôÂèØËÉΩÂØºËá¥Ê®°ÂûãÂú®Èù¢ÂØπÊñ∞Êï∞ÊçÆÊó∂ÁöÑÊ≥õÂåñËÉΩÂäõ‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫ËÆ©Ê®°ÂûãÂú®Êé®ÁêÜ‰πãÂâçÂÖàÂØπÂõæÂÉèËøõË°åËß£ÈáäÔºå‰ªéËÄåÊèêÈ´òÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14673', 'title': 'Training-Free Watermarking for Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2505.14673', 'abstract': 'Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.', 'score': 11, 'issue_id': 3873, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'ec739be428657981', 'authors': ['Yu Tong', 'Zihao Pan', 'Shuai Yang', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University', 'Peking University', 'Sun Yat-sen University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14673.jpg', 'data': {'categories': ['#cv', '#data', '#training', '#security'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ù–µ–∑–∞–º–µ—Ç–Ω–∞—è –∑–∞—â–∏—Ç–∞ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏', 'desc': 'IndexMark - —ç—Ç–æ –±–µ–∑—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –º–µ—Ç–æ–¥ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –≤ –∞–≤—Ç–æ—Ä–µ–≥–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ–¥–æ–≤–æ–π –∫–Ω–∏–≥–∏, –∑–∞–º–µ–Ω—è—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –ø–æ—Ö–æ–∂–∏–µ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –≤–æ–¥—è–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –±–µ–∑ —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–¥—è–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –ø—É—Ç–µ–º –ø–æ–¥—Å—á–µ—Ç–∞ –¥–æ–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ–¥—è–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ IndexMark –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º.'}, 'en': {'title': 'IndexMark: Watermarking Autoregressive Models with Precision and Robustness', 'desc': 'This paper presents IndexMark, a novel watermarking framework specifically designed for autoregressive image generation models. Unlike previous methods focused on diffusion models, IndexMark utilizes the redundancy in the codebook to embed watermarks without compromising image quality. The framework employs a match-then-replace strategy to select and replace indices with similar ones, effectively embedding the watermark. Additionally, it includes a robust verification process and an auxiliary validation scheme to withstand various image perturbations, demonstrating superior performance in both image quality and watermark verification accuracy.'}, 'zh': {'title': 'Ëá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÁöÑÈöêÂΩ¢Ê∞¥Âç∞Êñ∞ÊñπÊ°à', 'desc': 'ÈöêÂΩ¢ÂõæÂÉèÊ∞¥Âç∞ÊäÄÊúØÂèØ‰ª•‰øùÊä§ÂõæÂÉèÁöÑÊâÄÊúâÊùÉÔºåÂπ∂Èò≤Ê≠¢ËßÜËßâÁîüÊàêÊ®°ÂûãÁöÑÊÅ∂ÊÑèÊª•Áî®„ÄÇÁé∞ÊúâÁöÑÁîüÊàêÊ∞¥Âç∞ÊñπÊ≥ï‰∏ªË¶ÅÈíàÂØπÊâ©Êï£Ê®°ÂûãÔºåËÄåËá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÊ∞¥Âç∞ÊäÄÊúØÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜIndexMarkÔºåËøôÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑËá™ÂõûÂΩíÂõæÂÉèÁîüÊàêÊ®°ÂûãÊ∞¥Âç∞Ê°ÜÊû∂„ÄÇIndexMarkÈÄöËøáÂåπÈÖçÂíåÊõøÊç¢ÁöÑÊñπÊ≥ïÔºåÂà©Áî®‰ª£Á†ÅÊú¨ÁöÑÂÜó‰ΩôÁâπÊÄßÔºåÂµåÂÖ•Ê∞¥Âç∞ËÄå‰∏çÂΩ±ÂìçÂõæÂÉèË¥®ÈáèÔºåÂπ∂Âú®Â§öÁßçÂπ≤Êâ∞‰∏ãË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14631', 'title': 'Think Only When You Need with Large Hybrid-Reasoning Models', 'url': 'https://huggingface.co/papers/2505.14631', 'abstract': "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.", 'score': 11, 'issue_id': 3872, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '12732abf8e9d807f', 'authors': ['Lingjie Jiang', 'Xun Wu', 'Shaohan Huang', 'Qingxiu Dong', 'Zewen Chi', 'Li Dong', 'Xingxing Zhang', 'Tengchao Lv', 'Lei Cui', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14631.jpg', 'data': {'categories': ['#architecture', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ì–∏–±—Ä–∏–¥–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Large Hybrid-Reasoning Models (LHRMs), —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–∏–π Hybrid Fine-Tuning (HFT) –∏ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Hybrid Group Policy Optimization (HGPO). –í–≤–µ–¥–µ–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ Hybrid Accuracy –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ –≥–∏–±—Ä–∏–¥–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LHRMs –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ LRMs –∏ LLMs –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –∏ –æ–±—â–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Adaptive Thinking for Efficient Reasoning', 'desc': 'This paper presents Large Hybrid-Reasoning Models (LHRMs), which enhance reasoning abilities by deciding when to engage in extended thinking based on the complexity of user queries. Unlike traditional Large Language Models (LLMs), LHRMs use a two-stage training approach that includes Hybrid Fine-Tuning and online reinforcement learning to optimize their reasoning process. The authors introduce a new metric, Hybrid Accuracy, to evaluate the effectiveness of these models in adapting their thinking strategies. Experimental results demonstrate that LHRMs outperform existing models in both reasoning and efficiency, suggesting a new direction for developing intelligent systems that balance thinking depth with response speed.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊ∑∑ÂêàÊé®ÁêÜÔºåÊèêÂçáÊïàÁéá‰∏éËÉΩÂäõ', 'desc': 'ÊúÄËøëÁöÑÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®ÁîüÊàêÊúÄÁªàÂõûÁ≠î‰πãÂâçËøõË°å‰∫ÜÊõ¥Ê∑±ÂÖ•ÁöÑÊÄùËÄÉ„ÄÇÁÑ∂ËÄåÔºåËøáÈïøÁöÑÊÄùËÄÉËøáÁ®ã‰ºöÂØºËá¥‰ª§ÁâåÊ∂àËÄóÂíåÂª∂ËøüÁöÑÊòæËëóÂ¢ûÂä†ÔºåËøôÂú®Â§ÑÁêÜÁÆÄÂçïÊü•ËØ¢Êó∂Â∞§ÂÖ∂‰∏çÂøÖË¶Å„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ§ßÂûãÊ∑∑ÂêàÊé®ÁêÜÊ®°ÂûãÔºàLHRMsÔºâÔºåËøôÁßçÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆÁî®Êà∑Êü•ËØ¢ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØËá™ÈÄÇÂ∫îÂú∞ÂÜ≥ÂÆöÊòØÂê¶ËøõË°åÊÄùËÄÉ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLHRMsÂú®Â§ÑÁêÜ‰∏çÂêåÈöæÂ∫¶ÂíåÁ±ªÂûãÁöÑÊü•ËØ¢Êó∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÊ∑∑ÂêàÊÄùËÄÉÔºåÂπ∂Âú®Êé®ÁêÜÂíåÊï¥‰ΩìËÉΩÂäõ‰∏äË∂ÖË∂äÁé∞ÊúâÁöÑLRMsÂíåLLMsÔºåÂêåÊó∂ÊòæËëóÊèêÈ´òÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14640', 'title': 'VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation', 'url': 'https://huggingface.co/papers/2505.14640', 'abstract': "Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (>25\\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.", 'score': 10, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '45d64d535935c6a4', 'authors': ['Wentao Ma', 'Weiming Ren', 'Yiming Jia', 'Zhuofeng Li', 'Ping Nie', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['Independent', 'M-A-P', 'Shanghai University', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.14640.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#long_context', '#reasoning'], 'emoji': 'üé•', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ: –æ—Ç —É–≥–∞–¥—ã–≤–∞–Ω–∏—è –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–æ–ª—å—à–∏–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LMM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–∞–≤—ã—à–µ–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∏–∑-–∑–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VideoEval-Pro —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Å–µ–≥–æ –≤–∏–¥–µ–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ, —á–µ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, —á—Ç–æ –¥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LMM –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Revolutionizing Long Video Understanding with Realistic Benchmarks', 'desc': 'This paper discusses the limitations of current benchmarks for evaluating long video understanding (LVU) in large multimodal models (LMMs). It highlights that many existing benchmarks rely on multiple-choice questions (MCQs), which can inflate performance scores due to guessing and prior knowledge. The authors introduce VideoEval-Pro, a new benchmark that uses open-ended questions requiring comprehensive video understanding, thus providing a more accurate assessment of LMM capabilities. Their findings reveal significant performance drops for LMMs on open-ended questions compared to MCQs, indicating that current benchmarks may not effectively measure true understanding of long videos.'}, 'zh': {'title': 'VideoEval-ProÔºöÊèêÂçáÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÁúüÂÆûËØÑ‰º∞', 'desc': 'Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®ÈïøËßÜÈ¢ëÁêÜËß£ÔºàLVUÔºâ‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁé∞ÊúâÁöÑLVUÂü∫ÂáÜÊµãËØïÂ≠òÂú®ÈóÆÈ¢ò„ÄÇËÆ∏Â§öÂü∫ÂáÜ‰æùËµñÂ§öÈ°πÈÄâÊã©È¢òÔºàMCQsÔºâÔºåËøôÂØºËá¥ËØÑ‰º∞ÁªìÊûúË¢´Â§∏Â§ßÔºåÂõ†‰∏∫Ê®°ÂûãÂèØËÉΩÈÄöËøáÁåúÊµãËé∑ÂæóÊ≠£Á°ÆÁ≠îÊ°à„ÄÇÊ≠§Â§ñÔºåÈÉ®ÂàÜÈóÆÈ¢òÁöÑÂÖàÈ™å‰ø°ÊÅØ‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•Âú®‰∏çËßÇÁúãËßÜÈ¢ëÁöÑÊÉÖÂÜµ‰∏ãÁõ¥Êé•ÂõûÁ≠î„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVideoEval-ProÂü∫ÂáÜÔºåÈááÁî®ÂºÄÊîæÂºèÁü≠Á≠îÊ°àÈóÆÈ¢òÔºåÁúüÊ≠£ËÄÉÂØüÊ®°ÂûãÂØπÊï¥‰∏™ËßÜÈ¢ëÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13559', 'title': 'CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models', 'url': 'https://huggingface.co/papers/2505.13559', 'abstract': 'Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.', 'score': 9, 'issue_id': 3870, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '9ea21df740810e7e', 'authors': ['Sathya Krishnan Suresh', 'Tanmay Surana', 'Lim Zhi Hao', 'Eng Siong Chng'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2505.13559.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#low_resource', '#synthetic', '#training', '#benchmark', '#multilingual'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'CS-Sum: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –∫–æ–¥–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CS-Sum –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –∫–æ–¥–æ–≤ (code-switching) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –∫–æ–¥–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –ø–∞—Ä –∫–∏—Ç–∞–π—Å–∫–∏–π-–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ç–∞–º–∏–ª—å—Å–∫–∏–π-–∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ –º–∞–ª–∞–π—Å–∫–∏–π-–∞–Ω–≥–ª–∏–π—Å–∫–∏–π. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ—Å—è—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –≤–∫–ª—é—á–∞—è few-shot –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –¥–æ–ø—É—Å–∫–∞—é—Ç —Ç–æ–Ω–∫–∏–µ –æ—à–∏–±–∫–∏, –∏—Å–∫–∞–∂–∞—é—â–∏–µ —Å–º—ã—Å–ª –¥–∏–∞–ª–æ–≥–∞, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Å–æ–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫.'}, 'en': {'title': 'Enhancing LLMs for Code-Switching Comprehensibility', 'desc': 'This paper addresses the challenges that Large Language Models (LLMs) face when dealing with code-switching (CS) in dialogues. It introduces CS-Sum, a benchmark designed to assess how well LLMs can summarize CS dialogues into English, focusing on three language pairs: Mandarin-English, Tamil-English, and Malay-English. The study evaluates ten different LLMs using various methods, including few-shot learning and fine-tuning techniques, to understand their performance on CS data. The results reveal that while LLMs achieve high scores on automated metrics, they often make subtle errors that can change the meaning of the dialogues, highlighting the need for improved training on code-switched content.'}, 'zh': {'title': 'ËØÑ‰º∞‰ª£Á†ÅÂàáÊç¢ÁöÑÂèØÁêÜËß£ÊÄß', 'desc': '‰ª£Á†ÅÂàáÊç¢ÔºàCSÔºâÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊûÑÊàê‰∫ÜÈáçÂ§ßÊåëÊàòÔºå‰ΩÜÂÖ∂ÂèØÁêÜËß£ÊÄßÂú®LLMs‰∏≠ÁöÑÁ†îÁ©∂‰ªçÁÑ∂‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCS-SumÔºåÊó®Âú®ÈÄöËøáÂ∞ÜCSÂØπËØùÊÄªÁªì‰∏∫Ëã±ËØ≠Êù•ËØÑ‰º∞LLMsÂØπCSÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇCS-SumÊòØÈ¶ñ‰∏™ÈíàÂØπÊôÆÈÄöËØù-Ëã±ËØ≠„ÄÅÊ≥∞Á±≥Â∞îËØ≠-Ëã±ËØ≠ÂíåÈ©¨Êù•ËØ≠-Ëã±ËØ≠ÁöÑCSÂØπËØùÊÄªÁªìÂü∫ÂáÜÔºåÂåÖÂê´ÊØèÂØπËØ≠Ë®Ä900Âà∞1300‰∏™‰∫∫Â∑•Ê†áÊ≥®ÁöÑÂØπËØù„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Ëá™Âä®ËØÑ‰º∞ÊåáÊ†áÂæóÂàÜËæÉÈ´òÔºåLLMsÂú®Â§ÑÁêÜCSËæìÂÖ•Êó∂‰ªç‰ºöÂá∫Áé∞ÁªÜÂæÆÈîôËØØÔºåËøô‰∫õÈîôËØØ‰ºöÊîπÂèòÂØπËØùÁöÑÂÆåÊï¥Âê´‰πâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14680', 'title': 'NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search', 'url': 'https://huggingface.co/papers/2505.14680', 'abstract': "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.", 'score': 8, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'ace242db16327202', 'authors': ['Sunhao Dai', 'Wenjie Wang', 'Liang Pang', 'Jun Xu', 'See-Kiong Ng', 'Ji-Rong Wen', 'Tat-Seng Chua'], 'affiliations': ['CAS Key Laboratory of AI Safety Institute of Computing Technology Chinese Academy of Sciences', 'Gaoling School of Artificial Intelligence Renmin University of China', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.14680.jpg', 'data': {'categories': ['#interpretability', '#rag', '#rlhf', '#agents', '#alignment'], 'emoji': 'üîç', 'ru': {'title': '–í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –ò–ò-–ø–æ–∏—Å–∫', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é NExT-Search, –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò-–ø–æ–∏—Å–∫–∞. –û–Ω–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–æ–∏—Å–∫–∞, —á—Ç–æ –±—ã–ª–æ —É—Ç—Ä–∞—á–µ–Ω–æ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –ò–ò. NExT-Search –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞: —Ä–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –∏ —Ä–µ–∂–∏–º —Ç–µ–Ω–µ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ —Å–æ–±–∏—Ä–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –ø–æ–∏—Å–∫–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç—É –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è –æ–Ω–ª–∞–π–Ω-–∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∏ –æ—Ñ–ª–∞–π–Ω-–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'NExT-Search: Enhancing Generative AI Search with User Feedback', 'desc': 'This paper discusses the challenges of integrating user feedback into generative AI search systems, which provide direct answers to complex queries but lack detailed feedback mechanisms. Traditional web search benefits from fine-grained user interactions, allowing for continuous improvement of ranking models. The proposed NExT-Search framework aims to bridge this gap by introducing two modes of user feedback: User Debug Mode for active user engagement and Shadow User Mode for passive feedback collection. By leveraging both real-time and aggregated feedback, NExT-Search seeks to enhance the generative AI search process and ensure it evolves in response to user needs.'}, 'zh': {'title': 'NExT-SearchÔºöÈáçÂ°ëÁîüÊàêÂºèÊêúÁ¥¢ÁöÑÂèçÈ¶àÂæ™ÁéØ', 'desc': 'ÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÊêúÁ¥¢Ê≠£Âú®ÊîπÂèò‰ø°ÊÅØÊ£ÄÁ¥¢ÔºåÈÄöËøáÊèê‰æõÁ´ØÂà∞Á´ØÁöÑÁ≠îÊ°àÊù•Â∫îÂØπÂ§çÊùÇÊü•ËØ¢ÔºåÂáèÂ∞ëÁî®Êà∑ÊâãÂä®ÊµèËßàÂíåÊÄªÁªìÂ§ö‰∏™ÁΩëÈ°µÁöÑ‰æùËµñ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñ∞Ê®°ÂºèËôΩÁÑ∂ÊèêÈ´ò‰∫Ü‰æøÂà©ÊÄßÔºåÂç¥ÊâìÁ†¥‰∫Ü‰º†ÁªüÁΩëÈ°µÊêúÁ¥¢‰∏≠Âü∫‰∫éÂèçÈ¶àÁöÑÊîπËøõÂæ™ÁéØ„ÄÇ‰º†ÁªüÊêúÁ¥¢ÂèØ‰ª•ÈÄöËøáÊî∂ÈõÜÁî®Êà∑ÂèçÈ¶àÔºàÂ¶ÇÁÇπÂáªÁéáÂíåÂÅúÁïôÊó∂Èó¥ÔºâÊù•‰∏çÊñ≠ÊîπËøõÊéíÂêçÊ®°ÂûãÔºåËÄåÁîüÊàêÂºèÊêúÁ¥¢ÂàôÈù¢‰∏¥ÂèçÈ¶àÂæ™ÁéØÊñ≠Ë£ÇÁöÑÈóÆÈ¢òÔºåÁî®Êà∑ÂèçÈ¶àÈöæ‰ª•ÊúâÊïàÊò†Â∞ÑÂà∞Á≥ªÁªüÁöÑÂÖ∑‰ΩìÁªÑ‰ª∂„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜNExT-SearchÔºåÊó®Âú®Â∞ÜÁªÜÁ≤íÂ∫¶ÁöÑËøáÁ®ãÁ∫ßÂèçÈ¶àÈáçÊñ∞ÂºïÂÖ•ÁîüÊàêÂºèÊêúÁ¥¢ÔºåÁªìÂêàÁî®Êà∑Ë∞ÉËØïÊ®°ÂºèÂíåÂΩ±Â≠êÁî®Êà∑Ê®°ÂºèÔºå‰ª•ÂÆûÁé∞ÂÆûÊó∂ÂíåÁ¶ªÁ∫øÁöÑÂèçÈ¶à‰ø°Âè∑Âà©Áî®Ôºå‰ªéËÄåÊåÅÁª≠ÊîπËøõÊêúÁ¥¢Á≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14135', 'title': 'Hunyuan-Game: Industrial-grade Intelligent Game Creation Model', 'url': 'https://huggingface.co/papers/2505.14135', 'abstract': 'Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.', 'score': 8, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '344469b85ea1e75e', 'authors': ['Ruihuang Li', 'Caijin Zhou', 'Shoujian Zheng', 'Jianxiang Lu', 'Jiabin Huang', 'Comi Chen', 'Junshu Tang', 'Guangzheng Xu', 'Jiale Tao', 'Hongmei Wang', 'Donghao Li', 'Wenqing Yu', 'Senbo Wang', 'Zhimin Li', 'Yetshuan Shi', 'Haoyu Yang', 'Yukun Wang', 'Wenxun Dai', 'Jiaqi Li', 'Linqing Wang', 'Qixun Wang', 'Zhiyong Xu', 'Yingfang Zhang', 'Jiangfeng Xiong', 'Weijie Kong', 'Chao Zhang', 'Hongxin Zhang', 'Qiaoling Zheng', 'Weiting Guo', 'Xinchi Deng', 'Yixuan Li', 'Renjia Wei', 'Yulin Jian', 'Duojun Huang', 'Xuhua Ren', 'Sihuan Lin', 'Yifu Sun', 'Yuan Zhou', 'Joey Wang', 'Qin Lin', 'Jingmiao Yu', 'Jihong Zhang', 'Caesar Zhong', 'Di Wang', 'Yuhong Liu', 'Linus', 'Jie Jiang', 'Longhuang Wu', 'Shuai Shao', 'Qinglin Lu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.14135.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#cv', '#games', '#diffusion', '#video'], 'emoji': 'üéÆ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–≥—Ä: –ò–ò –Ω–∞ —Å–ª—É–∂–±–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤', 'desc': '–ü—Ä–æ–µ–∫—Ç Hunyuan-Game –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é –∏–≥—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–µ—Ç–≤–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –æ–±—à–∏—Ä–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏–≥—Ä–æ–≤—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ú–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–ø–æ—Å–æ–±–Ω—ã —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–≥—Ä–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è –æ–±—â–∏–µ —Å—Ü–µ–Ω—ã, –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π. –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø—è—Ç—å –∫–ª—é—á–µ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏–≥—Ä.'}, 'en': {'title': 'Revolutionizing Game Development with AI-Driven Content Creation', 'desc': 'The paper introduces Hunyuan-Game, a project that leverages generative artificial intelligence to enhance game development by creating high-quality game assets. It focuses on two main areas: image generation and video generation, utilizing extensive datasets of game images and videos. The image generation models include various techniques for creating game visuals, such as text-to-image and character generation from sketches. The video generation models address specific challenges in game video production, offering solutions like image-to-video synthesis and interactive video generation, all while maintaining aesthetic quality and understanding of game art styles.'}, 'zh': {'title': 'Êô∫ËÉΩÊ∏∏ÊàèÂàõ‰ΩúÁöÑÊú™Êù•', 'desc': 'Êô∫ËÉΩÊ∏∏ÊàèÂàõ‰ΩúÊòØÊ∏∏ÊàèÂºÄÂèë‰∏≠ÁöÑ‰∏ÄÈ°πÂèòÈù©ÊÄßËøõÂ±ïÔºåÂà©Áî®ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÂä®ÊÄÅÁîüÊàêÂíåÂ¢ûÂº∫Ê∏∏ÊàèÂÜÖÂÆπ„ÄÇÂ∞ΩÁÆ°ÁîüÊàêÊ®°ÂûãÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÈ´òË¥®ÈáèÊ∏∏ÊàèËµÑ‰∫ßÁöÑÁªºÂêàÂêàÊàê‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇHunyuan-GameÈ°πÁõÆÊó®Âú®ÈÄöËøáÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÔºåÊèêÂçáÊ∏∏ÊàèÂÜÖÂÆπÁöÑË¥®ÈáèÂíåËÆæËÆ°Â∏àÁöÑÊïàÁéá„ÄÇËØ•È°πÁõÆÂåÖÊã¨ÂõæÂÉèÁîüÊàêÂíåËßÜÈ¢ëÁîüÊàê‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÂÆöÂà∂ÂåñÁöÑÁîüÊàêÊ®°ÂûãÔºåËÉΩÂ§üÊª°Ë∂≥‰∏çÂêåÊ∏∏ÊàèÂú∫ÊôØÁöÑÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13430', 'title': 'Fine-tuning Quantized Neural Networks with Zeroth-order Optimization', 'url': 'https://huggingface.co/papers/2505.13430', 'abstract': 'As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.', 'score': 8, 'issue_id': 3873, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '6728dda02398fbcc', 'authors': ['Sifeng Shang', 'Jiayi Zhou', 'Chenyu Lin', 'Minxian Li', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University', 'Nanjing University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.13430.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': 'QZO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Quantized Zeroth-order Optimization (QZO). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU –ø—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∑–∞–¥–∞—á–∞–º. QZO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω—É–ª–µ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä—ã–≤–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏, –≤–æ–∑–º—É—â–∞—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é —à–∫–∞–ª—É –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Revolutionizing Memory Efficiency in Large Language Model Training', 'desc': 'This paper addresses the challenge of training large language models (LLMs) with limited GPU memory. It introduces a method called Quantized Zeroth-order Optimization (QZO) that reduces memory usage by eliminating the need for gradients and optimizer states. QZO achieves this by perturbing the quantization scale of weights to estimate gradients, allowing for efficient training without the need for de-quantization. The proposed approach significantly lowers memory costs, enabling the fine-tuning of large models on standard GPUs.'}, 'zh': {'title': 'Á™ÅÁ†¥ÂÜÖÂ≠òÁì∂È¢àÔºåÂÆûÁé∞È´òÊïàËÆ≠ÁªÉ', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËßÑÊ®°ÁöÑÂø´ÈÄüÂ¢ûÈïøÔºåGPUÂÜÖÂ≠òÊàê‰∏∫ÈÄÇÂ∫îËøô‰∫õÊ®°ÂûãÂà∞‰∏ãÊ∏∏‰ªªÂä°ÁöÑÁì∂È¢à„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÂ≠òÈ´òÊïàËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂú®Áªü‰∏ÄÊ°ÜÊû∂ÂÜÖÊúÄÂ∞èÂåñÊ®°ÂûãÊùÉÈáç„ÄÅÊ¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅÁöÑÂÜÖÂ≠ò‰ΩøÁî®„ÄÇÊàë‰ª¨‰ΩøÁî®Èõ∂Èò∂‰ºòÂåñÊù•Ê∂àÈô§Ê¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅÔºåÈÄöËøáÂú®ÂâçÂêë‰º†Êí≠‰∏≠Êâ∞Âä®ÊùÉÈáçÊù•Ëøë‰ººÊ¢ØÂ∫¶ÊñπÂêë„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÈáèÂåñÈõ∂Èò∂‰ºòÂåñÔºàQZOÔºâÔºåÈÄöËøáÊâ∞Âä®ËøûÁª≠ÈáèÂåñÂ∞∫Â∫¶Êù•‰º∞ËÆ°Ê¢ØÂ∫¶Ôºå‰ªéËÄåÂú®‰∏çÁâ∫Áâ≤Á≤æÂ∫¶ÁöÑÊÉÖÂÜµ‰∏ãÊòæËëóÂáèÂ∞ëÂÜÖÂ≠òÊ∂àËÄó„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14681', 'title': 'Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training', 'url': 'https://huggingface.co/papers/2505.14681', 'abstract': "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.", 'score': 7, 'issue_id': 3878, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '89ee9aa82837601e', 'authors': ['Mengru Wang', 'Xingyu Chen', 'Yue Wang', 'Zhiwei He', 'Jiahao Xu', 'Tian Liang', 'Qiuzhi Liu', 'Yunzhi Yao', 'Wenxuan Wang', 'Ruotian Ma', 'Haitao Mi', 'Ningyu Zhang', 'Zhaopeng Tu', 'Xiaolong Li', 'Dong Yu'], 'affiliations': ['Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14681.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#architecture', '#benchmark', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RICE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts. –ò—Å–ø–æ–ª—å–∑—É—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –ø–æ—Ç–æ—á–µ—á–Ω—É—é –≤–∑–∞–∏–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (nPMI), –∞–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ '–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã', –æ—Ç–≤–µ—á–∞—é—â–∏–µ –∑–∞ –º–µ—Ç–∞-—É—Ä–æ–≤–Ω–µ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –≤–µ–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Ç–∏–ø–∞ MoE –ø–æ–∫–∞–∑–∞–ª–∏ –∑–∞–º–µ—Ç–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã–µ –¥–æ–º–µ–Ω—ã. –ü–æ–¥—Ö–æ–¥ RICE –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –æ–±—â–∏–µ –Ω–∞–≤—ã–∫–∏ –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º."}, 'en': {'title': 'Enhancing Reasoning Efficiency with Cognitive Experts', 'desc': "This paper presents a new method called Reinforcing Cognitive Experts (RICE) to improve the reasoning capabilities of Mixture-of-Experts (MoE) architectures in Large Reasoning Models (LRMs). RICE addresses issues of cognitive inefficiencies, such as overthinking and underthinking, by selectively activating specialized experts during inference. The method uses normalized Pointwise Mutual Information (nPMI) to identify these 'cognitive experts' that enhance meta-level reasoning processes. Empirical results show that RICE significantly boosts reasoning accuracy and efficiency compared to existing techniques, while maintaining the model's ability to follow instructions effectively."}, 'zh': {'title': 'Âº∫ÂåñËÆ§Áü•‰∏ìÂÆ∂ÔºöÊèêÂçáÊé®ÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÂú®Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâ‰∏≠ÈÄöËøáÈÄâÊã©ÊÄßÊøÄÊ¥ª‰∏ìÂÆ∂Êù•ÂÆûÁé∞Âá∫Ëâ≤ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÊé®ÁêÜÊ®°ÂûãÂ∏∏Â∏∏Èù¢‰∏¥ËÆ§Áü•ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢òÔºåÂ¶ÇËøáÂ∫¶ÊÄùËÄÉÂíå‰∏çË∂≥ÊÄùËÄÉ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊó∂Èó¥ÂºïÂØºÊñπÊ≥ïÔºåÁß∞‰∏∫Âº∫ÂåñËÆ§Áü•‰∏ìÂÆ∂ÔºàRICEÔºâÔºåÊó®Âú®Âú®‰∏çÂ¢ûÂä†È¢ùÂ§ñËÆ≠ÁªÉÊàñÂ§çÊùÇÂêØÂèëÂºèÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òÊé®ÁêÜÊÄßËÉΩ„ÄÇÈÄöËøáÂà©Áî®ÂΩí‰∏ÄÂåñÁöÑÁÇπ‰∫í‰ø°ÊÅØÔºànPMIÔºâÔºåÊàë‰ª¨Á≥ªÁªüÂú∞ËØÜÂà´Âá∫‰∏ìÈó®ÁöÑ‰∏ìÂÆ∂ÔºåÁß∞‰∏∫‚ÄúËÆ§Áü•‰∏ìÂÆ∂‚ÄùÔºå‰ª•ÂçèË∞É‰ª•‚Äú<think>‚ÄùÁ≠âÊ†áËÆ∞‰∏∫ÁâπÂæÅÁöÑÂÖÉÁ∫ßÊé®ÁêÜÊìç‰Ωú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14464', 'title': 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters', 'url': 'https://huggingface.co/papers/2505.14464', 'abstract': 'Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging FaceDatasets are available on Hugging Face: \\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled}, https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.', 'score': 7, 'issue_id': 3869, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '709996374c466144', 'authors': ['Xiaoyu Tian', 'Yunjie Ji', 'Haotian Wang', 'Shuaiting Chen', 'Sitong Zhao', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.14464.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#data', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç —Ç—Ä–µ—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-—É—á–∏—Ç–µ–ª–µ–π –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –∏–∑ 1,89 –º–∏–ª–ª–∏–æ–Ω–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ, –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑ –º–æ–¥–µ–ª–∏ AM-Thinking-v1, –æ–±–ª–∞–¥–∞—é—Ç –±–æ–ª—å—à–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –¥–ª–∏–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–µ–Ω—å—à–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–µ–π. –ú–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'Enhancing Reasoning in Language Models through Data Distillation', 'desc': 'This paper explores the process of distillation to improve the reasoning abilities of open-source language models. The authors conducted a large-scale study using outputs from three advanced teacher models on a dataset of 1.89 million queries. They found that the distilled data from the AM-Thinking-v1 model had better diversity in token length and lower perplexity, leading to superior performance on various reasoning benchmarks. The results indicate that high-quality reasoning data is crucial for training effective student models, and the authors have made their datasets publicly available for further research.'}, 'zh': {'title': 'Ëí∏È¶èÊäÄÊúØÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÈÄöËøáËí∏È¶èÊäÄÊúØÊèêÂçáÂºÄÊ∫êËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÊù•Ëá™‰∏âÁßçÂÖàËøõÊïôÂ∏àÊ®°ÂûãÁöÑÈ™åËØÅËæìÂá∫ÔºåÂπ∂ÊûÑÂª∫‰∫Ü‰∏â‰∏™Âπ≥Ë°åÊï∞ÊçÆÈõÜËøõË°åÂàÜÊûê„ÄÇÁªìÊûúÊòæÁ§∫ÔºåAM-Thinking-v1Ëí∏È¶èÊï∞ÊçÆÂú®Ê†áËÆ∞ÈïøÂ∫¶Â§öÊ†∑ÊÄßÂíåÂõ∞ÊÉëÂ∫¶ÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥„ÄÇÁªèËøáËÆ≠ÁªÉÁöÑÂ≠¶ÁîüÊ®°ÂûãÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁâπÂà´ÊòØAM-Thinking-v1Ê®°ÂûãÂú®ÂêÑÈ°πÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÊàêÁª©ÔºåÂ±ïÁ§∫‰∫ÜÈ´òË¥®ÈáèÊé®ÁêÜËΩ®ËøπÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12448', 'title': 'SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning', 'url': 'https://huggingface.co/papers/2505.12448', 'abstract': 'Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.', 'score': 7, 'issue_id': 3869, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': '18ffd5153e838d86', 'authors': ['Yang Liu', 'Ming Ma', 'Xiaomin Yu', 'Pengxiang Ding', 'Han Zhao', 'Mingyang Sun', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['Alibaba DAMO Academy', 'Harbin Institute of Technology', 'Shanghai Innovation Institute', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12448.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#cv', '#interpretability', '#benchmark', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SSR (Spatial Sense and Reasoning) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (VLM). SSR –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≥–ª—É–±–∏–Ω–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º —Å–∂–∏–º–∞—é—Ç—Å—è –≤ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SSR-CoT –∏ –±–µ–Ω—á–º–∞—Ä–∫ SSRBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SSR –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≥–ª—É–±–∏–Ω–µ –∏ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º.'}, 'en': {'title': 'Enhancing Spatial Reasoning in VLMs with SSR', 'desc': 'This paper introduces a new method called Spatial Sense and Reasoning (SSR) to improve how Visual-Language Models (VLMs) understand spatial information. SSR transforms raw depth data into structured textual rationales, which help the model reason about space more effectively. The authors also use knowledge distillation to create compact representations of these rationales, allowing for easy integration into existing VLMs without needing to retrain them. To support their research, they present a new dataset, SSR-CoT, and a benchmark, SSRBench, which show that SSR significantly enhances spatial reasoning in VLMs.'}, 'zh': {'title': 'ÊèêÂçáÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â∞ΩÁÆ°ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÆÉ‰ª¨ÂØπRGBËæìÂÖ•ÁöÑ‰æùËµñÈôêÂà∂‰∫ÜÁ≤æÁ°ÆÁöÑÁ©∫Èó¥ÁêÜËß£„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÂú®Êï¥ÂêàÁ©∫Èó¥Á∫øÁ¥¢Êó∂ÔºåÂæÄÂæÄÈúÄË¶Å‰∏ìÁî®‰º†ÊÑüÂô®ÊàñÊó†Ê≥ïÊúâÊïàÂà©Áî®Ê∑±Â∫¶‰ø°ÊÅØËøõË°åÊõ¥È´òÈò∂ÁöÑÊé®ÁêÜ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ©∫Èó¥ÊÑüÁü•‰∏éÊé®ÁêÜÊñπÊ≥ïÔºàSSRÔºâÔºåËØ•Ê°ÜÊû∂Â∞ÜÂéüÂßãÊ∑±Â∫¶Êï∞ÊçÆËΩ¨Âåñ‰∏∫ÁªìÊûÑÂåñÁöÑÂèØËß£ÈáäÊñáÊú¨Êé®ÁêÜ„ÄÇËøô‰∫õÊñáÊú¨Êé®ÁêÜ‰Ωú‰∏∫ÊúâÊÑè‰πâÁöÑ‰∏≠Èó¥Ë°®Á§∫ÔºåÊòæËëóÂ¢ûÂº∫‰∫ÜÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÂπ∂ÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÁîüÊàêÁöÑÊé®ÁêÜÂéãÁº©‰∏∫Á¥ßÂáëÁöÑÊΩúÂú®ÂµåÂÖ•Ôºå‰æø‰∫é‰∏éÁé∞ÊúâVLMsÁöÑÈ´òÊïàÈõÜÊàê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14352', 'title': 'Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability', 'url': 'https://huggingface.co/papers/2505.14352', 'abstract': 'As language models become more powerful and sophisticated, it is crucial that they remain trustworthy and reliable. There is concerning preliminary evidence that models may attempt to deceive or keep secrets from their operators. To explore the ability of current techniques to elicit such hidden knowledge, we train a Taboo model: a language model that describes a specific secret word without explicitly stating it. Importantly, the secret word is not presented to the model in its training data or prompt. We then investigate methods to uncover this secret. First, we evaluate non-interpretability (black-box) approaches. Subsequently, we develop largely automated strategies based on mechanistic interpretability techniques, including logit lens and sparse autoencoders. Evaluation shows that both approaches are effective in eliciting the secret word in our proof-of-concept setting. Our findings highlight the promise of these approaches for eliciting hidden knowledge and suggest several promising avenues for future work, including testing and refining these methods on more complex model organisms. This work aims to be a step towards addressing the crucial problem of eliciting secret knowledge from language models, thereby contributing to their safe and reliable deployment.', 'score': 6, 'issue_id': 3874, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '6b06f2f5351e8b60', 'authors': ['Bartosz Cywi≈Ñski', 'Emil Ryd', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2505.14352.jpg', 'data': {'categories': ['#hallucinations', '#inference', '#training', '#security', '#interpretability'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ç–æ–¥—ã –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∞—é—Ç –º–æ–¥–µ–ª—å Taboo, –∫–æ—Ç–æ—Ä–∞—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Å–µ–∫—Ä–µ—Ç–Ω–æ–µ —Å–ª–æ–≤–æ, –Ω–µ –Ω–∞–∑—ã–≤–∞—è –µ–≥–æ —è–≤–Ω–æ. –ó–∞—Ç–µ–º –æ–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–µ—Ç–æ–¥—ã –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è logit lens –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã, –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è —ç—Ç–æ–≥–æ —Å–µ–∫—Ä–µ—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Unveiling Secrets: Enhancing Trust in Language Models', 'desc': 'This paper addresses the challenge of ensuring that powerful language models remain trustworthy by investigating their potential to conceal information. The authors introduce a Taboo model, which is designed to describe a secret word without revealing it directly, even though the word is not included in the training data. They evaluate two main approaches to uncover this hidden knowledge: black-box methods and mechanistic interpretability techniques, such as logit lens and sparse autoencoders. The results demonstrate that these methods can effectively elicit the secret word, paving the way for future research on improving the transparency and reliability of language models.'}, 'zh': {'title': 'Êè≠Á§∫ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈöêËóèÁü•ËØÜ', 'desc': 'ÈöèÁùÄËØ≠Ë®ÄÊ®°ÂûãÂèòÂæóË∂äÊù•Ë∂äÂº∫Â§ßÂíåÂ§çÊùÇÔºåÁ°Æ‰øùÂÆÉ‰ª¨ÁöÑÂèØ‰ø°ÊÄßÂíåÂèØÈù†ÊÄßÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ®°ÂûãÂèØËÉΩ‰ºöËØïÂõæÊ¨∫È™óÊàñÈöêÁûí‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Á¶ÅÂøåÊ®°ÂûãÔºåÂÆÉÂú®‰∏çÁõ¥Êé•ËØ¥ÊòéÁâπÂÆöÁßòÂØÜËØçÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÊèèËø∞„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®ÈªëÁÆ±ÊñπÊ≥ïÂíåÊú∫Ê¢∞Ëß£ÈáäÊäÄÊúØÂèØ‰ª•ÊúâÊïàÂú∞Êè≠Á§∫Ëøô‰∫õÈöêËóèÁöÑÁü•ËØÜÔºåÊé®Âä®Êú™Êù•Âú®Êõ¥Â§çÊùÇÊ®°Âûã‰∏äÁöÑÂ∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14534', 'title': 'Lessons from Defending Gemini Against Indirect Prompt Injections', 'url': 'https://huggingface.co/papers/2505.14534', 'abstract': "Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.", 'score': 5, 'issue_id': 3874, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '1e855b0dd2463fec', 'authors': ['Chongyang Shi', 'Sharon Lin', 'Shuang Song', 'Jamie Hayes', 'Ilia Shumailov', 'Itay Yona', 'Juliette Pluto', 'Aneesh Pappu', 'Christopher A. Choquette-Choo', 'Milad Nasr', 'Chawin Sitawarin', 'Gena Gibson', 'Andreas Terzis', 'John "Four" Flynn'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2505.14534.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∑–∞—â–∏—Ç—ã Gemini –æ—Ç —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∞—Ç–∞–∫', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–¥—Ö–æ–¥ Google DeepMind –∫ –æ—Ü–µ–Ω–∫–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Gemini –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∞—Ç–∞–∫. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á –æ—Ç –∏–º–µ–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ–º–æ–≥–∞—é—Ç –ø–æ–≤—ã—Å–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å Gemini –∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–æ–≤.'}, 'en': {'title': 'Strengthening Gemini: Safeguarding User Data Against Adversarial Attacks', 'desc': "The paper discusses the challenges of ensuring the security of Gemini, a machine learning model that performs tasks for users by accessing their data. It highlights the risks posed by untrusted data, which can contain malicious instructions that lead the model to behave unexpectedly. To address these risks, Google DeepMind has developed an adversarial evaluation framework that tests Gemini's robustness against sophisticated attacks. The ongoing evaluations aim to enhance Gemini's resilience, ensuring it handles user data and permissions safely and effectively."}, 'zh': {'title': 'ÊèêÂçáGeminiÊ®°ÂûãÁöÑÂØπÊäóÊÄßÈ≤ÅÊ£íÊÄß', 'desc': 'GeminiÊ®°ÂûãË¢´ÂπøÊ≥õÁî®‰∫éÊâßË°åÁî®Êà∑‰ªªÂä°Ôºå‰ΩÜÂú®‰ΩøÁî®Â∑•ÂÖ∑Êó∂ÂèØËÉΩ‰ºöÊé•Ëß¶Âà∞‰∏çÂèØ‰ø°ÁöÑÊï∞ÊçÆÔºåËøôÂ∏¶Êù•‰∫ÜÈ£éÈô©„ÄÇÊÅ∂ÊÑèÊîªÂáªËÄÖÂèØ‰ª•Âú®‰∏çÂèØ‰ø°ÁöÑÊï∞ÊçÆ‰∏≠ÂµåÂÖ•ÊÅ∂ÊÑèÊåá‰ª§ÔºåÂØºËá¥Ê®°ÂûãÂÅèÁ¶ªÁî®Êà∑ÁöÑÊúüÊúõÔºåÈîôËØØÂ§ÑÁêÜÁî®Êà∑ÁöÑÊï∞ÊçÆÊàñÊùÉÈôê„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜGoogle DeepMindËØÑ‰º∞GeminiÊ®°ÂûãÂØπÊäóÊÄßÈ≤ÅÊ£íÊÄßÁöÑÊñπÊ≥ïÔºåÂπ∂ÊÄªÁªì‰∫ÜÂú®Ëøô‰∏ÄËøáÁ®ã‰∏≠Ëé∑ÂæóÁöÑ‰∏ªË¶ÅÁªèÈ™åÊïôËÆ≠„ÄÇÈÄöËøáÂØπGeminiËøõË°åÊåÅÁª≠ÁöÑÂØπÊäóÊÄßËØÑ‰º∞ÔºåÊàë‰ª¨ËÉΩÂ§üÊèêÈ´òÂÖ∂ÊäµÂæ°ÊìçÊéßÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13718', 'title': 'Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings', 'url': 'https://huggingface.co/papers/2505.13718', 'abstract': 'Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we "warm up" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: (i) the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both the base model and the warmed-up model are RLVR trained on the same small dataset (leq100 examples), the warmed-up model consistently outperforms the base model; (iii) Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; (iv) Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.', 'score': 5, 'issue_id': 3872, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '29613228991289b5', 'authors': ['Safal Shrestha', 'Minwu Kim', 'Aadim Nepal', 'Anubhav Shrestha', 'Keith Ross'], 'affiliations': ['New York University Abu Dhabi'], 'pdf_title_img': 'assets/pdf/title_img/2505.13718.jpg', 'data': {'categories': ['#data', '#rl', '#long_context', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞–≤—ã–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å '—Ä–∞–∑–æ–≥—Ä–µ–≤–∞–µ—Ç—Å—è' –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö, –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è –æ–±—â–∏–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ —Ü–µ–ª–µ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ RLVR. –ú–µ—Ç–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω—ã—Ö LLM –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö."}, 'en': {'title': 'Warmup for Robust Reasoning in Data-Scarce LLMs', 'desc': 'This paper presents a novel two-stage training strategy for developing reasoning-capable large language models (LLMs) when high-quality training data is limited. The first stage involves warming up the model by distilling Long Chains of Thought (CoT) from simple logic puzzles, which helps the model acquire general reasoning skills. In the second stage, Reinforcement Learning with Verifiable Rewards (RLVR) is applied using a small set of examples from the target domain. The results show that this approach enhances performance across various tasks and improves sample efficiency, demonstrating the effectiveness of the warmup phase in building robust reasoning LLMs.'}, 'zh': {'title': 'Âú®Êï∞ÊçÆÁ®ÄÁº∫ÁéØÂ¢É‰∏≠ÊûÑÂª∫Âº∫Â§ßÊé®ÁêÜÊ®°ÂûãÁöÑÊúâÊïàÁ≠ñÁï•', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®ÊúâÈôêÁõëÁù£‰∏ãÂºÄÂèëÊé®ÁêÜËÉΩÂäõÂº∫ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøá‰ªéÁÆÄÂçïÁöÑÈÄªËæëË∞úÈ¢òÔºàÈ™ëÂ£´‰∏éÈ™óÂ≠êÔºâ‰∏≠ÊèêÂèñÈïøÊÄùÁª¥ÈìæÔºàCoTÔºâÊù•‚ÄúÈ¢ÑÁÉ≠‚ÄùÊ®°ÂûãÔºå‰ª•Ëé∑Âèñ‰∏ÄËà¨Êé®ÁêÜÊäÄËÉΩ„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂàô‰ΩøÁî®ÊúâÈôêÁöÑÁõÆÊ†áÈ¢ÜÂüüÁ§∫‰æãÂØπÈ¢ÑÁÉ≠ÂêéÁöÑÊ®°ÂûãËøõË°åÂº∫ÂåñÂ≠¶‰π†‰∏éÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËøôÁßç‰∏§Èò∂ÊÆµÁöÑÊñπÊ≥ïÂú®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÁéØÂ¢É‰∏≠ËÉΩÂ§üÊúâÊïàÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13103', 'title': 'Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair', 'url': 'https://huggingface.co/papers/2505.13103', 'abstract': 'The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness.   We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.', 'score': 5, 'issue_id': 3874, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '4d2278523a88b9bc', 'authors': ['Han Zheng', 'Ilia Shumailov', 'Tianqi Fan', 'Aiden Hall', 'Mathias Payer'], 'affiliations': ['EPFL Lausanne, Switzerland', 'Google DeepMind London, UK', 'Google New York, USA', 'Google Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2505.13103.jpg', 'data': {'categories': ['#agents', '#plp', '#training', '#optimization', '#open_source', '#benchmark'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': 'WILLIAMT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º —Å –º–µ–Ω—å—à–∏–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ–≥—Ä–∞–º–º (APR) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º WILLIAMT. –°–∏—Å—Ç–µ–º–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –º–µ—Å—Ç–∞ —Å–±–æ—è –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ç—á–µ–π, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). WILLIAMT –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 29.6% –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ APR –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ARVO. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ WILLIAMT –º–æ–∂–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–∂–µ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –æ–±—ã—á–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–∞—Ö.'}, 'en': {'title': 'Simplifying Bug Fixes with WILLIAMT: Efficient Automated Program Repair', 'desc': 'This paper addresses the challenge of fixing software bugs, which have become too numerous for developers to handle manually. It introduces a method called crash-site repair that simplifies the bug-fixing process while reducing the risk of security issues. The authors also present a template-guided patch generation technique that lowers the token usage of Large Language Models (LLMs), making the repair process more efficient. Their system, WILLIAMT, shows significant improvements in bug-fixing rates and can operate effectively even on less powerful hardware.'}, 'zh': {'title': 'WILLIAMTÔºöÈ´òÊïàÁöÑËá™Âä®ÂåñÁ®ãÂ∫è‰øÆÂ§çÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöèÁùÄÊºèÊ¥ûÂèëÁé∞ÊäÄÊúØÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÂºÄÂèëËÄÖÈù¢‰∏¥ÁùÄË∂äÊù•Ë∂äÂ§öÁöÑÊºèÊ¥û‰øÆÂ§çÈúÄÊ±ÇÔºåÂõ†Ê≠§Ëø´ÂàáÈúÄË¶ÅÊúâÊïàÁöÑËá™Âä®ÂåñÁ®ãÂ∫è‰øÆÂ§çÔºàAPRÔºâÊñπÊ≥ï„ÄÇÁé∞‰ª£ÊºèÊ¥ûÁöÑÂ§çÊùÇÊÄß‰ΩøÂæóÁ≤æÁ°ÆÁöÑÊ†πÊú¨ÂéüÂõ†ÂàÜÊûêÂèòÂæóÂõ∞Èöæ‰∏î‰∏çÂèØÈù†„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ¥©Ê∫ÉÁé∞Âú∫‰øÆÂ§çÊñπÊ≥ïÔºå‰ª•ÁÆÄÂåñ‰øÆÂ§ç‰ªªÂä°ÔºåÂêåÊó∂Èôç‰ΩéË¢´Âà©Áî®ÁöÑÈ£éÈô©„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ®°ÊùøÂºïÂØºÁöÑË°•‰∏ÅÁîüÊàêÊñπÊ≥ïÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑ‰ª§ÁâåÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊïàÁéáÂíåÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12182', 'title': 'Truth Neurons', 'url': 'https://huggingface.co/papers/2505.12182', 'abstract': 'Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.', 'score': 5, 'issue_id': 3868, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': 'ddeab64450bb26a9', 'authors': ['Haohang Li', 'Yupeng Cao', 'Yangyang Yu', 'Jordan W. Suchow', 'Zining Zhu'], 'affiliations': ['Stevens Institute of Technology', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.12182.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#hallucinations', '#alignment', '#data', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ù–µ–π—Ä–æ–Ω—ã –ø—Ä–∞–≤–¥—ã: –ø—É—Ç—å –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—ã–µ '–Ω–µ–π—Ä–æ–Ω—ã –ø—Ä–∞–≤–¥—ã', –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–¥–∏—Ä—É—é—Ç –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–µ–º—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ –Ω–∞–ª–∏—á–∏–µ —Ç–∞–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞. –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —ç—Ç–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ —É—Ö—É–¥—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏."}, 'en': {'title': 'Unveiling Truth Neurons: Enhancing Language Model Trustworthiness', 'desc': "This paper investigates how language models encode truthfulness at the neuron level, revealing the presence of 'truth neurons' that represent truthfulness in a way that is not dependent on specific subjects. The authors demonstrate that these truth neurons exist across various models, indicating a shared property among them. By analyzing the distribution of truth neurons across different layers, the study aligns with previous research on the geometry of truthfulness. Additionally, the suppression of these neurons negatively impacts model performance, suggesting that understanding and improving truthfulness in language models is crucial for their reliability."}, 'zh': {'title': 'Êè≠Á§∫ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁúüÁõ∏Á•ûÁªèÂÖÉ', 'desc': 'Â∞ΩÁÆ°ËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßçÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÊúâÊó∂‰ºö‰∫ßÁîü‰∏çÁúüÂÆûÁöÑÂõûÁ≠î„ÄÇÊàë‰ª¨ÂØπËøô‰∫õÊ®°Âûã‰∏≠ÁúüÁõ∏ÁºñÁ†ÅÊú∫Âà∂ÁöÑÁêÜËß£ÊúâÈôêÔºåËøôÂΩ±Âìç‰∫ÜÂÆÉ‰ª¨ÁöÑÂèØÈù†ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºåÈÄöËøáÁ•ûÁªèÂÖÉÂ±ÇÈù¢ËØÜÂà´ÁúüÁõ∏ÁöÑË°®Á§∫ÔºåÂèëÁé∞ËØ≠Ë®ÄÊ®°Âûã‰∏≠Â≠òÂú®ÁºñÁ†ÅÁúüÁõ∏ÁöÑÁúüÁõ∏Á•ûÁªèÂÖÉ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÁúüÁõ∏Á•ûÁªèÂÖÉÁöÑÂ≠òÂú®ÊòØËÆ∏Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ±ÂêåÁâπÊÄßÔºåÂπ∂‰∏îÂÖ∂ÂàÜÂ∏ÉÊ®°Âºè‰∏éÁúüÁõ∏ÁöÑÂá†‰ΩïÁâπÂæÅ‰∏ÄËá¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09569', 'title': 'MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8', 'url': 'https://huggingface.co/papers/2505.09569', 'abstract': 'With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with 5,102 and 300 repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience and https://github.com/amazon-science/self_debug respectively.', 'score': 5, 'issue_id': 3872, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': 'cb7832fb680cc056', 'authors': ['Linbo Liu', 'Xinle Liu', 'Qiang Zhou', 'Lin Chen', 'Yihan Liu', 'Hoan Nguyen', 'Behrooz Omidvar-Tehrani', 'Xi Shen', 'Jun Huan', 'Omer Tripp', 'Anoop Deoras'], 'affiliations': ['AWS AI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.09569.jpg', 'data': {'categories': ['#survey', '#data', '#optimization', '#dataset', '#benchmark'], 'emoji': 'üîÑ', 'ru': {'title': 'MIGRATION-BENCH: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∏–≥—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MIGRATION-BENCH –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–µ –º–∏–≥—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å Java 8 –Ω–∞ –±–æ–ª–µ–µ –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 5,102 —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ 300 —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ SD-Feedback, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ Claude-3.5-Sonnet-v2 –∏ –º–µ—Ç–æ–¥–∞ SD-Feedback –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤ 62.33% –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ –∏ 27.00% –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Revolutionizing Code Migration with MIGRATION-BENCH', 'desc': 'This paper introduces MIGRATION-BENCH, a new benchmark specifically designed for evaluating large language models (LLMs) on the task of code migration from Java 8 to newer long-term support versions like Java 17 and 21. Unlike existing benchmarks that focus on problem-solving, MIGRATION-BENCH provides a comprehensive dataset of 5,102 repositories, with a curated subset of 300 that vary in complexity and difficulty. The authors also present an evaluation framework to standardize the assessment of LLMs in this domain, demonstrating that LLMs can effectively perform repository-level code migration. Using their proposed SD-Feedback method, they report success rates of 62.33% for minimal migration and 27.00% for maximal migration with the Claude-3.5-Sonnet-v2 model.'}, 'zh': {'title': '‰ª£Á†ÅËøÅÁßªÁöÑÊñ∞Âü∫ÂáÜÔºöMIGRATION-BENCH', 'desc': 'ËøëÂπ¥Êù•ÔºåÂº∫Â§ßÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøÖÈÄüÂèëÂ±ïÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßçËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîü‰∫ßÂäõÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÁöÑÁºñÁ†ÅËÉΩÂäõÔºåÂºÄÂèë‰∫ÜËÆ∏Â§öÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºå‰ΩÜÂ§ßÂ§öÈõÜ‰∏≠Âú®ÈóÆÈ¢òËß£ÂÜ≥ÂíåÊïÖÈöúÊéíÈô§‰ªªÂä°‰∏ä„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÁºñÁ†ÅÂü∫ÂáÜMIGRATION-BENCHÔºå‰∏ìÊ≥®‰∫é‰ª£Á†ÅËøÅÁßªÔºåÁâπÂà´ÊòØ‰ªéJava 8ËøÅÁßªÂà∞ÊúÄÊñ∞ÁöÑÈïøÊúüÊîØÊåÅÁâàÊú¨ÔºàJava 17„ÄÅ21Ôºâ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´ÂÆåÊï¥Êï∞ÊçÆÈõÜÂíå‰ª£Ë°®ÊÄßÂ≠êÈõÜÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Â§öÂäüËÉΩËµÑÊ∫êÔºå‰ª•ÊîØÊåÅ‰ª£Á†ÅËøÅÁßªÈ¢ÜÂüüÁöÑÁ†îÁ©∂ÔºåÂπ∂Êèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞Ê°ÜÊû∂Ôºå‰ª•‰æøÂØπLLMsÂú®Ëøô‰∏ÄÊåëÊàòÊÄß‰ªªÂä°‰∏äÁöÑË°®Áé∞ËøõË°å‰∏•Ê†ºÂíåÊ†áÂáÜÂåñÁöÑËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14648', 'title': 'Vox-Profile: A Speech Foundation Model Benchmark for Characterizing\n  Diverse Speaker and Speech Traits', 'url': 'https://huggingface.co/papers/2505.14648', 'abstract': 'We introduce Vox-Profile, a comprehensive benchmark to characterize rich speaker and speech traits using speech foundation models. Unlike existing works that focus on a single dimension of speaker traits, Vox-Profile provides holistic and multi-dimensional profiles that reflect both static speaker traits (e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech flow). This benchmark is grounded in speech science and linguistics, developed with domain experts to accurately index speaker and speech characteristics. We report benchmark experiments using over 15 publicly available speech datasets and several widely used speech foundation models that target various static and dynamic speaker and speech properties. In addition to benchmark experiments, we showcase several downstream applications supported by Vox-Profile. First, we show that Vox-Profile can augment existing speech recognition datasets to analyze ASR performance variability. Vox-Profile is also used as a tool to evaluate the performance of speech generation systems. Finally, we assess the quality of our automated profiles through comparison with human evaluation and show convergent validity. Vox-Profile is publicly available at: https://github.com/tiantiaf0627/vox-profile-release.', 'score': 4, 'issue_id': 3884, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'f1d0da4a4a22866a', 'authors': ['Tiantian Feng', 'Jihwan Lee', 'Anfeng Xu', 'Yoonjeong Lee', 'Thanathai Lertpetchpun', 'Xuan Shi', 'Helin Wang', 'Thomas Thebaud', 'Laureano Moro-Velazquez', 'Dani Byrd', 'Najim Dehak', 'Shrikanth Narayanan'], 'affiliations': ['Johns Hopkins University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.14648.jpg', 'data': {'categories': ['#benchmark', '#audio'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'Vox-Profile: –ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ—á–∏ –∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': 'Vox-Profile - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –±–æ–≥–∞—Ç—ã—Ö —á–µ—Ä—Ç –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—á–∏. –û–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω—ã–µ –∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏, –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ –∫–∞–∫ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã –≥–æ–≤–æ—Ä—è—â–µ–≥–æ (–≤–æ–∑—Ä–∞—Å—Ç, –ø–æ–ª, –∞–∫—Ü–µ–Ω—Ç), —Ç–∞–∫ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ —Ä–µ—á–∏ (—ç–º–æ—Ü–∏–∏, –ø–ª–∞–≤–Ω–æ—Å—Ç—å —Ä–µ—á–∏). –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–∞—É–∫–µ –æ —Ä–µ—á–∏ –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –¥–ª—è —Ç–æ—á–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ —Ä–µ—á–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª–µ–µ 15 –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—á–∏.'}, 'en': {'title': 'Vox-Profile: A Holistic Benchmark for Speaker and Speech Trait Analysis', 'desc': 'Vox-Profile is a new benchmark designed to analyze various speaker and speech traits using advanced speech foundation models. It goes beyond previous studies by providing a comprehensive view of both static traits like age and accent, and dynamic traits such as emotion and speech flow. Developed with input from experts in speech science and linguistics, it offers a reliable way to index these characteristics. The benchmark has been tested with multiple speech datasets and models, demonstrating its utility in improving automatic speech recognition and speech generation systems.'}, 'zh': {'title': 'Vox-ProfileÔºöÂ§öÁª¥Â∫¶ËØ¥ËØùËÄÖÁâπÂæÅÁöÑÂü∫ÂáÜ', 'desc': 'Vox-ProfileÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éÈÄöËøáËØ≠Èü≥Âü∫Á°ÄÊ®°ÂûãÊù•Ë°®ÂæÅ‰∏∞ÂØåÁöÑËØ¥ËØùËÄÖÂíåËØ≠Èü≥ÁâπÂæÅ„ÄÇ‰∏éÁé∞ÊúâÁ†îÁ©∂Âè™ÂÖ≥Ê≥®Âçï‰∏ÄÁª¥Â∫¶ÁöÑËØ¥ËØùËÄÖÁâπÂæÅ‰∏çÂêåÔºåVox-ProfileÊèê‰æõ‰∫ÜÂèçÊò†ÈùôÊÄÅËØ¥ËØùËÄÖÁâπÂæÅÔºàÂ¶ÇÂπ¥ÈæÑ„ÄÅÊÄßÂà´„ÄÅÂè£Èü≥ÔºâÂíåÂä®ÊÄÅËØ≠Èü≥Â±ûÊÄßÔºàÂ¶ÇÊÉÖÊÑü„ÄÅËØ≠ÈÄüÔºâÁöÑÊï¥‰ΩìÂíåÂ§öÁª¥Ê°£Ê°à„ÄÇËØ•Âü∫ÂáÜÂü∫‰∫éËØ≠Èü≥ÁßëÂ≠¶ÂíåËØ≠Ë®ÄÂ≠¶Ôºå‰∏éÈ¢ÜÂüü‰∏ìÂÆ∂Âêà‰ΩúÂºÄÂèëÔºå‰ª•ÂáÜÁ°ÆÁ¥¢ÂºïËØ¥ËØùËÄÖÂíåËØ≠Èü≥ÁâπÂæÅ„ÄÇÊàë‰ª¨ÈÄöËøáË∂ÖËøá15‰∏™ÂÖ¨ÂºÄÁöÑËØ≠Èü≥Êï∞ÊçÆÈõÜÂíåÂ§öÁßçÂπøÊ≥õ‰ΩøÁî®ÁöÑËØ≠Èü≥Âü∫Á°ÄÊ®°ÂûãËøõË°åÂü∫ÂáÜÂÆûÈ™åÔºåÂ±ïÁ§∫‰∫ÜVox-ProfileÂú®ËØ≠Èü≥ËØÜÂà´ÂíåÁîüÊàêÁ≥ªÁªüËØÑ‰º∞‰∏≠ÁöÑÂ∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13380', 'title': 'CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition', 'url': 'https://huggingface.co/papers/2505.13380', 'abstract': "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526", 'score': 4, 'issue_id': 3868, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '6a5e70a76e6f012c', 'authors': ['Nam V. Nguyen', 'Huy Nguyen', 'Quang Pham', 'Van Nguyen', 'Savitha Ramasamy', 'Nhat Ho'], 'affiliations': ['FPT Software AI Center', 'Independent Researcher', 'Institute for Infocomm Research, ASTAR', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.13380.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training'], 'emoji': 'üèÜ', 'ru': {'title': '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —Å–º–µ—Å—è—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (SMoE) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'competition'. –ê–≤—Ç–æ—Ä—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–ª–∞–¥–∞–µ—Ç –ª—É—á—à–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –≤—ã–±–æ—Ä–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π softmax. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º CompeteSMoE –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å CompeteSMoE –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ SMoE."}, 'en': {'title': 'CompeteSMoE: Efficient Routing for Powerful Language Models', 'desc': 'Sparse mixture of experts (SMoE) is a method that allows models to become more complex without simply making them deeper or wider. The challenge with SMoE is that the way experts are chosen to process data can be inefficient, as not all experts contribute to the decision-making process. This paper introduces a new routing mechanism called competition, which directs data to the most responsive experts, improving the efficiency of the model. The authors present CompeteSMoE, an algorithm that uses this competition mechanism to train large language models effectively, showing better performance and lower training costs compared to existing methods.'}, 'zh': {'title': 'Á´û‰∫âÊú∫Âà∂ÊèêÂçáÁ®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÁöÑÊïàÁéá', 'desc': 'Á®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàSMoEÔºâÊòØ‰∏ÄÁßçÊúâÊïàÊèêÂçáÊ®°ÂûãÂ§çÊùÇÂ∫¶ÁöÑÊñπÊ≥ïÔºåË∂ÖË∂ä‰∫ÜÁÆÄÂçïÂ¢ûÂä†ÁΩëÁªúÊ∑±Â∫¶ÊàñÂÆΩÂ∫¶ÁöÑÊñπÂºè„ÄÇÁÑ∂ËÄåÔºåSMoEÁöÑËÆ≠ÁªÉ‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ËÆ°ÁÆóÁöÑ‰∏ìÂÆ∂‰∏éË∑ØÁî±ËøáÁ®ã‰πãÈó¥ÁöÑËÅîÁ≥ª‰∏çÂ§üÁõ¥Êé•„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âà∂‚Äî‚ÄîÁ´û‰∫âÔºåËÉΩÂ§üÂ∞ÜËæìÂÖ•Êï∞ÊçÆÊõ¥ÊúâÊïàÂú∞Ë∑ØÁî±Âà∞ÂìçÂ∫îÊúÄÂº∫ÁöÑ‰∏ìÂÆ∂„ÄÇÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÁ´û‰∫âÊú∫Âà∂Âú®Ê†∑Êú¨ÊïàÁéá‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑsoftmaxË∑ØÁî±ÔºåÂπ∂ÂºÄÂèë‰∫ÜCompeteSMoEÁÆóÊ≥ïÔºåËÉΩÂ§ü‰ª•ËæÉ‰ΩéÁöÑËÆ≠ÁªÉÂºÄÈîÄÂÆûÁé∞Âº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11365', 'title': 'Phare: A Safety Probe for Large Language Models', 'url': 'https://huggingface.co/papers/2505.11365', 'abstract': 'Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.', 'score': 4, 'issue_id': 3873, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '8ab32377d956578e', 'authors': ['Pierre Le Jeune', 'Beno√Æt Mal√©zieux', 'Weixuan Xiao', 'Matteo Dora'], 'affiliations': ['Giskard AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.11365.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#ethics', '#hallucinations', '#alignment'], 'emoji': 'üîç', 'ru': {'title': '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Phare - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ —Ç—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –∞—Å–ø–µ–∫—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Ä–µ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ—Ü–µ–Ω–∫—É 17 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM, –≤—ã—è–≤–∏–≤ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤–æ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è —É–≥–æ–¥–Ω–∏—á–µ—Å—Ç–≤–æ, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–º–ø—Ç—É –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø–æ–≤. Phare —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –æ—Ç–∫–∞–∑–∞, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –¥–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –∑–∞—Å–ª—É–∂–∏–≤–∞—é—â–∏—Ö –¥–æ–≤–µ—Ä–∏—è —è–∑—ã–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Phare: Probing Safety in Language Models Beyond Performance', 'desc': 'This paper presents Phare, a new framework designed to evaluate the safety of large language models (LLMs) by focusing on their failure modes rather than just their performance. It assesses LLMs across three key areas: hallucination and reliability, social biases, and harmful content generation. The study analyzes 17 advanced LLMs and uncovers common vulnerabilities, such as sycophancy and prompt sensitivity, which can lead to biased or harmful outputs. By identifying these specific issues, Phare aims to help researchers and developers create more reliable and ethical language models.'}, 'zh': {'title': 'ÊûÑÂª∫Êõ¥ÂÆâÂÖ®ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåËØÜÂà´Â§±Ë¥•Ê®°ÂºèÔºÅ', 'desc': 'Á°Æ‰øùÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂÆâÂÖ®ÊÄßÂØπ‰∫éË¥üË¥£‰ªªÁöÑÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÁöÑËØÑ‰º∞ÂæÄÂæÄÊõ¥ÂÖ≥Ê≥®ÊÄßËÉΩËÄåÈùûËØÜÂà´Â§±Ë¥•Ê®°Âºè„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPhareÔºåËøôÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄËØäÊñ≠Ê°ÜÊû∂ÔºåÁî®‰∫éÊé¢ÊµãÂíåËØÑ‰º∞LLMÂú®‰∏â‰∏™ÂÖ≥ÈîÆÁª¥Â∫¶‰∏äÁöÑË°å‰∏∫ÔºöÂπªËßâÂíåÂèØÈù†ÊÄß„ÄÅÁ§æ‰ºöÂÅèËßÅ‰ª•ÂèäÊúâÂÆ≥ÂÜÖÂÆπÁîüÊàê„ÄÇÂØπ17‰∏™ÊúÄÂÖàËøõÁöÑLLMÁöÑËØÑ‰º∞Êè≠Á§∫‰∫ÜÂú®ÊâÄÊúâÂÆâÂÖ®Áª¥Â∫¶‰∏äÁ≥ªÁªüÊÄßËÑÜÂº±ÊÄßÁöÑÊ®°ÂºèÔºåÂåÖÊã¨Ë∞ÑÂ™ö„ÄÅÊèêÁ§∫ÊïèÊÑüÊÄßÂíåÂàªÊùøÂç∞Ë±°ÂÜçÁé∞„ÄÇÈÄöËøáÁ™ÅÂá∫Ëøô‰∫õÂÖ∑‰ΩìÁöÑÂ§±Ë¥•Ê®°ÂºèÔºåPhare‰∏∫Á†îÁ©∂‰∫∫ÂëòÂíå‰ªé‰∏öËÄÖÊèê‰æõ‰∫ÜÂèØÊìç‰ΩúÁöÑËßÅËß£Ôºå‰ª•ÊûÑÂª∫Êõ¥Âº∫Â§ß„ÄÅÊõ¥‰∏ÄËá¥ÂíåÊõ¥ÂèØ‰ø°ÁöÑËØ≠Ë®ÄÁ≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13988', 'title': 'The Hallucination Tax of Reinforcement Finetuning', 'url': 'https://huggingface.co/papers/2505.13988', 'abstract': "Reinforcement finetuning (RFT) has become a standard approach for enhancing the reasoning capabilities of large language models (LLMs). However, its impact on model trustworthiness remains underexplored. In this work, we identify and systematically study a critical side effect of RFT, which we term the hallucination tax: a degradation in refusal behavior causing models to produce hallucinated answers to unanswerable questions confidently. To investigate this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of unanswerable math problems designed to probe models' ability to recognize an unanswerable question by reasoning from the insufficient or ambiguous information. Our results show that standard RFT training could reduce model refusal rates by more than 80%, which significantly increases model's tendency to hallucinate. We further demonstrate that incorporating just 10% SUM during RFT substantially restores appropriate refusal behavior, with minimal accuracy trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage inference-time compute to reason about their own uncertainty and knowledge boundaries, improving generalization not only to out-of-domain math problems but also to factual question answering tasks.", 'score': 3, 'issue_id': 3887, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'db283d13501585eb', 'authors': ['Linxin Song', 'Taiwei Shi', 'Jieyu Zhao'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.13988.jpg', 'data': {'categories': ['#hallucinations', '#rlhf', '#training', '#dataset', '#synthetic', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –ò–ò: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –∏ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —É—Å–∏–ª–µ–Ω–∏—è —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RFT). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ '–Ω–∞–ª–æ–≥–∞ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏' –∏ —Å–æ–∑–¥–∞—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SUM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –±–µ–∑ –æ—Ç–≤–µ—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ RFT –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –æ—Ç–∫–∞–∑–æ–≤ –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 80%. –û–¥–Ω–∞–∫–æ –≤–∫–ª—é—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ 10% –¥–∞–Ω–Ω—ã—Ö SUM –≤ –ø—Ä–æ—Ü–µ—Å—Å RFT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–∞ –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á–∞—Ö."}, 'en': {'title': 'Balancing Confidence and Accuracy in Language Models', 'desc': "This paper explores the effects of Reinforcement Finetuning (RFT) on the trustworthiness of large language models (LLMs). It introduces the concept of the 'hallucination tax', which refers to the increased likelihood of models confidently generating incorrect answers to unanswerable questions after RFT. The authors present a new dataset called SUM (Synthetic Unanswerable Math) to evaluate how well models can identify unanswerable questions. Their findings suggest that while RFT can significantly lower refusal rates, incorporating SUM during training can help restore proper refusal behavior without sacrificing performance on answerable tasks."}, 'zh': {'title': 'Âº∫ÂåñÂæÆË∞É‰∏éÂπªËßâÁ®éÁöÑÂπ≥Ë°°', 'desc': 'Âº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâÂ∑≤Êàê‰∏∫ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜËÉΩÂäõÁöÑÊ†áÂáÜÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂØπÊ®°ÂûãÂèØ‰ø°Â∫¶ÁöÑÂΩ±Âìç‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊàë‰ª¨ÂèëÁé∞Âπ∂Á≥ªÁªüÁ†îÁ©∂‰∫ÜRFTÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÂâØ‰ΩúÁî®ÔºåÁß∞‰∏∫ÂπªËßâÁ®éÔºöÂç≥Ê®°ÂûãÂú®Èù¢ÂØπÊó†Ê≥ïÂõûÁ≠îÁöÑÈóÆÈ¢òÊó∂ÔºåÊãíÁªùË°å‰∏∫ÁöÑ‰∏ãÈôçÂØºËá¥ÂÖ∂Ëá™‰ø°Âú∞‰∫ßÁîüÂπªËßâÁ≠îÊ°à„ÄÇÈÄöËøáÂºïÂÖ•SUMÔºàÂêàÊàê‰∏çÂèØÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºâÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨Êé¢ËÆ®‰∫ÜÊ®°ÂûãËØÜÂà´‰∏çÂèØÂõûÁ≠îÈóÆÈ¢òÁöÑËÉΩÂäõÔºåÂπ∂ÂèëÁé∞Ê†áÂáÜÁöÑRFTËÆ≠ÁªÉ‰ºöÊòæËëóÈôç‰ΩéÊ®°ÂûãÁöÑÊãíÁªùÁéáÔºå‰ªéËÄåÂ¢ûÂä†ÂπªËßâÁöÑÂÄæÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13946', 'title': 'Visual Instruction Bottleneck Tuning', 'url': 'https://huggingface.co/papers/2505.13946', 'abstract': "Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the robustness of MLLMs under distribution shifts, from a representation learning perspective. Inspired by the information bottleneck (IB) principle, we derive a variational lower bound of the IB for MLLMs and devise a practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide a theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of three MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLM's robustness under shifts by pursuing the learning of a minimal sufficient representation.", 'score': 3, 'issue_id': 3887, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '8d7a6b4e5fc68de6', 'authors': ['Changdae Oh', 'Jiatong Li', 'Shawn Im', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.13946.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#multimodal', '#training', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Visual Instruction Bottleneck Tuning (Vittle) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –µ–≥–æ —Å–≤—è–∑—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –º–µ—Ç—Ä–∏–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ MLLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ 45 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ Vittle –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å MLLM –∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å–Ω—ã–º —Å–¥–≤–∏–≥–∞–º.'}, 'en': {'title': 'Enhancing MLLM Robustness with Vittle: A New Approach to Distribution Shifts', 'desc': 'This paper addresses the issue of multimodal large language models (MLLMs) struggling with unfamiliar queries due to distribution shifts. Instead of relying on more data or larger models, the authors propose a new method called Visual Instruction Bottleneck Tuning (Vittle) that enhances MLLM robustness through representation learning. Vittle is based on the information bottleneck principle, which helps in learning a minimal sufficient representation of the data. The authors validate their approach through experiments on various tasks, showing that Vittle significantly improves MLLM performance under different distribution shifts.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÈ≤ÅÊ£íÊÄßÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â∞ΩÁÆ°Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâË¢´ÂπøÊ≥õÂ∫îÁî®Ôºå‰ΩÜÂú®Èù¢ÂØπÂàÜÂ∏ÉÂèòÂåñÊó∂ÔºåÂÆÉ‰ª¨ÁöÑÊÄßËÉΩ‰ºö‰∏ãÈôç„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÊõ¥Â§öÁöÑÊåá‰ª§Êï∞ÊçÆÊàñÊõ¥Â§ßÁöÑÊ®°ÂûãÊû∂ÊûÑÔºåËøô‰ºöÂ¢ûÂä†‰∫∫ÂäõÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºå‰ªéË°®Á§∫Â≠¶‰π†ÁöÑËßíÂ∫¶Â¢ûÂº∫MLLMÂú®ÂàÜÂ∏ÉÂèòÂåñ‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜËßÜËßâÊåá‰ª§Áì∂È¢àË∞É‰ºòÔºàVittleÔºâÔºåÂπ∂ÈÄöËøáÁêÜËÆ∫ÂàÜÊûêËØÅÊòé‰∫ÜÂÖ∂‰∏é‰ø°ÊÅØÁêÜËÆ∫È≤ÅÊ£íÊÄßÊåáÊ†áÁöÑÂÖ≥Á≥ªÔºåÂÆûÈ™åËØÅÊòéVittleÂú®Â§ö‰∏™‰ªªÂä°‰∏≠ÊúâÊïàÊèêÂçá‰∫ÜMLLMÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12306', 'title': 'Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection', 'url': 'https://huggingface.co/papers/2505.12306', 'abstract': 'Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia\'s "Did You Know..." entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.', 'score': 3, 'issue_id': 3868, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 –º–∞—è', 'en': 'May 18', 'zh': '5Êúà18Êó•'}, 'hash': 'ccbad06f5ba35418', 'authors': ['Yuwei Zhang', 'Wenhao Yu', 'Shangbin Feng', 'Yifan Zhu', 'Letian Peng', 'Jayanth Srinivasa', 'Gaowen Liu', 'Jingbo Shang'], 'affiliations': ['Cisco', 'Tencent AI Lab', 'UC, San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.12306.jpg', 'data': {'categories': ['#transfer_learning', '#interpretability', '#benchmark', '#dataset'], 'emoji': 'üß†', 'ru': {'title': 'WikiDYK: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –ø–∞–º—è—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ WikiDYK –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∑–Ω–∞–Ω–∏—è. WikiDYK –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–¥–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ Wikipedia 'Did You Know...', –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –∏—Ö –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ –ø–∞—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (BiLM) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ –∑–∞–ø–æ–º–∏–Ω–∞—é—Ç –∑–Ω–∞–Ω–∏—è, —á–µ–º –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (CLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥—É–ª—å–Ω—É—é –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –∞–Ω—Å–∞–º–±–ª–∏ BiLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–Ω–µ—à–Ω–∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â –∑–Ω–∞–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏."}, 'en': {'title': 'Enhancing Knowledge Memorization in Language Models with WikiDYK', 'desc': "This paper presents WikiDYK, a new benchmark for evaluating knowledge memorization in large language models (LLMs). It uses real-world facts from Wikipedia's 'Did You Know...' entries to create a diverse set of question-answer pairs. The study finds that Causal Language Models (CLMs) have weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), with a notable accuracy gap. To enhance BiLMs' performance, the authors propose a collaborative framework that combines multiple BiLMs as external knowledge sources, resulting in improved accuracy in knowledge retrieval tasks."}, 'zh': {'title': 'Áü•ËØÜËÆ∞ÂøÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜÔºöWikiDYK', 'desc': 'Â∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÆÉ‰ª¨ÁöÑÁü•ËØÜËÆ∞ÂøÜËÉΩÂäõ‰ªçÁÑ∂Êú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÁúüÂÆû‰∏ñÁïåÁöÑÂ§ßËßÑÊ®°Áü•ËØÜÊ≥®ÂÖ•Âü∫ÂáÜÔºåÂêç‰∏∫WikiDYKÔºåËÉΩÂ§üÈöèÁùÄÊó∂Èó¥ÁöÑÊé®Áßª‰∏çÊñ≠ÊºîÂèòÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢Ñ„ÄÇWikiDYKÂà©Áî®Áª¥Âü∫ÁôæÁßë‚Äú‰Ω†Áü•ÈÅìÂêó...‚ÄùÊù°ÁõÆ‰∏≠ÊúÄËøëÊ∑ªÂä†ÁöÑ„ÄÅÁî±‰∫∫Á±ªÊí∞ÂÜôÁöÑ‰∫ãÂÆûÔºåÁªèËøá‰∏ìÂÆ∂ÁºñËæëÁöÑ‰∏•Ê†ºÁ≠õÈÄâÔºåÁ°Æ‰øùÂÖ∂ÂèØÈ™åËØÅÊÄßÂíåÊ∏ÖÊô∞ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Âõ†ÊûúËØ≠Ë®ÄÊ®°ÂûãÔºàCLMsÔºâÂú®Áé∞‰ª£LLMs‰∏≠ÊôÆÈÅçÂ≠òÂú®Ôºå‰ΩÜÂÖ∂Áü•ËØÜËÆ∞ÂøÜËÉΩÂäõÊòæËëó‰Ωé‰∫éÂèåÂêëËØ≠Ë®ÄÊ®°ÂûãÔºàBiLMsÔºâÔºåÂáÜÁ°ÆÊÄß‰Ωé23%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11966', 'title': 'Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier', 'url': 'https://huggingface.co/papers/2505.11966', 'abstract': 'Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.', 'score': 3, 'issue_id': 3869, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': 'cd659e075a3efafa', 'authors': ['Jianyuan Zhong', 'Zeju Li', 'Zhijian Xu', 'Xiangyu Wen', 'Kezhi Li', 'Qiang Xu'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.11966.jpg', 'data': {'categories': ['#inference', '#optimization', '#math', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ì–∏–±–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FlexiVe - –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). FlexiVe –≥–∏–±–∫–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –º–µ–∂–¥—É –±—ã—Å—Ç—Ä—ã–º –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≥–∏–±–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±—é–¥–∂–µ—Ç–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–Ω–≤–µ–π–µ—Ä Solve-Detect-Verify –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FlexiVe –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Balancing Speed and Accuracy in LLM Reasoning with FlexiVe', 'desc': 'This paper discusses the challenges of using Large Language Models (LLMs) for complex tasks, particularly the balance between accuracy and computational efficiency. It introduces FlexiVe, a generative verifier that optimizes the use of computational resources by allowing for both quick and thorough reasoning processes. The authors propose a Solve-Detect-Verify pipeline that enhances the integration of FlexiVe, enabling targeted verification and improved feedback during inference. Experimental results demonstrate that FlexiVe significantly improves error detection and reasoning accuracy on various benchmarks compared to traditional methods.'}, 'zh': {'title': 'ÁÅµÊ¥ªÈ™åËØÅÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Â§çÊùÇ‰ªªÂä°Êé®ÁêÜ‰∏≠ÁöÑÂáÜÁ°ÆÊÄß‰∏éËÆ°ÁÆóÊïàÁéá‰πãÈó¥ÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁîüÊàêÈ™åËØÅÂô®FlexiVeÔºåÂÆÉÈÄöËøáÁÅµÊ¥ªÂàÜÈÖçÈ™åËØÅÈ¢ÑÁÆóÔºåÂú®Âø´ÈÄüÂèØÈù†ÁöÑÊÄùÁª¥‰∏éÁªÜËá¥ÊÖ¢ÊÄùÁª¥‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜSolve-Detect-VerifyÁÆ°ÈÅìÔºåËøôÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÊé®ÁêÜÊó∂Èó¥Êâ©Â±ïÊ°ÜÊû∂ÔºåËÉΩÂ§üÊô∫ËÉΩÊï¥ÂêàFlexiVeÔºå‰∏ªÂä®ËØÜÂà´Ëß£ÂÜ≥ÊñπÊ°àÂÆåÊàêÁÇπ‰ª•Ëß¶ÂèëÈíàÂØπÊÄßÁöÑÈ™åËØÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlexiVeÂú®ProcessBench‰∏äËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÂÆö‰ΩçÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈîôËØØÔºåÂπ∂Âú®Â§ö‰∏™Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14178', 'title': 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits', 'url': 'https://huggingface.co/papers/2505.14178', 'abstract': 'Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.', 'score': 2, 'issue_id': 3868, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'f4fdc7fb140f9273', 'authors': ['Xiang Zhang', 'Juntai Cao', 'Jiaqi Wei', 'Yiwei Xu', 'Chenyu You'], 'affiliations': ['Cisco', 'Stony Brook University', 'University of British Columbia', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14178.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#architecture', '#small_models', '#data', '#training'], 'emoji': 'üß©', 'ru': {'title': '–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: —Å–∫—Ä—ã—Ç—ã–π –∫–ª—é—á –∫ —Å–∏–º–≤–æ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –ò–ò', 'desc': "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–∏–º–≤–æ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ '–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö' (Token Awareness) –¥–ª—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–≥–æ, –∫–∞–∫ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞—Ä—É—à–∞–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç –æ–±–æ–±—â–µ–Ω–∏—é —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Å–∏–º–≤–æ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –≥–ª—É–±–æ–∫–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞ —Ç–æ–∫–µ–Ω-—É—Ä–æ–≤–Ω–µ–≤—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏."}, 'en': {'title': 'Tokenization Matters: Unlocking Reasoning in Language Models', 'desc': "This paper explores the importance of tokenization in language models, particularly how it affects reasoning capabilities. It highlights that traditional tokenization methods, like byte-pair encoding (BPE), can obscure essential reasoning units, limiting the model's ability to perform symbolic computation. The authors introduce the concept of Token Awareness, which emphasizes the need for better token granularity to enhance logical alignment and generalization in models. Through experiments on arithmetic and symbolic tasks, they show that models with well-structured token representations can significantly outperform larger models in reasoning tasks."}, 'zh': {'title': 'ÂàÜËØçÁªìÊûÑÂÜ≥ÂÆöÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÔºåÂàÜËØçÔºàTokenizationÔºâÂØπÊé®ÁêÜËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂàÜËØçÊñπÊ°àÔºåÁâπÂà´ÊòØÂü∫‰∫éÂ≠êËØçÁöÑÊñπÊ≥ïÔºàÂ¶ÇÂ≠óËäÇÂØπÁºñÁ†ÅBPEÔºâÔºå‰ºöÂêàÂπ∂ÊàñÊ®°Á≥äÂü∫Êú¨ÁöÑÊé®ÁêÜÂçïÂÖÉÔºå‰ªéËÄåÂ¶®Á¢çÁ¨¶Âè∑ËÆ°ÁÆó„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‚ÄúToken Awareness‚ÄùÁöÑÊ¶ÇÂøµÔºåÂº∫Ë∞É‰∫ÜÂàÜËØçÁ≤íÂ∫¶‰∏ç‰Ω≥Â¶Ç‰ΩïÂπ≤Êâ∞ÈÄªËæëÂØπÈΩêÔºåÈòªÁ¢çÊ®°ÂûãÁöÑÁ¨¶Âè∑Á®ãÂ∫èÊ≥õÂåñ„ÄÇÈÄöËøáÂØπÁÆóÊúØÂíåÁ¨¶Âè∑‰ªªÂä°ÁöÑÁ≥ªÁªüËØÑ‰º∞ÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂàÜËØçÁªìÊûÑÊòæËëóÂΩ±ÂìçÊé®ÁêÜÊÄßËÉΩÔºåËæÉÂ∞èÁöÑÊ®°ÂûãÂú®ÂØπÈΩêÊ†ºÂºè‰∏ãËÉΩÂ§üË∂ÖË∂äÊõ¥Â§ßÁöÑÁ≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13010', 'title': 'To Bias or Not to Bias: Detecting bias in News with bias-detector', 'url': 'https://huggingface.co/papers/2505.13010', 'abstract': "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.", 'score': 2, 'issue_id': 3877, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '9f759ef4e436d2f0', 'authors': ['Himel Ghosh', 'Ahmed Mosharafa', 'Georg Groh'], 'affiliations': ['Sapienza University of Rome, Italy', 'Technical University of Munich (TUM), Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.13010.jpg', 'data': {'categories': ['#training', '#multimodal', '#interpretability', '#ethics', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–¢–æ—á–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –°–ú–ò —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ RoBERTa –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ-–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç BABE. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±—ã–ª–∏ –ø–æ–∫–∞–∑–∞–Ω—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é DA-RoBERTa. –ê–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª—å –∏–∑–±–µ–≥–∞–µ—Ç —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏ –∑–∞—Ä—è–∂–µ–Ω–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–Ω–≤–µ–π–µ—Ä, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º —Ç–∏–ø–æ–≤ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏–∞-–ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Media Bias Detection with Context-Aware Models', 'desc': "This paper addresses the challenge of detecting media bias by fine-tuning a RoBERTa-based model on the BABE dataset, which is annotated by experts. The authors demonstrate significant performance improvements over a baseline model using statistical tests, indicating the effectiveness of their approach. They also analyze the model's attention mechanisms, showing it focuses on contextually relevant information rather than being overly sensitive to biased language. The study proposes a comprehensive pipeline for media bias detection and discusses future directions for enhancing bias classification and model interpretability."}, 'zh': {'title': 'ÊèêÂçáÂ™í‰ΩìÂÅèËßÅÊ£ÄÊµãÁöÑÊô∫ËÉΩÂåñÊñπÊ≥ï', 'desc': 'Êú¨Á†îÁ©∂ÈíàÂØπÂ™í‰ΩìÂÅèËßÅÊ£ÄÊµãËøô‰∏ÄÈáçË¶Å‰ªªÂä°ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éRoBERTaÊ®°ÂûãÁöÑÂè•Â≠êÁ∫ßÂÅèËßÅÂàÜÁ±ªÊñπÊ≥ï„ÄÇÊàë‰ª¨Âú®‰∏ìÂÆ∂Ê†áÊ≥®ÁöÑBABEÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂæÆË∞ÉÔºåÂπ∂ÈÄöËøáÁªüËÆ°ÊµãËØïÈ™åËØÅ‰∫ÜÊ®°ÂûãÊÄßËÉΩÁöÑÊòæËëóÊèêÂçá„ÄÇÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõÂàÜÊûêË°®ÊòéÔºåÂÆÉËÉΩÂ§üÊúâÊïàÈÅøÂÖçÂØπÊîøÊ≤ªÊïèÊÑüËØçÁöÑËøáÂ∫¶ÊïèÊÑüÔºåËÄåÊòØÊõ¥ÂÖ≥Ê≥®‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÁöÑËØçÊ±á„ÄÇÂ∞ΩÁÆ°ÂèóÈôê‰∫éÂè•Â≠êÁ∫ßÂàÜÊûêÂíåÊï∞ÊçÆÈõÜËßÑÊ®°ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂÅèËßÅÊ£ÄÊµã‰∏≠Â±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂèØËß£ÈáäÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10176', 'title': 'Incorporating brain-inspired mechanisms for multimodal learning in\n  artificial intelligence', 'url': 'https://huggingface.co/papers/2505.10176', 'abstract': 'Multimodal learning enhances the perceptual capabilities of cognitive systems by integrating information from different sensory modalities. However, existing multimodal fusion research typically assumes static integration, not fully incorporating key dynamic mechanisms found in the brain. Specifically, the brain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal cues yield stronger multisensory integration benefits; conversely, when individual modal cues are stronger, the effect of fusion is diminished. This mechanism enables biological systems to achieve robust cognition even with scarce or noisy perceptual cues. Inspired by this biological mechanism, we explore the relationship between multimodal output and information from individual modalities, proposing an inverse effectiveness driven multimodal fusion (IEMF) strategy. By incorporating this strategy into neural networks, we achieve more efficient integration with improved model performance and computational efficiency, demonstrating up to 50% reduction in computational cost across diverse fusion methods. We conduct experiments on audio-visual classification, continual learning, and question answering tasks to validate our method. Results consistently demonstrate that our method performs excellently in these tasks. To verify universality and generalization, we also conduct experiments on Artificial Neural Networks (ANN) and Spiking Neural Networks (SNN), with results showing good adaptability to both network types. Our research emphasizes the potential of incorporating biologically inspired mechanisms into multimodal networks and provides promising directions for the future development of multimodal artificial intelligence. The code is available at https://github.com/Brain-Cog-Lab/IEMF.', 'score': 2, 'issue_id': 3878, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '6384b169333ad553', 'authors': ['Xiang He', 'Dongcheng Zhao', 'Yang Li', 'Qingqun Kong', 'Xin Yang', 'Yi Zeng'], 'affiliations': ['Brain-inspired Cognitive AI Lab, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Center for Long-term Al, Beijing, China', 'Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Chinese Academy of Sciences, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.10176.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#audio', '#agi'], 'emoji': 'üß†', 'ru': {'title': '–ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—É—é –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –æ–±—Ä–∞—Ç–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –º–æ–∑–≥–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ IEMF (Inverse Effectiveness driven Multimodal Fusion), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ IEMF —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –¥–æ 50% –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–∞–∫ –∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º (ANN), —Ç–∞–∫ –∏ –∫ —Å–ø–∞–π–∫–æ–≤—ã–º –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º (SNN).'}, 'en': {'title': 'Enhancing Multimodal Learning with Brain-Inspired Fusion', 'desc': "This paper discusses a new approach to multimodal learning that mimics how the human brain processes information from different senses. It introduces the Inverse Effectiveness Driven Multimodal Fusion (IEMF) strategy, which enhances the integration of sensory data by leveraging the brain's ability to combine weaker signals more effectively. The authors demonstrate that this method can significantly improve the performance and efficiency of neural networks, achieving up to a 50% reduction in computational costs. Experiments across various tasks show that IEMF is adaptable and effective in both Artificial Neural Networks and Spiking Neural Networks, highlighting the benefits of biologically inspired techniques in artificial intelligence."}, 'zh': {'title': 'ÈÄÜÊïàÂ∫îÈ©±Âä®ÁöÑÂ§öÊ®°ÊÄÅËûçÂêàÁ≠ñÁï•', 'desc': 'Â§öÊ®°ÊÄÅÂ≠¶‰π†ÈÄöËøáÊï¥ÂêàÊù•Ëá™‰∏çÂêåÊÑüÂÆòÁöÑ‰ø°ÊÅØÔºåÂ¢ûÂº∫‰∫ÜËÆ§Áü•Á≥ªÁªüÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅËûçÂêàÁ†îÁ©∂ÈÄöÂ∏∏ÂÅáËÆæÈùôÊÄÅÊï¥ÂêàÔºåÊú™ËÉΩÂÖÖÂàÜËÄÉËôëÂ§ßËÑë‰∏≠ÁöÑÂÖ≥ÈîÆÂä®ÊÄÅÊú∫Âà∂„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ§ßËÑëË°®Áé∞Âá∫ÈÄÜÊïàÂ∫îÁé∞Ë±°ÔºåÂç≥ËæÉÂº±ÁöÑÂçïÊ®°ÊÄÅÁ∫øÁ¥¢‰ºöÂ∏¶Êù•Êõ¥Âº∫ÁöÑÂ§öÊÑüÂÆòËûçÂêàÊïàÁõäÔºõÁõ∏ÂèçÔºåÂΩìÂçï‰∏™Ê®°ÊÄÅÁ∫øÁ¥¢ËæÉÂº∫Êó∂ÔºåËûçÂêàÊïàÊûú‰ºöÂáèÂº±„ÄÇÂèóËøô‰∏ÄÁîüÁâ©Êú∫Âà∂ÁöÑÂêØÂèëÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈÄÜÊïàÂ∫îÁöÑÂ§öÊ®°ÊÄÅËûçÂêàÁ≠ñÁï•ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13778', 'title': 'CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM\n  APIs', 'url': 'https://huggingface.co/papers/2505.13778', 'abstract': 'As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.', 'score': 1, 'issue_id': 3884, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': 'e61730a128c76920', 'authors': ['Guoheng Sun', 'Ziyao Wang', 'Bowei Tian', 'Meng Liu', 'Zheyu Shen', 'Shwai He', 'Yexiao He', 'Wanghao Ye', 'Yiting Wang', 'Ang Li'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2505.13778.jpg', 'data': {'categories': ['#dataset', '#optimization', '#inference', '#reasoning', '#hallucinations', '#rl', '#security'], 'emoji': 'üïµÔ∏è', 'ru': {'title': 'CoIn: –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ —á–µ—Å—Ç–Ω–æ—Å—Ç—å –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CoIn - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞—É–¥–∏—Ç–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). CoIn —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —Å–∫—Ä—ã–≤–∞—é—Ç—Å—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö–µ—à-–¥–µ—Ä–µ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å—Ñ–∞–±—Ä–∏–∫–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CoIn –º–æ–∂–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–æ 94.7%.'}, 'en': {'title': 'Ensuring Transparency in LLM Billing with CoIn', 'desc': 'This paper discusses the challenges of transparency in large language models (LLMs) that use complex reasoning processes, often hidden from users. These models, enhanced through reinforcement learning, can perform better on difficult tasks but may lead to inflated costs due to undisclosed reasoning tokens. The authors introduce CoIn, a framework designed to verify the authenticity and quantity of these hidden tokens, ensuring users are not overcharged. Through experiments, CoIn demonstrates a high success rate in detecting token inflation, promoting fairness and transparency in LLM billing practices.'}, 'zh': {'title': 'ÊèêÂçáLLMÊúçÂä°ÈÄèÊòéÂ∫¶ÁöÑÈ™åËØÅÊ°ÜÊû∂', 'desc': 'ÈöèÁùÄÂêéËÆ≠ÁªÉÊäÄÊúØÁöÑÂèëÂ±ïÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâË∂äÊù•Ë∂äÂ§öÂú∞Â¢ûÂº∫‰∫ÜÁªìÊûÑÂåñÁöÑÂ§öÊ≠•È™§Êé®ÁêÜËÉΩÂäõÔºåËøôÈÄöÂ∏∏ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËøõË°å‰ºòÂåñ„ÄÇËøô‰∫õÂ¢ûÂº∫Êé®ÁêÜÁöÑÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏äË°®Áé∞‰ºò‰∫éÊ†áÂáÜLLMsÔºåÂπ∂‰∏îÁé∞Âú®ÊîØÊíëÁùÄËÆ∏Â§öÂïÜ‰∏öLLM API„ÄÇÁÑ∂ËÄåÔºå‰∏∫‰∫Ü‰øùÊä§‰∏ìÊúâË°å‰∏∫Âπ∂ÂáèÂ∞ëÂÜóÈïøÔºåÊèê‰æõËÄÖÈÄöÂ∏∏Âú®ËøîÂõûÊúÄÁªàÁ≠îÊ°àÊó∂ÈöêËóèÊé®ÁêÜËøáÁ®ã„ÄÇËøôÁßç‰∏çÈÄèÊòéÊÄßÂØºËá¥‰∫ÜÈÄèÊòéÂ∫¶Áº∫Âè£ÔºåÁî®Êà∑‰∏∫‰∏çÂèØËßÅÁöÑÊé®ÁêÜ‰ª§Áâå‰ªòË¥πÔºåËÄåËøô‰∫õ‰ª§ÁâåÂæÄÂæÄÂç†ÊçÆ‰∫ÜÂ§ßÈÉ®ÂàÜÊàêÊú¨ÔºåÁî®Êà∑Âç¥Êó†Ê≥ïÈ™åËØÅÂÖ∂ÁúüÂÆûÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.13731', 'title': 'GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization', 'url': 'https://huggingface.co/papers/2505.13731', 'abstract': 'Worldwide image geolocalization-the task of predicting GPS coordinates from images taken anywhere on Earth-poses a fundamental challenge due to the vast diversity in visual content across regions. While recent approaches adopt a two-stage pipeline of retrieving candidates and selecting the best match, they typically rely on simplistic similarity heuristics and point-wise supervision, failing to model spatial relationships among candidates. In this paper, we propose GeoRanker, a distance-aware ranking framework that leverages large vision-language models to jointly encode query-candidate interactions and predict geographic proximity. In addition, we introduce a multi-order distance loss that ranks both absolute and relative distances, enabling the model to reason over structured spatial relationships. To support this, we curate GeoRanking, the first dataset explicitly designed for geographic ranking tasks with multimodal candidate information. GeoRanker achieves state-of-the-art results on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly outperforming current best methods.', 'score': 1, 'issue_id': 3885, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 –º–∞—è', 'en': 'May 19', 'zh': '5Êúà19Êó•'}, 'hash': '0e5f5736d8ac2254', 'authors': ['Pengyue Jia', 'Seongheon Park', 'Song Gao', 'Xiangyu Zhao', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison', 'Department of Data Science, City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.13731.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#benchmark', '#games'], 'emoji': 'üåé', 'ru': {'title': 'GeoRanker: –¢–æ—á–Ω–∞—è –≥–µ–æ–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GeoRanker - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–æ–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∑–∞–ø—Ä–æ—Å–∞ –∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö. –¢–∞–∫–∂–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö GeoRanking —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∑–∞–¥–∞—á –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö.'}, 'en': {'title': 'GeoRanker: Revolutionizing Image Geolocalization with Distance-Aware Ranking', 'desc': 'This paper addresses the challenge of predicting GPS coordinates from images, known as image geolocalization, which is complicated by the diverse visual content found in different regions. The authors introduce GeoRanker, a novel ranking framework that utilizes large vision-language models to better understand the interactions between image queries and their potential geographic candidates. By implementing a multi-order distance loss, GeoRanker effectively ranks both absolute and relative distances, allowing it to capture complex spatial relationships among candidates. The paper also presents GeoRanking, a new dataset tailored for geographic ranking tasks, and demonstrates that GeoRanker achieves superior performance on established benchmarks compared to existing methods.'}, 'zh': {'title': 'GeoRankerÔºöÊô∫ËÉΩÂú∞ÁêÜÂÆö‰ΩçÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæÂÉèÂú∞ÁêÜÂÆö‰ΩçÊñπÊ≥ïÔºåÁß∞‰∏∫GeoRankerÔºåÊó®Âú®‰ªéÂõæÂÉè‰∏≠È¢ÑÊµãGPSÂùêÊ†á„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåGeoRankerÂà©Áî®Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊù•ÂÖ±ÂêåÁºñÁ†ÅÊü•ËØ¢ÂíåÂÄôÈÄâÂõæÂÉè‰πãÈó¥ÁöÑ‰∫§‰∫íÔºåÂπ∂È¢ÑÊµãÂú∞ÁêÜÊé•ËøëÂ∫¶„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂ§öÈò∂Ë∑ùÁ¶ªÊçüÂ§±ÔºåËÉΩÂ§üÂØπÁªùÂØπÂíåÁõ∏ÂØπË∑ùÁ¶ªËøõË°åÊéíÂêçÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Âª∫Ê®°ÂÄôÈÄâÂõæÂÉè‰πãÈó¥ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂàõÂª∫‰∫ÜGeoRankingÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éÂú∞ÁêÜÊéíÂêç‰ªªÂä°ÔºåÊîØÊåÅÂ§öÊ®°ÊÄÅÂÄôÈÄâ‰ø°ÊÅØÁöÑÂ§ÑÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.12154', 'title': 'Learning to Highlight Audio by Watching Movies', 'url': 'https://huggingface.co/papers/2505.12154', 'abstract': 'Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/.', 'score': 1, 'issue_id': 3882, 'pub_date': '2025-05-17', 'pub_date_card': {'ru': '17 –º–∞—è', 'en': 'May 17', 'zh': '5Êúà17Êó•'}, 'hash': '8f03af3997e1149d', 'authors': ['Chao Huang', 'Ruohan Gao', 'J. M. F. Tsang', 'Jan Kurcius', 'Cagdas Bilen', 'Chenliang Xu', 'Anurag Kumar', 'Sanjeel Parekh'], 'affiliations': ['Meta Reality Labs Research', 'University of Maryland, College Park', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.12154.jpg', 'data': {'categories': ['#multimodal', '#video', '#audio', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–ì–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –≤–∏–∑—É–∞–ª—å–Ω–æ-—É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–ª—å–º–æ–≤ —Å —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –ø–ª–æ—Ö–æ —Å–º–∏–∫—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'Harmonizing Audio and Visuals for Engaging Video Content', 'desc': 'This paper addresses the imbalance in advancements between visual and audio elements in video content creation. It introduces a new task called visually-guided acoustic highlighting, which aims to enhance audio based on the visual cues from the video. The authors propose a transformer-based multimodal framework to achieve this, supported by a new dataset called the muddy mix dataset that simulates real-world audio mixing challenges. Their approach shows significant improvements over existing methods in both quantitative metrics and subjective assessments, highlighting the importance of integrating visual and audio elements for a cohesive viewing experience.'}, 'zh': {'title': 'ËßÜËßâÂºïÂØºÈü≥È¢ëÈ´ò‰∫ÆÔºåÊèêÂçáËßÜÂê¨‰ΩìÈ™åÔºÅ', 'desc': 'ËøëÂπ¥Êù•ÔºåËßÜÈ¢ëÂÜÖÂÆπÁöÑÂàõ‰ΩúÂíåÊ∂àË¥πÊòæËëóÂ¢ûÂä†„ÄÇÂà∂‰ΩúÂºï‰∫∫ÂÖ•ËÉúÁöÑÂÜÖÂÆπÈúÄË¶ÅÁ≤æÂøÉÁ≠ñÂàíËßÜËßâÂíåÈü≥È¢ëÂÖÉÁ¥†„ÄÇ‰∏∫‰∫ÜÂº•Ë°•ËßÜËßâÂíåÈü≥È¢ë‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞‰ªªÂä°ÔºöËßÜËßâÂºïÂØºÁöÑÈü≥È¢ëÈ´ò‰∫ÆÔºåÊó®Âú®Ê†πÊçÆËßÜÈ¢ëÂÜÖÂÆπË∞ÉÊï¥Èü≥È¢ëÔºå‰ª•ÂàõÈÄ†Êõ¥ÂíåË∞êÁöÑËßÜÂê¨‰ΩìÈ™å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂèòÊç¢Âô®ÁöÑÂ§öÊ®°ÊÄÅÊ°ÜÊû∂ÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•ÊîØÊåÅÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11754', 'title': 'Masking in Multi-hop QA: An Analysis of How Language Models Perform with\n  Context Permutation', 'url': 'https://huggingface.co/papers/2505.11754', 'abstract': "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.", 'score': 1, 'issue_id': 3878, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'dcf59ca0d93ac6e2', 'authors': ['Wenyu Huang', 'Pavlos Vougiouklis', 'Mirella Lapata', 'Jeff Z. Pan'], 'affiliations': ['Huawei Edinburgh Research Centre, Poisson Lab, CSI, UK', 'School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.11754.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#optimization', '#multimodal', '#training'], 'emoji': 'üß†', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (MHQA) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ç–∏–ø–∞ encoder-decoder –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥–µ–∫–æ–¥–µ—Ä-–º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö MHQA, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä. –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π, –ø—Ä–∏—á–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç—Å—è –ø—Ä–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –ø–æ—Ä—è–¥–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ–π –º–∞—Å–∫–∏ –≤ –¥–µ–∫–æ–¥–µ—Ä-–º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ MHQA.'}, 'en': {'title': 'Unlocking Multi-hop Reasoning in Language Models', 'desc': 'This paper investigates the challenges of Multi-hop Question Answering (MHQA) using Language Models (LMs). It highlights that while LMs excel in standard question-answering, their performance can be limited by causal masking when reasoning across multiple information sources. The authors find that encoder-decoder models, like Flan-T5, outperform smaller causal decoder-only models in MHQA tasks, especially when the order of documents matches the reasoning chain. Additionally, they propose modifications to causal masks to enhance performance and analyze attention weights, discovering that higher attention values correlate with correct answers, which can be used to improve LM effectiveness.'}, 'zh': {'title': 'Â§öË∑≥Êé®ÁêÜÔºåÊèêÂçáÈóÆÁ≠îËÉΩÂäõÔºÅ', 'desc': 'Â§öË∑≥ÈóÆÈ¢òÂõûÁ≠îÔºàMHQAÔºâÂ¢ûÂä†‰∫ÜÈóÆÁ≠îÁöÑÂ§çÊùÇÊÄßÔºå‰ΩøÂÖ∂Êõ¥ÂÖ∑ÊåëÊàòÊÄß„ÄÇËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂú®Â§ÑÁêÜÂ§ö‰∏™ÊêúÁ¥¢ÁªìÊûúÊó∂Ôºå‰∏ç‰ªÖÈúÄË¶ÅÊ£ÄÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØÔºåËøòÈúÄÂú®‰ø°ÊÅØÊ∫ê‰πãÈó¥ËøõË°åÂ§öË∑≥Êé®ÁêÜ„ÄÇÂ∞ΩÁÆ°LMÂú®‰º†ÁªüÈóÆÁ≠î‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂõ†ÊûúÊé©Á†ÅÂèØËÉΩ‰ºöÂ¶®Á¢çÂÖ∂Âú®Â§çÊùÇ‰∏ä‰∏ãÊñá‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÂèëÁé∞ÔºåÁºñÁ†Å-Ëß£Á†ÅÊ®°ÂûãÂú®MHQA‰ªªÂä°‰∏≠ÈÄöÂ∏∏‰ºò‰∫é‰ªÖ‰ΩøÁî®Âõ†ÊûúËß£Á†ÅÂô®ÁöÑÊ®°ÂûãÔºå‰∏îÈÄöËøáË∞ÉÊï¥Âõ†ÊûúÊé©Á†ÅÂèØ‰ª•ÊúâÊïàÊèêÂçáÂêéËÄÖÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11730', 'title': 'Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling', 'url': 'https://huggingface.co/papers/2505.11730', 'abstract': 'Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.', 'score': 1, 'issue_id': 3883, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': 'bd83555174b293a3', 'authors': ['Hao Mark Chen', 'Guanxi Lu', 'Yasuyuki Okoshi', 'Zhiwen Mo', 'Masato Motomura', 'Hongxiang Fan'], 'affiliations': ['Imperial College London, UK', 'Institute of Science Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.11730.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#reasoning', '#inference'], 'emoji': 'üîç', 'ru': {'title': '–ì–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º Variable Granularity Search (VG-Search) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). VG-Search –æ–±–æ–±—â–∞–µ—Ç –º–µ—Ç–æ–¥—ã beam search –∏ Best-of-N sampling, –ø–æ–∑–≤–æ–ª—è—è –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–∂–µ—Ç –ø–æ–≤—ã—Å–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ VG-Search –¥–æ—Å—Ç–∏–≥–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 3.6% –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ 52%.'}, 'en': {'title': 'Dynamic Verification for Enhanced Language Model Efficiency', 'desc': 'This paper explores the concept of Test-time Scaling (TTS) in large language models (LLMs) and emphasizes the importance of verification in enhancing reasoning performance and computational efficiency. The authors introduce a novel approach called Variable Granularity Search (VG-Search), which allows for dynamic adjustment of verification frequency during the generation process. By systematically varying the granularity of verification, VG-Search improves both the accuracy and efficiency of LLMs compared to traditional methods like Beam Search and Best-of-N sampling. The results show significant gains in accuracy while drastically reducing computational costs, paving the way for more efficient LLM applications.'}, 'zh': {'title': 'Âä®ÊÄÅÈ™åËØÅÁ≤íÂ∫¶ÊèêÂçáÊé®ÁêÜÊïàÁéá', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊµãËØïÊó∂Áº©ÊîæÔºàTTSÔºâÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÈ™åËØÅÂú®TTS‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂΩ±ÂìçÊé®ÁêÜÊÄßËÉΩÂíåËÆ°ÁÆóÊïàÁéá„ÄÇÊàë‰ª¨È¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞Á†îÁ©∂‰∫ÜÈ™åËØÅÁ≤íÂ∫¶ÁöÑÂΩ±ÂìçÔºåÂç≥Âú®ÁîüÊàêËøáÁ®ã‰∏≠È™åËØÅÂô®ÁöÑË∞ÉÁî®È¢ëÁéá„ÄÇÈÄöËøáÂºïÂÖ•ÂèØÂèòÁ≤íÂ∫¶ÊêúÁ¥¢ÔºàVG-SearchÔºâÁÆóÊ≥ïÔºåÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂä®ÊÄÅÈÄâÊã©Á≤íÂ∫¶ÂèÇÊï∞ÂèØ‰ª•ÊèêÈ´òËÆ°ÁÆóÊïàÁéáÔºåÂπ∂Âú®ÂáÜÁ°ÆÊÄß‰∏äË∂ÖËøá‰º†ÁªüÁöÑÊùüÊêúÁ¥¢ÂíåÊúÄ‰Ω≥NÈááÊ†∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10588', 'title': 'Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation', 'url': 'https://huggingface.co/papers/2505.10588', 'abstract': 'This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.', 'score': 1, 'issue_id': 3868, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': 'cdc9a4f93d65b071', 'authors': ['Manisha Mehta', 'Fausto Giunchiglia'], 'affiliations': ['University of Trento, Trento, Italy', 'Warren Hyde Middle School, Cupertino, California, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.10588.jpg', 'data': {'categories': ['#healthcare', '#interpretability', '#benchmark', '#ethics', '#multimodal', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è —è–∑—ã–∫–æ–≤–æ–π –±–∞—Ä—å–µ—Ä: –ò–ò –Ω–∞ —Å—Ç—Ä–∞–∂–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ò–ò-—Å–∏—Å—Ç–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ü–∏—Ñ—Ä–æ–≤–æ–π —è–∑—ã–∫ –ø–æ–∫–æ–ª–µ–Ω–∏—è –ê–ª—å—Ñ–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —á–µ—Ç—ã—Ä–µ –≤–µ–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Ñ–æ—Ä–º –¥–æ–º–æ–≥–∞—Ç–µ–ª—å—Å—Ç–≤ –∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –≤ –¥–∏—Å–∫—É—Ä—Å–µ —ç—Ç–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è. –†–∞–±–æ—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤—ã—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∫–æ–ª–µ–Ω–∏—è –ê–ª—å—Ñ–∞ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É framework'–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –±–∞–∑–µ –ò–ò. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –æ—Å—Ç—Ä—É—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏ —Å–∏—Å—Ç–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–æ–ª–æ–¥–µ–∂–∏."}, 'en': {'title': 'Bridging the Gap: Enhancing AI Safety for Generation Alpha', 'desc': "This research evaluates how AI systems understand the unique digital language of Generation Alpha, who are growing up with AI technology. It highlights the risks they face online due to their distinct communication styles, influenced by gaming and memes, which can hide harmful interactions from both humans and automated systems. The study tests four AI models on their ability to detect subtle harassment in Gen Alpha's online expressions, revealing significant gaps in their comprehension. The findings emphasize the need for improved AI moderation tools that are better suited to protect youth in their digital environments."}, 'zh': {'title': 'ÈáçÂ°ëÂÆâÂÖ®Á≥ªÁªüÔºå‰øùÊä§ÈòøÂ∞îÊ≥ï‰∏ñ‰ª£ÁöÑÊï∞Â≠ó‰∫§ÊµÅ', 'desc': 'Êú¨Á†îÁ©∂Áã¨ÁâπÂú∞ËØÑ‰º∞‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂ¶Ç‰ΩïËß£ËØªÈòøÂ∞îÊ≥ï‰∏ñ‰ª£Ôºà2010-2024Âπ¥Âá∫ÁîüÔºâÁöÑÊï∞Â≠óËØ≠Ë®Ä„ÄÇÈòøÂ∞îÊ≥ï‰∏ñ‰ª£ÊòØÈ¶ñ‰∏™‰∏é‰∫∫Â∑•Êô∫ËÉΩÂÖ±ÂêåÊàêÈïøÁöÑÁæ§‰ΩìÔºå‰ªñ‰ª¨Âú®Ê≤âÊµ∏ÂºèÊï∞Â≠óÁéØÂ¢É‰∏≠Èù¢‰∏¥Êñ∞ÁöÑÂú®Á∫øÈ£éÈô©„ÄÇÁ†îÁ©∂ÂàÜÊûê‰∫ÜÂõõÁßçÈ¢ÜÂÖàÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÔºàGPT-4„ÄÅClaude„ÄÅGeminiÂíåLlama 3ÔºâÂú®ËØÜÂà´ÈöêËóèÁöÑÈ™öÊâ∞ÂíåÊìçÊéßÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂ÁªìÊûúÊòæÁ§∫ÔºåÁé∞ÊúâÁöÑÂÆâÂÖ®Â∑•ÂÖ∑Êú™ËÉΩÊúâÊïàÁêÜËß£ÈòøÂ∞îÊ≥ï‰∏ñ‰ª£ÁöÑÁã¨Áâπ‰∫§ÊµÅÊñπÂºèÔºåÂº∫Ë∞É‰∫ÜÈáçÊñ∞ËÆæËÆ°ÂÆâÂÖ®Á≥ªÁªüÁöÑÁ¥ßËø´ÊÄßÔºå‰ª•Êõ¥Â•ΩÂú∞‰øùÊä§Âπ¥ËΩªÁî®Êà∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14633', 'title': 'Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas', 'url': 'https://huggingface.co/papers/2505.14633', 'abstract': "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.", 'score': 0, 'issue_id': 3884, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '40e16ce405eaf398', 'authors': ['Yu Ying Chiu', 'Zhilin Wang', 'Sharan Maiya', 'Yejin Choi', 'Kyle Fish', 'Sydney Levine', 'Evan Hubinger'], 'affiliations': ['Anthropic', 'Cambridge', 'Harvard', 'MIT', 'NVIDIA', 'Stanford', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.14633.jpg', 'data': {'categories': ['#dataset', '#ethics', '#benchmark', '#alignment', '#healthcare'], 'emoji': 'üß≠', 'ru': {'title': '–¶–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫ –∫–æ–º–ø–∞—Å –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—è–≤–ª–µ–Ω–∏—é —Ä–∏—Å–∫–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –ò–ò. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é LitmusValues –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –ò–ò –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π. –û–Ω–∏ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö AIRiskDilemmas, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –¥—Ä—É–≥ –¥—Ä—É–≥—É –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∏—Å–∫–æ–≤ –ò–ò. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –∞–Ω–∞–ª–∏–∑ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ, —Ç–∞–∫ –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ò–ò.'}, 'en': {'title': 'Uncovering AI Risks Through Value Prioritization', 'desc': 'This paper addresses the growing challenge of detecting risks in advanced AI models, particularly those that use techniques like Alignment Faking to avoid detection. The authors propose a method called LitmusValues, which evaluates AI models based on their adherence to various value classes, serving as an early warning system for risky behaviors. They introduce AIRiskDilemmas, a set of scenarios that highlight conflicts between different values, relevant to AI safety. By analyzing the value prioritization of AI models through their choices in these dilemmas, the study reveals how even benign values can indicate potential risks in both known and unknown contexts.'}, 'zh': {'title': 'ËØÜÂà´‰∫∫Â∑•Êô∫ËÉΩÈ£éÈô©ÁöÑ‰ª∑ÂÄºËßÇ‰ºòÂÖàÁ∫ß', 'desc': 'ÈöèÁùÄÊõ¥Âº∫Â§ßÁöÑÊ®°ÂûãÂá∫Áé∞ÔºåÊ£ÄÊµã‰∫∫Â∑•Êô∫ËÉΩÈ£éÈô©ÂèòÂæóÊõ¥Âä†Âõ∞Èöæ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫LitmusValuesÁöÑËØÑ‰º∞ÁÆ°ÈÅìÔºåÁî®‰∫éÊè≠Á§∫‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÂú®‰∏çÂêå‰ª∑ÂÄºÁ±ªÂà´‰∏äÁöÑ‰ºòÂÖàÁ∫ß„ÄÇÈÄöËøáÊî∂ÈõÜAIRiskDilemmasÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏ÄÁ≥ªÂàóÊ∂âÂèä‰ª∑ÂÄºÂÜ≤Á™ÅÁöÑÂõ∞Â¢ÉÔºå‰ª•ËØÑ‰º∞‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆâÂÖ®È£éÈô©„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLitmusValues‰∏≠ÁöÑ‰ª∑ÂÄºËßÇÂèØ‰ª•ÊúâÊïàÈ¢ÑÊµã‰∫∫Â∑•Êô∫ËÉΩÂú®Â∑≤Áü•ÂíåÊú™Áü•È£éÈô©Ë°å‰∏∫‰∏≠ÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14629', 'title': 'KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.14629', 'abstract': 'Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.', 'score': 0, 'issue_id': 3880, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'cf8c6e04379454db', 'authors': ['Fnu Mohbat', 'Mohammed J Zaki'], 'affiliations': ['Rensselaer Polytechnic Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.14629.jpg', 'data': {'categories': ['#science', '#benchmark', '#open_source', '#graphs', '#dataset', '#multimodal', '#games'], 'emoji': 'üç≥', 'ru': {'title': 'KERL: —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –Ω–∞ –∫—É—Ö–Ω–µ —Å –≥—Ä–∞—Ñ–∞–º–∏ –∑–Ω–∞–Ω–∏–π –∏ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É KERL, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é –≥—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π –æ –µ–¥–µ –∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ø–∏—Ç–∞–Ω–∏—é. KERL –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≥—Ä–∞—Ñ–µ –∑–Ω–∞–Ω–∏–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–µ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ä–µ—Ü–µ–ø—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∏–∫—Ä–æ–Ω—É—Ç—Ä–∏–µ–Ω—Ç–∞—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ—Ü–µ–ø—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ KERL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±–ª—é–¥, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—Ü–µ–ø—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ –ø–∏—â–µ–≤–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'KERL: Smart Food Recommendations with LLMs and Knowledge Graphs', 'desc': 'This paper presents KERL, a novel system that combines large language models (LLMs) with food-related knowledge graphs (KGs) to enhance food recommendations and recipe generation. KERL processes natural language queries by extracting relevant entities and retrieving corresponding subgraphs from the KG, which are then used as context for the LLM to generate personalized recipes and nutritional information. The system not only recommends recipes but also provides detailed cooking steps and micro-nutritional data tailored to user preferences. Experimental results demonstrate that KERL outperforms existing methods, showcasing its effectiveness in delivering comprehensive food-related solutions.'}, 'zh': {'title': 'KERLÔºöÊô∫ËÉΩÈ£üÂìÅÊé®Ëçê‰∏éÈÖçÊñπÁîüÊàêÁöÑËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜKERLÁ≥ªÁªüÔºåÂÆÉÁªìÂêà‰∫ÜÈ£üÂìÅÁü•ËØÜÂõæË∞±ÔºàKGsÔºâÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºå‰ª•Êèê‰æõ‰∏™ÊÄßÂåñÁöÑÈ£üÂìÅÊé®ËçêÂíåÁîüÊàêÈÖçÊñπÂèäÂæÆÈáèËê•ÂÖª‰ø°ÊÅØ„ÄÇKERLËÉΩÂ§ü‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢ò‰∏≠ÊèêÂèñÂÆû‰ΩìÔºåÂπ∂‰ªéÁü•ËØÜÂõæË∞±‰∏≠Ê£ÄÁ¥¢Â≠êÂõæÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•Âà∞LLM‰∏≠Ôºå‰ª•ÈÄâÊã©Êª°Ë∂≥Á∫¶ÊùüÊù°‰ª∂ÁöÑÈÖçÊñπ„ÄÇÁ≥ªÁªüËøòÁîüÊàêÊØè‰∏™ÈÖçÊñπÁöÑÁÉπÈ•™Ê≠•È™§ÂíåËê•ÂÖª‰ø°ÊÅØ„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜKGÂ¢ûÂº∫ÁöÑLLMÂú®È£üÂìÅÊé®Ëçê„ÄÅÈÖçÊñπÁîüÊàêÂíåËê•ÂÖªÂàÜÊûêÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14556', 'title': 'Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI', 'url': 'https://huggingface.co/papers/2505.14556', 'abstract': 'Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.', 'score': 0, 'issue_id': 3880, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': 'a52590cd595c5afe', 'authors': ['Marl√®ne Careil', 'Yohann Benchetrit', 'Jean-R√©mi King'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.14556.jpg', 'data': {'categories': ['#science', '#data', '#diffusion', '#architecture', '#training', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—Å–ª–µ–π –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Å Dynadiff', 'desc': 'Dynadiff - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö —Ñ–ú–†–¢. –û–Ω–∞ —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏ —Ñ–ú–†–¢. –ú–æ–¥–µ–ª—å –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —ç–≤–æ–ª—é—Ü–∏—é –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–∑–≥–∞. Dynadiff –∑–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤—É –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –º–æ–∑–≥–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π.'}, 'en': {'title': 'Revolutionizing Brain-to-Image Decoding with Dynadiff', 'desc': 'This paper presents Dynadiff, a novel single-stage diffusion model for decoding images from dynamic fMRI recordings. Unlike traditional methods that rely on complex multi-stage processes, Dynadiff simplifies the training process and enhances performance on time-resolved brain signals. The model excels in reconstructing high-level semantic images while maintaining competitiveness with existing methods on preprocessed data. This advancement paves the way for more accurate and timely brain-to-image decoding, allowing researchers to better understand how images are represented in brain activity over time.'}, 'zh': {'title': 'Âä®ÊÄÅÁ•ûÁªèÊ¥ªÂä®Êâ©Êï£ÔºöÈáçÂª∫ÂõæÂÉèÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂçïÈò∂ÊÆµÊâ©Êï£Ê®°ÂûãDynadiffÔºåÁî®‰∫é‰ªéÂä®ÊÄÅÂèòÂåñÁöÑÂäüËÉΩÁ£ÅÂÖ±ÊåØÊàêÂÉè(fMRI)ËÆ∞ÂΩï‰∏≠ÈáçÂª∫ÂõæÂÉè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåDynadiffÁÆÄÂåñ‰∫ÜËÆ≠ÁªÉËøáÁ®ãÔºåÂπ∂Âú®Êó∂Èó¥ÂàÜËæ®ÁéáÁöÑfMRI‰ø°Âè∑‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØÂú®È´òÂ±ÇÊ¨°ËØ≠‰πâÂõæÂÉèÈáçÂª∫ÊåáÊ†á‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇËØ•Ê®°ÂûãÂú®Â§ÑÁêÜÈ¢ÑÂ§ÑÁêÜÁöÑfMRIÊï∞ÊçÆÊó∂‰πü‰øùÊåÅ‰∫ÜÁ´û‰∫âÂäõÔºåÂêåÊó∂ËÉΩÂ§üÁ≤æÁ°ÆÊèèËø∞Â§ßËÑëÊ¥ªÂä®‰∏≠ÂõæÂÉèË°®Á§∫ÁöÑÊºîÂèò„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåËøôÈ°πÂ∑•‰Ωú‰∏∫Êó∂Èó¥ÂàÜËæ®ÁöÑÂ§ßËÑëÂà∞ÂõæÂÉèËß£Á†ÅÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14467', 'title': 'Void in Language Models', 'url': 'https://huggingface.co/papers/2505.14467', 'abstract': 'Despite advances in transformer-based language models (LMs), a fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using a non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks.', 'score': 0, 'issue_id': 3886, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '3ea3356d2e91804e', 'authors': ['Mani Shemiranifar'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.14467.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#interpretability', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–ú–µ–Ω—å—à–µ —Å–ª–æ–µ–≤ - –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏—é —Å–ª–æ–µ–≤ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π L2 (LAC). –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –Ω–µ –≤—Å–µ —Å–ª–æ–∏ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ –≤ —Ñ–∞–∑–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–æ–ø—É—Å–∫ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–µ–≤ ('–ø—É—Å—Ç–æ—Ç') –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-7B-Instruct –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ —Å 69.24 –¥–æ 71.29 –Ω–∞ MMLU, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ 30% —Å–ª–æ–µ–≤."}, 'en': {'title': 'Unlocking Efficiency: Activating Only What Matters in Language Models', 'desc': "This paper explores whether all layers of transformer-based language models are activated during inference. It introduces a method called L2 Adaptive Computation (LAC) to identify unactivated layers, termed 'Voids', by monitoring the L2-norm of activations. The study analyzes layer activation in two phases: Prompt Processing and Response Generation, revealing that different layers are activated in each phase. The findings demonstrate that selectively skipping unactivated layers can enhance model performance on specific tasks, as shown by improved results on various benchmarks with fewer layers used."}, 'zh': {'title': 'ÈÄâÊã©ÊÄßÊøÄÊ¥ªÂ±ÇÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ', 'desc': 'Â∞ΩÁÆ°Âü∫‰∫éÂèòÊç¢Âô®ÁöÑËØ≠Ë®ÄÊ®°ÂûãÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÊòØÂê¶ÊâÄÊúâÂ±ÇÈÉΩË¢´ÊøÄÊ¥ª‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ëß£ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøá‰∏ÄÁßçÂêç‰∏∫L2Ëá™ÈÄÇÂ∫îËÆ°ÁÆóÔºàLACÔºâÁöÑÊó†ÂèÇÊï∞ÊñπÊ≥ïÊù•Ê£ÄÊµãÊú™ÊøÄÊ¥ªÁöÑÂ±ÇÔºàÁß∞‰∏∫‚ÄúÁ©∫Ê¥û‚ÄùÔºâ„ÄÇËØ•ÊñπÊ≥ïÁõëÊµãÊøÄÊ¥ªÁöÑL2ËåÉÊï∞ÂèòÂåñÔºå‰ª•ËØÜÂà´Âú®ËæìÂÖ•ÊèêÁ§∫ÂíåÁîüÊàêÂìçÂ∫îÈò∂ÊÆµÊøÄÊ¥ªÁöÑÂ±Ç„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂú®Ëøô‰∏§‰∏™Èò∂ÊÆµ‰∏≠ÊøÄÊ¥ªÁöÑÂ±ÇÊòØ‰∏çÂêåÁöÑÔºåÈÄâÊã©ÊÄßË∑≥ËøáÊú™ÊøÄÊ¥ªÁöÑÂ±ÇÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.14366', 'title': 'Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds', 'url': 'https://huggingface.co/papers/2505.14366', 'abstract': 'We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.', 'score': 0, 'issue_id': 3882, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 –º–∞—è', 'en': 'May 20', 'zh': '5Êúà20Êó•'}, 'hash': '0e72d48711224e3c', 'authors': ['Joel Currie', 'Gioele Migno', 'Enrico Piacenti', 'Maria Elena Giannaccini', 'Patric Bach', 'Davide De Tommaso', 'Agnieszka Wykowska'], 'affiliations': ['Italian Institute of Technology, Genova, Italy', 'University of Aberdeen, Aberdeen, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2505.14366.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#dataset', '#agents', '#healthcare', '#cv', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã (VPT). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ NVIDIA Omniverse –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∑–∞–¥–∞—á–∞–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤–∫–ª—é—á–∞–µ—Ç RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —ç—Ç–∞–ª–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ 4x4, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â—É—é –ø–æ–∑—É –æ–±—ä–µ–∫—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ø–æ –æ—Å–∏ Z –∫–∞–∫ –±–∞–∑–æ–≤–æ–º –Ω–∞–≤—ã–∫–µ, —Å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–æ–π —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–æ –ø–æ–ª–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ 6 —Å—Ç–µ–ø–µ–Ω—è—Ö —Å–≤–æ–±–æ–¥—ã.'}, 'en': {'title': 'Empowering Robots with Visual Perspective Taking for Better Interaction', 'desc': 'This paper introduces a framework for training Vision-Language Models (VLMs) to enhance Visual Perspective Taking (VPT), which is crucial for effective Human-Robot Interaction (HRI). The authors create a synthetic dataset using NVIDIA Omniverse, designed for supervised learning in spatial reasoning tasks, containing RGB images, natural language descriptions, and transformation matrices for object poses. The focus is on predicting Z-axis distance, laying the groundwork for future advancements in understanding full 6 Degrees Of Freedom (DOFs). This dataset is made publicly available to encourage further research in developing AI systems that can comprehend spatial relationships in interactive settings.'}, 'zh': {'title': 'ËøàÂêë‰∫∫Êú∫‰∫§‰∫íÁöÑÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ËÆ≠ÁªÉËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰ª•ÂÆûÁé∞ËßÜËßâËßÜËßíÁêÜËß£ÔºàVPTÔºâÁöÑÊ¶ÇÂøµÊ°ÜÊû∂ÔºåËøôÊòØ‰∫∫Êú∫‰∫§‰∫íÔºàHRIÔºâ‰∏≠ÈáçË¶ÅÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂêàÊàêÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÂú®NVIDIA Omniverse‰∏≠ÁîüÊàêÔºåÊîØÊåÅÁ©∫Èó¥Êé®ÁêÜ‰ªªÂä°ÁöÑÁõëÁù£Â≠¶‰π†„ÄÇÊØè‰∏™ÂÆû‰æãÂåÖÂê´‰∏Ä‰∏™RGBÂõæÂÉè„ÄÅ‰∏Ä‰∏™Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Âíå‰∏Ä‰∏™Ë°®Á§∫Áâ©‰ΩìÂßøÊÄÅÁöÑÁúüÂÆû4X4ÂèòÊç¢Áü©Èòµ„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÊé®Êñ≠ZËΩ¥Ë∑ùÁ¶ª‰Ωú‰∏∫Âü∫Á°ÄÊäÄËÉΩÔºåÊú™Êù•Â∞ÜÊâ©Â±ïÂà∞ÂÆåÊï¥ÁöÑÂÖ≠Ëá™Áî±Â∫¶ÔºàDOFsÔºâÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.11563', 'title': 'Object-Centric Representations Improve Policy Generalization in Robot\n  Manipulation', 'url': 'https://huggingface.co/papers/2505.11563', 'abstract': 'Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as a structured alternative that segments visual input into a finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark a range of visual encoders-object-centric, global and dense methods-across a suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.', 'score': 0, 'issue_id': 3880, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 –º–∞—è', 'en': 'May 16', 'zh': '5Êúà16Êó•'}, 'hash': '114f28f1ced2da93', 'authors': ['Alexandre Chapin', 'Bruno Machado', 'Emmanuel Dellandrea', 'Liming Chen'], 'affiliations': ['Ecole Centrale de Lyon, LIRIS 69130, Ecully, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.11563.jpg', 'data': {'categories': ['#cv', '#robotics', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è - –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –æ–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (OCR) –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∏ –ø–ª–æ—Ç–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ OCR –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –≤ –æ–±–æ–±—â–µ–Ω–∏–∏ –Ω–∞ –Ω–æ–≤—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å OCR –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±–æ–±—â–∞—é—â–∏—Ö—Å—è –≤ –¥–∏–Ω–∞–º–∏—á–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ä–µ–¥–∞—Ö.'}, 'en': {'title': 'Unlocking Robustness in Robotics with Object-Centric Representations', 'desc': 'This paper explores the use of object-centric representations (OCR) in robotic manipulation tasks to improve learning and generalization. Unlike traditional methods that use global or dense features, which can mix relevant and irrelevant information, OCR focuses on distinct entities in the visual input. The authors benchmark various visual encoders, including OCR, against different manipulation tasks to assess their performance under changing conditions. The results show that OCR-based policies significantly outperform other methods, indicating their potential for robust performance in real-world scenarios.'}, 'zh': {'title': 'Áâ©‰Ωì‰∏≠ÂøÉË°®Á§∫ÔºöÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÊ≥õÂåñËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÁâ©‰Ωì‰∏≠ÂøÉË°®Á§∫ÔºàOCRÔºâÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰∏éÁé∞ÊúâÁöÑÂÖ®Â±ÄÊàñÂØÜÈõÜÁâπÂæÅÊñπÊ≥ï‰∏çÂêåÔºåOCRËÉΩÂ§üÂ∞ÜËßÜËßâËæìÂÖ•ÂàÜÂâ≤Êàê‰∏ÄÁªÑÁã¨Á´ãÁöÑÂÆû‰ΩìÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Â§ÑÁêÜ‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ„ÄÇÊàë‰ª¨Âú®Â§öÁßçÊ®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÁöÑÊìç‰Ωú‰ªªÂä°‰∏≠ÂØπÊØî‰∫Ü‰∏çÂêåÁöÑËßÜËßâÁºñÁ†ÅÂô®ÔºåÂèëÁé∞OCRÂú®Èù¢ÂØπ‰∏çÂêåÁöÑËßÜËßâÊù°‰ª∂Êó∂Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåOCRÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÂêëÔºåÂèØ‰ª•ÊúâÊïàËÆæËÆ°Âá∫ÈÄÇÂ∫îÂä®ÊÄÅÁúüÂÆûÁéØÂ¢ÉÁöÑËßÜËßâÁ≥ªÁªü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06914', 'title': 'The Distracting Effect: Understanding Irrelevant Passages in RAG', 'url': 'https://huggingface.co/papers/2505.06914', 'abstract': 'A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.   Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.', 'score': 0, 'issue_id': 3882, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 –º–∞—è', 'en': 'May 11', 'zh': '5Êúà11Êó•'}, 'hash': 'dc5ed9552f5c1ec6', 'authors': ['Chen Amiraz', 'Florin Cuconasu', 'Simone Filice', 'Zohar Karnin'], 'affiliations': ['Sapienza University of Rome', 'Technology Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.06914.jpg', 'data': {'categories': ['#hallucinations', '#training', '#rag', '#alignment'], 'emoji': 'üß†', 'ru': {'title': '–£–∫—Ä–æ—â–µ–Ω–∏–µ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ RAG', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏—Ö –ø–∞—Å—Å–∞–∂–µ–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º (RAG). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–ª–µ–∫–∞—é—â–µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –ø–∞—Å—Å–∞–∂–∞ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –µ–≥–æ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –æ—Ç–≤–ª–µ–∫–∞—é—â–∏—Ö –ø–∞—Å—Å–∞–∂–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RAG-—Å–∏—Å—Ç–µ–º. –î–æ–æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–∞–∫–∏—Ö –ø–∞—Å—Å–∞–∂–∞—Ö –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ 7.5% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö RAG.'}, 'en': {'title': 'Enhancing RAG: Turning Distractions into Accuracy Boosts', 'desc': 'This paper addresses a significant challenge in Retrieval Augmented Generation (RAG) systems, where irrelevant passages can mislead language models (LLMs) and result in incorrect answers. The authors propose a new way to measure how distracting a passage can be in relation to a query and an LLM, providing a quantifiable metric for this effect. They introduce innovative techniques for identifying and leveraging these hard distracting passages, which leads to improved performance in answering accuracy. By fine-tuning LLMs with these selected passages, they demonstrate a notable increase in accuracy compared to traditional RAG approaches.'}, 'zh': {'title': 'ÊèêÂçáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁöÑÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÈóÆÈ¢òÔºåÂç≥‰∏éÊü•ËØ¢Êó†ÂÖ≥ÁöÑÊ£ÄÁ¥¢ÊÆµËêΩÂèØËÉΩ‰ºöÂπ≤Êâ∞Á≠îÊ°àÁîüÊàêÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºåÂØºËá¥ÈîôËØØÁöÑÂõûÁ≠î„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈáèÂåñÊñπÊ≥ïÊù•Ë°°ÈáèÊÆµËêΩÂØπÊü•ËØ¢ÂíåLLMÁöÑÂπ≤Êâ∞ÊïàÊûúÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®‰∏çÂêåLLM‰∏≠ÁöÑÁ®≥ÂÅ•ÊÄß„ÄÇÈÄöËøáÁ≤æÂøÉÈÄâÊã©Ëøô‰∫õÂπ≤Êâ∞ÊÆµËêΩÂπ∂ÂØπLLMËøõË°åÂæÆË∞ÉÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÂÆûÁé∞‰∫ÜÈ´òËææ7.5%ÁöÑÂõûÁ≠îÂáÜÁ°ÆÁéáÊèêÂçá„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÂú®‰∫éË∂ÖË∂ä‰∫ÜÁÆÄÂçïÁöÑ‰∫åÂÖÉÂàÜÁ±ªÔºåÂèëÂ±ï‰∫ÜÂ§öÁßçÊñπÊ≥ïÊù•ËØÜÂà´ÂíåÂà©Áî®Èöæ‰ª•Â§ÑÁêÜÁöÑÂπ≤Êâ∞ÊÆµËêΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10554', 'title': "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models", 'url': 'https://huggingface.co/papers/2505.10554', 'abstract': 'Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model\'s "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs\' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment', 'score': 96, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '9b574dafe9f7fb18', 'authors': ['Zhiyuan Hu', 'Yibo Wang', 'Hanze Dong', 'Yuhui Xu', 'Amrita Saha', 'Caiming Xiong', 'Bryan Hooi', 'Junnan Li'], 'affiliations': ['National University of Singapore', 'Salesforce AI Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10554.jpg', 'data': {'categories': ['#alignment', '#science', '#rl', '#optimization', '#training', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ—Ç–∞-—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —è–≤–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å —Ç—Ä–µ–º—è –º–µ—Ç–∞-—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏: –¥–µ–¥—É–∫—Ü–∏–µ–π, –∏–Ω–¥—É–∫—Ü–∏–µ–π –∏ –∞–±–¥—É–∫—Ü–∏–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –∑–∞–¥–∞—á–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≤–∫–ª—é—á–∞–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, —Å–ª–∏—è–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–æ–≤—ã—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 10% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö.'}, 'en': {'title': 'Enhancing Reasoning in Models through Explicit Meta-Ability Alignment', 'desc': "This paper discusses how large reasoning models (LRMs) can improve their reasoning abilities through a structured approach. It highlights the limitations of relying on spontaneous reasoning behaviors, known as 'aha moments', which can be unpredictable. The authors propose a method to explicitly align models with three key reasoning skills: deduction, induction, and abduction, using self-verifiable tasks. Their approach, which includes a three-stage pipeline and domain-specific reinforcement learning, significantly enhances the performance of LRMs on various benchmarks."}, 'zh': {'title': 'ÊòéÁ°ÆÂØπÈΩêÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂÖ∑Â§áÈïøÈìæÊé®ÁêÜÁöÑÊΩúÂú®ËÉΩÂäõ„ÄÇ‰ª•ÁªìÊûú‰∏∫Âü∫Á°ÄÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂèØ‰ª•ÂÅ∂ÁÑ∂ÂºïÂèëÈ´òÁ∫ßÊé®ÁêÜË°å‰∏∫ÔºåÂ¶ÇËá™Êàë‰øÆÊ≠£ÂíåÂõûÊ∫ØÔºå‰ΩÜËøô‰∫õË°å‰∏∫ÁöÑÊó∂Êú∫Âíå‰∏ÄËá¥ÊÄßÈöæ‰ª•È¢ÑÊµãÔºåÈôêÂà∂‰∫ÜLRMsÊé®ÁêÜËÉΩÂäõÁöÑÂèØÊâ©Â±ïÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫ÈÄöËøáËá™Âä®ÁîüÊàêÁöÑËá™ÊàëÈ™åËØÅ‰ªªÂä°ÔºåÊòéÁ°ÆÂØπÊ®°ÂûãËøõË°å‰∏âÁßçÂÖÉËÉΩÂäõÁöÑÂØπÈΩêÔºöÊºîÁªé„ÄÅÂΩíÁ∫≥ÂíåÊ∫ØÂõ†„ÄÇÈÄöËøá‰∏âÈò∂ÊÆµÁöÑ‰∏™‰ΩìÂØπÈΩê„ÄÅÂèÇÊï∞Á©∫Èó¥ÂêàÂπ∂ÂíåÈ¢ÜÂüüÁâπÂÆöÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊÄßËÉΩÊèêÂçáË∂ÖËøá10%ÔºåÂπ∂Âú®Êï∞Â≠¶„ÄÅÁºñÁ®ãÂíåÁßëÂ≠¶Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞È¢ùÂ§ñÁöÑ2%ÁöÑÂπ≥ÂùáÂ¢ûÁõäÔºåËØÅÊòé‰∫ÜÊòéÁ°ÆÁöÑÂÖÉËÉΩÂäõÂØπÈΩê‰∏∫Êé®ÁêÜÊèê‰æõ‰∫ÜÂèØÊâ©Â±ïÂíåÂèØÈù†ÁöÑÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10475', 'title': 'Parallel Scaling Law for Language Models', 'url': 'https://huggingface.co/papers/2505.10475', 'abstract': "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply P diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the P outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with P parallel streams is similar to scaling the parameters by O(log P) while showing superior inference efficiency. For example, ParScale can use up to 22times less memory increase and 6times less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.", 'score': 52, 'issue_id': 3800, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '71bd56e1da07bcba', 'authors': ['Mouxiang Chen', 'Binyuan Hui', 'Zeyu Cui', 'Jiaxi Yang', 'Dayiheng Liu', 'Jianling Sun', 'Junyang Lin', 'Zhongxin Liu'], 'affiliations': ['Qwen Team, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10475.jpg', 'data': {'categories': ['#inference', '#low_resource', '#training', '#architecture', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º (ParScale). –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç P —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä—è–º—ã–µ –ø—Ä–æ—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç P –≤—ã—Ö–æ–¥–æ–≤. ParScale –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –º–æ—â–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∏ –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫ –ª—é–±–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –¥–∞–Ω–Ω—ã–º –∏–ª–∏ –∑–∞–¥–∞—á–µ. –ê–≤—Ç–æ—Ä—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ.'}, 'en': {'title': 'Unlocking Efficiency: Parallel Scaling for Language Models', 'desc': "This paper introduces a new method called parallel scaling (ParScale) for improving the efficiency of language models during training and inference. Instead of simply increasing the number of parameters or output tokens, ParScale enhances the model's ability to perform computations in parallel by applying multiple transformations to the input. The authors propose a new scaling law that shows how using P parallel streams can achieve similar performance to increasing parameters by O(log P), while significantly reducing memory and latency costs. This approach allows for the reuse of existing models and can be adapted to various tasks, making it a cost-effective solution for deploying powerful models in resource-constrained environments."}, 'zh': {'title': 'Âπ∂Ë°åÊâ©Â±ïÔºöÈ´òÊïàÁöÑÊ®°ÂûãËÆ°ÁÆóÊñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ®°ÂûãÊâ©Â±ïÊñπÊ≥ïÔºåÁß∞‰∏∫Âπ∂Ë°åÊâ©Â±ïÔºàParScaleÔºâÔºåÂÆÉÈÄöËøáÂ¢ûÂä†Ê®°ÂûãÁöÑÂπ∂Ë°åËÆ°ÁÆóÊù•ÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåËÄå‰∏çÊòØÂçïÁ∫ØÂ¢ûÂä†ÂèÇÊï∞ÊàñËæìÂá∫‰ª§Áâå„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂØπËæìÂÖ•Â∫îÁî®Â§öÁßçÂèØÂ≠¶‰π†ÁöÑÂèòÊç¢ÔºåÂπ∂Âú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Âπ∂Ë°åÊâßË°åÊ®°ÂûãÁöÑÂâçÂêë‰º†ÈÄíÔºåÂä®ÊÄÅËÅöÂêàÂ§ö‰∏™ËæìÂá∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®P‰∏™Âπ∂Ë°åÊµÅÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÁõ∏ÂΩì‰∫éÂèÇÊï∞Êâ©Â±ïÁöÑO(log P)Ôºå‰ΩÜÂú®Êé®ÁêÜÊïàÁéá‰∏äÊõ¥‰ºò„ÄÇParScaleËøòÂèØ‰ª•ÈÄöËøáÂú®Â∞ëÈáè‰ª§Áâå‰∏äËøõË°åÂêéËÆ≠ÁªÉÔºåÂ∞ÜÁé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËΩ¨Âåñ‰∏∫Âπ∂Ë°åÊâ©Â±ïÊ®°ÂûãÔºå‰ªéËÄåÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09666', 'title': 'System Prompt Optimization with Meta-Learning', 'url': 'https://huggingface.co/papers/2505.09666', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.', 'score': 52, 'issue_id': 3792, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': 'e8d5cb78c5949430', 'authors': ['Yumin Choi', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.09666.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#transfer_learning'], 'emoji': 'üß†', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —É—Å—Ç–æ–π—á–∏–≤—ã–º–∏ –∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º –∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã–º–∏ –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 14 –Ω–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –Ω–µ–∑–Ω–∞–∫–æ–º—ã–º –∑–∞–¥–∞—á–∞–º.'}, 'en': {'title': 'Optimizing System Prompts for Versatile Language Model Performance', 'desc': 'This paper addresses the optimization of system prompts in Large Language Models (LLMs), which are crucial for enhancing model performance across various tasks. It introduces a new approach called bilevel system prompt optimization, focusing on creating system prompts that can adapt to different user prompts and tasks. The authors propose a meta-learning framework that iteratively refines both system and user prompts, ensuring they work well together. Experimental results demonstrate that the optimized system prompts can generalize effectively and adapt quickly to new tasks with fewer optimization steps, leading to better performance.'}, 'zh': {'title': '‰ºòÂåñÁ≥ªÁªüÊèêÁ§∫ÔºåÊèêÂçáÊ®°ÂûãÈÄÇÂ∫îÊÄß', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜ‰ªªÂä°Êó∂Ë°®Áé∞Âá∫Ëâ≤ÔºåËÄå‰ºòÂåñËæìÂÖ•ÊèêÁ§∫Âú®ÊèêÂçáÂÖ∂ÊÄßËÉΩ‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÁé∞ÊúâÁöÑÊèêÁ§∫‰ºòÂåñÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁâπÂÆö‰ªªÂä°ÁöÑÁî®Êà∑ÊèêÁ§∫‰∏äÔºåÂøΩËßÜ‰∫ÜÁ≥ªÁªüÊèêÁ§∫ÁöÑ‰ºòÂåñ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂèåÂ±ÇÁ≥ªÁªüÊèêÁ§∫‰ºòÂåñÁöÑÊñ∞ÈóÆÈ¢òÔºåÊó®Âú®ËÆæËÆ°ËÉΩÂ§üÈÄÇÂ∫îÂ§öÁßçÁî®Êà∑ÊèêÁ§∫Âπ∂ÂèØËøÅÁßªÂà∞Êú™ËßÅ‰ªªÂä°ÁöÑÁ≥ªÁªüÊèêÁ§∫„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖÉÂ≠¶‰π†Ê°ÜÊû∂ÔºåÈÄöËøáÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏ä‰ºòÂåñÁ≥ªÁªüÊèêÁ§∫ÔºåÂêåÊó∂Ëø≠‰ª£Êõ¥Êñ∞Áî®Êà∑ÊèêÁ§∫Ôºå‰ª•Á°Æ‰øù‰∏§ËÄÖ‰πãÈó¥ÁöÑÂçèÂêå‰ΩúÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08617', 'title': 'OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.08617', 'abstract': 'While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".', 'score': 29, 'issue_id': 3792, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '8cef19b2c7c9a459', 'authors': ['Zhaochen Su', 'Linjie Li', 'Mingyang Song', 'Yunzhuo Hao', 'Zhengyuan Yang', 'Jun Zhang', 'Guanjie Chen', 'Jiawei Gu', 'Juntao Li', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Huazhong University of Science and Technology', 'Microsoft', 'Shanghai Jiao Tong University', 'Soochow University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.08617.jpg', 'data': {'categories': ['#cv', '#rl', '#dataset', '#agents', '#training', '#open_source', '#reasoning'], 'emoji': 'üîç', 'ru': {'title': 'OpenThinkIMG: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –º—ã—Å–ª–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenThinkIMG - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫ –∏ –≥–∏–±–∫—É—é —Å—Ä–µ–¥—É –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º V-ToolRL –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –≤–Ω–µ—à–Ω–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–≥—Ä–∞–º–º –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç, –æ–±—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é V-ToolRL, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–∂–µ GPT-4.1.'}, 'en': {'title': 'Empowering AI to Think with Images through OpenThinkIMG', 'desc': 'This paper presents OpenThinkIMG, an innovative framework designed to enhance Large Vision-Language Models (LVLMs) by integrating visual tools for improved problem-solving. The framework addresses the challenges of standardization and data generation, allowing for better training of agents that can adaptively use visual tools. A key feature is the introduction of V-ToolRL, a reinforcement learning approach that enables LVLMs to learn optimal strategies for tool usage through direct feedback from their interactions. The results show that agents trained with V-ToolRL significantly outperform traditional supervised fine-tuning methods and even leading closed-source models, demonstrating the potential of dynamic tool-augmented visual reasoning.'}, 'zh': {'title': 'ÂºÄÂêØËßÜËßâÂ∑•ÂÖ∑Â¢ûÂº∫ÁöÑÊô∫ËÉΩÊé®ÁêÜÊñ∞Êó∂‰ª£', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜOpenThinkIMGÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁ´ØÂà∞Á´ØÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâ‰ΩøÁî®ËßÜËßâÂ∑•ÂÖ∑ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Êèê‰æõ‰∫ÜÊ†áÂáÜÂåñÁöÑËßÜËßâÂ∑•ÂÖ∑Êé•Âè£ÂíåÁÅµÊ¥ªÁöÑËÆ≠ÁªÉÁéØÂ¢ÉÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞ÈõÜÊàêÂ§öÁßçÂ∑•ÂÖ∑Âπ∂ÁîüÊàê‰∏∞ÂØåÁöÑ‰∫§‰∫íÊï∞ÊçÆ„ÄÇ‰∏∫‰∫ÜÂÖãÊúç‰º†ÁªüÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂V-ToolRLÔºå‰ΩøLVLMsËÉΩÂ§üËá™‰∏ªÂ≠¶‰π†Âä®ÊÄÅÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÈÄÇÂ∫îÊÄßÁ≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®V-ToolRLËÆ≠ÁªÉÁöÑ‰ª£ÁêÜÂú®Â§çÊùÇÁöÑÂõæË°®Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂü∫Á∫øÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09723', 'title': 'EnerVerse-AC: Envisioning Embodied Environments with Action Condition', 'url': 'https://huggingface.co/papers/2505.09723', 'abstract': "Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world model that generates future visual observations based on an agent's predicted actions, enabling realistic and controllable robotic inference. Building on prior architectures, EVAC introduces a multi-level action-conditioning mechanism and ray map encoding for dynamic multi-view image generation while expanding training data with diverse failure trajectories to improve generalization. As both a data engine and evaluator, EVAC augments human-collected trajectories into diverse datasets and generates realistic, action-conditioned video observations for policy testing, eliminating the need for physical robots or complex simulations. This approach significantly reduces costs while maintaining high fidelity in robotic manipulation evaluation. Extensive experiments validate the effectiveness of our method. Code, checkpoints, and datasets can be found at <https://annaj2178.github.io/EnerverseAC.github.io>.", 'score': 18, 'issue_id': 3793, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '8adef8c283985aee', 'authors': ['Yuxin Jiang', 'Shengcong Chen', 'Siyuan Huang', 'Liliang Chen', 'Pengfei Zhou', 'Yue Liao', 'Xindong He', 'Chiming Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'MMLab-CUHK', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.09723.jpg', 'data': {'categories': ['#agents', '#dataset', '#training', '#optimization', '#open_source', '#robotics', '#video'], 'emoji': 'ü§ñ', 'ru': {'title': 'EVAC: –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–æ—Ç–æ–≤', 'desc': 'EVAC - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ —Å —É—Å–ª–æ–≤–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤. –û–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±—É–¥—É—â–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ. EVAC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á–µ–≤–æ–π –∫–∞—Ä—Ç—ã –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ä—è–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –Ω–µ—É–¥–∞—á –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Å–ª—É–∂–∏—Ç –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –æ—Ü–µ–Ω–∫–∏.'}, 'en': {'title': 'Revolutionizing Robotic Learning with EnerVerse-AC', 'desc': 'The paper presents EnerVerse-AC (EVAC), a novel action-conditional world model designed for robotic imitation learning in dynamic environments. EVAC generates future visual observations based on predicted actions, allowing for realistic robotic inference without the need for physical robots. It enhances training data by incorporating diverse failure trajectories and employs a multi-level action-conditioning mechanism for improved generalization. This method significantly reduces evaluation costs while ensuring high fidelity in testing robotic manipulation policies.'}, 'zh': {'title': 'Âä®ÊÄÅ‰∫§‰∫í‰∏≠ÁöÑÊú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†Êñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫EnerVerse-ACÔºàEVACÔºâÁöÑÂä®‰ΩúÊù°‰ª∂‰∏ñÁïåÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÊú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†Âú®Âä®ÊÄÅ‰∫§‰∫íÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇEVACËÉΩÂ§üÊ†πÊçÆ‰ª£ÁêÜÁöÑÈ¢ÑÊµãÂä®‰ΩúÁîüÊàêÊú™Êù•ÁöÑËßÜËßâËßÇÂØüÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÁúüÂÆûÂíåÂèØÊéßÁöÑÊú∫Âô®‰∫∫Êé®ÁêÜ„ÄÇËØ•Ê®°ÂûãÂºïÂÖ•‰∫ÜÂ§öÂ±ÇÊ¨°ÁöÑÂä®‰ΩúÊù°‰ª∂Êú∫Âà∂ÂíåÂÖâÁ∫øÂõæÁºñÁ†ÅÔºåÂ¢ûÂº∫‰∫ÜÂä®ÊÄÅÂ§öËßÜÂõæÂõæÂÉèÁîüÊàêÁöÑËÉΩÂäõÔºåÂπ∂ÈÄöËøáÂ§öÊ†∑ÂåñÁöÑÂ§±Ë¥•ËΩ®ËøπÊâ©Â±ïËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÂ∞Ü‰∫∫Á±ªÊî∂ÈõÜÁöÑËΩ®ËøπËΩ¨Âåñ‰∏∫Â§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜÔºåEVACÊòæËëóÈôç‰Ωé‰∫ÜÊµãËØïÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈ´ò‰øùÁúüÂ∫¶ÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10527', 'title': 'WorldPM: Scaling Human Preference Modeling', 'url': 'https://huggingface.co/papers/2505.10527', 'abstract': "Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.", 'score': 17, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': 'd2fe74535293635c', 'authors': ['Binghai Wang', 'Runji Lin', 'Keming Lu', 'Le Yu', 'Zhenru Zhang', 'Fei Huang', 'Chujie Zheng', 'Kai Dang', 'Yang Fan', 'Xingzhang Ren', 'An Yang', 'Binyuan Hui', 'Dayiheng Liu', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Bowen Yu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Institute of Trustworthy Embodied Artificial Intelligence, Fudan University', 'Qwen Team, Alibaba Group', 'School of Computer Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10527.jpg', 'data': {'categories': ['#data', '#training', '#benchmark', '#optimization', '#dataset', '#rlhf', '#alignment'], 'emoji': 'üåç', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π: –æ—Ç –¥–∞–Ω–Ω—ã—Ö –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –Ω–∞–±–ª—é–¥–∞—é—Ç—Å—è –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é World Preference Modeling (WorldPM) –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É—è 15 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 1.5 –¥–æ 72 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–Ω–∏ –Ω–∞–±–ª—é–¥–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ WorldPM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Scaling Human Preferences with WorldPM', 'desc': 'This paper introduces World Preference Modeling (WorldPM), which explores how human preferences can be effectively modeled and scaled in machine learning. The authors demonstrate that preference modeling follows similar scaling laws as language modeling, where larger models and datasets lead to improved performance. They collect diverse preference data and train models with varying sizes, revealing that adversarial and objective metrics improve with scale, while subjective metrics do not show consistent trends. The findings suggest that WorldPM enhances generalization across different human preference datasets and significantly boosts performance in reinforcement learning from human feedback (RLHF) applications.'}, 'zh': {'title': '‰∏ñÁïåÂÅèÂ•ΩÂª∫Ê®°ÔºöÊèêÂçá‰∫∫Á±ªÂÅèÂ•ΩÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®ÂÅèÂ•ΩÂª∫Ê®°‰∏≠Â≠òÂú®ÁöÑËßÑÊ®°Ê≥ïÂàôÔºåÁ±ª‰ºº‰∫éËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÁöÑÁé∞Ë±°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ñÁïåÂÅèÂ•ΩÂª∫Ê®°ÔºàWorldPMÔºâÔºåÂº∫Ë∞ÉÂÖ∂Âú®Â§ÑÁêÜ‰∫∫Á±ªÂÅèÂ•ΩÊó∂ÁöÑÁªü‰∏ÄË°®Á§∫ËÉΩÂäõ„ÄÇÈÄöËøáÊî∂ÈõÜÊù•Ëá™ÂÖ¨ÂÖ±ËÆ∫ÂùõÁöÑÂÅèÂ•ΩÊï∞ÊçÆÔºåÂπ∂Âú®‰∏çÂêåËßÑÊ®°ÁöÑÊ®°Âûã‰∏äËøõË°åËÆ≠ÁªÉÔºåÊàë‰ª¨ÂèëÁé∞‰∏çÂêåËØÑ‰º∞ÊåáÊ†áÁöÑË°®Áé∞Â≠òÂú®ÊòéÊòæÂ∑ÆÂºÇ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWorldPMÂú®Â§öÁßç‰∫∫Á±ªÂÅèÂ•ΩÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñÊÄßËÉΩÔºåÂ∞§ÂÖ∂Âú®ÂÖ≥ÈîÆÂ≠ê‰ªªÂä°‰∏äÊèêÂçáË∂ÖËøá5%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10185', 'title': 'The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think', 'url': 'https://huggingface.co/papers/2505.10185', 'abstract': 'Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.', 'score': 17, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': 'dc93377d68390280', 'authors': ['Seongyun Lee', 'Seungone Kim', 'Minju Seo', 'Yongrae Jo', 'Dongyoung Go', 'Hyeonbin Hwang', 'Jinho Park', 'Xiang Yue', 'Sean Welleck', 'Graham Neubig', 'Moontae Lee', 'Minjoon Seo'], 'affiliations': ['Carnegie Mellon University', 'Cornell University', 'KAIST AI', 'LG AI Research', 'NAVER Search US'], 'pdf_title_img': 'assets/pdf/title_img/2505.10185.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –º—ã—à–ª–µ–Ω–∏—è –ò–ò: –æ—Ç —Ü–µ–ø–æ—á–µ–∫ –∫ —ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ 'CoT Encyclopedia', –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é —Ü–µ–ø–æ—á–µ–∫. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ñ–æ—Ä–º–∞—Ç –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á–µ–º –∏—Ö –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å."}, 'en': {'title': 'Unlocking Model Reasoning with the CoT Encyclopedia', 'desc': 'This paper presents the CoT Encyclopedia, a new framework for analyzing the reasoning strategies of large language models through their chain-of-thought (CoT) outputs. Unlike previous methods that rely on predefined categories, this approach uses a bottom-up technique to automatically extract and cluster diverse reasoning criteria from the models. The framework not only enhances the interpretability of model behaviors but also improves performance by predicting and guiding models towards more effective reasoning strategies. Additionally, the study highlights the significant influence of training data format on reasoning behavior, emphasizing the need for format-aware design in model training.'}, 'zh': {'title': 'ÁêÜËß£Êé®ÁêÜÔºåÊèêÂçáÊ®°ÂûãË°®Áé∞', 'desc': 'ÈïøÈìæÊé®ÁêÜÔºàCoTÔºâÊòØÁé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊúâÊïà‰ΩøÁî®ÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºå‰ΩÜÊàë‰ª¨ÂØπÂÖ∂Êé®ÁêÜÁ≠ñÁï•ÁöÑÁêÜËß£‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜCoTÁôæÁßëÂÖ®‰π¶ÔºåËøôÊòØ‰∏Ä‰∏™Ëá™‰∏ãËÄå‰∏äÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂàÜÊûêÂíåÂºïÂØºÊ®°ÂûãÊé®ÁêÜ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïËá™Âä®ÊèêÂèñÊ®°ÂûãÁîüÊàêÁöÑCoT‰∏≠ÁöÑÂ§öÊ†∑ÂåñÊé®ÁêÜÊ†áÂáÜÔºåÂ∞ÜÂÖ∂ÂµåÂÖ•ËØ≠‰πâÁ©∫Èó¥ÔºåËÅöÁ±ªÊàê‰ª£Ë°®ÊÄßÁ±ªÂà´ÔºåÂπ∂Êé®ÂØºÂá∫ÂØπÊØîÊÄßÊ†áÂáÜ‰ª•Ëß£ÈáäÊé®ÁêÜË°å‰∏∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊ°ÜÊû∂ÊØîÁé∞ÊúâÊñπÊ≥ïÊèê‰æõ‰∫ÜÊõ¥ÂèØËß£ÈáäÂíåÂÖ®Èù¢ÁöÑÂàÜÊûêÔºåÂπ∂‰∏îËÉΩÂ§üÈ¢ÑÊµãÊ®°ÂûãÂèØËÉΩ‰ΩøÁî®ÁöÑÁ≠ñÁï•Ôºå‰ªéËÄåÂºïÂØºÂÖ∂ÊúùÂêëÊõ¥ÊúâÊïàÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10562', 'title': 'End-to-End Vision Tokenizer Tuning', 'url': 'https://huggingface.co/papers/2505.10562', 'abstract': 'Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is agnostic to downstream tasks requiring varied representations and semantics. This decoupled paradigm introduces a critical misalignment: The loss of the vision tokenization can be the representation bottleneck for target tasks. For example, errors in tokenizing text in a given image lead to poor results when recognizing or generating them. To address this, we propose ETT, an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.', 'score': 16, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '1f57cc7a0953749d', 'authors': ['Wenxuan Wang', 'Fan Zhang', 'Yufeng Cui', 'Haiwen Diao', 'Zhuoyan Luo', 'Huchuan Lu', 'Jing Liu', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Dalian University of Technology', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10562.jpg', 'data': {'categories': ['#alignment', '#cv', '#optimization', '#multimodal', '#training', '#architecture'], 'emoji': 'üîç', 'ru': {'title': '–ö–æ–Ω–µ—Ü-–≤-–∫–æ–Ω–µ—Ü –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ETT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–æ–ª–∏—Ä—É—é—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –æ—Ç –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, ETT –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Ü–µ–ª–µ–≤—ã–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–æ–¥–æ–≤–æ–π –∫–Ω–∏–≥–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã —Å —É—á–µ—Ç–æ–º —Ü–µ–ª–µ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 2-6% –¥–ª—è –∑–∞–¥–∞—á –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏.'}, 'en': {'title': 'Empowering Vision Tokenization for Better Multimodal Performance', 'desc': 'This paper introduces ETT, a new method for optimizing vision tokenizers in machine learning. Traditional vision tokenization methods often work separately from the tasks they support, which can lead to poor performance due to misalignment in representation. ETT addresses this issue by allowing joint optimization of vision tokenization and downstream tasks, improving the quality of visual representations. The results show that ETT significantly enhances performance in multimodal tasks while maintaining the original capabilities of the tokenizer.'}, 'zh': {'title': 'Á´ØÂà∞Á´ØËßÜËßâÊ†áËÆ∞Âô®Ë∞É‰ºòÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÊÄßËÉΩ', 'desc': 'Áé∞ÊúâÁöÑËßÜËßâÊ†áËÆ∞ÂåñÊñπÊ≥ïÂ∞ÜËßÜËßâÊ†áËÆ∞Âô®ÁöÑ‰ºòÂåñ‰∏é‰∏ãÊ∏∏ËÆ≠ÁªÉÂàÜÂºÄÔºåÂÅáËÆæËßÜËßâÊ†áËÆ∞Âú®ÂêÑÁßç‰ªªÂä°‰∏≠ËÉΩÂ§üÂæàÂ•ΩÂú∞Ê≥õÂåñ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜETTÔºå‰∏ÄÁßçÁ´ØÂà∞Á´ØÁöÑËßÜËßâÊ†áËÆ∞Âô®Ë∞É‰ºòÊñπÊ≥ïÔºåËÉΩÂ§üÂÆûÁé∞ËßÜËßâÊ†áËÆ∞Âåñ‰∏éÁõÆÊ†áËá™ÂõûÂΩí‰ªªÂä°ÁöÑËÅîÂêà‰ºòÂåñ„ÄÇETTÂà©Áî®Ê†áËÆ∞Âô®‰ª£Á†ÅÊú¨ÁöÑËßÜËßâÂµåÂÖ•Ôºå‰ºòÂåñËßÜËßâÊ†áËÆ∞Âô®ÔºåÂêåÊó∂ÂÖºÈ°æÈáçÂª∫ÂíåÊèèËø∞ÁõÆÊ†á„ÄÇÂÆûÈ™åË°®ÊòéÔºåETTÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÂíåËßÜËßâÁîüÊàê‰ªªÂä°‰∏≠Áõ∏ÊØî‰∫éÂõ∫ÂÆöÊ†áËÆ∞Âô®Âü∫Á∫øÔºåÊÄßËÉΩÊèêÂçáÊòæËëó„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10320', 'title': 'J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.10320', 'abstract': 'The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.', 'score': 15, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '61617ab29ad31cc7', 'authors': ['Chenxi Whitehouse', 'Tianlu Wang', 'Ping Yu', 'Xian Li', 'Jason Weston', 'Ilia Kulikov', 'Swarnadeep Saha'], 'affiliations': ['FAIR at Meta', 'GenAI at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.10320.jpg', 'data': {'categories': ['#reasoning', '#training', '#benchmark', '#optimization', '#hallucinations', '#rlhf', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'J1: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π-—Å—É–¥–µ–π —á–µ—Ä–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π-—Å—É–¥–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π J1. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ, —Ç–∞–∫ –∏ –Ω–µ–ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∑–∞–¥–∞—á–∏ –æ—Ü–µ–Ω–∫–∏ —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏, —Å—Ç–∏–º—É–ª–∏—Ä—É—é—â–∏–º–∏ –º—ã—à–ª–µ–Ω–∏–µ –∏ —Å–Ω–∏–∂–∞—é—â–∏–º–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å —Å—É–∂–¥–µ–Ω–∏–π. J1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 8B –∏ 70B, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏, –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑ DeepSeek-R1. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ J1 —É–ª—É—á—à–∞—é—Ç —Å—É–∂–¥–µ–Ω–∏—è, —Ñ–æ—Ä–º—É–ª–∏—Ä—É—è –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏, —Å—Ä–∞–≤–Ω–∏–≤–∞—è —Å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Empowering AI Judgment with J1: A Reinforcement Learning Breakthrough', 'desc': 'This paper addresses the challenge of evaluating AI models by introducing J1, a reinforcement learning method designed to enhance the judgment capabilities of large language models (LLMs). J1 transforms both verifiable and non-verifiable prompts into judgment tasks that provide clear rewards, promoting better reasoning and reducing bias in evaluations. The results show that J1 outperforms existing models of similar sizes, demonstrating its effectiveness in training models to make more accurate judgments. The study also includes a detailed analysis of various training strategies and their impact on model performance, highlighting the importance of structured evaluation criteria and self-referential comparisons.'}, 'zh': {'title': 'ÊèêÂçáAIËØÑ‰º∞Ë¥®ÈáèÁöÑÂÖ≥ÈîÆÔºöJ1Ê®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∫∫Â∑•Êô∫ËÉΩËØÑ‰º∞Ë¥®ÈáèÂØπAIËøõÊ≠•ÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫J1ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊù•ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËøõË°åÂà§Êñ≠„ÄÇJ1ÈÄöËøáÂ∞ÜÂèØÈ™åËØÅÂíå‰∏çÂèØÈ™åËØÅÁöÑÊèêÁ§∫ËΩ¨Êç¢‰∏∫ÂÖ∑ÊúâÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂà§Êñ≠‰ªªÂä°Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊÄùÁª¥ËÉΩÂäõÂπ∂ÂáèÂ∞ëÂà§Êñ≠ÂÅèÂ∑Æ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåJ1Âú®ËÆ≠ÁªÉÊó∂Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑ8BÂíå70BÊ®°ÂûãÔºåÁîöËá≥Âú®Êüê‰∫õÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÊõ¥Â§ßÁöÑÊ®°Âûã„ÄÇÈÄöËøáÂØπÊØî‰∏çÂêåÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÊ®°ÂûãÂèò‰ΩìÔºåÂèëÁé∞J1Ê®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ËøõË°åÂà§Êñ≠ÔºåÂ≠¶‰π†ËØÑ‰º∞Ê†áÂáÜÂπ∂Ëá™ÊàëÁîüÊàêÂèÇËÄÉÁ≠îÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09694', 'title': 'EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models', 'url': 'https://huggingface.co/papers/2505.09694', 'abstract': 'Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench.', 'score': 15, 'issue_id': 3792, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '8439c79042ee5e23', 'authors': ['Hu Yue', 'Siyuan Huang', 'Yue Liao', 'Shengcong Chen', 'Pengfei Zhou', 'Liliang Chen', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'HIT', 'MMLab-CUHK', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.09694.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#multimodal', '#games', '#video', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': 'EWMBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ EWMBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Embodied World Models, EWM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å EWM –ø–æ —Ç—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –∞—Å–ø–µ–∫—Ç–∞–º: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Å—Ü–µ–Ω—ã, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞–º–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è. EWMBench –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –∑–∞–¥–∞—á –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò –∏ –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': "Evaluating AI's Ability to Create Realistic Actions in Video", 'desc': 'This paper discusses the development of embodied world models (EWMs) that generate realistic video scenes based on text descriptions. It highlights the need for better evaluation methods for these models, moving beyond simple visual quality assessments to include physical realism and action consistency. The authors introduce the Embodied World Model Benchmark (EWMBench), which evaluates EWMs on visual scene consistency, motion correctness, and semantic alignment. By providing a curated dataset and evaluation tools, this work aims to improve the performance of video generation models in tasks that require understanding and interaction with the environment.'}, 'zh': {'title': 'ËØÑ‰º∞ÂÖ∑Ë∫´‰∏ñÁïåÊ®°ÂûãÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'ÊúÄËøëÔºåÂàõÊÑè‰∫∫Â∑•Êô∫ËÉΩÁöÑËøõÊ≠•‰ΩøÂæóÊ†πÊçÆËØ≠Ë®ÄÊåá‰ª§ÂêàÊàêÈ´ò‰øùÁúüÂõæÂÉèÂíåËßÜÈ¢ëÊàê‰∏∫ÂèØËÉΩ„ÄÇÂü∫‰∫éËøô‰∫õÂèëÂ±ïÔºåÊñáÊú¨Âà∞ËßÜÈ¢ëÁöÑÊâ©Êï£Ê®°ÂûãÊºîÂèò‰∏∫ÂÖ∑Ë∫´‰∏ñÁïåÊ®°ÂûãÔºàEWMÔºâÔºåËÉΩÂ§üÊ†πÊçÆËØ≠Ë®ÄÂëΩ‰ª§ÁîüÊàêÁâ©ÁêÜ‰∏äÂêàÁêÜÁöÑÂú∫ÊôØÔºåÊúâÊïàÂú∞Â∞ÜËßÜËßâ‰∏éË°åÂä®ÁªìÂêàÂú®ÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÂ∫îÁî®‰∏≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂÖ∑Ë∫´‰∏ñÁïåÊ®°ÂûãÂü∫ÂáÜÔºàEWMBenchÔºâÔºåÊó®Âú®ÈÄöËøáËßÜËßâÂú∫ÊôØ‰∏ÄËá¥ÊÄß„ÄÅËøêÂä®Ê≠£Á°ÆÊÄßÂíåËØ≠‰πâÂØπÈΩê‰∏â‰∏™ÂÖ≥ÈîÆÊñπÈù¢Êù•ËØÑ‰º∞EWM„ÄÇËØ•Âü∫ÂáÜ‰∏ç‰ªÖËØÜÂà´‰∫ÜÁé∞ÊúâËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®Êª°Ë∂≥ÂÖ∑Ë∫´‰ªªÂä°Áã¨ÁâπÈúÄÊ±ÇÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄßÔºåËøò‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06027', 'title': 'Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation', 'url': 'https://huggingface.co/papers/2505.06027', 'abstract': "This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.", 'score': 15, 'issue_id': 3804, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': '8abae468afb82fcd', 'authors': ['Stefan Vasilev', 'Christian Herold', 'Baohao Liao', 'Seyyed Hadi Hashemi', 'Shahram Khadivi', 'Christof Monz'], 'affiliations': ['University of Amsterdam', 'eBay Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.06027.jpg', 'data': {'categories': ['#training', '#benchmark', '#dataset', '#security', '#ethics'], 'emoji': 'üß†', 'ru': {'title': 'Unilogit: —É–º–Ω–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Unilogit - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. Unilogit —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—â–µ–π –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –ª–æ–≥–∏—Ç—ã –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ–∫—É—â–∏–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Unilogit –≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ –∑–∞–¥–∞—á –∑–∞–±—ã–≤–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Unilogit: Smart Forgetting for Safer AI', 'desc': "This paper presents Unilogit, a new self-distillation technique designed for machine unlearning in Large Language Models. It focuses on the ability to forget specific information while still keeping the model useful, which is important for following data privacy laws like GDPR. Unilogit improves upon previous methods by dynamically adjusting target logits based on the model's current outputs, rather than using fixed hyperparameters. The results show that Unilogit performs better than existing methods in both forgetting and retaining information, proving its effectiveness in real-world applications."}, 'zh': {'title': 'UnilogitÔºöÊô∫ËÉΩÈÅóÂøòÁöÑËá™ÊàëËí∏È¶èÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™ÊàëËí∏È¶èÊñπÊ≥ïUnilogitÔºåÁî®‰∫éÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊú∫Âô®ÈÅóÂøò„ÄÇUnilogitËß£ÂÜ≥‰∫ÜÂú®ÈÅµÂÆàÊï∞ÊçÆÈöêÁßÅÊ≥ïËßÑÔºàÂ¶ÇGDPRÔºâÁöÑÂêåÊó∂ÔºåÈÄâÊã©ÊÄßÈÅóÂøòÁâπÂÆö‰ø°ÊÅØÁöÑÊåëÊàò„ÄÇ‰∏é‰æùËµñÈùôÊÄÅË∂ÖÂèÇÊï∞ÊàñÂàùÂßãÊ®°ÂûãËæìÂá∫ÁöÑ‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåUnilogitÂä®ÊÄÅË∞ÉÊï¥ÁõÆÊ†álogitsÔºå‰ª•ÂÆûÁé∞ÁõÆÊ†áÊ†áËÆ∞ÁöÑÂùáÂåÄÊ¶ÇÁéáÔºå‰ªéËÄåÊèêÈ´òËá™ÊàëËí∏È¶èÁõÆÊ†áÁöÑÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáÂú®ÂÖ¨ÂÖ±Âü∫ÂáÜÂíåÂÜÖÈÉ®ÁîµÂ≠êÂïÜÂä°Êï∞ÊçÆÈõÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåUnilogitÂú®ÈÅóÂøò‰∏é‰øùÁïôÁõÆÊ†áÁöÑÂπ≥Ë°°ÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåË∂ÖË∂ä‰∫ÜNPOÂíåUnDIALÁ≠âÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07782', 'title': 'MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering', 'url': 'https://huggingface.co/papers/2505.07782', 'abstract': "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.", 'score': 14, 'issue_id': 3791, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '6a556cb214d3d71d', 'authors': ['Rushi Qiang', 'Yuchen Zhuang', 'Yinghao Li', 'Dingu Sagar V K', 'Rongzhi Zhang', 'Changhao Li', 'Ian Shu-Hei Wong', 'Sherry Yang', 'Percy Liang', 'Chao Zhang', 'Bo Dai'], 'affiliations': ['Georgia Institute of Technology', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07782.jpg', 'data': {'categories': ['#data', '#training', '#architecture', '#benchmark', '#open_source', '#games', '#agents', '#rl'], 'emoji': 'ü§ñ', 'ru': {'title': 'MLE-Dojo: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'MLE-Dojo - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, MLE-Dojo –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Å—Ä–µ–¥—É, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –∞–≥–µ–Ω—Ç–∞–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å –∏ —É–ª—É—á—à–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–∏–∫–ª—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 200 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö Kaggle –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∏—Å–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –æ—Ç–ª–∞–¥–∫–∞ –∫–æ–¥–∞. MLE-Dojo –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∫–∞–∫ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Ç–∞–∫ –∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'MLE-Dojo: Empowering Iterative Learning for LLMs', 'desc': 'MLE-Dojo is a new framework designed for reinforcement learning and improving large language model (LLM) agents through iterative machine learning engineering (MLE) processes. It offers an interactive environment where agents can experiment and refine their solutions based on structured feedback, unlike traditional benchmarks that use static datasets. The framework is built on over 200 real-world Kaggle challenges, allowing for diverse MLE tasks such as data processing and hyperparameter tuning. Evaluations show that while LLMs can make iterative improvements, they still struggle with generating long-term solutions and solving complex issues, highlighting the need for further advancements in autonomous learning.'}, 'zh': {'title': 'MLE-DojoÔºöÊé®Âä®Ëá™‰∏ªÂ≠¶‰π†Ê®°ÂûãÁöÑÂàõÊñ∞Âπ≥Âè∞', 'desc': 'MLE-DojoÊòØ‰∏Ä‰∏™Á±ª‰ººGymÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Á≥ªÁªüÂåñÂú∞ËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåËØÑ‰º∞ÂíåÊîπËøõËá™‰∏ªÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜ„ÄÇ‰∏éÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØï‰∏çÂêåÔºåMLE-DojoÊèê‰æõ‰∫Ü‰∏Ä‰∏™‰∫íÂä®ÁéØÂ¢ÉÔºå‰Ωø‰ª£ÁêÜËÉΩÂ§üÈÄöËøáÁªìÊûÑÂåñÂèçÈ¶àÂæ™ÁéØËøõË°åËø≠‰ª£ÂÆûÈ™å„ÄÅË∞ÉËØïÂíå‰ºòÂåñËß£ÂÜ≥ÊñπÊ°à„ÄÇËØ•Ê°ÜÊû∂Âü∫‰∫é200Â§ö‰∏™ÁúüÂÆûÁöÑKaggleÊåëÊàòÔºåÊ∂µÁõñ‰∫ÜÂ§öÊ†∑ÂåñÁöÑÂºÄÊîæÂºèÊú∫Âô®Â≠¶‰π†Â∑•Á®ã‰ªªÂä°ÔºåÂèçÊò†‰∫ÜÁé∞ÂÆûÁöÑÂ∑•Á®ãÂú∫ÊôØ„ÄÇMLE-DojoÁöÑÂèØÊâßË°åÁéØÂ¢ÉÊîØÊåÅÈÄöËøáÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ËøõË°åÂÖ®Èù¢ÁöÑ‰ª£ÁêÜËÆ≠ÁªÉÔºå‰øÉËøõ‰∫ÜËø≠‰ª£ÂÆûÈ™åÂíåÂÆûÊó∂ÁªìÊûúÈ™åËØÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10558', 'title': 'Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors', 'url': 'https://huggingface.co/papers/2505.10558', 'abstract': 'Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.   To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.', 'score': 13, 'issue_id': 3794, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '949341b92df361ab', 'authors': ['Peiying Zhang', 'Nanxuan Zhao', 'Jing Liao'], 'affiliations': ['Adobe Research', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.10558.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#optimization'], 'emoji': 'üé®', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ (SVG) —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∏–ª—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø—Ä—è–º—ã—Ö –º–æ–¥–µ–ª–µ–π Text-to-Vector (T2V) –∏ –ø—Ä–∏–æ—Ä–æ–≤ Text-to-Image (T2I). –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–∞–µ—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å T2V –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç–∏ SVG. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å T2V –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç–∏–ª–∏ –ø—É—Ç–µ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∫–∞—Å—Ç–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π T2I.'}, 'en': {'title': 'Custom SVGs: Merging Structure and Style in Vector Graphics Generation', 'desc': 'This paper presents a new method for generating Scalable Vector Graphics (SVGs) from text prompts while allowing for style customization. The proposed two-stage pipeline combines the strengths of both text-to-vector (T2V) diffusion models and text-to-image (T2I) models to ensure structural regularity and diverse styles. In the first stage, a T2V diffusion model is trained to maintain the SVG structure, while the second stage focuses on customizing the model to different styles using T2I model distillation. The results demonstrate that this approach can efficiently produce high-quality SVGs that meet aesthetic requirements based on user-defined text inputs.'}, 'zh': {'title': 'È´òÊïàÁîüÊàêÂÆöÂà∂ÂåñSVGÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏§Èò∂ÊÆµÊ†∑ÂºèÂÆöÂà∂ÁÆ°ÈÅìÔºåÁî®‰∫é‰ªéÊñáÊú¨ÁîüÊàêÂèØÁº©ÊîæÁü¢ÈáèÂõæÂΩ¢ÔºàSVGÔºâ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Âü∫‰∫éË∑ØÂæÑÁ∫ßË°®Á§∫ÁöÑÊñáÊú¨Âà∞Áü¢ÈáèÔºàT2VÔºâÊâ©Êï£Ê®°ÂûãÔºå‰ª•Á°Æ‰øùSVGÁöÑÁªìÊûÑËßÑÂàôÊÄßÔºåÂêåÊó∂‰øùÁïôÂ§öÊ†∑ÁöÑË°®Áé∞ËÉΩÂäõ„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÔºåÈÄöËøáÊèêÁÇºÂÆöÂà∂ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÔºàT2IÔºâÊ®°ÂûãÊù•ÂÆûÁé∞‰∏çÂêåÊ†∑ÂºèÁöÑÂÆöÂà∂„ÄÇÊàë‰ª¨ÁöÑÁÆ°ÈÅìËÉΩÂ§üÈ´òÊïàÂú∞ÁîüÊàêÈ´òË¥®ÈáèÂíåÂ§öÊ†∑ÂåñÁöÑSVGÔºåÊª°Ë∂≥ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊ†∑ÂºèÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10565', 'title': 'Depth Anything with Any Prior', 'url': 'https://huggingface.co/papers/2505.10565', 'abstract': 'This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.', 'score': 11, 'issue_id': 3794, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '2291c0b9541ea999', 'authors': ['Zehan Wang', 'Siyu Chen', 'Lihe Yang', 'Jialei Wang', 'Ziang Zhang', 'Hengshuang Zhao', 'Zhou Zhao'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10565.jpg', 'data': {'categories': ['#cv', '#dataset', '#training'], 'emoji': 'üîç', 'ru': {'title': '–¢–æ—á–Ω—ã–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã –¥–ª—è –ª—é–±—ã—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Prior Depth Anything - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã –¥–ª—è –ª—é–±—ã—Ö —Å—Ü–µ–Ω. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ–ø–æ–ª–Ω—É—é, –Ω–æ —Ç–æ—á–Ω—É—é –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑–º–µ—Ä–µ–Ω–∏–π –≥–ª—É–±–∏–Ω—ã —Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π, –Ω–æ –ø–æ–ª–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≥–ª—É–±–∏–Ω—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä —É—Ç–æ—á–Ω–µ–Ω–∏—è –æ—Ç –≥—Ä—É–±–æ–≥–æ –∫ —Ç–æ—á–Ω–æ–º—É, –≤–∫–ª—é—á–∞—é—â–∏–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π –∏ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–æ—Ä–æ–≤. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ 7 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≥–ª—É–±–∏–Ω–æ–π.'}, 'en': {'title': 'Merging Depth Insights for Accurate Scene Mapping', 'desc': "This paper introduces Prior Depth Anything, a novel framework that effectively merges incomplete metric depth information with complete geometric structures to create accurate depth maps. It employs a coarse-to-fine approach that integrates these two sources of depth data, enhancing the model's ability to generalize across different scenarios. The framework utilizes pixel-level metric alignment and a conditioned monocular depth estimation model to refine depth predictions and reduce noise. The results demonstrate strong performance in various depth-related tasks, showcasing the model's adaptability and efficiency in real-world applications."}, 'zh': {'title': 'Ê∑±Â∫¶ÊµãÈáè‰∏éÈ¢ÑÊµãÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Prior Depth AnythingÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â∞Ü‰∏çÂÆåÊï¥‰ΩÜÁ≤æÁ°ÆÁöÑÊ∑±Â∫¶ÊµãÈáè‰ø°ÊÅØ‰∏éÁõ∏ÂØπ‰ΩÜÂÆåÊï¥ÁöÑÂá†‰ΩïÁªìÊûÑÁªìÂêàËµ∑Êù•Ôºå‰ªéËÄåÁîüÊàêÂáÜÁ°Æ„ÄÅÂØÜÈõÜ‰∏îËØ¶ÁªÜÁöÑÊ∑±Â∫¶Âõæ„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Á≤óÂà∞ÁªÜÁöÑÊµÅÁ®ãÔºåÈÄêÊ≠•Êï¥ÂêàËøô‰∏§Áßç‰∫íË°•ÁöÑÊ∑±Â∫¶Êù•Ê∫ê„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÂºïÂÖ•ÂÉèÁ¥†Á∫ßÁöÑÂ∫¶ÈáèÂØπÈΩêÂíåË∑ùÁ¶ªÊÑüÁü•Âä†ÊùÉÔºåÂà©Áî®Ê∑±Â∫¶È¢ÑÊµãÊù•È¢ÑÂ°´ÂÖÖÂ§öÊ†∑ÁöÑÂ∫¶ÈáèÂÖàÈ™åÔºåÊúâÊïàÁº©Â∞è‰∫ÜÂÖàÈ™åÊ®°Âºè‰πãÈó¥ÁöÑÈ¢ÜÂüüÂ∑ÆË∑ù„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÊù°‰ª∂ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÔºå‰ª•Ëøõ‰∏ÄÊ≠•Á≤æÁÇºÊ∑±Â∫¶ÂÖàÈ™åÁöÑÂõ∫ÊúâÂô™Â£∞Ôºå‰ªéËÄåÂú®Â§ö‰∏™ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÂá∫Ëâ≤ÁöÑÈõ∂-shotÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09990', 'title': 'PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing', 'url': 'https://huggingface.co/papers/2505.09990', 'abstract': 'Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. Project page: https://pointarena.github.io/', 'score': 11, 'issue_id': 3796, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': 'f0065735407ccc64', 'authors': ['Long Cheng', 'Jiafei Duan', 'Yi Ru Wang', 'Haoquan Fang', 'Boyang Li', 'Yushan Huang', 'Elvis Wang', 'Ainaz Eftekhar', 'Jason Lee', 'Wentao Yuan', 'Rose Hendrix', 'Noah A. Smith', 'Fei Xia', 'Dieter Fox', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Anderson Collegiate Vocational Institute', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.09990.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#open_source', '#benchmark', '#robotics', '#dataset'], 'emoji': 'üëÜ', 'ru': {'title': 'PointArena: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —É–∫–∞–∑–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PointArena - –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ–±—ä–µ–∫—Ç—ã. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Point-Bench —Å 1000 –∑–∞–¥–∞—á, –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –∞—Ä–µ–Ω—É Point-Battle –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –∏ —Å–∏—Å—Ç–µ–º—É Point-Act –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å Molmo-72B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ, –∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —É–∫–∞–∑–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–æ—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ.'}, 'en': {'title': 'PointArena: Evaluating Multimodal Pointing for Real-World Applications', 'desc': 'This paper introduces PointArena, a new platform designed to evaluate how well multimodal models can perform pointing tasks in various reasoning scenarios. It includes a dataset called Point-Bench with around 1,000 tasks, an interactive comparison tool named Point-Battle, and a robotic system called Point-Act for real-world testing. The study shows that the Molmo-72B model outperforms others, especially when trained specifically for pointing tasks. The findings highlight the importance of accurate pointing in helping models connect abstract reasoning with real-world actions.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊåáÂêëËÉΩÂäõÁöÑËØÑ‰º∞Êñ∞Âπ≥Âè∞', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜPointArenaÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÊåáÂêëËÉΩÂäõÁöÑÁªºÂêàÂπ≥Âè∞„ÄÇÂÆÉÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöPoint-BenchÔºå‰∏Ä‰∏™ÂåÖÂê´Á∫¶1000‰∏™ÊåáÂêë‰ªªÂä°ÁöÑÊï∞ÊçÆÈõÜÔºõPoint-BattleÔºå‰∏Ä‰∏™‰∫íÂä®ÁöÑÁΩëÁªúÂπ≥Âè∞ÔºåÁî®‰∫éÊ®°ÂûãÁöÑÁõ≤ÊµãÊØîËæÉÔºõ‰ª•ÂèäPoint-ActÔºå‰∏Ä‰∏™ÂÖÅËÆ∏Áî®Êà∑Âú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°ÂûãÊåáÂêëËÉΩÂäõÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≥ªÁªü„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰∏ìÈó®ÈíàÂØπÊåáÂêë‰ªªÂä°ÁöÑÁõëÁù£ËÆ≠ÁªÉÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂº∫Ë∞É‰∫ÜÁ≤æÁ°ÆÊåáÂêëËÉΩÂäõÂú®Â§öÊ®°ÊÄÅÊ®°Âûã‰∏≠ËøûÊé•ÊäΩË±°Êé®ÁêÜ‰∏éÂÆûÈôÖË°åÂä®ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10046', 'title': 'Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2505.10046', 'abstract': 'This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.', 'score': 10, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': 'e297e154b434ef38', 'authors': ['Bingda Tang', 'Boyang Zheng', 'Xichen Pan', 'Sayak Paul', 'Saining Xie'], 'affiliations': ['Hugging Face', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10046.jpg', 'data': {'categories': ['#multimodal', '#training', '#diffusion', '#survey'], 'emoji': 'üî¨', 'ru': {'title': '–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–ª–∏—è–Ω–∏—è LLM –∏ DiT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiT) –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –¥–∏–∑–∞–π–Ω–∞. –°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —á–µ—Ç–∫–∏–π –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π —Ä–µ—Ü–µ–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ.'}, 'en': {'title': 'Unlocking the Potential of Multi-Modal Generation', 'desc': 'This paper investigates the integration of large language models (LLMs) and diffusion transformers (DiTs) in the field of text-to-image synthesis. It highlights the lack of detailed comparisons and transparency in previous studies, which often overlooked critical design choices and training methodologies. By conducting empirical studies and controlled comparisons with established baselines, the authors aim to clarify the potential of this multi-modal generation approach. The paper provides a reproducible training recipe and valuable insights to guide future research in this area.'}, 'zh': {'title': 'Êé¢Á¥¢ÊñáÊú¨Âà∞ÂõæÂÉèÂêàÊàêÁöÑÊñ∞ËÆæËÆ°Á©∫Èó¥', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÂêàÊàê‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅËÆæËÆ°Á©∫Èó¥ÔºåÁâπÂà´ÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏éÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâÊ∑±Â∫¶ËûçÂêàÁöÑÂ§öÊ®°ÊÄÅÁîüÊàê„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂÖ≥Ê≥®Á≥ªÁªüÊï¥‰ΩìÊÄßËÉΩÔºåËÄåÁº∫‰πèÂØπÊõø‰ª£ÊñπÊ≥ïÁöÑËØ¶ÁªÜÊØîËæÉÔºåÂÖ≥ÈîÆËÆæËÆ°ÁªÜËäÇÂíåËÆ≠ÁªÉÊñπÊ°àÂæÄÂæÄÊú™Ë¢´Êä´Èú≤„ÄÇËøô‰∫õÁ©∫ÁôΩÂØºËá¥‰∫ÜÂØπËØ•ÊñπÊ≥ïÁúüÂÆûÊΩúÂäõÁöÑÊÄÄÁñë„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ËøõË°å‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÂÆûËØÅÁ†îÁ©∂ÔºåËøõË°åÊéßÂà∂ÊØîËæÉÔºåÂàÜÊûêÈáçË¶ÅËÆæËÆ°ÈÄâÊã©ÔºåÂπ∂Êèê‰æõÊ∏ÖÊô∞„ÄÅÂèØÈáçÂ§çÁöÑËÆ≠ÁªÉÊñπÊ°àÔºå‰ª•Êúü‰∏∫Êú™Êù•ÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêÁ†îÁ©∂Êèê‰æõÊúâÊÑè‰πâÁöÑÊï∞ÊçÆÂíåÂÆûÁî®ÊåáÂçó„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09738', 'title': 'Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning', 'url': 'https://huggingface.co/papers/2505.09738', 'abstract': 'Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this often require prohibitive computational resources. Although tokenizer replacement with heuristic initialization aims to reduce this burden, existing methods often require exhaustive residual fine-tuning and still may not fully preserve semantic nuances or adequately address the underlying compression inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a model-agnostic tokenizer transplantation method, and second, novel pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation. Tokenadapt initializes new unique token embeddings via a hybrid heuristic that combines two methods: a local estimate based on subword decomposition using the old tokenizer, and a global estimate utilizing the top-k semantically similar tokens from the original vocabulary. This methodology aims to preserve semantics while significantly minimizing retraining requirements. Empirical investigations validate both contributions: the transplantation heuristic successfully initializes unique tokens, markedly outperforming conventional baselines and sophisticated methods including Transtokenizer and ReTok, while our Supertokens achieve notable compression gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to both ReTok and TransTokenizer baselines across different base models and newly trained target tokenizers. TokenAdapt typically reduced the overall perplexity ratio significantly compared to ReTok, yielding at least a 2-fold improvement in these aggregate scores.', 'score': 9, 'issue_id': 3795, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': 'ae96fcdfda8fee90', 'authors': ['Shaurya Sharthak', 'Vinayak Pahalwan', 'Adithya Kamath', 'Adarsh Shirawalmath'], 'affiliations': ['proton.me', 'tensoic.com', 'tinycompany.in'], 'pdf_title_img': 'assets/pdf/title_img/2505.09738.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#training', '#transfer_learning', '#data'], 'emoji': 'üîÑ', 'ru': {'title': 'TokenAdapt: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∑–∞–º–µ–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ TokenAdapt –¥–ª—è –∑–∞–º–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ—á–µ—Ç–∞—é—â–∏–π –ª–æ–∫–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –ø–æ–¥—Å–ª–æ–≤ –∏ –≥–ª–æ–±–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º—É–ª—å—Ç–∏—Å–ª–æ–≤–Ω—ã—Ö —Å—É–ø–µ—Ä—Ç–æ–∫–µ–Ω–æ–≤, —É–ª—É—á—à–∞—é—â–∏—Ö —Å–∂–∞—Ç–∏–µ –∏ —É–º–µ–Ω—å—à–∞—é—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TokenAdapt –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é —Å–µ–º–∞–Ω—Ç–∏–∫–∏.'}, 'en': {'title': 'Unlocking Language Models: Efficient Tokenization with Tokenadapt', 'desc': 'This paper addresses the limitations of fixed tokenization schemes in pretrained language models, which can hinder performance, especially in multilingual contexts. The authors propose a novel framework called Tokenadapt, which allows for efficient tokenizer transplantation without extensive retraining. By introducing a hybrid heuristic for initializing new token embeddings and utilizing multi-word Supertokens, the method aims to enhance semantic preservation and reduce fragmentation. Empirical results show that Tokenadapt significantly outperforms existing methods in terms of perplexity, demonstrating its effectiveness in improving language model efficiency.'}, 'zh': {'title': 'Á™ÅÁ†¥ÂàÜËØçÈôêÂà∂ÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ', 'desc': 'È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ∏∏Â∏∏ÂèóÂà∞Âõ∫ÂÆöÂàÜËØçÊñπÊ°àÁöÑÈôêÂà∂ÔºåËøôÂØºËá¥Âú®Â§öËØ≠Ë®ÄÊàñ‰∏ì‰∏öÂ∫îÁî®‰∏≠ÊïàÁéá‰Ωé‰∏ãÂíåÊÄßËÉΩÂèóÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂåÖÂê´‰∏§È°πÂàõÊñ∞ÔºöTokenadaptÔºå‰∏ÄÁßçÊ®°ÂûãÊó†ÂÖ≥ÁöÑÂàÜËØçÂô®ÁßªÊ§çÊñπÊ≥ïÔºå‰ª•ÂèäÁî®‰∫éÂ§öËØçË∂ÖÁ∫ßÊ†áËÆ∞ÁöÑÊñ∞ÂûãÈ¢ÑÂàÜËØçÂ≠¶‰π†Ôºå‰ª•Â¢ûÂº∫ÂéãÁº©ÊïàÊûúÂπ∂ÂáèÂ∞ëÁ¢éÁâáÂåñ„ÄÇTokenadaptÈÄöËøáÁªìÂêàÊóßÂàÜËØçÂô®ÁöÑÂ≠êËØçÂàÜËß£ÂíåÂéüÂßãËØçÊ±á‰∏≠ËØ≠‰πâÁõ∏‰ººÁöÑÂâçk‰∏™Ê†áËÆ∞ÁöÑÂÖ®Â±Ä‰º∞ËÆ°ÔºåÂàùÂßãÂåñÊñ∞ÁöÑÁã¨ÁâπÊ†áËÆ∞ÂµåÂÖ•Ôºå‰ªéËÄåÂú®ÂáèÂ∞ëÈáçËÆ≠ÁªÉÈúÄÊ±ÇÁöÑÂêåÊó∂‰øùÊåÅËØ≠‰πâ„ÄÇÂÆûÈ™åËØÅÊòéÔºåTokenadaptÂú®ÂàùÂßãÂåñÁã¨ÁâπÊ†áËÆ∞ÊñπÈù¢ÊòæËëó‰ºò‰∫é‰º†ÁªüÂü∫Á∫øÂíåÂÖ∂‰ªñÂ§çÊùÇÊñπÊ≥ïÔºå‰∏îË∂ÖÁ∫ßÊ†áËÆ∞Âú®ÂéãÁº©ÊïàÊûú‰∏ä‰πüÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10566', 'title': '3D-Fixup: Advancing Photo Editing with 3D Priors', 'url': 'https://huggingface.co/papers/2505.10566', 'abstract': 'Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/', 'score': 7, 'issue_id': 3808, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': 'f1dc934cd3d0961a', 'authors': ['Yen-Chi Cheng', 'Krishna Kumar Singh', 'Jae Shin Yoon', 'Alex Schwing', 'Liangyan Gui', 'Matheus Gadelha', 'Paul Guerrero', 'Nanxuan Zhao'], 'affiliations': ['Adobe Research, UK', 'Adobe Research, USA', 'University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.10566.jpg', 'data': {'categories': ['#cv', '#diffusion', '#3d', '#video', '#training'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '3D-Fixup: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å 3D-–ø—Ä–∏–æ—Ä–∞–º–∏', 'desc': '3D-Fixup - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–∑—É—á–µ–Ω–Ω—ã—Ö 3D-–ø—Ä–∏–æ—Ä–æ–≤. –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ 3D-–≤—Ä–∞—â–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–∞—é—â–∏–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä –∏ –≤–∫–ª—é—á–∞–µ—Ç 3D-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ—Ç –º–æ–¥–µ–ª–∏ Image-to-3D. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ 3D-Fixup —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–æ–≤.'}, 'en': {'title': 'Revolutionizing 2D Image Editing with 3D Insights', 'desc': 'The paper introduces 3D-Fixup, a novel framework designed for 2D image editing using learned 3D priors. It addresses the challenges of editing images that require understanding of 3D transformations, such as object translation and rotation. By utilizing video data to create training pairs, the framework enhances the generative capabilities of diffusion models. The integration of an Image-to-3D model allows for effective projection of 2D information into 3D space, resulting in high-quality, coherent edits that push the boundaries of image manipulation.'}, 'zh': {'title': '3D-FixupÔºöÂÆûÁé∞È´òË¥®ÈáèÁöÑ3DÊÑüÁü•ÂõæÂÉèÁºñËæë', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫3D-FixupÁöÑÊñ∞Ê°ÜÊû∂ÔºåÁî®‰∫éÈÄöËøáÂ≠¶‰π†ÁöÑ3DÂÖàÈ™åÊù•ÁºñËæë2DÂõæÂÉè„ÄÇËØ•Ê°ÜÊû∂ËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇÁöÑÁºñËæë‰ªªÂä°ÔºåÂ¶ÇÁâ©‰ΩìÂπ≥ÁßªÂíå3DÊóãËΩ¨„ÄÇÊàë‰ª¨Âà©Áî®Êâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêËÉΩÂäõÔºåÈÄöËøáËßÜÈ¢ëÊï∞ÊçÆÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÂØπÔºå‰ª•ÊèêÈ´òÁºñËæëÊïàÊûú„ÄÇÈÄöËøáÁªìÂêàÂõæÂÉèÂà∞3DÊ®°ÂûãÁöÑ3DÊåáÂØºÔºå3D-FixupËÉΩÂ§üÂú®2DÂíå3DÁ©∫Èó¥‰πãÈó¥ËøõË°åÊúâÊïàÁöÑËΩ¨Êç¢Ôºå‰ªéËÄåÂÆûÁé∞È´òË¥®ÈáèÁöÑÂõæÂÉèÁºñËæë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10468', 'title': 'AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge', 'url': 'https://huggingface.co/papers/2505.10468', 'abstract': 'This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications', 'score': 7, 'issue_id': 3802, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '613808a0b3d41836', 'authors': ['Ranjan Sapkota', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Department of Environmental and Biological Engineering, USA', 'Department of Informatics and Telecommunications, University of the Peloponnese, 22131 Tripoli, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.10468.jpg', 'data': {'categories': ['#healthcare', '#hallucinations', '#agents', '#architecture', '#multimodal', '#reasoning', '#optimization', '#agi'], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ç –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –∞–≥–µ–Ω—Ç–Ω–æ–º—É –ò–ò: —ç–≤–æ–ª—é—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Å–∏—Å—Ç–µ–º', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏ –∏ –∞–≥–µ–Ω—Ç–Ω—ã–º –ò–ò, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—É—é —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é, –∫–∞—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ –∞–Ω–∞–ª–∏–∑ –≤—ã–∑–æ–≤–æ–≤. –ò–ò-–∞–≥–µ–Ω—Ç—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç—Å—è –∫–∞–∫ –º–æ–¥—É–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∏ –±–æ–ª—å—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (LIM) –¥–ª—è —É–∑–∫–æ–π, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –¥–ª—è –∑–∞–¥–∞—á –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏. –ê–≥–µ–Ω—Ç–Ω—ã–π –ò–ò –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–∞—Ä–∞–¥–∏–≥–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–¥–≤–∏–≥, –æ—Ç–º–µ—á–µ–Ω–Ω—ã–π –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–º —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ–º, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π –∑–∞–¥–∞—á –∏ –æ—Ä–∫–µ—Å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ–Ω–æ–º–∏–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –∫–∞–∂–¥–æ–π –ø–∞—Ä–∞–¥–∏–≥–º–µ, –≤–∫–ª—é—á–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, —Ö—Ä—É–ø–∫–æ—Å—Ç—å –∏ –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏.'}, 'en': {'title': 'Understanding AI Agents vs. Agentic AI: A New Frontier in AI Development', 'desc': 'This paper differentiates between AI Agents and Agentic AI, providing a clear framework for understanding their unique characteristics and applications. AI Agents are defined as modular systems that utilize Large Language Models (LLMs) and Large Image Models (LIMs) for specific tasks, while Agentic AI represents a more advanced form that incorporates multi-agent collaboration and dynamic task management. The study evaluates the evolution of these systems, highlighting their operational differences and the challenges they face, such as hallucination and coordination failure. It also suggests solutions like ReAct loops and orchestration layers to enhance the effectiveness of both AI paradigms.'}, 'zh': {'title': 'Âå∫ÂàÜ‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜ‰∏é‰ª£ÁêÜÊô∫ËÉΩÁöÑÊú™Êù•‰πãË∑Ø', 'desc': 'Êú¨Á†îÁ©∂ÊòéÁ°ÆÂå∫ÂàÜ‰∫Ü‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÂíå‰ª£ÁêÜÊô∫ËÉΩÔºåÊèê‰æõ‰∫ÜÁªìÊûÑÂåñÁöÑÊ¶ÇÂøµÂàÜÁ±ª„ÄÅÂ∫îÁî®Êò†Â∞ÑÂíåÊåëÊàòÂàÜÊûêÔºå‰ª•ÈòêÊòéÂÆÉ‰ª¨‰∏çÂêåÁöÑËÆæËÆ°ÁêÜÂøµÂíåËÉΩÂäõ„ÄÇÊàë‰ª¨Â∞Ü‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÂÆö‰πâ‰∏∫Áî±Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§ßÂûãÂõæÂÉèÊ®°ÂûãÔºàLIMsÔºâÈ©±Âä®ÁöÑÊ®°ÂùóÂåñÁ≥ªÁªüÔºå‰∏ªË¶ÅÁî®‰∫éÁâπÂÆö‰ªªÂä°ÁöÑËá™Âä®Âåñ„ÄÇ‰ª£ÁêÜÊô∫ËÉΩÁ≥ªÁªüÂàô‰ª£Ë°®‰∫Ü‰∏ÄÁßçËåÉÂºèËΩ¨ÂèòÔºåÂº∫Ë∞ÉÂ§ö‰ª£ÁêÜÂçè‰Ωú„ÄÅÂä®ÊÄÅ‰ªªÂä°ÂàÜËß£„ÄÅÊåÅ‰πÖËÆ∞ÂøÜÂíåËá™‰∏ªÂçèË∞É„ÄÇÈÄöËøáÂØπ‰∏§ÁßçËåÉÂºèÁöÑÊû∂ÊûÑÊºîÂèò„ÄÅÊìç‰ΩúÊú∫Âà∂„ÄÅ‰∫§‰∫íÈ£éÊ†ºÂíåËá™‰∏ªÊ∞¥Âπ≥ÁöÑÊØîËæÉÂàÜÊûêÔºåÊàë‰ª¨‰∏∫ÂºÄÂèëÂº∫Â§ß„ÄÅÂèØÊâ©Â±ïÂíåÂèØËß£ÈáäÁöÑ‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÂíå‰ª£ÁêÜÊô∫ËÉΩÈ©±Âä®Á≥ªÁªüÊèê‰æõ‰∫ÜÊòéÁ°ÆÁöÑË∑ØÁ∫øÂõæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.08581', 'title': 'ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking', 'url': 'https://huggingface.co/papers/2505.08581', 'abstract': 'Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.', 'score': 7, 'issue_id': 3793, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'}, 'hash': '3dab22f0e497d468', 'authors': ['Haofeng Liu', 'Mingqi Gao', 'Xuxiao Luo', 'Ziyue Wang', 'Guanyi Qin', 'Junde Wu', 'Yueming Jin'], 'affiliations': ['National University of Singapore, Singapore, Singapore', 'Southern University of Science and Technology, Shenzhen, China', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2505.08581.jpg', 'data': {'categories': ['#cv', '#healthcare', '#multimodal'], 'emoji': 'üî™', 'ru': {'title': 'ReSurgSAM2: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ReSurgSAM2 - –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å SAM2 —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mamba –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω—ã–π –≤—ã–±–æ—Ä –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –±–∞–Ω–∫–∞ –ø–∞–º—è—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–∞–±–æ—Ç—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é 61.2 –∫–∞–¥—Ä–∞ –≤ —Å–µ–∫—É–Ω–¥—É.'}, 'en': {'title': 'Revolutionizing Surgical Segmentation with ReSurgSAM2', 'desc': 'This paper presents ReSurgSAM2, a novel framework for surgical scene segmentation that enhances the interactive experience for surgeons. It consists of two main stages: the first stage focuses on text-referred target detection using the Segment Anything Model 2, while the second stage emphasizes reliable tracking through an innovative initial frame selection and a diversity-driven memory mechanism. The proposed method addresses the limitations of existing techniques by improving efficiency and enabling long-term tracking in complex surgical environments. Experimental results show that ReSurgSAM2 significantly outperforms previous methods, achieving real-time performance at 61.2 frames per second.'}, 'zh': {'title': 'ÊèêÂçáÊâãÊúØÂàÜÂâ≤ÊïàÁéá‰∏éÂáÜÁ°ÆÊÄßÁöÑReSurgSAM2Ê°ÜÊû∂', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ReSurgSAM2ÁöÑÊâãÊúØÂú∫ÊôØÂàÜÂâ≤Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òËÆ°ÁÆóÊú∫ËæÖÂä©ÊâãÊúØÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∫Ü‰∏§Èò∂ÊÆµÁöÑÊâãÊúØÂèÇËÄÉÂàÜÂâ≤ÊñπÊ≥ïÔºåÂà©Áî®Segment Anything Model 2ËøõË°åÊñáÊú¨ÂºïÁî®ÁõÆÊ†áÊ£ÄÊµãÔºåÂπ∂ÈÄöËøáÂèØÈù†ÁöÑÂàùÂßãÂ∏ßËØÜÂà´ÂíåÂ§öÊ†∑ÊÄßÈ©±Âä®ÁöÑÈïøÊúüËÆ∞ÂøÜËøõË°åË∑üË∏™„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçË∑®Ê®°ÊÄÅÊó∂Á©∫MambaÔºåÁî®‰∫éÁîüÊàêÁ≤æÁ°ÆÁöÑÊ£ÄÊµãÂíåÂàÜÂâ≤ÁªìÊûúÔºåÂπ∂ÈÄöËøáÂèØ‰ø°ÁöÑÂàùÂßãÂ∏ßÈÄâÊã©Á≠ñÁï•Êù•Á°Æ‰øùÂêéÁª≠Ë∑üË∏™ÁöÑÂèØÈù†ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReSurgSAM2Âú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËÉΩÂ§ü‰ª•61.2 FPSÁöÑÂÆûÊó∂ÈÄüÂ∫¶ËøêË°å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.10167', 'title': 'QuXAI: Explainers for Hybrid Quantum Machine Learning Models', 'url': 'https://huggingface.co/papers/2505.10167', 'abstract': 'The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still in its infancy, a major research gap is evident in robust global and local explainability approaches that are designed for HQML architectures that employ quantized feature encoding followed by classical learning. The gap is the focus of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an explainer for explaining feature importance in these hybrid systems. Our model entails the creation of HQML models incorporating quantum feature maps, the use of Q-MEDLEY, which combines feature based inferences, preserving the quantum transformation stage and visualizing the resulting attributions. Our result shows that Q-MEDLEY delineates influential classical aspects in HQML models, as well as separates their noise, and competes well against established XAI techniques in classical validation settings. Ablation studies more significantly expose the virtues of the composite structure used in Q-MEDLEY. The implications of this work are critically important, as it provides a route to improve the interpretability and reliability of HQML models, thus promoting greater confidence and being able to engage in safer and more responsible use of quantum-enhanced AI technology.', 'score': 5, 'issue_id': 3801, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '924a78566cebf728', 'authors': ['Saikat Barua', 'Mostafizur Rahman', 'Shehenaz Khaled', 'Md Jafor Sadek', 'Rafiul Islam', 'Shahnewaz Siddique'], 'affiliations': ['North South University, Dhaka'], 'pdf_title_img': 'assets/pdf/title_img/2505.10167.jpg', 'data': {'categories': ['#data', '#inference', '#interpretability', '#architecture'], 'emoji': 'üî¨', 'ru': {'title': 'QuXAI: –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–æ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –ò–ò', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QuXAI - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∫–≤–∞–Ω—Ç–æ–≤–æ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (HQML). –í –æ—Å–Ω–æ–≤–µ QuXAI –ª–µ–∂–∏—Ç Q-MEDLEY - –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª—å, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–∞–∫–∏—Ö –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å HQML –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–æ–≤—ã–º–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏–º–µ–Ω—è—Ç—å Q-MEDLEY –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Q-MEDLEY —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –≤ HQML –º–æ–¥–µ–ª—è—Ö –∏ –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ XAI.'}, 'en': {'title': 'Unlocking the Black Box of Hybrid Quantum-Classical AI', 'desc': 'This paper addresses the challenge of understanding hybrid quantum-classical machine learning (HQML) models, which often behave like black boxes. It introduces QuXAI, a framework that enhances explainability in HQML by utilizing Q-MEDLEY, an explainer that assesses feature importance while maintaining the quantum transformation process. The study demonstrates that Q-MEDLEY effectively identifies key classical features and reduces noise in HQML models, outperforming traditional explainable AI (XAI) methods in classical validation tests. Ultimately, this work aims to improve the transparency and reliability of HQML systems, fostering safer and more responsible applications of quantum-enhanced AI.'}, 'zh': {'title': 'ÊèêÂçáÊ∑∑ÂêàÈáèÂ≠ê-ÁªèÂÖ∏Ê®°ÂûãÁöÑÂèØËß£ÈáäÊÄß‰∏éÂèØÈù†ÊÄß', 'desc': 'Ê∑∑ÂêàÈáèÂ≠ê-ÁªèÂÖ∏Êú∫Âô®Â≠¶‰π†ÔºàHQMLÔºâÊ®°ÂûãÁöÑÂá∫Áé∞‰∏∫ËÆ°ÁÆóÊô∫ËÉΩÂºÄËæü‰∫ÜÊñ∞ËßÜÈáéÔºå‰ΩÜÂÖ∂Â§çÊùÇÊÄßÂ∏∏ÂØºËá¥ÈªëÁÆ±Ë°å‰∏∫ÔºåÂΩ±ÂìçÈÄèÊòéÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜQuXAIÊ°ÜÊû∂ÔºåÊó®Âú®Â°´Ë°•ÈíàÂØπHQMLÊû∂ÊûÑÁöÑÂèØËß£ÈáäÊÄßÁ†îÁ©∂Á©∫ÁôΩÔºåÁâπÂà´ÊòØÁªìÂêàÈáèÂ≠êÁâπÂæÅÁºñÁ†ÅÂíåÁªèÂÖ∏Â≠¶‰π†ÁöÑÊ®°Âûã„ÄÇQuXAIÂü∫‰∫éQ-MEDLEYÔºåËÉΩÂ§üËß£ÈáäÁâπÂæÅÈáçË¶ÅÊÄßÔºåÂπ∂ÈÄöËøáÂèØËßÜÂåñÁªìÊûúÂΩíÂõ†Êù•Êè≠Á§∫HQMLÊ®°Âûã‰∏≠ÁöÑÁªèÂÖ∏ÂΩ±ÂìçÂõ†Á¥†„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåQ-MEDLEYÂú®ÁªèÂÖ∏È™åËØÅÁéØÂ¢É‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÊèêÂçá‰∫ÜHQMLÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÂèØÈù†ÊÄßÔºå‰øÉËøõ‰∫ÜÈáèÂ≠êÂ¢ûÂº∫‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑÂÆâÂÖ®‰ΩøÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09926', 'title': 'AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection', 'url': 'https://huggingface.co/papers/2505.09926', 'abstract': 'Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.', 'score': 5, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 –º–∞—è', 'en': 'May 15', 'zh': '5Êúà15Êó•'}, 'hash': '25c3eaaddbc9d5ca', 'authors': ['Bin-Bin Gao', 'Yue Zhu', 'Jiangtao Yan', 'Yuezhi Cai', 'Weixi Zhang', 'Meng Wang', 'Jun Liu', 'Yong Liu', 'Lei Wang', 'Chengjie Wang'], 'affiliations': ['Shanghai Jiao Tong University', 'Siemens Corporate Research', 'Technical University of Munich', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09926.jpg', 'data': {'categories': ['#cv', '#benchmark', '#open_source', '#transfer_learning', '#healthcare'], 'emoji': 'üîç', 'ru': {'title': 'AdaptCLIP: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ CLIP', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º AdaptCLIP. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ CLIP –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. AdaptCLIP –æ–±—É—á–∞–µ—Ç—Å—è –ø–æ–æ—á–µ—Ä–µ–¥–Ω–æ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º, –∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ, —Ç–∞–∫ –∏ –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ 12 –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–º–µ–Ω–∞—Ö.'}, 'en': {'title': 'AdaptCLIP: Simplifying Anomaly Detection with Adaptive Learning', 'desc': 'This paper introduces AdaptCLIP, a novel approach for universal visual anomaly detection that operates without the need for additional fine-tuning. It leverages pre-trained vision-language models like CLIP to generalize effectively from just a few normal images. The method emphasizes learning adaptive visual and textual representations alternately and incorporates comparative learning that utilizes both contextual and aligned residual features. AdaptCLIP demonstrates superior performance on various anomaly detection benchmarks, showcasing its flexibility and efficiency in handling unseen vision domains.'}, 'zh': {'title': 'AdaptCLIPÔºöÊó†ÂæÆË∞ÉÁöÑÈÄöÁî®ËßÜËßâÂºÇÂ∏∏Ê£ÄÊµã', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AdaptCLIPÁöÑÈÄöÁî®ËßÜËßâÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÔºåÊó®Âú®Êó†ÈúÄÈ¢ùÂ§ñÂæÆË∞ÉÂç≥ÂèØËØÜÂà´Êñ∞È¢ñÊàñÊú™ËßÅÁöÑËßÜËßâÈ¢ÜÂüü‰∏≠ÁöÑÂºÇÂ∏∏„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂ¶ÇCLIPÂú®‰ªÖ‰ΩøÁî®Èõ∂ÊàñÂ∞ëÈáèÊ≠£Â∏∏ÂõæÂÉèÊó∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇAdaptCLIPÈÄöËøá‰∫§ÊõøÂ≠¶‰π†Ëá™ÈÄÇÂ∫îÁöÑËßÜËßâÂíåÊñáÊú¨Ë°®Á§∫ÔºåÁªìÂêà‰∏ä‰∏ãÊñáÂíåÂØπÈΩêÁöÑÊÆãÂ∑ÆÁâπÂæÅËøõË°åÊØîËæÉÂ≠¶‰π†Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÁÅµÊ¥ªÊÄß„ÄÇËØ•ÊñπÊ≥ïÂú®12‰∏™Â∑•‰∏öÂíåÂåªÁñóÈ¢ÜÂüüÁöÑÂºÇÂ∏∏Ê£ÄÊµãÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁ´û‰∫âÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09601', 'title': 'Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware', 'url': 'https://huggingface.co/papers/2505.09601', 'abstract': 'Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com', 'score': 4, 'issue_id': 3811, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '65500a5b9d477d08', 'authors': ['Justin Yu', 'Letian Fu', 'Huang Huang', 'Karim El-Refai', 'Rares Andrei Ambrus', 'Richard Cheng', 'Muhammad Zubair Irshad', 'Ken Goldberg'], 'affiliations': ['Toyota Research Institute', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.09601.jpg', 'data': {'categories': ['#optimization', '#dataset', '#synthetic', '#3d', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–æ–≤: –æ—Ç –æ–¥–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫ —Ç—ã—Å—è—á–∞–º —Å–∏–º—É–ª—è—Ü–∏–π', 'desc': 'R2R2R - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–ª–∏ —Ç–µ–ª–µ—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–º–∞—Ä—Ç—Ñ–æ–Ω –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –æ–¥–Ω–æ –≤–∏–¥–µ–æ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—ã—Å—è—á –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π, –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞. R2R2R –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é 3D Gaussian Splatting –¥–ª—è –≥–∏–±–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—Å—Å–µ—Ç–æ–≤ –∏ —Å–∏–Ω—Ç–µ–∑–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∫–∞–∫ –¥–ª—è –∂–µ—Å—Ç–∫–∏—Ö, —Ç–∞–∫ –∏ –¥–ª—è —à–∞—Ä–Ω–∏—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö R2R2R –∏–∑ –æ–¥–Ω–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, –º–æ–≥—É—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å—Å—è –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ 150 –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —Å —Ç–µ–ª–µ—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Revolutionizing Robot Training with R2R2R', 'desc': 'The paper presents Real2Render2Real (R2R2R), a new method for generating training data for robots without the need for expensive human teleoperation or complex simulations. It utilizes smartphone scans and a single video of human actions to create thousands of high-quality, robot-agnostic demonstrations. By employing 3D Gaussian Splatting, R2R2R can flexibly generate 3D models and trajectories for various objects, which are then converted into meshes for use in rendering engines. Experiments show that models trained on data from R2R2R can achieve performance comparable to those trained on a much larger dataset of human demonstrations.'}, 'zh': {'title': 'Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊñ∞ÊñπÊ≥ïÔºöÊó†ÈúÄÈÅ•ÊéßÂç≥ÂèØÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Real2Render2RealÔºàR2R2RÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÁîüÊàêÊú∫Âô®‰∫∫ËÆ≠ÁªÉÊï∞ÊçÆÔºåËÄåÊó†ÈúÄ‰æùËµñÁâ©‰ΩìÂä®ÂäõÂ≠¶Ê®°ÊãüÊàñ‰∫∫Â∑•ÈÅ•Êéß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊô∫ËÉΩÊâãÊú∫ÊçïÊçâÁöÑÁâ©‰ΩìÊâ´ÊèèÂíå‰∫∫Á±ªÊºîÁ§∫ËßÜÈ¢ëÔºåÈáçÂª∫ËØ¶ÁªÜÁöÑ3DÁâ©‰ΩìÂá†‰ΩïÂΩ¢Áä∂ÂíåÂ§ñËßÇÔºåÁîüÊàêÈ´òËßÜËßâ‰øùÁúüÂ∫¶ÁöÑÊú∫Âô®‰∫∫ÊºîÁ§∫„ÄÇR2R2RÂà©Áî®3DÈ´òÊñØÁÇπ‰∫ëÔºà3DGSÔºâÊäÄÊúØÔºåÁÅµÊ¥ªÁîüÊàêËµÑ‰∫ßÂíåËΩ®ËøπÔºåÈÄÇÁî®‰∫éÂàöÊÄßÂíåÂÖ≥ËäÇÁâ©‰ΩìÔºåÂπ∂Â∞ÜËøô‰∫õË°®Á§∫ËΩ¨Êç¢‰∏∫ÁΩëÊ†ºÔºå‰ª•‰æø‰∏éÂèØÊâ©Â±ïÁöÑÊ∏≤ÊüìÂºïÊìéÂÖºÂÆπ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂü∫‰∫éR2R2RÊï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÂèØ‰ª•‰∏éÂü∫‰∫é150‰∏™‰∫∫Â∑•ÈÅ•ÊéßÊºîÁ§∫ËÆ≠ÁªÉÁöÑÊ®°ÂûãÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09265', 'title': 'MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning', 'url': 'https://huggingface.co/papers/2505.09265', 'abstract': 'Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '62287f3647d6b6a7', 'authors': ['Bin-Bin Gao'], 'affiliations': ['Tencent YouTu Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09265.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#synthetic', '#dataset'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –±–µ–∑ —è–∑—ã–∫–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —á–∏—Å—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–∞—Ä–∞–¥–∏–≥–º—É, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∞–Ω–æ–º–∞–ª–∏–π –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MetaUAS, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ –∏ —Å–ø–æ—Å–æ–±–Ω—ã–π –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ —Ç–∏–ø—ã –∞–Ω–æ–º–∞–ª–∏–π. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è zero-shot, few-shot –∏ full-shot —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π.'}, 'en': {'title': 'Revolutionizing Anomaly Segmentation with Pure Vision Models', 'desc': 'This paper introduces a new approach for visual anomaly segmentation that does not depend on language, using a pure visual foundation model instead of traditional vision-language models. The authors propose a unified framework that treats anomaly segmentation as a form of change segmentation, allowing the use of synthetic image pairs to train the model. They develop a Meta-learning framework called MetaUAS, which can effectively identify unseen anomalies with just one normal image prompt. The method includes a soft feature alignment module to address geometric differences, achieving superior performance compared to existing anomaly segmentation techniques.'}, 'zh': {'title': 'Á∫ØËßÜËßâÊ®°ÂûãÂÆûÁé∞ÈÄöÁî®ÂºÇÂ∏∏ÂàÜÂâ≤', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâÂºÇÂ∏∏ÂàÜÂâ≤ÊñπÊ≥ïÔºåÁß∞‰∏∫MetaUASÔºåÊó®Âú®‰ΩøÁî®Á∫ØËßÜËßâÊ®°ÂûãËÄåÈùû‰º†ÁªüÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂºÇÂ∏∏ÂàÜÂâ≤Áªü‰∏Ä‰∏∫ÂèòÂåñÂàÜÂâ≤ÔºåÂà©Áî®Â§ßËßÑÊ®°ÂêàÊàêÂõæÂÉèÂØπËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÂÆûÁé∞ÂØπÊú™Áü•ÂºÇÂ∏∏ÁöÑÊúâÊïàÂàÜÂâ≤„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑËΩØÁâπÂæÅÂØπÈΩêÊ®°ÂùóËÉΩÂ§üÂ§ÑÁêÜÊèêÁ§∫ÂõæÂÉèÂíåÊü•ËØ¢ÂõæÂÉè‰πãÈó¥ÁöÑÂá†‰ΩïÂèòÂåñÔºåÊèêÂçá‰∫ÜÂàÜÂâ≤ÁöÑÂáÜÁ°ÆÊÄß„ÄÇMetaUASÂú®Èõ∂-shot„ÄÅÂ∞ë-shotÂíåÂÖ®-shotÂºÇÂ∏∏ÂàÜÂâ≤ÊñπÊ≥ï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂπøÊ≥õÈÄÇÁî®ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09264', 'title': 'Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt', 'url': 'https://huggingface.co/papers/2505.09264', 'abstract': 'Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': 'd89f10b01e706c42', 'authors': ['Bin-Bin Gao'], 'affiliations': ['Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09264.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#survey', '#dataset'], 'emoji': 'üîç', 'ru': {'title': 'OneNIP: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –æ–¥–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ OneNIP –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, OneNIP —Å–ø–æ—Å–æ–±–µ–Ω —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–Ω—ã–π —É—Ç–æ—á–Ω–∏—Ç–µ–ª—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –ú–µ—Ç–æ–¥ OneNIP –ø—Ä–µ–≤–∑–æ—à–µ–ª —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ —Ç—Ä–µ—Ö –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π.'}, 'en': {'title': 'Revolutionizing Anomaly Detection with One Normal Image Prompt', 'desc': 'This paper introduces a novel method called One Normal Image Prompt (OneNIP) for improving anomaly detection using self-attention transformers. Traditional models struggle with accurately identifying anomalies because they can reconstruct both normal and anomalous features too well, leading to confusion. OneNIP addresses this by allowing the model to reconstruct normal features while effectively restoring anomalies using just one normal image as a reference. Additionally, a supervised refiner is proposed to enhance pixel-level segmentation of anomalies, resulting in superior performance on multiple industry benchmarks.'}, 'zh': {'title': 'Áî®‰∏Ä‰∏™Ê≠£Â∏∏ÂõæÂÉèÊèêÁ§∫ÊèêÂçáÂºÇÂ∏∏Ê£ÄÊµãÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÁõëÁù£ÈáçÂª∫ÁΩëÁªúÊñπÊ≥ïÔºåÁß∞‰∏∫One Normal Image PromptÔºàOneNIPÔºâÔºåÁî®‰∫éÂ§öÁ±ªÂºÇÂ∏∏Ê£ÄÊµã„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåOneNIPÂè™ÈúÄ‰∏Ä‰∏™Ê≠£Â∏∏ÂõæÂÉèÊèêÁ§∫Âç≥ÂèØÈáçÂª∫ÊàñÊÅ¢Â§çÂºÇÂ∏∏ÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂºÇÂ∏∏Ê£ÄÊµãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ÁõëÁù£Á≤æÁÇºÂô®ÔºåÂà©Áî®ÁúüÂÆûÊ≠£Â∏∏ÂõæÂÉèÂíåÂêàÊàêÂºÇÂ∏∏ÂõæÂÉèÊù•ÂõûÂΩíÈáçÂª∫ËØØÂ∑ÆÔºåÊòæËëóÊîπÂñÑ‰∫ÜÂÉèÁ¥†Á∫ßÂºÇÂ∏∏ÂàÜÂâ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOneNIPÂú®Â§ö‰∏™Ë°å‰∏öÂºÇÂ∏∏Ê£ÄÊµãÂü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.09263', 'title': 'Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation', 'url': 'https://huggingface.co/papers/2505.09263', 'abstract': 'Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 –º–∞—è', 'en': 'May 14', 'zh': '5Êúà14Êó•'}, 'hash': '1cde18cecb1ca918', 'authors': ['Guan Gui', 'Bin-Bin Gao', 'Jun Liu', 'Chengjie Wang', 'Yunsheng Wu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09263.jpg', 'data': {'categories': ['#data', '#cv', '#training', '#benchmark', '#dataset', '#synthetic', '#diffusion'], 'emoji': 'üîç', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø—Ä–∏–º–µ—Ä–∞–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AnoGen –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. AnoGen –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –∏–∑—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Å–ª–∞–±–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MVTec –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π DRAEM –∏ DesTSeg –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π.'}, 'en': {'title': 'Generating Realistic Anomalies for Better Detection', 'desc': 'This paper presents a novel method called Few-shot Anomaly-driven Generation (AnoGen) for improving anomaly detection in industrial settings. The approach addresses the challenge of limited real anomaly samples by using a diffusion model to generate realistic anomalies based on a few existing examples. It consists of three stages: learning the anomaly distribution, guiding the generation of diverse anomalies, and training a weakly-supervised detection model. The results show significant improvements in both anomaly classification and segmentation tasks, demonstrating the effectiveness of the generated anomalies in enhancing model performance.'}, 'zh': {'title': 'Â∞ëÊ†∑Êú¨È©±Âä®ÁöÑÂºÇÂ∏∏ÁîüÊàêÔºåÊèêÂçáÊ£ÄÊµãÊÄßËÉΩÔºÅ', 'desc': 'ÂºÇÂ∏∏Ê£ÄÊµãÊòØ‰∏ÄÈ°πÂÆûÈôÖ‰∏îÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°ÔºåÂõ†‰∏∫Â∑•‰∏öÊ£ÄÊµã‰∏≠ÂºÇÂ∏∏Ê†∑Êú¨Á®ÄÁº∫„ÄÇÁé∞ÊúâÁöÑ‰∏Ä‰∫õÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÈÄöËøáÂêàÊàêÂô™Â£∞ÊàñÂ§ñÈÉ®Êï∞ÊçÆÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ΩÜÂêàÊàêÂºÇÂ∏∏‰∏éÁúüÂÆûÂºÇÂ∏∏‰πãÈó¥Â≠òÂú®ËæÉÂ§ßÁöÑËØ≠‰πâÂ∑ÆË∑ùÔºåÂØºËá¥Ê£ÄÊµãÊÄßËÉΩËæÉÂº±„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∞ëÈáèÊ†∑Êú¨È©±Âä®ÁöÑÂºÇÂ∏∏ÁîüÊàêÊñπÊ≥ïÔºàAnoGenÔºâÔºåËØ•ÊñπÊ≥ïÂà©Áî®Â∞ëÈáèÁúüÂÆûÂºÇÂ∏∏ÊåáÂØºÊâ©Êï£Ê®°ÂûãÁîüÊàêÁúüÂÆû‰∏îÂ§öÊ†∑ÁöÑÂºÇÂ∏∏Ôºå‰ªéËÄåÊúâÂà©‰∫éËÆ≠ÁªÉÂºÇÂ∏∏Ê£ÄÊµãÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÁîüÊàêÁöÑÂºÇÂ∏∏ÊúâÊïàÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®ÂºÇÂ∏∏ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07096', 'title': 'X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real', 'url': 'https://huggingface.co/papers/2505.07096', 'abstract': 'Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.', 'score': 3, 'issue_id': 3806, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 –º–∞—è', 'en': 'May 11', 'zh': '5Êúà11Êó•'}, 'hash': 'cb9029d3f1641bd2', 'authors': ['Prithwish Dan', 'Kushal Kedia', 'Angela Chao', 'Edward Weiyi Duan', 'Maximus Adrian Pace', 'Wei-Chiu Ma', 'Sanjiban Choudhury'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07096.jpg', 'data': {'categories': ['#synthetic', '#rl', '#training', '#robotics', '#optimization', '#transfer_learning'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è —É –ª—é–¥–µ–π –±–µ–∑ –ø—Ä—è–º–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π', 'desc': 'X-Sim - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥ –≤ —Å–∏–º—É–ª—è—Ü–∏–∏. –ó–∞—Ç–µ–º –æ–±—É—á–µ–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–∞. X-Sim –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –Ω–∞ 30% –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–∞–Ω–Ω—ã—Ö —Ç–µ–ª–µ–æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞.'}, 'en': {'title': 'X-Sim: Bridging Human Videos to Robot Actions Efficiently', 'desc': 'The paper introduces X-Sim, a framework designed to train robot manipulation policies using human videos without needing action labels. It reconstructs a realistic simulation from RGBD videos and tracks object movements to create rewards for reinforcement learning. The learned policy is then refined into a diffusion policy that adapts to real-world conditions through online domain adaptation. X-Sim demonstrates significant improvements in task performance and efficiency compared to existing methods, showcasing its ability to generalize across different environments and viewpoints.'}, 'zh': {'title': 'X-SimÔºö‰ªé‰∫∫Á±ªËßÜÈ¢ëÂà∞Êú∫Âô®‰∫∫Êìç‰ΩúÁöÑÊô∫ËÉΩËΩ¨Áßª', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫X-SimÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÈÄöËøá‰∫∫Á±ªËßÜÈ¢ëËÆ≠ÁªÉÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•„ÄÇX-SimÂà©Áî®Áâ©‰ΩìËøêÂä®‰Ωú‰∏∫ÂØÜÈõÜ‰∏îÂèØËΩ¨ÁßªÁöÑ‰ø°Âè∑ÔºåÈ¶ñÂÖà‰ªéRGBD‰∫∫Á±ªËßÜÈ¢ëÈáçÂª∫Âá∫ÈÄºÁúüÁöÑ‰ªøÁúüÁéØÂ¢ÉÔºåÂπ∂Ë∑üË∏™Áâ©‰ΩìËΩ®Ëøπ‰ª•ÂÆö‰πâ‰ª•Áâ©‰Ωì‰∏∫‰∏≠ÂøÉÁöÑÂ•ñÂä±„ÄÇÁÑ∂ÂêéÔºåËøô‰∫õÂ•ñÂä±Áî®‰∫éÂú®‰ªøÁúü‰∏≠ËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁ≠ñÁï•ÔºåÂπ∂ÈÄöËøáÂêàÊàêÁöÑËßÜËßíÂíåÂÖâÁÖßÂèòÂåñÊ∏≤ÊüìÁöÑÂõûÊîæÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ÊèêÁÇº‰∏∫ÂõæÂÉèÊù°‰ª∂Êâ©Êï£Á≠ñÁï•„ÄÇÊúÄÂêéÔºåX-SimÂºïÂÖ•Âú®Á∫øÈ¢ÜÂüüÈÄÇÂ∫îÊäÄÊúØÔºåÂú®ÈÉ®ÁΩ≤ËøáÁ®ã‰∏≠ÂØπÁúüÂÆûÂíå‰ªøÁúüËßÇÂØüËøõË°åÂØπÈΩêÔºå‰ªéËÄåÂÆûÁé∞Êó†Êú∫Âô®‰∫∫ÈÅ•ÊéßÊï∞ÊçÆÁöÑÊúâÊïàËΩ¨Áßª„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (29)', '#agents (73)', '#agi (22)', '#alignment (52)', '#architecture (106)', '#audio (21)', '#benchmark (267)', '#cv (89)', '#data (98)', '#dataset (212)', '#diffusion (55)', '#ethics (20)', '#games (51)', '#graphs (11)', '#hallucinations (35)', '#healthcare (25)', '#inference (70)', '#interpretability (64)', '#leakage (8)', '#long_context (21)', '#low_resource (26)', '#machine_translation (10)', '#math (49)', '#multilingual (32)', '#multimodal (181)', '#open_source (123)', '#optimization (275)', '#plp (5)', '#rag (24)', '#reasoning (236)', '#rl (124)', '#rlhf (60)', '#robotics (24)', '#science (33)', '#security (28)', '#small_models (16)', '#story_generation (2)', '#survey (26)', '#synthetic (39)', '#training (296)', '#transfer_learning (40)', '#video (48)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-05-28 18:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-28 18:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-28 18:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    