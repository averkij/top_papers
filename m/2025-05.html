
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 159 papers. May 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞœĞ°Ğ¹ 2025</span> | <span id="title-articles-count">159 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-04.html">â¬…ï¸ <span id="prev-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-06.html">â¡ï¸ <span id="next-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞœĞ°Ğ¹ 2025', 'en': 'May 2025', 'zh': '5æœˆ2025å¹´'};
        let feedDateNext = {'ru': '06.2025', 'en': '06/2025', 'zh': '6æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '04.2025', 'en': '04/2025', 'zh': '4æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.20438', 'title': 'PixelHacker: Image Inpainting with Structural and Semantic Consistency', 'url': 'https://huggingface.co/papers/2504.20438', 'abstract': 'Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/PixelHacker.', 'score': 23, 'issue_id': 3584, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': '987ce511e3c86e06', 'authors': ['Ziyang Xu', 'Kangsheng Duan', 'Xiaolei Shen', 'Zhifeng Ding', 'Wenyu Liu', 'Xiaohu Ruan', 'Xiaoxin Chen', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'VIVO AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.20438.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#open_source', '#optimization', '#diffusion', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'PixelHacker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PixelHacker. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¼Ğ°ÑĞºĞ° Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°. PixelHacker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PixelHacker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'PixelHacker: Revolutionizing Image Inpainting with Latent Categories Guidance', 'desc': 'This paper presents a new approach to image inpainting called PixelHacker, which aims to improve the quality of generated images by addressing issues with complex structures and semantics. The authors introduce a large dataset of 14 million image-mask pairs to train their model, focusing on distinguishing between foreground and background categories. They utilize a diffusion-based model that incorporates linear attention to enhance the denoising process, ensuring better consistency in texture and color. Experimental results demonstrate that PixelHacker significantly outperforms existing state-of-the-art methods across various datasets, achieving superior image restoration results.'}, 'zh': {'title': 'PixelHackerï¼šå›¾åƒä¿®å¤çš„æ–°çªç ´', 'desc': 'å›¾åƒä¿®å¤æ˜¯å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆä¹‹é—´çš„ä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•æ¢ç´¢äº†æ–°é¢–çš„æ³¨æ„åŠ›æœºåˆ¶ã€è½»é‡çº§æ¶æ„å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œè¯­ä¹‰æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒå‡ºç°ä¼ªå½±å’Œä¸å½“ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ä¿®å¤èŒƒå¼ï¼Œç§°ä¸ºæ½œåœ¨ç±»åˆ«å¼•å¯¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹PixelHackerã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01079', 'title': 'Improving Editability in Image Generation with Layer-wise Memory', 'url': 'https://huggingface.co/papers/2505.01079', 'abstract': 'Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.', 'score': 18, 'issue_id': 3582, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ', 'en': 'May 2', 'zh': '5æœˆ2æ—¥'}, 'hash': 'e1aa83ea7926943e', 'authors': ['Daneul Kim', 'Jaeah Lee', 'Jaesik Park'], 'affiliations': ['Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01079.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº, Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Background Consistency Guidance Ğ¸ Multi-Query Disentanglement. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Seamless Sequential Image Editing with Context Preservation', 'desc': 'This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input.'}, 'zh': {'title': 'å®ç°è‡ªç„¶è¿ç»­çš„å›¾åƒç¼–è¾‘', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç¼–è¾‘ä¸­çš„å¤šæ¬¡è¿ç»­ç¼–è¾‘é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªå¯¹è±¡çš„ä¿®æ”¹æ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒä¹‹å‰ç¼–è¾‘å†…å®¹çš„åŒæ—¶è‡ªç„¶åœ°èå…¥æ–°å¯¹è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æ–¹æ¡ˆï¼šä¸€æ˜¯æ”¯æŒç²—ç•¥çš„æ©è†œè¾“å…¥ï¼Œä»¥ä¿ç•™ç°æœ‰å†…å®¹å¹¶è‡ªç„¶æ•´åˆæ–°å…ƒç´ ï¼›äºŒæ˜¯æ”¯æŒå¤šæ¬¡ä¿®æ”¹çš„ä¸€è‡´æ€§ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å±‚çº§è®°å¿†å­˜å‚¨å…ˆå‰ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºå’Œæç¤ºåµŒå…¥ï¼Œåˆ©ç”¨èƒŒæ™¯ä¸€è‡´æ€§å¼•å¯¼ä¿æŒåœºæ™¯çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿­ä»£å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”¨æˆ·åªéœ€æä¾›ç²—ç•¥æ©è†œå³å¯å®ç°é«˜è´¨é‡çš„ç¼–è¾‘æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21117', 'title': 'Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts', 'url': 'https://huggingface.co/papers/2504.21117', 'abstract': 'Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.', 'score': 11, 'issue_id': 3587, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': '2a43e27932acf80e', 'authors': ['Hanhua Hong', 'Chenghao Xiao', 'Yang Wang', 'Yiqi Liu', 'Wenge Rong', 'Chenghua Lin'], 'affiliations': ['Beihang University', 'Durham University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2504.21117.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#benchmark', '#interpretability'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ NLG ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (NLG) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ†. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚Ñ€ÑƒĞ´Ğ¾ĞµĞ¼ĞºĞ¾Ğ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM).'}, 'en': {'title': 'Revolutionizing NLG Evaluation with Inversion Learning', 'desc': 'This paper addresses the difficulties in evaluating natural language generation (NLG) systems, particularly the inconsistencies and biases in human evaluations. It introduces an inversion learning method that creates effective prompts for evaluating models by learning from their outputs. This approach allows for automatic generation of tailored evaluation prompts, requiring only one sample, which enhances efficiency. The proposed method aims to improve the robustness of LLM-based evaluations, paving the way for more standardized assessment in NLG.'}, 'zh': {'title': 'æå‡è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°çš„æ•ˆç‡ä¸ç¨³å¥æ€§', 'desc': 'è¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ç³»ç»Ÿæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæœ‰æ•ˆè¾“å‡ºçš„å¤šæ ·æ€§ä½¿å¾—æ ‡å‡†åŒ–è¯„ä¼°å˜å¾—å›°éš¾ã€‚è™½ç„¶äººå·¥è¯„ä¼°è¢«è®¤ä¸ºæ˜¯é‡‘æ ‡å‡†ï¼Œä½†å…¶å­˜åœ¨ä¸ä¸€è‡´æ€§ã€ç¼ºä¹æ ‡å‡†åŒ–å’Œäººå£åè§ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å¯é‡å¤æ€§ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¯¹æç¤ºè®¾è®¡éå¸¸æ•æ„Ÿï¼Œå¾®å°çš„å˜åŒ–å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åæ¼”å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»æ¨¡å‹è¾“å‡ºåå‘æ˜ å°„åˆ°è¾“å…¥æŒ‡ä»¤ï¼Œä»è€Œè‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆçš„ã€ç‰¹å®šäºæ¨¡å‹çš„è¯„ä¼°æç¤ºï¼Œæå‡äº†è¯„ä¼°çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00949', 'title': 'Llama-Nemotron: Efficient Reasoning Models', 'url': 'https://huggingface.co/papers/2505.00949', 'abstract': 'We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.', 'score': 8, 'issue_id': 3587, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ', 'en': 'May 2', 'zh': '5æœˆ2æ—¥'}, 'hash': 'cbc28025b0c6bde3', 'authors': ['Akhiad Bercovich', 'Itay Levy', 'Izik Golan', 'Mohammad Dabbah', 'Ran El-Yaniv', 'Omri Puny', 'Ido Galil', 'Zach Moshe', 'Tomer Ronen', 'Najeeb Nabwani', 'Ido Shahaf', 'Oren Tropp', 'Ehud Karpas', 'Ran Zilberstein', 'Jiaqi Zeng', 'Soumye Singhal', 'Alexander Bukharin', 'Yian Zhang', 'Tugrul Konuk', 'Gerald Shen', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Yoshi Suhara', 'Olivier Delalleau', 'Zijia Chen', 'Zhilin Wang', 'David Mosallanezhad', 'Adi Renduchintala', 'Haifeng Qian', 'Dima Rekesh', 'Fei Jia', 'Somshubra Majumdar', 'Vahid Noroozi', 'Wasi Uddin Ahmad', 'Sean Narenthiran', 'Aleksander Ficek', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Igor Gitman', 'Ivan Moshkov', 'Wei Du', 'Shubham Toshniwal', 'George Armstrong', 'Branislav Kisacanin', 'Matvei Novikov', 'Daria Gitman', 'Evelina Bakhturina', 'Jane Polak Scowcroft', 'John Kamalu', 'Dan Su', 'Kezhi Kong', 'Markus Kliegl', 'Rabeeh Karimi', 'Ying Lin', 'Sanjeev Satheesh', 'Jupinder Parmar', 'Pritam Gundecha', 'Brandon Norick', 'Joseph Jennings', 'Shrimai Prabhumoye', 'Syeda Nahida Akter', 'Mostofa Patwary', 'Abhinav Khattar', 'Deepak Narayanan', 'Roger Waleffe', 'Jimmy Zhang', 'Bor-Yiing Su', 'Guyue Huang', 'Terry Kong', 'Parth Chadha', 'Sahil Jain', 'Christine Harvey', 'Elad Segal', 'Jining Huang', 'Sergey Kashirsky', 'Robert McQueen', 'Izzy Putterman', 'George Lam', 'Arun Venkatesan', 'Sherry Wu', 'Vinh Nguyen', 'Manoj Kilaru', 'Andrew Wang', 'Anna Warno', 'Abhilash Somasamudramath', 'Sandip Bhaskar', 'Maka Dong', 'Nave Assaf', 'Shahar Mor', 'Omer Ullman Argov', 'Scot Junkin', 'Oleksandr Romanenko', 'Pedro Larroy', 'Monika Katariya', 'Marco Rovinelli', 'Viji Balas', 'Nicholas Edelman', 'Anahita Bhiwandiwalla', 'Muthu Subramaniam', 'Smita Ithape', 'Karthik Ramamoorthy', 'Yuting Wu', 'Suguna Varshini Velury', 'Omri Almog', 'Joyjit Daw', 'Denys Fridman', 'Erick Galinkin', 'Michael Evans', 'Katherine Luna', 'Leon Derczynski', 'Nikki Pope', 'Eileen Long', 'Seth Schneider', 'Guillermo Siman', 'Tomasz Grzegorzek', 'Pablo Ribalta', 'Monika Katariya', 'Joey Conway', 'Trisha Saar', 'Ann Guan', 'Krzysztof Pawelec', 'Shyamala Prayaga', 'Oleksii Kuchaiev', 'Boris Ginsburg', 'Oluwatobi Olabiyi', 'Kari Briski', 'Jonathan Cohen', 'Bryan Catanzaro', 'Jonah Alben', 'Yonatan Geifman', 'Eric Chung'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00949.jpg', 'data': {'categories': ['#agi', '#training', '#rl', '#open_source', '#architecture', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-Nemotron - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… (8B, 49B, 253B) Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Efficiency', 'desc': 'The Llama-Nemotron series introduces a new family of reasoning models designed for efficient inference and strong reasoning capabilities. These models come in three sizes, allowing flexibility for different applications while maintaining competitive performance against leading models. The training process involves advanced techniques like neural architecture search, knowledge distillation, and reinforcement learning to enhance reasoning abilities. Additionally, these models are open-source, providing resources for further research and development in the machine learning community.'}, 'zh': {'title': 'å¼€æ”¾æ¨ç†æ¨¡å‹ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼', 'desc': 'Llama-Nemotronç³»åˆ—æ¨¡å‹æ˜¯ä¸€ç§å¼€æ”¾çš„å¼‚æ„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰å“è¶Šçš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„æ¨ç†æ€§èƒ½ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬ä¸‰ç§ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼šNanoï¼ˆ8Bï¼‰ã€Superï¼ˆ49Bï¼‰å’ŒUltraï¼ˆ253Bï¼‰ï¼Œåœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨äº†ç¥ç»æ¶æ„æœç´¢ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œæœ€åé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†ä¸“æ³¨çš„åè®­ç»ƒé˜¶æ®µã€‚Llama-Nemotronæ¨¡å‹æ˜¯é¦–ä¸ªæ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢çš„å¼€æºæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åœ¨æ ‡å‡†èŠå¤©æ¨¡å¼å’Œæ¨ç†æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00174', 'title': 'Real-World Gaps in AI Governance Research', 'url': 'https://huggingface.co/papers/2505.00174', 'abstract': 'Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.', 'score': 8, 'issue_id': 3582, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '7618edbafcee6b13', 'authors': ['Ilan Strauss', 'Isobel Moure', "Tim O'Reilly", 'Sruly Rosenblat'], 'affiliations': ['AI Disclosures Project, Social Science Research Council', 'Institute for Innovation and Public Purpose, University College London', 'OReilly Media'], 'pdf_title_img': 'assets/pdf/title_img/2505.00174.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#alignment', '#healthcare', '#hallucinations', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ 1178 Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· 9439 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ñ ÑĞ½Ğ²Ğ°Ñ€Ñ 2020 Ğ¿Ğ¾ Ğ¼Ğ°Ñ€Ñ‚ 2025 Ğ³Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ»Ğ°Ğ±ĞµĞ²Ğ°ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ.'}, 'en': {'title': 'Bridging the Gap: Enhancing AI Safety in Deployment', 'desc': 'This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps.'}, 'zh': {'title': 'å…³æ³¨äººå·¥æ™ºèƒ½éƒ¨ç½²é˜¶æ®µçš„ç ”ç©¶ç¼ºå£', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†1178ç¯‡å®‰å…¨æ€§å’Œå¯é æ€§è®ºæ–‡ä¸9439ç¯‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è®ºæ–‡ï¼Œæ¯”è¾ƒäº†ä¸»è¦äººå·¥æ™ºèƒ½å…¬å¸å’Œå¤§å­¦çš„ç ”ç©¶æˆæœã€‚ç ”ç©¶å‘ç°ï¼Œä¼ä¸šçš„äººå·¥æ™ºèƒ½ç ”ç©¶è¶Šæ¥è¶Šé›†ä¸­åœ¨æ¨¡å‹å¯¹é½å’Œæµ‹è¯•è¯„ä¼°ç­‰é¢„éƒ¨ç½²é¢†åŸŸï¼Œè€Œå¯¹éƒ¨ç½²é˜¶æ®µé—®é¢˜å¦‚æ¨¡å‹åè§çš„å…³æ³¨æœ‰æ‰€å‡å°‘ã€‚é«˜é£é™©éƒ¨ç½²é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èã€è™šå‡ä¿¡æ¯ç­‰ï¼‰å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†æ”¹å–„å¯¹å·²éƒ¨ç½²äººå·¥æ™ºèƒ½çš„å¯è§‚å¯Ÿæ€§ï¼Œå»ºè®®æ‰©å¤§å¤–éƒ¨ç ”ç©¶äººå‘˜å¯¹éƒ¨ç½²æ•°æ®çš„è®¿é—®ï¼Œå¹¶ç³»ç»ŸåŒ–å¸‚åœºä¸­äººå·¥æ™ºèƒ½è¡Œä¸ºçš„å¯è§‚å¯Ÿæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00023', 'title': 'CORG: Generating Answers from Complex, Interrelated Contexts', 'url': 'https://huggingface.co/papers/2505.00023', 'abstract': 'In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.', 'score': 5, 'issue_id': 3582, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 25', 'zh': '4æœˆ25æ—¥'}, 'hash': '46da290a5c894311', 'authors': ['Hyunji Lee', 'Franck Dernoncourt', 'Trung Bui', 'Seunghyun Yoon'], 'affiliations': ['Adobe Research', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.00023.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#graphs', '#architecture', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CORG: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Context Organizer (CORG) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. CORG Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CORG ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸.'}, 'en': {'title': 'Organizing Contexts for Better Language Understanding', 'desc': 'This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods.'}, 'zh': {'title': 'ä¸Šä¸‹æ–‡ç»„ç»‡ï¼Œæå‡æ¨¡å‹æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'åœ¨ç°å®ä¸–ç•Œçš„è¯­æ–™åº“ä¸­ï¼ŒçŸ¥è¯†ç»å¸¸åœ¨æ–‡æ¡£ä¸­é‡å¤å‡ºç°ï¼Œä½†ç”±äºå‘½åæ¨¡ç³Šã€ä¿¡æ¯è¿‡æ—¶æˆ–é”™è¯¯ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ä¹‹é—´å­˜åœ¨å¤æ‚çš„ç›¸äº’å…³ç³»ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤æ‚æ€§æ—¶é€šå¸¸åªå…³æ³¨å•ä¸€å› ç´ ã€‚æˆ‘ä»¬å°†è¿™äº›å…³ç³»åˆ†ä¸ºå››ç§ç±»å‹ï¼šå¹²æ‰°ã€æ¨¡ç³Šã€åäº‹å®å’Œé‡å¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡ç»„ç»‡å™¨ï¼ˆCORGï¼‰ï¼Œå®ƒå°†å¤šä¸ªä¸Šä¸‹æ–‡ç»„ç»‡æˆç‹¬ç«‹å¤„ç†çš„ç»„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00562', 'title': 'TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching', 'url': 'https://huggingface.co/papers/2505.00562', 'abstract': "Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': 'bf5b246f5848fa6e', 'authors': ['Yue Meng', 'Chuchu Fan'], 'affiliations': ['Department of Aeronautics and Astronautics, MIT, Cambridge, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.00562.jpg', 'data': {'categories': ['#dataset', '#inference', '#agents', '#robotics', '#graphs', '#optimization'], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TeLoGraF - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² (STL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ flow-matching Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ STL-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ¿ÑÑ‚Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… 2D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ TeLoGraF Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'TeLoGraF: Fast and Robust Solutions for Complex Temporal Logic Tasks', 'desc': 'This paper introduces TeLoGraF, a novel approach that leverages Graph Neural Networks (GNN) to effectively learn solutions for complex tasks defined by signal temporal logic (STL) specifications. The authors address the limitations of previous methods that relied on fixed STL templates by creating a diverse dataset of 200,000 STL specifications paired with demonstrations. Through extensive experiments across various simulation environments, TeLoGraF demonstrates superior performance in STL satisfaction rates and significantly faster inference times compared to traditional STL planning algorithms. Additionally, the graph-encoding technique shows robustness in handling complex and out-of-distribution STL specifications, making it a versatile tool for real-world applications.'}, 'zh': {'title': 'TeLoGraFï¼šé«˜æ•ˆè§£å†³å¤æ‚æ—¶åºé€»è¾‘ä»»åŠ¡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•TeLoGraFï¼Œç”¨äºè§£å†³å¤æ‚ä»»åŠ¡çš„ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰è§„èŒƒã€‚æˆ‘ä»¬åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç¼–ç å™¨å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œå­¦ä¹ é€šç”¨STLè§„èŒƒçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ”¶é›†20ä¸‡ä¸ªé…å¯¹ç¤ºä¾‹ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨STLæ»¡è¶³ç‡ä¸Šä¼˜äºå…¶ä»–åŸºçº¿ã€‚ä¸ä¼ ç»Ÿçš„STLè§„åˆ’ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦ä¸Šå¿«10åˆ°100å€ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€‚åº”ä»»ä½•ç³»ç»ŸåŠ¨æ€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.20859', 'title': 'X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2504.20859', 'abstract': "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.", 'score': 2, 'issue_id': 3583, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': '2102f697cfc2375e', 'authors': ['Guy Hadad', 'Haggai Roitman', 'Yotam Eshel', 'Bracha Shapira', 'Lior Rokach'], 'affiliations': ['Ben-Gurion University of the Negev Beer Sheva, Israel', 'eBay Netanya, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.20859.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#low_resource', '#training', '#multimodal'], 'emoji': 'ğŸ”€', 'ru': {'title': 'X-Cross: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ X-Cross Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ (LoRA). X-Cross Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Amazon Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ X-Cross Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ LoRA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 25% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ½Ğ° 50-75% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'X-Cross: Efficient Cross-Domain Recommendations with Minimal Data', 'desc': "The paper introduces 'X-Cross', a new model designed for cross-domain sequential recommendations that can quickly adapt to new product categories without extensive retraining. It utilizes multiple domain-specific language models, each fine-tuned with low-rank adapters (LoRA), to enhance the recommendation process. By refining the representations of these models layer by layer, X-Cross effectively integrates knowledge from different domains while maintaining their unique characteristics. The model shows strong performance on Amazon datasets, requiring significantly less fine-tuning data and parameters compared to traditional methods, making it efficient for data-limited scenarios."}, 'zh': {'title': 'X-Crossï¼šé«˜æ•ˆçš„è·¨é¢†åŸŸæ¨èè§£å†³æ–¹æ¡ˆ', 'desc': 'éšç€æ–°äº§å“çš„ä¸æ–­æ¶Œç°ï¼Œæ¨èç³»ç»Ÿéœ€è¦å¿«é€Ÿé€‚åº”æ–°é¢†åŸŸï¼Œè€Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†â€œX-Crossâ€æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è·¨é¢†åŸŸåºåˆ—æ¨èæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹æ¥æ¨èæ–°é¢†åŸŸçš„äº§å“ã€‚X-Crossé€šè¿‡é€å±‚æ“ä½œåŠ¨æ€åœ°ä¼˜åŒ–æ¯ä¸ªæºè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºï¼Œç¡®ä¿åœ¨è·¨é¢†åŸŸé€‚åº”æ—¶ä¿ç•™é¢†åŸŸç‰¹æœ‰çš„ç»†å¾®å·®åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Crossåœ¨è·¨é¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ‰€éœ€çš„å¾®è°ƒæ•°æ®é‡æ˜¾è‘—ä½äºä¼ ç»Ÿæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03318', 'title': 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.03318', 'abstract': "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.", 'score': 63, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'f0871f80f0b8fdd9', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.03318.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#multimodal', '#training', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ UnifiedReward-Think - Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Vision Models with Long-Chain Reasoning', 'desc': 'This paper introduces UnifiedReward-Think, a novel multimodal reward model that enhances the alignment of vision models with human preferences through long-chain reasoning. By integrating explicit chains of thought (CoT) into the reward reasoning process, the model improves the accuracy and reliability of reward signals. The approach involves a two-step training process: first, distilling reasoning from a small dataset, and then fine-tuning with large-scale multimodal preference data. Experimental results show that this method significantly outperforms existing models in various vision tasks, demonstrating its effectiveness in complex reasoning scenarios.'}, 'zh': {'title': 'é•¿é“¾æ€ç»´æå‡å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„å¯é æ€§', 'desc': 'æœ€è¿‘åœ¨å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ–¹é¢çš„è¿›å±•æ˜¾ç¤ºå‡ºå°†è§†è§‰æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„RMsé€šå¸¸åªèƒ½æä¾›ç›´æ¥å“åº”æˆ–è¿›è¡Œæµ…å±‚æ¨ç†ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸å‡†ç¡®ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†æ˜ç¡®çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰çº³å…¥å¥–åŠ±æ¨ç†è¿‡ç¨‹å¯ä»¥æ˜¾è‘—å¢å¼ºå…¶å¯é æ€§å’Œç¨³å¥æ€§ã€‚æœ¬æ–‡æå‡ºäº†UnifiedReward-Thinkï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºCoTçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šç»´åº¦ã€é€æ­¥çš„é•¿é“¾æ¨ç†ï¼Œé€‚ç”¨äºè§†è§‰ç†è§£å’Œç”Ÿæˆå¥–åŠ±ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03335', 'title': 'Absolute Zero: Reinforced Self-play Reasoning with Zero Data', 'url': 'https://huggingface.co/papers/2505.03335', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.', 'score': 59, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'b53e736d1884218d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rlhf', '#training', '#rl', '#math', '#optimization', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ˜Ğ˜: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Absolute Zero. Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Absolute Zero Reasoner (AZR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ñ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, AZR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Self-Learning AI: No Data, No Problem!', 'desc': 'This paper introduces a new approach in reinforcement learning called Absolute Zero, which allows a model to learn and improve its reasoning skills without needing external data or human supervision. The proposed Absolute Zero Reasoner (AZR) autonomously generates tasks that enhance its learning and validates its own reasoning through a code executor. This self-sufficient learning method leads to state-of-the-art performance in coding and mathematical reasoning tasks, surpassing models that rely on large datasets of human-created examples. The findings suggest that AZR can adapt to different model sizes and types, showcasing its versatility and potential for future AI development.'}, 'zh': {'title': 'ç»å¯¹é›¶ï¼šè‡ªæˆ‘è¿›åŒ–çš„æ¨ç†æ¨¡å‹', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç›´æ¥ä»ç»“æœå¯¼å‘çš„å¥–åŠ±ä¸­å­¦ä¹ ã€‚æœ€è¿‘çš„RLVRç ”ç©¶åœ¨é›¶è®¾ç½®ä¸‹è¿è¡Œï¼Œé¿å…äº†å¯¹æ¨ç†è¿‡ç¨‹çš„ç›‘ç£ï¼Œä½†ä»ä¾èµ–äºäººå·¥ç­–åˆ’çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚ç”±äºé«˜è´¨é‡äººç±»ç”Ÿæˆç¤ºä¾‹çš„ç¨€ç¼ºæ€§ï¼Œä¾èµ–äººç±»ç›‘ç£çš„é•¿æœŸå¯æ‰©å±•æ€§å—åˆ°è´¨ç–‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼ï¼Œç§°ä¸ºç»å¯¹é›¶ï¼ˆAbsolute Zeroï¼‰ï¼Œè¯¥èŒƒå¼ä¸‹çš„æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æå‡ºä»»åŠ¡ä»¥æœ€å¤§åŒ–å­¦ä¹ è¿›å±•ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2505.03005', 'title': 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale', 'url': 'https://huggingface.co/papers/2505.03005', 'abstract': "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper", 'score': 22, 'issue_id': 3625, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '0fe1c0b1575b6708', 'authors': ['Daniel Goldstein', 'Eric Alcaide', 'Janna Lu', 'Eugene Cheah'], 'affiliations': ['Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'EleutherAI', 'George Mason University', 'Recursal AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.03005.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#inference', '#benchmark', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RWKV Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B, 32B Ğ¸ 72B. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 350-700 Ğ¼Ğ»Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 0,005% Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Transforming Transformers: Efficient Linear Attention Models with RADLADS', 'desc': 'The paper introduces Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a method for transforming softmax attention transformers into efficient linear attention models. This conversion process is highly efficient, requiring only a small fraction of the original training data, specifically 350-700M tokens. Despite the reduced training cost, the resulting 72B linear attention model maintains performance levels comparable to its transformer counterparts. The authors also present new RWKV-variant architectures and make their models available on HuggingFace, demonstrating state-of-the-art results on standard benchmarks for linear attention models.'}, 'zh': {'title': 'å¿«é€Ÿè½¬æ¢ï¼Œçº¿æ€§æ³¨æ„åŠ›çš„æœªæ¥', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¿«é€Ÿæ³¨æ„åŠ›è’¸é¦åˆ°çº¿æ€§æ³¨æ„è§£ç å™¨çš„åè®®ï¼ˆRADLADSï¼‰ï¼Œå¯ä»¥è¿…é€Ÿå°†è½¯æœ€å¤§æ³¨æ„åŠ›å˜æ¢å™¨è½¬æ¢ä¸ºçº¿æ€§æ³¨æ„è§£ç æ¨¡å‹ã€‚æˆ‘ä»¬çš„è½¬æ¢è¿‡ç¨‹åªéœ€350-700Mä¸ªæ ‡è®°ï¼Œè¿œä½äºåŸå§‹æ•™å¸ˆæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„0.005%çš„æ ‡è®°æ•°é‡ã€‚è½¬æ¢ä¸ºæˆ‘ä»¬çš„72Bçº¿æ€§æ³¨æ„æ¨¡å‹çš„æˆæœ¬ä¸åˆ°2000ç¾å…ƒï¼Œä½†æ¨ç†è´¨é‡ä»æ¥è¿‘åŸå§‹å˜æ¢å™¨ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†åŒç±»æœ€ä½³çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œå¹¶å°†æ‰€æœ‰æ¨¡å‹å‘å¸ƒåœ¨HuggingFaceä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03730', 'title': 'FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios', 'url': 'https://huggingface.co/papers/2505.03730', 'abstract': 'Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/', 'score': 19, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '2329fe6d2462c9c9', 'authors': ['Shiyi Zhang', 'Junhao Zhuang', 'Zhaoyang Zhang', 'Ying Shan', 'Yansong Tang'], 'affiliations': ['Tencent ARC Lab, China', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03730.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#multimodal', '#video'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlexiAct - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², FlexiAct Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ, Ñ€Ğ°ĞºÑƒÑ€Ñ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ RefAdapter - Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ FAE (Frequency-aware Action Extraction) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'FlexiAct: Flexible Action Transfer for Diverse Video Customization', 'desc': 'The paper presents FlexiAct, a novel approach for customizing action videos by transferring actions from a reference video to a target image, regardless of differences in layout, viewpoint, and skeletal structure. This method addresses the limitations of existing techniques that require strict spatial consistency, allowing for greater adaptability across various subjects and scenarios. FlexiAct utilizes a lightweight image-conditioned adapter called RefAdapter to ensure identity consistency while adapting spatial structures. Additionally, it introduces Frequency-aware Action Extraction (FAE) to enhance action extraction during the denoising process, achieving superior results in maintaining both appearance and structural flexibility.'}, 'zh': {'title': 'çµæ´»çš„åŠ¨ä½œè½¬ç§»ï¼Œæ‰“ç ´ç©ºé—´é™åˆ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFlexiActçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®è¾“å…¥æ§åˆ¶ä¿¡å·ç”Ÿæˆè§†é¢‘ï¼Œå…è®¸åœ¨ä¸åŒå¸ƒå±€ã€è§†è§’å’Œéª¨æ¶ç»“æ„ä¹‹é—´è¿›è¡ŒåŠ¨ä½œè½¬ç§»ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFlexiActèƒ½å¤Ÿåœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé€‚åº”ç›®æ ‡å›¾åƒçš„ç©ºé—´ç»“æ„å˜åŒ–ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ–‡ç« å¼•å…¥äº†RefAdapterï¼Œä¸€ä¸ªè½»é‡çº§çš„å›¾åƒæ¡ä»¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨å¤–è§‚ä¸€è‡´æ€§å’Œç»“æ„çµæ´»æ€§ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæå‡ºçš„FAEæ–¹æ³•åœ¨å»å™ªè¿‡ç¨‹ä¸­ç›´æ¥æå–åŠ¨ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02922', 'title': 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference', 'url': 'https://huggingface.co/papers/2505.02922', 'abstract': 'The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.', 'score': 18, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'd7e3545dcade10b4', 'authors': ['Yaoqi Chen', 'Jinkai Zhang', 'Baotong Lu', 'Qianxi Zhang', 'Chengruidong Zhang', 'Jingjia Luo', 'Di Liu', 'Huiqiang Jiang', 'Qi Chen', 'Jing Liu', 'Bailu Ding', 'Xiao Yan', 'Jiawei Jiang', 'Chen Chen', 'Mingxing Zhang', 'Yuqing Yang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02922.jpg', 'data': {'categories': ['#long_context', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'RetroInfer: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'RetroInfer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºÑÑˆ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ²Ğ¾Ğ»Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². RetroInfer Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ»Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ±ÑƒÑ„ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ GPU Ğ¸ CPU. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 4.5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ¸ Ğ´Ğ¾ 10.5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Accelerating Long-Context Inference with RetroInfer', 'desc': 'This paper introduces RetroInfer, a system designed to improve the efficiency of large language models (LLMs) during inference by addressing GPU memory and bandwidth limitations. It innovatively redefines the key-value (KV) cache as a vector storage system that leverages attention sparsity to speed up processing of long contexts. The core component, the wave index, utilizes advanced techniques for token retrieval, ensuring both efficiency and accuracy. Additionally, the wave buffer optimizes the coordination of KV cache and computation, achieving significant speed improvements while maintaining high model accuracy.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œçªç ´ä¸Šä¸‹æ–‡é™åˆ¶ï¼', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ¨ç†æ•ˆç‡é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºGPUå†…å­˜å’Œå¸¦å®½çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†RetroInferï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç³»ç»Ÿï¼Œå°†å…³é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é‡æ–°æ¦‚å¿µåŒ–ä¸ºå‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œåˆ©ç”¨å†…åœ¨çš„æ³¨æ„åŠ›ç¨€ç–æ€§æ¥åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ã€‚å…¶æ ¸å¿ƒæ˜¯æ³¢åŠ¨ç´¢å¼•ï¼ˆwave indexï¼‰ï¼Œä¸€ç§æ³¨æ„åŠ›æ„ŸçŸ¥å‘é‡ç´¢å¼•ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸‰æ–¹æ³¨æ„åŠ›è¿‘ä¼¼ã€ç²¾åº¦å—é™çš„æ³¨æ„åŠ›ä¼°è®¡å’Œåˆ†æ®µèšç±»ç­‰æŠ€æœ¯é«˜æ•ˆå‡†ç¡®åœ°æ£€ç´¢å…³é”®æ ‡è®°ã€‚ä¸ä»¥å¾€åœ¨æ ‡è®°é€‰æ‹©å’Œç¡¬ä»¶åè°ƒä¸Šå­˜åœ¨å›°éš¾çš„ç¨€ç–æ€§æ–¹æ³•ä¸åŒï¼ŒRetroInferåœ¨ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æä¾›äº†å¼ºå¤§çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02872', 'title': 'Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading', 'url': 'https://huggingface.co/papers/2505.02872', 'abstract': "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.", 'score': 14, 'issue_id': 3629, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': 'aae5b0f63189eb32', 'authors': ['Cfir Avraham Hadar', 'Omer Shubi', 'Yoav Meiri', 'Yevgeni Berzak'], 'affiliations': ['Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.02872.jpg', 'data': {'categories': ['#multimodal', '#science', '#dataset', '#benchmark', '#interpretability', '#agi'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ñ†ĞµĞ»ĞµĞ¹ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ³Ğ»Ğ°Ğ· Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ¹ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»Ğ°Ğ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»Ğ°Ğ· Ğ¿Ñ€Ğ¸ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ°Ğ· Ğ¸ Ñ‚ĞµĞºÑÑ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑĞ¿ĞµÑ… Ğ² Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ†ĞµĞ»ÑÑ… Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»Ğ°Ğ·.'}, 'en': {'title': 'Decoding Reading Goals from Eye Movements with LLMs', 'desc': "This paper explores how eye movements during reading can reveal a reader's specific goals, such as information seeking. The authors introduce new tasks for classifying and reconstructing these goals using large-scale eye tracking data. They develop and evaluate various multimodal large language models (LLMs) that integrate eye movement data with text to improve goal understanding. The results indicate that these models can effectively decode readers' intentions, demonstrating the potential of LLMs in understanding reading behavior."}, 'zh': {'title': 'ä»çœ¼åŠ¨æ•°æ®è§£ç é˜…è¯»ç›®æ ‡çš„åˆ›æ–°ç ”ç©¶', 'desc': 'æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†å¦‚ä½•ä»é˜…è¯»æ—¶çš„çœ¼åŠ¨æ•°æ®è‡ªåŠ¨è§£ç å¼€æ”¾å¼é˜…è¯»ç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ç›®æ ‡åˆ†ç±»å’Œç›®æ ‡é‡å»ºä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨äº†å¤§è§„æ¨¡çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®ã€‚é€šè¿‡ç»“åˆçœ¼åŠ¨å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼€å‘å¹¶æ¯”è¾ƒäº†å¤šç§åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æå–è¯»è€…çš„æ–‡æœ¬ç‰¹å®šç›®æ ‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æˆåŠŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02214', 'title': 'An Empirical Study of Qwen3 Quantization', 'url': 'https://huggingface.co/papers/2505.02214', 'abstract': "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.", 'score': 12, 'issue_id': 3627, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': 'be32ffc34f30d354', 'authors': ['Xingyu Zheng', 'Yuye Li', 'Haoran Chu', 'Yue Feng', 'Xudong Ma', 'Jie Luo', 'Jinyang Guo', 'Haotong Qin', 'Michele Magno', 'Xianglong Liu'], 'affiliations': ['Beihang University', 'ETH ZÃ¼rich', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02214.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source', '#low_resource'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Qwen3: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3, Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ 5 ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 8 Ğ±Ğ¸Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Qwen3 ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM.'}, 'en': {'title': "Unlocking Efficiency: Evaluating Qwen3's Performance Under Quantization", 'desc': "The Qwen series represents a significant advancement in open-source Large Language Models (LLMs), particularly with the introduction of Qwen3, which excels in natural language understanding tasks. This paper investigates the effects of low-bit quantization on Qwen3's performance, focusing on how different quantization techniques impact its robustness. By evaluating five classic post-training quantization methods across various bit-widths, the study reveals that while Qwen3 performs well at moderate bit-widths, it struggles with linguistic tasks at ultra-low precision. The findings highlight the challenges of compressing LLMs and suggest the need for further research to improve quantization strategies without sacrificing model accuracy."}, 'zh': {'title': 'æ¢ç´¢Qwen3çš„é‡åŒ–æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'Qwenç³»åˆ—æ˜¯ä¸€ä¸ªé¢†å…ˆçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ€è¿‘å‘å¸ƒçš„Qwen3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¸å¼•äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­é«˜æ•ˆéƒ¨ç½²çš„å…³æ³¨ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†Qwen3åœ¨ä¸åŒé‡åŒ–è®¾ç½®ä¸‹çš„é²æ£’æ€§ï¼Œæ¢è®¨äº†å‹ç¼©è¿™ä¸€å…ˆè¿›æ¨¡å‹çš„æœºé‡ä¸æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡åœ¨ä¸­ç­‰ä½å®½ä¸‹Qwen3çš„æ€§èƒ½ä»å…·ç«äº‰åŠ›ï¼Œä½†åœ¨è¶…ä½ç²¾åº¦ä¸‹è¯­è¨€ä»»åŠ¡çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œå¼ºè°ƒäº†LLMå‹ç¼©ä¸­çš„æŒç»­éš¾é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03735', 'title': 'Multi-Agent System for Comprehensive Soccer Understanding', 'url': 'https://huggingface.co/papers/2505.03735', 'abstract': 'Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.', 'score': 9, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'f62fbd87d3f6f548', 'authors': ['Jiayuan Rao', 'Zifeng Li', 'Haoning Wu', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['SAI, Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03735.jpg', 'data': {'categories': ['#open_source', '#survey', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#agents'], 'emoji': 'âš½', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°: Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ SoccerWiki - Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğµ, Ğ¸ SoccerBench - Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ° Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ SoccerAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°.'}, 'en': {'title': 'Revolutionizing Soccer Understanding with AI', 'desc': 'This paper presents a new framework for understanding soccer using AI, addressing the limitations of previous research that focused on narrow tasks. The authors introduce SoccerWiki, a large multimodal knowledge base that contains detailed information about various aspects of soccer, enabling better reasoning. They also create SoccerBench, a comprehensive benchmark with thousands of multimodal question-answer pairs to evaluate soccer understanding tasks. Finally, the paper introduces SoccerAgent, a multi-agent system that collaborates to answer complex soccer questions, demonstrating improved performance through extensive evaluations.'}, 'zh': {'title': 'å…¨é¢æå‡è¶³çƒç†è§£çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¶³çƒç†è§£æ¡†æ¶ï¼Œä»¥å¡«è¡¥ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚æˆ‘ä»¬æ„å»ºäº†SoccerWikiï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€è¶³çƒçŸ¥è¯†åº“ï¼Œæ•´åˆäº†å…³äºçƒå‘˜ã€çƒé˜Ÿã€è£åˆ¤å’Œåœºé¦†çš„ä¸°å¯Œé¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†SoccerBenchï¼Œè¿™æ˜¯æœ€å¤§çš„è¶³çƒç‰¹å®šåŸºå‡†ï¼ŒåŒ…å«çº¦10,000ä¸ªæ ‡å‡†åŒ–çš„å¤šæ¨¡æ€å¤šé€‰é—®ç­”å¯¹ï¼Œæ¶µç›–13ä¸ªä¸åŒçš„ç†è§£ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†SoccerAgentï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡åä½œæ¨ç†åˆ†è§£å¤æ‚çš„è¶³çƒé—®é¢˜ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03739', 'title': 'VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model', 'url': 'https://huggingface.co/papers/2505.03739', 'abstract': 'With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.', 'score': 6, 'issue_id': 3632, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'd1a6985b437b7562', 'authors': ['Zuwei Long', 'Yunhang Shen', 'Chaoyou Fu', 'Heting Gao', 'Lijiang Li', 'Peixian Chen', 'Mengdan Zhang', 'Hang Shao', 'Jian Li', 'Jinlong Peng', 'Haoyu Cao', 'Ke Li', 'Rongrong Ji', 'Xing Sun'], 'affiliations': ['Nanjing University', 'Tencent Youtu Lab', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03739.jpg', 'data': {'categories': ['#training', '#multimodal', '#audio', '#inference', '#open_source'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ…: Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹', 'desc': 'VITA-Audio - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ñ€ĞµÑ‡ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (MCTP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ‡Ğ¸. VITA-Audio Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 3-5 Ñ€Ğ°Ğ· Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ASR, TTS Ğ¸ SQA.'}, 'en': {'title': 'VITA-Audio: Fast, Real-Time Speech Generation for Seamless Interaction', 'desc': 'The paper introduces VITA-Audio, a novel end-to-end speech model designed to enhance real-time human-computer interaction by reducing latency in audio generation. It features a Multiple Cross-modal Token Prediction (MCTP) module that allows for the simultaneous generation of multiple audio tokens, significantly speeding up the inference process. The model employs a four-stage progressive training strategy to maintain high speech quality while achieving faster performance. VITA-Audio stands out as the first multi-modal large language model capable of producing audio output during the initial forward pass, demonstrating a 3-5x speedup compared to existing models while excelling in various speech-related tasks.'}, 'zh': {'title': 'VITA-Audioï¼šå®æ—¶è¯­éŸ³ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'éšç€äººæœºäº¤äº’éœ€æ±‚çš„å¢åŠ ï¼ŒåŸºäºè¯­éŸ³çš„ç³»ç»Ÿå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰çš„è¯­éŸ³æ¨¡å‹åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªéŸ³é¢‘æ ‡è®°æ—¶å­˜åœ¨é«˜å»¶è¿Ÿï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VITA-Audioï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤§å‹è¯­éŸ³æ¨¡å‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”ŸæˆéŸ³é¢‘æ–‡æœ¬æ ‡è®°ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¼•å…¥è½»é‡çº§çš„å¤šæ¨¡æ€äº¤å‰æ ‡è®°é¢„æµ‹æ¨¡å—ï¼Œæ˜¾è‘—åŠ å¿«äº†æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨æµåª’ä½“åœºæ™¯ä¸­å‡å°‘äº†å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03368', 'title': 'Geospatial Mechanistic Interpretability of Large Language Models', 'url': 'https://huggingface.co/papers/2505.03368', 'abstract': 'Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs\' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.', 'score': 5, 'issue_id': 3629, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '1901718a6f967dbe', 'authors': ['Stef De Sabbata', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Leicester, UK', 'University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2505.03368.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#science', '#data', '#architecture', '#interpretability'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ—Ğ°Ğ³Ğ»ÑĞ´Ñ‹Ğ²Ğ°Ñ Ğ² Ğ¼Ğ¾Ğ·Ğ³ Ğ˜Ğ˜: ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ¸Ğ½Ğ³ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸Ñ… Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Unveiling How LLMs Think Geographically', 'desc': 'This paper explores how Large Language Models (LLMs) understand and process geographical information. It introduces a framework for geospatial mechanistic interpretability, which aims to reverse engineer the internal workings of LLMs using spatial analysis. The authors utilize probing techniques and sparse autoencoders to uncover how LLMs represent geographic concepts, revealing patterns in their internal features. By demonstrating spatial autocorrelation in placenames, the study provides insights into the spatial reasoning capabilities of LLMs and their implications for geography.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åœ°ç†ä¿¡æ¯å¤„ç†æœºåˆ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬å’Œä»£ç çš„å¤„ç†ä¸ç”Ÿæˆæ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ç ”ç©¶LLMså¦‚ä½•å¤„ç†åœ°ç†ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å…¶å†…éƒ¨æœºåˆ¶çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬é€šè¿‡ç©ºé—´åˆ†æå’Œæ¢æµ‹æŠ€æœ¯ï¼Œæ­ç¤ºLLMså†…éƒ¨ç»“æ„ï¼Œå¹¶åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨å°†å¤šä¹‰æ€§ç‰¹å¾åˆ†è§£ä¸ºæ›´æ˜“è§£é‡Šçš„å•ä¹‰ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ°åç‰¹å¾ä¸å…¶åœ°ç†ä½ç½®ä¹‹é—´å­˜åœ¨ç©ºé—´ç›¸å…³æ€§ï¼Œä»è€Œä¸ºç†è§£LLMså¤„ç†åœ°ç†ä¿¡æ¯çš„æ–¹å¼æä¾›äº†æ–°çš„è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03164', 'title': 'InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships', 'url': 'https://huggingface.co/papers/2505.03164', 'abstract': "Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.", 'score': 4, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '377a2c082764680a', 'authors': ['Ji Won Chung', 'Tongyu Zhou', 'Ivy Chen', 'Kevin Hsu', 'Ryan A. Rossi', 'Alexa Siu', 'Shunan Guo', 'Franck Dernoncourt', 'James Tompkin', 'Jeff Huang'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03164.jpg', 'data': {'categories': ['#multimodal', '#video'], 'emoji': 'ğŸ­', 'ru': {'title': 'InfoVids: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ InfoVids - Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ°ĞºĞµÑ‚, Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ InfoVids ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞµĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ ÑƒĞ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Data Presentation with InfoVids', 'desc': 'This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualizations in a more cohesive manner. By using infographics-inspired videos, the authors aim to enhance viewer engagement and reduce distractions that typically arise from traditional 2D slides. The study evaluates InfoVids against standard presentation methods across various metrics, revealing that they foster a more interactive and natural experience for viewers. The findings suggest that this innovative approach can transform the dynamics of data presentation, making it more human-centric and effective.'}, 'zh': {'title': 'é‡æ–°å®šä¹‰æ¼”ç¤ºè€…ä¸å¯è§†åŒ–çš„å…³ç³»', 'desc': 'ä¼ ç»Ÿçš„æ•°æ®å±•ç¤ºé€šå¸¸å°†æ¼”ç¤ºè€…å’Œå¯è§†åŒ–åˆ†å¼€ï¼Œåˆ†åˆ«åœ¨3Dä¸–ç•Œå’Œ2Då±å¹•ä¸­è¿›è¡Œï¼Œå¼ºè°ƒä»¥å¯è§†åŒ–ä¸ºä¸­å¿ƒçš„å™è¿°ã€‚ä¸ºäº†åˆ›é€ æ›´ä»¥äººä¸ºæœ¬çš„è§‚çœ‹ä½“éªŒï¼Œæˆ‘ä»¬é€šè¿‡InfoVidså»ºç«‹äº†å¯è§†åŒ–ä¸æ¼”ç¤ºè€…ä¹‹é—´æ›´å¹³ç­‰çš„å…³ç³»ã€‚è¿™äº›å—ä¿¡æ¯å›¾å¯å‘çš„ä¿¡æ¯è§†é¢‘æ—¨åœ¨é‡æ–°å®šä¹‰æ¼”ç¤ºè€…ä¸å¯è§†åŒ–ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒInfoVidså‡å°‘äº†è§‚ä¼—çš„æ³¨æ„åŠ›åˆ†æ•£ï¼Œä½¿ç„¦ç‚¹ä»å¯è§†åŒ–è½¬å‘æ¼”ç¤ºè€…ï¼Œå¹¶ä¸ºè§‚ä¼—æä¾›äº†æ›´äº’åŠ¨ã€è‡ªç„¶å’Œå¼•äººå…¥èƒœçš„æ•°æ®è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21650', 'title': 'HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation', 'url': 'https://huggingface.co/papers/2504.21650', 'abstract': "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.", 'score': 4, 'issue_id': 3632, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': 'a04fa605a99969e5', 'authors': ['Haiyang Zhou', 'Wangbo Yu', 'Jiawen Guan', 'Xinhua Cheng', 'Yonghong Tian', 'Li Yuan'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory', 'School of Electronic and Computer Engineering, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21650.jpg', 'data': {'categories': ['#3d', '#video', '#cv', '#dataset', '#diffusion'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹: Ğ¾Ñ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº 4D-ÑÑ†ĞµĞ½Ğ°Ğ¼', 'desc': 'HoloTime - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 4D-ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ VR Ğ¸ AR Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Panoramic Animator. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 4D-ÑÑ†ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Panoramic Space-Time Reconstruction. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 360World Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Transforming VR and AR with HoloTime: Immersive 4D Experiences from Panoramic Videos', 'desc': 'This paper introduces HoloTime, a novel framework that enhances virtual reality (VR) and augmented reality (AR) experiences by generating immersive 4D assets from video diffusion models. It addresses the limitations of current models that focus on static 3D scenes by proposing a method to create panoramic videos from a single image, which are then transformed into 4D representations. The authors present the 360World dataset, a unique collection of panoramic videos that supports advanced 4D scene reconstruction tasks. Their approach, validated through comparative analysis, shows significant improvements in generating high-quality panoramic videos and reconstructing 4D scenes, ultimately leading to more engaging user experiences in immersive environments.'}, 'zh': {'title': 'HoloTimeï¼šæå‡VRå’ŒARæ²‰æµ¸ä½“éªŒçš„å…¨æ™¯è§†é¢‘ç”Ÿæˆæ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHoloTimeçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¨æ™¯è§†é¢‘ï¼Œä»è€Œæå‡è™šæ‹Ÿç°å®ï¼ˆVRï¼‰å’Œå¢å¼ºç°å®ï¼ˆARï¼‰æŠ€æœ¯çš„æ²‰æµ¸ä½“éªŒã€‚æˆ‘ä»¬å¼•å…¥äº†360Worldæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äº4Dåœºæ™¯é‡å»ºä»»åŠ¡çš„å…¨æ™¯è§†é¢‘é›†åˆã€‚é€šè¿‡Panoramic Animatoræ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†å…¨æ™¯å›¾åƒè½¬æ¢ä¸ºé«˜è´¨é‡çš„å…¨æ™¯è§†é¢‘ã€‚æœ€åï¼Œåˆ©ç”¨ç©ºé—´-æ—¶é—´æ·±åº¦ä¼°è®¡æ–¹æ³•ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆçš„è§†é¢‘è½¬åŒ–ä¸º4Dç‚¹äº‘ï¼Œå®ç°äº†æ›´çœŸå®çš„4Dåœºæ™¯é‡å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21798', 'title': 'SWE-smith: Scaling Data for Software Engineering Agents', 'url': 'https://huggingface.co/papers/2504.21798', 'abstract': 'Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.', 'score': 3, 'issue_id': 3638, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '35185d0e663ab134', 'authors': ['John Yang', 'Kilian Leret', 'Carlos E. Jimenez', 'Alexander Wettig', 'Kabir Khandpur', 'Yanzhe Zhang', 'Binyuan Hui', 'Ofir Press', 'Ludwig Schmidt', 'Diyi Yang'], 'affiliations': ['Alibaba Qwen', 'Indepedent', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21798.jpg', 'data': {'categories': ['#benchmark', '#data', '#synthetic', '#open_source', '#dataset', '#training'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'SWE-smith: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'SWE-smith - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Python Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ñ‚Ğ½Ğ¸ Ğ¸Ğ»Ğ¸ Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SWE-smith Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 50 Ñ‚Ñ‹ÑÑÑ‡ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¸Ğ· 128 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ…. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SWE-agent-LM-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Pass@1 Ğ² 40.2% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Scaling Software Engineering Data Generation with SWE-smith', 'desc': 'This paper presents SWE-smith, a new method for generating large-scale training data for language models in software engineering. Traditional datasets are limited in size and require extensive manual effort to curate, which hinders their effectiveness. SWE-smith automates the creation of execution environments and synthesizes numerous task instances from Python codebases, resulting in a dataset of 50,000 instances from 128 GitHub repositories. The authors demonstrate that their model, SWE-agent-LM-32B, achieves a state-of-the-art performance on the SWE-bench Verified benchmark, and they provide all resources to facilitate further research in this area.'}, 'zh': {'title': 'SWE-smithï¼šå¤§è§„æ¨¡ç”Ÿæˆè½¯ä»¶å·¥ç¨‹è®­ç»ƒæ•°æ®çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'å°½ç®¡è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸå–å¾—äº†è¿›å±•ï¼Œä½†æ”¶é›†è®­ç»ƒæ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œé€šå¸¸æ¥è‡ª11ä¸ªæˆ–æ›´å°‘çš„GitHubä»“åº“ï¼Œæœ€å¤šåªæœ‰å‡ åƒä¸ªè®­ç»ƒå®ä¾‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SWE-smithï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤§è§„æ¨¡ç”Ÿæˆè½¯ä»¶å·¥ç¨‹è®­ç»ƒæ•°æ®çš„æ–°å‹ç®¡é“ã€‚é€šè¿‡SWE-smithï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«5ä¸‡ä¸ªå®ä¾‹çš„æ•°æ®é›†ï¼Œæ˜¾è‘—è¶…è¿‡äº†ä»¥å¾€çš„å·¥ä½œï¼Œå¹¶ä¸”å¼€æºäº†ç›¸å…³èµ„æºï¼Œä»¥é™ä½è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ç ”ç©¶çš„é—¨æ§›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03052', 'title': 'Teaching Models to Understand (but not Generate) High-risk Data', 'url': 'https://huggingface.co/papers/2505.03052', 'abstract': "Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.", 'score': 2, 'issue_id': 3642, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'a988d97eaa0eb984', 'authors': ['Ryan Wang', 'Matthew Finlayson', 'Luca Soldaini', 'Swabha Swayamdipta', 'Robin Jia'], 'affiliations': ['Allen Institute for AI', 'Department of Computer Science, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.03052.jpg', 'data': {'categories': ['#training', '#hallucinations', '#rlhf', '#ethics'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ: Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SLUNG. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ĞµĞ³Ğ¾. SLUNG Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SLUNG ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Learn to Understand, Not to Generate Toxicity', 'desc': "This paper presents a new training method called Selective Loss to Understand but Not Generate (SLUNG) for language models. SLUNG allows models to learn from high-risk content, like toxic text, without generating it in their outputs. By selectively applying loss functions, the model is trained to predict safe tokens that follow harmful ones, enhancing its understanding of sensitive topics. The results show that SLUNG improves the model's ability to recognize toxic content while preventing it from producing harmful responses."}, 'zh': {'title': 'ç†è§£é«˜é£é™©å†…å®¹ï¼Œé¿å…ç”Ÿæˆæœ‰å®³è¾“å‡º', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºé€‰æ‹©æ€§æŸå¤±ä»¥ç†è§£ä½†ä¸ç”Ÿæˆï¼ˆSLUNGï¼‰ã€‚è¯¥æ–¹æ³•å…è®¸æ¨¡å‹åœ¨ä¸ç”Ÿæˆé«˜é£é™©å†…å®¹çš„æƒ…å†µä¸‹ï¼Œç†è§£è¿™äº›å†…å®¹ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°é¿å…æ¿€åŠ±ç”Ÿæˆé«˜é£é™©æ ‡è®°ï¼ŒSLUNGç¡®ä¿è¿™äº›å†…å®¹ä»ç„¶åœ¨æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å†…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLUNGæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹é«˜é£é™©æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶æ²¡æœ‰å¢åŠ ç”Ÿæˆçš„æœ‰å®³å†…å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02311', 'title': 'Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering', 'url': 'https://huggingface.co/papers/2505.02311', 'abstract': 'The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.', 'score': 2, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '2a0b4af232b71fc8', 'authors': ['Jihao Zhao', 'Chunlai Zhou', 'Biao Qin'], 'affiliations': ['School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02311.jpg', 'data': {'categories': ['#hallucinations', '#small_models', '#training', '#optimization', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AttenHScore Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AttenHScore Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Enhancing Hallucination Detection in Language Models with AttenHScore', 'desc': 'This paper introduces a new metric called AttenHScore to improve the detection of hallucinations in small language models (LMs) during their generation process. Hallucinations refer to incorrect or nonsensical outputs produced by LMs, and the proposed metric helps identify when these errors occur in real-time. By adjusting the detection threshold dynamically, the method enhances the invocation of larger LMs to provide more accurate responses. Additionally, the paper discusses how uncertainty-aware knowledge reorganization can help small LMs better understand and utilize critical information from text, leading to improved performance without requiring extra training.'}, 'zh': {'title': 'æå‡å°å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ£€æµ‹èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡AttenHScoreï¼Œç”¨äºåœ¨å°å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­æ£€æµ‹å’Œä¼ æ’­å¹»è§‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®æ—¶è°ƒç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæé«˜å¹»è§‰æ£€æµ‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„çŸ¥è¯†é‡ç»„ï¼Œå¸®åŠ©å°å‹è¯­è¨€æ¨¡å‹æ›´å¥½åœ°æ•æ‰å…³é”®ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAttenHScoreåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šä¼˜äºå¤§å¤šæ•°åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18373', 'title': 'Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant', 'url': 'https://huggingface.co/papers/2504.18373', 'abstract': 'In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.', 'score': 2, 'issue_id': 3629, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 25', 'zh': '4æœˆ25æ—¥'}, 'hash': '296dd197ef9ea331', 'authors': ['Lei Shen', 'Xiaoyu Shen'], 'affiliations': ['GEB Tech', 'Ningbo Institute of Digital Twin, EIT, Ningbo'], 'pdf_title_img': 'assets/pdf/title_img/2504.18373.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Auto-SLURP: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Auto-SLURP - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ². Auto-SLURP Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ SLURP, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ€Ğ²ĞµÑ€Ñ‹ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Auto-SLURP Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ñ‹ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.'}, 'en': {'title': 'Auto-SLURP: Benchmarking Multi-Agent LLMs for Intelligent Assistants', 'desc': 'This paper introduces Auto-SLURP, a new benchmark dataset designed to evaluate multi-agent frameworks that utilize large language models (LLMs) in the realm of intelligent personal assistants. It enhances the original SLURP dataset by relabeling data and incorporating simulated servers and external services, allowing for a more thorough assessment of language understanding, task execution, and response generation. The authors demonstrate that Auto-SLURP poses significant challenges to existing state-of-the-art frameworks, indicating that the development of reliable multi-agent personal assistants is still ongoing. The dataset and its associated code are made publicly available for further research and development.'}, 'zh': {'title': 'Auto-SLURPï¼šè¯„ä¼°æ™ºèƒ½ä¸ªäººåŠ©ç†çš„åŸºå‡†æ•°æ®é›†', 'desc': 'è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶å‘å±•è¿…é€Ÿã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸“é—¨ç”¨äºè¯„ä¼°è¿™äº›æ¡†æ¶æ€§èƒ½çš„åŸºå‡†æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Auto-SLURPï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶çš„åŸºå‡†æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ™ºèƒ½ä¸ªäººåŠ©ç†çš„èƒŒæ™¯ä¸‹ã€‚Auto-SLURPé€šè¿‡é‡æ–°æ ‡è®°æ•°æ®å¹¶æ•´åˆæ¨¡æ‹ŸæœåŠ¡å™¨å’Œå¤–éƒ¨æœåŠ¡ï¼Œæ‰©å±•äº†åŸå§‹çš„SLURPæ•°æ®é›†ï¼Œä»è€Œå®ç°äº†å…¨é¢çš„ç«¯åˆ°ç«¯è¯„ä¼°æµç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00212', 'title': 'Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2505.00212', 'abstract': "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution", 'score': 1, 'issue_id': 3639, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '1b127d9b18e51c40', 'authors': ['Shaokun Zhang', 'Ming Yin', 'Jieyu Zhang', 'Jiale Liu', 'Zhiguang Han', 'Jingyang Zhang', 'Beibin Li', 'Chi Wang', 'Huazheng Wang', 'Yiran Chen', 'Qingyun Wu'], 'affiliations': ['Duke University', 'Google DeepMind', 'Meta', 'Nanyang Technological University', 'Oregon State University', 'Pennsylvania State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.00212.jpg', 'data': {'categories': ['#dataset', '#open_source', '#agents', '#reasoning'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞšÑ‚Ğ¾ Ğ²Ğ¸Ğ½Ğ¾Ğ²Ğ°Ñ‚ Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ°: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ±Ğ¾ĞµĞ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… LLM-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ ÑĞ±Ğ¾ĞµĞ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Who&When, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸ ÑĞ±Ğ¾ĞµĞ² Ğ¸Ğ· 127 Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… LLM-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ±Ğ¾Ğ¸ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ ÑĞ±Ğ¾ĞµĞ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 53.5% Ğ² Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ° ÑĞ±Ğ¾Ğ¸, Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 14.2% Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² ÑĞ±Ğ¾ĞµĞ².'}, 'en': {'title': 'Automating Failure Attribution in Multi-Agent Systems', 'desc': 'This paper addresses the challenge of identifying which agent and which step in a multi-agent system led to task failures, a process known as failure attribution. The authors introduce a new dataset called Who&When, which contains detailed failure logs from 127 LLM multi-agent systems, annotated to link failures to specific agents and error steps. They propose three automated methods for failure attribution and evaluate their performance, revealing that the best method only achieves 53.5% accuracy in identifying responsible agents and 14.2% in pinpointing failure steps. The findings indicate that current state-of-the-art models struggle with this task, emphasizing the complexity of automated failure attribution and the need for further research.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–å¤±è´¥å½’å› ï¼šLLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ–°æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œå¦‚ä½•è‡ªåŠ¨è¯†åˆ«å¯¼è‡´ä»»åŠ¡å¤±è´¥çš„æ™ºèƒ½ä½“å’Œæ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç ”ç©¶é¢†åŸŸï¼šè‡ªåŠ¨åŒ–å¤±è´¥å½’å› ï¼Œå¹¶å¼•å…¥äº†Who&Whenæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«127ä¸ªLLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¯¦ç»†å¤±è´¥æ—¥å¿—ã€‚é€šè¿‡ä½¿ç”¨Who&Whenæ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘å¹¶è¯„ä¼°äº†ä¸‰ç§è‡ªåŠ¨åŒ–å¤±è´¥å½’å› æ–¹æ³•ï¼Œå¹¶æ€»ç»“äº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚å°½ç®¡æœ€ä½³æ–¹æ³•åœ¨è¯†åˆ«å¤±è´¥è´£ä»»æ™ºèƒ½ä½“æ–¹é¢è¾¾åˆ°äº†53.5%çš„å‡†ç¡®ç‡ï¼Œä½†åœ¨ç¡®å®šå¤±è´¥æ­¥éª¤æ–¹é¢ä»…ä¸º14.2%ï¼Œæ˜¾ç¤ºå‡ºè¯¥ä»»åŠ¡çš„å¤æ‚æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09568', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset', 'url': 'https://huggingface.co/papers/2505.09568', 'abstract': 'Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.', 'score': 6, 'issue_id': 3768, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '822f8dd79d39211b', 'authors': ['Jiuhai Chen', 'Zhiyang Xu', 'Xichen Pan', 'Yushi Hu', 'Can Qin', 'Tom Goldstein', 'Lifu Huang', 'Tianyi Zhou', 'Saining Xie', 'Silvio Savarese', 'Le Xue', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['New York University', 'Salesforce Research', 'UC Davis', 'University of Maryland', 'University of Washington', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.09568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#open_source', '#multimodal', '#training', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… CLIP-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VAE-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BLIP3-o, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unifying Image Understanding and Generation with BLIP3-o', 'desc': 'This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks.'}, 'zh': {'title': 'ç»Ÿä¸€å›¾åƒç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¼ºè°ƒäº†è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç”¨æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„CLIPå›¾åƒç‰¹å¾ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å…ˆè¿›è¡Œå›¾åƒç†è§£è®­ç»ƒï¼Œå†è¿›è¡Œå›¾åƒç”Ÿæˆè®­ç»ƒçš„é¡ºåºé¢„è®­ç»ƒç­–ç•¥ï¼Œä¿æŒäº†å›¾åƒç†è§£èƒ½åŠ›çš„åŒæ—¶å¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†é«˜è´¨é‡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†BLIP3o-60kï¼Œä»¥æ”¯æŒå›¾åƒç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21635', 'title': 'Sadeed: Advancing Arabic Diacritization Through Small Language Model', 'url': 'https://huggingface.co/papers/2504.21635', 'abstract': "Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.", 'score': 44, 'issue_id': 3530, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': 'af5a1b038b3ccab3', 'authors': ['Zeina Aldallal', 'Sara Chrouf', 'Khalil Hennara', 'Mohamed Motaism Hamed', 'Muhammad Hreden', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21635.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#multilingual', '#benchmark', '#data', '#machine_translation'], 'emoji': 'ğŸ” ', 'ru': {'title': 'Sadeed: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Sadeed - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Kuwain 1.5B. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Sadeed Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SadeedDiac-25 Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Sadeed: Advancing Arabic Diacritization with Efficiency and Precision', 'desc': 'This paper presents Sadeed, a new method for Arabic text diacritization using a fine-tuned decoder-only language model based on Kuwain 1.5B. Sadeed is specifically trained on high-quality diacritized datasets, which were created through a thorough data-cleaning process. The model demonstrates competitive performance against larger proprietary models while requiring less computational power. Additionally, the authors introduce SadeedDiac-25, a new benchmark for evaluating Arabic diacritization, aiming to improve assessment practices in the field.'}, 'zh': {'title': 'Sadeedï¼šé˜¿æ‹‰ä¼¯è¯­æ ‡è®°åŒ–çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬æ ‡è®°åŒ–æ–¹æ³•ï¼Œåä¸ºSadeedã€‚è¯¥æ–¹æ³•åŸºäºç»è¿‡å¾®è°ƒçš„è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„ä¸°å¯Œå½¢æ€ç‰¹å¾è¿›è¡Œä¼˜åŒ–ã€‚Sadeedåœ¨é«˜è´¨é‡çš„æ ‡è®°åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå°½ç®¡è®¡ç®—èµ„æºæœ‰é™ï¼Œä½†å…¶æ€§èƒ½ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œä¸”ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†SadeedDiac-25åŸºå‡†ï¼Œä»¥ä¿ƒè¿›å¯¹é˜¿æ‹‰ä¼¯è¯­æ ‡è®°åŒ–çš„å…¬å¹³è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21776', 'title': 'WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability', 'url': 'https://huggingface.co/papers/2504.21776', 'abstract': 'Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.', 'score': 26, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '61ce82abe42f584a', 'authors': ['Xiaoxi Li', 'Jiajie Jin', 'Guanting Dong', 'Hongjin Qian', 'Yutao Zhu', 'Yongkang Wu', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Huawei Poisson Lab', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21776.jpg', 'data': {'categories': ['#rl', '#science', '#agents', '#rlhf', '#optimization', '#reasoning', '#benchmark'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'WebThinker: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ²ĞµĞ±-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'WebThinker - ÑÑ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Deep Web Explorer Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ². WebThinker Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WebThinker Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering LRMs with Real-Time Web Research Capabilities', 'desc': "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."}, 'zh': {'title': 'WebThinkerï¼šè®©æ¨ç†æ¨¡å‹æ›´æ™ºèƒ½çš„ç ”ç©¶åŠ©æ‰‹', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1åœ¨é•¿æ—¶é—´æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¾èµ–é™æ€å†…éƒ¨çŸ¥è¯†ï¼Œé™åˆ¶äº†åœ¨å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WebThinkerï¼Œä¸€ä¸ªæ·±åº¦ç ”ç©¶ä»£ç†ï¼Œèƒ½å¤Ÿè®©LRMsè‡ªä¸»æœç´¢ç½‘ç»œã€æµè§ˆç½‘é¡µå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ’°å†™ç ”ç©¶æŠ¥å‘Šã€‚WebThinkeré›†æˆäº†æ·±ç½‘æ¢ç´¢æ¨¡å—ï¼Œä½¿LRMsåœ¨é‡åˆ°çŸ¥è¯†ç©ºç™½æ—¶èƒ½å¤ŸåŠ¨æ€æœç´¢å’Œæå–ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜WebThinkeråœ¨å¤æ‚æ¨ç†åŸºå‡†å’Œç§‘å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21850', 'title': 'COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning', 'url': 'https://huggingface.co/papers/2504.21850', 'abstract': 'Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.', 'score': 20, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '3db97f245360deb4', 'authors': ['Xindi Wu', 'Hee Seung Hwang', 'Polina Kirichenko', 'Olga Russakovsky'], 'affiliations': ['Meta AI', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21850.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#data', '#optimization', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'COMPACT: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MLLM ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ COMPACT. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². COMPACT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ MLLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ LLaVA-665k, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 10% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞµĞ³Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Complex Tasks with Efficient Compositional Training', 'desc': "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."}, 'zh': {'title': 'æå‡å¤æ‚è§†è§‰è¯­è¨€ä»»åŠ¡çš„èƒ½åŠ›', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç®€å•çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦å¤šç§èƒ½åŠ›çš„å¤æ‚ä»»åŠ¡ä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è§†è§‰æŒ‡ä»¤è°ƒä¼˜ï¼ˆVITï¼‰ä¸»è¦å…³æ³¨æ•°æ®é‡çš„æ‰©å¤§ï¼Œè€Œå¿½è§†äº†è®­ç»ƒç¤ºä¾‹çš„ç»„åˆå¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†COMPACTï¼ˆç»„åˆåŸå­åˆ°å¤æ‚è§†è§‰èƒ½åŠ›è°ƒä¼˜ï¼‰ï¼Œå®ƒç”Ÿæˆä¸€ä¸ªæ˜ç¡®æ§åˆ¶è®­ç»ƒç¤ºä¾‹ç»„åˆå¤æ‚æ€§çš„è®­ç»ƒæ•°æ®é›†ã€‚COMPACTä½¿å¾—MLLMsèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å­¦ä¹ å¤æ‚èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤æ‚å¤šèƒ½åŠ›ä»»åŠ¡æ—¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21233', 'title': 'Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math', 'url': 'https://huggingface.co/papers/2504.21233', 'abstract': 'Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.', 'score': 19, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '0b800a9195884db4', 'authors': ['Haoran Xu', 'Baolin Peng', 'Hany Awadalla', 'Dongdong Chen', 'Yen-Chun Chen', 'Mei Gao', 'Young Jin Kim', 'Yunsheng Li', 'Liliang Ren', 'Yelong Shen', 'Shuohang Wang', 'Weijian Xu', 'Jianfeng Gao', 'Weizhu Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21233.jpg', 'data': {'categories': ['#training', '#rl', '#transfer_learning', '#small_models', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (SLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Phi-4-Mini (3.8 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ğ² Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Unlocking Reasoning Power in Small Models', 'desc': 'This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning.'}, 'zh': {'title': 'å°æ¨¡å‹ä¹Ÿèƒ½å¼ºæ¨ç†ï¼', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¥æå‡å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å°å‹æ¨¡å‹ç”±äºå®¹é‡é™åˆ¶ï¼Œæå‡æ¨ç†èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä»LLMç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œè’¸é¦ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„SLMçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼Œæœ€ç»ˆåœ¨Phi-4-Miniæ¨¡å‹ä¸Šå®ç°äº†è¶…è¶Šæ›´å¤§æ¨¡å‹çš„æ¨ç†è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.20708', 'title': 'Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think', 'url': 'https://huggingface.co/papers/2504.20708', 'abstract': "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.", 'score': 17, 'issue_id': 3532, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': 'b26e58cf1cee464f', 'authors': ['Hasan Abed Al Kader Hammoud', 'Hani Itani', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2504.20708.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#interpretability', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ´Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ² LLM: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (Ğ¿Ğ¾Ğ´Ğ¼Ñ‹ÑĞ»ĞµĞ¹) Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ĞµĞµ 13%.'}, 'en': {'title': 'Unlocking Better Answers Through Subthoughts in LLMs', 'desc': 'This paper investigates the reasoning process of Large Language Models (LLMs) by focusing on intermediate reasoning steps, called subthoughts, rather than just the final answer. It questions whether the final answer is the best conclusion and explores if different reasoning paths can lead to varied results. The authors propose a method that segments reasoning into subthoughts and generates multiple potential answers from these segments, aggregating them to find the most frequent answer for improved accuracy. Their experiments demonstrate that this approach can enhance the accuracy of LLMs by up to 13% on challenging mathematical reasoning tasks.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œæå‡æ¨¡å‹å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé—®é¢˜ï¼šæœ€ç»ˆç­”æ¡ˆæ˜¯å¦å¯é åœ°ä»£è¡¨æ¨¡å‹çš„æœ€ä½³ç»“è®ºï¼Ÿä¸åŒçš„æ¨ç†è·¯å¾„æ˜¯å¦ä¼šäº§ç”Ÿä¸åŒçš„ç»“æœï¼Ÿä¸ºäº†è§£ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œç§°ä¸ºå­æ€ç»´ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè¿™äº›å‘ç°çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é€‰æ‹©æœ€é¢‘ç¹çš„ç­”æ¡ˆï¼ˆä¼—æ•°ï¼‰æ¥èšåˆä¸åŒå­æ€ç»´çš„ç­”æ¡ˆï¼Œå‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œæœ€é«˜å¯è¾¾13%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21318', 'title': 'Phi-4-reasoning Technical Report', 'url': 'https://huggingface.co/papers/2504.21318', 'abstract': 'We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.', 'score': 14, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '7004ae060f674e9a', 'authors': ['Marah Abdin', 'Sahaj Agarwal', 'Ahmed Awadallah', 'Vidhisha Balachandran', 'Harkirat Behl', 'Lingjiao Chen', 'Gustavo de Rosa', 'Suriya Gunasekar', 'Mojan Javaheripi', 'Neel Joshi', 'Piero Kauffmann', 'Yash Lara', 'Caio CÃ©sar Teodoro Mendes', 'Arindam Mitra', 'Besmira Nushi', 'Dimitris Papailiopoulos', 'Olli Saarikivi', 'Shital Shah', 'Vaishnavi Shrivastava', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi', 'Guoqing Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.21318.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#math', '#transfer_learning', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Phi-4-reasoning - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Phi-4-reasoning Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Complex Reasoning with Phi-4-Reasoning', 'desc': 'The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models.'}, 'zh': {'title': 'æ¨ç†æ¨¡å‹çš„æ–°çªç ´ï¼šPhi-4-reasoning', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Phi-4-reasoningï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰140äº¿å‚æ•°çš„æ¨ç†æ¨¡å‹ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹ç²¾å¿ƒæŒ‘é€‰çš„â€œå¯æ•™â€æç¤ºè¿›è¡Œç›‘ç£å¾®è°ƒè®­ç»ƒï¼Œç”Ÿæˆè¯¦ç»†çš„æ¨ç†é“¾ï¼Œæœ‰æ•ˆåˆ©ç”¨æ¨ç†æ—¶çš„è®¡ç®—èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†Phi-4-reasoning-plusï¼Œé€šè¿‡åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å¢å¼ºï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é•¿çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦æ¨ç†ã€ç¼–ç ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºæ›´å¤§çš„å¼€æ”¾æƒé‡æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.20966', 'title': 'Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax', 'url': 'https://huggingface.co/papers/2504.20966', 'abstract': 'We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.', 'score': 14, 'issue_id': 3526, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': 'cb610c1427bdf307', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#architecture', '#inference', '#optimization', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Softpick: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ softpick - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¹ softmax. Softpick ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ 'attention sink' Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ softmax. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ softpick ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ÑĞºÑÑ†ĞµÑÑĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ softpick Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ softmax Ğ¿Ñ€Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."}, 'en': {'title': 'Softpick: A Smarter Alternative to Softmax for Transformers', 'desc': 'This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability.'}, 'zh': {'title': 'softpickï¼šæå‡Transformeræ³¨æ„åŠ›çš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºsoftpickçš„æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ›¿ä»£transformeræ³¨æ„åŠ›æœºåˆ¶ä¸­çš„softmaxã€‚softpickä¸éœ€è¦å°†æƒé‡å½’ä¸€åŒ–ä¸º1ï¼Œèƒ½å¤Ÿæ¶ˆé™¤æ³¨æ„åŠ›æ²‰æ²¡å’Œå¤§è§„æ¨¡æ¿€æ´»ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨softpickçš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¸softmaxè¡¨ç°ç›¸å½“ï¼Œä½†æ³¨æ„åŠ›æ²‰æ²¡ç‡ä¸º0%ï¼Œå¹¶ä¸”ç”Ÿæˆçš„éšè—çŠ¶æ€å…·æœ‰æ›´ä½çš„å³°åº¦ã€‚softpickåœ¨é‡åŒ–å’Œä½ç²¾åº¦è®­ç»ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨è¾ƒä½ä½æ•°ç²¾åº¦ä¸‹å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¨€ç–æ€§ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.19720', 'title': 'Taming the Titans: A Survey of Efficient LLM Inference Serving', 'url': 'https://huggingface.co/papers/2504.19720', 'abstract': 'Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.', 'score': 9, 'issue_id': 3526, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 28', 'zh': '4æœˆ28æ—¥'}, 'hash': 'e74f8b7af65e09fd', 'authors': ['Ranran Zhen', 'Juntao Li', 'Yixin Ji', 'Zhenlin Yang', 'Tong Liu', 'Qingrong Xia', 'Xinyu Duan', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang'], 'affiliations': ['Huawei Cloud', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2504.19720.jpg', 'data': {'categories': ['#survey', '#inference', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ° Ğ´Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Optimizing LLMs: Balancing Performance and Efficiency in Generative AI', 'desc': 'This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services.'}, 'zh': {'title': 'æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡çš„ç ”ç©¶è¿›å±•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åºå¤§çš„å‚æ•°é‡å’Œæ³¨æ„åŠ›æœºåˆ¶çš„é«˜è®¡ç®—éœ€æ±‚å¯¼è‡´äº†å†…å­˜å¼€é”€å¤§ï¼Œå½±å“äº†æ¨ç†æœåŠ¡çš„ä½å»¶è¿Ÿå’Œé«˜ååé‡ã€‚æœ¬æ–‡å…¨é¢è°ƒæŸ¥äº†åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å®ä¾‹çº§å’Œé›†ç¾¤çº§çš„ç­–ç•¥ï¼Œä»¥åŠæ–°å…´åœºæ™¯çš„æ–¹å‘ã€‚æˆ‘ä»¬è®¨è®ºäº†æ¨¡å‹éƒ¨ç½²ã€è¯·æ±‚è°ƒåº¦ã€è§£ç é•¿åº¦é¢„æµ‹ç­‰å®ä¾‹çº§æ–¹æ³•ï¼Œä»¥åŠGPUé›†ç¾¤éƒ¨ç½²å’Œå¤šå®ä¾‹è´Ÿè½½å‡è¡¡ç­‰é›†ç¾¤çº§ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ï¼Œä»¥æ¨åŠ¨LLMæ¨ç†æœåŠ¡çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21855', 'title': 'ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction', 'url': 'https://huggingface.co/papers/2504.21855', 'abstract': 'In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '5d8989ce0c77aa23', 'authors': ['Qihao Liu', 'Ju He', 'Qihang Yu', 'Liang-Chieh Chen', 'Alan Yuille'], 'affiliations': ['Independent Researcher', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21855.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games', '#video', '#small_models'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ReVision: Ñ„Ğ¸Ğ·Ğ¸ĞºĞ° Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ReVision - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ 2D Ğ¸ 3D Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ReVision Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'ReVision: Enhancing Video Generation with 3D Physical Knowledge', 'desc': 'The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation.'}, 'zh': {'title': 'é€šè¿‡3Dç‰©ç†çŸ¥è¯†æå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿä¸å¯æ§æ€§', 'desc': 'è¿‘å¹´æ¥ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå¤æ‚åŠ¨ä½œå’Œäº¤äº’æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReVisionï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ’æ‹”çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†å‚æ•°åŒ–çš„ä¸‰ç»´ç‰©ç†çŸ¥è¯†é›†æˆåˆ°é¢„è®­ç»ƒçš„æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„èƒ½åŠ›ã€‚ReVisionåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆç²—ç•¥è§†é¢‘ï¼Œç„¶åæå–2Då’Œ3Dç‰¹å¾æ„å»ºä¸‰ç»´ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼Œæœ€åé€šè¿‡å‚æ•°åŒ–ç‰©ç†å…ˆéªŒæ¨¡å‹ç²¾ç‚¼è¿åŠ¨åºåˆ—ï¼Œåé¦ˆåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ä»¥ç”Ÿæˆä¸€è‡´çš„è¿åŠ¨è§†é¢‘ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReVisionåœ¨å¤æ‚è§†é¢‘ç”Ÿæˆä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³ä»¥è¾ƒå°‘çš„å‚æ•°è¶…è¶Šäº†å¤§å‹æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.19056', 'title': 'Generative AI for Character Animation: A Comprehensive Survey of\n  Techniques, Applications, and Future Directions', 'url': 'https://huggingface.co/papers/2504.19056', 'abstract': 'Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.', 'score': 7, 'issue_id': 3535, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 27', 'zh': '4æœˆ27æ—¥'}, 'hash': 'deba947a8e69f6ee', 'authors': ['Mohammad Mahdi Abootorabi', 'Omid Ghahroodi', 'Pardis Sadat Zahraei', 'Hossein Behzadasl', 'Alireza Mirrokni', 'Mobina Salimipanah', 'Arash Rasouli', 'Bahar Behzadipour', 'Sara Azarnoush', 'Benyamin Maleki', 'Erfan Sadraiye', 'Kiarash Kiani Feriz', 'Mahdi Teymouri Nahad', 'Ali Moghadasi', 'Abolfazl Eshagh Abianeh', 'Nizi Nazar', 'Hamid R. Rabiee', 'Mahdieh Soleymani Baghshah', 'Meisam Ahmadi', 'Ehsaneddin Asgari'], 'affiliations': ['Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Iran University of Science and Technology', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2504.19056.jpg', 'data': {'categories': ['#survey', '#diffusion', '#cv', '#games', '#multimodal', '#video'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ². Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¸Ñ‡ĞºĞ¾Ğ² Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Character Animation with Generative AI', 'desc': 'This paper reviews the recent advancements in generative AI technologies specifically for character animation, which includes facial animation, gesture modeling, and motion synthesis. It highlights how foundation and diffusion models have significantly lowered the costs and time required for creating animated content. The survey provides a comprehensive overview of the state-of-the-art techniques, practical applications, and datasets used in the field, making it a valuable resource for newcomers. Additionally, it discusses ongoing challenges and suggests future research directions to enhance AI-driven character animation.'}, 'zh': {'title': 'ç”Ÿæˆæ€§AIï¼šé‡å¡‘è§’è‰²åŠ¨ç”»çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨è§’è‰²åŠ¨ç”»é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨åŠ¨ç”»ã€è¡¨æƒ…æ¸²æŸ“å’ŒåŠ¨ä½œåˆæˆç­‰æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚é€šè¿‡æ•´åˆä¸åŒçš„ç”Ÿæˆæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œç ”ç©¶è€…ä»¬æ˜¾è‘—é™ä½äº†åŠ¨ç”»å†…å®¹çš„åˆ¶ä½œæ—¶é—´å’Œæˆæœ¬ã€‚è®ºæ–‡è¿˜æä¾›äº†å¯¹å½“å‰ç ”ç©¶ã€å®é™…åº”ç”¨ã€å¸¸ç”¨æ•°æ®é›†å’Œæ–°å…´è¶‹åŠ¿çš„å…¨é¢å›é¡¾ï¼Œå¸®åŠ©æ–°æ‰‹äº†è§£åŸºç¡€æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æœ€åï¼Œä½œè€…è®¨è®ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18904', 'title': 'RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning', 'url': 'https://huggingface.co/papers/2504.18904', 'abstract': 'Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.', 'score': 7, 'issue_id': 3525, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 26', 'zh': '4æœˆ26æ—¥'}, 'hash': 'c724dcb5ceb5df7b', 'authors': ['Haoran Geng', 'Feishi Wang', 'Songlin Wei', 'Yuyang Li', 'Bangjun Wang', 'Boshi An', 'Charlie Tianyue Cheng', 'Haozhe Lou', 'Peihao Li', 'Yen-Jen Wang', 'Yutong Liang', 'Dylan Goetting', 'Chaoyi Xu', 'Haozhe Chen', 'Yuxi Qian', 'Yiran Geng', 'Jiageng Mao', 'Weikang Wan', 'Mingtong Zhang', 'Jiangran Lyu', 'Siheng Zhao', 'Jiazhao Zhang', 'Jialiang Zhang', 'Chengyang Zhao', 'Haoran Lu', 'Yufei Ding', 'Ran Gong', 'Yuran Wang', 'Yuxuan Kuang', 'Ruihai Wu', 'Baoxiong Jia', 'Carlo Sferrazza', 'Hao Dong', 'Siyuan Huang', 'Yue Wang', 'Jitendra Malik', 'Pieter Abbeel'], 'affiliations': ['BIGAI', 'CMU', 'PKU', 'Stanford', 'UC Berkeley', 'UCLA', 'UIUC', 'UMich', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2504.18904.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#synthetic', '#optimization', '#transfer_learning', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboVerse: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'RoboVerse - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼. RoboVerse Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks', 'desc': 'This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field.'}, 'zh': {'title': 'RoboVerseï¼šæ¨åŠ¨æœºå™¨äººå­¦ä¹ çš„å¼ºå¤§æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†RoboVerseï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººé¢†åŸŸæ•°æ®æ”¶é›†å’Œè¯„ä¼°çš„æŒ‘æˆ˜ã€‚RoboVerseåŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿå¹³å°ã€ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå¤šç§æ¨¡æ‹Ÿå™¨å’Œæœºå™¨äººå½¢æ€ã€‚é€šè¿‡é«˜ä¿çœŸç‰©ç†å’Œé€¼çœŸçš„æ¸²æŸ“ï¼Œåˆæˆæ•°æ®é›†æä¾›äº†é«˜è´¨é‡å’Œå¤šæ ·æ€§çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboVerseæ˜¾è‘—æå‡äº†æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œä»æ¨¡æ‹Ÿåˆ°ç°å®çš„è½¬ç§»æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æ•°æ®é›†å’ŒåŸºå‡†çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21039', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report', 'url': 'https://huggingface.co/papers/2504.21039', 'abstract': 'As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.', 'score': 5, 'issue_id': 3528, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 28', 'zh': '4æœˆ28æ—¥'}, 'hash': 'a66fbdd0c4cc7250', 'authors': ['Paul Kassianik', 'Baturay Saglam', 'Alexander Chen', 'Blaine Nelson', 'Anu Vellore', 'Massimo Aufiero', 'Fraser Burch', 'Dhruv Kedia', 'Avi Zohary', 'Sajana Weerawardhena', 'Aman Priyanshu', 'Adam Swanda', 'Amy Chang', 'Hyrum Anderson', 'Kojin Oshiba', 'Omar Santos', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Foundation AI Cisco Systems Inc.', 'Security & Trust Organization Cisco Systems Inc.', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21039.jpg', 'data': {'categories': ['#architecture', '#data', '#open_source', '#training', '#dataset', '#benchmark', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Foundation-Sec-8B - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Llama 3.1 Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Foundation-Sec-8B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Llama 3.1-70B Ğ¸ GPT-4o-mini Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering Cybersecurity with Foundation-Sec-8B', 'desc': 'This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors.'}, 'zh': {'title': 'æ¨åŠ¨ç½‘ç»œå®‰å…¨çš„AIå·¥å…·è¿›æ­¥', 'desc': 'éšç€åŸºäºå˜æ¢å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬åœ¨è½¯ä»¶å·¥ç¨‹ã€åˆ›æ„å†™ä½œå’Œæ•°å­—è‰ºæœ¯ç­‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ä¸“ä¸šçš„è®­ç»ƒæ•°æ®å’Œè¡¨ç¤ºç½‘ç»œå®‰å…¨ç‰¹å®šçŸ¥è¯†çš„å¤æ‚æ€§ï¼Œå®ƒä»¬åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Foundation-Sec-8Bï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºç½‘ç»œå®‰å…¨çš„LLMï¼ŒåŸºäºLlama 3.1æ¶æ„ï¼Œå¹¶é€šè¿‡åœ¨ç²¾å¿ƒç­–åˆ’çš„ç½‘ç»œå®‰å…¨è¯­æ–™åº“ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒæ¥å¢å¼ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªç½‘ç»œå®‰å…¨åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†Foundation-Sec-8Bï¼Œç»“æœæ˜¾ç¤ºå®ƒåœ¨æŸäº›ç½‘ç»œå®‰å…¨ç‰¹å®šä»»åŠ¡ä¸Šä¸Llama 3.1-70Bå’ŒGPT-4o-miniç›¸åŒ¹é…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21336', 'title': 'UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation', 'url': 'https://huggingface.co/papers/2504.21336', 'abstract': 'Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.', 'score': 2, 'issue_id': 3532, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': 'b4a0872fb3eb8547', 'authors': ['Linshan Wu', 'Yuxiang Nie', 'Sunan He', 'Jiaxin Zhuang', 'Hao Chen'], 'affiliations': ['Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China', 'Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China', 'State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.21336.jpg', 'data': {'categories': ['#optimization', '#cv', '#science', '#multimodal', '#dataset', '#healthcare', '#interpretability', '#agi'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'UniBiomed: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'UniBiomed - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. UniBiomed Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ Ğº Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'UniBiomed: Revolutionizing Biomedical Image Interpretation', 'desc': "This paper presents UniBiomed, a groundbreaking universal foundation model designed for interpreting biomedical images by integrating Multi-modal Large Language Models (MLLM) and segmentation techniques. Unlike traditional AI methods that treat text generation and image segmentation separately, UniBiomed combines these processes to provide a cohesive understanding of biomedical data. It utilizes a large-scale dataset of over 27 million image-text pairs across various imaging modalities, enabling it to perform multiple tasks such as segmentation, disease recognition, and report generation. The model's ability to automate grounded interpretation marks a significant advancement in clinical workflows, enhancing diagnostic efficiency and accuracy."}, 'zh': {'title': 'UniBiomedï¼šç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„æ–°çªç ´', 'desc': 'å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦å›¾åƒçš„è§£é‡Šä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æå¼€è¾Ÿäº†æ–°çš„æœºä¼šã€‚ä¼ ç»Ÿçš„äººå·¥æ™ºèƒ½æ–¹æ³•é€šå¸¸ä¾èµ–äºåˆ†ç¦»çš„è®­ç»ƒï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­ç¼ºä¹çµæ´»æ€§ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•´ä½“ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UniBiomedï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒè§£é‡Šçš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿç»Ÿä¸€ç”Ÿæˆä¸´åºŠæ–‡æœ¬å’Œç›¸åº”ç”Ÿç‰©åŒ»å­¦å¯¹è±¡çš„åˆ†å‰²ã€‚UniBiomedåœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†è¯Šæ–­æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.19043', 'title': 'Selecting Optimal Candidate Profiles in Adversarial Environments Using\n  Conjoint Analysis and Machine Learning', 'url': 'https://huggingface.co/papers/2504.19043', 'abstract': 'Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such experiments in the political analysis context, respondents are asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings indicate that incorporating adversarial dynamics into conjoint analysis may yield unique insight into social science data from experiments.', 'score': 2, 'issue_id': 3538, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 26', 'zh': '4æœˆ26æ—¥'}, 'hash': 'c7a7b2771c1e5c1f', 'authors': ['Connor T. Jerzak', 'Priyanshi Chandra', 'Rishi Hazra'], 'affiliations': ['Department of Government, University of Texas at Austin', 'Department of Statistics, Harvard College', 'Faculty of Informatics, UniversitÃ  della Svizzera Italiana'], 'pdf_title_img': 'assets/pdf/title_img/2504.19043.jpg', 'data': {'categories': [], 'emoji': 'ğŸ—³ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğ¼ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ½ĞµÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Optimizing Political Candidate Selection through Adversarial Conjoint Analysis', 'desc': 'This paper explores the use of conjoint analysis in political candidate selection, focusing on how to identify optimal candidate profiles. It highlights the challenge of having too many possible candidate features compared to the limited number of observations in typical experiments. To solve this, the authors propose a stochastic intervention that generates a probability distribution of candidate attributes to maximize favorable outcomes. The study shows that considering adversarial dynamics between political parties leads to more accurate predictions of candidate success compared to traditional non-adversarial methods.'}, 'zh': {'title': 'å¯¹æŠ—æ€§åŠ¨æ€æå‡è”åˆåˆ†æçš„æ´å¯ŸåŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è”åˆåˆ†æåœ¨æ”¿æ²»å€™é€‰äººé€‰æ‹©ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•è¯†åˆ«æœ€ä½³å€™é€‰äººç‰¹å¾ç»„åˆã€‚ç”±äºç‰¹å¾ç»„åˆçš„æ•°é‡è¿œè¶…è§‚å¯Ÿæ ·æœ¬ï¼Œæ— æ³•ç²¾ç¡®ç¡®å®šæœ€ä½³é…ç½®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æœ€ä¼˜éšæœºå¹²é¢„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ¦‚ç‡åˆ†å¸ƒå®ç°æœ€æœ‰åˆ©çš„å¹³å‡ç»“æœã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¯¹æŠ—æ€§ç¯å¢ƒä¸­ï¼Œæ‰€å»ºè®®çš„ç­–ç•¥æ›´å¯èƒ½ä¸å®é™…å€™é€‰äººåŒ¹é…ï¼Œæ­ç¤ºäº†å¯¹æŠ—åŠ¨æ€åœ¨ç¤¾ä¼šç§‘å­¦å®éªŒæ•°æ®åˆ†æä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02567', 'title': 'Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities', 'url': 'https://huggingface.co/papers/2505.02567', 'abstract': "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).", 'score': 51, 'issue_id': 3655, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '0d49b4c41b7654a0', 'authors': ['Xinjie Zhang', 'Jintao Guo', 'Shanshan Zhao', 'Minghao Fu', 'Lunhao Duan', 'Guo-Hua Wang', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba Group', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02567.jpg', 'data': {'categories': ['#dataset', '#architecture', '#survey', '#multimodal', '#diffusion', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Bridging the Gap: Unifying Multimodal Understanding and Image Generation', 'desc': 'This paper surveys the integration of multimodal understanding and image generation models, which have traditionally developed separately. It highlights the architectural differences between autoregressive and diffusion-based models, emphasizing the challenges in unifying these approaches. The authors categorize existing unified models into three paradigms: diffusion-based, autoregressive-based, and hybrid methods. They also provide resources such as datasets and benchmarks to support future research in this emerging field.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„æœªæ¥æ¢ç´¢', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€ç†è§£æ¨¡å‹å’Œå›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™ä¸¤ä¸ªé¢†åŸŸçš„å‘å±•ç›¸å¯¹ç‹¬ç«‹ï¼Œå¯¼è‡´äº†ä¸åŒçš„æ¶æ„èŒƒå¼ã€‚è‡ªå›å½’æ¶æ„åœ¨å¤šæ¨¡æ€ç†è§£ä¸­å ä¸»å¯¼åœ°ä½ï¼Œè€Œæ‰©æ•£æ¨¡å‹åˆ™æˆä¸ºå›¾åƒç”Ÿæˆçš„åŸºçŸ³ã€‚æœ¬æ–‡ç»¼è¿°äº†å½“å‰ç»Ÿä¸€æ¡†æ¶çš„åŠªåŠ›ï¼Œä»‹ç»äº†å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŸºç¡€æ¦‚å¿µåŠæœ€æ–°è¿›å±•ï¼Œå¹¶åˆ†æäº†ä¸‰ç§ä¸»è¦çš„ç»Ÿä¸€æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†è¿™ä¸€æ–°å…´é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å‚è€ƒèµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04588', 'title': 'ZeroSearch: Incentivize the Search Capability of LLMs without Searching', 'url': 'https://huggingface.co/papers/2505.04588', 'abstract': "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", 'score': 29, 'issue_id': 3647, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '24edc7c3c5e5e23d', 'authors': ['Hao Sun', 'Zile Qiao', 'Jiayan Guo', 'Xuanbo Fan', 'Yingyan Hou', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Yan Zhang'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.04588.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#reasoning', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'ZeroSearch: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ZeroSearch - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ZeroSearch Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° API. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ZeroSearch ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'ZeroSearch: Enhancing LLM Search Without Real Engines', 'desc': 'This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines.'}, 'zh': {'title': 'æå‡LLMsæœç´¢èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ‰æ•ˆçš„ä¿¡æ¯æœç´¢å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZeroSearchçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMsçš„æœç´¢èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¸çœŸå®æœç´¢å¼•æ“äº’åŠ¨ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒï¼Œå°†LLMè½¬å˜ä¸ºä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥é™ä½ç”Ÿæˆæ–‡æ¡£çš„è´¨é‡ï¼Œä»¥æ¿€å‘æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03821', 'title': 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models', 'url': 'https://huggingface.co/papers/2505.03821', 'abstract': "We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.", 'score': 18, 'issue_id': 3655, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ', 'en': 'May 3', 'zh': '5æœˆ3æ—¥'}, 'hash': 'abede452b390c7de', 'authors': ['Gracjan GÃ³ral', 'Alicja Ziarko', 'Piotr MiÅ‚oÅ›', 'MichaÅ‚ Nauman', 'Maciej WoÅ‚czyk', 'MichaÅ‚ KosiÅ„ski'], 'affiliations': ['Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, S. Banacha 2, 02-097 Warsaw, PL', 'Graduate School of Business, Stanford University, Stanford, CA 94305, USA', 'IDEAS NCBR, Chmielna 69, 00-801 Warsaw, PL', 'Institute of Mathematics, Polish Academy of Sciences, J. & J. Sniadeckich 8, 00-656 Warsaw, PL', 'Robot Learning Lab, University of California, Berkeley, CA 94720, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.03821.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#training'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'VLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 144 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ°Ñ‚ÑÑ€Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ³ÑƒÑ€ĞºĞ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4-Turbo Ğ¸ Claude Sonnet, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ†ĞµĞ½, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ñ… VLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Spatial Reasoning and Perspective Taking', 'desc': "This paper explores how well Vision Language Models (VLMs) can understand visual perspectives through a series of unique tasks. The tasks involve a humanoid figure and an object in various spatial arrangements, designed to test scene understanding, spatial reasoning, and visual perspective taking. The study evaluates several advanced models, finding that while they perform well in recognizing scenes, they struggle with more complex reasoning tasks. The results highlight a significant gap in the models' abilities, suggesting that future developments should focus on incorporating geometric representations and specialized training methods."}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰è§†è§’ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿ç”¨äº†ä¸€ç»„æ–°é¢–çš„è§†è§‰ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡çµæ„Ÿæ¥æºäºäººç±»çš„ç»å…¸æµ‹è¯•ã€‚æˆ‘ä»¬è®¾è®¡äº†144ä¸ªç‹¬ç‰¹çš„è§†è§‰ä»»åŠ¡ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜ç©ºé—´é…ç½®ï¼Œå¦‚ç‰©ä½“ç›¸å¯¹äºäººå½¢å°äººå¶çš„ä½ç½®å’Œæ–¹å‘ï¼Œæ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚æ¯ä¸ªè§†è§‰ä»»åŠ¡é…æœ‰7ä¸ªè¯Šæ–­é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°åœºæ™¯ç†è§£ã€ç©ºé—´æ¨ç†å’Œè§†è§‰è§†è§’ç†è§£ä¸‰ä¸ªå±‚æ¬¡çš„è§†è§‰è®¤çŸ¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è¿™äº›å…ˆè¿›æ¨¡å‹åœ¨åœºæ™¯ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œè§†è§’ç†è§£æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­ï¼Œè¡¨é¢ç‰©ä½“è¯†åˆ«ä¸æ›´æ·±å±‚æ¬¡çš„ç©ºé—´å’Œè§†è§’æ¨ç†ä¹‹é—´å­˜åœ¨å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00358', 'title': 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training', 'url': 'https://huggingface.co/papers/2505.00358', 'abstract': "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.", 'score': 14, 'issue_id': 3652, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '74b251baea8510bd', 'authors': ['Albert Ge', 'Tzu-Heng Huang', 'John Cooper', 'Avi Trost', 'Ziyi Chu', 'Satya Sai Srinath Namburi GNVV', 'Ziyang Cai', 'Kendall Park', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.00358.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#data', '#multimodal'], 'emoji': 'ğŸ”€', 'ru': {'title': 'R&B: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº R&B Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. R&B Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ“Ñ€Ğ°Ğ¼Ğ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ R&B Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ½ĞµĞ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'R&B: Smarter Data Mixing for Language Models', 'desc': 'This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods.'}, 'zh': {'title': 'R&Bï¼šé«˜æ•ˆçš„æ•°æ®æ··åˆæ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ··åˆç­–ç•¥R&Bï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦ç¼ºé™·ã€‚é¦–å…ˆï¼ŒR&Bé€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§é‡æ–°åˆ’åˆ†è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºæ›´ç»†ç²’åº¦çš„æ•°æ®åŸŸï¼Œä»è€Œæ•æ‰åˆ°é‡è¦çš„è¯­ä¹‰ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„é¢†åŸŸæ¢¯åº¦çš„GramçŸ©é˜µï¼Œä¼˜åŒ–æ•°æ®ç»„åˆï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR&Båœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä»¥æå°çš„è®¡ç®—æˆæœ¬è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿›æ•°æ®æ··åˆç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04622', 'title': 'PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer', 'url': 'https://huggingface.co/papers/2505.04622', 'abstract': 'Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '8205883cc18835a6', 'authors': ['Jingwen Ye', 'Yuze He', 'Yanning Zhou', 'Yiqin Zhu', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Wei Yang', 'Xiao Han'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04622.jpg', 'data': {'categories': ['#optimization', '#3d', '#cv', '#games'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PrimitiveAnything - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹, Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ±Ğ¾Ñ€Ğ¾Ğº Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². PrimitiveAnything Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… 3D-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing 3D Shape Understanding with PrimitiveAnything', 'desc': 'This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games.'}, 'zh': {'title': 'å½¢çŠ¶æŠ½è±¡çš„æ–°çªç ´ï¼šPrimitiveAnything', 'desc': 'å½¢çŠ¶åŸå§‹æŠ½è±¡æ˜¯å°†å¤æ‚çš„3Då½¢çŠ¶åˆ†è§£ä¸ºç®€å•å‡ ä½•å…ƒç´ çš„è¿‡ç¨‹ï¼Œè¿™å¯¹äººç±»è§†è§‰è®¤çŸ¥è‡³å…³é‡è¦ï¼Œå¹¶åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰çš„åŸå§‹æŠ½è±¡æ–¹æ³•é€šå¸¸ä¾èµ–äºå‡ ä½•ä¼˜åŒ–ï¼Œç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œæˆ–è€…ä»…ä»å°è§„æ¨¡ã€ç‰¹å®šç±»åˆ«çš„æ•°æ®é›†ä¸­å­¦ä¹ ï¼Œéš¾ä»¥åœ¨å¤šæ ·çš„å½¢çŠ¶ç±»åˆ«ä¸­è¿›è¡Œæ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†PrimitiveAnythingï¼Œä¸€ä¸ªå°†å½¢çŠ¶åŸå§‹æŠ½è±¡é‡æ–°å®šä¹‰ä¸ºåŸå§‹ç»„è£…ç”Ÿæˆä»»åŠ¡çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤§è§„æ¨¡äººç±»åˆ›ä½œçš„æŠ½è±¡å­¦ä¹ åŸå§‹ç»„è£…è¿‡ç¨‹ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»å¦‚ä½•å°†å¤æ‚å½¢çŠ¶åˆ†è§£ä¸ºåŸå§‹å…ƒç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04512', 'title': 'HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation', 'url': 'https://huggingface.co/papers/2505.04512', 'abstract': 'Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '82e5839ef846d9d8', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Sen Liang', 'Yuan Zhou', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.04512.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'HunyuanCustom - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLaVA Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ğ´Ñ€Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾- Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº AudioNet Ğ¸ ÑĞµÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HunyuanCustom Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'HunyuanCustom: Consistent and Multi-Modal Video Generation', 'desc': 'This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation.'}, 'zh': {'title': 'å¤šæ¨¡æ€å®šåˆ¶è§†é¢‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'å®šåˆ¶è§†é¢‘ç”Ÿæˆæ—¨åœ¨æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ¡ä»¶ç”Ÿæˆç‰¹å®šä¸»é¢˜çš„è§†é¢‘ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨èº«ä»½ä¸€è‡´æ€§å’Œè¾“å…¥æ¨¡æ€æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†HunyuanCustomï¼Œä¸€ä¸ªå¤šæ¨¡æ€å®šåˆ¶è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå¼ºè°ƒä¸»é¢˜ä¸€è‡´æ€§ï¼Œå¹¶æ”¯æŒå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¼•å…¥åŸºäºLLaVAçš„æ–‡æœ¬-å›¾åƒèåˆæ¨¡å—å’Œå›¾åƒIDå¢å¼ºæ¨¡å—ï¼Œè§£å†³äº†å›¾åƒ-æ–‡æœ¬æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œä»è€Œå¢å¼ºå¤šæ¨¡æ€ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHunyuanCustomåœ¨èº«ä»½ä¸€è‡´æ€§ã€çœŸå®æ„Ÿå’Œæ–‡æœ¬-è§†é¢‘å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€æ¡ä»¶å’Œèº«ä»½ä¿æŒç­–ç•¥åœ¨å¯æ§è§†é¢‘ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04364', 'title': "Benchmarking LLMs' Swarm intelligence", 'url': 'https://huggingface.co/papers/2505.04364', 'abstract': 'Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.', 'score': 12, 'issue_id': 3648, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '4b0575d2194aee20', 'authors': ['Kai Ruan', 'Mowen Huang', 'Ji-Rong Wen', 'Hao Sun'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04364.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#open_source', '#multimodal'], 'emoji': 'ğŸ', 'ru': {'title': 'SwarmBench: Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SwarmBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ¾ĞµĞ²Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. SwarmBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² 2D-ÑĞµÑ‚ĞºĞµ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'Unlocking Swarm Intelligence in Language Models', 'desc': "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."}, 'zh': {'title': 'æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¾¤ä½“æ™ºèƒ½æ½œåŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­åœ¨ä¸¥æ ¼çº¦æŸä¸‹çš„åè°ƒèƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨ç¾¤ä½“æ™ºèƒ½çš„ç»†å¾®å·®åˆ«æ–¹é¢ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€æ— æ³•å®Œå…¨æ•æ‰åˆ°åœ¨ä¸å®Œæ•´æ—¶ç©ºä¿¡æ¯ä¸‹ï¼Œæ™ºèƒ½ä½“è¿›è¡Œå»ä¸­å¿ƒåŒ–åè°ƒæ‰€é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SwarmBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMsä½œä¸ºå»ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“çš„ç¾¤ä½“æ™ºèƒ½èƒ½åŠ›ã€‚é€šè¿‡è¯„ä¼°å¤šä¸ªé¢†å…ˆçš„LLMsï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œçªæ˜¾äº†åœ¨å±€éƒ¨ä¿¡æ¯é™åˆ¶ä¸‹çš„åè°ƒå›°éš¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04528', 'title': 'Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving', 'url': 'https://huggingface.co/papers/2505.04528', 'abstract': 'As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.', 'score': 7, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '0e9e0d509e4b4624', 'authors': ['Qi Liu', 'Xinhao Zheng', 'Renqiu Xia', 'Xingzhi Qi', 'Qinxiang Cao', 'Junchi Yan'], 'affiliations': ['Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04528.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#alignment', '#interpretability', '#reasoning', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº FPS (Formal Problem-Solving), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ D-FPS (Deductive FPS), Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ RPE Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Problem-Solving with Formal Frameworks', 'desc': 'This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods.'}, 'zh': {'title': 'å½¢å¼åŒ–é—®é¢˜è§£å†³çš„æ–°æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é—®é¢˜è§£å†³çš„å½¢å¼åŒ–ï¼Œæå‡ºäº†ä¸€ç§å°†é—®é¢˜è§£å†³è§†ä¸ºç¡®å®šæ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„æ¡†æ¶ã€‚ä½œè€…ä»‹ç»äº†FPSï¼ˆæ­£å¼é—®é¢˜è§£å†³ï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ç°æœ‰çš„æ­£å¼å®šç†è¯æ˜ç¯å¢ƒè¿›è¡Œè¿‡ç¨‹éªŒè¯çš„é—®é¢˜è§£å†³ã€‚ä¸ºäº†æé«˜äººç±»å¯¹é½ï¼Œè®ºæ–‡è¿˜æå‡ºäº†D-FPSï¼ˆæ¼”ç»FPSï¼‰ï¼Œå°†æ±‚è§£ä¸ç­”æ¡ˆéªŒè¯è§£è€¦ã€‚æœ€åï¼Œä½œè€…æ„å»ºäº†ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¬¦å·æ–¹æ³•RPEæ¥è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03912', 'title': 'OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2505.03912', 'abstract': 'Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.', 'score': 6, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'f7347c1b093f9488', 'authors': ['Can Cui', 'Pengxiang Ding', 'Wenxuan Song', 'Shuanghao Bai', 'Xinyang Tong', 'Zirui Ge', 'Runze Suo', 'Wanqi Zhou', 'Yang Liu', 'Bofang Jia', 'Han Zhao', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['HKUST(GZ)', 'Westlake University', 'Xian Jiaotong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03912.jpg', 'data': {'categories': ['#architecture', '#optimization', '#agents', '#open_source', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… VLA Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ VLA (Vision-Language-Action) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¦ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering Embodied Intelligence with Open-Source VLA Models', 'desc': 'This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers.'}, 'zh': {'title': 'æ¨åŠ¨åŒç³»ç»ŸVLAæ¶æ„çš„å¼€æºæ¢ç´¢', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŒç³»ç»Ÿè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¶æ„åœ¨å…·èº«æ™ºèƒ½ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºç›®å‰ç¼ºä¹è¶³å¤Ÿçš„å¼€æºå·¥ä½œæ¥è¿›è¡Œæ€§èƒ½åˆ†æå’Œä¼˜åŒ–ã€‚ä½œè€…æ€»ç»“å¹¶æ¯”è¾ƒäº†ç°æœ‰åŒç³»ç»Ÿæ¶æ„çš„ç»“æ„è®¾è®¡ï¼Œå¹¶å¯¹å…¶æ ¸å¿ƒè®¾è®¡å…ƒç´ è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯è¯„ä¼°ã€‚æœ€ç»ˆï¼Œæœ¬æ–‡å°†æä¾›ä¸€ä¸ªä½æˆæœ¬çš„å¼€æºæ¨¡å‹ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶ã€‚è¯¥é¡¹ç›®å°†æŒç»­æ›´æ–°ï¼Œæä¾›æ›´å¤šå®éªŒç»“è®ºå’Œæ€§èƒ½æ”¹è¿›çš„å¼€æºæ¨¡å‹ä¾›å¤§å®¶é€‰æ‹©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04606', 'title': 'OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution', 'url': 'https://huggingface.co/papers/2505.04606', 'abstract': "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", 'score': 5, 'issue_id': 3657, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '25e97f182730fc25', 'authors': ['Lianghong Guo', 'Wei Tao', 'Runhan Jiang', 'Yanlin Wang', 'Jiachi Chen', 'Xilin Liu', 'Yuchi Ma', 'Mingzhi Mao', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University, China', 'Huawei Cloud Computing Technologies Co., Ltd., China', 'Independent Researcher, China', 'Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04606.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset', '#long_context', '#multimodal'], 'emoji': 'ğŸ™', 'ru': {'title': 'OmniGIRL: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ GitHub', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniGIRL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° GitHub. OmniGIRL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 959 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° OmniGIRL, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ½ĞµÑƒĞ´Ğ°Ñ‡ LLM Ğ½Ğ° OmniGIRL Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'OmniGIRL: A Comprehensive Benchmark for GitHub Issue Resolution', 'desc': 'This paper introduces OmniGIRL, a new benchmark for automatically resolving GitHub issues using large language models (LLMs). Unlike existing benchmarks, OmniGIRL is designed to be multilingual, multimodal, and multi-domain, addressing the limitations of focusing on a single programming language and a narrow range of issues. The benchmark includes 959 instances from four programming languages and eight domains, highlighting the diversity of real-world problems. Evaluation results show that current LLMs perform poorly on this benchmark, particularly in resolving issues that require understanding images, indicating a need for further advancements in model capabilities.'}, 'zh': {'title': 'OmniGIRLï¼šå¤šè¯­è¨€å¤šæ¨¡æ€çš„GitHubé—®é¢˜è§£å†³åŸºå‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†OmniGIRLï¼Œä¸€ä¸ªå¤šè¯­è¨€ã€å¤šæ¨¡æ€å’Œå¤šé¢†åŸŸçš„GitHubé—®é¢˜è§£å†³åŸºå‡†ã€‚ç°æœ‰çš„åŸºå‡†å­˜åœ¨ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼šåªå…³æ³¨å•ä¸€ç¼–ç¨‹è¯­è¨€ã€è¦†ç›–é¢†åŸŸç‹­çª„ä»¥åŠä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯ã€‚OmniGIRLåŒ…å«æ¥è‡ªå››ç§ç¼–ç¨‹è¯­è¨€å’Œå…«ä¸ªä¸åŒé¢†åŸŸçš„959ä¸ªä»»åŠ¡å®ä¾‹ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰çš„è¯­è¨€æ¨¡å‹åœ¨OmniGIRLä¸Šçš„è¡¨ç°æœ‰é™ï¼Œå°¤å…¶åœ¨å¤„ç†éœ€è¦ç†è§£å›¾åƒçš„é—®é¢˜æ—¶è¡¨ç°æ›´å·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03570', 'title': 'OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents', 'url': 'https://huggingface.co/papers/2505.03570', 'abstract': 'In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.', 'score': 4, 'issue_id': 3654, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'e87199c8805bce4f', 'authors': ['Mariya Davydova', 'Daniel Jeffries', 'Patrick Barker', 'Arturo MÃ¡rquez Flores', 'SinÃ©ad Ryan'], 'affiliations': ['Kentauros AI Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.03570.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#optimization', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'OSUniverse: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OSUniverse - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 50% ÑƒÑĞ¿ĞµÑ…Ğ°, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ„Ğ¸ÑĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ½ĞµĞµ 2%.'}, 'en': {'title': 'OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks', 'desc': 'This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.'}, 'zh': {'title': 'OSUniverseï¼šGUIå¯¼èˆªAIçš„å…¨æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†OSUniverseï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜çº§GUIå¯¼èˆªAIä»£ç†çš„å¤æ‚å¤šæ¨¡æ€æ¡Œé¢ä»»åŠ¡åŸºå‡†ï¼Œæ—¨åœ¨æ˜“ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å…¨é¢è¦†ç›–æµ‹è¯•æ¡ˆä¾‹å’Œè‡ªåŠ¨éªŒè¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬å°†ä»»åŠ¡åˆ†ä¸ºä¸åŒå¤æ‚åº¦çš„çº§åˆ«ï¼Œä»åŸºæœ¬çš„ç²¾ç¡®ç‚¹å‡»åˆ°éœ€è¦çµæ´»æ€§ã€ç²¾ç¡®æ€§å’Œæ¸…æ™°æ€ç»´çš„å¤šæ­¥éª¤ã€å¤šåº”ç”¨ç¨‹åºæµ‹è¯•ã€‚åœ¨åŸºå‡†çš„ç¬¬ä¸€ç‰ˆä¸­ï¼Œæˆ‘ä»¬è°ƒæ•´äº†æµ‹è¯•æ¡ˆä¾‹çš„å¤æ‚æ€§ï¼Œä»¥ç¡®ä¿å½“æ—¶çš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ä»£ç†çš„ç»“æœä¸è¶…è¿‡50%ï¼Œè€Œæ™®é€šç™½é¢†å·¥äººå¯ä»¥å®Œç¾å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡ã€‚è¯¥åŸºå‡†å¯ä»¥æ‰‹åŠ¨è¯„åˆ†ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¹³å‡é”™è¯¯ç‡ä½äº2%çš„è‡ªåŠ¨éªŒè¯æœºåˆ¶ï¼Œä¸ºå…¨é¢è‡ªåŠ¨åŒ–æµ‹é‡GUIå¯¼èˆªAIä»£ç†çš„è¿›å±•ã€èƒ½åŠ›å’Œæœ‰æ•ˆæ€§æä¾›äº†åšå®åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03418', 'title': 'Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey', 'url': 'https://huggingface.co/papers/2505.03418', 'abstract': 'Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.', 'score': 4, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '8417799a01a2ecc2', 'authors': ['Da Zheng', 'Lun Du', 'Junwei Su', 'Yuchen Tian', 'Yuqi Zhu', 'Jintian Zhang', 'Lanning Wei', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['Ant Group, China', 'The University of Hong Kong, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03418.jpg', 'data': {'categories': ['#rl', '#survey', '#math', '#training', '#reasoning', '#science', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, ĞºĞ°Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought), Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Complex Problem-Solving with Large Language Models', 'desc': 'This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼šå¤æ‚é—®é¢˜è§£å†³çš„æ–°å·¥å…·', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚ä¸ä¼ ç»Ÿè®¡ç®—ç³»ç»Ÿä¸åŒï¼ŒLLMsç»“åˆäº†å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å’Œäººç±»æ¨ç†çš„è¿‘ä¼¼ï¼Œèƒ½å¤Ÿç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œè¿›è¡Œæ¨ç†ã€‚å°½ç®¡LLMsåœ¨å¤šæ­¥éª¤æ¨ç†ã€é¢†åŸŸçŸ¥è¯†æ•´åˆå’Œç»“æœéªŒè¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œä½†å®ƒä»¬åœ¨è½¯ä»¶å·¥ç¨‹ã€æ•°å­¦æ¨ç†ã€æ•°æ®åˆ†æå’Œç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†å½“å‰LLMè§£å†³æ–¹æ¡ˆçš„åŸºæœ¬å±€é™æ€§ä»¥åŠæœªæ¥åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„å‘å±•æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04601', 'title': 'OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning', 'url': 'https://huggingface.co/papers/2505.04601', 'abstract': "OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.", 'score': 2, 'issue_id': 3666, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '0b9c03d9680cfe04', 'authors': ['Xianhang Li', 'Yanqing Liu', 'Haoqin Tu', 'Hongru Zhu', 'Cihang Xie'], 'affiliations': ['University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.04601.jpg', 'data': {'categories': ['#small_models', '#dataset', '#open_source', '#multimodal', '#architecture'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'OpenVision: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'OpenVision Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ CLIP Ğ¾Ñ‚ OpenAI Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ğ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ OpenVision Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIPS Ğ¸ Recap-DataComp-1B, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² (Ğ¾Ñ‚ 5,9 Ğ´Ğ¾ 632,1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. OpenVision Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'OpenVision: Open and Efficient Vision Encoders for Multimodal Models', 'desc': "This paper introduces OpenVision, a new family of vision encoders that are fully open and cost-effective, designed to compete with OpenAI's CLIP. OpenVision not only matches but can also surpass CLIP's performance when used in multimodal frameworks like LLaVA. The authors leverage existing methodologies and datasets to improve encoder quality and provide insights into enhancing multimodal models. By offering a range of models with varying parameters, OpenVision allows users to choose between high performance and efficiency for their specific applications."}, 'zh': {'title': 'å¼€æ”¾è§†è§‰ç¼–ç å™¨ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½', 'desc': 'OpenVisionæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„è§†è§‰ç¼–ç å™¨å®¶æ—ï¼Œæ—¨åœ¨ä¸OpenAIçš„CLIPç›¸åª²ç¾æˆ–è¶…è¶Šå…¶æ€§èƒ½ã€‚è¯¥è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„è®­ç»ƒæ¡†æ¶å’Œæ•°æ®é›†ï¼Œæå‡ç¼–ç å™¨çš„è´¨é‡ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•æä¾›å®ç”¨çš„å¥½å¤„ã€‚OpenVisionæä¾›äº†ä»590ä¸‡åˆ°6.32äº¿å‚æ•°çš„å¤šç§é€‰æ‹©ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥åœ¨æ¨¡å‹å®¹é‡å’Œæ•ˆç‡ä¹‹é—´çµæ´»æƒè¡¡ã€‚é€šè¿‡è¿™äº›å¼€æ”¾çš„èµ„æºï¼ŒOpenVisionä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„æ„å»ºæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04253', 'title': 'LLM-Independent Adaptive RAG: Let the Question Speak for Itself', 'url': 'https://huggingface.co/papers/2505.04253', 'abstract': 'Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '7ce9a465202a9c3c', 'authors': ['Maria Marina', 'Nikolay Ivanov', 'Sergey Pletenev', 'Mikhail Salnikov', 'Daria Galimzianova', 'Nikita Krayko', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.04253.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rag', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ 27 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 7 Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ¸ Ğ¸Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° 6 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Adaptive Retrieval: Enhancing QA with External Information', 'desc': 'This paper addresses the issue of hallucinations in Large Language Models (LLMs) and presents a solution through Retrieval-Augmented Generation (RAG). The authors propose lightweight, LLM-independent adaptive retrieval methods that utilize external information, aiming to improve efficiency while maintaining performance. They analyze 27 features across 7 groups to create effective hybrid combinations for retrieval. The results indicate that their approach can achieve comparable performance to complex LLM-based methods but with significant efficiency improvements.'}, 'zh': {'title': 'è½»é‡çº§è‡ªé€‚åº”æ£€ç´¢ï¼Œæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œè€Œå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰å¯ä»¥å¸®åŠ©å‡è½»è¿™ä¸€é—®é¢˜ï¼Œä½†ä»£ä»·é«˜æ˜‚ä¸”å¯èƒ½å¯¼è‡´é”™è¯¯ä¿¡æ¯ã€‚è‡ªé€‚åº”æ£€ç´¢æ—¨åœ¨ä»…åœ¨å¿…è¦æ—¶è¿›è¡Œæ£€ç´¢ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºåŸºäºLLMçš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ•ˆç‡ä½ä¸‹ä¸”ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤–éƒ¨ä¿¡æ¯çš„è½»é‡çº§LLMç‹¬ç«‹è‡ªé€‚åº”æ£€ç´¢æ–¹æ³•ï¼Œç ”ç©¶äº†27ä¸ªç‰¹å¾å¹¶å°†å…¶ç»„ç»‡æˆ7ä¸ªç»„åŠå…¶æ··åˆç»„åˆã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨6ä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸å¤æ‚çš„LLMæ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œå±•ç¤ºäº†å¤–éƒ¨ä¿¡æ¯åœ¨è‡ªé€‚åº”æ£€ç´¢ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03538', 'title': 'RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT', 'url': 'https://huggingface.co/papers/2505.03538', 'abstract': 'Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '9e7cb17dec2eda27', 'authors': ['Chuyu Zhao', 'Hao Huang', 'Jiashuo Guo', 'Ziyu Shen', 'Zhongwei Zhou', 'Jie Liu', 'Zekuan Yu'], 'affiliations': ['Academy for Engineering and Technology, Fudan University, Shanghai 200433, China', 'Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China', 'School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03538.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#transfer_learning', '#training'], 'emoji': 'ğŸ¦·', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·ÑƒĞ±Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑƒĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·ÑƒĞ±Ğ¾Ğ² Ğ½Ğ° ĞšĞ›ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Region-Aware Instructive Learning (RAIL). RAIL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ´Ğ²Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Disagreement-Focused Supervision (DFS) Controller Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Confidence-Aware Learning (CAL) Modulator Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAIL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing 3D Tooth Segmentation with RAIL: A Smart Learning Approach', 'desc': 'This paper presents a new method called Region-Aware Instructive Learning (RAIL) for improving 3D tooth segmentation from CBCT scans using semi-supervised learning. RAIL addresses challenges like limited supervision in ambiguous areas and the unreliability of pseudo-labels by employing a dual-group, dual-student framework with a shared teacher network. It introduces two key mechanisms: Disagreement-Focused Supervision (DFS) to enhance learning in uncertain regions, and Confidence-Aware Learning (CAL) to stabilize predictions by focusing on high-confidence areas. Experimental results demonstrate that RAIL outperforms existing methods, making it a significant advancement in the field of medical image segmentation.'}, 'zh': {'title': 'åŒºåŸŸæ„ŸçŸ¥æŒ‡å¯¼å­¦ä¹ ï¼šæå‡3Dç‰™é½¿åˆ†å‰²çš„åŠç›‘ç£æ–¹æ³•', 'desc': 'åŠç›‘ç£å­¦ä¹ åœ¨CBCTæ‰«æçš„3Dç‰™é½¿åˆ†å‰²ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåœ¨ç»“æ„æ¨¡ç³Šæˆ–æ ‡æ³¨é”™è¯¯çš„åŒºåŸŸï¼Œç›‘ç£è®­ç»ƒä¸­çš„çº æ­£ç›‘ç£æœ‰é™ï¼›ä»¥åŠåœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šï¼Œç”±äºä¸å¯é çš„ä¼ªæ ‡ç­¾å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒºåŸŸæ„ŸçŸ¥æŒ‡å¯¼å­¦ä¹ ï¼ˆRAILï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒç»„åŒå­¦ç”Ÿçš„åŠç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡äº¤æ›¿è®­ç»ƒä¿ƒè¿›ç»„é—´çŸ¥è¯†è½¬ç§»å’Œåä½œæŒ‡å¯¼ï¼ŒåŒæ—¶å‡å°‘å¯¹å•ä¸€æ¨¡å‹ç‰¹å¾çš„è¿‡æ‹Ÿåˆã€‚RAILå¼•å…¥äº†ä¸¤ç§æŒ‡å¯¼æœºåˆ¶ï¼Œåˆ†åˆ«æ˜¯å…³æ³¨åˆ†æ­§çš„ç›‘ç£æ§åˆ¶å™¨å’ŒåŸºäºç½®ä¿¡åº¦çš„å­¦ä¹ è°ƒèŠ‚å™¨ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šåŒºåŸŸçš„å­¦ä¹ æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03105', 'title': 'Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation', 'url': 'https://huggingface.co/papers/2505.03105', 'abstract': 'Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI\'s evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.', 'score': 1, 'issue_id': 3657, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '24cdaf99b9b04dad', 'authors': ['Xule Lin'], 'affiliations': ['Department of Management and Entrepreneurship, Imperial College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.03105.jpg', 'data': {'categories': ['#agents', '#healthcare', '#science', '#ethics', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Cognitio Emergens: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ĞºĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Cognitio Emergens (CE) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…. CE Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ñ€Ğ¾Ğ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ˜Ğ˜, ĞºĞ°ĞºĞ¸Ğµ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¸ ĞºĞ°ĞºĞ¸Ğµ ÑĞ¸Ğ»Ñ‹ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€ÑÑ‚Ğ²Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜. CE Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€Ğ¾Ğ»ÑŒ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒĞºĞµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºĞ°Ğº Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¼Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ².'}, 'en': {'title': 'Transforming Scientific Collaboration: Humans and AI as Co-Evolutionary Partners', 'desc': 'This paper discusses how the relationship between humans and AI in scientific research is changing from a simple tool-user dynamic to a more collaborative partnership. It introduces a new framework called Cognitio Emergens (CE) that addresses the limitations of existing models by focusing on the evolving roles and interactions between humans and AI over time. CE includes three main components: Agency Configurations that describe how authority is shared, Epistemic Dimensions that outline capabilities developed through collaboration, and Partnership Dynamics that explore how these relationships change. By viewing human-AI collaboration as a co-evolutionary process, the framework aims to enhance scientific understanding while ensuring that human input remains significant in the face of advancing AI capabilities.'}, 'zh': {'title': 'äººç±»ä¸AIçš„å…±åŒè¿›åŒ–ï¼šçŸ¥è¯†åˆ›é€ çš„æ–°è§†è§’', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¹‹é—´çš„åˆä½œå…³ç³»å¦‚ä½•ä»ç®€å•çš„å·¥å…·ä½¿ç”¨è€…è½¬å˜ä¸ºå…±åŒè¿›åŒ–çš„çŸ¥è¯†ä¼™ä¼´ã€‚æ–‡ç« ä»‹ç»äº†Cognitio Emergensï¼ˆCEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¼ºè°ƒç§‘å­¦ç†è§£æ˜¯å¦‚ä½•é€šè¿‡äººç±»ä¸AIçš„äº’åŠ¨é€æ­¥å½¢æˆçš„ã€‚CEæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šä»£ç†é…ç½®ã€è®¤çŸ¥ç»´åº¦å’Œä¼™ä¼´åŠ¨æ€ï¼Œå¸®åŠ©æè¿°äººç±»ä¸AIä¹‹é—´çš„æƒåŠ›åˆ†é…ã€åˆä½œèƒ½åŠ›å’Œå…³ç³»æ¼”å˜ã€‚é€šè¿‡é‡æ–°å®šä¹‰äººç±»ä¸AIçš„ç§‘å­¦åˆä½œï¼ŒCEæä¾›äº†ä¿ƒè¿›æœ‰æ„ä¹‰çš„äººç±»å‚ä¸å’Œç§‘å­¦çªç ´çš„æ¦‚å¿µå·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02820', 'title': 'AutoLibra: Agent Metric Induction from Open-Ended Feedback', 'url': 'https://huggingface.co/papers/2505.02820', 'abstract': 'Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don\'t click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent\'s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra\'s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.', 'score': 1, 'issue_id': 3661, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '52333e4c868e25f8', 'authors': ['Hao Zhu', 'Phil Cuvin', 'Xinkai Yu', 'Charlotte Ka Yee Yan', 'Jason Zhang', 'Diyi Yang'], 'affiliations': ['stanford.edu', 'upenn.edu', 'utoronto.ca'], 'pdf_title_img': 'assets/pdf/title_img/2505.02820.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rlhf', '#alignment', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AutoLibra: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': "AutoLibra - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. AutoLibra Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ 'Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ' Ğ¸ 'Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ AutoLibra ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."}, 'en': {'title': 'Transforming Feedback into Fine-Grained Agent Evaluation Metrics', 'desc': "The paper introduces AutoLibra, a novel framework for evaluating agents using fine-grained metrics derived from open-ended human feedback. It transforms qualitative feedback into quantitative metrics by clustering behaviors and defining clear examples, allowing for a more nuanced assessment of agent performance. AutoLibra also proposes two meta-metrics, 'coverage' and 'redundancy', to ensure that the evaluation metrics align well with the feedback provided. Experimental results show that AutoLibra improves agent performance significantly compared to traditional success metrics, making it a valuable tool for enhancing language agents."}, 'zh': {'title': 'AutoLibraï¼šæ™ºèƒ½ä½“è¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAutoLibraçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¼€æ”¾å¼äººç±»åé¦ˆè½¬åŒ–ä¸ºç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ¥æ›´å¥½åœ°è¡¡é‡æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚AutoLibraé€šè¿‡å¯¹åé¦ˆè¿›è¡Œå½’ç±»å’Œå®šä¹‰ï¼Œç”Ÿæˆå…·ä½“çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ ‡å‡†æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoLibraèƒ½å¤Ÿç”Ÿæˆæ¯”ç°æœ‰åŸºå‡†æ›´æœ‰æ•ˆçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02393', 'title': 'Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection', 'url': 'https://huggingface.co/papers/2505.02393', 'abstract': 'Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.', 'score': 1, 'issue_id': 3651, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'b5c708abbb25e1ce', 'authors': ['Sungheon Jeong', 'Jihong Park', 'Mohsen Imani'], 'affiliations': ['MOLOCO', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.02393.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#synthetic'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸Ğ· RGB Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ IEF-VAD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ RGB-ĞºĞ°Ğ´Ñ€Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆÑƒĞ¼ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° ĞšĞ°Ğ»Ğ¼Ğ°Ğ½Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ ÑĞ»Ğ¸Ñ‚Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ. IEF-VAD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Anomaly Detection with Image-Event Fusion', 'desc': 'The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data.'}, 'zh': {'title': 'å›¾åƒä¸äº‹ä»¶èåˆï¼Œæå‡è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§', 'desc': 'ç°æœ‰çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹å™¨ä¸»è¦ä¾èµ–RGBå¸§ï¼Œä½†è¿™äº›å¸§ç¼ºä¹æ•æ‰çªå‘æˆ–ç¬æ€è¿åŠ¨çº¿ç´¢çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒ-äº‹ä»¶èåˆçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼ˆIEF-VADï¼‰ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»RGBè§†é¢‘åˆæˆäº‹ä»¶è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„è¿‡ç¨‹å°†å…¶ä¸å›¾åƒç‰¹å¾èåˆã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼å»ºæ¨¡é‡å°¾ä¼ æ„Ÿå™¨å™ªå£°ï¼Œåº”ç”¨å¡å°”æ›¼é£æ ¼çš„é€å¸§æ›´æ–°æ¥å¹³è¡¡æ—¶é—´ä¸Šçš„æ¨¡æ€ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–èåˆçš„æ½œåœ¨çŠ¶æ€ä»¥æ¶ˆé™¤æ®‹ä½™çš„è·¨æ¨¡æ€å™ªå£°ã€‚IEF-VADåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„å¼‚å¸¸æ£€æµ‹åŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå±•ç¤ºäº†åˆæˆäº‹ä»¶è¡¨ç¤ºåœ¨å¼ºè°ƒRGBå¸§ä¸­å¸¸è¢«ä½ä¼°çš„è¿åŠ¨çº¿ç´¢æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01449', 'title': 'COSMOS: Predictable and Cost-Effective Adaptation of LLMs', 'url': 'https://huggingface.co/papers/2505.01449', 'abstract': 'Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.', 'score': 1, 'issue_id': 3665, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': 'cc6993ec1bb06272', 'authors': ['Jiayu Wang', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.01449.jpg', 'data': {'categories': ['#optimization', '#data', '#inference', '#training', '#benchmark'], 'emoji': 'ğŸ”®', 'ru': {'title': 'COSMOS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ COSMOS - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ fine-tuning Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° COSMOS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 92.72%.'}, 'en': {'title': 'COSMOS: Smart Predictions for Efficient LLM Adaptation', 'desc': 'This paper addresses the challenge of selecting the best model and adaptation strategy for large language models (LLMs) while managing resource constraints. It introduces COSMOS, a unified prediction framework designed to estimate the performance and cost of different adaptation strategies without the need for extensive trials. The framework utilizes lightweight proxy models and low-sample scaling laws to predict outcomes efficiently. The results indicate that COSMOS can significantly reduce computational costs while maintaining high prediction accuracy across various benchmarks.'}, 'zh': {'title': 'COSMOSï¼šé«˜æ•ˆé¢„æµ‹é€‚åº”ç»“æœçš„æ¡†æ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹é€‰æ‹©æœ€ä½³æ¨¡å‹å’Œé€‚åº”ç­–ç•¥éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†COSMOSï¼Œä¸€ä¸ªç»Ÿä¸€çš„é¢„æµ‹æ¡†æ¶ï¼Œå¯ä»¥åœ¨æœ€ä½æˆæœ¬ä¸‹é«˜æ•ˆä¼°è®¡é€‚åº”ç»“æœã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½»é‡çº§ä»£ç†æ¨¡å‹å’Œä½æ ·æœ¬æ‰©å±•æ³•åˆ™æ¥é¢„æµ‹å¾®è°ƒæ€§èƒ½å’Œæ£€ç´¢å¢å¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒCOSMOSåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜é¢„æµ‹å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†92.72%çš„è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07916', 'title': 'MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder', 'url': 'https://huggingface.co/papers/2505.07916', 'abstract': 'We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.', 'score': 80, 'issue_id': 3752, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '30175415a859995c', 'authors': ['Bowen Zhang', 'Congchao Guo', 'Geng Yang', 'Hang Yu', 'Haozhe Zhang', 'Heidi Lei', 'Jialong Mai', 'Junjie Yan', 'Kaiyue Yang', 'Mingqi Yang', 'Peikai Huang', 'Ruiyang Jin', 'Sitan Jiang', 'Weihua Cheng', 'Yawei Li', 'Yichen Xiao', 'Yiying Zhou', 'Yongmao Zhang', 'Yuan Lu', 'Yucen He'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07916.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#games', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸: MiniMax-Speech - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ»Ğ¾Ğ½', 'desc': 'MiniMax-Speech - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµĞ¼Ğ±Ñ€Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞµĞ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 32 ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°. MiniMax-Speech Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ‚ĞµĞ¼Ğ±Ñ€Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Speech Synthesis with MiniMax-Speech', 'desc': 'MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.'}, 'zh': {'title': 'MiniMax-Speechï¼šé«˜è´¨é‡è¯­éŸ³ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'MiniMax-Speechæ˜¯ä¸€ç§åŸºäºè‡ªå›å½’Transformerçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºå¯å­¦ä¹ çš„è¯´è¯äººç¼–ç å™¨ï¼Œå¯ä»¥ä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œè€Œæ— éœ€å…¶è½¬å½•ã€‚è¯¥æ¨¡å‹æ”¯æŒé›¶æ ·æœ¬ç”Ÿæˆå…·æœ‰å‚è€ƒéŸ³è‰²çš„è¡¨è¾¾æ€§è¯­éŸ³ï¼Œå¹¶ä¸”åœ¨å•æ ·æœ¬è¯­éŸ³å…‹éš†ä¸­è¡¨ç°å‡ºæé«˜çš„ç›¸ä¼¼åº¦ã€‚æ­¤å¤–ï¼ŒMiniMax-Speechåœ¨å¤šä¸ªå®¢è§‚å’Œä¸»è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒ32ç§è¯­è¨€ï¼Œå¹¶åœ¨è¯­éŸ³å…‹éš†çš„å®¢è§‚æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07591', 'title': 'A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models', 'url': 'https://huggingface.co/papers/2505.07591', 'abstract': "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.", 'score': 7, 'issue_id': 3750, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'ca7c47ccc0066e55', 'authors': ['Junjie Ye', 'Caishuang Huang', 'Zhuohan Chen', 'Wenjie Fu', 'Chenyuan Yang', 'Leyi Yang', 'Yilong Wu', 'Peng Wang', 'Meng Zhou', 'Xiaolong Yang', 'Tao Gui', 'Qi Zhang', 'Zhongchao Shi', 'Jianping Fan', 'Xuanjing Huang'], 'affiliations': ['Institute of Modern Languages and Linguistics, Fudan University', 'Lenovo Research', 'School of Computer Science, Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.07591.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#optimization', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°, Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ 1200 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 19 LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ğ¸Ğ¶Ğ°Ğ»Ğ°ÑÑŒ Ñ 77,67% Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ I Ğ´Ğ¾ 32,96% Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ IV.'}, 'en': {'title': 'Enhancing LLMs with Multi-Dimensional Constraints', 'desc': 'This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters.'}, 'zh': {'title': 'å¤šç»´çº¦æŸæ¡†æ¶æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éµå¾ªç”¨æˆ·å®šä¹‰çº¦æŸæ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºæ¨¡æ¿åŒ–çš„çº¦æŸæç¤ºï¼Œç¼ºä¹çœŸå®ä¸–ç•Œä½¿ç”¨çš„å¤šæ ·æ€§ï¼Œé™åˆ¶äº†ç»†è‡´çš„æ€§èƒ½è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç»´çº¦æŸæ¡†æ¶ï¼Œæ¶µç›–ä¸‰ç§çº¦æŸæ¨¡å¼ã€å››ç§çº¦æŸç±»åˆ«å’Œå››ä¸ªéš¾åº¦çº§åˆ«ã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æŒ‡ä»¤ç”Ÿæˆç®¡é“ï¼Œç”Ÿæˆäº†1200ä¸ªå¯éªŒè¯çš„æŒ‡ä»¤éµå¾ªæµ‹è¯•æ ·æœ¬ï¼Œå¹¶è¯„ä¼°äº†19ä¸ªLLMåœ¨ä¸åŒçº¦æŸå½¢å¼ä¸‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07215', 'title': 'Measuring General Intelligence with Generated Games', 'url': 'https://huggingface.co/papers/2505.07215', 'abstract': 'We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.', 'score': 5, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'ed99ec3875dd9a95', 'authors': ['Vivek Verma', 'David Huang', 'William Chen', 'Dan Klein', 'Nicholas Tomlin'], 'affiliations': ['Computer Science Division, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.07215.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#rl', '#games', '#synthetic'], 'emoji': 'ğŸ®', 'ru': {'title': 'gg-bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ gg-bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. gg-bench Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´ Ğ¸Ğ³Ñ€. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (7-9% Ğ¿Ğ¾Ğ±ĞµĞ´), Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 31-36% Ğ¿Ğ¾Ğ±ĞµĞ´.'}, 'en': {'title': 'Dynamic Game Environments for Evaluating AI Reasoning', 'desc': 'The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI.'}, 'zh': {'title': 'gg-benchï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'æˆ‘ä»¬æå‡ºäº†gg-benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ä¸€èˆ¬æ¨ç†èƒ½åŠ›çš„æ¸¸æˆç¯å¢ƒé›†åˆã€‚ä¸å¤§å¤šæ•°é™æ€åŸºå‡†æµ‹è¯•ä¸åŒï¼Œgg-benchæ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯ä»¥éšæ—¶ç”Ÿæˆæ–°çš„è¯„ä¼°å®ä¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œgg-benché€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–°æ¸¸æˆçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶å°†æ¯ä¸ªæ¸¸æˆå®ç°ä¸ºGymç¯å¢ƒï¼Œæœ€åé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡å‹åœ¨è¿™äº›æ¸¸æˆä¸­ä¸å¼ºåŒ–å­¦ä¹ ä»£ç†çš„èƒœç‡æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨gg-benchä¸Šçš„èƒœç‡ä»…ä¸º7-9%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05464', 'title': 'Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging', 'url': 'https://huggingface.co/papers/2505.05464', 'abstract': 'Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.', 'score': 4, 'issue_id': 3760, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': 'd0d13229ec81018d', 'authors': ['Shiqi Chen', 'Jinghan Zhang', 'Tongyao Zhu', 'Wei Liu', 'Siyang Gao', 'Miao Xiong', 'Manling Li', 'Junxian He'], 'affiliations': ['City University of Hong Kong', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05464.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#architecture', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ - Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ…. ĞŸĞ¾ÑĞ»Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‚ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼.'}, 'en': {'title': 'Merging Models: Bridging Vision and Reasoning', 'desc': 'This paper investigates how to combine the visual understanding of Vision-Language Models (VLMs) with the reasoning skills of Large Language Models (LLMs) through a process called model merging. The authors propose a novel approach that merges models from different modalities, allowing VLMs to gain reasoning capabilities without the need for additional training. Their experiments reveal that while perception is mainly captured in the early layers of the model, reasoning is enhanced across all layers after merging. This study highlights the effectiveness of model merging for improving multimodal integration and understanding the interplay between perception and reasoning.'}, 'zh': {'title': 'æ¨¡å‹åˆå¹¶ï¼šè·¨æ¨¡æ€çš„æ¨ç†ä¸æ„ŸçŸ¥ç»“åˆ', 'desc': 'è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†è§†è§‰æ„ŸçŸ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ç»“åˆåœ¨ä¸€èµ·ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¦‚ä½•ç»“åˆå¹¶ç›¸äº’è´¡çŒ®ä»ç„¶ä¸å¤ªæ¸…æ¥šã€‚æˆ‘ä»¬é€šè¿‡æ¨¡å‹åˆå¹¶çš„æ–¹æ³•æ¥æ¢ç´¢æ„ŸçŸ¥ä¸æ¨ç†çš„ç»„åˆï¼Œè¿æ¥ä¸åŒæ¨¡å‹çš„å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶èƒ½å¤ŸæˆåŠŸåœ°å°†LLMsçš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°VLMsï¼Œå¹¶ä¸”æœ‰åŠ©äºç†è§£æ„ŸçŸ¥ä¸æ¨ç†çš„å†…éƒ¨æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08638', 'title': 'TRAIL: Trace Reasoning and Agentic Issue Localization', 'url': 'https://huggingface.co/papers/2505.08638', 'abstract': 'The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '823a40ec2bb1f793', 'authors': ['Darshan Deshpande', 'Varun Gangal', 'Hersh Mehta', 'Jitin Krishnan', 'Anand Kannappan', 'Rebecca Qian'], 'affiliations': ['Patronus AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.08638.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#agents', '#open_source'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ¾Ñ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TRAIL Ğ¸Ğ· 148 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¾Ğ¹ Ñ‚Ñ€Ğ°ÑÑ, Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini-2.5-pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ²ÑĞµĞ³Ğ¾ 11% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° TRAIL. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ ĞºĞ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Evaluation for Agentic Workflows', 'desc': 'This paper addresses the challenges of evaluating agentic workflows, which are increasingly used in various fields. Current methods rely on manual analysis of complex workflow traces, making them inefficient as the volume of data grows. The authors propose a new framework that includes a taxonomy of error types specific to agentic systems and introduce a dataset of 148 annotated traces for evaluation. Their findings indicate that existing large language models struggle with debugging these traces, highlighting the need for improved evaluation techniques in this area.'}, 'zh': {'title': 'æ™ºèƒ½å·¥ä½œæµè¯„ä¼°çš„æ–°æ–¹æ³•', 'desc': 'éšç€æ™ºèƒ½å·¥ä½œæµåœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›ç³»ç»Ÿç”Ÿæˆçš„å¤æ‚è½¨è¿¹å˜å¾—è‡³å…³é‡è¦ã€‚ç›®å‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºäººå·¥ã€ç‰¹å®šé¢†åŸŸçš„åˆ†æï¼Œè¿™ç§æ–¹æ³•æ— æ³•é€‚åº”æ—¥ç›Šå¤æ‚å’Œåºå¤§çš„æ™ºèƒ½è¾“å‡ºã€‚é”™è¯¯åˆ†æåœ¨è¿™äº›ç¯å¢ƒä¸­å˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºå¤–éƒ¨å·¥å…·è¾“å‡ºä¸è¯­è¨€æ¨¡å‹æ¨ç†ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä½¿å¾—è°ƒè¯•å˜å¾—æ›´åŠ å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ™ºèƒ½å·¥ä½œæµè½¨è¿¹çš„åŠ¨æ€è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§é”™è¯¯ç±»å‹çš„æ­£å¼åˆ†ç±»æ³•ï¼ŒåŒæ—¶æ„å»ºäº†148ä¸ªå¤§å‹äººç±»æ ‡æ³¨çš„è½¨è¿¹æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥åœ¨æ™ºèƒ½å·¥ä½œæµè¯„ä¼°æ–¹é¢çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08175', 'title': 'Fast Text-to-Audio Generation with Adversarial Post-Training', 'url': 'https://huggingface.co/papers/2505.08175', 'abstract': 'Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating approx12s of 44.1kHz stereo audio in approx75ms on an H100, and approx7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.', 'score': 3, 'issue_id': 3761, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '845d8b64408c2d62', 'authors': ['Zachary Novack', 'Zach Evans', 'Zack Zukowski', 'Josiah Taylor', 'CJ Carr', 'Julian Parker', 'Adnan Al-Sinan', 'Gian Marco Iodice', 'Julian McAuley', 'Taylor Berg-Kirkpatrick', 'Jordi Pons'], 'affiliations': ['Arm', 'Stability AI', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.08175.jpg', 'data': {'categories': ['#audio', '#inference', '#optimization', '#diffusion', '#open_source'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ARC Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ - Adversarial Relativistic-Contrastive (ARC) Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. ARC ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ€ĞµĞ»ÑÑ‚Ğ¸Ğ²Ğ¸ÑÑ‚ÑĞºĞ¸Ğ¹ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Stable Audio Open Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 12 ÑĞµĞºÑƒĞ½Ğ´ ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ 44.1ĞºĞ“Ñ† Ğ·Ğ° 75 Ğ¼Ñ Ğ½Ğ° H100 Ğ¸ 7 ÑĞµĞºÑƒĞ½Ğ´ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'Accelerating Text-to-Audio: The Fastest Model Yet!', 'desc': "This paper introduces Adversarial Relativistic-Contrastive (ARC) post-training, a novel method designed to speed up text-to-audio systems without relying on distillation. ARC post-training enhances diffusion and flow models by applying a relativistic adversarial approach combined with a contrastive discriminator objective, which improves the model's ability to follow prompts accurately. The authors demonstrate that their method significantly reduces inference time, achieving audio generation in approximately 75 milliseconds on high-performance hardware and 7 seconds on mobile devices. This advancement positions ARC post-training as the fastest text-to-audio model currently available, making it more practical for creative applications."}, 'zh': {'title': 'åŠ é€Ÿæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆçš„é©å‘½æ€§æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åè®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºå¯¹æŠ—ç›¸å¯¹å¯¹æ¯”ï¼ˆARCï¼‰ï¼Œæ—¨åœ¨åŠ é€Ÿæ–‡æœ¬åˆ°éŸ³é¢‘ç³»ç»Ÿçš„æ¨ç†é€Ÿåº¦ã€‚ARCåè®­ç»ƒç®—æ³•ä¸ä¾èµ–äºè’¸é¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ‰©æ•£/æµæ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡ç»“åˆç›¸å¯¹å¯¹æŠ—å½¢å¼å’Œå¯¹æ¯”åˆ¤åˆ«å™¨ç›®æ ‡ï¼ŒARCé¼“åŠ±æ¨¡å‹æ›´å¥½åœ°éµå¾ªæç¤ºã€‚ç»è¿‡ä¼˜åŒ–åï¼Œè¯¥æ¨¡å‹åœ¨H100ä¸Šèƒ½åœ¨çº¦75æ¯«ç§’å†…ç”Ÿæˆ12ç§’çš„44.1kHzç«‹ä½“å£°éŸ³é¢‘ï¼Œæˆä¸ºç›®å‰å·²çŸ¥çš„æœ€å¿«æ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08727', 'title': 'Memorization-Compression Cycles Improve Generalization', 'url': 'https://huggingface.co/papers/2505.08727', 'abstract': 'We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.', 'score': 2, 'issue_id': 3760, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '07a8c687bc82dfeb', 'authors': ['Fangyuan Yu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.08727.jpg', 'data': {'categories': ['#optimization', '#data', '#training', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Information Bottleneck Language Modeling (IBLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ· Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Gated Phase Transition (GAPT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Compressing Knowledge for Better Generalization', 'desc': 'This paper demonstrates that improving generalization in machine learning models can be achieved not only by increasing the amount of data but also by compressing the internal representations of the model. The authors introduce the Information Bottleneck Language Modeling (IBLM) objective, which focuses on minimizing the complexity of representations while maintaining effective prediction capabilities. They observe a cycle of memorization and compression during the pretraining of large language models, which aligns with the principles of IBLM and reflects biological processes of learning and memory consolidation. To leverage this insight, they propose the Gated Phase Transition (GAPT) algorithm, which alternates between phases of memorization and compression, leading to significant improvements in model performance and generalization.'}, 'zh': {'title': 'å‹ç¼©ä¸è®°å¿†çš„å¹³è¡¡æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›', 'desc': 'æœ¬æ–‡ç†è®ºä¸Šè¯æ˜ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸ä»…é€šè¿‡å¢åŠ æ•°æ®é‡æ¥æé«˜ï¼Œè¿˜å¯ä»¥é€šè¿‡å‹ç¼©å†…éƒ¨è¡¨ç¤ºæ¥å®ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¿¡æ¯ç“¶é¢ˆè¯­è¨€å»ºæ¨¡ï¼ˆIBLMï¼‰ç›®æ ‡ï¼Œå°†è¯­è¨€å»ºæ¨¡é‡æ–°æ„å»ºä¸ºä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ï¼šåœ¨æœ€ä¼˜é¢„æµ‹æ€§èƒ½çš„æ¡ä»¶ä¸‹ï¼Œæœ€å°åŒ–è¡¨ç¤ºç†µã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‡ºç°äº†è®°å¿†ä¸å‹ç¼©çš„å¾ªç¯ç°è±¡ï¼Œè¿™ç§ç°è±¡ä¸IBLMæ‰€æè¿°çš„é¢„æµ‹-å‹ç¼©æƒè¡¡å¯†åˆ‡ç›¸å…³ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†é—¨æ§ç›¸å˜ï¼ˆGAPTï¼‰è®­ç»ƒç®—æ³•ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åœ¨è®°å¿†å’Œå‹ç¼©é˜¶æ®µä¹‹é—´åˆ‡æ¢ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08665', 'title': 'SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation', 'url': 'https://huggingface.co/papers/2505.08665', 'abstract': 'Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.', 'score': 2, 'issue_id': 3750, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '8cbf16dc2ec90273', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Free University of Bozen-Bolzano Bozen-Bolzano, IT'], 'pdf_title_img': 'assets/pdf/title_img/2505.08665.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'SkillFormer: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'SkillFormer - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ CrossViewFusion Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Low-Rank Adaptation, SkillFormer Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ EgoExo4D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 4,5 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'SkillFormer: Efficient Multi-View Skill Assessment', 'desc': 'This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation.'}, 'zh': {'title': 'SkillFormerï¼šé«˜æ•ˆçš„å¤šè§†è§’æŠ€èƒ½è¯„ä¼°æ¶æ„', 'desc': 'è¯„ä¼°äººç±»åœ¨å¤æ‚æ´»åŠ¨ä¸­çš„æŠ€èƒ½æ°´å¹³æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºä½“è‚²ã€åº·å¤å’ŒåŸ¹è®­é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†SkillFormerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¶æ„ï¼Œèƒ½å¤Ÿä»è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨è§†è§’çš„è§†é¢‘ä¸­ç»Ÿä¸€ä¼°è®¡æŠ€èƒ½æ°´å¹³ã€‚SkillFormeråŸºäºTimeSformeréª¨å¹²ç½‘ï¼Œå¼•å…¥äº†CrossViewFusionæ¨¡å—ï¼Œé€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›ã€å¯å­¦ä¹ çš„é—¨æ§å’Œè‡ªé€‚åº”è‡ªæ ¡å‡†æ¥èåˆè§†è§’ç‰¹å¾ã€‚é€šè¿‡ä½ç§©é€‚åº”æŠ€æœ¯ï¼Œæˆ‘ä»¬ä»…å¾®è°ƒå°‘é‡å‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶åœ¨EgoExo4Dæ•°æ®é›†ä¸Šå®ç°äº†å¤šè§†è§’è®¾ç½®ä¸‹çš„æœ€å…ˆè¿›å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08311', 'title': 'AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale', 'url': 'https://huggingface.co/papers/2505.08311', 'abstract': 'We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.', 'score': 2, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '492d10424bc0d97d', 'authors': ['Yunjie Ji', 'Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.08311.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#open_source', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ 32B-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ°Ğ½ĞºÑƒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…', 'desc': 'AM-Thinking-v1 - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DeepSeek-R1 Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. AM-Thinking-v1 Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ supervised fine-tuning Ğ¸ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Innovation', 'desc': 'AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.'}, 'zh': {'title': 'å¼€æºåˆ›æ–°ï¼Œæ¨ç†æ–°é«˜åº¦', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†AM-Thinking-v1ï¼Œè¿™æ˜¯ä¸€ä¸ª32Bçš„å¯†é›†è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨äº†æ¨ç†çš„å‰æ²¿ï¼Œä½“ç°äº†å¼€æºåˆ›æ–°çš„åˆä½œç²¾ç¥ã€‚å®ƒåœ¨AIME 2024ã€AIME 2025å’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†DeepSeek-R1ï¼Œå¹¶ä¸é¢†å…ˆçš„ä¸“å®¶æ··åˆæ¨¡å‹å¦‚Qwen3-235B-A22Bå’ŒSeed1.5-Thinkingç›¸åª²ç¾ã€‚AM-Thinking-v1å®Œå…¨åŸºäºå¼€æºçš„Qwen2.5-32BåŸºç¡€æ¨¡å‹ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†å¼€æºç¤¾åŒºå¯ä»¥åœ¨32Bè§„æ¨¡ä¸Šå®ç°é«˜æ€§èƒ½ï¼Œå¹³è¡¡é¡¶çº§æ€§èƒ½ä¸å®é™…å¯ç”¨æ€§ï¼Œæ¿€åŠ±æ›´å¤šåˆä½œåŠªåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21475', 'title': 'Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines', 'url': 'https://huggingface.co/papers/2504.21475', 'abstract': 'This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.', 'score': 2, 'issue_id': 3756, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '86e4cc7ad4a84ece', 'authors': ['Serry Sibaee', 'Samar Ahmed', 'Abdullah Al Harbi', 'Omer Nacar', 'Adel Ammar', 'Yasser Habashi', 'Wadii Boulila'], 'affiliations': ['College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia', 'Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia', 'Independent Researcher, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21475.jpg', 'data': {'categories': ['#training', '#data', '#machine_translation', '#dataset', '#architecture', '#multilingual', '#low_resource'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞµ: Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ° Ğ¿Ğ¾ Ğ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»ÑƒÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ğ»ĞµĞºÑĞ¸ĞºĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ğ°Ğ±Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ ARBERTv2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ.'}, 'en': {'title': 'Bridging the Gap in Arabic Language Processing with a Reverse Dictionary', 'desc': 'This paper presents a new system for Arabic natural language processing called the Arabic Reverse Dictionary (RD), which helps users find words based on their meanings. The authors introduce a transformer-based model with a unique semi-encoder architecture that improves performance on Arabic RD tasks. They also create a detailed dataset and establish quality standards for Arabic definitions, showing that specialized Arabic models outperform general multilingual ones. Additionally, the study offers a theoretical framework for reverse dictionaries and introduces a Python library for easy implementation and training.'}, 'zh': {'title': 'é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ï¼šæå‡è¯­è¨€å¤„ç†çš„åˆ©å™¨', 'desc': 'æœ¬ç ”ç©¶è§£å†³äº†é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ç³»ç»Ÿï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®æè¿°æˆ–å«ä¹‰æ‰¾åˆ°å•è¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå˜æ¢å™¨çš„åŠç¼–ç ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå…·æœ‰å‡ ä½•é€’å‡å±‚ï¼Œè¾¾åˆ°äº†é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ä»»åŠ¡çš„æœ€å…ˆè¿›ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶æ–¹æ³•åŒ…æ‹¬å…¨é¢çš„æ•°æ®é›†æ„å»ºè¿‡ç¨‹ï¼Œå¹¶å»ºç«‹äº†é˜¿æ‹‰ä¼¯è¯­è¯å…¸å®šä¹‰çš„æ­£å¼è´¨é‡æ ‡å‡†ã€‚å®éªŒè¡¨æ˜ï¼Œä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„æ¨¡å‹æ˜¾è‘—ä¼˜äºé€šç”¨å¤šè¯­è¨€åµŒå…¥ï¼ŒARBERTv2æ¨¡å‹è·å¾—äº†æœ€ä½³æ’ååˆ†æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08751', 'title': 'Aya Vision: Advancing the Frontier of Multilingual Multimodality', 'url': 'https://huggingface.co/papers/2505.08751', 'abstract': 'Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.', 'score': 1, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': 'c063c125e504fa88', 'authors': ['Saurabh Dash', 'Yiyang Nan', 'John Dang', 'Arash Ahmadian', 'Shivalika Singh', 'Madeline Smith', 'Bharat Venkitesh', 'Vlad Shmyhlo', 'Viraat Aryabumi', 'Walter Beller-Morales', 'Jeremy Pekmez', 'Jason Ozuzu', 'Pierre Richemond', 'Acyr Locatelli', 'Nick Frosst', 'Phil Blunsom', 'Aidan Gomez', 'Ivan Zhang', 'Marzieh Fadaee', 'Manoj Govindassamy', 'Sudip Roy', 'Matthias GallÃ©', 'Beyza Ermis', 'Ahmet ÃœstÃ¼n', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.08751.jpg', 'data': {'categories': ['#synthetic', '#data', '#low_resource', '#multimodal', '#training', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Aya-Vision, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Advancing Multimodal Language Models for Multilingual Mastery', 'desc': 'This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.'}, 'zh': {'title': 'å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„çªç ´æ€§è¿›å±•', 'desc': 'æ„å»ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é¢ä¸´è®¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å¯¹é½ã€é«˜è´¨é‡æŒ‡ä»¤æ•°æ®çš„æ•´ç†ï¼Œä»¥åŠåœ¨å¼•å…¥è§†è§‰åé¿å…æ–‡æœ¬èƒ½åŠ›çš„é€€åŒ–ã€‚åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œè¿™äº›å›°éš¾æ›´åŠ çªå‡ºï¼Œå› ä¸ºä¸åŒè¯­è¨€çš„å¤šæ¨¡æ€æ•°æ®éœ€æ±‚åŠ å‰§äº†æ•°æ®ç¨€ç¼ºï¼Œæœºå™¨ç¿»è¯‘å¸¸å¸¸æ‰­æ›²æ„ä¹‰ï¼Œå¹¶ä¸”ç¾éš¾æ€§é—å¿˜ç°è±¡æ›´åŠ æ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåˆæˆæ³¨é‡Šæ¡†æ¶ï¼Œç”¨äºæ•´ç†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¤šè¯­è¨€å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ï¼Œä½¿Aya Visionæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­å¯¹å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆè‡ªç„¶ã€ç¬¦åˆäººç±»åå¥½çš„å“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œæœ‰æ•ˆå‡è½»ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶å¢å¼ºå¤šæ¨¡æ€ç”Ÿæˆæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08712', 'title': 'NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance', 'url': 'https://huggingface.co/papers/2505.08712', 'abstract': "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.", 'score': 1, 'issue_id': 3752, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': 'a68cc40de86ece61', 'authors': ['Wenzhe Cai', 'Jiaqi Peng', 'Yuqiang Yang', 'Yujian Zhang', 'Meng Wei', 'Hanqing Wang', 'Yilun Chen', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.08712.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#diffusion', '#robotics', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'NavDP: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NavDP - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…. NavDP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Efficient Robot Navigation with Simulation-Driven Learning', 'desc': 'This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.'}, 'zh': {'title': 'æœºå™¨äººå¯¼èˆªçš„æ–°çªç ´ï¼šå¯¼èˆªæ‰©æ•£ç­–ç•¥', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¯¼èˆªæ‰©æ•£ç­–ç•¥ï¼ˆNavDPï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©æœºå™¨äººåœ¨åŠ¨æ€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚è¯¥æ–¹æ³•å®Œå…¨åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒï¼Œèƒ½å¤Ÿé›¶-shotè¿ç§»åˆ°ä¸åŒçš„å®é™…åº”ç”¨ä¸­ã€‚NavDPç»“åˆäº†åŸºäºæ‰©æ•£çš„è½¨è¿¹ç”Ÿæˆå’Œç”¨äºè½¨è¿¹é€‰æ‹©çš„è¯„ä»·å‡½æ•°ï¼Œåˆ©ç”¨å…±äº«ç­–ç•¥å˜æ¢å™¨ç¼–ç çš„å±€éƒ¨è§‚å¯Ÿä¿¡æ¯è¿›è¡Œæ¡ä»¶åŒ–ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿä¸­è·å–å…¨çƒç¯å¢ƒçš„ç‰¹æƒä¿¡æ¯ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜è´¨é‡çš„æ¼”ç¤ºæ•°æ®ï¼Œå¹¶åœ¨å¤šç§å®¤å†…å¤–ç¯å¢ƒä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¯¼èˆªæ€§èƒ½å’Œå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08445', 'title': 'Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency', 'url': 'https://huggingface.co/papers/2505.08445', 'abstract': 'Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.', 'score': 1, 'issue_id': 3756, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '7b4d8b3daa6efb6d', 'authors': ['Adel Ammar', 'Anis Koubaa', 'Omer Nacar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, P.O. Box 50927, Riyadh 11533, Saudi Arabia', 'Prince Sultan University, Rafha Street, P.O. Box 66833, Riyadh 11586, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.08445.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#healthcare', '#hallucinations', '#rag'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RAG: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG). Ğ˜ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Chroma Ğ¸ Faiss, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑˆĞµÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Optimizing RAG: Balancing Speed and Accuracy for Better AI Responses', 'desc': 'This paper explores how hyperparameters affect the performance of Retrieval-Augmented Generation (RAG) systems, which combine language generation with external information retrieval. It evaluates different vector stores, chunking strategies, and re-ranking methods to find the best balance between speed and accuracy. The findings indicate that while Chroma is faster, Faiss offers better retrieval precision, highlighting a trade-off between these two factors. The study also shows that with optimal hyperparameter tuning, RAG systems can achieve very high retrieval accuracy, which is crucial for applications like healthcare decision support.'}, 'zh': {'title': 'ä¼˜åŒ–RAGç³»ç»Ÿï¼Œå®ç°é«˜æ•ˆå‡†ç¡®çš„æ£€ç´¢', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡è¡¨ç°ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å¸¸å¸¸ä¼šå‡ºç°å¹»è§‰æˆ–ä¾èµ–è¿‡æ—¶çŸ¥è¯†ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å°†ç”Ÿæˆä¸å¤–éƒ¨æœç´¢ç»“åˆï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬åˆ†æäº†è¶…å‚æ•°å¦‚ä½•å½±å“RAGç³»ç»Ÿçš„é€Ÿåº¦å’Œè´¨é‡ï¼ŒåŒ…æ‹¬å‘é‡å­˜å‚¨ã€åˆ†å—ç­–ç•¥ã€äº¤å‰ç¼–ç é‡æ’åºå’Œæ¸©åº¦ç­‰ï¼Œå¹¶è¯„ä¼°äº†å…­ä¸ªæŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœå¸®åŠ©ä»ä¸šè€…åœ¨è°ƒæ•´RAGç³»ç»Ÿæ—¶å¹³è¡¡è®¡ç®—æˆæœ¬å’Œå‡†ç¡®æ€§ï¼Œä»¥å®ç°é€æ˜ä¸”æœ€æ–°çš„å“åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07416', 'title': 'ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation', 'url': 'https://huggingface.co/papers/2505.07416', 'abstract': 'Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP', 'score': 1, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '4f126d39b5476454', 'authors': ['Truc Mai-Thanh Nguyen', 'Dat Minh Nguyen', 'Son T. Luu', 'Kiet Van Nguyen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07416.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#multilingual', '#low_resource', '#open_source'], 'emoji': 'ğŸ‡»ğŸ‡³', 'ru': {'title': 'Ğ˜Ğ˜ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViMRHP Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ·Ğ¸Ğ»Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 4 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 2000 Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ² Ğ¸ 46000 Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction', 'desc': 'This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks.'}, 'zh': {'title': 'æå‡è¶Šå—è¯­è¯„è®ºæœ‰ç”¨æ€§çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ', 'desc': 'å¤šæ¨¡æ€è¯„è®ºæœ‰ç”¨æ€§é¢„æµ‹ï¼ˆMRHPï¼‰æ˜¯æ¨èç³»ç»Ÿä¸­çš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨ç”µå­å•†åŠ¡å¹³å°ä¸Šã€‚æœ¬æ–‡ä»‹ç»äº†ViMRHPï¼ˆè¶Šå—è¯­å¤šæ¨¡æ€è¯„è®ºæœ‰ç”¨æ€§é¢„æµ‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¶Šå—è¯­çš„MRHPä»»åŠ¡çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†å››ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬2000ä¸ªäº§å“å’Œ46000æ¡è¯„è®ºã€‚ä¸ºäº†ä¼˜åŒ–æ³¨é‡Šè¿‡ç¨‹ï¼Œæˆ‘ä»¬åˆ©ç”¨äººå·¥æ™ºèƒ½è¾…åŠ©æ³¨é‡Šè€…æ„å»ºViMRHPæ•°æ®é›†ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†æ³¨é‡Šæ—¶é—´å’Œæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ•°æ®è´¨é‡ã€‚å°½ç®¡AIç”Ÿæˆçš„æ³¨é‡Šåœ¨å¤æ‚ä»»åŠ¡ä¸­ä»å­˜åœ¨å±€é™æ€§ï¼Œä½†æˆ‘ä»¬é€šè¿‡è¯¦ç»†çš„æ€§èƒ½åˆ†æè¿›ä¸€æ­¥æ¢è®¨äº†è¿™äº›é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21853', 'title': 'A Survey of Interactive Generative Video', 'url': 'https://huggingface.co/papers/2504.21853', 'abstract': 'Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.', 'score': 40, 'issue_id': 3550, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '4e975f915f638955', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Hao Chen', 'Xihui Liu'], 'affiliations': ['Kuaishou Technology, Shenzhen, China', 'The Hong Kong University of Science and Technology (HKUST), Hong Kong', 'The University of Hong Kong, Pok Fu Lam, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.21853.jpg', 'data': {'categories': ['#robotics', '#interpretability', '#video', '#optimization', '#survey', '#agents', '#games', '#multimodal'], 'emoji': 'ğŸ®', 'ru': {'title': 'IGV: Ğ‘ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ’Ğ¸Ğ´ĞµĞ¾ (IGV), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ IGV Ğ² Ñ‚Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¸Ğ³Ñ€Ñ‹, Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ IGV Ğ½Ğ° Ğ¿ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ, ĞŸĞ°Ğ¼ÑÑ‚ÑŒ, Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¸ Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° IGV.'}, 'en': {'title': 'Empowering Interactive Experiences with Generative Video', 'desc': 'Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field.'}, 'zh': {'title': 'æ¨åŠ¨äº’åŠ¨ç”Ÿæˆè§†é¢‘æŠ€æœ¯çš„æœªæ¥å‘å±•', 'desc': 'äº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰æ˜¯ä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œæ—¨åœ¨æ»¡è¶³å¯¹é«˜è´¨é‡äº’åŠ¨è§†é¢‘å†…å®¹çš„éœ€æ±‚ã€‚æœ¬æ–‡å°†IGVå®šä¹‰ä¸ºä¸€ç§ç»“åˆç”Ÿæˆèƒ½åŠ›å’Œäº’åŠ¨ç‰¹æ€§çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿé€šè¿‡æ§åˆ¶ä¿¡å·å’Œåé¦ˆå®ç°ç”¨æˆ·å‚ä¸ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†IGVåœ¨æ¸¸æˆã€å…·èº«äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ç­‰ä¸‰ä¸ªä¸»è¦é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå°†ç†æƒ³çš„IGVç³»ç»Ÿåˆ†è§£ä¸ºç”Ÿæˆã€æ§åˆ¶ã€è®°å¿†ã€åŠ¨æ€å’Œæ™ºèƒ½äº”ä¸ªæ¨¡å—ã€‚é€šè¿‡ç³»ç»Ÿåˆ†ææŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼Œæœ¬æ–‡æ—¨åœ¨æ¨åŠ¨IGVé¢†åŸŸçš„ç ”ç©¶ä¸å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00662', 'title': 'DeepCritic: Deliberate Critique with Large Language Models', 'url': 'https://huggingface.co/papers/2505.00662', 'abstract': 'As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.', 'score': 37, 'issue_id': 3549, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '259dddc97137d27a', 'authors': ['Wenkai Yang', 'Jingwen Chen', 'Yankai Lin', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00662.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#reasoning', '#rlhf', '#math', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Qwen2.5-72B-Instruct Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen2.5-7B-Instruct Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ LLM-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions', 'desc': "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ‰¹è¯„èƒ½åŠ›', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæä¾›å‡†ç¡®çš„åé¦ˆå’Œå¯æ‰©å±•çš„ç›‘ç£å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºLLMsåœ¨æ•°å­¦æ‰¹è¯„æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„é€æ­¥æ‰¹è¯„ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«é”™è¯¯å¹¶æä¾›æ·±å…¥çš„åé¦ˆï¼Œå¸®åŠ©ç”Ÿæˆæ¨¡å‹çº æ­£é”™è¯¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ”¹è¿›åçš„æ‰¹è¯„æ¨¡å‹åœ¨é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMæ‰¹è¯„è€…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00703', 'title': 'T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT', 'url': 'https://huggingface.co/papers/2505.00703', 'abstract': 'Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1', 'score': 28, 'issue_id': 3548, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': 'ca564761ff71d15e', 'authors': ['Dongzhi Jiang', 'Ziyu Guo', 'Renrui Zhang', 'Zhuofan Zong', 'Hao Li', 'Le Zhuo', 'Shilin Yan', 'Pheng-Ann Heng', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'CUHK MiuLar Lab', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.00703.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T2I-R1 - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BiCoT-GRPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Janus-Pro Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… T2I-CompBench Ğ¸ WISE.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Enhanced Reasoning', 'desc': 'This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests.'}, 'zh': {'title': 'åŒå±‚æ€ç»´é“¾æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹T2I-R1ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’ŒåŒå±‚æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ€ç»´é“¾ï¼šè¯­ä¹‰å±‚æ¬¡çš„æ€ç»´é“¾ç”¨äºé«˜å±‚æ¬¡çš„æç¤ºè§„åˆ’ï¼Œä»¤ç”Ÿæˆè¿‡ç¨‹æ›´å…·é€»è¾‘æ€§ï¼›è€Œä»¤ç‰Œå±‚æ¬¡çš„æ€ç»´é“¾åˆ™ç”¨äºåœ¨é€å—ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œä½å±‚æ¬¡çš„åƒç´ å¤„ç†ã€‚é€šè¿‡å¼•å…¥BiCoT-GRPOï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨åŒä¸€è®­ç»ƒæ­¥éª¤ä¸­ä¼˜åŒ–è¿™ä¸¤ç§æ€ç»´é“¾ï¼Œä»è€Œæå‡ç”Ÿæˆæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2I-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00497', 'title': 'KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution', 'url': 'https://huggingface.co/papers/2505.00497', 'abstract': 'Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.', 'score': 9, 'issue_id': 3554, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': 'dc645b5c7e476cea', 'authors': ['Antoni Bigata', 'Rodrigo Mira', 'Stella Bounareli', 'MichaÅ‚ StypuÅ‚kowski', 'Konstantinos Vougioukas', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London', 'University of WrocÅ‚aw'], 'pdf_title_img': 'assets/pdf/title_img/2505.00497.jpg', 'data': {'categories': ['#cv', '#leakage', '#video', '#open_source', '#architecture'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'KeySync: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'KeySync - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹ Ğ»Ğ¸Ñ†Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. KeySync Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ LipLeak. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ÑƒĞ±Ğ»ÑĞ¶.'}, 'en': {'title': 'KeySync: Mastering Lip Synchronization with Precision', 'desc': 'This paper introduces KeySync, a two-stage framework designed to improve lip synchronization in videos by aligning lip movements with new audio inputs. It addresses common challenges in talking head generation, such as maintaining temporal consistency and managing expression leakage and facial occlusions. KeySync employs a unique masking strategy to effectively tackle these issues, resulting in enhanced visual quality and reduced leakage as measured by a new metric called LipLeak. The framework demonstrates state-of-the-art performance in lip reconstruction and cross-synchronization, validated through various ablation studies.'}, 'zh': {'title': 'KeySyncï¼šæå‡å”‡éƒ¨åŒæ­¥çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKeySyncçš„åŒé˜¶æ®µæ¡†æ¶ï¼Œç”¨äºè§£å†³å”‡éƒ¨åŒæ­¥ä¸­çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é®ç½©ç­–ç•¥ï¼Œè§£å†³äº†è¾“å…¥è§†é¢‘ä¸­çš„è¡¨æƒ…æ³„æ¼å’Œé¢éƒ¨é®æŒ¡ç­‰æ–°æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKeySyncåœ¨å”‡éƒ¨é‡å»ºå’Œäº¤å‰åŒæ­¥æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰è´¨é‡å¹¶å‡å°‘äº†è¡¨æƒ…æ³„æ¼ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¤šé¡¹æ¶ˆèç ”ç©¶éªŒè¯äº†æ–°é®ç½©æ–¹æ³•åœ¨å¤„ç†é®æŒ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00234', 'title': 'Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks', 'url': 'https://huggingface.co/papers/2505.00234', 'abstract': 'Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.', 'score': 9, 'issue_id': 3563, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '9e975cfc8ecf9dea', 'authors': ['Vishnu Sarukkai', 'Zhiqiang Xie', 'Kayvon Fatahalian'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.00234.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#rl', '#agents', '#optimization', '#benchmark', '#games'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹.'}, 'en': {'title': 'Learning from Success: Automating Improvement in LLM Agents', 'desc': 'This paper explores a new approach for improving Large Language Model (LLM) agents in sequential decision-making tasks without relying on extensive task-specific knowledge engineering. Instead of manually tuning prompts or creating curated examples, the authors propose that LLM agents can learn from their own successful experiences by building a database of self-generated examples. The study shows that simply accumulating successful task trajectories can significantly enhance performance on various benchmarks. Additionally, the paper introduces methods for selecting high-performing examples from this database, leading to even better results that rival more complex, knowledge-intensive methods.'}, 'zh': {'title': 'è‡ªåŠ¨å­¦ä¹ æå‡LLMä»£ç†æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„æˆåŠŸç»éªŒæ¥è‡ªåŠ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨é¡ºåºå†³ç­–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè€Œä¸æ˜¯ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†å·¥ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç®€å•åœ°ç§¯ç´¯æˆåŠŸçš„è½¨è¿¹å¯ä»¥æ˜¾è‘—æé«˜åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§æ‰©å±•æ–¹æ³•ï¼šé€šè¿‡åŸºäºç§ç¾¤çš„è®­ç»ƒé€‰æ‹©é«˜æ•ˆçš„ç¤ºä¾‹é›†åˆï¼Œä»¥åŠæ ¹æ®å®è¯æ•ˆç”¨ä¿ç•™ä¸ªåˆ«è½¨è¿¹ã€‚è¿™äº›æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†è‡ªåŠ¨è½¨è¿¹æ•°æ®åº“æ„å»ºä½œä¸ºçŸ¥è¯†å·¥ç¨‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18715', 'title': 'Spatial Speech Translation: Translating Across Space With Binaural\n  Hearables', 'url': 'https://huggingface.co/papers/2504.18715', 'abstract': "Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.", 'score': 7, 'issue_id': 3566, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 25', 'zh': '4æœˆ25æ—¥'}, 'hash': '979ba773cd3e90a0', 'authors': ['Tuochao Chen', 'Qirui Wang', 'Runlin He', 'Shyam Gollakota'], 'affiliations': ['Paul G. Allen School, University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.18715.jpg', 'data': {'categories': ['#audio', '#multilingual', '#machine_translation'], 'emoji': 'ğŸ§', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ€ĞµÑ‡Ğ¸: ÑĞ»Ñ‹ÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€ Ğ½Ğ° ÑĞ²Ğ¾ĞµĞ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ² Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ»ĞµĞ¿Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. ĞŸÑ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ… Ğ¸ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ BLEU Ğ´Ğ¾ 22.01 Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Transforming Speech Translation with Spatial Awareness', 'desc': "This paper presents a new approach to spatial speech translation using hearables, which are devices that can translate spoken language in real-time while preserving the spatial characteristics of the speakers. The authors address key challenges such as separating overlapping voices, accurately locating speakers, and ensuring that translations are expressive and timely, all while running efficiently on Apple M2 hardware. Their prototype demonstrates significant improvements over existing models, achieving a BLEU score of 22.01 in noisy environments, indicating high translation quality. User studies validate the system's ability to maintain the spatial cues of translated speech, marking a significant advancement in integrating spatial awareness into speech translation technology."}, 'zh': {'title': 'ç©ºé—´è¯­éŸ³ç¿»è¯‘ï¼šè®©äº¤æµæ— éšœç¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç©ºé—´è¯­éŸ³ç¿»è¯‘æ¦‚å¿µï¼Œæ—¨åœ¨é€šè¿‡è€³æœºå°†ç¯å¢ƒä¸­è¯´è¯è€…çš„è¯­è¨€å®æ—¶ç¿»è¯‘ä¸ºä½©æˆ´è€…çš„æ¯è¯­ï¼ŒåŒæ—¶ä¿ç•™æ¯ä½è¯´è¯è€…çš„æ–¹å‘å’Œç‹¬ç‰¹å£°éŸ³ç‰¹å¾ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶è§£å†³äº†å¤šä¸ªæŠ€æœ¯æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç›²æºåˆ†ç¦»ã€å®šä½ã€å®æ—¶è¡¨è¾¾ç¿»è¯‘å’ŒåŒè€³æ¸²æŸ“ï¼Œä»¥ç¡®ä¿ç¿»è¯‘éŸ³é¢‘ä¸­çš„è¯´è¯è€…æ–¹å‘å¾—ä»¥ä¿ç•™ã€‚é€šè¿‡åŸå‹åŒè€³è€³æœºçš„æ¦‚å¿µéªŒè¯è¯„ä¼°ï¼Œç ”ç©¶è¡¨æ˜è¯¥ç³»ç»Ÿåœ¨å¼ºå¹²æ‰°ç¯å¢ƒä¸­ä»èƒ½å®ç°é«˜è¾¾22.01çš„BLEUåˆ†æ•°ï¼Œä¼˜äºç°æœ‰æ¨¡å‹ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥ç¡®è®¤äº†è¯¥ç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒä¸­ç©ºé—´æ¸²æŸ“ç¿»è¯‘è¯­éŸ³çš„æœ‰æ•ˆæ€§ï¼Œæ ‡å¿—ç€å°†ç©ºé—´æ„ŸçŸ¥æ•´åˆåˆ°è¯­éŸ³ç¿»è¯‘ä¸­çš„ç¬¬ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21659', 'title': 'AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization', 'url': 'https://huggingface.co/papers/2504.21659', 'abstract': 'Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1', 'score': 6, 'issue_id': 3549, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '6487e5a67faf07a5', 'authors': ['Haotian Luo', 'Haiying He', 'Yibo Wang', 'Jinluan Yang', 'Rui Liu', 'Naiqiang Tan', 'Xiaochun Cao', 'Dacheng Tao', 'Li Shen'], 'affiliations': ['China Agricultural University', 'Didichuxing Co. Ltd', 'Nanyang Technological University', 'Sun Yat-sen University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21659.jpg', 'data': {'categories': ['#reasoning', '#inference', '#math', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ±Ğ¸-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ°ÑÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%.'}, 'en': {'title': 'Adaptive Reasoning for Efficient Inference in Complex Tasks', 'desc': 'This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¨ç†ï¼Œæå‡æ•ˆç‡ï¼', 'desc': 'æœ€è¿‘ï¼Œé•¿æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å¯¼è‡´æ¨ç†å¼€é”€å¤§ï¼Œå› æ­¤æ•ˆç‡æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œä½¿ç”¨é•¿é“¾æ¨ç†ï¼ˆLong-CoTï¼‰çš„å¥½å¤„åœ¨ä¸åŒé—®é¢˜ä¸Šå·®å¼‚å¾ˆå¤§ï¼šæœ‰äº›é—®é¢˜éœ€è¦å¤æ‚æ¨ç†ï¼Œè€Œå…¶ä»–é—®é¢˜åˆ™æ²¡æœ‰æ”¹å–„ï¼Œç”šè‡³å‡†ç¡®ç‡ä¸‹é™ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºè‡ªé€‚åº”æ¨ç†ç­–ç•¥ï¼Œæ ¹æ®è¾“å…¥è°ƒæ•´æ¨ç†æ·±åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡æ··åˆé•¿çŸ­é“¾æ¨ç†æ¨¡å‹å’ŒåŒå±‚åå¥½è®­ç»ƒï¼Œæ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.19394', 'title': 'LLMs for Engineering: Teaching Models to Design High Powered Rockets', 'url': 'https://huggingface.co/papers/2504.19394', 'abstract': "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.", 'score': 6, 'issue_id': 3558, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 27', 'zh': '4æœˆ27æ—¥'}, 'hash': '5b65f8f309063f13', 'authors': ['Toby Simonds'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.19394.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#agi', '#benchmark', '#games', '#reasoning'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ˜Ğ˜ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°ĞºĞµÑ‚', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºĞµÑ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° RocketBench. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ°Ğ´ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ÑÑ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Rocket Design Challenges', 'desc': 'This paper investigates the use of Large Language Models (LLMs) in the field of physical engineering, specifically in high-powered rocketry design. It introduces RocketBench, a benchmark that connects LLMs to advanced rocket simulations, and evaluates their performance on design tasks like optimizing target altitude and achieving precision landings. The results indicate that while LLMs possess a solid foundation in engineering knowledge, they struggle to improve their designs based on simulation feedback, often falling short of human capabilities. However, by incorporating reinforcement learning, a 7B parameter model surpasses both state-of-the-art models and human experts, highlighting the potential of RL-enhanced LLMs in complex engineering optimization tasks.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›ç«ç®­è®¾è®¡ï¼Œè¶…è¶Šäººç±»ä¸“å®¶ï¼', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç‰©ç†å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨ä»ç„¶ä¸å¤Ÿæ·±å…¥ã€‚æœ¬æ–‡é€šè¿‡RocketBenchåŸºå‡†è¯„ä¼°LLMsåœ¨é«˜åŠŸç‡ç«ç®­è®¾è®¡ä¸­çš„èƒ½åŠ›ï¼Œè¿æ¥LLMsä¸é«˜ä¿çœŸç«ç®­æ¨¡æ‹Ÿã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸¤ä¸ªå¤æ‚çš„è®¾è®¡ä»»åŠ¡ï¼šç›®æ ‡é«˜åº¦ä¼˜åŒ–å’Œç²¾ç¡®ç€é™†æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æœ€å…ˆè¿›çš„LLMså±•ç°å‡ºå¼ºå¤§çš„å·¥ç¨‹çŸ¥è¯†ï¼Œä½†åœ¨æ ¹æ®æ¨¡æ‹Ÿç»“æœè¿­ä»£è®¾è®¡æ—¶è¡¨ç°ä¸ä½³ï¼Œæœ€ç»ˆæœªèƒ½è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ï¼›ç„¶è€Œï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºåï¼Œ7Bå‚æ•°æ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰åŸºç¡€æ¨¡å‹å’Œäººç±»ä¸“å®¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18983', 'title': 'MediAug: Exploring Visual Augmentation in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.18983', 'abstract': 'Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.', 'score': 5, 'issue_id': 3554, 'pub_date': '2025-04-26', 'pub_date_card': {'ru': '26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 26', 'zh': '4æœˆ26æ—¥'}, 'hash': '13143b6fb9ae9216', 'authors': ['Xuyin Qi', 'Zeyu Zhang', 'Canxuan Gang', 'Hao Zhang', 'Lei Zhang', 'Zhiwei Zhang', 'Yang Zhao'], 'affiliations': ['AIML', 'ANU', 'La Trobe', 'PSU', 'UCAS', 'UNSW'], 'pdf_title_img': 'assets/pdf/title_img/2504.18983.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#healthcare', '#synthetic', '#training'], 'emoji': 'ğŸ©º', 'ru': {'title': 'MediAug: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MediAug - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¸Ñ… Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ¸ Ğ³Ğ»Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (ResNet-50), Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (ViT-B). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Enhancing Medical Imaging with Advanced Data Augmentation', 'desc': 'This paper addresses the challenges of data augmentation in medical imaging, particularly the domain gap between natural images and medical scans. It introduces MediAug, a benchmark framework that evaluates six mix-based augmentation methods across different neural network architectures. The study finds that MixUp and SnapMix significantly enhance classification accuracy for brain tumors and eye diseases, respectively. By providing a systematic evaluation, the paper clarifies the effectiveness of various augmentation strategies in improving medical image analysis.'}, 'zh': {'title': 'åŒ»å­¦å½±åƒæ•°æ®å¢å¼ºçš„æ–°çªç ´', 'desc': 'æ•°æ®å¢å¼ºåœ¨åŒ»å­¦å½±åƒä¸­å¯¹äºæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€ç—…å˜æ£€æµ‹å’Œå™¨å®˜åˆ†å‰²è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼ŒåŒ»å­¦å½±åƒä¸è‡ªç„¶ç…§ç‰‡ä¹‹é—´çš„æ˜¾è‘—é¢†åŸŸå·®è·å¯èƒ½ä¼šæ‰­æ›²å…³é”®çš„ç–¾ç—…ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„å¢å¼ºç ”ç©¶å¾€å¾€å±€é™äºå•ä¸€ä»»åŠ¡æˆ–æ¶æ„ï¼Œå¯¼è‡´å…ˆè¿›çš„æ··åˆç­–ç•¥çš„ä¼˜åŠ¿ä¸æ˜ç¡®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ•´åˆäº†å…­ç§åŸºäºæ··åˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå¹¶åœ¨è„‘è‚¿ç˜¤MRIå’Œçœ¼ç—…è§†ç½‘è†œå›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.20605', 'title': 'TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models', 'url': 'https://huggingface.co/papers/2504.20605', 'abstract': 'Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.', 'score': 4, 'issue_id': 3554, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': '9637fef0c8d474ed', 'authors': ['Mihai Nadas', 'Laura Diosan', 'Andrei Piscoran', 'Andreea Tomescu'], 'affiliations': ['Babes-Bolyai University', 'KlusAI Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.20605.jpg', 'data': {'categories': ['#story_generation', '#ethics', '#multimodal', '#dataset', '#benchmark', '#alignment', '#open_source'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ TF1-EN-3M - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°ÑĞµĞ½, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¾ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑˆĞµÑÑ‚Ğ¸ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€. ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPT Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚, ĞºĞ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ´ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'Empowering Moral Storytelling with TF1-EN-3M', 'desc': 'This paper introduces TF1-EN-3M, a novel dataset containing three million English fables designed to teach moral lessons, generated by smaller instruction-tuned models. The stories are structured using a six-slot framework that ensures each narrative includes essential elements like characters, traits, and morals. A unique evaluation system combines assessments from a GPT-based critic and various metrics to ensure quality and diversity in the generated fables. The dataset and associated tools are made publicly available, promoting further research in areas like narrative intelligence and ethical AI development.'}, 'zh': {'title': 'é“å¾·æ•…äº‹ç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†TF1-EN-3Mï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«ä¸‰ç™¾ä¸‡ä¸ªè‹±è¯­å¯“è¨€çš„å¼€æ”¾æ•°æ®é›†ï¼Œä¸“é—¨ç”±æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ç”Ÿæˆã€‚æ¯ä¸ªæ•…äº‹éµå¾ªå…­ä¸ªéƒ¨åˆ†çš„ç»“æ„ï¼ŒåŒ…æ‹¬è§’è‰²ã€ç‰¹å¾ã€èƒŒæ™¯ã€å†²çªã€è§£å†³æ–¹æ¡ˆå’Œé“å¾·ï¼Œç¡®ä¿äº†æ•…äº‹çš„è¿è´¯æ€§å’Œä¸»é¢˜çš„å¤šæ ·æ€§ã€‚è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ··åˆè¯„ä¼°æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºGPTçš„è¯„åˆ†å’Œæ— å‚è€ƒçš„å¤šæ ·æ€§ä¸å¯è¯»æ€§æŒ‡æ ‡ã€‚TF1-EN-3Mä¸ºé“å¾·æ•…äº‹çš„ç”Ÿæˆå’Œæ•™è‚²AIçš„ç ”ç©¶æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œè¡¨æ˜å¤§è§„æ¨¡çš„é“å¾·å™äº‹ä¸å†éœ€è¦ä¸“æœ‰çš„å¤§å‹æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.20406', 'title': 'Skill Discovery for Software Scripting Automation via Offline\n  Simulations with LLMs', 'url': 'https://huggingface.co/papers/2504.20406', 'abstract': "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.", 'score': 3, 'issue_id': 3570, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': '995b07a7273b51fd', 'authors': ['Paiheng Xu', 'Gang Wu', 'Xiang Chen', 'Tong Yu', 'Chang Xiao', 'Franck Dernoncourt', 'Tianyi Zhou', 'Wei Ai', 'Viswanathan Swaminathan'], 'affiliations': ['Adobe Research', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.20406.jpg', 'data': {'categories': ['#agents', '#dataset', '#alignment', '#graphs', '#games', '#architecture', '#data'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: LLM ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ² Ğ¿Ğ¾ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ñƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (GNN) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ API. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Adobe Illustrator Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Users with Automated Scripting through AI', 'desc': 'This paper presents a framework that helps users automate tasks in software without needing deep programming skills. It uses Large Language Models (LLMs) to generate verified scripts by simulating tasks and refining them based on execution feedback. The framework includes a Graph Neural Network (GNN) to explore API synergies, which helps create a diverse set of useful scripts. Experiments show that this approach improves automation success rates and reduces costs compared to traditional methods.'}, 'zh': {'title': 'åˆ©ç”¨LLMæå‡è½¯ä»¶è„šæœ¬è‡ªåŠ¨åŒ–çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¦»çº¿æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…¬å¼€çš„è„šæœ¬æŒ‡å—ï¼Œåˆ›å»ºä¸€ä¸ªè½¯ä»¶ç‰¹å®šçš„æŠ€èƒ½é›†ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šä»»åŠ¡åˆ›å»ºå’ŒæŠ€èƒ½ç”Ÿæˆï¼Œé€šè¿‡æ‰§è¡Œåé¦ˆæ¥ä¼˜åŒ–å’ŒéªŒè¯è„šæœ¬ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„é“¾æ¥é¢„æµ‹æ¨¡å‹ï¼Œä»¥æ•æ‰APIä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»è€Œç”Ÿæˆå¤šæ ·åŒ–çš„æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªåŠ¨åŒ–æˆåŠŸç‡ã€å“åº”æ—¶é—´å’Œè¿è¡Œæ—¶æˆæœ¬æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä»£ç ç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00534', 'title': 'A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic', 'url': 'https://huggingface.co/papers/2505.00534', 'abstract': 'Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.', 'score': 2, 'issue_id': 3560, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': 'd411bc0f9d34adf4', 'authors': ['Muhammad Imran Zaman', 'Usama Ijaz Bajwa', 'Gulshan Saleem', 'Rana Hammad Raza'], 'affiliations': ['Department of Computer Science, COMSATS University Islamabad, Lahore Campus, Lahore, Pakistan', 'Pakistan Navy Engineering College, National University of Sciences and Technology (NUST), Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2505.00534.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#optimization', '#video', '#transfer_learning'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Mask R-CNN Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ResNet-152 Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞ½ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AI City Challenge, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² IDF1 0.8289.'}, 'en': {'title': 'Revolutionizing Traffic Monitoring with Deep Learning Tracking', 'desc': 'This paper presents a deep learning framework for Multi-Object Multi-Camera Tracking (MO-MCT) aimed at improving traffic monitoring in Intelligent Transportation Systems. The framework utilizes Mask R-CNN for detecting vehicles and applies Non-Maximum Suppression (NMS) to manage overlapping detections. It incorporates transfer learning for vehicle re-identification, allowing for the tracking of vehicles across different cameras despite challenges like occlusions and varying lighting conditions. The system is evaluated on a large dataset and shows strong performance metrics, indicating its potential for real-world traffic applications.'}, 'zh': {'title': 'æ™ºèƒ½äº¤é€šä¸­çš„é«˜æ•ˆå¤šæ‘„åƒå¤´è·Ÿè¸ªè§£å†³æ–¹æ¡ˆ', 'desc': 'éšç€ç½‘ç»œæ‘„åƒå¤´æ•°é‡çš„å¢åŠ ï¼Œè§†è§‰ä¼ æ„Ÿå™¨åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢å¼ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šç›®æ ‡å¤šæ‘„åƒå¤´è·Ÿè¸ªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸå¸‚äº¤é€šåœºæ™¯ä¸­æ‰‹åŠ¨ç›®æ ‡è·Ÿè¸ªå’ŒåŒ¹é…çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Mask R-CNNè¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶é€šè¿‡éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰é€‰æ‹©é‡å æ£€æµ‹ä¸­çš„ç›®æ ‡ã€‚é€šè¿‡è¿ç§»å­¦ä¹ å®ç°è½¦è¾†çš„é‡æ–°è¯†åˆ«ï¼Œç»“åˆé€‚å½“çš„æŸå¤±å‡½æ•°å’Œè·ç¦»åº¦é‡ï¼Œæœ€ç»ˆåœ¨AI City Challengeæ•°æ®é›†ä¸Šå–å¾—äº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02707', 'title': 'Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play', 'url': 'https://huggingface.co/papers/2505.02707', 'abstract': "A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.", 'score': 56, 'issue_id': 3604, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '1cf06df6011df348', 'authors': ['Yemin Shi', 'Yu Shu', 'Siwei Dong', 'Guangyi Liu', 'Jaward Sesay', 'Jingwen Li', 'Zhiting Hu'], 'affiliations': ['MBZUAI', 'Maitrix.org', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.02707.jpg', 'data': {'categories': ['#open_source', '#agi', '#multimodal', '#audio', '#architecture', '#agents', '#reasoning', '#multilingual', '#machine_translation'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Voila: Ğ˜Ğ˜-ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸Ğº Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Voila - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²ĞµÑÑ‚Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Voila Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ²ÑĞµĞ³Ğ¾ 195 Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¿Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑÑĞ¼Ğ¿Ğ»Ğ°Ğ¼.'}, 'en': {'title': 'Voila: Revolutionizing Voice AI for Natural Conversations', 'desc': 'This paper presents Voila, an advanced voice AI agent designed for seamless interaction in daily life. Voila utilizes a novel end-to-end architecture that allows for real-time, emotionally expressive conversations, achieving a response time of just 195 milliseconds. It combines large language models with sophisticated acoustic modeling to generate natural and persona-aware voice outputs, enabling users to customize voice characteristics easily. Additionally, Voila supports various voice-based applications, including automatic speech recognition and multilingual translation, and is fully open-sourced to promote further research in human-machine communication.'}, 'zh': {'title': 'Voilaï¼šå®ç°è‡ªç„¶æƒ…æ„Ÿäº’åŠ¨çš„è¯­éŸ³AIä»£ç†', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Voilaï¼Œä¸€ä¸ªæ–°å‹çš„è¯­éŸ³AIä»£ç†ï¼Œæ—¨åœ¨å®ç°ä¸äººç±»çš„è‡ªç„¶äº’åŠ¨ã€‚Voilaé‡‡ç”¨ç«¯åˆ°ç«¯æ¶æ„ï¼Œæ”¯æŒå…¨åŒå·¥ã€ä½å»¶è¿Ÿçš„å¯¹è¯ï¼Œèƒ½å¤Ÿæ•æ‰è¯­éŸ³ä¸­çš„æƒ…æ„Ÿå’Œç»†å¾®å·®åˆ«ã€‚å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºå¤§çš„å£°å­¦å»ºæ¨¡ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç®€å•çš„æ–‡æœ¬æŒ‡ä»¤å®šä¹‰è¯´è¯è€…çš„èº«ä»½å’Œè¯­è°ƒã€‚Voilaè¿˜æ”¯æŒè¶…è¿‡ä¸€ç™¾ä¸‡ç§é¢„æ„å»ºçš„å£°éŸ³ï¼Œå¹¶èƒ½ä»çŸ­è‡³10ç§’çš„éŸ³é¢‘æ ·æœ¬ä¸­é«˜æ•ˆå®šåˆ¶æ–°å£°éŸ³ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02387', 'title': 'RM-R1: Reward Modeling as Reasoning', 'url': 'https://huggingface.co/papers/2505.02387', 'abstract': "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.", 'score': 43, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '6cac1dc82fc6bcdc', 'authors': ['Xiusi Chen', 'Gaotang Li', 'Ziqi Wang', 'Bowen Jin', 'Cheng Qian', 'Yu Wang', 'Hongru Wang', 'Yu Zhang', 'Denghui Zhang', 'Tong Zhang', 'Hanghang Tong', 'Heng Ji'], 'affiliations': ['Stevens Institute of Technology', 'Texas A&M University', 'University of California, San Diego', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02387.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#training', '#benchmark', '#rlhf', '#alignment', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ - Reasoning Reward Models (ReasRMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ReasRM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñƒ Ğ´Ğ¾ 13.8%.'}, 'en': {'title': 'Enhancing Reward Models with Reasoning for Better Interpretability', 'desc': 'This paper discusses the importance of reward modeling in aligning large language models (LLMs) with human preferences using reinforcement learning from human feedback (RLHF). It highlights the limitations of existing reward models, which often lack interpretability and struggle to incorporate natural language critiques. The authors introduce Reasoning Reward Models (ReasRMs), which enhance interpretability by treating reward modeling as a reasoning task. Their approach includes a two-stage training process that improves performance and achieves state-of-the-art results on various benchmarks, outperforming larger models.'}, 'zh': {'title': 'æ¨ç†å¥–åŠ±æ¨¡å‹ï¼šæå‡å¯è§£é‡Šæ€§çš„å…³é”®', 'desc': 'å¥–åŠ±å»ºæ¨¡åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯é€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¾€å¾€ç”Ÿæˆä¸é€æ˜çš„æ ‡é‡åˆ†æ•°æˆ–ç›´æ¥é¢„æµ‹åå¥½çš„ç­”æ¡ˆï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ¨ç†ä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡é«˜è´¨é‡æ¨ç†é“¾çš„è’¸é¦å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.20752', 'title': 'Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers', 'url': 'https://huggingface.co/papers/2504.20752', 'abstract': 'Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio phi_r of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing phi_r drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.', 'score': 41, 'issue_id': 3604, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 29', 'zh': '4æœˆ29æ—¥'}, 'hash': '46858ed83065e6ae', 'authors': ['Roman Abramov', 'Felix Steinbauer', 'Gjergji Kasneci'], 'affiliations': ['School of Computation, Information and Technology, Technical University of Munich, Munich, Germany', 'School of Social Sciences and Technology, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.20752.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#dataset', '#reasoning', '#synthetic', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ĞµÑ€ÑŒ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¸Ğ»Ğ¸Ñ‚ÑŒ ÑÑ…ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 95-100% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ 2WikiMultiHopQA, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Unlocking Reasoning in Transformers with Grokking and Data Augmentation', 'desc': "This paper explores how Transformers, a type of neural network, can improve their ability to reason through complex factual information. It introduces the concept of 'grokking' to real-world data by enhancing knowledge graphs with synthetic data, which helps the model learn logical patterns instead of just memorizing facts. Interestingly, the study finds that even incorrect synthetic data can aid in developing reasoning capabilities by emphasizing relational structures. The results show significant improvements in multi-hop reasoning tasks, achieving high accuracy and suggesting that this method can enhance the reasoning abilities of large language models."}, 'zh': {'title': 'åˆ©ç”¨åˆæˆæ•°æ®æå‡å¤šæ­¥æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å˜æ¢å™¨åœ¨å¤šæ­¥äº‹å®æ¨ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®ä¸–ç•ŒçŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬é¦–æ¬¡å°†grokkingæ‰©å±•åˆ°çœŸå®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œå¹¶é€šè¿‡å¢å¼ºçŸ¥è¯†å›¾è°±æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯äº‹å®ä¸æ­£ç¡®çš„åˆæˆæ•°æ®ä¹Ÿèƒ½å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†95-100%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02222', 'title': 'Practical Efficiency of Muon for Pretraining', 'url': 'https://huggingface.co/papers/2505.02222', 'abstract': 'We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.', 'score': 26, 'issue_id': 3608, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': '3b2cca9779803131', 'authors': ['Essential AI', ':', 'Ishaan Shah', 'Anthony M. Polloreno', 'Karl Stratos', 'Philip Monk', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Ashish Tanwer', 'Darsh J Shah', 'Khoi Nguyen', 'Kurt Smith', 'Michael Callahan', 'Michael Pust', 'Mohit Parmar', 'Peter Rushton', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Somanshu Singla', 'Tim Romanski', 'Yash Vanjani', 'Ashish Vaswani'], 'affiliations': ['Essential AI, San Francisco, CA'], 'pdf_title_img': 'assets/pdf/title_img/2505.02222.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Muon: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Muon, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ AdamW Ğ¿Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Muon ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Muon Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (muP) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Muon: A New Era of Efficient Optimization in Machine Learning', 'desc': 'This paper introduces Muon, a second-order optimizer that improves upon AdamW by expanding the Pareto frontier in terms of compute-time efficiency. Muon demonstrates superior data efficiency at large batch sizes, even exceeding the critical batch size, while maintaining computational efficiency for cost-effective training. The authors also explore the integration of Muon with the maximal update parameterization (muP) to enhance hyperparameter transfer efficiency. Their extensive experiments, involving models with up to four billion parameters, validate the effectiveness of Muon across various data distributions and architectures.'}, 'zh': {'title': 'Muonä¼˜åŒ–å™¨ï¼šè¶…è¶ŠAdamWçš„é«˜æ•ˆè®­ç»ƒ', 'desc': 'æœ¬æ–‡å±•ç¤ºäº†Muonä½œä¸ºä¸€ç§äºŒé˜¶ä¼˜åŒ–å™¨çš„ç®€å•å®ç°ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—æ—¶é—´çš„æƒè¡¡ä¸Šæ˜¾è‘—æ‰©å±•Paretoå‰æ²¿ï¼Œç›¸æ¯”äºAdamWæ›´å…·ä¼˜åŠ¿ã€‚ç ”ç©¶å‘ç°ï¼ŒMuonåœ¨å¤§æ‰¹é‡è®­ç»ƒä¸­ä¿æŒæ•°æ®æ•ˆç‡ï¼Œè¶…è¶Šäº†æ‰€è°“çš„ä¸´ç•Œæ‰¹é‡å¤§å°ï¼ŒåŒæ—¶ä»ç„¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä»è€Œå®ç°æ›´ç»æµçš„è®­ç»ƒã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†Muonä¸æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆmuPï¼‰çš„ç»“åˆï¼Œä»¥å®ç°é«˜æ•ˆçš„è¶…å‚æ•°è½¬ç§»ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„é€’å½’ç®—æ³•ï¼Œèƒ½å¤Ÿè€ƒè™‘muPä¸­çš„æ‰€æœ‰è¯¯å·®æ¥æºï¼ŒåŒæ—¶ä»…å¼•å…¥é€‚åº¦çš„èµ„æºå¼€é”€ã€‚é€šè¿‡å¯¹æ¨¡å‹è§„æ¨¡è¾¾åˆ°å››åäº¿å‚æ•°çš„å¹¿æ³›å®éªŒä»¥åŠå¯¹æ•°æ®åˆ†å¸ƒå’Œæ¶æ„çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02735', 'title': 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.02735', 'abstract': 'Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.', 'score': 20, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '1ffa5567eb2f4acc', 'authors': ['Zhouliang Yu', 'Ruotian Peng', 'Keyi Ding', 'Yizhe Li', 'Zhongyuan Peng', 'Minghao Liu', 'Yifan Zhang', 'Zheng Yuan', 'Huajian Xin', 'Wenhao Huang', 'Yandong Wen', 'Ge Zhang', 'Weiyang Liu'], 'affiliations': ['M-A-P', 'Max Planck Institute for Intelligent Systems, TÃ¼bingen', 'Numina', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02735.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'FormalMATH: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'FormalMATH - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Lean4, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 5560 Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM-Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ² 16.46% Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹.'}, 'en': {'title': 'FormalMATH: Advancing AI in Formal Mathematical Reasoning', 'desc': 'The paper introduces FormalMATH, a comprehensive benchmark designed to enhance formal mathematical reasoning in AI. It consists of 5,560 verified problems that cover a wide range of mathematical topics, from high school to undergraduate level. The authors propose a human-in-the-loop autoformalization pipeline that utilizes large language models (LLMs) for automating the formalization process, significantly reducing the need for expert input. The study also highlights the limitations of current LLM-based theorem provers, revealing their low success rates and biases across different mathematical domains.'}, 'zh': {'title': 'FormalMATHï¼šæ¨åŠ¨æ­£å¼æ•°å­¦æ¨ç†çš„åŸºå‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†FormalMATHï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„Lean4åŸºå‡†ï¼ŒåŒ…å«5560ä¸ªç»è¿‡æ­£å¼éªŒè¯çš„æ•°å­¦é—®é¢˜ï¼Œæ¶µç›–ä»é«˜ä¸­å¥¥æ—åŒ¹å…‹æŒ‘æˆ˜åˆ°æœ¬ç§‘å®šç†çš„å¤šä¸ªé¢†åŸŸã€‚ä¸ºäº†è§£å†³æ‰‹åŠ¨å½¢å¼åŒ–çš„ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„äººæœºåä½œè‡ªåŠ¨å½¢å¼åŒ–æµç¨‹ï¼Œç»“åˆäº†ä¸“é—¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯­å¥è‡ªåŠ¨å½¢å¼åŒ–ã€å¤šLLMè¯­ä¹‰éªŒè¯å’ŒåŸºäºå¦å®šçš„åé©³è¿‡æ»¤ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMå®šç†è¯æ˜å™¨åœ¨å®é™…é‡‡æ ·é¢„ç®—ä¸‹çš„æˆåŠŸç‡ä»…ä¸º16.46%ï¼Œå¹¶ä¸”åœ¨ä¸åŒé¢†åŸŸè¡¨ç°å‡ºæ˜æ˜¾çš„åå·®ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒFormalMATHä¸ºæ­£å¼æ•°å­¦æ¨ç†æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02819', 'title': 'ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations', 'url': 'https://huggingface.co/papers/2505.02819', 'abstract': "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.", 'score': 19, 'issue_id': 3608, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '7f169d4491ab6864', 'authors': ['Dmitriy Shopkhoev', 'Ammar Ali', 'Magauiya Zhussip', 'Valentin Malykh', 'Stamatios Lefkimmiatis', 'Nikos Komodakis', 'Sergey Zagoruyko'], 'affiliations': ['Archimedes Athena RC', 'IACM-Forth', 'IITU', 'ITMO University', 'MTS AI', 'Polynome', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2505.02819.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#inference', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'ReplaceMe - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ReplaceMe Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Effortless Depth Pruning with ReplaceMe', 'desc': 'ReplaceMe is a novel method for depth pruning in transformer models that does not require additional training. Instead of retraining the model after pruning, it uses a small calibration dataset to create a linear transformation that approximates the pruned blocks. This allows for significant compression of the model while maintaining high performance, achieving up to 25% pruning with around 90% of the original performance. The method is efficient and can be easily integrated into existing architectures without adding extra parameters, making it a competitive alternative to traditional pruning techniques.'}, 'zh': {'title': 'ReplaceMeï¼šé«˜æ•ˆçš„æ— è®­ç»ƒæ·±åº¦å‰ªææ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºReplaceMeçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œå®ƒä¸éœ€è¦è®­ç»ƒå°±èƒ½æœ‰æ•ˆåœ°å°†å˜æ¢å™¨å—æ›¿æ¢ä¸ºçº¿æ€§æ“ä½œï¼ŒåŒæ—¶åœ¨ä½å‹ç¼©æ¯”ä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å‰ªææ–¹æ³•ä¸åŒï¼ŒReplaceMeåªéœ€ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œä»è€Œè¿‘ä¼¼å‰ªæåçš„å—ã€‚è¿™ä¸ªä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥ä¸å‰©ä½™çš„å˜æ¢å™¨å—æ— ç¼åˆå¹¶ï¼Œé¿å…äº†é¢å¤–çš„ç½‘ç»œå‚æ•°éœ€æ±‚ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå®ç°é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶ä¿ç•™çº¦90%çš„åŸå§‹æ¨¡å‹æ€§èƒ½ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–è°ƒæ•´æ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02391', 'title': 'Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL', 'url': 'https://huggingface.co/papers/2505.02391', 'abstract': 'Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.', 'score': 19, 'issue_id': 3605, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '48b57ad8bd358e5d', 'authors': ['Jiarui Yao', 'Yifan Hao', 'Hanning Zhang', 'Hanze Dong', 'Wei Xiong', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.02391.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GVM-RAFT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ 2-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ RAFT.'}, 'en': {'title': 'Dynamic Resource Allocation for Enhanced Reasoning in LLMs', 'desc': 'This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains.'}, 'zh': {'title': 'åŠ¨æ€æ ·æœ¬åˆ†é…ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ¨ç†ï¼ˆCoTï¼‰å¦‚ä½•è¢«å½¢å¼åŒ–ä¸ºæ½œå˜é‡é—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ä»¥å¾€çš„æ–¹æ³•å¦‚è¿­ä»£å¥–åŠ±æ’åå¾®è°ƒï¼ˆRAFTï¼‰åœ¨å¤„ç†ä¸åŒéš¾åº¦çš„æç¤ºæ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†é¢„ç®—ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†GVM-RAFTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®šæç¤ºçš„åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—é™åˆ¶ä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†2-4å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01658', 'title': 'A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency', 'url': 'https://huggingface.co/papers/2505.01658', 'abstract': 'Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine', 'score': 19, 'issue_id': 3604, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ', 'en': 'May 3', 'zh': '5æœˆ3æ—¥'}, 'hash': '56f8dc116dbd737d', 'authors': ['Sihyeong Park', 'Sungryeol Jeon', 'Chaelyn Lee', 'Seokhun Jeon', 'Byung-Soo Kim', 'Jemin Lee'], 'affiliations': ['Electronics and Telecommunications Research Institute, South Korea', 'Korea Electronics Technology Institute, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.01658.jpg', 'data': {'categories': ['#inference', '#open_source', '#optimization', '#survey'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 25 Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼, Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚ÑŒ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ LLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimizing Inference: A Deep Dive into LLM Engines', 'desc': 'This paper evaluates 25 open-source and commercial inference engines designed for large language models (LLMs). It focuses on key factors such as ease-of-use, deployment, scalability, and performance in terms of throughput and latency. The study also investigates the optimization techniques supported by each engine and assesses the maturity of the open-source ecosystem compared to commercial solutions. Finally, it outlines future research directions to enhance LLM services, hardware support, and security, providing valuable insights for researchers and developers.'}, 'zh': {'title': 'ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“çš„ç ”ç©¶ä¸è¯„ä¼°', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èŠå¤©æœºå™¨äººã€ä»£ç ç”Ÿæˆå™¨å’Œæœç´¢å¼•æ“ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç”±äºé“¾å¼æ€ç»´ã€å¤æ‚æ¨ç†å’Œä»£ç†æœåŠ¡ç­‰å·¥ä½œè´Ÿè½½çš„å¢åŠ ï¼Œæ¨ç†æˆæœ¬æ˜¾è‘—ä¸Šå‡ã€‚è™½ç„¶å·²ç»é‡‡ç”¨å¹¶è¡Œã€å‹ç¼©å’Œç¼“å­˜ç­‰ä¼˜åŒ–æ–¹æ³•æ¥é™ä½æˆæœ¬ï¼Œä½†å¤šæ ·åŒ–çš„æœåŠ¡éœ€æ±‚ä½¿å¾—é€‰æ‹©åˆé€‚çš„æ–¹æ³•å˜å¾—å›°éš¾ã€‚æœ¬æ–‡å¯¹25ä¸ªå¼€æºå’Œå•†ä¸šæ¨ç†å¼•æ“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¢è®¨äº†å®ƒä»¬çš„æ˜“ç”¨æ€§ã€å¯éƒ¨ç½²æ€§ã€é€šç”¨æ”¯æŒã€å¯æ‰©å±•æ€§ä»¥åŠé€‚åˆååé‡å’Œå»¶è¿Ÿæ„ŸçŸ¥è®¡ç®—çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02835', 'title': 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.02835', 'abstract': "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.", 'score': 17, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '5134d59b0389ade9', 'authors': ['Yi-Fan Zhang', 'Xingyu Lu', 'Xiao Hu', 'Chaoyou Fu', 'Bin Wen', 'Tianke Zhang', 'Changyi Liu', 'Kaiyu Jiang', 'Kaibing Chen', 'Kaiyu Tang', 'Haojie Ding', 'Jiankang Chen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Liang Wang'], 'affiliations': ['CASIA', 'KuaiShou', 'NJU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2505.02835.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#multimodal', '#rl', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MRM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ StableReinforce, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… RL-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MRM. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ R1-Reward, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ StableReinforce, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Stable Reinforcement for Enhanced Multimodal Reward Modeling', 'desc': 'This paper investigates the use of Reinforcement Learning (RL) to enhance Multimodal Reward Models (MRMs) for Multimodal Large Language Models (MLLMs). It identifies challenges in applying traditional RL algorithms to reward modeling, which can lead to instability during training. To overcome these issues, the authors introduce the StableReinforce algorithm, which improves training dynamics through better loss refinement and reward design. The results show that their reward model, R1-Reward, outperforms existing state-of-the-art models on multimodal benchmarks, demonstrating the effectiveness of their approach.'}, 'zh': {'title': 'ç¨³å®šå¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ€§èƒ½', 'desc': 'å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æ”¹å–„å¥–åŠ±å»ºæ¨¡ï¼Œæå‡ºå°†å¥–åŠ±å»ºæ¨¡é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºåŸºäºè§„åˆ™çš„RLä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„StableReinforceç®—æ³•é€šè¿‡ä¼˜åŒ–è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡ï¼Œè§£å†³äº†ç°æœ‰RLç®—æ³•åœ¨å¥–åŠ±å»ºæ¨¡ä¸­å¯¼è‡´çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚ç»è¿‡StableReinforceç®—æ³•è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹R1-Rewardåœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†RLç®—æ³•åœ¨ä¼˜åŒ–MRMsä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01441', 'title': 'Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.01441', 'abstract': 'Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.', 'score': 17, 'issue_id': 3603, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 28', 'zh': '4æœˆ28æ—¥'}, 'hash': '4c0a590eccbb1960', 'authors': ['Joykirat Singh', 'Raghav Magazine', 'Yash Pandya', 'Akshay Nambi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01441.jpg', 'data': {'categories': ['#agents', '#rl', '#reasoning', '#training', '#benchmark', '#optimization', '#rlhf'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ARTIST: ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ARTIST - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ARTIST Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ARTIST Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² LLM.'}, 'en': {'title': 'Empowering LLMs with Dynamic Reasoning and Tool Integration', 'desc': "This paper presents ARTIST, a new framework that enhances large language models (LLMs) by integrating agentic reasoning, reinforcement learning (RL), and tool usage. Unlike traditional LLMs that rely on static knowledge, ARTIST allows models to dynamically decide when and how to use external tools during complex reasoning tasks. The framework employs outcome-based RL to improve the model's strategies for interacting with tools and environments, enabling it to learn without needing detailed step-by-step guidance. Experimental results demonstrate that ARTIST significantly outperforms existing models, achieving better reasoning and tool utilization in challenging scenarios."}, 'zh': {'title': 'ARTISTï¼šæ™ºèƒ½æ¨ç†ä¸å·¥å…·é›†æˆçš„æ–°å‰æ²¿', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å—åˆ°é™æ€å†…éƒ¨çŸ¥è¯†å’Œä»…åŸºäºæ–‡æœ¬æ¨ç†çš„é™åˆ¶ã€‚ç°å®ä¸–ç•Œçš„é—®é¢˜è§£å†³é€šå¸¸éœ€è¦åŠ¨æ€çš„å¤šæ­¥éª¤æ¨ç†ã€é€‚åº”æ€§å†³ç­–ä»¥åŠä¸å¤–éƒ¨å·¥å…·å’Œç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ARTISTï¼ˆè‡ªæˆ‘æ”¹è¿›å˜æ¢å™¨ä¸­çš„ä»£ç†æ¨ç†å’Œå·¥å…·é›†æˆï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ä»£ç†æ¨ç†ã€å¼ºåŒ–å­¦ä¹ å’Œå·¥å…·é›†æˆç´§å¯†ç»“åˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARTISTåœ¨æ•°å­¦æ¨ç†å’Œå¤šè½®å‡½æ•°è°ƒç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰é«˜è¾¾22%çš„ç»å¯¹æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02156', 'title': 'Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents', 'url': 'https://huggingface.co/papers/2505.02156', 'abstract': "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction rightarrow deep contemplation) based on real-time context. Our framework's core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach", 'score': 16, 'issue_id': 3602, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': '13e5cf390c136aeb', 'authors': ['Minzheng Wang', 'Yongbin Li', 'Haobo Wang', 'Xinghua Zhang', 'Nan Xu', 'Bingli Wu', 'Fei Huang', 'Haiyang Yu', 'Wenji Mao'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.02156.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adaptive Mode Learning (AML) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². AML Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Adaptive Mode Policy Optimization (AMPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AML Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ° 15.6% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° 32.8% Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Dynamic Reasoning for Smarter Social Agents', 'desc': 'This paper introduces Adaptive Mode Learning (AML), a new approach for simulating social intelligence in language agents. Unlike existing methods that either lack dynamic reasoning or use a one-size-fits-all strategy, AML allows agents to switch between four different thinking modes based on the context of the interaction. The core of this framework is the Adaptive Mode Policy Optimization (AMPO) algorithm, which enhances reasoning efficiency by adapting the depth of thought to the situation. Experimental results show that AML significantly improves task performance and reduces the length of reasoning chains compared to current state-of-the-art methods.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼šæå‡ç¤¾äº¤æ™ºèƒ½çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰ï¼Œæ—¨åœ¨æé«˜ç¤¾äº¤æ™ºèƒ½æ¨¡æ‹Ÿçš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ·±åº¦ä¸Šç¼ºä¹çµæ´»æ€§ï¼Œå¯¼è‡´åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ¨ç†ä¸å¤Ÿé«˜æ•ˆã€‚AMLé€šè¿‡å®æ—¶ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©å››ç§æ€ç»´æ¨¡å¼ï¼Œä»ç›´è§‰ååº”åˆ°æ·±åº¦æ€è€ƒï¼Œä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½ä»»åŠ¡ä¸­æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†15.6%çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¨ç†é“¾é•¿åº¦å‡å°‘äº†32.8%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02094', 'title': 'SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations', 'url': 'https://huggingface.co/papers/2505.02094', 'abstract': 'We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.', 'score': 14, 'issue_id': 3604, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': '1ae5a69dab5de633', 'authors': ['Runyi Yu', 'Yinhuai Wang', 'Qihan Zhao', 'Hok Wai Tsui', 'Jingbo Wang', 'Ping Tan', 'Qifeng Chen'], 'affiliations': ['HKUST Hong Kong, China', 'Shanghai AI Laboratory Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02094.jpg', 'data': {'categories': ['#games', '#training', '#rl', '#optimization', '#data'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (RLID). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹: Stitched Trajectory Graph (STG) Ğ¸ State Transition Field (STF). ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Adaptive Trajectory Sampling (ATS) Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging Skills: Enhancing Reinforcement Learning with Augmented Trajectories', 'desc': 'This paper tackles the issue of noise and limited coverage in Reinforcement Learning from Interaction Demonstration (RLID). It highlights that even with noisy and sparse data, there are countless feasible trajectories that can connect different demonstrated skills. The authors introduce two innovative techniques: the Stitched Trajectory Graph (STG) for identifying transitions between skills, and the State Transition Field (STF) for linking arbitrary states. Their method, combined with Adaptive Trajectory Sampling (ATS) and a historical encoding mechanism, enhances skill learning and generalization beyond the initial demonstrations, showing significant improvements in various tasks.'}, 'zh': {'title': 'å…‹æœæ¼”ç¤ºå™ªå£°ï¼Œå®ç°æŠ€èƒ½çš„é²æ£’å­¦ä¹ ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä»äº¤äº’æ¼”ç¤ºä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLIDï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯æ¼”ç¤ºå™ªå£°å’Œè¦†ç›–é™åˆ¶ã€‚ç°æœ‰çš„æ•°æ®æ”¶é›†æ–¹æ³•è™½ç„¶æä¾›äº†æœ‰ä»·å€¼çš„äº¤äº’æ¼”ç¤ºï¼Œä½†å¾€å¾€å¯¼è‡´ç¨€ç–ã€æ–­è£‚å’Œå™ªå£°çš„è½¨è¿¹ï¼Œæ— æ³•å…¨é¢æ•æ‰æŠ€èƒ½å˜åŒ–å’Œè¿‡æ¸¡çš„å…¨è²Œã€‚æˆ‘ä»¬æå‡ºçš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡æ¼”ç¤ºå­˜åœ¨å™ªå£°å’Œç¨€ç–æ€§ï¼Œä½†ä»ç„¶å­˜åœ¨æ— é™çš„ç‰©ç†å¯è¡Œè½¨è¿¹ï¼Œå¯ä»¥è‡ªç„¶åœ°è¿æ¥æ¼”ç¤ºæŠ€èƒ½æˆ–ä»å…¶é‚»è¿‘çŠ¶æ€ä¸­äº§ç”Ÿã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æŠ€èƒ½è·å–çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02471', 'title': 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction', 'url': 'https://huggingface.co/papers/2505.02471', 'abstract': 'We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'fa1cacd261a625e3', 'authors': ['Biao Gong', 'Cheng Zou', 'Dandan Zheng', 'Hu Yu', 'Jingdong Chen', 'Jianxin Sun', 'Junbo Zhao', 'Jun Zhou', 'Kaixiang Ji', 'Lixiang Ru', 'Libin Wang', 'Qingpei Guo', 'Rui Liu', 'Weilong Chai', 'Xinyu Xiao', 'Ziyuan Huang'], 'affiliations': ['Ant Group', 'Ming-Lite-Uni'], 'pdf_title_img': 'assets/pdf/title_img/2505.02471.jpg', 'data': {'categories': ['#agi', '#multimodal', '#open_source', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ', 'desc': 'Ming-Lite-Uni - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ming-Lite-Uni Ğ¸ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°.'}, 'en': {'title': 'Unifying Vision and Language for Advanced AI', 'desc': "Ming-Lite-Uni is an innovative open-source framework designed to integrate vision and language through a unified visual generator and a multimodal autoregressive model. It introduces advanced techniques like multi-scale learnable tokens and representation alignment to enhance the interaction between text and images. By utilizing a fixed MLLM alongside a learnable diffusion model, it supports both text-to-image generation and image editing based on instructions. The framework's strong performance and interactive capabilities highlight its potential impact on the development of advanced AI systems, contributing to the journey towards artificial general intelligence (AGI)."}, 'zh': {'title': 'Ming-Lite-Uniï¼šç»Ÿä¸€è§†è§‰ä¸è¯­è¨€çš„å¤šæ¨¡æ€æ¡†æ¶', 'desc': 'Ming-Lite-Uniæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ã€‚å®ƒå¼•å…¥äº†æ–°çš„è§†è§‰ç”Ÿæˆå™¨å’Œå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å¤šå°ºåº¦å¯å­¦ä¹ æ ‡è®°å’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ï¼Œæå‡äº†æ¨¡å‹çš„è¡¨ç°ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡éƒ½æ˜¯å¼€æºçš„ï¼Œé¼“åŠ±ç¤¾åŒºè¿›ä¸€æ­¥æ¢ç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02370', 'title': 'SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing', 'url': 'https://huggingface.co/papers/2505.02370', 'abstract': 'Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '5833f2e06b661a76', 'authors': ['Ming Li', 'Xin Gu', 'Fan Chen', 'Xiaoying Xing', 'Longyin Wen', 'Chen Chen', 'Sijie Zhu'], 'affiliations': ['ByteDance Intelligent Creation (USA)', 'Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2505.02370.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#training', '#cv'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 9.19% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Real-Edit Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 30 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ² 13 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Image Editing with Accurate Instructions and Contrastive Supervision', 'desc': 'This paper addresses the problem of noisy supervision in image editing tasks caused by mismatched editing instructions and image pairs. The authors propose a novel approach that involves creating more accurate editing instructions that align better with the original and edited images. They introduce contrastive editing instructions to enhance the effectiveness of these instructions and utilize a unified guide for vision-language models to rectify them. Their method improves supervision signals without relying on pre-training or VLM modules, achieving significant performance gains on benchmarks with less training data and smaller model sizes.'}, 'zh': {'title': 'æå‡å›¾åƒç¼–è¾‘æ•ˆæœçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å›¾åƒç¼–è¾‘æ¨¡å‹çš„ç›‘ç£ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºæ›´æœ‰æ•ˆçš„ç¼–è¾‘æŒ‡ä»¤ï¼Œä½¿å…¶ä¸åŸå§‹å’Œç¼–è¾‘åçš„å›¾åƒå¯¹æ›´å¥½åœ°å¯¹é½ï¼Œå¹¶å¼•å…¥å¯¹æ¯”ç¼–è¾‘æŒ‡ä»¤æ¥å¢å¼ºæ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œç¼–è¾‘æ¨¡å‹åœ¨ä¸åŒæ¨ç†æ­¥éª¤ä¸­è¡¨ç°å‡ºç‰¹å®šçš„ç”Ÿæˆå±æ€§ï¼Œè¿™äº›å±æ€§ä¸æ–‡æœ¬æ— å…³ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºä¹‹å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹æˆ–é¢„è®­ç»ƒä»»åŠ¡ï¼Œæä¾›äº†ä¸€ç§æ›´ç›´æ¥å’Œé«˜æ•ˆçš„ç›‘ç£ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01043', 'title': 'Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities', 'url': 'https://huggingface.co/papers/2505.01043', 'abstract': 'Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.', 'score': 9, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ', 'en': 'May 2', 'zh': '5æœˆ2æ—¥'}, 'hash': 'b67e3ec75756e896', 'authors': ['Zhiwei Hao', 'Jianyuan Guo', 'Li Shen', 'Yong Luo', 'Han Hu', 'Guoxia Wang', 'Dianhai Yu', 'Yonggang Wen', 'Dacheng Tao'], 'affiliations': ['Baidu Inc., Beijing 100000, China', 'College of Computing and Data Science, Nanyang Technological University, 639798, Singapore', 'Computer Science, City University of Hong Kong, Hong Kong 999077, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China', 'School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China', 'School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01043.jpg', 'data': {'categories': ['#optimization', '#inference', '#survey', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¸ Ñ†ĞµĞ»Ñ‹Ñ… Ñ‡Ğ¸ÑĞµĞ», Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ±Ğ·Ğ¾Ñ€ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Optimizing Large Language Models with Low-Precision Training', 'desc': 'This paper reviews low-precision training techniques for large language models (LLMs), which help improve training efficiency while reducing hardware resource demands. It categorizes these methods into three main groups based on numerical formats: fixed-point and integer-based, floating-point-based, and customized format-based methods. The survey also addresses quantization-aware training, which is closely related to low-precision training during model inference. By organizing the existing research, the paper aims to provide clarity and direction for future advancements in low-precision training.'}, 'zh': {'title': 'ä½ç²¾åº¦è®­ç»ƒï¼šæå‡æ•ˆç‡çš„å…³é”®', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¡¨ç°ï¼Œä½†å…¶è®­ç»ƒæ‰€éœ€çš„ç¡¬ä»¶èµ„æºæå¤§ï¼Œæˆä¸ºæ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„éšœç¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯è¢«å¹¿æ³›é‡‡ç”¨ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½ç²¾åº¦è®­ç»ƒæ¶‰åŠå¤šä¸ªç»„ä»¶ï¼Œå¦‚æƒé‡ã€æ¿€æ´»å’Œæ¢¯åº¦ï¼Œæ¯ä¸ªç»„ä»¶å¯ä»¥ç”¨ä¸åŒçš„æ•°å€¼æ ¼å¼è¡¨ç¤ºï¼Œå¯¼è‡´ç ”ç©¶é¢†åŸŸçš„ç¢ç‰‡åŒ–ã€‚æœ¬æ–‡ç»¼è¿°äº†ç°æœ‰çš„ä½ç²¾åº¦è®­ç»ƒæ–¹æ³•ï¼Œå¹¶æ ¹æ®æ•°å€¼æ ¼å¼å°†å…¶ç³»ç»Ÿåœ°åˆ†ç±»ä¸ºä¸‰å¤§ç±»ï¼Œä»¥ä¾¿äºç ”ç©¶è€…ç†è§£å’Œå‚è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02625', 'title': 'LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis', 'url': 'https://huggingface.co/papers/2505.02625', 'abstract': 'Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.', 'score': 7, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '2f8233e0d3083036', 'authors': ['Qingkai Fang', 'Yan Zhou', 'Shoutao Guo', 'Shaolei Zhang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02625.jpg', 'data': {'categories': ['#audio', '#small_models', '#benchmark'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA-Omni 2 - ÑĞµÑ€Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Qwen2.5 Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ° Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, LLaMA-Omni 2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Speech Interaction with LLaMA-Omni 2', 'desc': 'This paper presents LLaMA-Omni 2, a new series of speech language models designed for real-time speech interaction. These models, ranging from 0.5B to 14B parameters, utilize a speech encoder and an autoregressive streaming speech decoder to facilitate intelligent spoken chatbots. Remarkably, LLaMA-Omni 2 achieves high performance despite being trained on only 200K multi-turn speech dialogue samples. It outperforms previous models like GLM-4-Voice, which were trained on significantly larger datasets, showcasing the efficiency of the new architecture.'}, 'zh': {'title': 'å®æ—¶æ™ºèƒ½è¯­éŸ³äº¤äº’çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†LLaMA-Omni 2ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»åˆ—çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»0.5äº¿åˆ°14äº¿ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„å®æ—¶è¯­éŸ³äº¤äº’ã€‚è¯¥æ¨¡å‹åŸºäºQwen2.5ç³»åˆ—ï¼Œç»“åˆäº†è¯­éŸ³ç¼–ç å™¨å’Œè‡ªå›å½’æµå¼è¯­éŸ³è§£ç å™¨ã€‚å°½ç®¡ä»…åœ¨20ä¸‡å¤šè½®è¯­éŸ³å¯¹è¯æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒLLaMA-Omni 2åœ¨å¤šä¸ªè¯­éŸ³é—®ç­”å’Œè¯­éŸ³æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹GLM-4-Voiceã€‚è¿™è¡¨æ˜ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ•°æ®ä¹Ÿèƒ½è®­ç»ƒå‡ºé«˜æ•ˆçš„æ™ºèƒ½è¯­éŸ³èŠå¤©æœºå™¨äººã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01583', 'title': 'TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action', 'url': 'https://huggingface.co/papers/2505.01583', 'abstract': 'Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.', 'score': 6, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ', 'en': 'May 2', 'zh': '5æœˆ2æ—¥'}, 'hash': 'd29565337415d11f', 'authors': ['Jen-Hao Cheng', 'Vivian Wang', 'Huayu Wang', 'Huapeng Zhou', 'Yi-Hao Peng', 'Hou-I Liu', 'Hsiang-Wei Huang', 'Kuang-Ming Chen', 'Cheng-Yen Yang', 'Wenhao Chai', 'Yi-Ling Chen', 'Vibhav Vineet', 'Qin Cai', 'Jenq-Neng Hwang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.01583.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#video', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'TEMPURA: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹', 'desc': 'TEMPURA - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ TEMPURA ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VER, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 1 Ğ¼Ğ»Ğ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 500 Ñ‚Ñ‹Ñ. Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TEMPURA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'TEMPURA: Enhancing Video Understanding through Causal Reasoning and Temporal Segmentation', 'desc': 'This paper introduces TEMPURA, a novel framework designed to improve how vision-language models understand the timing and relationships of events in videos. It addresses the limitations of existing methods that either lose important details by compressing video data or treat videos as continuous streams without clear boundaries. TEMPURA employs a two-stage training process that first predicts missing events to create causal explanations and then segments videos into distinct events with precise descriptions. The framework is trained on a large dataset, VER, and shows significant improvements in tasks like temporal grounding and highlight detection compared to existing models.'}, 'zh': {'title': 'TEMPURAï¼šæå‡è§†é¢‘ç†è§£çš„å› æœæ¨ç†ä¸æ—¶é—´åˆ†å‰²ç»“åˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTEMPURAçš„æ¡†æ¶ï¼Œç”¨äºæé«˜è§†é¢‘çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚TEMPURAé€šè¿‡æ©è”½äº‹ä»¶é¢„æµ‹æ¨ç†ï¼Œé‡å»ºç¼ºå¤±äº‹ä»¶å¹¶ç”Ÿæˆé€æ­¥çš„å› æœè§£é‡Šï¼Œä»è€Œæ›´å¥½åœ°ç†è§£è§†é¢‘ä¸­çš„äº‹ä»¶å…³ç³»ã€‚æ¥ç€ï¼Œå®ƒå­¦ä¹ è§†é¢‘åˆ†å‰²å’Œå¯†é›†æ ‡æ³¨ï¼Œå°†è§†é¢‘åˆ†è§£ä¸ºä¸é‡å çš„äº‹ä»¶ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ—¶é—´æˆ³å¯¹é½æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTEMPURAåœ¨æ—¶é—´å®šä½å’Œé«˜äº®æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å› æœæ¨ç†ä¸ç»†ç²’åº¦æ—¶é—´åˆ†å‰²çš„ç»“åˆèƒ½å¤Ÿæå‡è§†é¢‘ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02823', 'title': 'MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing', 'url': 'https://huggingface.co/papers/2505.02823', 'abstract': 'Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.', 'score': 3, 'issue_id': 3603, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'da9d48f7aea6c0bd', 'authors': ['Zinan Guo', 'Pengze Zhang', 'Yanze Wu', 'Chong Mou', 'Songtao Zhao', 'Qian He'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2505.02823.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#multimodal', '#data'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MUSAR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ°Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'MUSAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ°Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. MUSAR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MUSAR Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ.'}, 'en': {'title': 'MUSAR: Unlocking Multi-Subject Customization with Single-Subject Data', 'desc': 'The paper presents MUSAR, a novel framework designed to enhance multi-subject customization in machine learning. It addresses two main challenges: the scarcity of diverse multi-subject training data and the issue of attribute entanglement among different subjects. MUSAR employs debiased diptych learning to create training pairs from single-subject images, correcting biases through static attention routing and dual-branch LoRA. Additionally, it utilizes a dynamic attention routing mechanism to establish clear mappings between generated images and their corresponding subjects, leading to improved image quality and consistency across subjects.'}, 'zh': {'title': 'MUSARï¼šå•ä¸€ä¸»é¢˜æ•°æ®ä¸‹çš„å¤šä¸»é¢˜å®šåˆ¶æ–°æ–¹æ³•', 'desc': 'å½“å‰çš„å¤šä¸»é¢˜å®šåˆ¶æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè·å–å¤šæ ·åŒ–çš„å¤šä¸»é¢˜è®­ç»ƒæ•°æ®çš„å›°éš¾ï¼Œä»¥åŠä¸åŒä¸»é¢˜ä¹‹é—´å±æ€§çš„çº ç¼ ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MUSARæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä»…éœ€å•ä¸€ä¸»é¢˜è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°ç¨³å¥çš„å¤šä¸»é¢˜å®šåˆ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†å»åå·®çš„åŒè”å­¦ä¹ ï¼Œé€šè¿‡ä»å•ä¸€ä¸»é¢˜å›¾åƒæ„å»ºåŒè”è®­ç»ƒå¯¹ï¼Œä¿ƒè¿›å¤šä¸»é¢˜å­¦ä¹ ï¼Œå¹¶é€šè¿‡é™æ€æ³¨æ„åŠ›è·¯ç”±å’ŒåŒåˆ†æ”¯LoRAä¸»åŠ¨çº æ­£åŒè”æ„å»ºå¼•å…¥çš„åˆ†å¸ƒåå·®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€æ³¨æ„åŠ›è·¯ç”±æœºåˆ¶ï¼Œé€‚åº”æ€§åœ°å»ºç«‹ç”Ÿæˆå›¾åƒä¸æ¡ä»¶ä¸»é¢˜ä¹‹é—´çš„åŒå°„æ˜ å°„ï¼Œä»è€Œæ¶ˆé™¤è·¨ä¸»é¢˜çš„çº ç¼ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02005', 'title': 'Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields', 'url': 'https://huggingface.co/papers/2505.02005', 'abstract': 'Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.', 'score': 3, 'issue_id': 3611, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': 'b0fc91d25884f885', 'authors': ['Zhenxing Mi', 'Ping Yin', 'Xue Xiao', 'Dan Xu'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR', 'Inspur Cloud Information Technology Co, Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2505.02005.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ NeRF Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½', 'desc': 'Switch-NeRF++ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ (NeRF). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ ÑĞ¼ĞµÑÑŒ Ñ…ĞµÑˆ-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (HMoHE) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… NeRF-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞµÑ‚ÑŒ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ…ĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ñ…ĞµÑˆ-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Switch-NeRF++ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Efficient Scene Decomposition for Scalable NeRFs', 'desc': 'This paper presents Switch-NeRF++, a novel approach to improve the scalability and efficiency of Neural Radiance Fields (NeRF) for large-scale scenes. It introduces a Heterogeneous Mixture of Hash Experts (HMoHE) network that learns to decompose scenes into specialized components, allowing for better handling of scene heterogeneity. The framework utilizes a gating network that allocates 3D points to different NeRF experts, optimizing the learning process in an end-to-end manner. The results show significant improvements in both training and rendering speeds, achieving state-of-the-art accuracy on large-scale datasets.'}, 'zh': {'title': 'é«˜æ•ˆå¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSwitch-NeRF++çš„ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡åœºæ™¯ä¸­çš„åœºæ™¯åˆ†è§£ã€å»ºæ¨¡å¼‚è´¨æ€§å’Œå»ºæ¨¡æ•ˆç‡ç­‰é—®é¢˜ã€‚è¯¥ç½‘ç»œé‡‡ç”¨äº†å¼‚è´¨æ··åˆå“ˆå¸Œä¸“å®¶ï¼ˆHMoHEï¼‰æ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å­¦ä¹ å¼‚è´¨åˆ†è§£å’Œå¼‚è´¨NeRFã€‚é€šè¿‡ä¸€ä¸ªé—¨æ§ç½‘ç»œï¼Œç³»ç»Ÿèƒ½å¤Ÿå°†3Dç‚¹åˆ†é…ç»™ä¸“é—¨çš„NeRFä¸“å®¶ï¼Œä»è€Œå®ç°ç«¯åˆ°ç«¯çš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡ä¸­å…·æœ‰ä¼˜è¶Šçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè®­ç»ƒé€Ÿåº¦æå‡8å€ï¼Œæ¸²æŸ“é€Ÿåº¦æå‡16å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01456', 'title': 'Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation', 'url': 'https://huggingface.co/papers/2505.01456', 'abstract': 'LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.', 'score': 2, 'issue_id': 3608, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '0201cbbcc6e52005', 'authors': ['Vaidehi Patil', 'Yi-Lin Sung', 'Peter Hase', 'Jie Peng', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science University of North Carolina at Chapel Hill', 'School of Artificial Intelligence and Data Science University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01456.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#interpretability', '#security', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ—Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº UnLOK-VQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· MLLM. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑˆĞµÑÑ‚ÑŒ Ñ†ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ ÑĞµĞ¼ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ñ‚Ğ°Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Safety in Multimodal LLMs through Targeted Unlearning', 'desc': 'This paper discusses the risks associated with large language models (LLMs) that are trained on extensive datasets, particularly the unintentional acquisition of sensitive information. It highlights the challenges of multimodal LLMs, which combine text and images, making them more vulnerable to adversarial attacks that can extract private details. The authors introduce a new benchmark called UnLOK-VQA for evaluating targeted unlearning in multimodal contexts, along with a framework for assessing methods to remove specific knowledge from these models. Their findings indicate that multimodal attacks are more effective than those targeting only text or images, and that larger models tend to be more robust against such attacks, emphasizing the need for improved unlearning techniques in multimodal settings.'}, 'zh': {'title': 'å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿä¿¡æ¯é—å¿˜æŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ•æ„Ÿä¿¡æ¯æ—¶çš„é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå’Œæ–‡æœ¬ç»“åˆçš„æƒ…å†µä¸‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨å¤šæ¨¡æ€æç¤ºæ¥æå–è¿™äº›æ•æ„Ÿä¿¡æ¯ï¼Œå› æ­¤éœ€è¦æœ‰æ•ˆçš„ç›®æ ‡æ€§é—å¿˜æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•UnLOK-VQAï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªæ”»å‡»ä¸é˜²å¾¡æ¡†æ¶ï¼Œä»¥è¯„ä¼°ä»MLLMsä¸­åˆ é™¤ç‰¹å®šå¤šæ¨¡æ€çŸ¥è¯†çš„æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€æ”»å‡»çš„æ•ˆæœä¼˜äºå•ä¸€æ–‡æœ¬æˆ–å›¾åƒæ”»å‡»ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹åœ¨åæœŸç¼–è¾‘æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œè¡¨æ˜æ¨¡å‹è§„æ¨¡å¯¹å®‰å…¨æ€§æœ‰ç§¯æå½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02130', 'title': 'Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data', 'url': 'https://huggingface.co/papers/2505.02130', 'abstract': "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: https://github.com/millioniron/LLM_exploration{LLM4Exploration}", 'score': 1, 'issue_id': 3616, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ', 'en': 'May 4', 'zh': '5æœˆ4æ—¥'}, 'hash': 'a07adea642e1877d', 'authors': ['Zhong Guan', 'Likang Wu', 'Hongke Zhao', 'Ming He', 'Jianpin Fan'], 'affiliations': ['AI Lab at Lenovo', 'College of Management and Economics, Tianjin University, Tianjin, China', 'Laboratory of Computation and Analytics of Complex Management Systems, Tianjin University, Tianjin, China', 'ai-deepcube'], 'pdf_title_img': 'assets/pdf/title_img/2505.02130.jpg', 'data': {'categories': ['#graphs', '#training', '#architecture', '#interpretability'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² LLM Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸. Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ LLM Ğ¿Ğ¾ ÑƒĞ·Ğ»Ğ°Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking LLMs: Enhancing Graph Understanding with Attention', 'desc': 'This paper investigates how large language models (LLMs) utilize attention mechanisms when processing graph-structured data. It highlights that while LLMs can identify interactions between text and graph nodes, they struggle with understanding the relationships between nodes due to their architectural limitations. The study reveals that the attention distribution in LLMs does not effectively reflect the underlying graph structure, leading to suboptimal performance. To address these issues, the authors propose using intermediate-state attention windows to enhance training and improve the transition to fully connected attention during inference.'}, 'zh': {'title': 'æ¢ç´¢æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾æ•°æ®ä¸­çš„è¡¨ç°', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å›¾ç»“æ„æ•°æ®æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿè¯†åˆ«å›¾æ•°æ®å¹¶æ•æ‰æ–‡æœ¬ä¸èŠ‚ç‚¹ä¹‹é—´çš„äº¤äº’ï¼Œä½†åœ¨å»ºæ¨¡èŠ‚ç‚¹é—´å…³ç³»æ—¶å­˜åœ¨å›°éš¾ã€‚æ³¨æ„åŠ›åˆ†å¸ƒæœªèƒ½ä¸ç†æƒ³çš„ç»“æ„æ¨¡å¼å¯¹é½ï¼Œæ˜¾ç¤ºå‡ºå¯¹å›¾æ‹“æ‰‘çš„é€‚åº”æ€§ä¸è¶³ã€‚é€šè¿‡å¼•å…¥ä¸­é—´çŠ¶æ€çš„æ³¨æ„åŠ›çª—å£ï¼Œç ”ç©¶è¡¨æ˜å¯ä»¥æé«˜LLMsçš„è®­ç»ƒæ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†æ—¶æ— ç¼è¿‡æ¸¡åˆ°å®Œå…¨è¿æ¥çš„çª—å£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01548', 'title': 'Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional\n  Motion-enhanced Event Representation', 'url': 'https://huggingface.co/papers/2505.01548', 'abstract': 'Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.', 'score': 1, 'issue_id': 3619, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ', 'en': 'May 2', 'zh': '5æœˆ2æ—¥'}, 'hash': '07a36a1d377e5cc7', 'authors': ['Zhen Yao', 'Xiaowen Ying', 'Mooi Choo Chuah'], 'affiliations': ['Lehigh University', 'Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.01548.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ RGB Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ RGB-ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ - Motion-enhanced Event Tensor (MET), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ĞºÑĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ¿Ğ»Ğ¾Ñ‚Ğ½ÑƒÑ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ BFAM Ğ¸ TFM Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ RGB Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing RGB-Event Fusion with Motion-Enhanced Event Tensor', 'desc': 'This paper addresses the challenges of combining RGB images with event camera data in computer vision. It identifies three main types of misalignments: temporal, spatial, and modal, which hinder effective integration. To overcome these issues, the authors introduce the Motion-enhanced Event Tensor (MET), which creates a dense and coherent representation of events. Additionally, they propose two modules, BFAM and TFM, to further align the data and improve performance in semantic segmentation tasks, achieving superior results on benchmark datasets.'}, 'zh': {'title': 'è¿åŠ¨å¢å¼ºäº‹ä»¶å¼ é‡ï¼šè§£å†³RGB-äº‹ä»¶èåˆæŒ‘æˆ˜çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº‹ä»¶è¡¨ç¤ºæ–¹æ³•ï¼Œç§°ä¸ºè¿åŠ¨å¢å¼ºäº‹ä»¶å¼ é‡ï¼ˆMETï¼‰ï¼Œæ—¨åœ¨è§£å†³RGB-äº‹ä»¶èåˆä¸­çš„ä¸‰ç§å†…åœ¨ä¸å¯¹é½é—®é¢˜ï¼šæ—¶é—´ã€ç©ºé—´å’Œæ¨¡æ€ä¸å¯¹é½ã€‚METé€šè¿‡åˆ©ç”¨å¯†é›†å…‰æµå’Œäº‹ä»¶æ—¶é—´ç‰¹å¾ï¼Œå°†ç¨€ç–äº‹ä»¶ä½“ç´ è½¬åŒ–ä¸ºå¯†é›†ä¸”æ—¶é—´ä¸€è‡´çš„å½¢å¼ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥åŒå‘æµèšåˆæ¨¡å—ï¼ˆBFAMï¼‰å’Œæ—¶é—´èåˆæ¨¡å—ï¼ˆTFMï¼‰ï¼Œä»¥è§£å†³æ¨¡æ€å’Œæ—¶ç©ºä¸å¯¹é½é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„RGB-äº‹ä»¶è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07062', 'title': 'Seed1.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2505.07062', 'abstract': 'We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)', 'score': 83, 'issue_id': 3722, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ', 'en': 'May 11', 'zh': '5æœˆ11æ—¥'}, 'hash': 'c3406b40cc21820d', 'authors': ['Dong Guo', 'Faming Wu', 'Feida Zhu', 'Fuxing Leng', 'Guang Shi', 'Haobin Chen', 'Haoqi Fan', 'Jian Wang', 'Jianyu Jiang', 'Jiawei Wang', 'Jingji Chen', 'Jingjia Huang', 'Kang Lei', 'Liping Yuan', 'Lishu Luo', 'Pengfei Liu', 'Qinghao Ye', 'Rui Qian', 'Shen Yan', 'Shixiong Zhao', 'Shuai Peng', 'Shuangye Li', 'Sihang Yuan', 'Sijin Wu', 'Tianheng Cheng', 'Weiwei Liu', 'Wenqian Wang', 'Xianhan Zeng', 'Xiao Liu', 'Xiaobo Qin', 'Xiaohan Ding', 'Xiaojun Xiao', 'Xiaoying Zhang', 'Xuanwei Zhang', 'Xuehan Xiong', 'Yanghua Peng', 'Yangrui Chen', 'Yanwei Li', 'Yanxu Hu', 'Yi Lin', 'Yiyuan Hu', 'Yiyuan Zhang', 'Youbin Wu', 'Yu Li', 'Yudong Liu', 'Yue Ling', 'Yujia Qin', 'Zanbo Wang', 'Zhiwu He', 'Aoxue Zhang', 'Bairen Yi', 'Bencheng Liao', 'Can Huang', 'Can Zhang', 'Chaorui Deng', 'Chaoyi Deng', 'Cheng Lin', 'Cheng Yuan', 'Chenggang Li', 'Chenhui Gou', 'Chenwei Lou', 'Chengzhi Wei', 'Chundian Liu', 'Chunyuan Li', 'Deyao Zhu', 'Donghong Zhong', 'Feng Li', 'Feng Zhang', 'Gang Wu', 'Guodong Li', 'Guohong Xiao', 'Haibin Lin', 'Haihua Yang', 'Haoming Wang', 'Heng Ji', 'Hongxiang Hao', 'Hui Shen', 'Huixia Li', 'Jiahao Li', 'Jialong Wu', 'Jianhua Zhu', 'Jianpeng Jiao', 'Jiashi Feng', 'Jiaze Chen', 'Jianhui Duan', 'Jihao Liu', 'Jin Zeng', 'Jingqun Tang', 'Jingyu Sun', 'Joya Chen', 'Jun Long', 'Junda Feng', 'Junfeng Zhan', 'Junjie Fang', 'Junting Lu', 'Kai Hua', 'Kai Liu', 'Kai Shen', 'Kaiyuan Zhang', 'Ke Shen', 'Ke Wang', 'Keyu Pan', 'Kun Zhang', 'Kunchang Li', 'Lanxin Li', 'Lei Li', 'Lei Shi', 'Li Han', 'Liang Xiang', 'Liangqiang Chen', 'Lin Chen', 'Lin Li', 'Lin Yan', 'Liying Chi', 'Longxiang Liu', 'Mengfei Du', 'Mingxuan Wang', 'Ningxin Pan', 'Peibin Chen', 'Pengfei Chen', 'Pengfei Wu', 'Qingqing Yuan', 'Qingyao Shuai', 'Qiuyan Tao', 'Renjie Zheng', 'Renrui Zhang', 'Ru Zhang', 'Rui Wang', 'Rui Yang', 'Rui Zhao', 'Shaoqiang Xu', 'Shihao Liang', 'Shipeng Yan', 'Shu Zhong', 'Shuaishuai Cao', 'Shuangzhi Wu', 'Shufan Liu', 'Shuhan Chang', 'Songhua Cai', 'Tenglong Ao', 'Tianhao Yang', 'Tingting Zhang', 'Wanjun Zhong', 'Wei Jia', 'Wei Weng', 'Weihao Yu', 'Wenhao Huang', 'Wenjia Zhu', 'Wenli Yang', 'Wenzhi Wang', 'Xiang Long', 'XiangRui Yin', 'Xiao Li', 'Xiaolei Zhu', 'Xiaoying Jia', 'Xijin Zhang', 'Xin Liu', 'Xinchen Zhang', 'Xinyu Yang', 'Xiongcai Luo', 'Xiuli Chen', 'Xuantong Zhong', 'Xuefeng Xiao', 'Xujing Li', 'Yan Wu', 'Yawei Wen', 'Yifan Du', 'Yihao Zhang', 'Yining Ye', 'Yonghui Wu', 'Yu Liu', 'Yu Yue', 'Yufeng Zhou', 'Yufeng Yuan', 'Yuhang Xu', 'Yuhong Yang', 'Yun Zhang', 'Yunhao Fang', 'Yuntao Li', 'Yurui Ren', 'Yuwen Xiong', 'Zehua Hong', 'Zehua Wang', 'Zewei Sun', 'Zeyu Wang', 'Zhao Cai', 'Zhaoyue Zha', 'Zhecheng An', 'Zhehui Zhao', 'Zhengzhuo Xu', 'Zhipeng Chen', 'Zhiyong Wu', 'Zhuofan Zheng', 'Zihao Wang', 'Zilong Huang', 'Ziyu Zhu', 'Zuquan Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07062.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#architecture', '#training', '#reasoning', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'desc': 'Seed1.5-VL - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 532 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ 20 Ğ¼Ğ»Ñ€Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ state-of-the-art Ğ½Ğ° 38 Ğ¸Ğ· 60 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Seed1.5-VL Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸, Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Empowering Multimodal Understanding with Seed1.5-VL', 'desc': 'Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles.'}, 'zh': {'title': 'Seed1.5-VLï¼šå¤šæ¨¡æ€ç†è§£ä¸æ¨ç†çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Seed1.5-VLï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚Seed1.5-VLç”±ä¸€ä¸ª532Må‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ªå…·æœ‰20Bæ´»è·ƒå‚æ•°çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoE LLMï¼‰ç»„æˆã€‚å°½ç®¡å…¶æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†åœ¨å¤šä¸ªå…¬å…±VLMåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨60ä¸ªå…¬å…±åŸºå‡†ä¸­æœ‰38ä¸ªè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨ä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾å½¢ç”¨æˆ·ç•Œé¢æ§åˆ¶å’Œæ¸¸æˆç©æ³•ï¼ŒSeed1.5-VLè¶…è¶Šäº†é¢†å…ˆçš„å¤šæ¨¡æ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬OpenAI CUAå’ŒClaude 3.7ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07608', 'title': 'MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining', 'url': 'https://huggingface.co/papers/2505.07608', 'abstract': "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.", 'score': 52, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '9db5f7b72add3369', 'authors': ['Xiaomi LLM-Core Team', ':', 'Bingquan Xia', 'Bowen Shen', 'Cici', 'Dawei Zhu', 'Di Zhang', 'Gang Wang', 'Hailin Zhang', 'Huaqiu Liu', 'Jiebao Xiao', 'Jinhao Dong', 'Liang Zhao', 'Peidian Li', 'Peng Wang', 'Shihua Yu', 'Shimao Chen', 'Weikun Wang', 'Wenhan Ma', 'Xiangwei Deng', 'Yi Huang', 'Yifan Song', 'Zihan Jiang', 'Bowen Ye', 'Can Cai', 'Chenhong He', 'Dong Zhang', 'Duo Zhang', 'Guoan Wang', 'Hao Tian', 'Haochen Zhao', 'Heng Qu', 'Hongshen Xu', 'Jun Shi', 'Kainan Bao', 'QingKai Fang', 'Kang Zhou', 'Kangyang Zhou', 'Lei Li', 'Menghang Zhu', 'Nuo Chen', 'Qiantong Wang', 'Shaohui Liu', 'Shicheng Li', 'Shuhao Gu', 'Shuhuai Ren', 'Shuo Liu', 'Sirui Deng', 'Weiji Zhuang', 'Weiwei Lv', 'Wenyu Yang', 'Xin Zhang', 'Xing Yong', 'Xing Zhang', 'Xingchen Song', 'Xinzhe Xu', 'Xu Wang', 'Yihan Yan', 'Yu Tu', 'Yuanyuan Tian', 'Yudong Wang', 'Yue Yu', 'Zhenru Lin', 'Zhichao Song', 'Zihao Yue'], 'affiliations': ['Xiaomi LLM-Core Team'], 'pdf_title_img': 'assets/pdf/title_img/2505.07608.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#dataset', '#math', '#rl', '#data', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MiMo-7B: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'MiMo-7B - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ»Ğ¾ÑÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 130 Ñ‚Ñ‹ÑÑÑ‡ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MiMo-7B-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'MiMo-7B: Revolutionizing Reasoning with Advanced Training Techniques', 'desc': 'MiMo-7B is a large language model specifically designed for reasoning tasks, optimized through both pre-training and post-training processes. In the pre-training phase, it utilizes an advanced data preprocessing pipeline and a three-stage data mixing strategy to enhance its reasoning capabilities, training on a massive dataset of 25 trillion tokens. The post-training phase involves reinforcement learning with a curated dataset of 130,000 math and programming problems, addressing sparse-reward challenges with a code-reward scheme and strategic data resampling. Evaluations demonstrate that MiMo-7B-Base excels in reasoning tasks, outperforming larger models, while the final RL-tuned version, MiMo-7B-RL, achieves outstanding results in mathematics, coding, and general reasoning tasks.'}, 'zh': {'title': 'MiMo-7Bï¼šæ¨ç†ä»»åŠ¡çš„å¼ºå¤§è¯­è¨€æ¨¡å‹', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†MiMo-7Bï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–äº†é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¢å¼ºäº†æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥æå‡åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚MiMo-7B-Baseåœ¨25ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¢åŠ äº†å¤šæ ‡è®°é¢„æµ‹ç›®æ ‡ï¼Œä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ•´ç†äº†130Kä¸ªå¯éªŒè¯çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜æ•°æ®é›†ï¼Œç»“åˆæµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æœºåˆ¶ï¼Œè§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æˆ˜ç•¥æ€§æ•°æ®é‡é‡‡æ ·æ¥ç¨³å®šè®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07747', 'title': 'Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets', 'url': 'https://huggingface.co/papers/2505.07747', 'abstract': 'While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.', 'score': 46, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'd9ffe741ebae4acb', 'authors': ['Weiyu Li', 'Xuanyang Zhang', 'Zheng Sun', 'Di Qi', 'Hao Li', 'Wei Cheng', 'Weiwei Cai', 'Shihao Wu', 'Jiarui Liu', 'Zihao Wang', 'Xiao Chen', 'Feipeng Tian', 'Jianxiong Pan', 'Zeming Li', 'Gang Yu', 'Xiangyu Zhang', 'Daxin Jiang', 'Ping Tan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07747.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#data', '#diffusion', '#transfer_learning', '#3d', '#benchmark'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ AI-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Step1X-3D - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ VAE-DiT Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Step1X-3D Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸Ğ· 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº 3D-ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ.'}, 'en': {'title': 'Revolutionizing 3D Generation with Step1X-3D', 'desc': 'The paper introduces Step1X-3D, a framework designed to improve 3D generation in artificial intelligence. It tackles challenges like limited data and algorithmic issues by creating a high-quality dataset and employing a two-stage architecture that combines a geometry generator and a texture synthesis module. The framework allows for better detail preservation and consistency in 3D assets by integrating techniques from 2D generation. By providing open-source resources, it aims to enhance research and development in controllable 3D asset generation.'}, 'zh': {'title': 'Step1X-3Dï¼šå¼€åˆ›å¯æ§3Dç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†Step1X-3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³3Dç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç¼ºã€ç®—æ³•é™åˆ¶å’Œç”Ÿæ€ç³»ç»Ÿç¢ç‰‡åŒ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œå¤„ç†è¶…è¿‡500ä¸‡èµ„äº§ï¼Œåˆ›å»ºäº†ä¸€ä¸ª200ä¸‡é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ ‡å‡†åŒ–çš„å‡ ä½•å’Œçº¹ç†å±æ€§ã€‚å®ƒç»“åˆäº†æ··åˆVAE-DiTå‡ ä½•ç”Ÿæˆå™¨å’ŒåŸºäºæ‰©æ•£çš„çº¹ç†åˆæˆæ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„3Dæ¨¡å‹ã€‚Step1X-3Dè¿˜æ”¯æŒå°†2Dæ§åˆ¶æŠ€æœ¯ç›´æ¥è½¬ç§»åˆ°3Dåˆæˆï¼Œæ¨åŠ¨äº†å¯æ§3Dèµ„äº§ç”Ÿæˆçš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07787', 'title': 'Learning from Peers in Reasoning Models', 'url': 'https://huggingface.co/papers/2505.07787', 'abstract': 'Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP\'s robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .', 'score': 34, 'issue_id': 3722, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '350f28f20ab516fc', 'authors': ['Tongxu Luo', 'Wenyu Du', 'Jiaxi Bi', 'Stephen Chung', 'Zhengyang Tang', 'Hao Yang', 'Min Zhang', 'Benyou Wang'], 'affiliations': ['DualityRL', 'Huawei', 'The Chinese University of Hong Kong, Shenzhen', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2505.07787.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#small_models', '#reasoning', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼: ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ°', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ² 'Ğ»Ğ¾Ğ²ÑƒÑˆĞºÑƒ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°', ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑˆĞ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñƒ ÑĞ²ĞµÑ€ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²' (LeaP), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LeaP-T, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LeaP."}, 'en': {'title': 'Empowering LRMs through Peer Collaboration', 'desc': "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."}, 'zh': {'title': 'åŒä¼´å­¦ä¹ ï¼šæå‡æ¨ç†æ¨¡å‹çš„è‡ªæˆ‘çº æ­£èƒ½åŠ›', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·æœ‰è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ï¼Œä½†å½“æ¨ç†è¿‡ç¨‹ä»¥çŸ­è€Œå·®çš„å¼€å¤´å¼€å§‹æ—¶ï¼Œæ¨¡å‹å¾ˆéš¾æ¢å¤ã€‚æˆ‘ä»¬ç§°è¿™ç§ç°è±¡ä¸ºâ€œå‰ç¼€ä¸»å¯¼é™·é˜±â€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»åŒä¼´å­¦ä¹ â€ï¼ˆLeaPï¼‰ï¼Œé€šè¿‡è·¯ç”±æœºåˆ¶è®©æ¯ä¸ªæ¨ç†è·¯å¾„æ€»ç»“å…¶ä¸­é—´æ¨ç†å¹¶ä¸å…¶ä»–è·¯å¾„å…±äº«ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èå…¥åŒä¼´çš„è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeaPæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸åŒä»»åŠ¡éš¾åº¦æ—¶å±•ç°å‡ºå¼ºå¤§çš„é”™è¯¯å®¹å¿èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07447', 'title': 'Unified Continuous Generative Models', 'url': 'https://huggingface.co/papers/2505.07447', 'abstract': 'Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.', 'score': 31, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '8d18ef028e9905b1', 'authors': ['Peng Sun', 'Yi Jiang', 'Tao Lin'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07447.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¸ flow-matching, Ñ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ consistency models. Ğ˜Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, UCGM-{T,S}, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ImageNet 256x256, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, UCGM-S ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ FID Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unifying Generative Models for Superior Performance', 'desc': 'This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.'}, 'zh': {'title': 'ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ï¼Œæå‡ç”Ÿæˆæ€§èƒ½ï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„è¿ç»­ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆå¤šæ­¥å’Œå°‘æ­¥ç”Ÿæˆæ–¹æ³•çš„è®­ç»ƒå’Œé‡‡æ ·ã€‚é€šè¿‡å¼•å…¥ç»Ÿä¸€çš„è®­ç»ƒå’Œé‡‡æ ·å™¨ï¼ˆUCGM-{T,S}ï¼‰ï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒUCGM-Tèƒ½å¤Ÿåœ¨20æ­¥å†…å°†å¤šæ­¥æ¨¡å‹çš„FIDé™ä½åˆ°1.30ï¼Œè€Œå°‘æ­¥æ¨¡å‹åœ¨ä»…2æ­¥å†…è¾¾åˆ°1.42çš„FIDã€‚æ­¤å¤–ï¼Œä½¿ç”¨UCGM-Så¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼ŒFIDä»250æ­¥çš„1.26é™è‡³ä»…40æ­¥çš„1.06ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.06548', 'title': 'REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback', 'url': 'https://huggingface.co/papers/2505.06548', 'abstract': 'Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.', 'score': 26, 'issue_id': 3722, 'pub_date': '2025-05-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ', 'en': 'May 10', 'zh': '5æœˆ10æ—¥'}, 'hash': 'db28335cad79db53', 'authors': ['Aniruddha Roy', 'Pretam Ray', 'Abhilash Nandy', 'Somak Aditya', 'Pawan Goyal'], 'affiliations': ['Indian Institute of Technology, Kharagpur'], 'pdf_title_img': 'assets/pdf/title_img/2505.06548.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#small_models', '#rl', '#dataset', '#data'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğœ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLaMA 2-7B, LLama 2-13B, Mistral 7B) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² 63-66% Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering LLMs with Cost-Effective Instruction Generation', 'desc': 'This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods.'}, 'zh': {'title': 'é«˜æ•ˆç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œæå‡LLMsæ€§èƒ½', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æŒ‡ä»¤é©±åŠ¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¼€æºçš„å°å‹LLMsï¼ˆå¦‚LLaMA 2-7Bã€LLaMA 2-13Bå’ŒMistral 7Bï¼‰æ¥ç”ŸæˆæŒ‡ä»¤æ•°æ®é›†ï¼Œä»è€Œå‡å°‘äººå·¥å¹²é¢„å’Œæˆæœ¬ã€‚é€šè¿‡å¼•å…¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç®—æ³•ï¼Œç ”ç©¶è¡¨æ˜è¿™ç§æ–¹æ³•åœ¨63-66%çš„ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤æ•°æ®æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07293', 'title': 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection', 'url': 'https://huggingface.co/papers/2505.07293', 'abstract': "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.", 'score': 17, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '7403ae602b400fc4', 'authors': ['Kai Hua', 'Steven Wu', 'Ge Zhang', 'Ke Shen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.07293.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#data', '#optimization', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¯Ğœ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AttentionInfluence Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ AttentionInfluence Ğº ĞºĞ¾Ñ€Ğ¿ÑƒÑÑƒ SmolLM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with AttentionInfluence', 'desc': 'This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.'}, 'zh': {'title': 'æ— ç›‘ç£æ¨ç†æ•°æ®é€‰æ‹©çš„æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œç ”ç©¶è€…ä»¬è¶Šæ¥è¶Šå…³æ³¨æ”¶é›†æ¨ç†å¯†é›†å‹çš„é¢„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç›‘ç£åˆ†ç±»å™¨æ¥è¯†åˆ«è¿™äº›æ•°æ®ï¼Œè¿™éœ€è¦äººç±»æˆ–LLMsè¿›è¡Œæ ‡æ³¨ï¼Œå¸¸å¸¸å¼•å…¥é¢†åŸŸç‰¹å®šçš„åè§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAttentionInfluenceçš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ— ç›‘ç£è®­ç»ƒæ–¹æ³•ã€‚é€šè¿‡ç®€å•çš„æ³¨æ„åŠ›å¤´å±è”½æ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—ä¸€ä¸ªå°å‹çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹èƒ½å¤Ÿä½œä¸ºå¼ºå¤§çš„æ•°æ®é€‰æ‹©å™¨ï¼Œä»è€Œåœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07818', 'title': 'DanceGRPO: Unleashing GRPO on Visual Generation', 'url': 'https://huggingface.co/papers/2505.07818', 'abstract': 'Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.', 'score': 16, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '023078250e0d651f', 'authors': ['Zeyue Xue', 'Jie Wu', 'Yu Gao', 'Fangyuan Kong', 'Lingting Zhu', 'Mengzhao Chen', 'Zhiheng Liu', 'Wei Liu', 'Qiushan Guo', 'Weilin Huang', 'Ping Luo'], 'affiliations': ['ByteDance Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.07818.jpg', 'data': {'categories': ['#alignment', '#optimization', '#video', '#multimodal', '#rl', '#diffusion', '#benchmark', '#rlhf'], 'emoji': 'ğŸ¨', 'ru': {'title': 'DanceGRPO: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DanceGRPO - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Group Relative Policy Optimization (GRPO) Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. DanceGRPO ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. DanceGRPO ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'DanceGRPO: Unifying Reinforcement Learning for Visual Generation', 'desc': 'This paper presents DanceGRPO, a novel framework that enhances visual content generation by integrating Group Relative Policy Optimization (GRPO) with generative models like diffusion models and rectified flows. It addresses key challenges in reinforcement learning (RL) for visual generation, such as instability during training and compatibility with modern sampling methods. DanceGRPO is versatile, supporting multiple tasks including text-to-image and video generation, and utilizes various foundational and reward models to improve output quality. The framework shows significant performance improvements over existing methods, making it a promising solution for aligning generative models with human preferences in visual synthesis.'}, 'zh': {'title': 'DanceGRPOï¼šè§†è§‰ç”Ÿæˆçš„ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†DanceGRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åº”ç”¨äºè§†è§‰ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨ç°ä»£å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰é‡‡æ ·ã€è®­ç»ƒç¨³å®šæ€§å’Œè§†é¢‘ç”ŸæˆéªŒè¯æ–¹é¢çš„å±€é™æ€§ã€‚DanceGRPOèƒ½å¤Ÿåœ¨æ‰©æ•£æ¨¡å‹å’Œä¿®æ­£æµç­‰å¤šç§ç”ŸæˆèŒƒå¼ä¸­æ— ç¼é€‚åº”ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŸºçº¿ï¼Œå±•ç¤ºäº†åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07263', 'title': 'Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2505.07263', 'abstract': 'We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.', 'score': 15, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '9735af402c0df35f', 'authors': ['Xiaokun Wang', 'Chris', 'Jiangbo Pei', 'Wei Shen', 'Yi Peng', 'Yunzhuo Hao', 'Weijie Qiu', 'Ai Jian', 'Tianyidan Xie', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07263.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#alignment', '#open_source', '#training', '#architecture', '#dataset'], 'emoji': 'ğŸŒŸ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Skywork-VL Reward - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Qwen2.5-VL-7B-Instruct Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹. Skywork-VL Reward Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Mixed Preference Optimization.'}, 'en': {'title': 'Skywork-VL Reward: Advancing Multimodal Understanding and Reasoning', 'desc': 'The paper introduces Skywork-VL Reward, a novel multimodal reward model designed to enhance understanding and reasoning across different types of data, such as text and images. It utilizes a large-scale dataset that captures diverse tasks, with feedback gathered from both traditional vision-language models and more sophisticated reasoning models. The architecture is built on Qwen2.5-VL-7B-Instruct, featuring a reward head and employing multi-stage fine-tuning with pairwise ranking loss to optimize performance. Experimental results demonstrate that this model not only excels in multimodal tasks but also improves training for Mixed Preference Optimization, marking a significant step forward in creating effective reward models for multimodal alignment.'}, 'zh': {'title': 'Skywork-VL Rewardï¼šå¤šæ¨¡æ€å¯¹é½çš„çªç ´æ€§è¿›å±•', 'desc': 'æˆ‘ä»¬æå‡ºäº†Skywork-VL Rewardï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºå¤šæ¨¡æ€ç†è§£å’Œæ¨ç†ä»»åŠ¡æä¾›å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬çš„æŠ€æœ¯æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€åå¥½æ•°æ®é›†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„ä»»åŠ¡å’Œåœºæ™¯ï¼Œæ•°æ®æ¥è‡ªæ ‡å‡†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨ç†æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºQwen2.5-VL-7B-Instructçš„å¥–åŠ±æ¨¡å‹æ¶æ„ï¼Œé›†æˆäº†å¥–åŠ±å¤´ï¼Œå¹¶åœ¨æˆå¯¹åå¥½æ•°æ®ä¸Šåº”ç”¨äº†å¤šé˜¶æ®µå¾®è°ƒã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSkywork-VL Rewardåœ¨å¤šæ¨¡æ€VL-RewardBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨æ–‡æœ¬å¥–åŠ±åŸºå‡†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03733', 'title': 'WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch', 'url': 'https://huggingface.co/papers/2505.03733', 'abstract': "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.", 'score': 15, 'issue_id': 3722, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '1d5e56d00ea8d485', 'authors': ['Zimu Lu', 'Yunqiao Yang', 'Houxing Ren', 'Haotian Hou', 'Han Xiao', 'Ke Wang', 'Weikang Shi', 'Aojun Zhou', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Multimedia Laboratory (MMLab), The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.03733.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#agents', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'WebGen-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸', 'desc': 'WebGen-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ñ‹ Ñ Ğ½ÑƒĞ»Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GPT-4o Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ. Ğ›ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ - Bolt.diy Ñ DeepSeek-R1 - Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 27,8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°.'}, 'en': {'title': 'Benchmarking LLMs for Website Code Generation', 'desc': 'This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models.'}, 'zh': {'title': 'è¯„ä¼°LLMä»£ç†ç”Ÿæˆç½‘ç«™ä»£ç çš„æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•WebGen-Benchï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†åœ¨ä»é›¶å¼€å§‹åˆ›å»ºå¤šæ–‡ä»¶ç½‘ç«™ä»£ç åº“çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«å¤šæ ·åŒ–çš„ç½‘ç«™ç”ŸæˆæŒ‡ä»¤ï¼Œæ¶µç›–äº†ä¸‰å¤§ç±»å’Œåä¸‰å°ç±»ï¼Œå‡ ä¹åŒ…æ‹¬æ‰€æœ‰é‡è¦ç±»å‹çš„Webåº”ç”¨ç¨‹åºã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆç½‘ç«™çš„è´¨é‡ï¼Œä½¿ç”¨GPT-4oç”Ÿæˆé’ˆå¯¹æ¯ä¸ªåŠŸèƒ½çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æ‰‹åŠ¨è¿‡æ»¤å’Œè°ƒæ•´ï¼Œæœ€ç»ˆå½¢æˆ647ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚é€šè¿‡å¼ºå¤§çš„ç½‘é¡µå¯¼èˆªä»£ç†è‡ªåŠ¨æ‰§è¡Œæµ‹è¯•ï¼Œè¯„ä¼°ç”Ÿæˆç½‘ç«™çš„å“åº”æ˜¯å¦ç¬¦åˆé¢„æœŸç»“æœï¼Œç»“æœæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ç»„åˆçš„å‡†ç¡®ç‡ä»…ä¸º27.8%ï¼Œæ˜¾ç¤ºå‡ºåŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07796', 'title': 'Learning Dynamics in Continual Pre-Training for Large Language Models', 'url': 'https://huggingface.co/papers/2505.07796', 'abstract': 'Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.', 'score': 12, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'c63d617be0b4d13a', 'authors': ['Xingjin Wang', 'Howe Tissue', 'Lu Wang', 'Linjing Li', 'Daniel Dajun Zeng'], 'affiliations': ['Ritzz-AI', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07796.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ (CPT) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ CPT, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ CPT.'}, 'en': {'title': 'Unlocking the Dynamics of Continual Pre-Training', 'desc': "This paper investigates the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on how performance in general and specific domains changes during training. The authors analyze the CPT loss curve, revealing that it represents a transition between different performance states influenced by distribution shifts and learning rate adjustments. They propose a scaling law that predicts loss across various training steps and learning rate schedules, providing insights into key factors like peak learning rate and replay ratio. The findings are validated through extensive experiments, showing the law's applicability across different datasets and training configurations."}, 'zh': {'title': 'æŒç»­é¢„è®­ç»ƒçš„åŠ¨æ€ä¸ä¼˜åŒ–æ³•åˆ™', 'desc': 'æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ˜¯ä¸€ç§å°†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ï¼Œç‰¹åˆ«å…³æ³¨åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œé€šç”¨æ€§èƒ½å’Œä¸‹æ¸¸é¢†åŸŸæ€§èƒ½çš„æ¼”å˜ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°CPTæŸå¤±æ›²çº¿æœ¬è´¨ä¸Šæè¿°äº†ä»ä¸€ä¸ªæ›²çº¿åˆ°å¦ä¸€ä¸ªéšè—æ›²çº¿çš„è¿‡æ¸¡ï¼Œå¹¶é€šè¿‡è§£è€¦åˆ†å¸ƒå˜åŒ–å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“æ¥è¿›è¡Œæè¿°ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ç§CPTç¼©æ”¾æ³•åˆ™ï¼Œç»“åˆäº†è¿™ä¸¤ä¸ªå› ç´ ï¼Œä½¿å¾—èƒ½å¤Ÿé¢„æµ‹åœ¨ä»»ä½•ï¼ˆæŒç»­ï¼‰è®­ç»ƒæ­¥éª¤å’Œå­¦ä¹ ç‡è°ƒåº¦ä¸‹çš„æŸå¤±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07596', 'title': 'Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent', 'url': 'https://huggingface.co/papers/2505.07596', 'abstract': 'Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.', 'score': 10, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '10063104a79da512', 'authors': ['Ziyang Huang', 'Xiaowei Yuan', 'Yiming Ju', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.07596.jpg', 'data': {'categories': ['#reasoning', '#rag', '#agents', '#optimization', '#hallucinations', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞºĞ°Ñ‚ÑŒ, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒÑÑ ÑĞµĞ±Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° retrieval-augmented generation (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° IKEA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IKEA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Optimizing Knowledge Use in Language Models with IKEA', 'desc': 'This paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), a novel approach to enhance the performance of Large Language Models (LLMs) by optimizing their retrieval processes. IKEA intelligently determines when to use its internal knowledge versus when to perform external searches, reducing unnecessary retrievals and improving inference speed. The model employs a unique reward function that encourages effective use of internal knowledge while still allowing for external retrieval when needed. Evaluations show that IKEA not only outperforms existing methods but also generalizes well across various knowledge reasoning tasks.'}, 'zh': {'title': 'æ™ºèƒ½æ£€ç´¢ï¼Œä¼˜åŒ–çŸ¥è¯†åˆ©ç”¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œåä¸ºIKEAï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ£€ç´¢èƒ½åŠ›ã€‚IKEAèƒ½å¤Ÿè¯†åˆ«è‡ªèº«çŸ¥è¯†çš„è¾¹ç•Œï¼Œå¹¶ä¼˜å…ˆä½¿ç”¨å†…éƒ¨çŸ¥è¯†ï¼Œåªæœ‰åœ¨å†…éƒ¨çŸ¥è¯†ä¸è¶³æ—¶æ‰ä¼šè¿›è¡Œå¤–éƒ¨æ£€ç´¢ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„å¥–åŠ±å‡½æ•°å’Œè®­ç»ƒæ•°æ®é›†ï¼ŒIKEAèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å†—ä½™æ£€ç´¢ï¼Œæé«˜å›ç­”çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIKEAåœ¨å¤šä¸ªçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†æ£€ç´¢é¢‘ç‡ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.06176', 'title': "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", 'url': 'https://huggingface.co/papers/2505.06176', 'abstract': 'Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.', 'score': 7, 'issue_id': 3733, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ', 'en': 'May 9', 'zh': '5æœˆ9æ—¥'}, 'hash': '884e34691df4b88b', 'authors': ['Niladri Shekhar Dutt', 'Duygu Ceylan', 'Niloy J. Mitra'], 'affiliations': ['Adobe Research, UK', 'University College London, Adobe Research, UK', 'University College London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.06176.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#dataset', '#data', '#multimodal', '#interpretability', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµÑ‚ÑƒÑˆÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹: MLLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑ‚ÑƒÑˆĞ¸ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‹Ñ€Ñ‹Ğµ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. MLLM ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Photo Retouching with Intelligent Editing Guidance', 'desc': 'This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.'}, 'zh': {'title': 'åˆ©ç”¨å¤šæ¨¡æ€æ¨¡å‹æå‡ç…§ç‰‡ä¿®é¥°è´¨é‡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥æ”¹è¿›åŸå§‹ç…§ç‰‡çš„ä¿®é¥°è¿‡ç¨‹ã€‚æˆ‘ä»¬è®­ç»ƒMLLMç†è§£å›¾åƒå¤„ç†æ“ä½œï¼Œå¹¶é€šè¿‡è§£å†³è§†è§‰éš¾é¢˜æ¥å¢å¼ºå…¶æ“ä½œæ„è¯†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè§„åˆ’å’Œå»ºè®®ç¼–è¾‘åºåˆ—ï¼Œç¡®ä¿ä¿®é¥°æ“ä½œå¯¹ç”¨æˆ·å¯ç†è§£ï¼Œå¹¶ä¿ç•™å¯¹è±¡ç»†èŠ‚å’Œåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„ç”Ÿæˆå’Œç¨‹åºåŒ–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¯è§£é‡Šæ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07819', 'title': 'H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning', 'url': 'https://huggingface.co/papers/2505.07819', 'abstract': 'Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '9285a87dc24d7d07', 'authors': ['Yiyang Lu', 'Yufeng Tian', 'Zhecheng Yuan', 'Xianbang Wang', 'Pu Hua', 'Zhengrong Xue', 'Huazhe Xu'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute', 'Tsinghua University IIIS'], 'pdf_title_img': 'assets/pdf/title_img/2505.07819.jpg', 'data': {'categories': ['#diffusion', '#agents', '#robotics', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ - Ğ¢Ñ€Ğ¸ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ (H^3DP). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ: Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. H^3DP ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration', 'desc': 'This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.'}, 'zh': {'title': 'å¢å¼ºè§†è§‰ä¸åŠ¨ä½œç”Ÿæˆçš„ä¸‰å±‚æ¬¡å­¦ä¹ æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºä¸‰å±‚æ¬¡æ‰©æ•£ç­–ç•¥ï¼ˆH^{\textbf{3}DPï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œå¢å¼ºäº†è§†è§‰ç‰¹å¾ä¸åŠ¨ä½œç”Ÿæˆä¹‹é—´çš„ç»“åˆã€‚H^{3}DPåŒ…å«ä¸‰ä¸ªå±‚æ¬¡ï¼šåŸºäºæ·±åº¦ä¿¡æ¯çš„è¾“å…¥åˆ†å±‚ã€å¤šå°ºåº¦è§†è§‰è¡¨ç¤ºå’Œå±‚æ¬¡æ¡ä»¶æ‰©æ•£è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH^{3}DPåœ¨44ä¸ªä»¿çœŸä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å¹³å‡æé«˜äº†27.5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨å››ä¸ªå¤æ‚çš„åŒæ‰‹çœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07260', 'title': 'UMoE: Unifying Attention and FFN with Shared Experts', 'url': 'https://huggingface.co/papers/2505.07260', 'abstract': 'Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '6a820b0cd1c07ac4', 'authors': ['Yuanhang Yang', 'Chaozheng Wang', 'Jing Li'], 'affiliations': ['Hong Kong Polytechnic University, Hong Kong, China', 'Institute of Science Tokyo, Tokyo, Japan', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07260.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ MoE Ğ² Transformer: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (Sparse Mixture of Experts, MoE) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Transformer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ MoE Ğ´Ğ»Ñ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ², Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ UMoE, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° ÑÑ‡ĞµÑ‚ MoE Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. UMoE Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unifying MoE for Enhanced Transformer Performance', 'desc': 'This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.'}, 'zh': {'title': 'ç»Ÿä¸€æ³¨æ„åŠ›ä¸å‰é¦ˆç½‘ç»œçš„ç¨€ç–ä¸“å®¶æ¶æ„', 'desc': 'ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºæ‰©å±•Transformeræ¨¡å‹ã€‚è™½ç„¶æ—©æœŸçš„ç ”ç©¶ä¸»è¦å°†MoEåº”ç”¨äºå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢å°†MoEæ‰©å±•åˆ°æ³¨æ„åŠ›å±‚ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç°æœ‰çš„åŸºäºæ³¨æ„åŠ›çš„MoEå±‚éœ€è¦ä¸“é—¨çš„å®ç°ï¼Œå¹¶ä¸”ä¸åŸºäºFFNçš„å±‚ç›¸æ¯”ï¼Œæ€§èƒ½ä¸å°½å¦‚äººæ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ³¨æ„åŠ›æœºåˆ¶é‡æ„ï¼Œç»Ÿä¸€äº†æ³¨æ„åŠ›å±‚å’ŒFFNå±‚ä¸­çš„MoEè®¾è®¡ï¼Œæå‡ºçš„UMoEæ¶æ„é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„MoEå±‚å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†FFNå’Œæ³¨æ„åŠ›ç»„ä»¶ä¹‹é—´çš„é«˜æ•ˆå‚æ•°å…±äº«ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00612', 'title': 'Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation', 'url': 'https://huggingface.co/papers/2505.00612', 'abstract': 'In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\\em leakage} and {\\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.', 'score': 5, 'issue_id': 3733, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '98b0a79b86c5cd15', 'authors': ['D. Sculley', 'Will Cukierski', 'Phil Culliton', 'Sohier Dane', 'Maggie Demkin', 'Ryan Holbrook', 'Addison Howard', 'Paul Mooney', 'Walter Reade', 'Megan Risdal', 'Nate Keating'], 'affiliations': ['Kaggle, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.00612.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#evaluation'], 'emoji': 'ğŸ†', 'ru': {'title': 'Ğ¡Ğ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ˜Ğ˜ - ĞºĞ»ÑÑ‡ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ğ·Ğ¸Ñ Ğ² ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğº Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ˜Ğ˜ ĞºĞ°Ğº Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI', 'desc': 'This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.'}, 'zh': {'title': 'ç”Ÿæˆæ€§AIè¯„ä¼°çš„æ–°æ ‡å‡†ï¼šäººå·¥æ™ºèƒ½ç«èµ›çš„ä»·å€¼', 'desc': 'åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„å®è¯è¯„ä¼°æ­£é¢ä¸´å±æœºï¼Œå› ä¸ºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ è¯„ä¼°å’ŒåŸºå‡†ç­–ç•¥æ— æ³•æ»¡è¶³ç°ä»£ç”Ÿæˆæ€§AIæ¨¡å‹å’Œç³»ç»Ÿçš„è¯„ä¼°éœ€æ±‚ã€‚è¿™äº›æ¨¡å‹é€šå¸¸å…·æœ‰å‡ ä¹æ— é™çš„è¾“å…¥å’Œè¾“å‡ºç©ºé—´ï¼Œç¼ºä¹æ˜ç¡®çš„çœŸå®ç›®æ ‡ï¼Œå¹¶ä¸”åœ¨é¢„æµ‹æ—¶å¼ºçƒˆä¾èµ–äºä¹‹å‰æ¨¡å‹è¾“å‡ºçš„ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æŒ‡å‡ºï¼Œæ³„æ¼å’Œæ±¡æŸ“é—®é¢˜æ˜¯ç”Ÿæˆæ€§AIè¯„ä¼°ä¸­æœ€é‡è¦ä¸”æœ€éš¾è§£å†³çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œäººå·¥æ™ºèƒ½ç«èµ›é¢†åŸŸå·²ç»å‘å±•å‡ºæœ‰æ•ˆçš„æªæ–½æ¥åº”å¯¹æ³„æ¼é—®é¢˜ï¼Œå› æ­¤åº”å°†å…¶è§†ä¸ºç”Ÿæˆæ€§AIè¯„ä¼°çš„é»„é‡‘æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07812', 'title': 'Continuous Visual Autoregressive Generation via Score Maximization', 'url': 'https://huggingface.co/papers/2505.07812', 'abstract': 'Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.', 'score': 3, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '5635f18df39cf275', 'authors': ['Chenze Shao', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07812.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#training', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (VAR) Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Continuous VAR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (strictly proper scoring rules). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (energy score), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² VAR, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Visual Data Generation with Continuous VAR', 'desc': 'This paper presents a new approach called Continuous Visual AutoRegressive (VAR) modeling, which allows for the generation of visual data without converting it into discrete formats. Traditional methods often lose important information during quantization, but this framework uses strictly proper scoring rules to evaluate and optimize generative models directly in continuous space. By focusing on energy scores as training objectives, the method avoids the challenges of probabilistic predictions in continuous domains. Additionally, it shows that previous continuous autoregressive methods can be derived from this new framework, enhancing their applicability and performance.'}, 'zh': {'title': 'æ— é‡åŒ–çš„è¿ç»­è‡ªå›å½’ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºè‡ªå›å½’æ¨¡å‹ä¸»è¦ç”¨äºå¤„ç†ç¦»æ•£æ•°æ®ã€‚åœ¨å¤„ç†è¿ç»­æ•°æ®ï¼ˆå¦‚è§†è§‰æ•°æ®ï¼‰æ—¶ï¼Œè§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼ˆVARï¼‰é€šå¸¸éœ€è¦é€šè¿‡é‡åŒ–æ–¹æ³•å°†æ•°æ®è½¬æ¢ä¸ºç¦»æ•£ç©ºé—´ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿ç»­VARæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥è¿›è¡Œè§†è§‰è‡ªå›å½’ç”Ÿæˆï¼Œè€Œæ— éœ€å‘é‡é‡åŒ–ã€‚è¯¥æ¡†æ¶çš„ç†è®ºåŸºç¡€æ˜¯ä¸¥æ ¼é€‚å½“çš„è¯„åˆ†è§„åˆ™ï¼Œè¿™ä¸ºè¯„ä¼°ç”Ÿæˆæ¨¡å‹å¦‚ä½•é€¼è¿‘çœŸå®åˆ†å¸ƒæä¾›äº†å¼ºå¤§çš„ç»Ÿè®¡å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07793', 'title': 'Overflow Prevention Enhances Long-Context Recurrent LLMs', 'url': 'https://huggingface.co/papers/2505.07793', 'abstract': 'A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.', 'score': 3, 'issue_id': 3728, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'f4bfefd5343cbf0c', 'authors': ['Assaf Ben-Kish', 'Itamar Zimerman', 'M. Jehanzeb Mirza', 'James Glass', 'Leonid Karlinsky', 'Raja Giryes'], 'affiliations': ['IBM Research', 'MIT CSAIL', 'Tel Aviv University', 'Xero'], 'pdf_title_img': 'assets/pdf/title_img/2505.07793.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ§Ğ°Ğ½ĞºĞ¸ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑƒĞ±-ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ÑĞ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LongBench. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Long-Context Potential with Chunk-Based Inference', 'desc': 'This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.'}, 'zh': {'title': 'æå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œé•¿æ–‡æœ¬æ¨¡å‹ï¼ˆLLMsï¼‰å‘å±•å‡ºäº†ä¸€ç§æ–°çš„å­äºŒæ¬¡æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ•ˆç‡ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸»è¦çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬å›ºå®šå¤§å°çš„é€’å½’è®°å¿†å¦‚ä½•å½±å“æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿è¿™äº›æ¨¡å‹ç»è¿‡é•¿ä¸Šä¸‹æ–‡è®­ç»ƒï¼Œå®ƒä»¬å¯¹é•¿ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ä»ç„¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºçš„åŸºäºå—çš„æ¨ç†æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å¹¶å¤„ç†è¾“å…¥ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£é€’å½’è®°å¿†çš„ä¸è¶³ï¼Œå¹¶åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.06324', 'title': 'Document Attribution: Examining Citation Relationships using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.06324', 'abstract': "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.   To tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.", 'score': 3, 'issue_id': 3724, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ', 'en': 'May 9', 'zh': '5æœˆ9æ—¥'}, 'hash': 'd1b4a407c1a67da8', 'authors': ['Vipula Rawte', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka'], 'affiliations': ['Adobe Inc.', 'Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.06324.jpg', 'data': {'categories': ['#training', '#interpretability', '#multimodal', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² LLM: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ flan-ul2, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 0,27% Ğ¸ 2,4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AttributionBench. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ flan-t5-small, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ»Ğ¸Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ F1 Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑĞ»Ğ¾ÑÑ…, ĞºÑ€Ğ¾Ğ¼Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ….'}, 'en': {'title': 'Enhancing Trust in LLMs through Improved Attribution Techniques', 'desc': 'This paper addresses the challenges of trustworthiness and interpretability in Large Language Models (LLMs) when used for document-based tasks. It introduces two techniques for improving attribution, which is the process of linking model outputs back to their source documents. The first technique is a zero-shot approach that treats attribution as a textual entailment task, showing measurable improvements in performance. The second technique investigates how the attention mechanism in LLMs can enhance attribution accuracy, achieving better F1 scores in most layers of a smaller model.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯ä¿¡æ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æ¡£æ‘˜è¦ã€é—®ç­”å’Œä¿¡æ¯æå–ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œç¡®ä¿è¿™äº›ç³»ç»Ÿçš„å¯ä¿¡æ€§å’Œå¯è§£é‡Šæ€§å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å½’å› æ–¹æ³•ï¼Œé€šè¿‡è¿½è¸ªç”Ÿæˆçš„è¾“å‡ºå›åˆ°å…¶æºæ–‡æ¡£æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æŠ€æœ¯ï¼šä¸€ç§æ˜¯é›¶æ ·æœ¬æ–¹æ³•ï¼Œå°†å½’å› è§†ä¸ºç®€å•çš„æ–‡æœ¬è•´å«ä»»åŠ¡ï¼Œå¦ä¸€ç§æ˜¯æ¢ç´¢æ³¨æ„åŠ›æœºåˆ¶åœ¨å¢å¼ºå½’å› è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07291', 'title': 'INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.07291', 'abstract': 'We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training.', 'score': 2, 'issue_id': 3742, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'bf4c666ff6a739cf', 'authors': ['Prime Intellect Team', 'Sami Jaghouar', 'Justus Mattern', 'Jack Min Ong', 'Jannik Straube', 'Manveer Basra', 'Aaron Pazdera', 'Kushal Thaman', 'Matthew Di Ferrante', 'Felix Gabriel', 'Fares Obeid', 'Kemal Erdem', 'Michael Keiblinger', 'Johannes Hagemann'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07291.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#rl', '#dataset', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'INTELLECT-2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, INTELLECT-2 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¼, Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ·Ğ»Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¹ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PRIME-RL, ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ TOPLOC Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SHARDCAST. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° GRPO Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Reinforcement Learning with Distributed Training', 'desc': 'INTELLECT-2 is a groundbreaking reinforcement learning model that utilizes a globally distributed training approach for a 32 billion parameter language model. It employs asynchronous training across a diverse group of independent compute contributors, which is a shift from traditional centralized methods. The paper introduces innovative components like PRIME-RL for managing distributed training, TOPLOC for verifying data from untrusted sources, and SHARDCAST for efficient communication of model updates. By refining the GRPO training method and implementing effective data filtering, INTELLECT-2 achieves enhanced stability and performance, surpassing previous models in its category.'}, 'zh': {'title': 'INTELLECT-2ï¼šå…¨çƒåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†INTELLECT-2ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨çƒåˆ†å¸ƒå¼çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä½¿ç”¨äº†320äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„é›†ä¸­å¼è®­ç»ƒä¸åŒï¼ŒINTELLECT-2é€šè¿‡ä¸€ä¸ªåŠ¨æ€çš„ã€å¼‚æ„çš„è®¡ç®—è´¡çŒ®è€…ç¾¤ä½“ï¼Œé‡‡ç”¨å®Œå…¨å¼‚æ­¥çš„å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæ¨ç†æ¨¡å‹ã€‚ä¸ºäº†æ”¯æŒè¿™ç§ç‹¬ç‰¹çš„åŸºç¡€è®¾æ–½ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹æ„å»ºäº†å¤šä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬ä¸“ä¸ºåˆ†å¸ƒå¼å¼‚æ­¥å¼ºåŒ–å­¦ä¹ è®¾è®¡çš„PRIME-RLè®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠéªŒè¯ä¸å¯ä¿¡æ¨ç†å·¥ä½œè€…çš„TOPLOCå’Œé«˜æ•ˆå¹¿æ’­ç­–ç•¥æƒé‡çš„SHARDCASTã€‚æˆ‘ä»¬è¿˜å¯¹æ ‡å‡†çš„GRPOè®­ç»ƒæ–¹æ³•å’Œæ•°æ®è¿‡æ»¤æŠ€æœ¯è¿›è¡Œäº†ä¿®æ”¹ï¼Œä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶æˆåŠŸå®ç°æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ï¼Œä»è€Œåœ¨320äº¿å‚æ•°èŒƒå›´å†…è¶…è¶Šäº†ç°æœ‰çš„QwQ-32Bæ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07233', 'title': 'DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2505.07233', 'abstract': 'Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG', 'score': 2, 'issue_id': 3734, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': 'b6c55b4c738d5230', 'authors': ['Jiashuo Sun', 'Xianrui Zhong', 'Sizhe Zhou', 'Jiawei Han'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07233.jpg', 'data': {'categories': ['#rag', '#optimization', '#interpretability', '#rl'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'DynamicRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. DynamicRAG Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'DynamicRAG: Optimizing Document Retrieval for Better Generation', 'desc': "This paper introduces DynamicRAG, a new framework for retrieval-augmented generation (RAG) systems that enhances the quality of generated responses by optimizing the reranking of retrieved documents. The reranker in DynamicRAG is designed to dynamically adjust both the order and the number of documents based on the specific query, addressing the challenge of selecting the optimal number of documents. By employing reinforcement learning, the reranker uses feedback from the quality of the language model's output to improve its decisions. The results show that DynamicRAG outperforms existing methods across multiple knowledge-intensive datasets, achieving state-of-the-art performance."}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´ï¼Œæå‡ç”Ÿæˆè´¨é‡çš„RAGæ¡†æ¶', 'desc': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œé€‚ç”¨äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„RAGæ¡†æ¶DynamicRAGï¼Œå…¶ä¸­çš„é‡æ’åºå™¨èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æ£€ç´¢æ–‡æ¡£çš„é¡ºåºå’Œæ•°é‡ã€‚æˆ‘ä»¬å°†é‡æ’åºå™¨å»ºæ¨¡ä¸ºä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–çš„æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨LLMè¾“å‡ºè´¨é‡ä½œä¸ºå¥–åŠ±æ¥ä¼˜åŒ–é‡æ’åºå†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynamicRAGåœ¨ä¸ƒä¸ªçŸ¥è¯†å¯†é›†å‹æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04918', 'title': 'Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction', 'url': 'https://huggingface.co/papers/2505.04918', 'abstract': "Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.", 'score': 2, 'issue_id': 3728, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '73efdd4a4328c88d', 'authors': ['Jiaqi Zheng', 'Qing Ling', 'Yerong Feng'], 'affiliations': ['Shenzhen Institute of Meteorological Innovation', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04918.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#graphs', '#architecture', '#training'], 'emoji': 'ğŸŒ', 'ru': {'title': 'PASSAT: Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸', 'desc': 'PASSAT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ Ğ°Ñ‚Ğ¼Ğ¾ÑÑ„ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ²ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ ĞĞ°Ğ²ÑŒĞµ-Ğ¡Ñ‚Ğ¾ĞºÑĞ° Ğ½Ğ° ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸ Ğ¸ Ğ°Ñ‚Ğ¼Ğ¾ÑÑ„ĞµÑ€Ñ‹. PASSAT Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹, ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ²ĞµĞºÑ†Ğ¸Ğ¸. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ERA5 Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 5.625Â° PASSAT Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ĞºĞ°Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ IFS T42.'}, 'en': {'title': 'PASSAT: Bridging Physics and Topology for Superior Weather Prediction', 'desc': "The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth's surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns."}, 'zh': {'title': 'PASSATï¼šç»“åˆç‰©ç†ä¸æ‹“æ‰‘çš„å¤©æ°”é¢„æµ‹æ–°æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤©æ°”é¢„æµ‹æ¨¡å‹PASSATï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç‰©ç†å­¦å’Œåœ°å½¢ä¿¡æ¯ã€‚PASSATé€šè¿‡å¯¹æµè¿‡ç¨‹å’Œåœ°çƒ-å¤§æ°”ç›¸äº’ä½œç”¨æ¥æè¿°å¤©æ°”æ¼”å˜ï¼Œå¹¶è€ƒè™‘äº†åœ°çƒè¡¨é¢çš„æ‹“æ‰‘ç»“æ„ã€‚è¯¥æ¨¡å‹åœ¨çƒé¢æµå½¢ä¸Šæ•°å€¼æ±‚è§£å¯¹æµæ–¹ç¨‹å’Œçº³ç»´-æ–¯æ‰˜å…‹æ–¯æ–¹ç¨‹ï¼Œå¹¶åˆ©ç”¨çƒé¢å›¾ç¥ç»ç½‘ç»œæ•æ‰åœ°çƒ-å¤§æ°”çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPASSATåœ¨5.625åº¦åˆ†è¾¨ç‡çš„ERA5æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ å¤©æ°”é¢„æµ‹æ¨¡å‹å’Œæ“ä½œæ€§æ•°å€¼å¤©æ°”é¢„æµ‹æ¨¡å‹IFS T42ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04066', 'title': 'LLAMAPIE: Proactive In-Ear Conversation Assistants', 'url': 'https://huggingface.co/papers/2505.04066', 'abstract': 'We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.', 'score': 1, 'issue_id': 3739, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': 'f8bf204612751793', 'authors': ['Tuochao Chen', 'Nicholas Batchelder', 'Alisa Liu', 'Noah Smith', 'Shyamnath Gollakota'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.04066.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#dataset', '#data'], 'emoji': 'ğŸ§', 'ru': {'title': 'LlamaPIE: ĞĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¶Ğ¸Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'LlamaPIE - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğµ, ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»ÑƒÑ…Ğ¾Ğ²Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ‚Ğ¾Ñ‚ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ¿Ñ€ĞµĞ´ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ±ĞµĞ· Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LlamaPIE Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ¹, Ğ½ĞµĞ½Ğ°Ğ²ÑĞ·Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸.'}, 'en': {'title': 'LlamaPIE: Enhancing Conversations with Proactive Assistance', 'desc': 'LlamaPIE is a novel real-time proactive assistant that enhances human conversations by providing discreet guidance through hearable devices. Unlike traditional models that wait for user prompts, LlamaPIE anticipates user needs and offers assistance without interrupting the flow of conversation. The system tackles challenges such as timing for responses, generating concise and relevant replies, and utilizing user context for personalized support, all while processing information on-device. Evaluations show that users prefer LlamaPIE over reactive models, indicating its effectiveness in improving live interactions.'}, 'zh': {'title': 'LlamaPIEï¼šæå‡å¯¹è¯çš„ä¸»åŠ¨åŠ©æ‰‹', 'desc': 'LlamaPIE æ˜¯é¦–ä¸ªå®æ—¶ä¸»åŠ¨åŠ©æ‰‹ï¼Œæ—¨åœ¨é€šè¿‡å¯ç©¿æˆ´è®¾å¤‡åœ¨å¯¹è¯ä¸­æä¾›ç®€æ´çš„æŒ‡å¯¼ã€‚ä¸ä¼ ç»Ÿè¯­è¨€æ¨¡å‹ä¸åŒï¼Œå®ƒåœ¨åå°è¿è¡Œï¼Œèƒ½å¤Ÿé¢„æµ‹ç”¨æˆ·éœ€æ±‚è€Œä¸æ‰“æ–­å¯¹è¯ã€‚æˆ‘ä»¬è§£å†³äº†å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä½•æ—¶å“åº”ã€å¦‚ä½•ç”Ÿæˆç®€æ´çš„å›åº”ä»¥åŠå¦‚ä½•åˆ©ç”¨ç”¨æˆ·çŸ¥è¯†æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¸®åŠ©ã€‚é€šè¿‡æ„å»ºåŠåˆæˆå¯¹è¯æ•°æ®é›†å’Œæå‡ºåŒæ¨¡å‹ç®¡é“ï¼Œæˆ‘ä»¬åœ¨çœŸå®æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºå‡ºå¯¹ä¸»åŠ¨åŠ©æ‰‹çš„å¼ºçƒˆåå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07086', 'title': 'Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design', 'url': 'https://huggingface.co/papers/2505.07086', 'abstract': "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.", 'score': 0, 'issue_id': 3725, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ', 'en': 'May 11', 'zh': '5æœˆ11æ—¥'}, 'hash': 'a2fce171208a1e7a', 'authors': ['Tong Chen', 'Yinuo Zhang', 'Sophia Tang', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer Science, Fudan University', 'Management and Technology Program, University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2505.07086.jpg', 'data': {'categories': ['#training', '#dataset', '#science', '#data', '#optimization'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸. MOG-DFM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ğº ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ°Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ”ĞĞš Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. MOG-DFM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞ±Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Optimizing Biomolecule Design with MOG-DFM', 'desc': "This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering."}, 'zh': {'title': 'å¤šç›®æ ‡ä¼˜åŒ–ï¼ŒåŠ©åŠ›ç”Ÿç‰©åˆ†å­è®¾è®¡', 'desc': 'åœ¨ç”Ÿç‰©åˆ†å­å·¥ç¨‹ä¸­ï¼Œè®¾è®¡æ»¡è¶³å¤šç§åŠŸèƒ½å’Œç”Ÿç‰©ç‰©ç†æ ‡å‡†çš„ç”Ÿç‰©åºåˆ—ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šç›®æ ‡å¼•å¯¼ç¦»æ•£æµåŒ¹é…ï¼ˆMOG-DFMï¼‰çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ ‡é‡ç›®æ ‡ä¹‹é—´å®ç°å¸•ç´¯æ‰˜æœ‰æ•ˆçš„æƒè¡¡ã€‚MOG-DFMé€šè¿‡è®¡ç®—æ··åˆæ’åæ–¹å‘åˆ†æ•°å’Œåº”ç”¨è‡ªé€‚åº”è¶…é”¥è¿‡æ»¤å™¨ï¼Œæ¥å¼•å¯¼é¢„è®­ç»ƒçš„ç¦»æ•£æ—¶é—´æµåŒ¹é…ç”Ÿæˆå™¨è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚æˆ‘ä»¬å±•ç¤ºäº†MOG-DFMåœ¨ç”Ÿæˆä¼˜åŒ–çš„è‚½ç»“åˆç‰©å’Œç‰¹å®šå¢å¼ºå­ç±»DNAåºåˆ—æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤šå±æ€§å¼•å¯¼çš„ç”Ÿç‰©åˆ†å­åºåˆ—è®¾è®¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04921', 'title': 'Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2505.04921', 'abstract': "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.", 'score': 102, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': 'a07d92b81581eea3', 'authors': ['Yunxin Li', 'Zhenyu Liu', 'Zitao Li', 'Xuanyu Zhang', 'Zhenran Xu', 'Xinyu Chen', 'Haoyuan Shi', 'Shenyuan Jiang', 'Xintong Wang', 'Jifang Wang', 'Shouzheng Huang', 'Xinping Zhao', 'Borui Jiang', 'Lanqing Hong', 'Longyue Wang', 'Zhuotao Tian', 'Baoxing Huai', 'Wenhan Luo', 'Weihua Luo', 'Zheng Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.04921.jpg', 'data': {'categories': ['#rl', '#multimodal', '#benchmark', '#survey', '#agents', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğº ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LMRMs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğº ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (MCoT) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (N-LMRMs) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Empowering AI with Multimodal Reasoning for Real-World Intelligence', 'desc': 'This paper discusses the importance of reasoning in artificial intelligence, especially in complex environments with multiple types of data like text, images, and audio. It introduces Large Multimodal Reasoning Models (LMRMs) that combine these different data types to enhance reasoning capabilities. The authors provide a structured survey of the evolution of multimodal reasoning, from early task-specific models to more integrated language-centric frameworks. They also highlight ongoing challenges in generalization and reasoning depth, while proposing a roadmap for future research in developing more adaptive and intelligent systems.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ¨ç†ï¼šæ™ºèƒ½çš„æœªæ¥', 'desc': 'æ¨ç†æ˜¯æ™ºèƒ½çš„æ ¸å¿ƒï¼Œå½±å“å†³ç­–ã€ç»“è®ºå’Œè·¨é¢†åŸŸçš„æ¦‚æ‹¬èƒ½åŠ›ã€‚åœ¨äººå·¥æ™ºèƒ½ä¸­ï¼Œéšç€ç³»ç»Ÿåœ¨å¼€æ”¾ã€ä¸ç¡®å®šå’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­è¿è¡Œï¼Œæ¨ç†å˜å¾—è‡³å…³é‡è¦ï¼Œä»¥å®ç°ç¨³å¥å’Œé€‚åº”æ€§çš„è¡Œä¸ºã€‚å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆLMRMsï¼‰æ•´åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€ï¼Œæ”¯æŒå¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨å®ç°å…¨é¢æ„ŸçŸ¥ã€ç²¾ç¡®ç†è§£å’Œæ·±åº¦æ¨ç†ã€‚æœ¬æ–‡å¯¹å¤šæ¨¡æ€æ¨ç†ç ”ç©¶è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæå‡ºäº†ä¸€ä¸ªå››é˜¶æ®µçš„å‘å±•è·¯çº¿å›¾ï¼Œåæ˜ äº†è¯¥é¢†åŸŸè®¾è®¡ç†å¿µçš„å˜åŒ–å’Œæ–°å…´èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04620', 'title': 'On Path to Multimodal Generalist: General-Level and General-Bench', 'url': 'https://huggingface.co/papers/2505.04620', 'abstract': 'The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/', 'score': 62, 'issue_id': 3671, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '57991e528141671e', 'authors': ['Hao Fei', 'Yuan Zhou', 'Juncheng Li', 'Xiangtai Li', 'Qingshan Xu', 'Bobo Li', 'Shengqiong Wu', 'Yaoting Wang', 'Junbao Zhou', 'Jiahao Meng', 'Qingyu Shi', 'Zhiyuan Zhou', 'Liangtao Shi', 'Minghe Gao', 'Daoan Zhang', 'Zhiqi Ge', 'Weiming Wu', 'Siliang Tang', 'Kaihang Pan', 'Yaobo Ye', 'Haobo Yuan', 'Tao Zhang', 'Tianjie Ju', 'Zixiang Meng', 'Shilin Xu', 'Liyu Jia', 'Wentao Hu', 'Meng Luo', 'Jiebo Luo', 'Tat-Seng Chua', 'Shuicheng Yan', 'Hanwang Zhang'], 'affiliations': ['HFUT', 'KAUST', 'NJU', 'NTU', 'NUS', 'PKU', 'SJTU', 'UR', 'WHU', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2505.04620.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agi'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº AGI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ General-Level. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ 5 ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ MLLM, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ General-Bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 700 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 325 800 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Towards Multimodal Generalists: Evaluating MLLM Progress', 'desc': 'The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to measure the progress of MLLMs towards achieving artificial general intelligence (AGI).'}, 'zh': {'title': 'è¿ˆå‘çœŸæ­£çš„å¤šæ¨¡æ€é€šç”¨äººå·¥æ™ºèƒ½', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆè¿›èƒ½åŠ›ã€‚ç°æœ‰çš„MLLMæ­£æœç€å¤šæ¨¡æ€é€šç”¨ä¸»ä¹‰è€…çš„æ–¹å‘æ¼”å˜ï¼Œä¸ä»…èƒ½å¤Ÿç†è§£å¤šç§æ¨¡æ€ï¼Œè¿˜èƒ½åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ç”Ÿæˆå†…å®¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶â€”â€”General-Levelï¼Œå®šä¹‰äº†MLLMæ€§èƒ½å’Œé€šç”¨æ€§çš„äº”ä¸ªç­‰çº§ï¼Œä»¥ä¾¿æ¯”è¾ƒä¸åŒæ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡General-Benchï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ›´å¹¿æ³›çš„æŠ€èƒ½å’Œä»»åŠ¡è¯„ä¼°ï¼Œæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€é€šç”¨æ¨¡å‹åœ¨å®ç°çœŸæ­£äººå·¥æ™ºèƒ½æ–¹é¢çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05470', 'title': 'Flow-GRPO: Training Flow Matching Models via Online RL', 'url': 'https://huggingface.co/papers/2505.05470', 'abstract': "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.", 'score': 44, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '8db85b6df75e7479', 'authors': ['Jie Liu', 'Gongye Liu', 'Jiajun Liang', 'Yangguang Li', 'Jiaheng Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wanli Ouyang'], 'affiliations': ['CUHK MMLab', 'Kuaishou Technology', 'Nanjing University', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05470.jpg', 'data': {'categories': ['#rl', '#cv', '#rlhf', '#alignment', '#games', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: RL Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Flow-GRPO â€“ ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞĞ”Ğ£ Ğ² Ğ¡Ğ”Ğ£ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Flow-GRPO Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Flow-GRPO', 'desc': 'Flow-GRPO is a novel method that combines online reinforcement learning with flow matching models to enhance performance in text-to-image tasks. It employs an ODE-to-SDE conversion to facilitate statistical sampling, allowing for better exploration in reinforcement learning. Additionally, the Denoising Reduction strategy optimizes training efficiency by minimizing unnecessary denoising steps while maintaining inference quality. The results show significant improvements in accuracy and human preference alignment, with minimal risk of reward hacking, ensuring high-quality and diverse outputs.'}, 'zh': {'title': 'Flow-GRPOï¼šå¼ºåŒ–å­¦ä¹ ä¸æµåŒ¹é…çš„å®Œç¾ç»“åˆ', 'desc': 'æˆ‘ä»¬æå‡ºäº†Flow-GRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é›†æˆåˆ°æµåŒ¹é…æ¨¡å‹ä¸­çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸¤ä¸ªå…³é”®ç­–ç•¥ï¼šé¦–å…ˆï¼Œé€šè¿‡å°†ç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¬æ¢ä¸ºç­‰æ•ˆçš„éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰ï¼Œå®ç°äº†åœ¨æ‰€æœ‰æ—¶é—´æ­¥é•¿ä¸Šä¸åŸå§‹æ¨¡å‹çš„è¾¹é™…åˆ†å¸ƒåŒ¹é…ï¼Œä»è€Œæ”¯æŒRLæ¢ç´¢çš„ç»Ÿè®¡é‡‡æ ·ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨å»å™ªå‡å°‘ç­–ç•¥ï¼Œåœ¨ä¿æŒåŸå§‹æ¨ç†æ—¶é—´æ­¥æ•°çš„åŒæ—¶å‡å°‘è®­ç»ƒå»å™ªæ­¥éª¤ï¼Œæ˜¾è‘—æé«˜äº†é‡‡æ ·æ•ˆç‡è€Œä¸é™ä½æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒFlow-GRPOåœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤æ‚ç»„åˆä¸­ï¼ŒRLè°ƒä¼˜çš„SD3.5å‡ ä¹å®Œç¾åœ°ç”Ÿæˆäº†å¯¹è±¡æ•°é‡ã€ç©ºé—´å…³ç³»å’Œç»†ç²’åº¦å±æ€§ï¼Œæ˜¾è‘—æé«˜äº†GenEvalçš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02847', 'title': 'Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.02847', 'abstract': "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.", 'score': 22, 'issue_id': 3671, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '9204f0ca97eb8bc7', 'authors': ['Bang Zhang', 'Ruotian Ma', 'Qingxuan Jiang', 'Peisong Wang', 'Jiaqi Chen', 'Zheng Xie', 'Xingyu Chen', 'Yue Wang', 'Fanghua Ye', 'Jian Li', 'Yifan Yang', 'Zhaopeng Tu', 'Xiaolong Li'], 'affiliations': ['Hunyuan AI Digital Human, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.02847.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#alignment', '#agents', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SAGE: Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¿Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAGE - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚. SAGE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 'Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ĞĞ³ĞµĞ½Ñ‚Ğ°', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ SAGE ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° LLM, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."}, 'en': {'title': 'Measuring Empathy in AI: The SAGE Framework', 'desc': 'The paper presents SAGE, an automated evaluation framework designed to assess how well large language models (LLMs) understand human emotions and social interactions. SAGE simulates a Sentient Agent that mimics human emotional responses and thoughts during conversations, allowing for a more nuanced evaluation of LLMs in multi-turn dialogues. By tracking emotional changes and reasoning about responses, SAGE generates a numerical emotion trajectory that correlates with established psychological metrics. The framework reveals significant performance gaps among various LLMs, highlighting the need for better measures of empathy and social cognition in AI systems.'}, 'zh': {'title': 'è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSAGEçš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»æƒ…æ„Ÿå’Œç¤¾äº¤è®¤çŸ¥çš„ç†è§£èƒ½åŠ›ã€‚SAGEé€šè¿‡æ¨¡æ‹Ÿäººç±»æƒ…æ„Ÿå˜åŒ–å’Œå†…å¿ƒæƒ³æ³•ï¼Œæä¾›äº†æ›´çœŸå®çš„å¤šè½®å¯¹è¯è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGEçš„æƒ…æ„Ÿè¯„åˆ†ä¸å¿ƒç†å­¦è¯„ä¼°å·¥å…·çš„è¯„åˆ†é«˜åº¦ç›¸å…³ï¼ŒéªŒè¯äº†å…¶å¿ƒç†å­¦çš„çœŸå®æ€§ã€‚è¯¥æ¡†æ¶è¿˜å»ºç«‹äº†ä¸€ä¸ªå…¬å¼€çš„Sentientæ’è¡Œæ¦œï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—å·®è·ï¼Œæ¨åŠ¨äº†å¯¹æ›´å…·åŒç†å¿ƒå’Œç¤¾äº¤èƒ½åŠ›çš„è¯­è¨€ä»£ç†çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05315', 'title': 'Scalable Chain of Thoughts via Elastic Reasoning', 'url': 'https://huggingface.co/papers/2505.05315', 'abstract': 'Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.', 'score': 18, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '0ce3be6057da3ed2', 'authors': ['Yuhui Xu', 'Hanze Dong', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Caiming Xiong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05315.jpg', 'data': {'categories': ['#plp', '#training', '#reasoning', '#optimization', '#benchmark', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ­Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ' Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ„Ğ°Ğ·Ñ‹ - Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ - Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ² GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 'Ğ­Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ' Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."}, 'en': {'title': 'Elastic Reasoning: Scalable and Efficient Thought Processes for ML Models', 'desc': 'This paper introduces Elastic Reasoning, a framework designed to enhance the performance of large reasoning models (LRMs) under strict resource constraints. It separates the reasoning process into two distinct phases: thinking and solution, allowing for independent budget allocation for each phase. The framework employs a budget-constrained rollout strategy that helps models adaptively reason even when the thinking phase is limited, ensuring reliability in various scenarios. Empirical results show that Elastic Reasoning not only meets budget constraints effectively but also reduces training costs while improving the efficiency of reasoning outputs.'}, 'zh': {'title': 'å¼¹æ€§æ¨ç†ï¼šå¯æ§æ¨ç†çš„æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è¾“å‡ºé•¿åº¦ä¸å—æ§åˆ¶ï¼Œç»™å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¼¹æ€§æ¨ç†çš„æ–°æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºæ€è€ƒå’Œè§£å†³ä¸¤ä¸ªé˜¶æ®µï¼Œå¹¶ä¸ºæ¯ä¸ªé˜¶æ®µåˆ†é…ç‹¬ç«‹çš„é¢„ç®—ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œå¼¹æ€§æ¨ç†ä¼˜å…ˆè€ƒè™‘è§£å†³æ–¹æ¡ˆçš„å®Œæ•´æ€§ï¼Œä»è€Œåœ¨èµ„æºç´§å¼ çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜å¯é æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼Œå¼¹æ€§æ¨ç†åœ¨ä¸¥æ ¼çš„é¢„ç®—é™åˆ¶ä¸‹è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶è®­ç»ƒæˆæœ¬æ˜¾è‘—ä½äºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05469', 'title': 'Generating Physically Stable and Buildable LEGO Designs from Text', 'url': 'https://huggingface.co/papers/2505.05469', 'abstract': 'We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.', 'score': 17, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '20752e033ce6b40a', 'authors': ['Ava Pun', 'Kangle Deng', 'Ruixuan Liu', 'Deva Ramanan', 'Changliu Liu', 'Jun-Yan Zhu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.05469.jpg', 'data': {'categories': ['#robotics', '#dataset', '#multimodal', '#3d', '#open_source'], 'emoji': 'ğŸ§±', 'ru': {'title': 'LegoGPT: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ LEGO', 'desc': 'LegoGPT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· ĞºÑƒĞ±Ğ¸ĞºĞ¾Ğ² LEGO Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ LEGO Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ĞºÑƒĞ±Ğ¸Ğº Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºĞ°Ñ‚ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LegoGPT ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ LEGO, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼.'}, 'en': {'title': 'Building LEGO Dreams with AI!', 'desc': 'LegoGPT is a novel machine learning model designed to generate stable LEGO brick structures from textual descriptions. It utilizes a large dataset of LEGO designs paired with captions to train an autoregressive language model that predicts the next brick to add based on the input prompt. To ensure the generated designs are physically stable, the model incorporates a validity check and physics-aware rollback mechanism during the generation process. The results demonstrate that LegoGPT can create diverse and visually appealing LEGO models that can be assembled by both humans and robotic systems.'}, 'zh': {'title': 'ä¹é«˜è®¾è®¡çš„æ™ºèƒ½ç”Ÿæˆä¸ç¨³å®šæ€§ä¿éšœ', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†LegoGPTï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»æ–‡æœ¬æç¤ºç”Ÿæˆç‰©ç†ç¨³å®šçš„ä¹é«˜ç –æ¨¡å‹çš„æ–¹æ³•ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ç‰©ç†ç¨³å®šä¹é«˜è®¾è®¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¦æ·»åŠ çš„ç –å—ã€‚ä¸ºäº†æé«˜è®¾è®¡çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬åœ¨è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨äº†æœ‰æ•ˆçš„æœ‰æ•ˆæ€§æ£€æŸ¥å’Œç‰©ç†æ„ŸçŸ¥å›æ»šï¼Œåˆ©ç”¨ç‰©ç†æ³•åˆ™å’Œç»„è£…çº¦æŸæ¥ä¿®å‰ªä¸å¯è¡Œçš„é¢„æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLegoGPTç”Ÿæˆçš„ä¹é«˜è®¾è®¡ç¨³å®šã€å¤šæ ·ä¸”ç¾è§‚ï¼Œä¸è¾“å…¥çš„æ–‡æœ¬æç¤ºç´§å¯†å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05071', 'title': 'FG-CLIP: Fine-Grained Visual and Textual Alignment', 'url': 'https://huggingface.co/papers/2505.05071', 'abstract': "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.", 'score': 15, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '4251cc9ddf64d2b8', 'authors': ['Chunyu Xie', 'Bin Wang', 'Fanjing Kong', 'Jincheng Li', 'Dawei Liang', 'Gengshen Zhang', 'Dawei Leng', 'Yuhui Yin'], 'affiliations': ['360 AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05071.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'FG-CLIP: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fine-Grained CLIP (FG-CLIP) - ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. FG-CLIP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 1,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. FG-CLIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ CLIP Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸.'}, 'en': {'title': 'Unlocking Fine-Grained Understanding with FG-CLIP', 'desc': 'The paper introduces Fine-Grained CLIP (FG-CLIP), an enhancement of the original CLIP model aimed at improving fine-grained understanding in multimodal tasks. FG-CLIP achieves this by generating a massive dataset of 1.6 billion long caption-image pairs, which helps capture detailed semantic information. Additionally, it constructs a high-quality dataset with 12 million images and 40 million bounding boxes, ensuring that the model learns from context-rich representations. By incorporating 10 million hard negative samples, FG-CLIP enhances its ability to differentiate subtle semantic differences, leading to superior performance in various tasks compared to the original CLIP and other leading models.'}, 'zh': {'title': 'ç»†ç²’åº¦ç†è§£çš„æ–°çªç ´ï¼šFG-CLIP', 'desc': 'å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç»†ç²’åº¦ç†è§£æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»†ç²’åº¦CLIPï¼ˆFG-CLIPï¼‰ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°æ¥å¢å¼ºç»†ç²’åº¦ç†è§£ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆ16äº¿å¯¹é•¿æ ‡é¢˜-å›¾åƒå¯¹ï¼Œä»¥æ•æ‰å…¨å±€è¯­ä¹‰ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«1200ä¸‡å¼ å›¾åƒå’Œ4000ä¸‡ä¸ªåŒºåŸŸç‰¹å®šçš„è¾¹ç•Œæ¡†ï¼Œç¡®ä¿ç²¾ç¡®ä¸”ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05467', 'title': 'StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant', 'url': 'https://huggingface.co/papers/2505.05467', 'abstract': 'We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.', 'score': 13, 'issue_id': 3674, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': 'c876dfdb1d290930', 'authors': ['Haibo Wang', 'Bo Feng', 'Zhengfeng Lai', 'Mingze Xu', 'Shiyu Li', 'Weifeng Ge', 'Afshin Dehghan', 'Meng Cao', 'Ping Huang'], 'affiliations': ['Apple', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05467.jpg', 'data': {'categories': ['#benchmark', '#video', '#dataset', '#long_context'], 'emoji': 'ğŸ¥', 'ru': {'title': 'StreamBridge: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'StreamBridge - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Video-LLM Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ÑƒÑ„ĞµÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸ĞµĞ¹ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Video-LLM. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ StreamBridge ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Stream-IT, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Transforming Video-LLMs for Real-Time Streaming Success', 'desc': 'StreamBridge is a framework designed to enhance offline Video-LLMs for real-time streaming applications. It tackles two main issues: the need for effective multi-turn interactions and the ability to provide proactive responses. By using a memory buffer with round-decayed compression, it allows models to handle longer contexts in conversations. Additionally, StreamBridge introduces a lightweight activation model that integrates easily with existing Video-LLMs, and it is supported by the Stream-IT dataset, which is specifically created for streaming video understanding tasks.'}, 'zh': {'title': 'StreamBridgeï¼šæµåª’ä½“è§†é¢‘ç†è§£çš„æ–°çªç ´', 'desc': 'StreamBridgeæ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æ¶ï¼Œå¯ä»¥å°†ç¦»çº¿è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰è½¬å˜ä¸ºæ”¯æŒæµåª’ä½“çš„æ¨¡å‹ã€‚å®ƒè§£å†³äº†åœ¨åœ¨çº¿åœºæ™¯ä¸­é€‚åº”ç°æœ‰æ¨¡å‹çš„ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šå¤šè½®å®æ—¶ç†è§£èƒ½åŠ›æœ‰é™å’Œç¼ºä¹ä¸»åŠ¨å“åº”æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒStreamBridgeç»“åˆäº†å†…å­˜ç¼“å†²åŒºå’Œé€è½®è¡°å‡å‹ç¼©ç­–ç•¥ï¼Œæ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„å¤šè½®äº¤äº’ï¼Œå¹¶ä¸”é‡‡ç”¨äº†è½»é‡çº§çš„è§£è€¦æ¿€æ´»æ¨¡å‹ï¼Œèƒ½å¤Ÿè½»æ¾é›†æˆåˆ°ç°æœ‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œå®ç°æŒç»­çš„ä¸»åŠ¨å“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†Stream-ITï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æµåª’ä½“è§†é¢‘ç†è§£çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«äº¤é”™çš„è§†é¢‘-æ–‡æœ¬åºåˆ—å’Œå¤šæ ·çš„æŒ‡ä»¤æ ¼å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05474', 'title': '3D Scene Generation: A Survey', 'url': 'https://huggingface.co/papers/2505.05474', 'abstract': '3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.', 'score': 12, 'issue_id': 3672, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '06bda1a6228b8f26', 'authors': ['Beichen Wen', 'Haozhe Xie', 'Zhaoxi Chen', 'Fangzhou Hong', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore 637335'], 'pdf_title_img': 'assets/pdf/title_img/2505.05474.jpg', 'data': {'categories': ['#3d', '#robotics', '#multimodal', '#synthetic', '#survey'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼: Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½ÑƒÑ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Advancing 3D Scene Generation with Deep Learning', 'desc': 'This paper reviews the latest techniques in 3D scene generation, which aims to create realistic and meaningful environments for various applications. It highlights the evolution from early procedural methods to modern deep generative models like GANs and diffusion models, which enhance the quality and diversity of generated scenes. The authors categorize these methods into four main paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation, analyzing their strengths and weaknesses. The paper also discusses ongoing challenges and future directions in the field, such as improving fidelity and integrating physics into scene generation.'}, 'zh': {'title': '3Dåœºæ™¯ç”Ÿæˆçš„æœªæ¥æ–¹å‘ä¸æŒ‘æˆ˜', 'desc': '3Dåœºæ™¯ç”Ÿæˆæ—¨åœ¨åˆæˆå…·æœ‰ç©ºé—´ç»“æ„ã€è¯­ä¹‰æ„ä¹‰å’Œç…§ç‰‡çœŸå®æ„Ÿçš„ç¯å¢ƒï¼Œå¹¿æ³›åº”ç”¨äºæ²‰æµ¸å¼åª’ä½“ã€æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œå…·èº«äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚æ—©æœŸåŸºäºç¨‹åºè§„åˆ™çš„æ–¹æ³•è™½ç„¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œä½†å¤šæ ·æ€§æœ‰é™ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANå’Œæ‰©æ•£æ¨¡å‹ï¼‰ä»¥åŠ3Dè¡¨ç¤ºï¼ˆå¦‚NeRFå’Œ3Dé«˜æ–¯ï¼‰å–å¾—äº†è¿›å±•ï¼Œä½¿å¾—èƒ½å¤Ÿå­¦ä¹ çœŸå®ä¸–ç•Œåœºæ™¯çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜äº†ç”Ÿæˆçš„çœŸå®æ„Ÿã€å¤šæ ·æ€§å’Œè§†å›¾ä¸€è‡´æ€§ã€‚æœ¬æ–‡ç»¼è¿°äº†æœ€æ–°çš„3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œåˆ†æäº†å…¶æŠ€æœ¯åŸºç¡€ã€æƒè¡¡å’Œä»£è¡¨æ€§ç»“æœï¼Œå¹¶è®¨è®ºäº†ç”Ÿæˆèƒ½åŠ›ã€3Dè¡¨ç¤ºã€æ•°æ®å’Œè¯„ä¼°ç­‰å…³é”®æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04842', 'title': 'Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers', 'url': 'https://huggingface.co/papers/2505.04842', 'abstract': "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL^V that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL^V boosts MATH accuracy by over 20\\% with parallel sampling and enables 8-32times efficient test-time compute scaling compared to the base RL method. RL^V also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL^V achieves 1.2-1.6times higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.", 'score': 12, 'issue_id': 3688, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': 'ae650f18905f196e', 'authors': ['Kusha Sareen', 'Morgane M Moss', 'Alessandro Sordoni', 'Rishabh Agarwal', 'Arian Hosseini'], 'affiliations': ['Google DeepMind, Mila', 'Microsoft Research, Mila', 'Mila, McGill University', 'Mila, Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2505.04842.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#optimization', '#rl', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RL^V: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RL^V, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RL^V Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. RL^V Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Enhancing LLMs with Value-Driven Reinforcement Learning', 'desc': 'This paper introduces RL^V, a novel reinforcement learning method that enhances large language models (LLMs) by integrating value functions into the training process. Unlike traditional methods that discard learned value functions, RL^V allows LLMs to act as both reasoners and generative verifiers, improving their ability to verify outputs. The approach significantly increases accuracy in mathematical tasks and improves computational efficiency during testing, allowing for faster processing. Additionally, RL^V demonstrates strong performance across various task difficulties and domains, showcasing its versatility and effectiveness in real-world applications.'}, 'zh': {'title': 'RL^Vï¼šæå‡æ¨ç†ä¸éªŒè¯çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•RL^Vï¼Œç”¨äºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRL^VåŒæ—¶è®­ç»ƒLLMä½œä¸ºæ¨ç†è€…å’Œç”ŸæˆéªŒè¯è€…ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç”Ÿæˆçš„æ•°æ®æ¥å¢å¼ºéªŒè¯èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL^Våœ¨MATHä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†è¶…è¿‡20%ï¼Œå¹¶ä¸”åœ¨æµ‹è¯•æ—¶è®¡ç®—æ•ˆç‡ä¸Šæå‡äº†8åˆ°32å€ã€‚è¯¥æ–¹æ³•è¿˜å±•ç¤ºäº†åœ¨ä¸åŒéš¾åº¦å’Œé¢†åŸŸä»»åŠ¡ä¸Šçš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05327', 'title': 'ICon: In-Context Contribution for Automatic Data Selection', 'url': 'https://huggingface.co/papers/2505.05327', 'abstract': 'Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.', 'score': 11, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': 'b64bd2ecee9bb211', 'authors': ['Yixin Yang', 'Qingxiu Dong', 'Linli Yao', 'Fangwei Zhu', 'Zhifang Sui'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05327.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ICon: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ICon. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (in-context learning) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº. ICon Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ñ‚Ñ€ĞµÑ… LLM Ğ¸ 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° 15% Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ICon Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Data Selection for Better Language Model Training', 'desc': 'This paper introduces a new method called In-context Learning for Contribution Measurement (ICon) that helps select the best data for training Large Language Models (LLMs) without using complex gradient calculations. ICon leverages the concept of in-context learning to evaluate how much each data sample contributes to model performance, making it more efficient than traditional methods. The approach reduces reliance on human-designed heuristics, which can introduce bias, and instead focuses on the intrinsic qualities of the data. Experiments show that models trained with ICon-selected data perform better than those trained on full datasets or those selected by existing methods.'}, 'zh': {'title': 'é«˜æ•ˆæ•°æ®é€‰æ‹©ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œç§°ä¸ºIConï¼ˆIn-context Learning for Contribution Measurementï¼‰ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚IConæ˜¯ä¸€ç§æ— æ¢¯åº¦çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„éšå¼å¾®è°ƒç‰¹æ€§æ¥è¯„ä¼°æ ·æœ¬çš„è´¡çŒ®ï¼Œè€Œæ— éœ€è®¡ç®—æ¢¯åº¦æˆ–è®¾è®¡æ‰‹åŠ¨æŒ‡æ ‡ã€‚é€šè¿‡åœ¨ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒIConæ˜¾ç¤ºå‡ºå…¶åœ¨é€‰æ‹©é«˜è´¡çŒ®æ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘è®¡ç®—æˆæœ¬çš„åŒæ—¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨IConé€‰æ‹©çš„15%æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†ä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03981', 'title': 'X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains', 'url': 'https://huggingface.co/papers/2505.03981', 'abstract': "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.", 'score': 11, 'issue_id': 3672, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '0e6c2f37e1536f9f', 'authors': ['Qianchu Liu', 'Sheng Zhang', 'Guanghui Qin', 'Timothy Ossowski', 'Yu Gu', 'Ying Jin', 'Sid Kiblawi', 'Sam Preston', 'Mu Wei', 'Paul Vozila', 'Tristan Naumann', 'Hoifung Poon'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.03981.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#transfer_learning', '#healthcare'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ X-Reasoner - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ X-Reasoner ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - X-Reasoner-Med, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ².'}, 'en': {'title': 'Unlocking Generalizable Reasoning Across Modalities with X-Reasoner', 'desc': 'This paper investigates whether reasoning abilities can be generalized across different types of data, specifically from text to other modalities like images. The authors present X-Reasoner, a vision-language model that is post-trained on general-domain text to enhance its reasoning capabilities. They employ a two-stage training process that includes supervised fine-tuning with detailed reasoning steps and reinforcement learning with measurable rewards. The results demonstrate that X-Reasoner not only excels in multimodal tasks but also improves in specialized fields, leading to the creation of X-Reasoner-Med, which sets new benchmarks in medical reasoning tasks.'}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›çš„è·¨æ¨¡æ€æ¨å¹¿', 'desc': 'æœ€è¿‘çš„ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚o3ï¼‰å±•ç¤ºäº†å¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å¼€æºç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®­ç»ƒä»…åŸºäºæ–‡æœ¬çš„æ¨ç†æ¨¡å‹ï¼Œè¯„ä¼°ä¹Ÿä¸»è¦é™äºæ•°å­¦å’Œä¸€èˆ¬é¢†åŸŸä»»åŠ¡ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°æ–‡æœ¬è¾“å…¥å’Œä¸€èˆ¬é¢†åŸŸä¹‹å¤–ä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªåŸºæœ¬çš„ç ”ç©¶é—®é¢˜ï¼šæ¨ç†æ˜¯å¦å¯ä»¥è·¨æ¨¡æ€å’Œé¢†åŸŸè¿›è¡Œæ¨å¹¿ï¼Ÿ'}}}, {'id': 'https://huggingface.co/papers/2505.05408', 'title': 'Crosslingual Reasoning through Test-Time Scaling', 'url': 'https://huggingface.co/papers/2505.05408', 'abstract': "Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.", 'score': 8, 'issue_id': 3676, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': 'a6c39be653cbaefe', 'authors': ['Zheng-Xin Yong', 'M. Farid Adilazuarda', 'Jonibek Mansurov', 'Ruochen Zhang', 'Niklas Muennighoff', 'Carsten Eickhoff', 'Genta Indra Winata', 'Julia Kreutzer', 'Stephen H. Bach', 'Alham Fikri Aji'], 'affiliations': ['Brown University', 'Capital One', 'Cohere Labs', 'MBZUAI', 'Stanford University', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.05408.jpg', 'data': {'categories': ['#math', '#reasoning', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ 'Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ-Ğ¸-Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ' Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµĞ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ·Ñ‹ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."}, 'en': {'title': 'Unlocking Multilingual Reasoning with English-Centric Models', 'desc': 'This paper explores how reasoning abilities of large language models, which are often trained in English, can be applied to other languages. The authors find that increasing computational resources for English-based reasoning models enhances their performance in multilingual mathematical reasoning, even in languages with fewer resources. They also identify a pattern where these models can effectively reason about non-English inputs by quoting and thinking in English. However, the study highlights challenges in transferring reasoning skills from STEM topics to cultural knowledge, indicating that while there is potential for cross-lingual reasoning, improvements are needed for low-resource languages and diverse contexts.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ä¸æŒ‘æˆ˜', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯è‹±è¯­æ¨ç†çš„å¾®è°ƒå¦‚ä½•åœ¨å…¶ä»–è¯­è¨€ä¸­æ¨å¹¿ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡å¢åŠ æ¨ç†è®¡ç®—èƒ½åŠ›ï¼Œè‹±è¯­ä¸­å¿ƒçš„æ¨ç†è¯­è¨€æ¨¡å‹åœ¨å¤šç§è¯­è¨€ï¼ˆåŒ…æ‹¬ä½èµ„æºè¯­è¨€ï¼‰ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå°½ç®¡è‹±è¯­ä¸­å¿ƒçš„æ¨ç†é“¾ä¸»è¦æ˜¯è‹±è¯­ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†éè‹±è¯­è¾“å…¥æ—¶ä»èƒ½éµå¾ªç‰¹å®šçš„æ¨ç†æ¨¡å¼ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€ä¸­çš„æ¨ç†æ•ˆæœæ›´å¥½ï¼Œä½†åœ¨è·¨é¢†åŸŸæ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä»STEMé¢†åŸŸåˆ°æ–‡åŒ–å¸¸è¯†çš„è¿ç§»ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05288', 'title': 'PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes', 'url': 'https://huggingface.co/papers/2505.05288', 'abstract': "We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.", 'score': 7, 'issue_id': 3675, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '7ffec9bccc965d17', 'authors': ['Ahmed Abdelreheem', 'Filippo Aleotti', 'Jamie Watson', 'Zawar Qureshi', 'Abdelrahman Eldesokey', 'Peter Wonka', 'Gabriel Brostow', 'Sara Vicente', 'Guillermo Garcia-Hernando'], 'affiliations': ['KAUST', 'Niantic Spatial', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.05288.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#3d', '#survey', '#dataset'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº 3D-ÑÑ†ĞµĞ½Ñ‹, 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµĞµ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² 3D. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ 3D ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Placing Objects with Words in 3D Spaces!', 'desc': 'This paper presents a new task called Language-Guided Object Placement in Real 3D Scenes, which involves placing a 3D object based on a textual description. The model must analyze a point cloud of a 3D scene and determine a suitable location for the object that aligns with the given prompt. This task is particularly challenging due to the ambiguity of multiple valid placements and the need for understanding 3D spatial relationships. The authors introduce a benchmark, evaluation protocol, and a dataset to train 3D language models, aiming to enhance the evaluation of generalist 3D models.'}, 'zh': {'title': 'è¯­è¨€å¼•å¯¼çš„3Dç‰©ä½“æ”¾ç½®æ–°æŒ‘æˆ˜', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šåœ¨çœŸå®3Dåœºæ™¯ä¸­è¿›è¡Œè¯­è¨€å¼•å¯¼çš„ç‰©ä½“æ”¾ç½®ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ¥æ”¶ä¸€ä¸ª3Dåœºæ™¯çš„ç‚¹äº‘ã€ä¸€ä¸ª3Dèµ„äº§å’Œä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œä»»åŠ¡æ˜¯æ‰¾åˆ°ä¸€ä¸ªç¬¦åˆæç¤ºçš„æœ‰æ•ˆæ”¾ç½®ä½ç½®ã€‚ä¸å…¶ä»–è¯­è¨€å¼•å¯¼çš„3Dåœºæ™¯å®šä½ä»»åŠ¡ç›¸æ¯”ï¼Œè¿™é¡¹ä»»åŠ¡å…·æœ‰ç‰¹å®šçš„æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå­˜åœ¨å¤šä¸ªæœ‰æ•ˆè§£ï¼Œå¹¶ä¸”éœ€è¦æ¨ç†3Då‡ ä½•å…³ç³»å’Œè‡ªç”±ç©ºé—´ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºæ–°çš„åŸºå‡†å’Œè¯„ä¼°åè®®æ¥å¼€å¯è¿™ä¸€ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºè®­ç»ƒ3Då¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠç¬¬ä¸€ä¸ªéå¹³å‡¡çš„åŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04769', 'title': 'Vision-Language-Action Models: Concepts, Progress, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2505.04769', 'abstract': 'Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models', 'score': 7, 'issue_id': 3682, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '75960b9512a0a05b', 'authors': ['Ranjan Sapkota', 'Yang Cao', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Biological & Environmental Engineering, Ithaca, New York, USA', 'The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong', 'University of the Peloponnese, Department of Informatics and Telecommunications, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.04769.jpg', 'data': {'categories': ['#agi', '#robotics', '#ethics', '#architecture', '#agents', '#multimodal', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ VLA Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ñ…, Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Unifying Vision, Language, and Action in AI', 'desc': 'Vision-Language-Action (VLA) models represent a significant step in AI by combining visual perception, language understanding, and physical actions into one system. This review organizes recent developments in VLA models into five key themes, highlighting their evolution from basic cross-modal learning to sophisticated generalist agents. It covers over 80 models from the last three years, focusing on innovations in architecture, training efficiency, and real-time performance. The paper also discusses applications in various fields and proposes solutions to challenges like real-time control and ethical deployment, aiming to guide future advancements in intelligent robotics and AI.'}, 'zh': {'title': 'ç»Ÿä¸€æ„ŸçŸ¥ä¸è¡ŒåŠ¨çš„æ™ºèƒ½æ¨¡å‹', 'desc': 'è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€é¡¹é‡è¦è¿›å±•ï¼Œæ—¨åœ¨å°†æ„ŸçŸ¥ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œå…·ä½“è¡ŒåŠ¨ç»Ÿä¸€åœ¨ä¸€ä¸ªè®¡ç®—æ¡†æ¶å†…ã€‚æœ¬æ–‡ç»¼è¿°äº†VLAæ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œç³»ç»Ÿåœ°ç»„ç»‡äº†äº”ä¸ªä¸»é¢˜æ”¯æŸ±ï¼Œæç»˜äº†è¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸã€‚æˆ‘ä»¬å›é¡¾äº†VLAç³»ç»Ÿçš„æ¦‚å¿µåŸºç¡€ï¼Œè¿½æº¯å…¶ä»è·¨æ¨¡æ€å­¦ä¹ æ¶æ„åˆ°ç´§å¯†é›†æˆè§†è§‰-è¯­è¨€æ¨¡å‹ã€è¡ŒåŠ¨è§„åˆ’å™¨å’Œå±‚æ¬¡æ§åˆ¶å™¨çš„æ¼”å˜ã€‚æ–‡ç« è¿˜æ¢è®¨äº†VLAæ¨¡å‹åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—ã€å†œä¸šå’Œå¢å¼ºç°å®ç­‰å¤šä¸ªåº”ç”¨é¢†åŸŸçš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03422', 'title': 'LiftFeat: 3D Geometry-Aware Local Feature Matching', 'url': 'https://huggingface.co/papers/2505.03422', 'abstract': 'Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.', 'score': 7, 'issue_id': 3678, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '5be3a3b002db41cc', 'authors': ['Yepeng Liu', 'Wenpeng Lai', 'Zhou Zhao', 'Yuxuan Xiong', 'Jinchi Zhu', 'Jun Cheng', 'Yongchao Xu'], 'affiliations': ['Institute for Infocomm Research, A*STAR, Singapore', 'SF Technology, Shenzhen, China', 'School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03422.jpg', 'data': {'categories': ['#cv', '#robotics', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'LiftFeat: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ LiftFeat Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. LiftFeat Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ¡ĞµÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ 2D Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ LiftFeat Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ñ‹, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'LiftFeat: Enhancing Feature Matching with 3D Geometry', 'desc': 'This paper introduces LiftFeat, a lightweight neural network designed to improve local feature matching in challenging visual conditions. It addresses issues like drastic lighting changes and low texture areas by incorporating 3D geometric features into the feature extraction process. The network uses a pre-trained monocular depth estimation model to create pseudo surface normal labels, which guide the extraction of 3D features. Experimental results show that LiftFeat significantly enhances the performance of visual localization and pose estimation tasks compared to existing lightweight methods.'}, 'zh': {'title': 'æå‡è§†è§‰ç‰¹å¾åŒ¹é…çš„é²æ£’æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§ç½‘ç»œLiftFeatï¼Œæ—¨åœ¨æé«˜åœ¨æç«¯æ¡ä»¶ä¸‹çš„è§†è§‰ç‰¹å¾åŒ¹é…èƒ½åŠ›ã€‚é€šè¿‡èšåˆä¸‰ç»´å‡ ä½•ç‰¹å¾ï¼ŒLiftFeatå¢å¼ºäº†åŸå§‹æè¿°ç¬¦çš„é²æ£’æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ç”Ÿæˆä¼ªè¡¨é¢æ³•çº¿æ ‡ç­¾ï¼Œä»¥æŒ‡å¯¼ä¸‰ç»´å‡ ä½•ç‰¹å¾çš„æå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLiftFeatåœ¨ç›¸å¯¹å§¿æ€ä¼°è®¡ã€å•åº”æ€§ä¼°è®¡å’Œè§†è§‰å®šä½ä»»åŠ¡ä¸­ä¼˜äºä¸€äº›è½»é‡çº§çš„æœ€æ–°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05064', 'title': 'WaterDrum: Watermarking for Data-centric Unlearning Metric', 'url': 'https://huggingface.co/papers/2505.05064', 'abstract': 'Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.', 'score': 6, 'issue_id': 3684, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '8d04facba381d919', 'authors': ['Xinyang Lu', 'Xinyuan Niu', 'Gregory Kang Ruey Lau', 'Bui Thi Cam Nhung', 'Rachael Hwee Ling Sim', 'Fanyu Wen', 'Chuan-Sheng Foo', 'See-Kiong Ng', 'Bryan Kian Hsiang Low'], 'affiliations': ['A*STAR, Way, Create', 'CNRS@CREATE, 1 Singapore #08Tower, Computer Singapore (CFAR), Create', 'Centre for Singapore Science, National University', 'Department of of Singapore', 'Frontier AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.05064.jpg', 'data': {'categories': ['#leakage', '#dataset', '#benchmark', '#data', '#open_source'], 'emoji': 'ğŸ§½', 'ru': {'title': 'WaterDrum: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ÑƒÑ WaterDrum. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. WaterDrum Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸.'}, 'en': {'title': 'WaterDrum: A New Era in LLM Unlearning Metrics', 'desc': 'This paper addresses the challenge of unlearning in large language models (LLMs), which is essential for removing sensitive or harmful data. Current metrics for evaluating unlearning focus on model performance but may not reflect true unlearning effectiveness, especially when data is semantically similar. The authors propose a new data-centric unlearning metric called WaterDrum, which utilizes robust text watermarking to better assess unlearning in practical scenarios. Additionally, they introduce benchmark datasets designed to test unlearning algorithms under various conditions, enhancing the evaluation process for LLM unlearning.'}, 'zh': {'title': 'æå‡å¤§è¯­è¨€æ¨¡å‹çš„é—å¿˜èƒ½åŠ›', 'desc': 'å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—å¿˜èƒ½åŠ›åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æœ‰æ•ˆå»é™¤ç”¨æˆ·çš„ç§äººã€ç‰ˆæƒæˆ–æœ‰å®³æ•°æ®å½±å“æ—¶ã€‚ç°æœ‰çš„ä»¥æ•ˆç”¨ä¸ºä¸­å¿ƒçš„é—å¿˜åº¦é‡å¯èƒ½æ— æ³•å‡†ç¡®è¯„ä¼°åœ¨è¯­ä¹‰ç›¸ä¼¼å†…å®¹çš„æƒ…å†µä¸‹çš„é—å¿˜ç¨‹åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ä¸­å¿ƒé—å¿˜åº¦é‡æ–¹æ³•ï¼Œç§°ä¸ºWaterDrumï¼Œåˆ©ç”¨ç¨³å¥çš„æ–‡æœ¬æ°´å°æŠ€æœ¯æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°LLMçš„é—å¿˜ç®—æ³•ï¼Œç¡®ä¿å¯ä»¥ä¸¥æ ¼æµ‹è¯•é—å¿˜æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02363', 'title': 'SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in\n  Language Model Preference Learning', 'url': 'https://huggingface.co/papers/2505.02363', 'abstract': 'Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.', 'score': 6, 'issue_id': 3687, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'da8e6360c88cd0ed', 'authors': ['Tianjian Li', 'Daniel Khashabi'], 'affiliations': ['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, US'], 'pdf_title_img': 'assets/pdf/title_img/2505.02363.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#dataset'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ on-policy Ğ¸ off-policy Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SIMPLEMIX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SIMPLEMIX Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'SIMPLEMIX: Uniting On-Policy and Off-Policy for Better Language Model Alignment', 'desc': 'This paper explores how to align language models with human preferences using two types of data: on-policy and off-policy. On-policy data is shown to excel in tasks requiring reasoning, like math and coding, while off-policy data is better for creative and open-ended tasks. The authors propose a new method called SIMPLEMIX, which effectively combines these two data sources to leverage their strengths. Their experiments demonstrate that SIMPLEMIX significantly enhances the performance of language models compared to existing methods, achieving notable improvements in alignment metrics.'}, 'zh': {'title': 'ç»“åˆåœ¨çº¿ä¸ç¦»çº¿æ•°æ®ï¼Œæå‡è¯­è¨€æ¨¡å‹å¯¹é½æ•ˆæœ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨åå¥½å­¦ä¹ ä¸­ï¼Œåœ¨çº¿æ•°æ®å’Œç¦»çº¿æ•°æ®çš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨çº¿æ•°æ®åœ¨æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œç¦»çº¿æ•°æ®åœ¨å¼€æ”¾å¼ä»»åŠ¡ï¼ˆå¦‚åˆ›æ„å†™ä½œå’Œä¸ªäººæ¨èï¼‰ä¸­æ›´å…·ä¼˜åŠ¿ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSIMPLEMIXçš„æ–¹æ³•ï¼Œé€šè¿‡ç®€å•åœ°æ··åˆè¿™ä¸¤ç§æ•°æ®æºï¼Œç»“åˆå®ƒä»¬çš„ä¼˜ç‚¹ã€‚å®éªŒè¯æ˜ï¼ŒSIMPLEMIXåœ¨å¤šç§ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04955', 'title': 'Chain-of-Thought Tokens are Computer Program Variables', 'url': 'https://huggingface.co/papers/2505.04955', 'abstract': 'Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically study the role of CoT tokens in LLMs on two compositional tasks: multi-digit multiplication and dynamic programming. While CoT is essential for solving these problems, we find that preserving only tokens that store intermediate results would achieve comparable performance. Furthermore, we observe that storing intermediate results in an alternative latent form will not affect model performance. We also randomly intervene some values in CoT, and notice that subsequent CoT tokens and the final answer would change correspondingly. These findings suggest that CoT tokens may function like variables in computer programs but with potential drawbacks like unintended shortcuts and computational complexity limits between tokens. The code and data are available at https://github.com/solitaryzero/CoTs_are_Variables.', 'score': 5, 'issue_id': 3677, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': 'a8bb5e6b1e09e5dd', 'authors': ['Fangwei Zhu', 'Peiyi Wang', 'Zhifang Sui'], 'affiliations': ['School of Computer Science, State Key Laboratory of Multimedia Information Processing, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04955.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#architecture', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ CoT Ğ² LLM: Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought, CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ¾Ğ»ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² CoT Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ CoT Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° CoT, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking the Power of Chain-of-Thought Tokens in LLMs', 'desc': "This paper investigates the role of chain-of-thought (CoT) tokens in large language models (LLMs) when solving complex reasoning tasks. The authors find that while CoT is important for tasks like multi-digit multiplication and dynamic programming, retaining only tokens that represent intermediate results can yield similar performance. They also discover that storing these results in a different latent form does not impact the model's effectiveness. Additionally, the study reveals that altering CoT values affects subsequent tokens and final answers, indicating that CoT tokens may act like variables in programming, albeit with some limitations."}, 'zh': {'title': 'é“¾å¼æ¨ç†ï¼šå˜é‡çš„åŠ›é‡ä¸æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†é“¾å¼æ¨ç†ï¼ˆCoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä½æ•°ä¹˜æ³•å’ŒåŠ¨æ€è§„åˆ’è¿™ä¸¤ä¸ªå¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶CoTå¯¹äºè§£å†³è¿™äº›é—®é¢˜è‡³å…³é‡è¦ï¼Œä½†ä»…ä¿ç•™å­˜å‚¨ä¸­é—´ç»“æœçš„æ ‡è®°ä¹Ÿèƒ½è¾¾åˆ°ç›¸ä¼¼çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œå°†ä¸­é—´ç»“æœä»¥å¦ä¸€ç§æ½œåœ¨å½¢å¼å­˜å‚¨ä¸ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€åï¼Œéšæœºå¹²é¢„CoTä¸­çš„æŸäº›å€¼ä¼šå¯¼è‡´åç»­çš„CoTæ ‡è®°å’Œæœ€ç»ˆç­”æ¡ˆç›¸åº”å˜åŒ–ï¼Œè¿™è¡¨æ˜CoTæ ‡è®°å¯èƒ½åƒè®¡ç®—æœºç¨‹åºä¸­çš„å˜é‡ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›æ½œåœ¨çš„ç¼ºé™·ï¼Œå¦‚æ„å¤–çš„æ·å¾„å’Œæ ‡è®°ä¹‹é—´çš„è®¡ç®—å¤æ‚æ€§é™åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.19314', 'title': 'BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese', 'url': 'https://huggingface.co/papers/2504.19314', 'abstract': "As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.", 'score': 4, 'issue_id': 3673, 'pub_date': '2025-04-27', 'pub_date_card': {'ru': '27 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 27', 'zh': '4æœˆ27æ—¥'}, 'hash': '06aff0f566bd3817', 'authors': ['Peilin Zhou', 'Bruce Leon', 'Xiang Ying', 'Can Zhang', 'Yifan Shao', 'Qichen Ye', 'Dading Chong', 'Zhiling Jin', 'Chenxuan Xie', 'Meng Cao', 'Yuxin Gu', 'Sixin Hong', 'Jing Ren', 'Jian Chen', 'Chao Liu', 'Yining Hua'], 'affiliations': ['Alibaba Group', 'HSBC', 'Harvard T.H. Chan School of Public Health', 'Hong Kong University of Science and Technology (Guangzhou)', 'MBZUAI', 'Mindverse AI', 'NIO', 'Peking University', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.19314.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#dataset', '#benchmark', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'BrowseComp-ZH: Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BrowseComp-ZH - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¯Ğœ) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ğ²ĞµĞ±-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 289 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² 11 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² BrowseComp-ZH Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ½Ğ¾ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Evaluating LLMs: The Challenge of Chinese Web Browsing', 'desc': 'This paper introduces BrowseComp-ZH, a benchmark designed to evaluate large language models (LLMs) on their ability to browse and retrieve information from the Chinese web. It consists of 289 challenging multi-hop questions across various domains, focusing on high difficulty and unique answers. The study reveals that most state-of-the-art models perform poorly, with accuracy rates often below 10%, highlighting the complexity of reasoning and retrieval in non-English contexts. The results indicate that current LLMs still face significant challenges in mastering the necessary skills for effective information retrieval and reasoning in diverse linguistic environments.'}, 'zh': {'title': 'ä¸­æ–‡ç½‘ç»œæ™ºèƒ½ä½“è¯„ä¼°æ–°åŸºå‡†ï¼šBrowseComp-ZH', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€æ¸æ¼”å˜ä¸ºä½¿ç”¨å·¥å…·çš„æ™ºèƒ½ä½“ï¼Œå®æ—¶æµè§ˆç½‘ç»œçš„èƒ½åŠ›æˆä¸ºè¡¡é‡å…¶æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›çš„é‡è¦æ ‡å‡†ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¦‚BrowseCompä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œå¿½è§†äº†å…¶ä»–ä¸»è¦ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿï¼ˆå°¤å…¶æ˜¯ä¸­æ–‡ï¼‰åœ¨è¯­è¨€ã€åŸºç¡€è®¾æ–½å’Œå®¡æŸ¥æ–¹é¢çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BrowseComp-ZHï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå…¨é¢è¯„ä¼°ä¸­æ–‡ç½‘ç»œä¸Šçš„LLMæ™ºèƒ½ä½“è€Œè®¾è®¡çš„é«˜éš¾åº¦åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«289ä¸ªè·¨è¶Š11ä¸ªä¸åŒé¢†åŸŸçš„å¤šè·³é—®é¢˜ï¼Œæ—¨åœ¨è€ƒå¯Ÿæ¨¡å‹çš„æ£€ç´¢ç­–ç•¥ã€æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯æ•´åˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02550', 'title': 'Bielik v3 Small: Technical Report', 'url': 'https://huggingface.co/papers/2505.02550', 'abstract': 'We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.', 'score': 53, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'a7c9d183be6447dd', 'authors': ['Krzysztof Ociepa', 'Åukasz Flis', 'Remigiusz Kinas', 'Krzysztof WrÃ³bel', 'Adrian GwoÅºdziej'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02550.jpg', 'data': {'categories': ['#small_models', '#plp', '#multilingual', '#low_resource', '#dataset', '#benchmark'], 'emoji': 'ğŸ‡µğŸ‡±', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ˜Ğ˜ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½ĞµĞµ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Bielik v3 - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Efficient Polish Language Models: Big Performance from Small Sizes', 'desc': 'Bielik v3 introduces efficient generative text models specifically designed for the Polish language, with sizes of 1.5B and 4.5B parameters. These models show that smaller architectures can perform as well as larger ones while using less computational power. Key innovations include a custom tokenizer for better token efficiency, a specialized loss function to balance learning, and an adaptive learning rate that adjusts during training. With training on a vast dataset, these models set new standards for language processing in Polish, making advanced AI more accessible for various applications.'}, 'zh': {'title': 'é«˜æ•ˆæ³¢å…°è¯­ç”Ÿæˆæ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Bielik v3ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹æ³¢å…°è¯­å¤„ç†çš„é«˜æ•ˆç”Ÿæˆæ–‡æœ¬æ¨¡å‹ï¼ˆ1.5Bå’Œ4.5Bå‚æ•°ï¼‰ã€‚è¿™äº›æ¨¡å‹è¡¨æ˜ï¼Œè¾ƒå°ä¸”ç»è¿‡ä¼˜åŒ–çš„æ¶æ„å¯ä»¥åœ¨è®¡ç®—èµ„æºå¤§å¹…å‡å°‘çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸æ›´å¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ›æ–°åŒ…æ‹¬å®šåˆ¶çš„æ³¢å…°è¯­åˆ†è¯å™¨ï¼ˆAPT4ï¼‰ï¼Œæ˜¾è‘—æé«˜äº†æ ‡è®°æ•ˆç‡ï¼Œä»¥åŠåŠ æƒæŒ‡ä»¤äº¤å‰ç†µæŸå¤±ï¼Œå¹³è¡¡ä¸åŒæŒ‡ä»¤ç±»å‹çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€è°ƒæ•´çš„å­¦ä¹ ç‡æ ¹æ®è®­ç»ƒè¿›åº¦è¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02410', 'title': 'Bielik 11B v2 Technical Report', 'url': 'https://huggingface.co/papers/2505.02410', 'abstract': "We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.", 'score': 44, 'issue_id': 3707, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'e9cb82cbeaac24ed', 'authors': ['Krzysztof Ociepa', 'Åukasz Flis', 'Krzysztof WrÃ³bel', 'Adrian GwoÅºdziej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets/pdf/title_img/2505.02410.jpg', 'data': {'categories': ['#inference', '#multilingual', '#low_resource', '#training', '#benchmark', '#architecture', '#optimization', '#reasoning'], 'emoji': 'ğŸ‡µğŸ‡±', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°: Bielik 11B v2 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Bielik 11B v2 - ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mistral 7B v0.2 Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¾ 11 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ’ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Bielik 11B v2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Polish Language Processing with Bielik 11B v2', 'desc': 'Bielik 11B v2 is a cutting-edge language model specifically designed for processing Polish text. It utilizes the Mistral 7B v0.2 architecture and has been enhanced to 11 billion parameters, achieving remarkable results on Polish language tasks while also performing well in cross-lingual scenarios. The model introduces innovative techniques like Weighted Instruction Cross-Entropy Loss for better learning from diverse instructions and an Adaptive Learning Rate that adjusts based on the context length. Its efficiency and quantization options allow it to run on various hardware, making it a significant advancement in AI for Polish language applications.'}, 'zh': {'title': 'æ³¢å…°è¯­å¤„ç†çš„æ–°æ ‡æ†ï¼šBielik 11B v2', 'desc': 'Bielik 11B v2 æ˜¯ä¸€ä¸ªé’ˆå¯¹æ³¢å…°è¯­æ–‡æœ¬å¤„ç†çš„å…ˆè¿›è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº Mistral 7B v0.2 æ¶æ„ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ° 11Bã€‚è¯¥æ¨¡å‹åœ¨æ³¢å…°è¯­åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶å…·å¤‡å¼ºå¤§çš„è·¨è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šåŠ æƒæŒ‡ä»¤äº¤å‰ç†µæŸå¤±ï¼Œé€šè¿‡ä¸ºè®­ç»ƒæ ·æœ¬åˆ†é…åŸºäºè´¨é‡çš„æƒé‡æ¥ä¼˜åŒ–ä¸åŒæŒ‡ä»¤ç±»å‹çš„å­¦ä¹ ï¼Œä»¥åŠè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦åŠ¨æ€è°ƒæ•´ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒBielik 11B v2 è¶…è¶Šäº†è®¸å¤šæ›´å¤§æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨è¯­è¨€ç†è§£å’Œå¤æ‚æ¨ç†ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–ä¸“é—¨çš„æ³¢å…°è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.06111', 'title': 'UniVLA: Learning to Act Anywhere with Task-centric Latent Actions', 'url': 'https://huggingface.co/papers/2505.06111', 'abstract': "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.", 'score': 17, 'issue_id': 3704, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ', 'en': 'May 9', 'zh': '5æœˆ9æ—¥'}, 'hash': 'bf19981dd100b8fb', 'authors': ['Qingwen Bu', 'Yanting Yang', 'Jisong Cai', 'Shenyuan Gao', 'Guanghui Ren', 'Maoqing Yao', 'Ping Luo', 'Hongyang Li'], 'affiliations': ['AgiBot', 'OpenDriveLab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.06111.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#benchmark', '#agents', '#transfer_learning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹Ğº', 'desc': 'UniVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. UniVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ OpenVLA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'UniVLA: Empowering Robots with Cross-Embodiment Learning', 'desc': 'The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches.'}, 'zh': {'title': 'UniVLAï¼šæå‡é€šç”¨æœºå™¨äººå­¦ä¹ æ•ˆç‡çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶UniVLAï¼Œç”¨äºå­¦ä¹ è·¨ä½“ç°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ï¼Œä»¥æé«˜é€šç”¨æœºå™¨äººåœ¨ä¸åŒç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡è§†é¢‘ä¸­çš„æ½œåœ¨åŠ¨ä½œæ¨¡å‹æå–ä»¥ä»»åŠ¡ä¸ºä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºï¼Œä»è€Œåˆ©ç”¨å¹¿æ³›çš„å¤šæ ·åŒ–æ•°æ®ã€‚ä¸ºäº†å‡å°‘ä¸ä»»åŠ¡æ— å…³çš„åŠ¨æ€å½±å“ï¼Œæˆ‘ä»¬ç»“åˆäº†è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åœ¨DINOç‰¹å¾ç©ºé—´ä¸­å»ºç«‹äº†æ½œåœ¨åŠ¨ä½œæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVLAåœ¨å¤šä¸ªæ“ä½œå’Œå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨é¢„è®­ç»ƒè®¡ç®—å’Œä¸‹æ¸¸æ•°æ®æ–¹é¢çš„éœ€æ±‚æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05026', 'title': 'G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness', 'url': 'https://huggingface.co/papers/2505.05026', 'abstract': 'Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.', 'score': 11, 'issue_id': 3705, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '41e61eccd430ea55', 'authors': ['Jaehyun Jeon', 'Jang Han Yoon', 'Min Soo Kim', 'Sumin Shim', 'Yejin Choi', 'Hanbin Kim', 'Youngjae Yu'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.05026.jpg', 'data': {'categories': ['#reasoning', '#cv', '#benchmark', '#optimization', '#inference'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ UI Ğ±ĞµĞ· A/B-Ñ‚ĞµÑÑ‚Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WiserUI-Bench Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 300 Ğ¿Ğ°Ñ€ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… UI-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ A/B-Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ G-FOCUS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ G-FOCUS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing UI Evaluation with G-FOCUS and WiserUI-Bench', 'desc': 'This paper discusses the importance of evaluating user interface (UI) design not just for its visual appeal but for its ability to influence user behavior, a concept known as Design Persuasiveness. The authors highlight the limitations of traditional A/B testing, which is often expensive and slow, and propose a new benchmark called WiserUI-Bench for assessing UI design effectiveness through pairwise comparisons. They introduce G-FOCUS, an innovative reasoning strategy that improves the accuracy of Vision-Language Models (VLMs) in evaluating UI persuasiveness by minimizing biases. The results demonstrate that G-FOCUS outperforms existing methods, paving the way for more efficient and scalable UI design optimization.'}, 'zh': {'title': 'æå‡ç”¨æˆ·ç•Œé¢è®¾è®¡çš„è¯´æœåŠ›è¯„ä¼°', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡çš„æœ‰æ•ˆæ€§è¯„ä¼°ï¼Œå¼ºè°ƒè®¾è®¡çš„è¯´æœåŠ›å¯¹ç”¨æˆ·è¡Œä¸ºçš„å½±å“ã€‚ä¼ ç»Ÿçš„A/Bæµ‹è¯•æ–¹æ³•è™½ç„¶å¸¸ç”¨ï¼Œä½†æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†WiserUI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæˆå¯¹UIè®¾è®¡è¯´æœåŠ›è¯„ä¼°çš„åŸºå‡†ï¼ŒåŒ…å«300å¯¹çœŸå®çš„UIå›¾åƒåŠå…¶A/Bæµ‹è¯•ç»“æœå’Œä¸“å®¶ç†ç”±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†G-FOCUSï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†ç­–ç•¥ï¼Œèƒ½å¤Ÿæé«˜åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯´æœåŠ›è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02686', 'title': 'Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2505.02686', 'abstract': 'Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.', 'score': 11, 'issue_id': 3706, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '22b290e68229e62f', 'authors': ['Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02686.jpg', 'data': {'categories': ['#survey', '#training', '#rlhf', '#alignment', '#benchmark', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ LLM Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Harnessing Rewards: The Future of Learning in LLMs', 'desc': 'This paper discusses the evolution of Large Language Models (LLMs) focusing on the shift from pre-training to learning from rewards. It highlights how reward signals guide LLM behavior through techniques like reinforcement learning, reward-guided decoding, and post-hoc correction. The authors categorize various strategies used in training, inference, and post-inference stages, emphasizing the importance of dynamic feedback for improving model alignment and reasoning. Additionally, the paper addresses benchmarks for reward models and outlines future challenges and directions in this area.'}, 'zh': {'title': 'ä»å¥–åŠ±ä¸­å­¦ä¹ ï¼Œèµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ä»é¢„è®­ç»ƒæ‰©å±•åˆ°åè®­ç»ƒå’Œæµ‹è¯•æ—¶æ‰©å±•ã€‚ä¸€ä¸ªå…³é”®çš„ç»Ÿä¸€èŒƒå¼å‡ºç°äº†ï¼šä»å¥–åŠ±ä¸­å­¦ä¹ ï¼Œå…¶ä¸­å¥–åŠ±ä¿¡å·ä½œä¸ºæŒ‡å¯¼æ˜Ÿï¼Œå¼•å¯¼LLMçš„è¡Œä¸ºã€‚è¿™ä¸ªèŒƒå¼æ”¯æŒäº†è®¸å¤šæµè¡Œçš„æŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆåœ¨RLHFã€DPOå’ŒGRPOä¸­ï¼‰ã€å¥–åŠ±å¼•å¯¼è§£ç å’Œäº‹åä¿®æ­£ã€‚é€šè¿‡è¿™ä¸ªèŒƒå¼ï¼ŒLLMsèƒ½å¤Ÿä»é™æ€æ•°æ®çš„è¢«åŠ¨å­¦ä¹ è½¬å‘ä»åŠ¨æ€åé¦ˆçš„ä¸»åŠ¨å­¦ä¹ ï¼Œèµ‹äºˆå®ƒä»¬å¯¹é½çš„åå¥½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.06046', 'title': 'Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information', 'url': 'https://huggingface.co/papers/2505.06046', 'abstract': "As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.", 'score': 10, 'issue_id': 3707, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ', 'en': 'May 9', 'zh': '5æœˆ9æ—¥'}, 'hash': 'c671de3a9e8ff4de', 'authors': ['Joshua Harris', 'Fan Grayson', 'Felix Feldman', 'Timothy Laurence', 'Toby Nonnenmacher', 'Oliver Higgins', 'Leo Loman', 'Selina Patel', 'Thomas Finnie', 'Samuel Collins', 'Michael Borowitz'], 'affiliations': ['UK Health Security Agency (UKHSA)'], 'pdf_title_img': 'assets/pdf/title_img/2505.06046.jpg', 'data': {'categories': ['#alignment', '#science', '#dataset', '#healthcare', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PubHealthBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ’ĞµĞ»Ğ¸ĞºĞ¾Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 8000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ¾Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğµ LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Evaluating LLMs for Public Health: Promising Yet Cautious', 'desc': "This paper investigates the knowledge of Large Language Models (LLMs) in the domain of UK public health information. It introduces a benchmark called PubHealthBench, which consists of over 8000 questions designed to evaluate LLMs' performance in Multiple Choice Question Answering (MCQA) and free form responses. The study assesses 24 different LLMs, revealing that the latest models perform well in MCQA tasks, achieving over 90% accuracy, but struggle with free form responses, none scoring above 75%. The findings suggest that while LLMs show promise as reliable sources of public health information, caution is needed when interpreting their free form outputs."}, 'zh': {'title': 'æå‡å…¬å…±å«ç”Ÿä¿¡æ¯çš„å‡†ç¡®æ€§', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œäº†è§£å®ƒä»¬åœ¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å˜å¾—è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å…¬å…±å«ç”Ÿé¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•PubHealthBenchï¼ŒåŒ…å«è¶…è¿‡8000ä¸ªé—®é¢˜ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨å…¬å…±å«ç”ŸæŸ¥è¯¢ä¸­çš„å¤šé¡¹é€‰æ‹©é—®ç­”ï¼ˆMCQAï¼‰å’Œè‡ªç”±å½¢å¼å›ç­”çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæœ€æ–°çš„ç§æœ‰LLMsï¼ˆå¦‚GPT-4.5å’ŒGPT-4.1ï¼‰åœ¨MCQAæµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¶…è¿‡90%ï¼Œç”šè‡³è¶…è¶Šäº†äººç±»çš„æœç´¢å¼•æ“ä½¿ç”¨ã€‚ç„¶è€Œï¼Œåœ¨è‡ªç”±å½¢å¼å›ç­”ä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°è¾ƒä½ï¼Œæ²¡æœ‰ä¸€ä¸ªæ¨¡å‹å¾—åˆ†è¶…è¿‡75%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.05621', 'title': 'A Preliminary Study for GPT-4o on Image Restoration', 'url': 'https://huggingface.co/papers/2505.05621', 'abstract': "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.", 'score': 4, 'issue_id': 3709, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ', 'en': 'May 8', 'zh': '5æœˆ8æ—¥'}, 'hash': '4fd37a23cd5db52f', 'authors': ['Hao Yang', 'Yan Yang', 'Ruikun Zhang', 'Liyuan Pan'], 'affiliations': ['Australian National University', 'Beijing Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.05621.jpg', 'data': {'categories': ['#optimization', '#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ GPT-4o Ğ¾Ñ‚ OpenAI Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ GPT-4o Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¥Ğ¾Ñ‚Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹, Ğ¾Ğ½Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¢ĞµĞ¼ Ğ½Ğµ Ğ¼ĞµĞ½ĞµĞµ, Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ GPT-4o Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ñ‹Ğ¼ĞºĞ¸, Ğ´Ğ¾Ğ¶Ğ´Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Harnessing GPT-4o for Enhanced Image Restoration', 'desc': "The paper explores the capabilities of OpenAI's GPT-4o model in the field of image restoration, highlighting its ability to generate visually appealing images. Despite its impressive outputs, the model struggles with maintaining pixel-level accuracy, leading to issues like incorrect object positioning and altered image proportions. The authors demonstrate that GPT-4o can enhance existing image restoration techniques, particularly in tasks like dehazing and deraining, by providing valuable visual priors. This work aims to establish a foundation for integrating GPT-4o into future restoration workflows and encourages further research in image generation."}, 'zh': {'title': 'GPT-4oï¼šå›¾åƒä¿®å¤çš„æ–°åŠ¨åŠ›', 'desc': 'OpenAIçš„GPT-4oæ¨¡å‹ç»“åˆäº†å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œå±•ç°äº†åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†GPT-4oåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­çš„æ½œåœ¨å½±å“ï¼Œå°½ç®¡å…¶ç”Ÿæˆçš„ä¿®å¤å›¾åƒåœ¨è§†è§‰ä¸Šå¸å¼•äººï¼Œä½†åœ¨åƒç´ çº§ç»“æ„ä¿çœŸåº¦ä¸Šä¸çœŸå®å›¾åƒç›¸æ¯”å­˜åœ¨é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å›¾åƒå»é›¾ã€å»é›¨å’Œä½å…‰å¢å¼ºç­‰æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†GPT-4oçš„è¾“å‡ºå¯ä»¥ä½œä¸ºå¼ºå¤§çš„è§†è§‰å…ˆéªŒï¼Œæ˜¾è‘—æå‡ç°æœ‰å»é›¾ç½‘ç»œçš„æ€§èƒ½ã€‚å¸Œæœ›æœ¬ç ”ç©¶èƒ½åŠ é€Ÿå›¾åƒç”Ÿæˆé¢†åŸŸçš„åˆ›æ–°ï¼Œå¹¶å°†å‘å¸ƒæ¥è‡ª10ä¸ªå¹¿æ³›ä½¿ç”¨çš„å›¾åƒä¿®å¤æ•°æ®é›†çš„GPT-4oä¿®å¤å›¾åƒä»¥æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.21467', 'title': 'Multiview Point Cloud Registration via Optimization in an Autoencoder\n  Latent Space', 'url': 'https://huggingface.co/papers/2504.21467', 'abstract': 'Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.', 'score': 0, 'issue_id': 3717, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': '0bb5c596b88819cc', 'authors': ['Luc Vedrenne', 'Sylvain Faisan', 'Denis Fortun'], 'affiliations': ['ICube Laboratory, IMAGeS team, UMR 7357, CNRS, University of Strasbourg, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.21467.jpg', 'data': {'categories': ['#3d', '#cv', '#synthetic', '#optimization', '#open_source'], 'emoji': 'ğŸŒ€', 'ru': {'title': 'POLAR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº', 'desc': 'POLAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ½ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. POLAR ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ² Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ³Ğ»Ğ°Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'POLAR: Efficient and Robust Multiview Point Cloud Registration', 'desc': 'This paper presents POLAR, a new method for aligning multiple 3D point clouds, which is crucial in computer vision. Unlike traditional pairwise registration methods that struggle with scalability, POLAR efficiently handles many views and is robust against significant data degradation. The approach utilizes a pretrained autoencoder to transform the registration task into a latent space, allowing for better optimization. By incorporating a specialized loss function and a multistart optimization strategy, POLAR outperforms existing techniques on both synthetic and real datasets.'}, 'zh': {'title': 'POLARï¼šé«˜æ•ˆçš„å¤šè§†è§’ç‚¹äº‘é…å‡†æ–¹æ³•', 'desc': 'ç‚¹äº‘åˆšæ€§é…å‡†æ˜¯3Dè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚åœ¨å¤šè§†è§’æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„6Då§¿æ€æ¥å¯¹é½ä¸€ç»„ç‰©ä½“ã€‚ä¼ ç»Ÿçš„åŸºäºæˆå¯¹é…å‡†çš„æ–¹æ³•åœ¨è§†è§’æ•°é‡å¢åŠ æ—¶æ‰©å±•æ€§è¾ƒå·®ï¼Œè€Œç”Ÿæˆæ–¹æ³•è™½ç„¶å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼Œä½†ä¸é€‚åˆå¤„ç†å¤§å˜æ¢å’Œé«˜é™çº§æ°´å¹³ã€‚æœ¬æ–‡æå‡ºäº†POLARï¼ˆç‚¹äº‘æ½œåœ¨é…å‡†ï¼‰ï¼Œå®ƒèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤§é‡è§†è§’ï¼ŒåŒæ—¶å¯¹é«˜é™çº§å’Œå¤§åˆå§‹è§’åº¦å…·æœ‰é²æ£’æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (11)', '#agents (26)', '#agi (9)', '#alignment (15)', '#architecture (31)', '#audio (6)', '#benchmark (68)', '#cv (20)', '#data (33)', '#dataset (64)', '#diffusion (13)', '#ethics (5)', '#games (14)', '#graphs (5)', '#hallucinations (8)', '#healthcare (7)', '#inference (15)', '#interpretability (19)', '#leakage (3)', '#long_context (5)', '#low_resource (10)', '#machine_translation (4)', '#math (15)', '#multilingual (12)', '#multimodal (51)', '#open_source (36)', '#optimization (75)', '#plp (3)', '#rag (4)', '#reasoning (50)', '#rl (27)', '#rlhf (15)', '#robotics (10)', '#science (8)', '#security (2)', '#small_models (10)', '#story_generation (1)', '#survey (13)', '#synthetic (10)', '#training (76)', '#transfer_learning (15)', '#video (13)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-05-15 07:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-15 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-15 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    