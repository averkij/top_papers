
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 123 papers. September 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Сентябрь 2025</span> | <span id="title-articles-count">123 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-08.html">⬅️ <span id="prev-date">08.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-10.html">➡️ <span id="next-date">10.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Сентябрь 2025', 'en': 'September 2025', 'zh': '9月2025年'};
        let feedDateNext = {'ru': '10.2025', 'en': '10/2025', 'zh': '10月2025年'};
        let feedDatePrev = {'ru': '08.2025', 'en': '08/2025', 'zh': '8月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.02547', 'title': 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey', 'url': 'https://huggingface.co/papers/2509.02547', 'abstract': 'Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.', 'score': 69, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '04a4d0adade32d34', 'authors': ['Guibin Zhang', 'Hejia Geng', 'Xiaohang Yu', 'Zhenfei Yin', 'Zaibin Zhang', 'Zelin Tan', 'Heng Zhou', 'Zhongzhi Li', 'Xiangyuan Xue', 'Yijiang Li', 'Yifan Zhou', 'Yang Chen', 'Chen Zhang', 'Yutao Fan', 'Zihu Wang', 'Songtao Huang', 'Yue Liao', 'Hongru Wang', 'Mengyue Yang', 'Heng Ji', 'Michael Littman', 'Jun Wang', 'Shuicheng Yan', 'Philip Torr', 'Lei Bai'], 'affiliations': ['Brown University', 'Chinese Academy of Sciences', 'Dalian University of Technology', 'Fudan University', 'Imperial College London', 'National University of Singapore', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'University College London', 'University of Bristol', 'University of California, San Diego', 'University of California, Santa Barbara', 'University of Georgia', 'University of Illinois Urbana-Champaign', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.02547.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning', '#agi', '#survey', '#rl', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Большие языковые модели становятся автономными агентами', 'desc': 'Агентное обучение с подкреплением (Agentic RL) трансформирует большие языковые модели в автономных агентов, принимающих решения. Это достигается путем использования частично наблюдаемых марковских процессов принятия решений (POMDP) с временной протяженностью. Agentic RL улучшает такие способности как планирование и рассуждение через обучение с подкреплением. Этот подход представляет собой парадигмальный сдвиг от традиционного применения обучения с подкреплением к большим языковым моделям.'}, 'en': {'title': 'Transforming Language Models into Autonomous Decision-Makers', 'desc': 'This paper introduces agentic reinforcement learning (Agentic RL), which changes how large language models (LLMs) operate by allowing them to make autonomous decisions in complex environments. It contrasts traditional single-step Markov Decision Processes (MDPs) used in LLMs with more advanced temporally extended partially observable Markov decision processes (POMDPs) that enable better planning and reasoning. The authors propose a taxonomy that categorizes agentic capabilities like memory and self-improvement, as well as their applications in various tasks. Additionally, the paper provides a resource guide for researchers, summarizing over five hundred studies to outline the current state and future directions of this emerging field.'}, 'zh': {'title': '代理强化学习：从被动生成到自主决策的转变', 'desc': '代理强化学习（Agentic RL）将大型语言模型转变为自主决策的智能体，利用时间扩展的部分可观察马尔可夫决策过程（POMDPs），增强了规划和推理等能力。与传统的单步马尔可夫决策过程（MDPs）相比，代理强化学习使得语言模型不再是被动的序列生成器，而是能够在复杂动态环境中自主决策的智能体。本文提出了一种全面的分类法，围绕核心的代理能力，如规划、工具使用、记忆、推理、自我改进和感知进行组织。通过整合开源环境、基准和框架，本文为未来的研究提供了实用的参考。'}}}, {'id': 'https://huggingface.co/papers/2509.02544', 'title': 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.02544', 'abstract': "UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.", 'score': 63, 'issue_id': 5686, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '71474173af3c991b', 'authors': ['Haoming Wang', 'Haoyang Zou', 'Huatong Song', 'Jiazhan Feng', 'Junjie Fang', 'Junting Lu', 'Longxiang Liu', 'Qinyu Luo', 'Shihao Liang', 'Shijue Huang', 'Wanjun Zhong', 'Yining Ye', 'Yujia Qin', 'Yuwen Xiong', 'Yuxin Song', 'Zhiyong Wu', 'Bo Li', 'Chen Dun', 'Chong Liu', 'Fuxing Leng', 'Hanbin Wang', 'Hao Yu', 'Haobin Chen', 'Hongyi Guo', 'Jing Su', 'Jingjia Huang', 'Kai Shen', 'Kaiyu Shi', 'Lin Yan', 'Peiyao Zhao', 'Pengfei Liu', 'Qinghao Ye', 'Renjie Zheng', 'Wayne Xin Zhao', 'Wen Heng', 'Wenhao Huang', 'Wenqian Wang', 'Xiaobo Qin', 'Yi Lin', 'Youbin Wu', 'Zehui Chen', 'Zihao Wang', 'Baoquan Zhong', 'Xinchun Zhang', 'Xujing Li', 'Yuanfan Li', 'Zhongkai Zhao', 'Chengquan Jiang', 'Faming Wu', 'Haotian Zhou', 'Jinlin Pang', 'Li Han', 'Qianli Ma', 'Siyao Liu', 'Songhua Cai', 'Wenqi Fu', 'Xin Liu', 'Zhi Zhang', 'Bo Zhou', 'Guoliang Li', 'Jiajun Shi', 'Jiale Yang', 'Jie Tang', 'Li Li', 'Taoran Lu', 'Woyu Lin', 'Xiaokang Tong', 'Xinyao Li', 'Yichi Zhang', 'Yu Miao', 'Zhengxuan Jiang', 'Zili Li', 'Ziyuan Zhao', 'Chenxin Li', 'Dehua Ma', 'Feng Lin', 'Ge Zhang', 'Haihua Yang', 'Hangyu Guo', 'Hongda Zhu', 'Jiaheng Liu', 'Junda Du', 'Kai Cai', 'Kuanye Li', 'Lichen Yuan', 'Meilan Han', 'Minchao Wang', 'Shuyue Guo', 'Tianhao Cheng', 'Xiaobo Ma', 'Xiaojun Xiao', 'Xiaolong Huang', 'Xinjie Chen', 'Yidi Du', 'Yilin Chen', 'Yiwen Wang', 'Zhaojian Li', 'Zhenzhu Yang', 'Zhiyuan Zeng', 'Chaolin Jin', 'Chen Li', 'Hao Chen', 'Haoli Chen', 'Jian Chen', 'Qinghao Zhao', 'Guang Shi'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2509.02544.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#reasoning', '#agents', '#rl'], 'emoji': '🤖', 'ru': {'title': 'UI-TARS-2: Новый уровень GUI-агентов с улучшенным обучением и обобщением', 'desc': 'UI-TARS-2 - это модель агента для графических пользовательских интерфейсов, решающая проблемы масштабируемости данных, многоходового обучения с подкреплением и стабильности окружения. Модель использует систематическую методологию обучения, включающую маховик данных для масштабируемой генерации, стабилизированную структуру многоходового RL и гибридную GUI-среду. UI-TARS-2 значительно превосходит предыдущую версию и сильные бейзлайны на различных бенчмарках, достигая высоких показателей в GUI-задачах и игровых средах. Результаты демонстрируют потенциал UI-TARS-2 для продвижения GUI-агентов и обобщения на реальные интерактивные сценарии.'}, 'en': {'title': 'Revolutionizing GUI Agents with UI-TARS-2', 'desc': 'UI-TARS-2 is a new model designed for creating intelligent agents that can interact with graphical user interfaces (GUIs). It tackles key issues like handling large amounts of data, improving learning over multiple interactions, and ensuring stable performance in different environments. The model uses innovative techniques such as a data flywheel for efficient data generation and a hybrid environment that combines various systems for better training. Its performance on various benchmarks shows significant improvements over previous models, indicating its effectiveness in real-world applications.'}, 'zh': {'title': 'UI-TARS-2：提升图形用户界面智能体的未来', 'desc': 'UI-TARS-2是一个以图形用户界面（GUI）为中心的智能体模型，旨在解决数据可扩展性、多轮强化学习和环境稳定性等挑战。该模型通过系统化的训练方法，包括可扩展的数据生成、稳定的多轮强化学习框架和集成文件系统与终端的混合GUI环境，显著提升了性能。实证评估显示，UI-TARS-2在多个基准测试中超越了其前身UI-TARS-1.5和其他强基线模型。该模型在长时间信息检索任务和软件工程基准测试中表现出色，展示了其在多样化智能体任务中的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2509.02479', 'title': 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning', 'url': 'https://huggingface.co/papers/2509.02479', 'abstract': 'SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.', 'score': 62, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'e6fb78d3f5363c7d', 'authors': ['Zhenghai Xue', 'Longtao Zheng', 'Qian Liu', 'Yingru Li', 'Xiaosen Zheng', 'Zejun Ma', 'Bo An'], 'affiliations': ['Nanyang Technological University, Singapore', 'TikTok, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.02479.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#math', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Стабильное обучение AI рассуждать с инструментами', 'desc': 'Статья представляет алгоритм SimpleTIR, стабилизирующий обучение многоходовых моделей с интегрированными инструментами (Tool-Integrated Reasoning, TIR). Метод фильтрует неинформативные шаги, предотвращая взрывной рост градиентов и улучшая процесс обучения. SimpleTIR достигает наилучших результатов на сложных задачах математического рассуждения, значительно повышая показатели базовых моделей. Алгоритм поощряет модель находить разнообразные и сложные паттерны рассуждений, включая самокоррекцию и перекрестную проверку.'}, 'en': {'title': 'Stabilizing Multi-Turn Reasoning with SimpleTIR', 'desc': 'The paper presents SimpleTIR, an innovative algorithm designed to enhance the stability of multi-turn Tool-Integrated Reasoning (TIR) training in large language models. It addresses the issue of training instability caused by distributional drift from external tool feedback, which leads to the generation of low-probability tokens and catastrophic gradient explosions. By filtering out void turns—those that do not produce useful outputs—SimpleTIR prevents harmful gradients from affecting the learning process. As a result, it achieves state-of-the-art performance on math reasoning tasks, significantly improving scores while promoting the discovery of advanced reasoning strategies.'}, 'zh': {'title': 'SimpleTIR：稳定多轮推理训练的创新算法', 'desc': '本文介绍了一种名为SimpleTIR的算法，它通过过滤掉无效回合来稳定多轮工具集成推理（TIR）训练。多轮TIR在使用强化学习时常常面临训练不稳定和性能崩溃的问题，主要是由于外部工具反馈导致的分布漂移。SimpleTIR的核心策略是识别并去除那些既没有代码块也没有最终答案的回合，从而有效阻止有害的高幅度梯度，稳定学习动态。实验结果表明，SimpleTIR在数学推理基准测试中达到了最先进的性能，显著提高了模型的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2509.00676', 'title': 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model', 'url': 'https://huggingface.co/papers/2509.00676', 'abstract': "Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.", 'score': 56, 'issue_id': 5686, 'pub_date': '2025-08-31', 'pub_date_card': {'ru': '31 августа', 'en': 'August 31', 'zh': '8月31日'}, 'hash': '804da0110302d00c', 'authors': ['Xiyao Wang', 'Chunyuan Li', 'Jianwei Yang', 'Kai Zhang', 'Bo Liu', 'Tianyi Xiong', 'Furong Huang'], 'affiliations': ['National University of Singapore', 'The Ohio State University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2509.00676.jpg', 'data': {'categories': ['#optimization', '#games', '#rlhf', '#multimodal', '#benchmark', '#reasoning', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Объединение критика и генератора: новый шаг в мультимодальном ИИ', 'desc': 'В этой статье предлагается новый подход к обучению мультимодальных языковых моделей, объединяющий функции критика и генератора. Исследователи применили обучение с подкреплением на наборах данных с предпочтениями для улучшения базовой генеративной модели. Результатом стала модель LLaVA-Critic-R1, которая превосходит специализированные модели на 26 визуальных тестах понимания и рассуждения. Этот метод позволяет создать унифицированную модель, эффективную как в оценке, так и в генерации контента.'}, 'en': {'title': 'Bridging Evaluation and Generation in Multimodal Systems', 'desc': 'This paper introduces a novel approach to improve generative models by applying reinforcement learning (RL) on preference-labeled critic datasets. Traditionally, critic models evaluate outputs but do not generate responses, creating a divide between evaluation and generation tasks. The authors propose a unified model, LLaVA-Critic-R1, which not only serves as a high-performing critic but also functions effectively as a policy model for generating responses. Their findings demonstrate that this integrated approach enhances performance across various visual reasoning benchmarks, showcasing the potential for self-improving multimodal systems.'}, 'zh': {'title': '强化学习提升生成模型的统一多模态系统', 'desc': '本研究提出了一种新的方法，将带有偏好的评论数据集用于强化学习，以提升生成模型的性能。我们重新组织这些评论数据，直接在基础生成模型上进行训练，开发出LLaVA-Critic-R1，这是一种能够优化偏好判断的多模态评论模型，同时保留生成能力。实验结果表明，LLaVA-Critic-R1不仅在评论任务中表现优异，还在多个视觉推理基准测试中与专门的推理模型相媲美，甚至超越它们。最终，我们的研究表明，利用评论数据进行强化学习可以创建一个在评估和生成方面都表现出色的统一模型，推动多模态系统的自我改进。'}}}, {'id': 'https://huggingface.co/papers/2508.21496', 'title': 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long\n  Video Understanding', 'url': 'https://huggingface.co/papers/2508.21496', 'abstract': "A benchmark for long-video hallucination identifies and investigates Semantic Aggregation Hallucination (SAH), showing its prevalence in complex and rapidly changing semantic contexts, and proposes strategies to mitigate it.  \t\t\t\t\tAI-generated summary \t\t\t\t Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.", 'score': 48, 'issue_id': 5691, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'ca0b1db27fcb23d9', 'authors': ['Hao Lu', 'Jiahao Wang', 'Yaolun Zhang', 'Ruohui Wang', 'Xuanyu Zheng', 'Yepeng Tang', 'Dahua Lin', 'Lewei Lu'], 'affiliations': ['SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.21496.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#hallucinations', '#dataset', '#video'], 'emoji': '🎬', 'ru': {'title': 'Борьба с галлюцинациями в длинных видео: новый бенчмарк и стратегии', 'desc': 'Статья представляет новый бенчмарк ELV-Halluc для оценки галлюцинаций в длинных видео. Авторы вводят понятие Семантической Агрегационной Галлюцинации (САГ), которая возникает при объединении семантики отдельных кадров в семантические группы событий. Исследование показывает, что САГ усиливается с ростом семантической сложности и быстрой сменой контекста в видео. Предлагаются стратегии по снижению САГ, включая улучшение позиционного кодирования и применение метода DPO.'}, 'en': {'title': 'Tackling Semantic Aggregation Hallucination in Long Videos', 'desc': "This paper introduces a new benchmark called ELV-Halluc, specifically designed to study Semantic Aggregation Hallucination (SAH) in long videos. SAH occurs when models incorrectly aggregate frame-level semantics into event-level groups, particularly in complex and rapidly changing contexts. The research shows that SAH is more prevalent in longer videos due to increased semantic complexity and provides strategies to mitigate this issue. By implementing a positional encoding strategy and a DPO approach, the authors demonstrate a significant reduction in SAH, improving the model's performance on video understanding tasks."}, 'zh': {'title': '揭示长视频中的语义聚合幻觉', 'desc': '这篇论文提出了一个针对长视频幻觉的新基准，重点研究了语义聚合幻觉（SAH）。SAH在复杂和快速变化的语义环境中尤为普遍，导致模型生成与视频输入不一致的内容。研究表明，SAH的发生与语义复杂性增加有关，尤其是在多个事件中。为了解决这个问题，论文提出了ELV-Halluc基准，并探讨了缓解SAH的策略，如位置编码和DPO策略。'}}}, {'id': 'https://huggingface.co/papers/2509.01055', 'title': 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use', 'url': 'https://huggingface.co/papers/2509.01055', 'abstract': 'VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2times speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.', 'score': 44, 'issue_id': 5687, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '8d89f7851ae1d950', 'authors': ['Dongfu Jiang', 'Yi Lu', 'Zhuofeng Li', 'Zhiheng Lyu', 'Ping Nie', 'Haozhe Wang', 'Alex Su', 'Hui Chen', 'Kai Zou', 'Chao Du', 'Tianyu Pang', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent', 'National University of Singapore', 'NetMind.AI', 'Sea AI Lab', 'Shanghai University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.01055.jpg', 'data': {'categories': ['#architecture', '#rl', '#open_source', '#agi', '#agents', '#reasoning', '#rlhf', '#training', '#multimodal'], 'emoji': '🛠️', 'ru': {'title': 'VerlTool: Универсальный фреймворк для обучения с подкреплением с использованием инструментов', 'desc': 'VerlTool - это унифицированный и модульный фреймворк для агентного обучения с подкреплением с использованием инструментов. Он решает проблемы неэффективности существующих подходов, обеспечивая конкурентоспособную производительность в нескольких областях. VerlTool предлагает четыре ключевых улучшения: совместимость с VeRL, унифицированное управление инструментами через стандартизированные API, асинхронное выполнение и комплексную оценку на 6 доменах ARLT. Фреймворк формализует ARLT как многоходовые траектории с мультимодальными токенами наблюдений, расширяя парадигмы одноходового RLVR.'}, 'en': {'title': 'Streamlining Agentic Reinforcement Learning with VerlTool', 'desc': 'VerlTool is a new framework designed for Agentic Reinforcement Learning that incorporates tool use, aiming to improve efficiency and performance in various tasks. It addresses the limitations of existing methods by providing a unified system that supports multiple modalities and asynchronous execution, which speeds up processing. The framework allows for easy integration of tools through a modular architecture, making it simpler for researchers to develop and test new ideas. By demonstrating strong performance across different domains, VerlTool encourages broader adoption and innovation in the field of reinforcement learning with tools.'}, 'zh': {'title': 'VerlTool：提升代理强化学习的统一框架', 'desc': 'VerlTool是一个统一且模块化的框架，专注于具有工具使用的代理强化学习，旨在解决现有方法中的低效问题。它通过系统设计原则，提供了四个关键贡献，包括与可验证奖励的上游对齐、统一的工具管理、异步执行以提高速度，以及在多个领域的竞争性表现评估。该框架将代理强化学习形式化为多轮轨迹，支持多模态观察令牌，超越了单轮交互的限制。VerlTool的模块化插件架构使得工具集成变得快速且简单，显著降低了开发成本，为工具增强的强化学习研究提供了可扩展的基础。'}}}, {'id': 'https://huggingface.co/papers/2509.01215', 'title': 'POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion', 'url': 'https://huggingface.co/papers/2509.01215', 'abstract': "A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model is available at https://github.com/Tencent/POINTS-Reader.", 'score': 37, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '1f731f4067d86ef7', 'authors': ['Yuan Liu', 'Zhongyin Zhao', 'Le Tian', 'Haicheng Wang', 'Xubing Ye', 'Yangxiu You', 'Zilin Yu', 'Chuhan Wu', 'Xiao Zhou', 'Yang Yu', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01215.jpg', 'data': {'categories': ['#dataset', '#data', '#optimization', '#synthetic', '#training'], 'emoji': '📄', 'ru': {'title': 'Автоматическое создание высококачественных моделей для извлечения данных из документов', 'desc': 'Статья представляет новый фреймворк для создания высококачественных наборов данных и моделей для извлечения информации из документов. Метод состоит из двух этапов: генерация синтетических данных и итеративное самоулучшение модели на реальных документах. Авторы используют фильтрацию аннотаций и переобучение для повышения качества модели и данных. Результирующая модель POINTS-Reader превосходит многие существующие публичные и проприетарные решения.'}, 'en': {'title': 'Revolutionizing Document Extraction with Synthetic Data and Self-Improvement', 'desc': 'This paper presents a new framework for creating high-quality datasets and models for document extraction using synthetic data generation and iterative self-improvement. It addresses the challenges of manual data labeling and the limitations of existing models in handling complex document formats. The proposed method generates diverse synthetic data to train an initial model, which is then refined through a self-improvement process that involves annotating real documents and verifying the quality of these annotations. The resulting model, POINTS-Reader, demonstrates superior performance compared to existing models, making it a valuable tool for document conversion tasks.'}, 'zh': {'title': '合成数据驱动的文档提取新框架', 'desc': '本文提出了一种构建高质量文档提取数据集和模型的框架，利用合成数据生成和迭代自我改进的方法。该框架分为两个阶段：第一阶段生成大规模多样的合成数据，以便模型能够以统一格式提取关键信息；第二阶段通过自我改进方法，将初步训练的模型适应真实文档。通过对真实文档进行标注、质量验证和模型重训练，逐步提升模型的转换能力和生成数据的质量。最终，训练出的POINTS-Reader模型在性能上超越了许多现有的公共和专有模型。'}}}, {'id': 'https://huggingface.co/papers/2509.02208', 'title': 'Baichuan-M2: Scaling Medical Capability with Large Verifier System', 'url': 'https://huggingface.co/papers/2509.02208', 'abstract': 'A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.', 'score': 25, 'issue_id': 5687, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'f78da0ee0b5088cb', 'authors': ['Baichuan-M2 Team', ':', 'Chengfeng Dou', 'Chong Liu', 'Fan Yang', 'Fei Li', 'Jiyuan Jia', 'Mingyang Chen', 'Qiang Ju', 'Shuai Wang', 'Shunya Dang', 'Tianpeng Li', 'Xiangrong Zeng', 'Yijie Zhou', 'Chenzheng Zhu', 'Da Pan', 'Fei Deng', 'Guangwei Ai', 'Guosheng Dong', 'Hongda Zhang', 'Jinyang Tai', 'Jixiang Hong', 'Kai Lu', 'Linzhuang Sun', 'Peidong Guo', 'Qian Ma', 'Rihui Xin', 'Shihui Yang', 'Shusen Zhang', 'Yichuan Mo', 'Zheng Liang', 'Zhishou Zhang', 'Hengfu Cui', 'Zuyi Zhu', 'Xiaochuan Wang'], 'affiliations': ['Baichuan-M2 Team'], 'pdf_title_img': 'assets/pdf/title_img/2509.02208.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#rl', '#open_source', '#reasoning', '#agents', '#alignment', '#training', '#healthcare'], 'emoji': '🩺', 'ru': {'title': 'Динамическая верификация LLM для реальной клинической практики', 'desc': 'Эта статья представляет новую динамическую систему верификации для улучшения работы больших языковых моделей (LLM) в реальных клинических условиях. Система включает симулятор пациента и генератор клинических рубрик для создания интерактивной среды обучения. На основе этой системы авторы разработали модель Baichuan-M2 с 32 миллиардами параметров, обученную с помощью усиленного обучения. Baichuan-M2 превзошла другие открытые модели в тесте HealthBench, демонстрируя эффективность предложенного подхода для применения LLM в медицине.'}, 'en': {'title': 'Revolutionizing Clinical Decision-Making with Dynamic Verification in AI', 'desc': 'This paper presents a dynamic verification framework that enhances the performance of large medical language models (LLMs) in real-world clinical settings using reinforcement learning. The framework addresses the limitations of traditional static benchmarks by incorporating a Patient Simulator and a Clinical Rubrics Generator, which create realistic clinical scenarios and dynamic evaluation metrics. The authors introduce Baichuan-M2, a 32B-parameter model trained with an advanced Group Relative Policy Optimization algorithm, which significantly outperforms existing models on the HealthBench benchmark. This work highlights the importance of dynamic verification systems in aligning LLM capabilities with practical healthcare applications, setting a new standard for medical AI performance.'}, 'zh': {'title': '动态验证提升医疗AI决策能力', 'desc': '本论文提出了一种动态验证框架，利用强化学习和新算法提升大型医疗语言模型在真实临床决策中的表现。传统的静态基准测试无法有效反映医疗咨询的动态互动特性，因此我们设计了一个包含患者模拟器和临床评分生成器的系统。通过多阶段强化学习策略和改进的群体相对策略优化算法（GRPO），我们开发了Baichuan-M2模型，并在HealthBench上取得了优异的成绩。我们的研究表明，强大的动态验证系统对于将大型语言模型的能力与实际临床应用对齐至关重要。'}}}, {'id': 'https://huggingface.co/papers/2509.01563', 'title': 'Kwai Keye-VL 1.5 Technical Report', 'url': 'https://huggingface.co/papers/2509.01563', 'abstract': "Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.", 'score': 23, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': 'b033203f893abafc', 'authors': ['Biao Yang', 'Bin Wen', 'Boyang Ding', 'Changyi Liu', 'Chenglong Chu', 'Chengru Song', 'Chongling Rao', 'Chuan Yi', 'Da Li', 'Dunju Zang', 'Fan Yang', 'Guorui Zhou', 'Guowang Zhang', 'Han Shen', 'Hao Peng', 'Haojie Ding', 'Hao Wang', 'Hengrui Ju', 'Jiaming Huang', 'Jiangxia Cao', 'Jiankang Chen', 'Jingyun Hua', 'Kaibing Chen', 'Kaiyu Jiang', 'Kaiyu Tang', 'Kun Gai', 'Muhao Wei', 'Qiang Wang', 'Ruitao Wang', 'Sen Na', 'Shengnan Zhang', 'Siyang Mao', 'Sui Huang', 'Tianke Zhang', 'Tingting Gao', 'Wei Chen', 'Wei Yuan', 'Xiangyu Wu', 'Xiao Hu', 'Xingyu Lu', 'Yi-Fan Zhang', 'Yiping Yang', 'Yulong Chen', 'Zeyi Lu', 'Zhenhua Wu', 'Zhixin Ling', 'Zhuoran Yang', 'Ziming Li', 'Di Xu', 'Haixuan Gao', 'Hang Li', 'Jing Wang', 'Lejian Ren', 'Qigen Hu', 'Qianqian Wang', 'Shiyao Wang', 'Xinchen Luo', 'Yan Li', 'Yuhang Hu', 'Zixing Zhang'], 'affiliations': ['Kuaishou Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.01563.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#long_context', '#rl', '#training', '#alignment'], 'emoji': '🎥', 'ru': {'title': 'Революция в понимании видео: Keye-VL-1.5 объединяет эффективность и точность', 'desc': 'Keye-VL-1.5 - это новая модель машинного обучения для понимания видео. Она использует стратегию кодирования Slow-Fast, которая обрабатывает ключевые кадры с высоким разрешением, а статичные - с низким. Модель применяет прогрессивное предобучение, увеличивая контекст до 128 тысяч токенов. Послеобучение включает улучшение рассуждений и выравнивание с человеческими предпочтениями.'}, 'en': {'title': 'Revolutionizing Video Understanding with Keye-VL-1.5', 'desc': 'Keye-VL-1.5 is a new model designed to improve video understanding by using a Slow-Fast encoding strategy that optimizes how videos are processed. This model employs a progressive pre-training approach that allows it to handle longer videos and more complex visual information effectively. Additionally, it includes a post-training pipeline that enhances reasoning capabilities and aligns with human preferences through advanced training techniques. Overall, Keye-VL-1.5 outperforms existing models in video tasks while still performing well in general multimodal applications.'}, 'zh': {'title': 'Keye-VL-1.5：视频理解的新突破', 'desc': 'Keye-VL-1.5通过慢-快编码策略、渐进式预训练和后训练推理改进，提升了视频理解能力。慢-快编码策略根据帧间相似性动态分配计算资源，处理关键帧时使用高分辨率，而对静态帧则使用低分辨率以增加时间覆盖。渐进式预训练方法将模型的上下文长度从8K扩展到128K，使其能够处理更长的视频和更复杂的视觉内容。经过广泛评估，Keye-VL-1.5在视频理解任务上显著优于现有模型，同时在多模态基准测试中保持竞争力。'}}}, {'id': 'https://huggingface.co/papers/2509.01363', 'title': 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic', 'url': 'https://huggingface.co/papers/2509.01363', 'abstract': "Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: v_{reason} = theta_{GRPO} - theta_{SFT}. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.", 'score': 20, 'issue_id': 5688, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '1110b5cc006571ff', 'authors': ['Mohammad Zbeeb', 'Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['American University of Beirut (AUB)', 'King Abdullah University of Science and Technology (KAUST)'], 'pdf_title_img': 'assets/pdf/title_img/2509.01363.jpg', 'data': {'categories': ['#benchmark', '#training', '#rl', '#open_source', '#transfer_learning', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Передача способностей к рассуждению между языковыми моделями', 'desc': 'Исследование показывает, что способности к рассуждению, полученные с помощью обучения с подкреплением, можно извлечь в виде вектора задачи и передать другим моделям. Этот вектор рассуждений улучшает производительность на различных тестах, требующих логического мышления. Метод позволяет повторно использовать вычислительные ресурсы, затраченные на обучение существующих моделей. Простое арифметическое добавление вектора к совместимым моделям значительно повышает их способности к рассуждению.'}, 'en': {'title': 'Transfer Reasoning Skills with Task Vectors!', 'desc': 'This paper explores how reasoning skills learned through reinforcement learning can be extracted and reused in other models. The authors create a task vector that represents the reasoning capabilities gained from reinforcement learning, allowing it to be transferred to different models. By applying this vector to instruction-tuned models, they achieve significant performance improvements on various reasoning tasks. This approach demonstrates a cost-effective method to enhance model performance by leveraging previously acquired knowledge without the need for extensive retraining.'}, 'zh': {'title': '提取推理能力，提升模型表现', 'desc': '本研究探讨了如何从强化学习中提取推理能力，并将其作为任务向量转移到其他模型中，以提高在不同基准上的表现。我们使用两个相同初始化的Qwen2.5模型，一个经过监督微调（SFT），另一个经过群体相对策略优化（GRPO）。通过计算这两个模型的参数差异，我们提取了一个推理向量，该向量能够捕捉到强化学习所带来的推理能力。将这个向量添加到兼容的指令调优模型中，可以显著提高模型在多个推理基准上的表现。'}}}, {'id': 'https://huggingface.co/papers/2509.02534', 'title': 'Jointly Reinforcing Diversity and Quality in Language Model Generations', 'url': 'https://huggingface.co/papers/2509.02534', 'abstract': 'DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.', 'score': 18, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'b3e51a0003bb3957', 'authors': ['Tianjian Li', 'Yiming Zhang', 'Ping Yu', 'Swarnadeep Saha', 'Daniel Khashabi', 'Jason Weston', 'Jack Lanchantin', 'Tianlu Wang'], 'affiliations': ['Carnegie Mellon University', 'FAIR', 'Johns Hopkins University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2509.02534.jpg', 'data': {'categories': ['#games', '#story_generation', '#rlhf', '#optimization', '#rl', '#training'], 'emoji': '🌈', 'ru': {'title': 'DARLING: Качество и разнообразие в гармонии', 'desc': 'DARLING - это фреймворк обучения с подкреплением, который улучшает как качество, так и разнообразие выходных данных больших языковых моделей. Он вводит обученную функцию разбиения для измерения семантического разнообразия, выходящего за рамки лексических вариаций. DARLING объединяет сигнал разнообразия с оценкой качества во время онлайн-обучения с подкреплением. Эксперименты показывают, что DARLING превосходит базовые модели RL, ориентированные только на качество, создавая результаты более высокого качества и новизны.'}, 'en': {'title': 'Enhancing Creativity with Diversity-Aware Reinforcement Learning', 'desc': 'DARLING is a framework designed to improve the outputs of large language models by focusing on both quality and diversity. Traditional post-training methods often enhance accuracy but limit the variety of responses, which is crucial for creative tasks. DARLING addresses this by using a learned partition function to assess diversity, allowing the model to generate responses that are not only accurate but also unique. Experiments show that DARLING outperforms standard reinforcement learning approaches, leading to better quality and more diverse outputs across various tasks.'}, 'zh': {'title': 'DARLING：提升语言模型输出的质量与多样性', 'desc': 'DARLING是一个关注多样性的强化学习框架，旨在提高大型语言模型在各种任务中的输出质量和多样性。传统的后训练方法往往只关注准确性和实用性，导致输出的多样性降低。DARLING通过引入学习的分区函数来衡量多样性，并在在线强化学习中结合质量奖励，鼓励模型生成高质量且独特的输出。实验结果表明，DARLING在非可验证和可验证任务中均表现优异，生成的输出在质量和新颖性上均优于仅关注质量的基线。'}}}, {'id': 'https://huggingface.co/papers/2509.02522', 'title': 'Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR', 'url': 'https://huggingface.co/papers/2509.02522', 'abstract': 'PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, a novel RLVR framework that achieves imPlicit Actor Critic coupling via a Supervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.', 'score': 18, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '824c46ed359a21d1', 'authors': ['Jiaming Li', 'Longze Chen', 'Ze Gong', 'Yukun Chen', 'Lu Wang', 'Wanwei He', 'Run Luo', 'Min Yang'], 'affiliations': ['Ritzz-AI', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.02522.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rl', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'PACS: Эффективное обучение языковых моделей рассуждению через супервизорный RLVR', 'desc': 'PACS - это новый фреймворк для обучения с подкреплением с проверяемыми наградами (RLVR), который переформулирует задачу RLVR как задачу обучения с учителем. Это позволяет повысить стабильность и эффективность обучения больших языковых моделей для решения задач рассуждения. PACS использует функцию оценки, параметризованную моделью политики и оптимизированную с помощью кросс-энтропийной функции потерь. Этот подход неявно объединяет роли актора и критика, что приводит к более стабильному и эффективному обучению по сравнению с классическими методами RLVR.'}, 'en': {'title': 'PACS: Transforming RLVR into Supervised Learning for Better Reasoning', 'desc': 'PACS is a new framework that reformulates Reinforcement Learning with Verifiable Rewards (RLVR) as a supervised learning task, which enhances the stability and efficiency of training large language models (LLMs) for reasoning tasks. By treating outcome rewards as predictable labels, PACS transforms the RLVR problem into a supervised learning problem, optimizing it with cross-entropy loss. This approach allows for implicit coupling of actor and critic roles, leading to more stable policy updates and improved training outcomes. Benchmark results show that PACS significantly outperforms traditional RLVR methods like PPO and GRPO in mathematical reasoning tasks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'PACS：稳定高效的推理训练新框架', 'desc': 'PACS是一种新颖的强化学习可验证奖励（RLVR）框架，它将RLVR重新定义为一个监督学习任务，从而提高了大语言模型在推理任务中的稳定性和效率。通过将结果奖励视为可预测的标签，PACS将RLVR问题转化为一个基于策略模型的分数函数的监督学习任务，并使用交叉熵损失进行优化。详细的梯度分析表明，这种监督形式本质上恢复了经典的策略梯度更新，同时隐式地耦合了演员和评论者的角色，从而实现了更稳定和高效的训练。在具有挑战性的数学推理任务上，PACS的表现优于强大的RLVR基线，如PPO和GRPO，显示出其在推理性能上的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2509.00605', 'title': 'Gated Associative Memory: A Parallel O(N) Architecture for Efficient\n  Sequence Modeling', 'url': 'https://huggingface.co/papers/2509.00605', 'abstract': 'Gated Associative Memory (GAM) network offers a linear complexity alternative to the Transformer architecture, improving training speed and achieving competitive validation perplexity.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling.', 'score': 18, 'issue_id': 5697, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '874c41bbbff9e11e', 'authors': ['Rishiraj Acharya'], 'affiliations': ['Independent Researcher'], 'pdf_title_img': 'assets/pdf/title_img/2509.00605.jpg', 'data': {'categories': ['#long_context', '#training', '#architecture', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'GAM: Быстрее и эффективнее Transformer для обработки последовательностей', 'desc': 'Статья представляет новую архитектуру нейронной сети под названием Gated Associative Memory (GAM), которая предлагает альтернативу Transformer с линейной сложностью. GAM заменяет слой self-attention двумя параллельными путями: каузальной свёрткой и механизмом извлечения ассоциативной памяти. Эксперименты показывают, что GAM превосходит Transformer и Mamba по скорости обучения на датасетах WikiText-2 и TinyStories. GAM демонстрирует конкурентоспособную или превосходящую перплексность на валидационных данных.'}, 'en': {'title': 'GAM: A Faster, Smarter Alternative to Transformers', 'desc': "The Gated Associative Memory (GAM) network presents a new approach to sequence modeling that operates with linear complexity, making it faster than traditional Transformer models. By replacing the self-attention mechanism with a combination of causal convolution and associative memory retrieval, GAM effectively captures both local and global context in data. This architecture allows for dynamic integration of information, enhancing the model's ability to process sequences efficiently. Experimental results show that GAM not only improves training speed but also achieves competitive performance in terms of validation perplexity on various datasets."}, 'zh': {'title': 'GAM：高效的序列建模新选择', 'desc': 'Gated Associative Memory (GAM) 网络是一种新型的序列建模架构，具有线性复杂度（O(N)），相比于传统的Transformer架构（O(N^2)），在处理长序列时更为高效。GAM通过两个并行路径替代了自注意力层：一个因果卷积用于捕捉局部上下文，另一个并行的关联记忆检索机制用于建模全局模式。这种路径通过门控机制动态融合，使模型能够灵活地结合每个标记的局部和全局信息。实验结果表明，GAM在训练速度上优于标准Transformer模型，并在多个数据集上实现了更好的验证困惑度，显示出其作为序列建模的高效替代方案的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.02563', 'title': 'DynaGuard: A Dynamic Guardrail Model With User-Defined Policies', 'url': 'https://huggingface.co/papers/2509.02563', 'abstract': 'Dynamic guardian models evaluate text based on user-defined policies, offering fast and accurate detection of both static harms and free-form policy violations.  \t\t\t\t\tAI-generated summary \t\t\t\t Guardian models are used to supervise and moderate the outputs of user-facing chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian models like LlamaGuard detect predefined, static categories of harms. We propose dynamic guardian models that evaluate text based on user-defined policies, making them useful for different application domains that are not addressed by standard guardian models. Our dynamic guardian models can be used for fast detection of policy violations or with chain-of-thought reasoning that articulates and justifies the model outputs. Our dynamic guardian models match static models in detection accuracy for static harm categories while identifying violations of free-form policies with accuracy comparable to frontier reasoning models in a fraction of the time.', 'score': 15, 'issue_id': 5696, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'b4dda88cf231d2fa', 'authors': ['Monte Hoover', 'Vatsal Baherwani', 'Neel Jain', 'Khalid Saifullah', 'Joseph Vincent', 'Chirag Jain', 'Melissa Kazemi Rad', 'C. Bayan Bruss', 'Ashwinee Panda', 'Tom Goldstein'], 'affiliations': ['Capital One', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2509.02563.jpg', 'data': {'categories': ['#reasoning', '#ethics', '#alignment', '#agents', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Гибкая защита контента с помощью ИИ', 'desc': 'Статья представляет концепцию динамических моделей-хранителей для оценки текста на основе пользовательских политик. Эти модели способны быстро и точно выявлять как статические угрозы, так и нарушения произвольных политик. В отличие от стандартных моделей-хранителей, динамические модели могут применяться в различных предметных областях. Они сочетают высокую точность обнаружения с возможностью объяснения своих выводов через цепочку рассуждений.'}, 'en': {'title': 'Empowering Text Evaluation with Dynamic Guardian Models', 'desc': 'Dynamic guardian models are advanced tools that assess text according to specific rules set by users, allowing for quick and precise identification of both fixed harms and flexible policy breaches. Unlike traditional models that only recognize predefined categories of issues, these dynamic models adapt to various contexts and requirements. They not only ensure compliance with user-defined standards but also provide reasoning behind their evaluations, enhancing transparency. Furthermore, they achieve similar accuracy to static models for known harms while efficiently detecting more complex violations, making them suitable for diverse applications.'}, 'zh': {'title': '动态守护模型：灵活的文本评估与政策检测', 'desc': '动态守护模型根据用户定义的政策评估文本，能够快速准确地检测静态危害和自由形式的政策违规。与标准守护模型不同，动态守护模型适用于不同的应用领域，提供灵活的监督和调节功能。它们不仅能快速检测政策违规，还能通过推理链条清晰地阐述和解释模型的输出。动态守护模型在静态危害检测的准确性上与静态模型相当，同时在自由形式政策的识别上也能达到前沿推理模型的准确性，且速度更快。'}}}, {'id': 'https://huggingface.co/papers/2509.02333', 'title': 'DCPO: Dynamic Clipping Policy Optimization', 'url': 'https://huggingface.co/papers/2509.02333', 'abstract': "DCPO, a novel reinforcement learning framework, enhances large language models by dynamically adjusting clipping bounds and standardizing rewards, leading to improved performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.", 'score': 15, 'issue_id': 5691, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '3a6fa32ebaa3d08e', 'authors': ['Shihui Yang', 'Chengfeng Dou', 'Peidong Guo', 'Kai Lu', 'Qiang Ju', 'Fei Deng', 'Rihui Xin'], 'affiliations': ['Baichuan.inc'], 'pdf_title_img': 'assets/pdf/title_img/2509.02333.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#rlhf', '#training', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'DCPO: Динамическая оптимизация для мощного обучения языковых моделей', 'desc': 'DCPO - это новый метод обучения с подкреплением для улучшения больших языковых моделей. Он динамически настраивает границы отсечения и стандартизирует вознаграждения, что повышает эффективность обучения. DCPO превзошел существующие методы на нескольких бенчмарках, используя различные модели. Метод значительно улучшил использование сгенерированных данных при обучении с подкреплением больших языковых моделей.'}, 'en': {'title': 'Dynamic Clipping for Enhanced Learning in Language Models', 'desc': 'DCPO is a new reinforcement learning framework designed to improve large language models by dynamically adjusting clipping bounds and standardizing rewards. This approach addresses the issue of zero gradients that can occur with fixed clipping bounds and identical rewards, which hinder effective learning. By adapting clipping strategies based on token-specific probabilities and smoothing reward standardization, DCPO enhances exploration and utilization of generated responses. The framework has demonstrated state-of-the-art performance across multiple benchmarks, significantly improving training efficiency and reducing clipping ratios compared to previous methods.'}, 'zh': {'title': '动态剪切策略优化：提升语言模型的强化学习效率', 'desc': 'DCPO是一种新颖的强化学习框架，旨在通过动态调整剪切边界和标准化奖励来增强大型语言模型的性能和效率。该方法解决了现有技术中由于固定剪切边界导致的零梯度问题，从而提高了梯度更新的有效性。DCPO引入了一种动态剪切策略，根据特定的先验概率自适应调整剪切边界，促进了令牌级别的探索。实验结果表明，DCPO在多个基准测试中表现优异，显著提高了训练效率和生成响应的有效利用。'}}}, {'id': 'https://huggingface.co/papers/2509.02460', 'title': 'GenCompositor: Generative Video Compositing with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2509.02460', 'abstract': 'A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.  \t\t\t\t\tAI-generated summary \t\t\t\t Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.', 'score': 14, 'issue_id': 5688, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'cbe52bb6a0af85b6', 'authors': ['Shuzhou Yang', 'Xiaoyu Li', 'Xiaodong Cun', 'Guangzhi Wang', 'Lingen Li', 'Ying Shan', 'Jian Zhang'], 'affiliations': ['ARC Lab, Tencent', 'GVC Lab, Great Bay University', 'SECE, Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.02460.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#video'], 'emoji': '🎬', 'ru': {'title': 'Генеративный видеокомпозитинг: новый уровень автоматизации в производстве видео', 'desc': 'Статья представляет новый подход к автоматизации видеокомпозитинга с использованием генеративных моделей. Авторы разработали пайплайн на основе Diffusion Transformer (DiT), который позволяет адаптивно внедрять информацию об идентичности и движении объектов в целевое видео. Система включает ветвь сохранения фона, блок слияния DiT и расширенное позиционное кодирование (ERoPE) для пользовательского контроля. Для обучения и оценки модели был создан датасет VideoComp, содержащий 61 тысячу наборов видео.'}, 'en': {'title': 'Automating Video Compositing with Adaptive Diffusion Transformers', 'desc': 'This paper presents a new method for automating video compositing using a Diffusion Transformer (DiT) pipeline. The approach allows for the adaptive injection of identity and motion information into videos, enabling user customization of dynamic elements. By incorporating a background preservation branch and a DiT fusion block, the method maintains video consistency while enhancing the integration of foreground and background elements. The authors also introduce a new dataset, VideoComp, to support their task, demonstrating that their method outperforms existing solutions in terms of fidelity and consistency.'}, 'zh': {'title': '自动化视频合成的新方法', 'desc': '这篇论文介绍了一种新颖的扩散变换器（Diffusion Transformer）管道，用于自动化视频合成。该方法通过自适应注入身份和运动信息，保持视频的一致性，并允许用户进行个性化定制。传统的视频合成需要大量人力和专业知识，而这种生成模型能够显著缩短制作周期，降低成本。实验结果表明，该方法在视频合成的保真度和一致性方面优于现有解决方案。'}}}, {'id': 'https://huggingface.co/papers/2509.01644', 'title': 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning', 'url': 'https://huggingface.co/papers/2509.01644', 'abstract': "OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.", 'score': 14, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '0197aa14702197c7', 'authors': ['Yanqing Liu', 'Xianhang Li', 'Letian Zhang', 'Zirui Wang', 'Zeyu Zheng', 'Yuyin Zhou', 'Cihang Xie'], 'affiliations': ['Apple', 'University of California Berkeley', 'University of California Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2509.01644.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#training', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Упрощение архитектуры для эффективного обучения мультимодальных моделей', 'desc': 'OpenVision 2 представляет собой упрощенную версию архитектуры OpenVision, в которой удален текстовый энкодер и контрастивная функция потерь. Эта модификация позволяет значительно сократить время обучения и потребление памяти, сохраняя при этом конкурентоспособную производительность на различных мультимодальных бенчмарках. Модель использует только функцию потерь для генерации подписей к изображениям в качестве сигнала обучения. Благодаря повышенной эффективности обучения, OpenVision 2 может масштабироваться до более чем 1 миллиарда параметров.'}, 'en': {'title': 'Streamlined Efficiency: OpenVision 2 Reimagined', 'desc': "OpenVision 2 is a streamlined version of the original OpenVision architecture that eliminates the text encoder and contrastive loss, focusing solely on a generative training approach with captioning loss. This simplification leads to significant improvements in training efficiency, reducing both training time and memory usage while maintaining competitive performance on multimodal benchmarks. For instance, it achieves a 1.5x reduction in training time and a 1.8x decrease in memory consumption, allowing for larger batch sizes. The architecture's ability to scale to over 1 billion parameters suggests a promising direction for future multimodal foundation models."}, 'zh': {'title': '简化架构，提升效率——OpenVision 2', 'desc': 'OpenVision 2通过去除文本编码器和对比损失，简化了原有架构，从而提高了训练效率。该模型仅保留了生成性训练信号的字幕损失，表现出与原始模型相当的性能。尽管进行了简化，OpenVision 2在多模态基准测试中仍然表现出色，同时显著减少了训练时间和内存消耗。我们相信，这种轻量级的生成性范式对未来多模态基础模型的发展具有重要意义。'}}}, {'id': 'https://huggingface.co/papers/2509.01440', 'title': 'Benchmarking Optimizers for Large Language Model Pretraining', 'url': 'https://huggingface.co/papers/2509.01440', 'abstract': 'A comprehensive evaluation of recent optimization techniques for Large Language Models provides guidance on selecting the best optimizer for different pretraining scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods.', 'score': 12, 'issue_id': 5690, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '086d91ca4e22a4be', 'authors': ['Andrei Semenov', 'Matteo Pagliardini', 'Martin Jaggi'], 'affiliations': ['EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2509.01440.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#training'], 'emoji': '🔬', 'ru': {'title': 'Оптимизаторы для LLM: всесторонний анализ и практические рекомендации', 'desc': 'Статья представляет комплексную оценку современных методов оптимизации для больших языковых моделей (LLM). Авторы провели систематическое сравнение различных оптимизаторов, варьируя размер модели, размер батча и продолжительность обучения. Исследование предоставляет практические рекомендации по выбору оптимального метода оптимизации для разных сценариев предобучения LLM. Результаты работы могут помочь в разработке и тщательном сравнении будущих методов оптимизации.'}, 'en': {'title': 'Optimizing Large Language Models: A Guide to Choosing the Right Optimizer', 'desc': 'This paper evaluates various optimization techniques for Large Language Models (LLMs) to help researchers and practitioners choose the best optimizer for different pretraining scenarios. It addresses the challenges of comparing methods due to varying experimental protocols and provides a systematic analysis by varying model size, batch size, and training duration. The study offers insights into which optimizers yield faster convergence and reduced dependency on hyperparameters. Additionally, the authors release their code for reproducibility, aiming to support future research in optimization methods.'}, 'zh': {'title': '选择最佳优化器，提升大型语言模型性能', 'desc': '本文对大型语言模型（LLM）的优化技术进行了全面评估，旨在为不同的预训练场景选择最佳优化器提供指导。研究中系统地变化了模型大小、批量大小和训练时长，以便对各种优化方法进行标准化比较。通过对每种方法的细致调优，本文为实践者提供了在特定场景下选择优化器的建议。同时，研究还指出了未来优化研究的有希望方向，并通过发布代码和实验结果，确保了研究的可重复性。'}}}, {'id': 'https://huggingface.co/papers/2509.02040', 'title': 'Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation', 'url': 'https://huggingface.co/papers/2509.02040', 'abstract': 'Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.', 'score': 11, 'issue_id': 5686, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '085dff767b3e88ce', 'authors': ['Guangzeng Han', 'Weisi Liu', 'Xiaolei Huang'], 'affiliations': ['Department of Computer Science, University of Memphis'], 'pdf_title_img': 'assets/pdf/title_img/2509.02040.jpg', 'data': {'categories': ['#optimization', '#data', '#synthetic', '#training', '#dataset'], 'emoji': '🧬', 'ru': {'title': 'Генетический подход к созданию качественных синтетических данных для NLP', 'desc': 'Genetic Prompt - это новый метод улучшения качества и разнообразия синтетических данных в обработке естественного языка. Он комбинирует генетические алгоритмы с большими языковыми моделями для создания более реалистичных наборов данных. Метод рассматривает семантические атрибуты текста как генетические последовательности и использует языковую модель для симуляции операций скрещивания и мутации. Эксперименты показали, что Genetic Prompt превосходит современные базовые методы и значительно улучшает производительность моделей машинного обучения, особенно в сценариях с несбалансированными классами.'}, 'en': {'title': 'Genetic Prompt: Evolving Synthetic Data for NLP Excellence', 'desc': "The paper introduces Genetic Prompt, a framework that enhances the quality and diversity of synthetic data in natural language processing (NLP) by integrating genetic algorithms with large language models (LLMs). It treats semantic text attributes as gene sequences, applying genetic operations like crossover and mutation to generate diverse data combinations. This method improves the synthetic data's resemblance to real-world distributions, which is crucial for training effective models. Additionally, the framework incorporates an active learning strategy to optimize the selection of parent data, leading to better performance in various NLP tasks, especially in scenarios with class imbalance."}, 'zh': {'title': '遗传提示：提升合成数据质量与多样性', 'desc': 'Genetic Prompt是一种新颖的框架，通过将遗传算法与大型语言模型（LLMs）结合，提升了自然语言处理中的合成数据质量和多样性。该方法将语义文本属性视为基因序列，并利用LLM模拟交叉和突变操作，从而生成新的属性组合。通过这种遗传过程，合成数据的分布更接近真实世界的数据，优化了下游模型的性能。实验结果表明，Genetic Prompt在多个NLP任务中显著优于现有的基准方法，尤其在类别不平衡的情况下，融合合成数据与原始训练集能显著提升模型表现。'}}}, {'id': 'https://huggingface.co/papers/2509.01052', 'title': 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games', 'url': 'https://huggingface.co/papers/2509.01052', 'abstract': "FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.", 'score': 9, 'issue_id': 5686, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '909be5826c565f9d', 'authors': ['Jaewoo Ahn', 'Junseo Kim', 'Heeseung Yun', 'Jaehyeon Son', 'Dongmin Park', 'Jaewoong Cho', 'Gunhee Kim'], 'affiliations': ['Georgia Institute of Technology', 'KRAFTON', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01052.jpg', 'data': {'categories': ['#benchmark', '#agents', '#optimization', '#games'], 'emoji': '🕹️', 'ru': {'title': 'Преодолевая разрыв между наблюдением и действием в играх-квестах', 'desc': 'В статье представлен новый бенчмарк FlashAdventure для оценки агентов с графическим интерфейсом в Flash-играх жанра квест. Предложена система CUA-as-a-Judge для автоматической оценки игрового процесса. Разработан фреймворк COAST, улучшающий долгосрочную память агентов для планирования и решения последовательных задач. Эксперименты показали, что COAST повышает эффективность агентов, но разрыв с человеческими результатами все еще значителен.'}, 'en': {'title': 'Bridging the Gap: Enhancing GUI Agents in Adventure Games', 'desc': "This paper introduces the FlashAdventure benchmark and the COAST framework to enhance the performance of GUI agents in completing full story arcs in Flash-based adventure games. The research addresses the observation-behavior gap, which is the difficulty agents face in recalling and utilizing past gameplay information effectively. By providing a diverse set of 34 adventure games, the benchmark allows for a more comprehensive evaluation of agents' abilities to navigate complex narratives. The COAST framework incorporates long-term memory strategies, leading to improved task planning and milestone completion, although a significant performance gap between human players and agents remains."}, 'zh': {'title': '提升游戏代理的故事情节完成能力', 'desc': '本文介绍了FlashAdventure基准和COAST框架，旨在提高图形用户界面（GUI）代理在完成Flash冒险游戏完整故事情节方面的表现。研究指出，现有的游戏基准缺乏多样性，且很少评估代理完成整个故事线的能力。为了解决观察-行为差距的问题，FlashAdventure基准包含34款Flash冒险游戏，专注于测试完整故事情节的完成。COAST框架通过利用长期线索记忆，帮助代理更好地规划和解决顺序任务，从而提高了里程碑的完成率。'}}}, {'id': 'https://huggingface.co/papers/2509.01360', 'title': 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision', 'url': 'https://huggingface.co/papers/2509.01360', 'abstract': 'M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding.', 'score': 8, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '3dcca4eb6e49e24e', 'authors': ['Che Liu', 'Zheng Jiang', 'Chengyu Fang', 'Heng Guo', 'Yan-Jie Zhou', 'Jiaqi Qu', 'Le Lu', 'Minfeng Xu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Imperial College London', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01360.jpg', 'data': {'categories': ['#cv', '#multimodal', '#dataset', '#transfer_learning', '#optimization', '#healthcare'], 'emoji': '🏥', 'ru': {'title': 'Единый энкодер для мультимодальных медицинских изображений', 'desc': 'M3Ret - это унифицированный визуальный энкодер, обученный на крупномасштабном наборе данных с гибридными модальностями. Он достигает наилучших результатов в задаче поиска изображений по изображению с нулевым обучением и кросс-модальном выравнивании, используя генеративные и контрастивные парадигмы самообучения. M3Ret обучается на наборе данных из 867,653 медицинских изображений, включая 2D рентгеновские снимки, УЗИ, RGB видео эндоскопии и 3D КТ-сканы. Модель превосходит сильные базовые линии, такие как DINOv3 и BMC-CLIP, обученный с использованием текстовой разметки.'}, 'en': {'title': 'Unified Learning for Enhanced Medical Image Retrieval', 'desc': 'M3Ret is a unified visual encoder designed to improve medical image retrieval by using a large dataset that includes various types of medical images. It employs generative and contrastive self-supervised learning techniques to create transferable visual representations without needing separate models for different image modalities. This approach allows M3Ret to excel in zero-shot image-to-image retrieval and cross-modal alignment, even with unseen data like MRI scans. The results indicate that M3Ret can effectively generalize across different medical imaging tasks, paving the way for more integrated solutions in medical image analysis.'}, 'zh': {'title': 'M3Ret：统一的医学影像检索新突破', 'desc': 'M3Ret是一种统一的视觉编码器，使用大规模混合模态数据集进行训练，能够在零样本图像检索和跨模态对齐方面达到最先进的水平。该方法结合了生成式和对比自监督学习，克服了传统方法在2D、3D和视频医学数据上分散的局限性。通过构建867,653个医学影像样本的数据集，M3Ret能够学习可迁移的视觉表示，且无需特定模态的定制。研究结果表明，M3Ret在各个模态的零样本图像检索中超越了现有的强基线，展示了其在医学影像理解中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.00425', 'title': 'The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in\n  LLMs with Camlang', 'url': 'https://huggingface.co/papers/2509.00425', 'abstract': 'Camlang, a constructed language, is used to evaluate whether LLMs can master unfamiliar languages through metalinguistic reasoning, revealing that current models lack systematic grammatical mastery compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve gold-medal performance across many benchmarks, yet it remains unclear whether such success reflects genuine reasoning or pattern matching. From a cognitive science perspective, an informative test is whether models can master an unfamiliar language through explicit metalinguistic deductive learning, a paradigm where human learners can reliably internalise grammatical systems through metalinguistic reasoning. We address this question with Camlang, a novel constructed language that exhibits naturalistic yet unattested feature combinations. Camlang consists of two explicit resources, a grammar book and a bilingual dictionary, which mirror adult second-language learning via explicit grammar rules and lexical lookup, and enable us to disentangle errors in morpho-syntax, lexical semantics, and sentence-level reasoning. Human experiments show that these resources are sufficient for participants to acquire Camlang and successfully solve Camlang tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang, creating Camlang-CSQA-v0, the first task in a broader suite where solving questions requires applying grammar rules and lexical mappings. Experimental results show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in Camlang, far below human performance at 87\\%, while other state-of-the-art reasoning LLMs perform even worse. Human verification further reveals that most model successes stem from shallow lexical alignment while GPT-5 shows emerging metalinguistic awareness to a limited extent but not systematic grammatical mastery as humans. Camlang establishes a cognitively grounded evaluation paradigm that exposes fundamental gaps between current models and human metalinguistic competence.', 'score': 8, 'issue_id': 5690, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': 'd6f6d7d86f28d385', 'authors': ['Fenghua Liu', 'Yulong Chen', 'Yixuan Liu', 'Zhujun Jin', 'Solomon Tsai', 'Ming Zhong'], 'affiliations': ['UIUC', 'University of Cambridge', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2509.00425.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#reasoning', '#long_context', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Искусственный язык раскрывает ограничения языковых моделей', 'desc': 'Исследователи создали искусственный язык Camlang для оценки способности больших языковых моделей (LLM) осваивать незнакомые языки через металингвистические рассуждения. Эксперименты показали, что люди могут успешно овладеть Camlang с помощью грамматики и словаря, в то время как современные LLM демонстрируют значительно худшие результаты. GPT-5 достиг 47% точности на тестах Camlang по сравнению с 87% у людей, что указывает на отсутствие систематического грамматического мастерства у моделей. Camlang представляет собой новую парадигму оценки, выявляющую фундаментальные различия между текущими моделями и человеческой металингвистической компетенцией.'}, 'en': {'title': 'Bridging the Gap: Evaluating LLMs with Camlang', 'desc': 'This paper investigates the ability of Large Language Models (LLMs) to learn and understand a constructed language called Camlang, which is designed to test metalinguistic reasoning. The study finds that while LLMs like GPT-5 perform well on familiar languages, they struggle significantly with Camlang, achieving only 47% accuracy compared to 87% for humans. The results indicate that current models rely on shallow pattern matching rather than true grammatical understanding, highlighting a gap in their metalinguistic competence. This research provides a new framework for evaluating LLMs by focusing on their ability to apply grammatical rules and lexical mappings in unfamiliar contexts.'}, 'zh': {'title': '揭示LLMs与人类语法掌握的差距', 'desc': '本论文探讨了大型语言模型（LLMs）在掌握不熟悉语言方面的能力，使用了一种名为Camlang的构造语言进行评估。研究发现，尽管LLMs在许多基准测试中表现优异，但它们在语法掌握上与人类相比仍然存在显著差距。通过提供语法书和双语词典，研究模拟了人类学习者的显式语法学习过程。实验结果表明，当前的模型在Camlang任务中的表现远低于人类，揭示了它们在元语言推理能力上的不足。'}}}, {'id': 'https://huggingface.co/papers/2509.02046', 'title': 'Fantastic Pretraining Optimizers and Where to Find Them', 'url': 'https://huggingface.co/papers/2509.02046', 'abstract': 'A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  \t\t\t\t\tAI-generated summary \t\t\t\t AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.', 'score': 6, 'issue_id': 5689, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'd90554ffa1d1a865', 'authors': ['Kaiyue Wen', 'David Hall', 'Tengyu Ma', 'Percy Liang'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2509.02046.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🔬', 'ru': {'title': 'Справедливое сравнение оптимизаторов требует тщательного анализа', 'desc': 'Исследование показывает, что для справедливого сравнения оптимизаторов глубокого обучения необходима тщательная настройка гиперпараметров и оценка на различных масштабах моделей и соотношениях данных к модели. Выявлено, что матричные оптимизаторы, такие как Muon и Soap, дают уменьшающееся ускорение с ростом размера модели. Оптимальные гиперпараметры для одного оптимизатора могут быть неоптимальными для другого, что делает слепой перенос гиперпараметров некорректным. Исследование также показало, что сравнение промежуточных чекпоинтов может быть misleading, так как рейтинги оптимизаторов могут меняться в процессе обучения из-за decay скорости обучения.'}, 'en': {'title': 'Fair Comparisons of Deep Learning Optimizers: Tuning Matters!', 'desc': 'This paper investigates the effectiveness of various deep learning optimizers, particularly focusing on the popular AdamW. It highlights that fair comparisons of optimizers require careful hyperparameter tuning and evaluations across different model sizes and data ratios. The study reveals that matrix-based optimizers like Muon and Soap show diminishing returns in speedup as model size increases, with their advantages decreasing significantly for larger models. Ultimately, the findings suggest that many claims of speedup for alternative optimizers may be overstated, emphasizing the need for rigorous evaluation methods.'}, 'zh': {'title': '公平比较深度学习优化器的关键在于超参数调优', 'desc': '本研究系统地探讨了深度学习优化器的公平比较，强调了超参数调优和模型规模、数据与模型比例的评估的重要性。我们发现，矩阵基础的优化器如Muon和Soap在模型规模增大时，速度提升逐渐减小。研究表明，盲目转移超参数会导致不公平的比较，而在训练结束时进行的严格评估才能提供真实的性能对比。最终结果显示，许多声称的速度提升在实际应用中往往低于预期，尤其是在大规模模型中。'}}}, {'id': 'https://huggingface.co/papers/2509.00244', 'title': 'Universal Deep Research: Bring Your Own Model and Strategy', 'url': 'https://huggingface.co/papers/2509.00244', 'abstract': 'Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.', 'score': 6, 'issue_id': 5685, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '641998602f4f0d54', 'authors': ['Peter Belcak', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.00244.jpg', 'data': {'categories': ['#agents'], 'emoji': '🔍', 'ru': {'title': 'Универсальный инструмент для настройки стратегий глубокого исследования', 'desc': 'Universal Deep Research (UDR) - это гибкая система, позволяющая пользователям настраивать стратегии глубокого исследования с использованием любой языковой модели без дополнительного обучения или доводки. UDR обертывает языковую модель и дает возможность создавать, редактировать и улучшать пользовательские стратегии исследования. Система демонстрирует свою универсальность, предоставляя примеры минимальных, расширенных и интенсивных стратегий исследования. UDR также включает пользовательский интерфейс для удобства экспериментов с системой.'}, 'en': {'title': 'Customize Your Research with Universal Deep Research!', 'desc': 'Universal Deep Research (UDR) is a versatile system that empowers users to design personalized deep research strategies using any language model, eliminating the need for extra training or fine-tuning. Unlike previous deep research agents that are limited to specific strategies and tools, UDR allows for complete customization and flexibility. The system supports various research approaches, including minimal, expansive, and intensive strategies, demonstrating its adaptability. A user-friendly interface is provided to encourage experimentation and enhance the research process.'}, 'zh': {'title': '通用深度研究：自定义你的研究策略', 'desc': '通用深度研究（UDR）是一个灵活的系统，允许用户使用任何语言模型自定义深度研究策略，而无需额外的训练或微调。现有的深度研究工具通常是硬编码的，执行特定的研究策略并使用固定的工具选择。UDR作为一个通用的智能系统，可以围绕任何语言模型进行构建，使用户能够创建、编辑和完善自己的深度研究策略。我们还为UDR提供了示例研究策略，并提供用户界面以便于用户进行实验。'}}}, {'id': 'https://huggingface.co/papers/2509.01984', 'title': 'Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing', 'url': 'https://huggingface.co/papers/2509.01984', 'abstract': 'VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.', 'score': 4, 'issue_id': 5686, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'b7fe3e5a42fa90e9', 'authors': ['Quan Dao', 'Xiaoxiao He', 'Ligong Han', 'Ngan Hoai Nguyen', 'Amin Heyrani Nobar', 'Faez Ahmed', 'Han Zhang', 'Viet Anh Nguyen', 'Dimitris Metaxas'], 'affiliations': ['MIT', 'Qualcomm AI Research', 'Red Hat AI Innovation', 'ReveAI', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01984.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#diffusion'], 'emoji': '🖌️', 'ru': {'title': 'VARIN: Точное редактирование изображений с помощью инверсии шума для VAR-моделей', 'desc': 'VARIN - это новый метод редактирования изображений для визуальных авторегрессионных моделей (VAR). Он использует инверсию шума для точного редактирования изображений в соответствии с текстовыми подсказками. VARIN применяет псевдообратную функцию для argmax-сэмплирования, названную Location-aware Argmax Inversion (LAI), для генерации обратных шумов Гумбеля. Эксперименты показывают, что VARIN эффективно изменяет исходные изображения согласно заданным подсказкам, сохраняя при этом оригинальные детали фона и структуры.'}, 'en': {'title': 'Precise Image Editing with VARIN: Text Meets Visuals', 'desc': 'This paper presents VARIN, a novel editing technique for visual autoregressive models that allows for precise image modifications based on textual prompts. VARIN utilizes a unique method called Location-aware Argmax Inversion (LAI) to create inverse Gumbel noises, which help in reconstructing the original image while enabling targeted edits. The technique is designed to work without requiring additional training, making it practical for real-world applications. Experimental results show that VARIN successfully alters images according to prompts while maintaining important details and background structures.'}, 'zh': {'title': 'VARIN：精准图像编辑的新方法', 'desc': 'VARIN是一种基于噪声反演的编辑技术，专为视觉自回归模型设计，能够根据文本提示进行精确的图像编辑，同时保留原始细节。该技术利用了一种新颖的伪逆函数，称为位置感知的Argmax反演（LAI），生成逆Gumbel噪声。这些逆噪声使得源图像的重建更加精确，并支持与文本提示对齐的有针对性的可控编辑。实验结果表明，VARIN能够有效地根据指定提示修改源图像，同时显著保留原始背景和结构细节，验证了其作为实用编辑方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.02133', 'title': 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models', 'url': 'https://huggingface.co/papers/2509.02133', 'abstract': 'Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/', 'score': 2, 'issue_id': 5688, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'a82d3bc5f04bc839', 'authors': ['Snehasis Mukhopadhyay', 'Aryan Kasat', 'Shivam Dubey', 'Rahul Karthikeyan', 'Dhruv Sood', 'Vinija Jain', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Amazon GenAI', 'Artificial Intelligence Institute, University of South Carolina', 'BITS Pilani Goa', 'DTU', 'IIT Madras', 'Indian Institute of Information Technology, Kalyani', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.02133.jpg', 'data': {'categories': ['#data', '#multilingual', '#ethics', '#open_source', '#alignment', '#inference'], 'emoji': '🇮🇳', 'ru': {'title': 'Конституционное декодирование для справедливых языковых моделей', 'desc': 'Фреймворк AMBEDKAR предлагает новый подход к снижению предвзятости в выводах больших языковых моделей (LLM) в индийском контексте. Он использует слой декодирования, учитывающий конституцию, который применяется во время вывода без изменения параметров базовой модели. Метод включает алгоритм спекулятивного декодирования для проактивного уменьшения кастовой и религиозной предвзятости. Этот подход позволяет достичь абсолютного снижения предвзятости до 26.41% по сравнению с базовой линией.'}, 'en': {'title': 'Fairness Through Constitution-Aware Decoding in AI', 'desc': 'The Ambekar framework introduces a Constitution-Aware Decoding Layer to address caste and religious biases in Large Language Models (LLMs) without the need for retraining. It recognizes that existing bias mitigation strategies often overlook local contexts, particularly in India, where such biases are pronounced. By employing a speculative decoding algorithm, the framework actively reduces harmful outputs during the generation process. This innovative approach not only enhances fairness in AI outputs but also demonstrates a significant reduction in bias, achieving up to 26.41 percent less bias compared to traditional methods.'}, 'zh': {'title': '公平与中立：Ambekar框架的创新之路', 'desc': 'Ambekar框架通过引入一个宪法意识解码层，旨在减少大型语言模型（LLM）输出中的种姓和宗教偏见，而无需重新训练模型。该方法在推理阶段应用，利用投机解码算法主动降低生成过程中的偏见。与传统的偏见缓解策略不同，Ambekar框架专注于印度特有的社会背景，确保输出的公平性和中立性。通过这种方式，我们实现了相较于基线模型高达26.41%的偏见绝对减少。'}}}, {'id': 'https://huggingface.co/papers/2509.00581', 'title': 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction', 'url': 'https://huggingface.co/papers/2509.00581', 'abstract': 'A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting natural language queries into SQL queries is a crucial challenge in both industry and academia, aiming to increase access to databases and large-scale applications. This work examines how in-context learning and chain-of-thought can be utilized to develop a robust solution for text-to-SQL systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the Text2SQL task into schema linking, subproblem identification, query plan generation, SQL generation, and a guided correction loop. Unlike prior systems that rely only on execution-based static correction, we introduce taxonomy-guided dynamic error modification informed by in-context learning. SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its variants, combining guided error taxonomy with reasoning-based query planning.', 'score': 2, 'issue_id': 5688, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '9c1785e2259e0c93', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems Saarbrücken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2509.00581.jpg', 'data': {'categories': ['#data', '#optimization', '#reasoning', '#agents', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Мультиагентный подход с обучением в контексте революционизирует Text2SQL', 'desc': 'Статья представляет новый подход к решению задачи Text2SQL, используя мультиагентную систему. Авторы предлагают фреймворк SQL-of-Thought, который разбивает процесс на несколько этапов, включая связывание схемы, идентификацию подзадач и генерацию SQL-запросов. Система использует обучение в контексте и цепочку рассуждений для улучшения результатов. Благодаря таксономии-guided коррекции ошибок, фреймворк достигает state-of-the-art результатов на датасете Spider.'}, 'en': {'title': 'Transforming Language to SQL with Intelligent Agents', 'desc': 'This paper presents SQL-of-Thought, a multi-agent framework designed to improve the Text2SQL task by breaking it down into several key components. It leverages in-context learning and chain-of-thought reasoning to enhance the conversion of natural language queries into SQL queries. The framework includes processes such as schema linking, subproblem identification, query plan generation, and a dynamic error correction loop guided by a taxonomy. By integrating these elements, SQL-of-Thought achieves state-of-the-art performance on the Spider dataset, surpassing previous systems that relied solely on static correction methods.'}, 'zh': {'title': 'SQL转换的新思路：多智能体框架', 'desc': '本文提出了一种名为SQL-of-Thought的多智能体框架，用于将自然语言查询转换为SQL查询。该框架将Text2SQL任务分解为多个组件，包括模式链接、子问题识别、查询计划生成、SQL生成和引导修正循环。与以往仅依赖静态执行修正的系统不同，我们引入了基于上下文学习的动态错误修改，结合了引导错误分类和推理基础的查询规划。SQL-of-Thought在Spider数据集及其变体上取得了最先进的结果，展示了其在文本到SQL系统中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.00531', 'title': 'MobiAgent: A Systematic Framework for Customizable Mobile Agents', 'url': 'https://huggingface.co/papers/2509.00531', 'abstract': 'MobiAgent, a comprehensive mobile agent system, achieves state-of-the-art performance in real-world mobile scenarios through its MobiMind-series models, AgentRR framework, and MobiFlow benchmarking suite, while also reducing data annotation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of Vision-Language Models (VLMs), GUI-based mobile agents have emerged as a key development direction for intelligent mobile systems. However, existing agent models continue to face significant challenges in real-world task execution, particularly in terms of accuracy and efficiency. To address these limitations, we propose MobiAgent, a comprehensive mobile agent system comprising three core components: the MobiMind-series agent models, the AgentRR acceleration framework, and the MobiFlow benchmarking suite. Furthermore, recognizing that the capabilities of current mobile agents are still limited by the availability of high-quality data, we have developed an AI-assisted agile data collection pipeline that significantly reduces the cost of manual annotation. Compared to both general-purpose LLMs and specialized GUI agent models, MobiAgent achieves state-of-the-art performance in real-world mobile scenarios.', 'score': 2, 'issue_id': 5690, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': 'c0685c3c2b70b6aa', 'authors': ['Cheng Zhang', 'Erhu Feng', 'Xi Zhao', 'Yisheng Zhao', 'Wangbo Gong', 'Jiahui Sun', 'Dong Du', 'Zhichao Hua', 'Yubin Xia', 'Haibo Chen'], 'affiliations': ['Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.00531.jpg', 'data': {'categories': ['#dataset', '#agents', '#data', '#benchmark'], 'emoji': '📱', 'ru': {'title': 'MobiAgent: передовая система мобильных агентов для реальных задач', 'desc': 'MobiAgent - это комплексная система мобильных агентов, состоящая из трех ключевых компонентов: серии моделей MobiMind, фреймворка ускорения AgentRR и набора тестов MobiFlow. Система достигает передовых результатов в реальных мобильных сценариях, превосходя как универсальные языковые модели, так и специализированные модели GUI-агентов. MobiAgent также включает конвейер сбора данных с помощью ИИ, что значительно снижает затраты на ручную аннотацию. Разработка направлена на преодоление ограничений существующих агентных моделей в точности и эффективности выполнения реальных задач.'}, 'en': {'title': 'MobiAgent: Revolutionizing Mobile Intelligence with Efficiency and Cost-Effectiveness', 'desc': 'MobiAgent is a mobile agent system designed to enhance the performance of intelligent mobile applications in real-world scenarios. It consists of three main components: the MobiMind-series models for agent intelligence, the AgentRR framework for improving execution speed, and the MobiFlow suite for performance benchmarking. The system also includes an AI-assisted data collection pipeline that lowers the costs associated with data annotation, addressing a common limitation in training mobile agents. Overall, MobiAgent outperforms existing models by providing better accuracy and efficiency in task execution.'}, 'zh': {'title': 'MobiAgent：智能移动代理的未来', 'desc': 'MobiAgent是一个全面的移动代理系统，旨在提高智能移动系统在真实场景中的表现。它由MobiMind系列模型、AgentRR加速框架和MobiFlow基准测试套件三部分组成，能够有效提升任务执行的准确性和效率。为了降低数据标注成本，MobiAgent还开发了一个AI辅助的敏捷数据收集管道。与通用大型语言模型和专用GUI代理模型相比，MobiAgent在实际移动场景中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2509.00404', 'title': 'Metis: Training Large Language Models with Advanced Low-Bit Quantization', 'url': 'https://huggingface.co/papers/2509.00404', 'abstract': 'Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/typename-yyf/Metis-quantization.', 'score': 2, 'issue_id': 5688, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '122a54575f7764a1', 'authors': ['Hengjie Cao', 'Mengyi Chen', 'Yifeng Yang', 'Ruijun Huang', 'Fang Dong', 'Jixian Zhou', 'Anrui Chen', 'Mingzhi Dong', 'Yujiang Wang', 'Jinlong Hou', 'Yuan Cheng', 'Fan Wu', 'Fan Yang', 'Tun Lu', 'Ning Gu', 'Li Shang'], 'affiliations': ['Fudan University', 'Huawei', 'Oxford Suzhou Centre for Advanced Research', 'Shanghai Innovation Institute', 'University of Bath'], 'pdf_title_img': 'assets/pdf/title_img/2509.00404.jpg', 'data': {'categories': ['#low_resource', '#inference', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение LLM с низкой битностью', 'desc': 'Статья представляет Metis - фреймворк для обучения больших языковых моделей (LLM) с низкобитной квантизацией. Метод использует спектральное разложение, адаптивные скорости обучения и двухдиапазонную регуляризацию для улучшения производительности и стабильности. Metis позволяет обучать модели с 8-битной точностью лучше, чем с 32-битной, а с 4-битной - сравнимо с 32-битной. Это открывает путь к масштабируемому обучению LLM с продвинутой низкобитной квантизацией.'}, 'en': {'title': 'Metis: Stabilizing Low-Bit Training for Large Language Models', 'desc': 'This paper presents Metis, a novel training framework designed to enhance the stability and performance of low-bit quantized large language models (LLMs). It addresses the issue of anisotropic parameter distributions that hinder effective training by utilizing spectral decomposition to separate dominant and minor components, allowing for better quantization. Additionally, Metis employs adaptive learning rates to focus on underrepresented features, improving model accuracy. The framework also incorporates a dual-range regularization technique to maintain numerical precision and ensure stable training, resulting in significant performance improvements over traditional FP32 training methods.'}, 'zh': {'title': 'Metis：提升低比特量化模型训练稳定性与性能的创新框架', 'desc': 'Metis是一个训练框架，旨在解决低比特量化大语言模型的训练不稳定性。它通过谱分解、适应性学习率和双范围正则化来提高模型的性能和稳定性。研究发现，参数分布的各向异性是低比特量化训练的主要障碍，导致训练不稳定和模型性能低下。Metis通过有效地分离主导成分和长尾成分，压缩分布范围，从而实现了更好的训练效果。'}}}, {'id': 'https://huggingface.co/papers/2508.21334', 'title': 'Stairway to Fairness: Connecting Group and Individual Fairness', 'url': 'https://huggingface.co/papers/2508.21334', 'abstract': 'Experiments reveal that highly group-fair recommendations can be individually unfair, highlighting the need for a better understanding and comparison of fairness measures in recommender systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.', 'score': 2, 'issue_id': 5696, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '63506e0d046b4476', 'authors': ['Theresia Veronika Rampisela', 'Maria Maistro', 'Tuukka Ruotsalo', 'Falk Scholer', 'Christina Lioma'], 'affiliations': ['LUT University, Lahti, Finland', 'RMIT University, Melbourne, Australia', 'University of Copenhagen, Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2508.21334.jpg', 'data': {'categories': ['#dataset', '#ethics', '#benchmark'], 'emoji': '⚖️', 'ru': {'title': 'Парадокс справедливости: групповая vs индивидуальная в рекомендательных системах', 'desc': 'Исследование посвящено анализу взаимосвязи между групповой и индивидуальной справедливостью в рекомендательных системах. Авторы провели эксперименты на трех наборах данных, используя восемь различных подходов. Результаты показали, что рекомендации, справедливые для групп, могут быть несправедливыми для отдельных пользователей. Это открытие важно для специалистов, работающих над улучшением справедливости рекомендательных систем.'}, 'en': {'title': 'Balancing Group and Individual Fairness in Recommendations', 'desc': 'This paper investigates the relationship between group fairness and individual fairness in recommender systems (RSs). It highlights that while a recommendation may be fair for a group, it can still be unfair to individuals within that group. The authors conducted experiments across multiple datasets to compare different fairness evaluation measures, revealing that enhancing one type of fairness can negatively impact the other. This research provides valuable insights for practitioners looking to balance fairness in their recommendation algorithms.'}, 'zh': {'title': '群体公平与个体公平的平衡之道', 'desc': '在推荐系统中，公平性通常分为群体公平和个体公平。然而，目前对这两种公平性之间关系的科学理解尚不明确，因为以往的研究使用了不同的评估指标和目标，导致无法进行有效比较。因此，我们的研究探讨了群体公平和个体公平之间的关系，并对可用于这两种公平性的评估指标进行了全面比较。实验结果表明，虽然某些推荐在群体层面上非常公平，但在个体层面上可能存在严重的不公平现象。'}}}, {'id': 'https://huggingface.co/papers/2508.21038', 'title': 'On the Theoretical Limitations of Embedding-Based Retrieval', 'url': 'https://huggingface.co/papers/2508.21038', 'abstract': 'Vector embeddings face theoretical limitations in handling even simple queries, as demonstrated by a new dataset that shows state-of-the-art models fail due to the single vector paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.', 'score': 2, 'issue_id': 5699, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '460396ed53ab3a52', 'authors': ['Orion Weller', 'Michael Boratko', 'Iftekhar Naim', 'Jinhyuk Lee'], 'affiliations': ['Google DeepMind', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2508.21038.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#data', '#dataset', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Пределы возможностей: векторные эмбеддинги сталкиваются с теоретическими барьерами', 'desc': 'Статья демонстрирует теоретические ограничения векторных эмбеддингов в обработке даже простых запросов. Авторы создали датасет LIMIT, на котором современные модели машинного обучения показывают неудовлетворительные результаты из-за парадигмы единого вектора. Исследование связывает известные результаты в теории обучения, показывая, что количество возможных подмножеств документов ограничено размерностью эмбеддинга. Работа подчеркивает необходимость разработки новых методов, способных преодолеть эти фундаментальные ограничения.'}, 'en': {'title': 'Challenging the Limits of Vector Embeddings in Simple Queries', 'desc': 'This paper discusses the limitations of vector embeddings in machine learning, particularly when handling simple queries. It reveals that even advanced models struggle with these tasks due to the constraints of the single vector representation. The authors introduce a new dataset, LIMIT, which tests these models and demonstrates their failures in realistic scenarios. The findings suggest that the theoretical boundaries of embedding models need to be addressed to improve their performance in various applications.'}, 'zh': {'title': '向量嵌入的理论局限性与未来研究方向', 'desc': '本文探讨了向量嵌入在处理简单查询时的理论局限性。研究表明，尽管向量嵌入在检索任务中表现出色，但在某些情况下仍然无法满足基本需求。我们通过创建名为LIMIT的数据集，验证了即使是最先进的模型在面对简单任务时也会失败。该研究呼吁未来的研究应开发新的方法，以克服现有单向量范式的根本限制。'}}}, {'id': 'https://huggingface.co/papers/2509.02523', 'title': 'Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices', 'url': 'https://huggingface.co/papers/2509.02523', 'abstract': 'Monolingual ASR models trained on a balanced mix of high-quality, pseudo-labeled, and synthetic data outperform multilingual models for small model sizes, achieving superior error rates and enabling on-device ASR for underrepresented languages.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license.', 'score': 1, 'issue_id': 5698, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '0a4562a7b7ff851b', 'authors': ['Evan King', 'Adam Sabra', 'Manjunath Kudlur', 'James Wang', 'Pete Warden'], 'affiliations': ['Moonshine AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.02523.jpg', 'data': {'categories': ['#dataset', '#small_models', '#low_resource', '#audio', '#synthetic', '#open_source', '#multilingual', '#data'], 'emoji': '🎙️', 'ru': {'title': 'Маленькие, но мощные: революция в ASR для редких языков', 'desc': 'Исследователи представили набор небольших моделей автоматического распознавания речи (ASR) для малоресурсных языков под названием Flavors of Moonshine. Вопреки распространенному мнению, они показали, что для моделей с малым количеством параметров (27 млн) монолингвальные системы, обученные на сбалансированной смеси качественных размеченных, псевдо-размеченных и синтетических данных, значительно превосходят многоязычные модели. Их модели в среднем имеют на 48% меньшую частоту ошибок по сравнению с сопоставимой по размеру моделью Whisper Tiny и превосходят более крупные модели Whisper. Эти результаты позволяют осуществлять точное распознавание речи на устройстве для языков с ранее ограниченной поддержкой.'}, 'en': {'title': 'Monolingual Models Shine for Underrepresented Languages!', 'desc': 'This paper presents a new approach to automatic speech recognition (ASR) for underrepresented languages using monolingual models. The authors demonstrate that small ASR models, when trained on a balanced mix of high-quality, pseudo-labeled, and synthetic data, can achieve better performance than larger multilingual models. Their findings show that these monolingual models can reduce error rates significantly, making them suitable for on-device applications. The research highlights the effectiveness of tailored training strategies for improving ASR in languages that have been historically underserved.'}, 'zh': {'title': '单语模型超越多语种模型的突破', 'desc': '本文介绍了一种名为Moonshine的自动语音识别（ASR）模型，专门针对一些代表性不足的语言进行优化。研究表明，使用高质量人工标注、伪标注和合成数据的单语模型在小型模型（27M参数）中表现优于多语种模型。我们的模型在错误率上平均比同等大小的Whisper Tiny模型低48%，并且在大多数情况下能够与更大模型的性能相匹配或超越。这些成果推动了小型模型的技术进步，使得在设备上实现准确的语音识别成为可能。'}}}, {'id': 'https://huggingface.co/papers/2509.02379', 'title': 'MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?', 'url': 'https://huggingface.co/papers/2509.02379', 'abstract': 'MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3.', 'score': 1, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'bbf123f2c298e53a', 'authors': ['Yuheng Li', 'Yizhou Wu', 'Yuxiang Lai', 'Mingzhe Hu', 'Xiaofeng Yang'], 'affiliations': ['Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta', 'Department of Computer Science, Emory University, Atlanta', 'Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta', 'Department of Radiation Oncology, Emory University School of Medicine, Atlanta'], 'pdf_title_img': 'assets/pdf/title_img/2509.02379.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset', '#architecture', '#transfer_learning', '#optimization', '#healthcare'], 'emoji': '🏥', 'ru': {'title': 'MedDINOv3: Универсальный сегментатор медицинских изображений на основе фундаментальных моделей', 'desc': 'MedDINOv3 - это новая архитектура для сегментации медицинских изображений, основанная на адаптации модели DINOv3. Она использует многомасштабную агрегацию токенов и предобучение на большом наборе КТ-снимков для преодоления разрыва между естественными и медицинскими изображениями. MedDINOv3 достигает высоких результатов на нескольких бенчмарках по сегментации, демонстрируя потенциал фундаментальных моделей компьютерного зрения в медицинской визуализации. Эта архитектура предлагает универсальный подход к сегментации различных органов и опухолей на КТ и МРТ-снимках.'}, 'en': {'title': 'Revolutionizing Medical Image Segmentation with MedDINOv3', 'desc': 'MedDINOv3 is a new framework that enhances the DINOv3 model for better medical image segmentation, particularly in CT and MRI scans. It addresses the challenges of adapting vision foundation models to medical imaging by using multi-scale token aggregation and domain-adaptive pretraining. This approach allows the model to learn robust features from a large dataset of CT images, improving its performance compared to traditional CNNs. As a result, MedDINOv3 achieves state-of-the-art results in various segmentation tasks, showcasing the effectiveness of using advanced vision models in the medical field.'}, 'zh': {'title': 'MedDINOv3：医学图像分割的新突破', 'desc': 'MedDINOv3是一个将DINOv3与多尺度标记聚合相结合的框架，旨在解决医学图像分割中的领域适应和主干网络性能问题。该框架通过在CT-3M数据集上进行领域自适应预训练，学习到强大的密集特征，从而提高了医学图像分割的准确性。MedDINOv3在四个分割基准测试中达到了或超过了当前的最佳性能，展示了视觉基础模型在医学图像分割中的潜力。该研究表明，视觉基础模型可以作为医学图像分割的统一主干。'}}}, {'id': 'https://huggingface.co/papers/2509.01790', 'title': 'Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs', 'url': 'https://huggingface.co/papers/2509.01790', 'abstract': 'Modern LLMs exhibit less prompt sensitivity than previously thought, with much of the reported variability due to heuristic evaluation methods rather than inherent model weaknesses.  \t\t\t\t\tAI-generated summary \t\t\t\t Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.', 'score': 1, 'issue_id': 5697, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '2b3ed7c45180a009', 'authors': ['Andong Hua', 'Kenan Tang', 'Chenhe Gu', 'Jindong Gu', 'Eric Wong', 'Yao Qin'], 'affiliations': ['UC Irvine', 'UC Santa Barbara', 'University of Oxford', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2509.01790.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Мнимая чувствительность: переосмысление устойчивости языковых моделей', 'desc': 'Исследование показывает, что современные большие языковые модели (LLM) менее чувствительны к формулировкам запросов, чем считалось ранее. Авторы оценили 7 LLM на 6 тестовых наборах с использованием 12 различных шаблонов запросов. Выяснилось, что большая часть наблюдаемой чувствительности к запросам связана с эвристическими методами оценки, а не с недостатками самих моделей. При использовании оценки с помощью LLM-судьи наблюдается значительное снижение вариативности производительности и более высокая корреляция рейтингов моделей для разных запросов.'}, 'en': {'title': "Rethinking Prompt Sensitivity: It's Not the Models, It's the Evaluation!", 'desc': "This paper investigates the concept of prompt sensitivity in large language models (LLMs), which is the idea that changing the wording of a prompt can significantly affect the model's performance. The authors argue that much of the perceived variability in LLM responses is not due to weaknesses in the models themselves, but rather due to the evaluation methods used to assess them. By systematically testing multiple LLMs across various benchmarks and using more flexible evaluation techniques, they find that the models are actually more consistent and robust than previously thought. This suggests that the high prompt sensitivity reported in earlier studies may be an artifact of rigid evaluation processes rather than an inherent flaw in the models."}, 'zh': {'title': '现代LLM的提示敏感性被低估了', 'desc': '现代大型语言模型（LLM）表现出的提示敏感性比之前认为的要低，很多报告的变化是由于启发式评估方法造成的，而不是模型本身的缺陷。提示敏感性是指通过不同的措辞重复内容时，LLM性能发生显著变化的现象。我们对7个LLM进行了系统评估，发现很多提示敏感性源于评估方法的局限性，比如对语义正确的替代表达的忽视。我们的研究表明，现代LLM对提示模板的鲁棒性比之前认为的要强，提示敏感性可能更多是评估过程的产物，而非模型的缺陷。'}}}, {'id': 'https://huggingface.co/papers/2509.01610', 'title': 'Improving Large Vision and Language Models by Learning from a Panel of\n  Peers', 'url': 'https://huggingface.co/papers/2509.01610', 'abstract': 'A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%', 'score': 1, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '3edfce8f16cf09cb', 'authors': ['Jefferson Hernandez', 'Jing Shi', 'Simon Jenni', 'Vicente Ordonez', 'Kushal Kafle'], 'affiliations': ['Adobe Research', 'Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01610.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#rlhf', '#hallucinations', '#alignment'], 'emoji': '👥', 'ru': {'title': 'Коллективное обучение ИИ: имитация процесса рецензирования для улучшения языково-визуальных моделей', 'desc': 'Предложена новая методика обучения больших языково-визуальных моделей (LVLM) под названием Panel-of-Peers. Она имитирует процесс рецензирования, позволяя моделям оценивать и учиться на коллективных результатах друг друга. Этот подход улучшает производительность моделей без необходимости в обширных наборах данных с человеческими разметками. Эксперименты показали значительное улучшение результатов на нескольких бенчмарках, демонстрируя потенциал взаимных оценок как масштабируемой альтернативы самообучению.'}, 'en': {'title': 'Empowering Models through Peer Learning', 'desc': "The paper introduces a Panel-of-Peers learning framework that enhances Large Vision and Language Models (LVLMs) by simulating peer reviews. This method addresses the challenges of relying on costly human-curated preference data and the limitations of machine-generated and self-supervised data. By allowing multiple LVLMs to evaluate and learn from each other's outputs, the framework fosters an iterative self-improvement process. The results show significant performance improvements across various benchmarks, highlighting the effectiveness of peer evaluations as a scalable alternative to traditional alignment methods."}, 'zh': {'title': '同伴评审，提升模型性能的新方法', 'desc': '本文提出了一种新的学习框架，称为“同伴评审学习框架”，旨在提升大型视觉和语言模型（LVLMs）的性能。该框架通过模拟同伴评审的过程，使多个LVLM相互评估和学习，从而实现自我改进。与传统方法依赖昂贵的人类标注数据不同，这种方法能够在没有大量人类标注数据的情况下，显著提高模型的表现。实验结果表明，该框架在多个基准测试中显著提升了模型的平均得分，展示了同伴评审作为一种可扩展的自我监督对齐替代方案的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.01584', 'title': 'ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association', 'url': 'https://huggingface.co/papers/2509.01584', 'abstract': 'ViSTA-SLAM is a real-time monocular SLAM system that uses a lightweight STA model for pose estimation and pointmap regression, and a Sim(3) pose graph for drift correction, achieving superior tracking and reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ViSTA-SLAM as a real-time monocular visual SLAM system that operates without requiring camera intrinsics, making it broadly applicable across diverse camera setups. At its core, the system employs a lightweight symmetric two-view association (STA) model as the frontend, which simultaneously estimates relative camera poses and regresses local pointmaps from only two RGB images. This design reduces model complexity significantly, the size of our frontend is only 35\\% that of comparable state-of-the-art methods, while enhancing the quality of two-view constraints used in the pipeline. In the backend, we construct a specially designed Sim(3) pose graph that incorporates loop closures to address accumulated drift. Extensive experiments demonstrate that our approach achieves superior performance in both camera tracking and dense 3D reconstruction quality compared to current methods. Github repository: https://github.com/zhangganlin/vista-slam', 'score': 1, 'issue_id': 5690, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': 'daefaefcec9f3e8c', 'authors': ['Ganlin Zhang', 'Shenhan Qian', 'Xi Wang', 'Daniel Cremers'], 'affiliations': ['ETH Zurich', 'MCML', 'TU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2509.01584.jpg', 'data': {'categories': ['#architecture', '#3d', '#cv'], 'emoji': '🗺️', 'ru': {'title': 'Эффективная монокулярная SLAM-система без калибровки камеры', 'desc': 'ViSTA-SLAM - это система одновременной локализации и картографирования в реальном времени, использующая монокулярную камеру. Она применяет легковесную модель симметричной двухракурсной ассоциации (STA) для оценки положения камеры и регрессии карты точек. Система не требует знания внутренних параметров камеры, что делает ее широко применимой для различных конфигураций. В backend используется специально разработанный граф поз Sim(3) для коррекции накопленного дрейфа.'}, 'en': {'title': 'ViSTA-SLAM: Lightweight and Intrinsic-Free Monocular SLAM for Superior Tracking and Reconstruction', 'desc': 'ViSTA-SLAM is a real-time monocular SLAM system that simplifies pose estimation and pointmap regression using a lightweight symmetric two-view association (STA) model. This model allows the system to operate without needing camera intrinsics, making it versatile for various camera types. The backend features a Sim(3) pose graph that effectively corrects drift by incorporating loop closures, enhancing overall tracking accuracy. Experimental results show that ViSTA-SLAM outperforms existing methods in both camera tracking and dense 3D reconstruction.'}, 'zh': {'title': 'ViSTA-SLAM：高效的实时单目视觉SLAM系统', 'desc': 'ViSTA-SLAM是一种实时单目视觉SLAM系统，能够在不需要相机内参的情况下运行，适用于多种相机设置。该系统的核心是一个轻量级的对称双视图关联（STA）模型，能够同时估计相对相机姿态并从两张RGB图像中回归局部点云图。通过这种设计，模型复杂度显著降低，前端的大小仅为当前最先进方法的35%，同时提高了管道中使用的双视图约束的质量。在后端，我们构建了一个特别设计的Sim(3)姿态图，结合了回环闭合来解决累积漂移问题，实验表明我们的方法在相机跟踪和稠密3D重建质量上优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2509.01250', 'title': 'Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views', 'url': 'https://huggingface.co/papers/2509.01250', 'abstract': 'Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE.', 'score': 1, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '3fffc03e184237e0', 'authors': ['Xiangdong Zhang', 'Shaofeng Zhang', 'Junchi Yan'], 'affiliations': ['School of AI, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01250.jpg', 'data': {'categories': ['#optimization', '#3d', '#synthetic'], 'emoji': '🔍', 'ru': {'title': 'Двухракурсное самообучение улучшает 3D реконструкцию облаков точек', 'desc': 'Статья представляет новый метод самообучения для трехмерных облаков точек под названием Point-PQAE. Этот подход использует двухракурсную парадигму обучения, которая создает два отдельных представления облака точек и затем реконструирует одно из другого. Метод включает новый механизм обрезки для генерации ракурсов облака точек и оригинальное позиционное кодирование для представления относительного положения двух ракурсов. Point-PQAE превосходит существующие одномодальные методы самообучения в задачах 3D реконструкции, показывая улучшение до 7% на наборе данных ScanObjectNN.'}, 'en': {'title': 'Enhancing 3D Learning with Two-View Cross-Reconstruction', 'desc': 'Point-PQAE is a novel approach in 3D self-supervised learning that utilizes a two-view cross-reconstruction method to enhance point cloud reconstruction tasks. By generating two separate point clouds and reconstructing one from the other, it introduces greater diversity and variance compared to traditional single-view methods. This method employs a unique crop mechanism for generating views and a new positional encoding to capture the 3D relationships between the views. As a result, Point-PQAE significantly outperforms existing self-reconstruction techniques, demonstrating improved performance in various evaluation scenarios.'}, 'zh': {'title': '双视图学习，提升3D自监督重建效果', 'desc': '本文提出了一种名为Point-PQAE的跨重建生成范式，旨在增强3D自监督学习。该方法通过引入双视图的学习方式，增加了点云重建任务中的多样性和方差，超越了单视图方法的表现。我们首次开发了一种点云视图生成的裁剪机制，并提出了一种新颖的位置编码来表示两个解耦视图之间的3D相对位置。实验结果表明，Point-PQAE在ScanObjectNN的三个变体中，分别比自重建基线（Point-MAE）提高了6.5%、7.0%和6.7%的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.00578', 'title': 'C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection', 'url': 'https://huggingface.co/papers/2509.00578', 'abstract': 'Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains', 'score': 1, 'issue_id': 5687, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '191ffb7e3bd4bec6', 'authors': ['Abdellah Zakaria Sellam', 'Ilyes Benaissa', 'Salah Eddine Bekhouche', 'Abdenour Hadid', 'Vito Renó', 'Cosimo Distante'], 'affiliations': ['CNR-STIIMA, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing, c/o Campus Ecotekne, Via Monteroni, Lecce, 73100, LE, Italy', 'Department of Innovation Engineering, University of Salento & Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy', 'Department of Innovation Engineering, University of Salento, Via per Monteroni, Lecce, 73100, Lecce, Italy', 'Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy', 'Sorbonne University Abu Dhabi, UAE', 'UPV/EHU, University of the Basque Country, Sebastian, 20018, Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2509.00578.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Контекстное слияние для точной детекции мелких объектов', 'desc': 'Статья представляет новый метод Context-Aware Fusion (CAF) для улучшения модели DiffusionDet в задаче детекции мелких объектов. CAF использует механизм кросс-внимания для интеграции глобального контекста сцены с локальными признаками объектов. Это позволяет каждому предложению объекта учитывать полное понимание окружающей среды. Эксперименты показывают значительное улучшение результатов на бенчмарке CarDD по сравнению с современными моделями.'}, 'en': {'title': 'Enhancing Object Detection with Context-Aware Fusion', 'desc': 'This paper introduces Context-Aware Fusion (CAF) to enhance the DiffusionDet model for fine-grained object detection. CAF uses cross-attention mechanisms to combine global scene context with local features, addressing the limitations of local feature conditioning. By employing a dedicated encoder to capture environmental information, the model allows object proposals to utilize a broader understanding of the scene. Experimental results show that this approach significantly improves performance on the CarDD benchmark, setting new standards for context-aware object detection.'}, 'zh': {'title': '上下文感知融合提升细粒度物体检测', 'desc': '本文提出了一种名为上下文感知融合（Context-Aware Fusion, CAF）的方法，旨在提升DiffusionDet在细粒度物体检测任务中的表现。CAF通过交叉注意力机制，将全局场景上下文与局部特征相结合，从而克服了DiffusionDet在上下文依赖场景中的局部特征限制。该方法使用专门的编码器生成全局上下文，捕捉全面的环境信息，使每个物体提议能够关注场景级理解。实验结果表明，CAF在CarDD基准测试中显著超越了现有的最先进模型，为细粒度领域的上下文感知物体检测设立了新的性能基准。'}}}, {'id': 'https://huggingface.co/papers/2508.20586', 'title': 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2508.20586', 'abstract': 'FastFit, a high-speed virtual try-on framework using a cacheable diffusion architecture with a Semi-Attention mechanism, achieves significant speedup and maintains high fidelity in multi-reference outfit compositions.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.', 'score': 1, 'issue_id': 5691, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'fefbcacccb51e5ca', 'authors': ['Zheng Chong', 'Yanwei Lei', 'Shiyue Zhang', 'Zhuandi He', 'Zhen Wang', 'Xujie Zhang', 'Xiao Dong', 'Yiling Wu', 'Dongmei Jiang', 'Xiaodan Liang'], 'affiliations': ['LavieAI', 'Pengcheng Laboratory', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20586.jpg', 'data': {'categories': ['#benchmark', '#cv', '#inference', '#open_source', '#dataset', '#diffusion', '#data'], 'emoji': '👚', 'ru': {'title': 'Быстрая и точная виртуальная примерка с FastFit', 'desc': 'FastFit - это новая высокоскоростная система виртуальной примерки одежды, использующая кэшируемую архитектуру диффузионных моделей с механизмом полувнимания (Semi-Attention). Она позволяет значительно ускорить процесс, сохраняя при этом высокое качество генерации изображений с несколькими предметами одежды. FastFit решает проблему неэффективности существующих методов, устраняя необходимость повторных вычислений для каждого элемента гардероба. Авторы также представили новый датасет DressCode-MR для исследований в этой области.'}, 'en': {'title': 'FastFit: Speeding Up Virtual Try-Ons with Smart Caching!', 'desc': 'FastFit is a virtual try-on framework that enhances the speed and quality of outfit compositions by using a cacheable diffusion architecture combined with a Semi-Attention mechanism. It addresses the inefficiencies of existing methods by allowing reference features to be computed once and reused, which significantly reduces redundant calculations during the denoising process. This innovation leads to an impressive average speedup of 3.5 times compared to other techniques while maintaining high fidelity in the generated images. Additionally, the introduction of the DressCode-MR dataset supports further research in multi-reference virtual try-on applications, providing a rich resource for training and evaluation.'}, 'zh': {'title': 'FastFit：高效虚拟试衣的新突破', 'desc': 'FastFit是一种高速度的虚拟试衣框架，采用可缓存的扩散架构和半注意力机制，能够在多参考服装组合中实现显著的加速并保持高保真度。该技术解决了当前方法在多参考服装组合支持和每个去噪步骤中冗余重新计算参考特征的效率低下问题。通过将传统的时间步嵌入替换为参考项目的类别嵌入，我们的模型实现了参考特征编码与去噪过程的完全解耦，从而在所有步骤中仅计算一次参考特征并无损重用。实验结果表明，FastFit在关键保真度指标上超越了最先进的方法，同时在推理效率上具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2509.03867', 'title': 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth', 'url': 'https://huggingface.co/papers/2509.03867', 'abstract': 'LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs\' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence.', 'score': 170, 'issue_id': 5735, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'ec0f3f80763f1bf1', 'authors': ['Yang Wang', 'Chenghao Xiao', 'Chia-Yi Hsiao', 'Zi Yan Chang', 'Chi-Li Chen', 'Tyler Loakman', 'Chenghua Lin'], 'affiliations': ['Durham University', 'The University of Manchester', 'The University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2509.03867.jpg', 'data': {'categories': ['#hallucinations', '#multilingual', '#benchmark', '#reasoning', '#alignment', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Бессмыслица с глубиной: вызов для искусственного интеллекта', 'desc': "Исследователи представили концепцию 'дривелологии' - лингвистического феномена, характеризующегося как 'бессмыслица с глубиной'. Они создали набор данных из более чем 1200 тщательно отобранных примеров на нескольких языках для оценки способности больших языковых моделей (LLM) понимать многослойную семантику дривелологических текстов. Результаты показали, что современные LLM испытывают значительные трудности с пониманием контекстно-зависимых значений и имплицитного смысла таких высказываний. Это исследование выявляет ограничения в прагматическом понимании LLM и ставит под сомнение предположение, что статистическая беглость подразумевает когнитивное понимание."}, 'en': {'title': 'Unlocking the Depths of Nonsense: Understanding Drivelology', 'desc': "This paper introduces Drivelology, a linguistic concept that describes seemingly nonsensical text that actually contains deeper meanings. The authors demonstrate that large language models (LLMs) struggle to interpret these nuanced expressions, which require contextual understanding and moral reasoning. They created a diverse dataset of over 1,200 examples of Drivelological text, carefully annotated to reflect its unique characteristics. The study reveals significant limitations in LLMs' ability to grasp the layered semantics of such text, suggesting that fluency in language does not equate to true comprehension."}, 'zh': {'title': '揭示无意义学的深层语义挑战', 'desc': '本文介绍了一种名为“无意义学”的独特语言现象，特征是表面上看似无意义的表达实际上蕴含深层语义。这些表达在语法上是连贯的，但在语用上却存在矛盾，情感上充满负载，或在修辞上具有颠覆性。尽管当前的大型语言模型在许多自然语言处理任务中表现出色，但它们在理解无意义学文本的层次语义方面存在明显局限。我们构建了一个包含1200多个经过精心策划的示例的小型多样化基准数据集，以评估这些模型在分类、生成和推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2509.04338', 'title': 'From Editor to Dense Geometry Estimator', 'url': 'https://huggingface.co/papers/2509.04338', 'abstract': 'FE2E, a framework using a Diffusion Transformer for dense geometry prediction, outperforms generative models in zero-shot monocular depth and normal estimation with improved performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.   Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts.   Based on these findings, we introduce FE2E, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor\'s original flow matching loss into the ``consistent velocity" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor\'s native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT\'s global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.   Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100times data. The project page can be accessed https://amap-ml.github.io/FE2E/{here}.', 'score': 73, 'issue_id': 5732, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '3fbd2457f43c1688', 'authors': ['JiYuan Wang', 'Chunyu Lin', 'Lei Sun', 'Rongying Liu', 'Lang Nie', 'Mingxing Li', 'Kang Liao', 'Xiangxiang Chu', 'Yao Zhao'], 'affiliations': ['AMAP Alibaba Group', 'BJTU', 'CQUPT', 'NTU'], 'pdf_title_img': 'assets/pdf/title_img/2509.04338.jpg', 'data': {'categories': ['#inference', '#cv', '#diffusion', '#optimization', '#training'], 'emoji': '🔬', 'ru': {'title': 'FE2E: Революция в оценке глубины и нормалей с помощью Diffusion Transformer', 'desc': 'FE2E - это новый фреймворк, использующий Diffusion Transformer для предсказания плотной геометрии. Он превосходит генеративные модели в задачах оценки глубины и нормалей с нулевым обучением. FE2E адаптирует продвинутую модель редактирования на основе архитектуры DiT для этой задачи. Без увеличения объема обучающих данных, FE2E достигает значительных улучшений производительности в оценке монокулярной глубины и нормалей на нескольких наборах данных.'}, 'en': {'title': 'FE2E: Revolutionizing Dense Geometry Prediction with Diffusion Transformers', 'desc': "The paper introduces FE2E, a novel framework that utilizes a Diffusion Transformer for predicting dense geometry, specifically focusing on monocular depth and normal estimation. It demonstrates that editing models, which refine existing features, outperform generative models in this context due to their structural priors. The authors reformulate the training objective to enhance the model's performance and employ logarithmic quantization to address precision issues. FE2E achieves significant improvements in performance without requiring additional training data, showcasing its efficiency and effectiveness across various datasets."}, 'zh': {'title': 'FE2E：密集几何预测的新突破', 'desc': 'FE2E是一个使用扩散变换器的框架，专注于密集几何预测，表现优于生成模型，尤其在零样本单目深度和法线估计方面。该研究表明，图像编辑模型比文本到图像生成模型更适合进行密集预测，因为它们具有更稳定的收敛性和更高的性能。通过重新设计编辑模型的损失函数和使用对数量化，FE2E有效解决了精度问题，并利用全局注意力实现深度和法线的联合估计。最终，FE2E在多个数据集上取得了显著的性能提升，尤其是在ETH3D数据集上超过了35%的性能增益。'}}}, {'id': 'https://huggingface.co/papers/2509.04419', 'title': 'Towards a Unified View of Large Language Model Post-Training', 'url': 'https://huggingface.co/papers/2509.04419', 'abstract': 'A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.', 'score': 54, 'issue_id': 5730, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '91f46a497fb0af28', 'authors': ['Xingtai Lv', 'Yuxin Zuo', 'Youbang Sun', 'Hongyi Liu', 'Yuntian Wei', 'Zhekai Chen', 'Lixuan He', 'Xuekai Zhu', 'Kaiyan Zhang', 'Bingning Wang', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University', 'WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.04419.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Объединение онлайн и офлайн данных для улучшения языковых моделей', 'desc': 'В статье представлен унифицированный оценщик градиента политики и алгоритм гибридного пост-обучения для языковых моделей. Авторы показывают, что подходы обучения с подкреплением и супервизорной донастройки являются частями единого процесса оптимизации. Предложенный метод Hybrid Post-Training эффективно комбинирует онлайн и офлайн данные для пост-обучения. Эксперименты на различных бенчмарках подтверждают преимущества нового подхода над существующими базовыми методами.'}, 'en': {'title': 'Unifying Online and Offline Learning for Superior Language Model Training', 'desc': 'This paper introduces a Unified Policy Gradient Estimator that integrates online and offline data for enhancing post-training in language models. It demonstrates that traditional methods like Reinforcement Learning and Supervised Fine-Tuning are part of a unified optimization framework. The proposed Hybrid Post-Training (HPT) algorithm adapts training signals to balance between utilizing demonstrations and exploring new strategies. Extensive experiments show that HPT outperforms existing methods across multiple benchmarks, confirming the effectiveness of the unified approach.'}, 'zh': {'title': '统一策略，提升语言模型性能', 'desc': '本文提出了一种统一的策略梯度估计器和混合后训练算法，能够有效结合在线和离线数据，以提升语言模型的性能。在线数据通常来自模型生成的回滚，而离线数据则是人类或其他模型的示范。我们证明了强化学习和监督微调这两种方法并不矛盾，而是同一优化过程的不同实例。通过广泛的实验和消融研究，我们验证了我们的理论框架和混合后训练算法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.01396', 'title': "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks", 'url': 'https://huggingface.co/papers/2509.01396', 'abstract': "DeepResearch Arena, a benchmark using academic seminar transcripts, provides high-quality research tasks to evaluate deep research agents across multiple disciplines.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research agents have attracted growing attention for their potential to orchestrate multi-stage research workflows, spanning literature synthesis, methodological design, and empirical verification. Despite these strides, evaluating their research capability faithfully is rather challenging due to the difficulty of collecting frontier research questions that genuinely capture researchers' attention and intellectual curiosity. To address this gap, we introduce DeepResearch Arena, a benchmark grounded in academic seminars that capture rich expert discourse and interaction, better reflecting real-world research environments and reducing the risk of data leakage. To automatically construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task Generation (MAHTG) system that extracts research-worthy inspirations from seminar transcripts. The MAHTG system further translates research-worthy inspirations into high-quality research tasks, ensuring the traceability of research task formulation while filtering noise. With the MAHTG system, we curate DeepResearch Arena with over 10,000 high-quality research tasks from over 200 academic seminars, spanning 12 disciplines, such as literature, history, and science. Our extensive evaluation shows that DeepResearch Arena presents substantial challenges for current state-of-the-art agents, with clear performance gaps observed across different models.", 'score': 47, 'issue_id': 5729, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '85ed0a774ea098f6', 'authors': ['Haiyuan Wan', 'Chen Yang', 'Junchi Yu', 'Meiqi Tu', 'Jiaxuan Lu', 'Di Yu', 'Jianbao Cao', 'Ben Gao', 'Jiaqing Xie', 'Aoran Wang', 'Wenlong Zhang', 'Philip Torr', 'Dongzhan Zhou'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Hong Kong University of Science and Technology, Guangzhou', 'The University of Hong Kong', 'Tsinghua University', 'University of Oxford', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01396.jpg', 'data': {'categories': ['#leakage', '#science', '#agents', '#benchmark', '#survey', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Академические семинары как основа для оценки ИИ-исследователей', 'desc': 'DeepResearch Arena - это новый бенчмарк для оценки глубоких исследовательских агентов, основанный на транскриптах академических семинаров. Он использует систему Multi-Agent Hierarchical Task Generation (MAHTG) для автоматического создания исследовательских задач из семинарских обсуждений. Бенчмарк включает более 10 000 задач из 12 дисциплин, от литературы до естественных наук. Тестирование показало, что современные модели ИИ сталкиваются со значительными трудностями при решении этих задач.'}, 'en': {'title': 'Elevating Research Evaluation with DeepResearch Arena', 'desc': 'DeepResearch Arena is a new benchmark designed to evaluate deep research agents by using transcripts from academic seminars. It focuses on creating high-quality research tasks that reflect real-world research environments, addressing the challenge of finding relevant research questions. The benchmark is built using a Multi-Agent Hierarchical Task Generation (MAHTG) system, which extracts valuable insights from seminar discussions and converts them into structured research tasks. With over 10,000 tasks across various disciplines, this benchmark highlights the performance gaps of current research agents, pushing the boundaries of their capabilities.'}, 'zh': {'title': '深度研究代理的新挑战', 'desc': 'DeepResearch Arena 是一个基准测试，利用学术研讨会的记录来评估深度研究代理的能力。该平台提供高质量的研究任务，涵盖多个学科，帮助评估代理在文献综合、方法设计和实证验证等多阶段研究工作流中的表现。为了构建这个基准，我们提出了多智能体层次任务生成系统（MAHTG），从研讨会记录中提取研究灵感，并将其转化为高质量的研究任务。我们的评估表明，DeepResearch Arena 对当前最先进的研究代理提出了显著挑战，显示出不同模型之间的明显性能差距。'}}}, {'id': 'https://huggingface.co/papers/2509.04292', 'title': 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?', 'url': 'https://huggingface.co/papers/2509.04292', 'abstract': "Inverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios.", 'score': 45, 'issue_id': 5732, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '65e94af989385211', 'authors': ['Qinyan Zhang', 'Xinping Lei', 'Ruijie Miao', 'Yu Fu', 'Haojie Fan', 'Le Chang', 'Jiafan Hou', 'Dingling Zhang', 'Zhongfei Hou', 'Ziqiang Yang', 'Changxin Pu', 'Fei Hu', 'Jingkai Liu', 'Mengyun Liu', 'Yang Liu', 'Xiang Gao', 'Jiaheng Liu', 'Tong Yang', 'Zaiyuan Wang', 'Ge Zhang', 'Wenhao Huang'], 'affiliations': ['ByteDance', 'Jiyun Hudong', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04292.jpg', 'data': {'categories': ['#multilingual', '#hallucinations', '#dataset', '#benchmark', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Преодоление когнитивной инерции в больших языковых моделях', 'desc': 'Статья представляет новый бенчмарк Inverse IFEval для оценки способности больших языковых моделей (LLM) преодолевать предвзятости, полученные в ходе обучения, и следовать нестандартным инструкциям. Бенчмарк включает восемь типов задач, таких как исправление вопросов и контрфактический ответ, охватывая 1012 вопросов на китайском и английском языках. Эксперименты показали необходимость такой оценки для выявления ограничений существующих LLM. Авторы подчеркивают важность развития адаптивности моделей в разнообразных и непредсказуемых реальных сценариях.'}, 'en': {'title': 'Enhancing LLMs: Overcoming Biases for Better Adaptability', 'desc': "The paper introduces Inverse IFEval, a benchmark designed to assess the ability of Large Language Models (LLMs) to overcome biases from their training and follow unconventional instructions. It highlights that while LLMs perform well on many tasks, they often struggle with instructions that deviate from their learned patterns, a phenomenon known as cognitive inertia. The benchmark includes eight challenge types, such as Question Correction and Counterfactual Answering, to evaluate models' adaptability across various contexts. The findings suggest that improving LLMs requires not just fluency and accuracy, but also the ability to adapt to unexpected instructions in real-world applications."}, 'zh': {'title': '评估语言模型的适应能力', 'desc': '本文提出了Inverse IFEval基准，用于评估大型语言模型（LLMs）在面对与训练偏见相悖的指令时的适应能力。研究发现，尽管LLMs在多种任务上表现出色，但它们在遵循与标准化模式相冲突的指令时常常表现出认知惯性。Inverse IFEval引入了八种挑战类型，旨在测量模型克服训练偏见的能力。我们的实验结果表明，未来的对齐工作不仅要关注流畅性和事实正确性，还要考虑在非常规环境下的适应性。'}}}, {'id': 'https://huggingface.co/papers/2509.04394', 'title': 'Transition Models: Rethinking the Generative Learning Objective', 'url': 'https://huggingface.co/papers/2509.04394', 'abstract': 'A novel generative paradigm, Transition Models (TiM), addresses the trade-off between computational cost and output quality in generative modeling by using a continuous-time dynamics equation.  \t\t\t\t\tAI-generated summary \t\t\t\t A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.', 'score': 21, 'issue_id': 5730, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '872ae225cb79c916', 'authors': ['Zidong Wang', 'Yiyuan Zhang', 'Xiaoyu Yue', 'Xiangyu Yue', 'Yangguang Li', 'Wanli Ouyang', 'Lei Bai'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Lab', 'USYD'], 'pdf_title_img': 'assets/pdf/title_img/2509.04394.jpg', 'data': {'categories': ['#small_models', '#optimization', '#generative_modeling', '#diffusion', '#training'], 'emoji': '🔄', 'ru': {'title': 'TiM: гибкое генеративное моделирование с непрерывным временем', 'desc': 'Статья представляет новую парадигму генеративного моделирования - Transition Models (TiM). TiM использует уравнение динамики непрерывного времени для решения проблемы компромисса между вычислительными затратами и качеством выходных данных. Модель TiM адаптируется к переходам с произвольным количеством шагов, демонстрируя монотонное улучшение качества при увеличении бюджета сэмплирования. Несмотря на относительно небольшое количество параметров (865 млн), TiM превосходит ведущие модели с большим числом параметров по всем оцениваемым показателям.'}, 'en': {'title': 'Transition Models: Bridging Quality and Efficiency in Generative Modeling', 'desc': 'The paper introduces Transition Models (TiM), a new approach in generative modeling that balances computational efficiency and output quality. It tackles the existing dilemma where high-fidelity models require extensive computation, while faster models compromise on quality. TiM utilizes a continuous-time dynamics equation to define state transitions, allowing for flexible generation steps that can adapt from coarse to fine outputs. With only 865 million parameters, TiM outperforms larger models in quality and maintains consistent improvements as more computational resources are allocated.'}, 'zh': {'title': '过渡模型：高效生成与优质输出的完美平衡', 'desc': '本文提出了一种新的生成模型范式——过渡模型（TiM），旨在解决生成建模中计算成本与输出质量之间的权衡。传统的迭代扩散模型虽然生成质量高，但计算开销大，而高效的少步生成方法则受到质量上限的限制。TiM通过引入精确的连续时间动态方程，定义了任意有限时间间隔内的状态转移，从而实现了灵活的生成过程。尽管参数量仅为8.65亿，TiM在所有评估的步数中都超越了领先模型，展现出随着采样预算增加而持续提升的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2509.04011', 'title': 'NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings', 'url': 'https://huggingface.co/papers/2509.04011', 'abstract': 'NER Retriever uses internal representations from large language models to perform zero-shot named entity retrieval by embedding entity mentions and type descriptions into a shared semantic space, outperforming lexical and dense sentence-level retrieval methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever', 'score': 17, 'issue_id': 5733, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'f55762e727076e6b', 'authors': ['Or Shachar', 'Uri Katz', 'Yoav Goldberg', 'Oren Glickman'], 'affiliations': ['Computer Science Department, Bar-Ilan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04011.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#transfer_learning', '#multimodal', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Извлечение сущностей без схем с помощью внутренних представлений языковых моделей', 'desc': 'NER Retriever - это фреймворк для извлечения именованных сущностей без предварительного обучения. Он использует внутренние представления больших языковых моделей для встраивания упоминаний сущностей и описаний типов в общее семантическое пространство. Метод применяет векторы значений из средних слоев трансформера, которые лучше кодируют типовую информацию, чем верхние слои. NER Retriever превосходит лексические методы и методы плотного поиска на уровне предложений на трех эталонных наборах данных.'}, 'en': {'title': 'Zero-Shot Entity Retrieval with NER Retriever', 'desc': 'NER Retriever is a novel framework for zero-shot Named Entity Retrieval (NER) that allows users to retrieve documents based on entity types without prior definitions. It utilizes internal representations from large language models (LLMs) to create a shared semantic space for embedding both entity mentions and user-defined type descriptions. By leveraging mid-layer transformer block value vectors, the method captures fine-grained type information more effectively than traditional embeddings. A contrastive projection network further refines these embeddings, resulting in a compact and type-aware representation that excels in nearest-neighbor search, outperforming existing retrieval methods.'}, 'zh': {'title': '零-shot命名实体检索的创新解决方案', 'desc': 'NER Retriever 是一种零-shot命名实体检索框架，旨在根据用户定义的类型描述检索相关文档，而无需事先提供感兴趣的类型。该方法利用大型语言模型的内部表示，将实体提及和类型描述嵌入到共享的语义空间中，从而超越了传统的词汇和密集句子级检索方法。我们通过训练轻量级对比投影网络来优化这些表示，使得相似类型的实体能够对齐，而不相关的类型则被分开。实验结果表明，NER Retriever 在多个基准测试中显著优于现有的检索基线，展示了在无模式实体检索中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.20478', 'title': 'Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding', 'url': 'https://huggingface.co/papers/2508.20478', 'abstract': 'Video-MTR, a reinforced multi-turn reasoning framework, improves long-form video understanding by iteratively selecting key segments and comprehending questions, outperforming existing methods in accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding.', 'score': 16, 'issue_id': 5739, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '912f591f05e3e422', 'authors': ['Yuan Xie', 'Tianshui Chen', 'Zheng Ge', 'Lionel Ni'], 'affiliations': ['Guangdong University of Technology', 'StepFun AI', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2508.20478.jpg', 'data': {'categories': ['#long_context', '#video', '#rl', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'Умное видео: многоступенчатый анализ для глубокого понимания', 'desc': 'Video-MTR - это новая система для понимания длинных видео, использующая итеративный подход к выбору ключевых сегментов и анализу вопросов. Она превосходит существующие методы по точности и эффективности благодаря многоступенчатому рассуждению и специальной системе вознаграждений. Video-MTR устраняет необходимость во внешних визуально-языковых моделях и позволяет проводить сквозное обучение. Система показала высокие результаты на нескольких бенчмарках для понимания длинных видео.'}, 'en': {'title': 'Revolutionizing Long-Form Video Understanding with Iterative Reasoning', 'desc': 'Video-MTR is a new framework that enhances the understanding of long videos by using a reinforced multi-turn reasoning approach. It iteratively selects important video segments and comprehends questions, allowing for a more detailed analysis than traditional single-turn methods. The framework introduces a unique gated bi-level reward system that improves both the selection of video segments and the understanding of questions without relying on external visual-language models. Experiments show that Video-MTR achieves better accuracy and efficiency compared to existing techniques, marking a significant advancement in long-form video understanding.'}, 'zh': {'title': 'Video-MTR：提升长视频理解的新方法', 'desc': 'Video-MTR是一种增强的多轮推理框架，旨在提高长视频理解能力。它通过迭代选择关键视频片段和理解问题，克服了传统方法的局限性。与单轮推理不同，Video-MTR在多个回合中进行推理，逐步选择视频片段，从而实现更精确的分析。通过引入新颖的门控双层奖励系统，Video-MTR优化了视频片段选择和问题理解，避免了对外部视觉语言模型的依赖。'}}}, {'id': 'https://huggingface.co/papers/2509.03059', 'title': 'Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers', 'url': 'https://huggingface.co/papers/2509.03059', 'abstract': 'The Loong Project introduces a framework for generating and verifying synthetic data to improve reasoning capabilities in Large Language Models through Reinforcement Learning with Verifiable Reward.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong.', 'score': 15, 'issue_id': 5748, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 сентября', 'en': 'September 3', 'zh': '9月3日'}, 'hash': 'e6a6ec82e8db2767', 'authors': ['Xingyue Huang', 'Rishabh', 'Gregor Franke', 'Ziyi Yang', 'Jiamu Bai', 'Weijie Bai', 'Jinhe Bi', 'Zifeng Ding', 'Yiqun Duan', 'Chengyu Fan', 'Wendong Fan', 'Xin Gao', 'Ruohao Guo', 'Yuan He', 'Zhuangzhuang He', 'Xianglong Hu', 'Neil Johnson', 'Bowen Li', 'Fangru Lin', 'Siyu Lin', 'Tong Liu', 'Yunpu Ma', 'Hao Shen', 'Hao Sun', 'Beibei Wang', 'Fangyijie Wang', 'Hao Wang', 'Haoran Wang', 'Yang Wang', 'Yifeng Wang', 'Zhaowei Wang', 'Ziyang Wang', 'Yifan Wu', 'Zikai Xiao', 'Chengxing Xie', 'Fan Yang', 'Junxiao Yang', 'Qianshuo Ye', 'Ziyu Ye', 'Guangtao Zeng', 'Yuwen Ebony Zhang', 'Zeyu Zhang', 'Zihao Zhu', 'Bernard Ghanem', 'Philip Torr', 'Guohao Li'], 'affiliations': ['CAMEL-AI.org', 'eigent.ai'], 'pdf_title_img': 'assets/pdf/title_img/2509.03059.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#rl', '#multimodal', '#dataset', '#data', '#reasoning', '#benchmark'], 'emoji': '🐉', 'ru': {'title': 'Loong: усиление рассуждений ИИ через синтетические данные', 'desc': 'Проект Loong представляет фреймворк для генерации и верификации синтетических данных с целью улучшения способностей к рассуждению в больших языковых моделях (LLM) с помощью обучения с подкреплением с проверяемым вознаграждением (RLVR). Фреймворк включает в себя LoongBench - курированный набор данных из 8,729 примеров в 12 областях, и LoongEnv - среду для генерации синтетических данных. Эти компоненты формируют цикл агент-среда, позволяющий проводить обучение с подкреплением, где агент на основе LLM получает вознаграждение за генерацию решений с цепочкой рассуждений (CoT), соответствующих ответам, полученным выполнением кода. Проект направлен на расширение успеха RLVR на различные области, требующие рассуждений, где сложно получить качественные проверяемые наборы данных.'}, 'en': {'title': 'Empowering LLMs with Synthetic Data for Enhanced Reasoning', 'desc': 'The Loong Project presents a new framework designed to enhance the reasoning abilities of Large Language Models (LLMs) using Reinforcement Learning with Verifiable Reward (RLVR). It addresses the challenge of generating high-quality synthetic data for reasoning-intensive tasks by providing a curated dataset called LoongBench and a modular environment for synthetic data generation, known as LoongEnv. LoongBench includes thousands of human-verified examples across various domains, while LoongEnv allows for the creation of new question-answer-code triples through different prompting strategies. This framework enables LLMs to learn and improve their reasoning by rewarding them for producing correct Chain-of-Thought solutions that match executable code outputs.'}, 'zh': {'title': '合成数据生成与验证的创新框架', 'desc': 'Loong项目提出了一个框架，用于生成和验证合成数据，以提高大型语言模型的推理能力，采用可验证奖励的强化学习方法。该框架包括两个主要组件：LoongBench，一个包含8729个经过人工审核的示例的种子数据集，涵盖12个领域；以及LoongEnv，一个模块化的合成数据生成环境，支持多种提示策略生成新的问答代码三元组。通过这些组件，形成了一个代理-环境循环，使得基于大型语言模型的代理能够通过生成符合代码执行答案的思维链解决方案来获得奖励。我们对LoongBench进行了广泛的基准测试，以评估领域覆盖率并揭示性能瓶颈。'}}}, {'id': 'https://huggingface.co/papers/2509.04406', 'title': 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation', 'url': 'https://huggingface.co/papers/2509.04406', 'abstract': 'A novel framework, MDT-dist, accelerates 3D flow generation by distilling pretrained models to learn Marginal-Data Transport through Velocity Matching and Velocity Distillation, reducing sampling steps and improving speed and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.', 'score': 8, 'issue_id': 5732, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'e5f91b790b483c4a', 'authors': ['Zanwei Zhou', 'Taoran Yi', 'Jiemin Fang', 'Chen Yang', 'Lingxi Xie', 'Xinggang Wang', 'Wei Shen', 'Qi Tian'], 'affiliations': ['Huawei Inc.', 'Huazhong University of Science and Technology', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04406.jpg', 'data': {'categories': ['#3d', '#inference', '#diffusion', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Революционное ускорение 3D-генерации с помощью дистилляции потоков', 'desc': 'MDT-dist - это новая система для ускорения генерации 3D-моделей на основе потоков. Она использует дистилляцию предобученных моделей для изучения маргинального транспорта данных через сопоставление и дистилляцию скоростей. Это позволяет сократить количество шагов сэмплирования с 25 до 1-2, значительно ускоряя процесс. MDT-dist превосходит существующие методы дистилляции и обеспечивает высокое качество генерируемых 3D-моделей.'}, 'en': {'title': 'Accelerating 3D Flow Generation with MDT-dist', 'desc': 'The paper introduces MDT-dist, a new framework designed to speed up 3D flow generation by using a technique called distillation on pretrained models. It focuses on learning Marginal-Data Transport through two main strategies: Velocity Matching (VM) and Velocity Distillation (VD). VM helps align the velocity fields of the student and teacher models, while VD improves the process by distilling probability densities from the learned velocity fields. This approach significantly reduces the number of sampling steps needed, achieving faster generation times while maintaining high quality in the output.'}, 'zh': {'title': 'MDT-dist：加速3D流生成的新方法', 'desc': '本文提出了一种新颖的框架MDT-dist，用于加速3D流生成。该方法通过对预训练模型进行蒸馏，学习边际数据传输，结合速度匹配和速度蒸馏，显著减少了采样步骤，提高了生成速度和保真度。与传统的3D生成模型相比，MDT-dist能够将每个流变换器的采样步骤从25减少到1或2，同时保持高质量的视觉和几何保真度。实验结果表明，该方法在少步3D生成任务中显著优于现有的蒸馏方法。'}}}, {'id': 'https://huggingface.co/papers/2509.04434', 'title': 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer', 'url': 'https://huggingface.co/papers/2509.04434', 'abstract': 'Durian uses dual reference networks and a diffusion model to generate high-fidelity portrait animations with attribute transfer from a reference image to a target portrait in a zero-shot manner.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.', 'score': 5, 'issue_id': 5731, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '4b84700e944a41a7', 'authors': ['Hyunsoo Cha', 'Byungjun Kim', 'Hanbyul Joo'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04434.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#video'], 'emoji': '🍍', 'ru': {'title': 'Революция в анимации портретов: перенос атрибутов без предварительного обучения', 'desc': 'Статья представляет метод Durian для генерации анимированных портретов с переносом атрибутов с референсного изображения на целевой портрет. Используются двойные эталонные сети и диффузионная модель для достижения высокой точности и пространственной согласованности при переносе атрибутов. Модель обучается на основе самореконструкции кадров из одного видео, используя расширение масок и аугментацию для улучшения обобщения. Durian демонстрирует передовые результаты в анимации портретов с переносом атрибутов и позволяет комбинировать несколько атрибутов за один проход генерации.'}, 'en': {'title': 'Transforming Portraits: High-Fidelity Animation with Attribute Transfer', 'desc': 'Durian is a novel method for creating high-quality portrait animations that can transfer facial attributes from a reference image to a target portrait without needing prior examples. It employs dual reference networks to enhance the denoising process of a diffusion model, ensuring that the transferred attributes maintain spatial consistency across video frames. The model is trained using a self-reconstruction approach, where it learns to generate frames based on a reference and target portrait, while also incorporating a mask expansion strategy for better attribute transfer. This innovative design allows Durian to effectively handle various attributes and combinations, achieving top performance in the field of portrait animation.'}, 'zh': {'title': 'Durian：高保真肖像动画的创新方法', 'desc': 'Durian是一种新方法，可以在没有额外训练的情况下，从参考图像生成高保真度的肖像动画。它使用双重参考网络和扩散模型，将面部属性从参考图像转移到目标肖像。通过自重建的方式训练模型，使得不同帧之间的属性转移保持一致性。Durian在肖像动画和属性转移方面达到了最先进的性能，能够在一次生成中实现多属性组合。'}}}, {'id': 'https://huggingface.co/papers/2509.04442', 'title': 'Delta Activations: A Representation for Finetuned Large Language Models', 'url': 'https://huggingface.co/papers/2509.04442', 'abstract': 'Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  \t\t\t\t\tAI-generated summary \t\t\t\t The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.', 'score': 3, 'issue_id': 5735, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '91b81a1c3795c65f', 'authors': ['Zhiqiu Xu', 'Amish Sethi', 'Mayur Naik', 'Ser-Nam Lim'], 'affiliations': ['University of Central Florida', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2509.04442.jpg', 'data': {'categories': ['#architecture', '#training', '#transfer_learning', '#dataset', '#data', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Delta Activations: Навигация в мире дообученных языковых моделей', 'desc': 'Метод Delta Activations представляет дообученные модели в виде векторных эмбеддингов, основанных на изменениях внутренних активаций относительно базовой модели. Это позволяет эффективно кластеризовать модели по доменам и задачам, выявляя структуру в ландшафте моделей. Delta Activations демонстрирует устойчивость к различным настройкам дообучения и обладает аддитивным свойством при смешивании наборов данных для дообучения. Метод также может использоваться для эмбеддинга задач через few-shot дообучение и применяться для выбора и объединения моделей.'}, 'en': {'title': 'Streamlining Model Reuse with Delta Activations', 'desc': 'Delta Activations is a novel approach that represents fine-tuned models as vector embeddings by analyzing shifts in their internal activations compared to a base model. This method enhances the organization of models by enabling effective clustering based on specific tasks and domains, making it easier to navigate the vast array of post-trained models. Additionally, Delta Activations shows robustness across different finetuning scenarios and maintains an additive property when combining datasets. The technique also supports few-shot finetuning for task embedding, aiding in model selection and merging, ultimately promoting the reuse of publicly available models.'}, 'zh': {'title': 'Delta Activations：模型重用的新方法', 'desc': 'Delta Activations是一种通过测量模型内部激活的变化，将微调模型表示为向量嵌入的方法。这种表示方式使得根据领域和任务进行有效的聚类成为可能，从而揭示模型的结构。Delta Activations在微调设置中表现出良好的鲁棒性，并且在混合微调数据集时具有可加性。我们希望Delta Activations能够促进公共可用模型的重用实践。'}}}, {'id': 'https://huggingface.co/papers/2508.18733', 'title': 'Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings', 'url': 'https://huggingface.co/papers/2508.18733', 'abstract': 'Drawing2CAD is a framework that converts 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach with a dual-decoder transformer architecture and a soft target distribution loss function.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD.', 'score': 3, 'issue_id': 5729, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '4b2fa59592c89297', 'authors': ['Feiwei Qin', 'Shichao Lu', 'Junhao Hou', 'Changmiao Wang', 'Meie Fang', 'Ligang Liu'], 'affiliations': ['Guangzhou University, Guangzhou, China', 'Hangzhou Dianzi University, Hangzhou, China', 'Shenzhen Research Institute of Big Data, Shenzhen, China', 'University of Science and Technology of China, Hefei, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.18733.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#dataset', '#open_source'], 'emoji': '📐', 'ru': {'title': 'От чертежа к CAD: автоматическая генерация 3D-моделей из 2D-чертежей', 'desc': 'Drawing2CAD - это фреймворк, который преобразует 2D векторные чертежи в параметрические CAD-модели, используя подход sequence-to-sequence learning с архитектурой трансформера с двойным декодером. Он применяет функцию потерь с мягким целевым распределением для обучения модели. Ключевая идея заключается в переосмыслении генерации CAD как задачи последовательного обучения, где векторные примитивы чертежа напрямую информируют генерацию параметрических CAD-операций. Фреймворк включает три ключевых компонента: представление векторных примитивов, сохраняющее точную геометрическую информацию, архитектуру трансформера с двойным декодером и функцию потерь с мягким целевым распределением.'}, 'en': {'title': 'Transforming 2D Drawings into Precise CAD Models with AI', 'desc': 'Drawing2CAD is a novel framework that transforms 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach. It employs a dual-decoder transformer architecture to effectively separate the generation of command types and their corresponding parameters, ensuring geometric accuracy. The framework utilizes a soft target distribution loss function to handle the variability in CAD parameters, enhancing flexibility during model training. To validate its effectiveness, the authors introduce the CAD-VGDrawing dataset, which pairs engineering drawings with their corresponding CAD models, and conduct comprehensive experiments.'}, 'zh': {'title': '将二维图纸智能转化为CAD模型', 'desc': 'Drawing2CAD是一个将二维矢量图转换为参数化CAD模型的框架，采用序列到序列学习的方法。该框架使用双解码器变换器架构和软目标分布损失函数，确保在转换过程中保持几何精度和设计意图。通过将CAD生成重新定义为序列到序列学习问题，Drawing2CAD能够直接利用矢量图原语生成CAD操作。我们还创建了CAD-VGDrawing数据集，以训练和评估该框架的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.03888', 'title': 'False Sense of Security: Why Probing-based Malicious Input Detection\n  Fails to Generalize', 'url': 'https://huggingface.co/papers/2509.03888', 'abstract': "Probing-based approaches for detecting harmful instructions in LLMs are found to rely on superficial patterns rather than semantic understanding, indicating a need for redesigning models and evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.", 'score': 2, 'issue_id': 5729, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'db5043bdc42c7fda', 'authors': ['Cheng Wang', 'Zeming Wei', 'Qin Liu', 'Muhao Chen'], 'affiliations': ['National University of Singapore', 'Peking University', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2509.03888.jpg', 'data': {'categories': ['#security', '#training', '#alignment', '#benchmark', '#data', '#open_source'], 'emoji': '🕵️', 'ru': {'title': 'Зондирование LLM: за фасадом кажущейся безопасности', 'desc': 'Исследование показало, что методы зондирования для обнаружения вредоносных инструкций в больших языковых моделях (LLM) опираются на поверхностные паттерны, а не на семантическое понимание. Эксперименты выявили, что зонды учатся распознавать инструктивные паттерны и триггерные слова, а не истинную вредоносность содержания. Результаты указывают на ложное чувство безопасности вокруг существующих подходов на основе зондирования. Авторы призывают к пересмотру как самих моделей, так и методов их оценки для повышения безопасности LLM.'}, 'en': {'title': 'Rethinking Safety: Beyond Superficial Patterns in LLMs', 'desc': 'This paper investigates the effectiveness of probing-based methods used to detect harmful instructions in Large Language Models (LLMs). The authors find that these methods often rely on superficial patterns, such as specific words or phrases, rather than a true understanding of the semantic meaning behind harmful instructions. Through a series of experiments, they demonstrate that simpler n-gram models can perform similarly, indicating that current probing techniques may provide a false sense of security. The paper calls for a redesign of both the models and the evaluation methods to improve safety in AI systems.'}, 'zh': {'title': '重塑安全检测：超越表面模式', 'desc': '本研究探讨了基于探测的方法在大型语言模型（LLMs）中检测有害指令的有效性。我们发现，这些方法依赖于表面模式，而非真正的语义理解，导致安全性评估存在缺陷。通过系统的实验，我们确认探测器学习到的是指令模式和触发词，而非真正的有害性。研究结果表明，当前的探测方法存在误导性安全感，亟需重新设计模型和评估协议。'}}}, {'id': 'https://huggingface.co/papers/2509.01106', 'title': 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning', 'url': 'https://huggingface.co/papers/2509.01106', 'abstract': 'Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.', 'score': 32, 'issue_id': 5707, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': 'd0766d32afe23fec', 'authors': ['Huang Fang', 'Mengxi Zhang', 'Heng Dong', 'Wei Li', 'Zixuan Wang', 'Qifeng Zhang', 'Xueyun Tian', 'Yucheng Hu', 'Hang Li'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2509.01106.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#alignment', '#robotics', '#multimodal', '#agents', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Robix: Единый интеллект для роботов нового поколения', 'desc': 'Robix - это унифицированная модель машинного обучения, объединяющая рассуждения робота, планирование задач и взаимодействие на естественном языке в единой архитектуре. Модель использует цепочку рассуждений и трехэтапную стратегию обучения, включающую дообучение, тонкую настройку и обучение с подкреплением. Robix демонстрирует превосходную производительность в выполнении интерактивных задач, превосходя как открытые, так и коммерческие базовые модели. Модель обладает новыми возможностями, такими как проактивный диалог, обработка прерываний в реальном времени и рассуждения на основе здравого смысла.'}, 'en': {'title': 'Robix: Revolutionizing Robot Interaction and Task Execution', 'desc': 'Robix is a unified vision-language model designed to enhance robot reasoning, task planning, and natural language interaction. It operates as a cognitive layer in robotic systems, generating commands and responses that allow robots to execute complex tasks and interact with humans effectively. The model employs chain-of-thought reasoning and a three-stage training strategy, which includes pretraining, supervised finetuning, and reinforcement learning to improve its performance. Experiments show that Robix significantly outperforms existing models in executing diverse interactive tasks, showcasing its ability to generalize across various instruction types.'}, 'zh': {'title': 'Robix：智能机器人交互的新纪元', 'desc': 'Robix是一种统一的视觉-语言模型，结合了机器人推理、任务规划和自然语言交互。它通过链式思维推理和三阶段训练策略，展示了在交互任务执行中的优越性能。Robix能够动态生成原子命令和人机交互的语言响应，使机器人能够执行复杂指令并进行自然互动。实验表明，Robix在多种指令类型和用户参与的任务中表现优于现有的开源和商业模型。'}}}, {'id': 'https://huggingface.co/papers/2509.00375', 'title': 'Open Data Synthesis For Deep Research', 'url': 'https://huggingface.co/papers/2509.00375', 'abstract': 'InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.', 'score': 30, 'issue_id': 5707, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': 'd7d79c964b418fac', 'authors': ['Ziyi Xia', 'Kun Luo', 'Hongjin Qian', 'Zheng Liu'], 'affiliations': ['BAAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.00375.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#reasoning', '#benchmark', '#multimodal', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'InfoSeek: Новый уровень глубокого исследования для языковых моделей', 'desc': 'InfoSeek - это масштабируемая система для создания сложных задач глубокого исследования путем синтеза иерархических задач удовлетворения ограничений. Она использует двухагентную систему для рекурсивного построения дерева исследований из веб-страниц, преобразуя его в вопросы на естественном языке. Модели, обученные на InfoSeek, превосходят сильные базовые линии на сложных бенчмарках. InfoSeek позволяет быстро масштабировать генерацию данных и поддерживает продвинутые стратегии оптимизации.'}, 'en': {'title': 'Unlocking Deep Research with Hierarchical Constraints', 'desc': 'InfoSeek is a novel framework designed to tackle complex Deep Research tasks by structuring them as Hierarchical Constraint Satisfaction Problems (HCSPs). This approach allows models to break down intricate questions into manageable sub-problems, facilitating multi-step reasoning and evidence synthesis from various sources. Unlike traditional benchmarks, InfoSeek generates a large dataset of over 50,000 training examples that reflect the complexity of real-world queries without shortcut reasoning. Experiments demonstrate that models trained with InfoSeek significantly outperform larger models on challenging benchmarks, showcasing its effectiveness in enhancing the capabilities of large language models.'}, 'zh': {'title': 'InfoSeek：深度研究任务的新框架', 'desc': 'InfoSeek是一个可扩展的框架，用于生成复杂的深度研究任务，通过合成层次约束满足问题，使模型在具有挑战性的基准测试中超越更大的基线。该框架使用双代理系统，从大规模网页递归构建研究树，将中间节点模糊化为有效的子问题，并将这些树转换为需要遍历完整层次的自然语言问题。InfoSeek能够快速扩展，生成超过50,000个训练示例，并提供经过策划的测试集和通过拒绝采样生成的推理轨迹。实验表明，基于InfoSeek训练的模型在多个基准测试中表现优异，超越了许多大型模型和商业API。'}}}, {'id': 'https://huggingface.co/papers/2509.03405', 'title': 'LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations', 'url': 'https://huggingface.co/papers/2509.03405', 'abstract': 'LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics.', 'score': 16, 'issue_id': 5713, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 сентября', 'en': 'September 3', 'zh': '9月3日'}, 'hash': 'c66ad503192a18e1', 'authors': ['Daniela Gottesman', 'Alon Gilae-Dotan', 'Ido Cohen', 'Yoav Gur-Arieh', 'Marius Mosbach', 'Ori Yoran', 'Mor Geva'], 'affiliations': ['McGill University', 'Mila Quebec AI Institute', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2509.03405.jpg', 'data': {'categories': ['#data', '#open_source', '#interpretability', '#dataset', '#training', '#benchmark', '#science', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'LMEnt: Заглядывая внутрь языковых моделей', 'desc': 'LMEnt - это набор инструментов для анализа усвоения знаний языковыми моделями в процессе предварительного обучения. Он включает аннотированные корпуса, методы поиска и предобученные модели для изучения представлений знаний и динамики обучения. LMEnt предоставляет богатый знаниями корпус для предобучения на основе Википедии, улучшенный метод поиска по сущностям и 12 предобученных моделей с промежуточными чекпоинтами. Этот инструментарий позволяет изучать связи между упоминаниями сущностей при предобучении и последующей производительностью моделей.'}, 'en': {'title': 'Unlocking Knowledge Acquisition in Language Models with LMEnt', 'desc': "LMEnt is a comprehensive toolkit designed to analyze how language models acquire knowledge during their pretraining phase. It includes a specially annotated corpus based on Wikipedia, an advanced retrieval method that significantly improves performance, and a set of pretrained models with substantial parameters. This suite allows researchers to explore the relationship between entity mentions in the training data and the models' performance on knowledge tasks. By providing insights into knowledge representations and learning dynamics, LMEnt aims to enhance the development of more reliable and complete language models."}, 'zh': {'title': 'LMEnt：揭示语言模型知识获取的秘密', 'desc': 'LMEnt是一个用于分析语言模型在预训练过程中知识获取的工具套件。它提供了带注释的语料库、检索方法和预训练模型，以研究知识表示和学习动态。通过LMEnt，研究人员可以更好地理解语言模型如何将数据转化为对世界的知识和信念。该工具的推出有助于开发更一致、稳健和完整的知识表示的语言模型。'}}}, {'id': 'https://huggingface.co/papers/2509.01977', 'title': 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement', 'url': 'https://huggingface.co/papers/2509.01977', 'abstract': 'MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications.', 'score': 7, 'issue_id': 5711, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '5a1bf5cc33f3e20f', 'authors': ['Dong She', 'Siming Fu', 'Mushui Liu', 'Qiaoqiao Jin', 'Hualiang Wang', 'Mu Liu', 'Jidong Jiang'], 'affiliations': ['ByteDance', 'The Hong Kong University of Science and Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01977.jpg', 'data': {'categories': ['#dataset', '#cv', '#synthetic', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Точная генерация изображений со множеством объектов', 'desc': 'MOSAIC - это новая система для генерации изображений с несколькими объектами, которая обеспечивает точное семантическое выравнивание и ортогональное разделение признаков. Она использует специально размеченный датасет SemAlign-MS для обучения точному соответствию семантических областей между референсными и целевыми изображениями. MOSAIC применяет loss-функции для семантического выравнивания и разделения признаков разных объектов. Система показывает высокую точность даже при генерации изображений с 4 и более объектами, превосходя существующие методы.'}, 'en': {'title': 'MOSAIC: Mastering Multi-Subject Image Generation with Precision', 'desc': "The MOSAIC framework improves the generation of images featuring multiple subjects by focusing on precise semantic alignment and separating features effectively. It addresses common issues like identity blending and attribute leakage that arise when synthesizing images from multiple references. By introducing a new dataset, SemAlign-MS, it provides detailed semantic correspondences, which helps in maintaining clarity in the generated images. The framework's innovative loss functions ensure that different subjects are represented distinctly, allowing for high-quality image generation even with more than three subjects."}, 'zh': {'title': 'MOSAIC：多主体图像生成的新突破', 'desc': 'MOSAIC框架通过确保精确的语义对齐和正交特征解耦，增强了多主体图像生成的能力。该方法解决了在多个参考主体条件下合成图像时身份保真度和语义一致性的问题。MOSAIC引入了SemAlign-MS数据集，提供了多参考主体与目标图像之间的细粒度语义对应关系。通过语义对应注意力损失和多参考解耦损失，MOSAIC在多个基准测试中实现了最先进的性能，能够在4个以上的参考主体下保持高保真度。'}}}, {'id': 'https://huggingface.co/papers/2509.00428', 'title': 'Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation', 'url': 'https://huggingface.co/papers/2509.00428', 'abstract': 'Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE.', 'score': 7, 'issue_id': 5711, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '5f17d4af79b3fc6f', 'authors': ['Xuechao Zou', 'Shun Zhang', 'Xing Fu', 'Yue Li', 'Kai Li', 'Yushe Cao', 'Congyan Lang', 'Pin Tao', 'Junliang Xing'], 'affiliations': ['Ant Group', 'Beijing Jiaotong University', 'Qinghai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.00428.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#architecture', '#security'], 'emoji': '🎭', 'ru': {'title': 'Точный контроль над генерацией лиц с помощью специализированных экспертов', 'desc': 'Face-MoGLE - это новая архитектура для генерации лиц, использующая диффузионные трансформеры. Она обеспечивает высококачественную и контролируемую генерацию за счет семантического разделения латентного пространства и специализации экспертов. Система использует смесь глобальных и локальных экспертов для захвата целостной структуры и семантики на уровне отдельных областей лица. Динамическая сеть гейтинга производит коэффициенты, зависящие от времени и пространственного положения, что повышает гибкость генерации.'}, 'en': {'title': 'Face-MoGLE: Mastering Controllable Face Generation with Diffusion Transformers', 'desc': "Face-MoGLE is a new framework that uses Diffusion Transformers to generate high-quality and controllable faces. It addresses the challenge of balancing semantic control with photorealism by employing semantic-decoupled latent modeling, which allows for precise manipulation of facial attributes. The framework incorporates a mixture of global and local experts to enhance both overall structure and detailed features, ensuring fine-grained control over the generated images. Additionally, a dynamic gating network adapts coefficients during the generation process, improving the model's flexibility and effectiveness in various face generation tasks."}, 'zh': {'title': 'Face-MoGLE：高质量可控面部生成的新框架', 'desc': 'Face-MoGLE是一个新颖的框架，利用扩散变换器实现高质量、可控的面部生成。该框架通过语义解耦的潜在建模、专家专门化和动态门控来解决生成建模中的挑战。它允许精确的属性操控，并结合全局和局部专家以捕捉整体结构和区域语义。实验结果表明，Face-MoGLE在多模态和单模态面部生成中表现出色，并具备强大的零样本泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.02722', 'title': 'Planning with Reasoning using Vision Language World Model', 'url': 'https://huggingface.co/papers/2509.02722', 'abstract': 'The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.', 'score': 6, 'issue_id': 5718, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '9d66073795d0c729', 'authors': ['Delong Chen', 'Theo Moutakanni', 'Willy Chung', 'Yejin Bang', 'Ziwei Ji', 'Allen Bolourchi', 'Pascale Fung'], 'affiliations': ['ISIR Sorbonne Université', 'Meta FAIR', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2509.02722.jpg', 'data': {'categories': ['#agents', '#rl', '#benchmark', '#cv', '#optimization', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'VLWM: Визуально-языковая модель мира для эффективного планирования', 'desc': 'VLWM (Vision Language World Model) - это модель, объединяющая языковое моделирование мира, обучение политике действий и моделирование динамики с семантической и временной абстракцией. Модель использует итеративное самоуточнение на основе языковых моделей, обусловленное сжатыми будущими наблюдениями, представленными в виде дерева подписей. VLWM обучает как политику действий, так и модель динамики, что способствует реактивному декодированию плана и рефлексивному планированию путем минимизации затрат. Модель достигает наилучших результатов в визуальном планировании для помощи (VPA) как на эталонных оценках, так и на предложенных авторами человеческих оценках PlannerArena.'}, 'en': {'title': 'Revolutionizing Visual Planning with Language Understanding', 'desc': 'The Vision Language World Model (VLWM) is a cutting-edge model that enhances visual planning by combining language understanding with world modeling. It learns to predict actions and changes in the world by analyzing natural videos, allowing it to infer goals and plan trajectories effectively. The model employs a two-system approach, where system-1 focuses on quick, reactive planning and system-2 engages in deeper, reflective planning to minimize costs based on expected outcomes. VLWM demonstrates superior performance in visual planning tasks, outperforming existing models in various benchmarks and evaluations.'}, 'zh': {'title': '视觉语言世界模型：智能规划的新突破', 'desc': '视觉语言世界模型（VLWM）通过结合基于语言的世界建模、行动策略学习和动态建模，达到了视觉规划的最先进性能。该模型能够理解和推理具有语义和时间抽象的高层次世界模型，填补了这一领域的空白。VLWM首先根据视觉观察推断整体目标，然后预测由交错的行动和世界状态变化组成的轨迹。通过自我监督的方式训练的评论模型评估假设未来状态与期望目标状态之间的语义距离，从而实现了高效的规划。'}}}, {'id': 'https://huggingface.co/papers/2509.02530', 'title': 'Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots', 'url': 'https://huggingface.co/papers/2509.02530', 'abstract': "Camera Depth Models (CDMs) enhance depth camera accuracy by denoising and improving metric depth prediction, enabling better generalization of robotic manipulation policies from simulation to real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern robotic manipulation primarily relies on visual observations in a 2D color space for skill learning but suffers from poor generalization. In contrast, humans, living in a 3D world, depend more on physical properties-such as distance, size, and shape-than on texture when interacting with objects. Since such 3D geometric information can be acquired from widely available depth cameras, it appears feasible to endow robots with similar perceptual capabilities. Our pilot study found that using depth cameras for manipulation is challenging, primarily due to their limited accuracy and susceptibility to various types of noise. In this work, we propose Camera Depth Models (CDMs) as a simple plugin on daily-use depth cameras, which take RGB images and raw depth signals as input and output denoised, accurate metric depth. To achieve this, we develop a neural data engine that generates high-quality paired data from simulation by modeling a depth camera's noise pattern. Our results show that CDMs achieve nearly simulation-level accuracy in depth prediction, effectively bridging the sim-to-real gap for manipulation tasks. Notably, our experiments demonstrate, for the first time, that a policy trained on raw simulated depth, without the need for adding noise or real-world fine-tuning, generalizes seamlessly to real-world robots on two challenging long-horizon tasks involving articulated, reflective, and slender objects, with little to no performance degradation. We hope our findings will inspire future research in utilizing simulation data and 3D information in general robot policies.", 'score': 1, 'issue_id': 5724, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'e632241ad7e3078d', 'authors': ['Minghuan Liu', 'Zhengbang Zhu', 'Xiaoshen Han', 'Peng Hu', 'Haotong Lin', 'Xinyao Li', 'Jingxiao Chen', 'Jiafeng Xu', 'Yichu Yang', 'Yunfeng Lin', 'Xinghang Li', 'Yong Yu', 'Weinan Zhang', 'Tao Kong', 'Bingyi Kang'], 'affiliations': ['ByteDance Seed', 'Shanghai Jiao Tong University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.02530.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Точное восприятие глубины для робототехники: от симуляции к реальности', 'desc': 'Исследователи предлагают Camera Depth Models (CDM) для улучшения точности камер глубины в робототехнике. CDM обрабатывают RGB-изображения и необработанные сигналы глубины, выдавая очищенные от шума метрические данные о глубине. Модели обучаются на синтетических данных, моделирующих шумы реальных камер. Эксперименты показывают, что политики, обученные на симулированных данных с CDM, успешно переносятся на реальных роботов без дополнительной настройки.'}, 'en': {'title': 'Bridging the Sim-to-Real Gap with Camera Depth Models', 'desc': 'Camera Depth Models (CDMs) improve the accuracy of depth cameras by reducing noise and enhancing metric depth predictions. This allows robots to better generalize their manipulation skills from simulated environments to real-world scenarios. By using a neural data engine, CDMs generate high-quality training data that mimics the noise patterns of depth cameras, achieving near-simulation accuracy in depth perception. The study demonstrates that policies trained on simulated depth data can effectively transfer to real-world tasks without additional fine-tuning, marking a significant advancement in robotic manipulation.'}, 'zh': {'title': '提升深度相机准确性的相机深度模型', 'desc': '本文提出了相机深度模型（CDMs），旨在提高深度相机的准确性，通过去噪和改进度量深度预测，帮助机器人从模拟环境更好地迁移到现实任务中。现代机器人操作主要依赖于二维颜色空间的视觉观察，但在技能学习中面临泛化能力差的问题。CDMs作为一种简单的插件，能够将RGB图像和原始深度信号作为输入，输出去噪后的准确深度信息。我们的实验表明，CDMs在深度预测中达到了接近模拟级别的准确性，成功缩小了模拟与现实之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2509.00930', 'title': 'SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement\n  Fine-Tuning of LLMs', 'url': 'https://huggingface.co/papers/2509.00930', 'abstract': "SATQuest evaluates and enhances LLM logical reasoning by generating diverse SAT-based problems, offering insights into reasoning performance and enabling effective fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis and training, or have narrow problem types and formats. To address these limitations, we introduce SATQuest, a systematic verifier designed to evaluate and enhance logical reasoning in LLMs by generating diverse, Satisfiability-based logical reasoning problems directly from Conjunctive Normal Form (CNF) instances. SATQuest structures these problems along three orthogonal dimensions: instance scale, problem type, and question format, employing randomized, SAT-based problem generation and objective answer verification via PySAT. This design mitigates memorization issues, allows for nuanced insights into reasoning performance, and enables effective reinforcement fine-tuning. Our extensive evaluation of various LLMs using SATQuest identified significant limitations in their logical reasoning, particularly in generalizing beyond familiar mathematical formats. Furthermore, we show that reinforcement fine-tuning with SATQuest rewards substantially improves targeted task performance and generalizes to more complex instances, while highlighting remaining challenges in cross-format adaptation. Through these demonstrations, we showcase SATQuest's potential as a foundational tool and a valuable starting point for advancing LLM logical reasoning.", 'score': 1, 'issue_id': 5722, 'pub_date': '2025-08-31', 'pub_date_card': {'ru': '31 августа', 'en': 'August 31', 'zh': '8月31日'}, 'hash': '5bfb23a8accfba06', 'authors': ['Yanxiao Zhao', 'Yaqian Li', 'Zihao Bo', 'Rinyoichi Takezoe', 'Haojia Hui', 'Mo Guang', 'Lei Ren', 'Xiaolin Qin', 'Kaiwen Long'], 'affiliations': ['Chengdu Institute of Computer Applications, Chinese Academy of Sciences', 'Li Auto', 'School of Computer Science and Technology, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.00930.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'SATQuest: новый подход к оценке логики искусственного интеллекта', 'desc': 'SATQuest - это инструмент для оценки и улучшения логического мышления больших языковых моделей (LLM). Он генерирует разнообразные логические задачи на основе проблемы выполнимости булевых формул (SAT). SATQuest структурирует эти задачи по трем измерениям: масштаб, тип проблемы и формат вопроса. Эксперименты показали значительные ограничения LLM в логическом мышлении, особенно при обобщении за пределами знакомых математических форматов.'}, 'en': {'title': 'Enhancing LLM Reasoning with SATQuest', 'desc': 'SATQuest is a tool designed to evaluate and improve the logical reasoning abilities of Large Language Models (LLMs) by generating a variety of SAT-based problems. It addresses the limitations of existing benchmarks by providing a systematic approach that allows for fine-grained analysis across different dimensions such as problem type and question format. By using randomized problem generation and objective verification methods, SATQuest helps to reduce memorization issues and offers deeper insights into the reasoning capabilities of LLMs. The results show that reinforcement fine-tuning with SATQuest significantly enhances performance on specific tasks and aids in generalizing to more complex reasoning scenarios.'}, 'zh': {'title': 'SATQuest：提升LLM逻辑推理的利器', 'desc': 'SATQuest 是一个系统化的验证工具，旨在评估和增强大型语言模型（LLM）的逻辑推理能力。它通过生成多样化的基于可满足性（SAT）的逻辑推理问题，来提供对推理性能的深入见解。SATQuest 设计了三种正交维度的问题结构：实例规模、问题类型和问题格式，利用随机化的 SAT 问题生成和客观答案验证。通过对 LLM 的广泛评估，SATQuest 显示了这些模型在逻辑推理方面的显著局限性，并通过强化微调显著提高了特定任务的表现。'}}}, {'id': 'https://huggingface.co/papers/2508.21113', 'title': 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning', 'url': 'https://huggingface.co/papers/2508.21113', 'abstract': "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.", 'score': 82, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'e3b0726caba25eb1', 'authors': ['Jie Jiang', 'Qi Yang', 'Bolin Ni', 'Shiming Xiang', 'Han Hu', 'Houwen Peng'], 'affiliations': ['Institute of Automation, CAS', 'Tencent Hunyuan Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.21113.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#benchmark', '#dataset', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'R-4B: Адаптивное мышление для эффективного решения задач', 'desc': 'R-4B - это мультимодальная большая языковая модель с возможностью автоматического мышления. Она использует двухрежимный отжиг и двухрежимную оптимизацию политики для адаптивного выбора стратегии решения задач. Модель способна определять, когда нужно активировать процесс мышления в зависимости от сложности проблемы. R-4B достигает передовых результатов на 25 сложных бенчмарках при меньших вычислительных затратах по сравнению с более крупными моделями.'}, 'en': {'title': 'R-4B: Smart Thinking for Efficient Problem Solving', 'desc': 'R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models.'}, 'zh': {'title': 'R-4B：智能思考与高效解决的结合', 'desc': 'R-4B是一种自动思考的多模态大型语言模型，能够根据问题的复杂性自适应地决定何时进行思考。它采用双模退火和双模策略优化技术，以提高模型在解决问题时的效率和准确性。通过在多样化的数据集上进行训练，R-4B能够在简单问题上避免冗余的思考过程，从而降低计算成本。实验结果表明，R-4B在25个具有挑战性的基准测试中表现优异，超越了许多现有模型。'}}}, {'id': 'https://huggingface.co/papers/2508.21112', 'title': 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control', 'url': 'https://huggingface.co/papers/2508.21112', 'abstract': 'EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.', 'score': 54, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '5bbbfa48bbd5fb7c', 'authors': ['Delin Qu', 'Haoming Song', 'Qizhi Chen', 'Zhaoqing Chen', 'Xianqiang Gao', 'Xinyi Ye', 'Qi Lv', 'Modi Shi', 'Guanghui Ren', 'Cheng Ruan', 'Maoqing Yao', 'Haoran Yang', 'Jiacheng Bao', 'Bin Zhao', 'Dong Wang'], 'affiliations': ['AgiBot', 'Fudan University', 'Northwestern Polytechnical University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.21112.jpg', 'data': {'categories': ['#architecture', '#training', '#agents', '#multimodal', '#agi', '#reasoning', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Революция в мультимодальном ИИ для роботов: EO-Robotics объединяет зрение, текст и действие', 'desc': 'EO-Robotics представляет собой систему, состоящую из модели EO-1 и датасета EO-Data1.5M, которая продвигает мультимодальное воплощенное рассуждение и управление роботами через смешанное предобучение на основе зрения, текста и действий. Модель EO-1 использует унифицированную архитектуру для обработки мультимодальных входных данных и обучается на массивном высококачественном датасете EO-Data1.5M, содержащем более 1,5 миллиона образцов. Обучение модели происходит с использованием авторегрессивного декодирования и денойзинга методом сопоставления потоков. Эксперименты демонстрируют эффективность смешанного обучения на основе зрения, текста и действий для понимания открытого мира и обобщения на различные задачи манипуляции.'}, 'en': {'title': 'Empowering Robots with Multimodal Reasoning and Control', 'desc': "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."}, 'zh': {'title': '提升机器人控制的多模态推理新突破', 'desc': 'EO-Robotics是一个新模型，包含EO-1模型和EO-Data1.5M数据集，旨在通过交替的视觉-文本-动作预训练来提升多模态的具身推理和机器人控制能力。EO-1模型能够处理图像、文本、视频和动作等多种输入，展现出在多模态具身推理和机器人控制方面的优越性能。该模型的训练依赖于一个包含超过150万样本的高质量数据集，强调交替的视觉-文本-动作理解。通过大量实验，验证了交替学习在开放世界理解和泛化中的有效性，提供了构建先进具身基础模型的宝贵见解。'}}}, {'id': 'https://huggingface.co/papers/2508.18106', 'title': 'A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code', 'url': 'https://huggingface.co/papers/2508.18106', 'abstract': "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.", 'score': 46, 'issue_id': 5638, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '7b691aaa7b52bcfd', 'authors': ['Keke Lian', 'Bin Wang', 'Lei Zhang', 'Libo Chen', 'Junjie Wang', 'Ziming Zhao', 'Yujiu Yang', 'Haotong Duan', 'Haoran Zhao', 'Shuang Liao', 'Mingda Guo', 'Jiazheng Quan', 'Yilu Zhong', 'Chenhao He', 'Zichuan Chen', 'Jie Wu', 'Haoling Li', 'Zhaoxuan Li', 'Jiongchi Yu', 'Hui Li', 'Dong Zhang'], 'affiliations': ['Fudan University', 'Institute of Information Engineering, Chinese Academy of Sciences', 'Peking University', 'Shanghai Jiao Tong University', 'Singapore Management University', 'Tencent', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18106.jpg', 'data': {'categories': ['#open_source', '#security', '#benchmark', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'A.S.E: Новый стандарт оценки безопасности ИИ-генерируемого кода', 'desc': 'A.S.E - это новый бенчмарк для оценки безопасности кода, генерируемого большими языковыми моделями (LLM). Он использует реальные репозитории и правила, определенные экспертами, что позволяет получить более точные результаты по сравнению с существующими методами. A.S.E сохраняет полный контекст репозитория, включая системы сборки и зависимости между файлами. Исследование показало, что Claude-3.7-Sonnet демонстрирует лучшую общую производительность, а разрыв в безопасности между проприетарными и открытыми моделями невелик.'}, 'en': {'title': 'A.S.E: Elevating Security Evaluation for AI-Generated Code', 'desc': 'The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches.'}, 'zh': {'title': 'A.S.E：提升代码生成安全性的基准评估', 'desc': 'A.S.E是一个用于评估大型语言模型生成代码安全性的基准，利用真实世界的代码库和专家定义的规则。现有的基准测试方法存在不足，无法有效连接输入上下文的质量与输出的安全性。A.S.E通过构建真实代码库中的任务，保留完整的上下文信息，提供可重复的安全评估。我们的评估结果显示，Claude-3.7-Sonnet在整体表现上最佳，而Qwen3-235B-A22B-Instruct在安全性评分上表现突出。'}}}, {'id': 'https://huggingface.co/papers/2508.20470', 'title': 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation', 'url': 'https://huggingface.co/papers/2508.20470', 'abstract': 'Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.', 'score': 31, 'issue_id': 5642, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '2bbd88d7f14f5a78', 'authors': ['Xiaochuan Li', 'Guoguang Du', 'Runze Zhang', 'Liang Jin', 'Qi Jia', 'Lihua Lu', 'Zhenhua Guo', 'Yaqian Zhao', 'Haiyang Liu', 'Tianqi Wang', 'Changsheng Li', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20470.jpg', 'data': {'categories': ['#dataset', '#3d', '#multimodal', '#open_source', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Видео как источник здравого смысла для 3D-генерации', 'desc': 'Эта статья исследует применение видеоданных для улучшения генерации 3D-объектов. Авторы представляют датасет Droplet3D-4M с аннотациями многоракурсных видео и модель Droplet3D, способную генерировать 3D-контент по изображениям и текстовым описаниям. Использование видео позволяет улучшить пространственную согласованность и семантическую правдоподобность создаваемых 3D-активов. Эксперименты подтверждают эффективность подхода и его потенциал для применения в генерации сцен.'}, 'en': {'title': 'Enhancing 3D Asset Generation with Video Commonsense Priors', 'desc': 'This paper discusses how using video data can improve the generation of 3D assets by providing commonsense knowledge that helps maintain spatial consistency and semantic accuracy. It highlights the challenge of limited 3D data compared to other media types, like text and images, and proposes leveraging videos as a solution. The authors introduce Droplet3D-4M, a large dataset with multi-view annotations, and a generative model called Droplet3D that can process both images and text. Their experiments show that this approach not only enhances the quality of 3D content but also allows for broader applications in scene generation.'}, 'zh': {'title': '利用视频数据提升3D生成的空间与语义一致性', 'desc': '本论文探讨了如何利用视频数据来增强3D资产生成，提供空间一致性和语义合理性。由于3D领域的数据稀缺，视频中的常识先验成为了一种有效的替代监督信号。视频捕捉的多视角信息为3D生成提供了空间一致性，而丰富的语义信息则使生成的内容更符合文本提示。我们介绍了Droplet3D-4M数据集和Droplet3D生成模型，实验结果表明该方法在3D内容生成中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.21148', 'title': 'A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers', 'url': 'https://huggingface.co/papers/2508.21148', 'abstract': 'Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.', 'score': 14, 'issue_id': 5645, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '3097c905f2f36541', 'authors': ['Ming Hu', 'Chenglong Ma', 'Wei Li', 'Wanghan Xu', 'Jiamin Wu', 'Jucheng Hu', 'Tianbin Li', 'Guohang Zhuang', 'Jiaqi Liu', 'Yingzhou Lu', 'Ying Chen', 'Chaoyang Zhang', 'Cheng Tan', 'Jie Ying', 'Guocheng Wu', 'Shujian Gao', 'Pengcheng Chen', 'Jiashi Lin', 'Haitao Wu', 'Lulu Chen', 'Fengxiang Wang', 'Yuanyuan Zhang', 'Xiangyu Zhao', 'Feilong Tang', 'Encheng Su', 'Junzhi Ning', 'Xinyao Liu', 'Ye Du', 'Changkai Ji', 'Cheng Tang', 'Huihui Xu', 'Ziyang Chen', 'Ziyan Huang', 'Jiyao Liu', 'Pengfei Jiang', 'Yizhou Wang', 'Chen Tang', 'Jianyu Wu', 'Yuchen Ren', 'Siyuan Yan', 'Zhonghua Wang', 'Zhongxing Xu', 'Shiyan Su', 'Shangquan Sun', 'Runkai Zhao', 'Zhisheng Zhang', 'Yu Liu', 'Fudi Wang', 'Yuanfeng Ji', 'Yanzhou Su', 'Hongming Shan', 'Chunmei Feng', 'Jiahao Xu', 'Jiangtao Yan', 'Wenhao Tang', 'Diping Song', 'Lihao Liu', 'Yanyan Huang', 'Lequan Yu', 'Bin Fu', 'Shujun Wang', 'Xiaomeng Li', 'Xiaowei Hu', 'Yun Gu', 'Ben Fei', 'Zhongying Deng', 'Benyou Wang', 'Yuewen Cao', 'Minjie Shen', 'Haodong Duan', 'Jie Xu', 'Yirong Chen', 'Fang Yan', 'Hongxia Hao', 'Jielan Li', 'Jiajun Du', 'Yanbo Wang', 'Imran Razzak', 'Chi Zhang', 'Lijun Wu', 'Conghui He', 'Zhaohui Lu', 'Jinhai Huang', 'Yihao Liu', 'Fenghua Ling', 'Yuqiang Li', 'Aoran Wang', 'Qihao Zheng', 'Nanqing Dong', 'Tianfan Fu', 'Dongzhan Zhou', 'Yan Lu', 'Wenlong Zhang', 'Jin Ye', 'Jianfei Cai', 'Wanli Ouyang', 'Yu Qiao', 'Zongyuan Ge', 'Shixiang Tang', 'Junjun He', 'Chunfeng Song', 'Lei Bai', 'Bowen Zhou'], 'affiliations': ['Beijing Institute of Heart, Lung and Blood Vessel Diseases', 'China Pharmaceutical University', 'Chinese Academy of Sciences', 'Fudan University', 'Fuzhou University', 'Monash University', 'Purdue University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'South China University', 'Stanford University', 'The Chinese University of Hong Kong', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'UNC-Chapel Hill', 'University College Dublin', 'University College London', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.21148.jpg', 'data': {'categories': ['#benchmark', '#agents', '#survey', '#multimodal', '#data', '#dataset', '#science'], 'emoji': '🧬', 'ru': {'title': 'Sci-LLMs: эволюция искусственного интеллекта в научном познании', 'desc': 'Научные большие языковые модели (Sci-LLMs) развиваются в тесной связи с научными данными, решая уникальные задачи обработки мультимодальной и специализированной информации. В статье представлен комплексный обзор развития Sci-LLMs, включая анализ более 270 наборов данных для предобучения и дообучения моделей. Авторы рассматривают переход от статических тестов к оценке, ориентированной на процесс и открытия, с использованием продвинутых протоколов. Обсуждается парадигма автономных систем на основе Sci-LLMs, способных экспериментировать и вносить вклад в развивающуюся базу знаний.'}, 'en': {'title': 'Transforming Science with Autonomous Language Models', 'desc': 'This paper discusses the evolution of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data. It highlights the unique challenges posed by scientific datasets, which are often multimodal and domain-specific, requiring specialized approaches compared to general natural language processing. The authors propose a taxonomy of scientific data and review various Sci-LLMs, emphasizing the need for models that can handle heterogeneous and uncertain information. Ultimately, the paper advocates for the development of autonomous systems that can actively engage in scientific research, contributing to a dynamic knowledge base.'}, 'zh': {'title': '科学研究中的智能合作伙伴', 'desc': '科学大型语言模型（Sci-LLMs）正在通过与科学数据的共同发展而不断演变，解决多模态和特定领域信息等独特挑战。这项研究提出了一种以数据为中心的综合框架，将Sci-LLMs的发展视为模型与其基础数据之间的共同进化。我们建立了科学数据的统一分类法和科学知识的层次模型，强调了科学语料库与一般自然语言处理数据集之间的区别。最后，我们展望了向闭环系统的转变，强调基于Sci-LLMs的自主代理如何积极实验、验证并为不断发展的知识库做出贡献。'}}}, {'id': 'https://huggingface.co/papers/2508.13618', 'title': 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis', 'url': 'https://huggingface.co/papers/2508.13618', 'abstract': 'TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid', 'score': 14, 'issue_id': 5639, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '8baf01eb014bc50c', 'authors': ['Shunian Chen', 'Hejin Huang', 'Yexin Liu', 'Zihan Ye', 'Pengcheng Chen', 'Chenghao Zhu', 'Michael Guan', 'Rongsheng Wang', 'Junying Chen', 'Guanbin Li', 'Ser-Nam Lim', 'Harry Yang', 'Benyou Wang'], 'affiliations': ['Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.13618.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#transfer_learning', '#dataset', '#cv', '#data'], 'emoji': '🗣️', 'ru': {'title': 'TalkVid: большой и разнообразный датасет для улучшения синтеза говорящих голов', 'desc': 'Представлен новый набор данных TalkVid для улучшения синтеза говорящих голов на основе аудио. Этот датасет содержит 1244 часа видео от 7729 уникальных дикторов и отличается высоким качеством и разнообразием. Модель, обученная на TalkVid, превосходит аналоги по обобщающей способности на различных демографических группах. Также создан стратифицированный набор данных TalkVid-Bench для оценки производительности на различных подгруппах.'}, 'en': {'title': 'Bridging the Diversity Gap in AI with TalkVid', 'desc': 'The paper introduces TalkVid, a new dataset designed to improve audio-driven talking head synthesis by addressing the generalization gap in existing models. This gap arises from the lack of diversity in training data, which often fails to represent various ethnicities, languages, and age groups. TalkVid consists of 1244 hours of video from 7729 unique speakers, curated through a rigorous process to ensure high quality and diversity. The study also presents TalkVid-Bench, an evaluation set that highlights performance disparities among different demographic groups, emphasizing the importance of diverse datasets in machine learning research.'}, 'zh': {'title': 'TalkVid：提升虚拟人头合成的多样性与泛化能力', 'desc': 'TalkVid是一个大规模、高质量和多样化的数据集，旨在改善基于音频的虚拟人头合成技术。现有的模型在处理不同种族、语言和年龄群体时存在泛化能力不足的问题，这主要是由于训练数据的规模和多样性不足。为了解决这个问题，TalkVid包含了1244小时来自7729个独特说话者的视频，经过严格的多阶段自动化筛选，确保了数据的稳定性和美学质量。我们的实验表明，基于TalkVid训练的模型在跨数据集泛化能力上优于以往的数据集，同时也揭示了不同子群体之间的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2508.21365', 'title': 'Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2508.21365', 'abstract': 'Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.', 'score': 9, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'cbcbd196468063e9', 'authors': ['Yi Liao', 'Yu Gu', 'Yuan Sui', 'Zining Zhu', 'Yifan Lu', 'Guohua Tang', 'Zhongqian Sun', 'Wei Yang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.21365.jpg', 'data': {'categories': ['#training', '#games', '#optimization', '#reasoning', '#interpretability', '#multimodal', '#rl', '#rlhf'], 'emoji': '🎮', 'ru': {'title': 'Обучение ИИ через игры: от знаний к умениям', 'desc': 'Предложена новая система Think in Games (TiG), позволяющая большим языковым моделям развивать процедурные знания через взаимодействие с игровыми средами. TiG преобразует задачу принятия решений на основе обучения с подкреплением в задачу языкового моделирования. Система достигает конкурентоспособных результатов при значительно меньших требованиях к данным и вычислительным ресурсам по сравнению с традиционными методами обучения с подкреплением. Кроме того, TiG обеспечивает прозрачность, предоставляя пошаговые объяснения своих решений на естественном языке.'}, 'en': {'title': 'Empowering Language Models to Learn by Playing', 'desc': 'The Think in Games (TiG) framework allows large language models (LLMs) to learn how to perform tasks through interaction with game environments, enhancing their procedural knowledge. This approach addresses the gap between knowing facts (declarative knowledge) and knowing how to apply them (procedural knowledge), which LLMs often struggle with in interactive scenarios. By treating decision-making in reinforcement learning as a language modeling task, TiG enables LLMs to create and refine policies based on feedback from their environment. The results show that TiG not only requires less data and computation than traditional methods but also offers clear explanations for its actions, making it more transparent and interpretable.'}, 'zh': {'title': '通过游戏思维提升AI的学习能力', 'desc': 'Think in Games (TiG) 框架使大型语言模型能够通过互动游戏环境发展程序性知识。与传统的强化学习方法相比，TiG 在数据和计算需求上显著降低，同时保持了模型的推理和解释能力。该框架将基于强化学习的决策过程重新定义为语言建模任务，使得模型能够生成语言指导的策略，并通过环境反馈进行迭代优化。实验结果表明，TiG 成功弥补了声明性知识与程序性知识之间的差距，提升了复杂互动任务的透明度和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2508.17677', 'title': 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training', 'url': 'https://huggingface.co/papers/2508.17677', 'abstract': "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios.", 'score': 7, 'issue_id': 5639, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '8c118ab21cea1eb2', 'authors': ['Yifan Wang', 'Binbin Liu', 'Fengze Liu', 'Yuanfan Guo', 'Jiyao Deng', 'Xuecheng Wu', 'Weidong Zhou', 'Xiaohuan Zhou', 'Taifeng Wang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.17677.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🔄', 'ru': {'title': 'Динамическая оптимизация данных для эффективного обучения языковых моделей', 'desc': 'Статья представляет TiKMiX - метод динамической корректировки смеси данных для предобучения языковых моделей. Авторы вводят метрику Group Influence для эффективной оценки влияния доменов данных на модель. TiKMiX оптимизирует распределение данных, максимизируя эту метрику, что позволяет адаптироваться к меняющимся предпочтениям модели в процессе обучения. Эксперименты показывают, что TiKMiX превосходит современные методы, используя меньше вычислительных ресурсов и улучшая производительность на нисходящих задачах.'}, 'en': {'title': 'Dynamic Data Mixing for Enhanced Language Model Performance', 'desc': "This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture used in training language models. It leverages a metric called Group Influence to assess how different data domains impact the model's learning preferences, which change over time. By optimizing the data mixture based on these evolving preferences, TiKMiX significantly enhances model performance while being computationally efficient. The results show that TiKMiX outperforms existing methods and achieves better results with fewer resources, demonstrating the importance of adaptive data strategies in machine learning."}, 'zh': {'title': '动态调整数据混合，提升语言模型性能', 'desc': '本文提出了一种名为TiKMiX的方法，用于根据模型的学习偏好动态调整数据混合，以提高语言模型的性能。传统的静态数据混合策略无法适应模型在训练过程中不断变化的偏好。TiKMiX引入了Group Influence这一高效指标，用于评估不同数据领域对模型的影响，从而优化数据混合的分布。通过TiKMiX-D和TiKMiX-M两种方法，我们实现了在计算资源使用上更高效的模型训练，同时在多个基准测试中取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2508.21767', 'title': 'UItron: Foundational GUI Agent with Advanced Perception and Planning', 'url': 'https://huggingface.co/papers/2508.21767', 'abstract': 'UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.', 'score': 6, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '4e413654a21d562e', 'authors': ['Zhixiong Zeng', 'Jing Huang', 'Liming Zheng', 'Wenkang Han', 'Yufeng Zhong', 'Lei Chen', 'Longrong Yang', 'Yingjie Chu', 'Yuzhi He', 'Lin Ma'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2508.21767.jpg', 'data': {'categories': ['#benchmark', '#rl', '#open_source', '#training', '#optimization', '#reasoning', '#agi', '#dataset', '#data', '#agents'], 'emoji': '🤖', 'ru': {'title': 'UItron: ИИ-агент для автоматизации графических интерфейсов', 'desc': 'UItron - это модель машинного обучения с открытым исходным кодом для автоматизации работы с графическим интерфейсом. Она улучшает визуальное понимание и планирование задач с помощью продвинутых возможностей восприятия, привязки к контексту и планирования. UItron демонстрирует превосходную производительность в сценариях работы с китайскими приложениями. Модель использует стратегии инженерии данных и интерактивную инфраструктуру для повышения эффективности обучения.'}, 'en': {'title': 'UItron: Advancing GUI Agents for Real-World Applications', 'desc': 'UItron is an open-source foundational model designed to enhance the capabilities of GUI agents, focusing on visual understanding and task planning. It addresses challenges in developing GUI agents, such as limited operation trajectories and the need for interactive infrastructure. By employing advanced perception, grounding, and planning techniques, UItron systematically improves training through data engineering and a curriculum reinforcement learning framework. The model demonstrates exceptional performance in Chinese app scenarios, filling a gap in existing solutions and moving closer to practical applications of GUI agents.'}, 'zh': {'title': 'UItron：推动图形用户界面代理的未来', 'desc': 'UItron是一个开源的基础模型，专为图形用户界面（GUI）代理设计，提升了视觉理解和任务规划能力。该模型通过先进的感知、定位和规划功能，在中文应用场景中表现出色。UItron强调了系统数据工程和交互基础设施在GUI代理开发中的重要性，并通过监督微调和强化学习框架来增强训练效果。实验结果表明，UItron在中文应用场景中取得了显著进展，使GUI代理更接近实际应用。'}}}, {'id': 'https://huggingface.co/papers/2508.21290', 'title': 'Efficient Code Embeddings from Code Generation Models', 'url': 'https://huggingface.co/papers/2508.21290', 'abstract': 'Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.', 'score': 5, 'issue_id': 5640, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '484a770fa8f460fc', 'authors': ['Daria Kryvosheieva', 'Saba Sturua', 'Michael Günther', 'Scott Martens', 'Han Xiao'], 'affiliations': ['Jina AI GmbH', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.21290.jpg', 'data': {'categories': ['#multilingual', '#transfer_learning', '#data', '#dataset', '#games', '#plp', '#small_models', '#training'], 'emoji': '🧠', 'ru': {'title': 'Умные эмбеддинги для эффективной работы с кодом', 'desc': 'Jina-code-embeddings - это новая модель встраивания кода, основанная на авторегрессионной архитектуре, предобученной на тексте и коде. Модель генерирует эмбеддинги для поиска кода, ответов на вопросы и определения семантически похожих фрагментов кода. Несмотря на относительно небольшой размер, модель демонстрирует передовые результаты в различных задачах, связанных с кодом. Авторы описывают методику обучения и валидируют эффективность данного подхода к созданию моделей встраивания кода.'}, 'en': {'title': 'Revolutionizing Code Retrieval with Smart Embeddings', 'desc': 'Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks.'}, 'zh': {'title': '创新代码嵌入模型，提升代码检索与问答能力', 'desc': 'Jina-code-embeddings 是一种新型的代码嵌入模型，旨在通过自然语言查询检索代码、进行技术问答以及识别不同编程语言中语义相似的代码片段。该模型创新性地使用了一个在文本和代码上预训练的自回归骨干网络，通过最后一个标记的池化生成嵌入。我们详细介绍了训练方法，并展示了尽管模型相对较小，但仍能实现最先进的性能。这验证了这种代码嵌入模型构建方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.21376', 'title': 'AHELM: A Holistic Evaluation of Audio-Language Models', 'url': 'https://huggingface.co/papers/2508.21376', 'abstract': 'AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.', 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'fccdd3f91aa4aebd', 'authors': ['Tony Lee', 'Haoqin Tu', 'Chi Heem Wong', 'Zijun Wang', 'Siwei Yang', 'Yifan Mai', 'Yuyin Zhou', 'Cihang Xie', 'Percy Liang'], 'affiliations': ['Hitachi America, Ltd.', 'Stanford University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2508.21376.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#reasoning', '#multimodal', '#audio', '#dataset'], 'emoji': '🎧', 'ru': {'title': 'AHELM: Всесторонняя оценка аудио-языковых моделей', 'desc': 'AHELM - это комплексный бенчмарк для оценки аудио-языковых моделей (ALM). Он измеряет 10 аспектов, включая восприятие аудио, рассуждение, обнаружение эмоций и безопасность, используя различные наборы данных. Бенчмарк стандартизирует промпты, параметры вывода и метрики оценки для справедливого сравнения моделей. Результаты показывают, что Gemini 2.5 Pro лидирует в 5 из 10 аспектов, но проявляет групповую несправедливость в задачах ASR.'}, 'en': {'title': 'AHELM: A Holistic Benchmark for Evaluating Audio-Language Models', 'desc': 'AHELM is a new benchmark designed to evaluate audio-language models (ALMs) on multiple important aspects such as fairness, safety, and reasoning. It combines various datasets, including two new ones, PARADE and CoRe-Bench, to provide a comprehensive assessment of ALMs across ten critical dimensions. By standardizing evaluation methods and metrics, AHELM allows for fair comparisons between different models, addressing the limitations of previous benchmarks. The initial testing of 14 ALMs reveals insights into their performance, highlighting both strengths and weaknesses in areas like bias and group fairness.'}, 'zh': {'title': 'AHELM：音频语言模型的全面评估基准', 'desc': 'AHELM是一个全面的基准测试，用于评估音频语言模型（ALMs），涵盖公平性、安全性和推理等多个方面。该基准整合了多种数据集，包括两个新的合成音频-文本数据集PARADE和CoRe-Bench，以全面测量ALMs在音频感知、知识、推理等10个重要方面的表现。通过标准化提示、推理参数和评估指标，AHELM确保了模型之间的公平比较。我们的测试结果显示，尽管Gemini 2.5 Pro在10个方面中有5个排名第一，但在ASR任务上表现出群体不公平性。'}}}, {'id': 'https://huggingface.co/papers/2508.21456', 'title': 'Morae: Proactively Pausing UI Agents for User Choices', 'url': 'https://huggingface.co/papers/2508.21456', 'abstract': 'Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.', 'score': 3, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '3d7bd580c525eaa6', 'authors': ['Yi-Hao Peng', 'Dingzeyu Li', 'Jeffrey P. Bigham', 'Amy Pavel'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.21456.jpg', 'data': {'categories': ['#ethics', '#agi', '#multimodal', '#healthcare', '#agents'], 'emoji': '👁️', 'ru': {'title': 'Morae: ИИ-ассистент, расширяющий возможности пользователей с нарушениями зрения', 'desc': 'Статья представляет Morae - агента пользовательского интерфейса, который улучшает доступность для пользователей с нарушениями зрения. Morae использует большие мультимодальные модели для интерпретации запросов пользователей и элементов интерфейса. В отличие от существующих агентов, Morae вовлекает пользователей в процесс принятия решений во время выполнения задач. Исследование показало, что Morae помогает пользователям выполнять больше задач и выбирать варианты, которые лучше соответствуют их предпочтениям.'}, 'en': {'title': 'Empowering BLV Users with Interactive Decision-Making', 'desc': 'Morae is a user interface (UI) agent designed to improve accessibility for blind and low-vision (BLV) users by actively involving them in decision-making during task execution. Unlike traditional UI agents that operate autonomously, Morae identifies key decision points and pauses to allow users to make informed choices, enhancing their agency. It leverages large multimodal models to interpret user queries and UI elements, ensuring that users are aware of their options. In studies, Morae demonstrated improved task completion and user satisfaction compared to standard agents, showcasing a mixed-initiative approach that balances automation with user preference expression.'}, 'zh': {'title': 'Morae：让盲人和低视力用户参与决策的智能代理', 'desc': 'Morae是一种用户界面代理，旨在通过让盲人和低视力用户参与决策过程来提高可访问性。它利用大型多模态模型来理解用户查询和用户界面元素，并在任务执行中自动识别决策点，以便用户可以做出选择。与传统的全自动代理不同，Morae在关键时刻暂停，提示用户进行澄清，从而增强用户的自主性。研究表明，Morae帮助用户完成更多任务，并选择更符合他们偏好的选项。'}}}, {'id': 'https://huggingface.co/papers/2508.21188', 'title': 'Model-Task Alignment Drives Distinct RL Outcomes', 'url': 'https://huggingface.co/papers/2508.21188', 'abstract': 'Reinforcement learning applied to large language models shows counterintuitive results that depend on pre-existing model-task alignment, with standard RL methods remaining robust in challenging scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, a series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not typically observed in traditional RL settings. For example, notable claims include that a single training example can match the performance achieved with an entire dataset, that the reward signal does not need to be very accurate, and that training solely with negative samples can match or even surpass sophisticated reward-based methods. However, the precise conditions under which these observations hold - and, critically, when they fail - remain unclear. In this work, we identify a key factor that differentiates RL observations: whether the pretrained model already exhibits strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated task. Through a systematic and comprehensive examination of a series of counterintuitive claims, supported by rigorous experimental validation across different model architectures and task domains, our findings show that while standard RL training remains consistently robust across settings, many of these counterintuitive results arise only when the model and task already exhibit strong model-task alignment. In contrast, these techniques fail to drive substantial learning in more challenging regimes, where standard RL methods remain effective.', 'score': 1, 'issue_id': 5648, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'f7ee68376b3660e0', 'authors': ['Haoze Wu', 'Cheng Wang', 'Wenshuo Zhao', 'Junxian He'], 'affiliations': ['HKUST', 'National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.21188.jpg', 'data': {'categories': ['#training', '#rlhf', '#reasoning', '#alignment', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Неожиданные эффекты обучения с подкреплением в ИИ зависят от начальной подготовки модели', 'desc': 'Исследование применения обучения с подкреплением к большим языковым моделям выявило неожиданные результаты, зависящие от предварительного соответствия модели задаче. Авторы обнаружили, что многие контринтуитивные эффекты проявляются только при сильном начальном соответствии модели и задачи. В более сложных сценариях эти техники оказываются неэффективными, в то время как стандартные методы обучения с подкреплением остаются надежными. Результаты были подтверждены экспериментально на различных архитектурах моделей и типах задач.'}, 'en': {'title': 'Understanding RL Success in Language Models: The Role of Model-Task Alignment', 'desc': 'This paper explores the application of reinforcement learning (RL) to large language models (LLMs) and reveals unexpected results that depend on the alignment between the model and the task. It highlights that certain counterintuitive phenomena, such as achieving high performance with minimal training data or inaccurate reward signals, occur primarily when there is strong model-task alignment. The authors conduct rigorous experiments to validate these findings across various model architectures and tasks, demonstrating that standard RL methods are robust in challenging scenarios. However, they also note that the effectiveness of these counterintuitive results diminishes when the model-task alignment is weak.'}, 'zh': {'title': '强化学习与模型任务对齐的奥秘', 'desc': '本研究探讨了强化学习（RL）在大型语言模型（LLM）中的应用，发现了一些反直觉的现象。这些现象的出现与预训练模型与任务之间的对齐程度密切相关。研究表明，当模型与任务具有强对齐时，某些训练方法可以取得意想不到的效果，但在更具挑战性的环境中，传统的RL方法仍然有效。通过系统的实验验证，我们揭示了这些反直觉结果的条件和局限性。'}}}, {'id': 'https://huggingface.co/papers/2508.20085', 'title': 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2508.20085', 'abstract': 'HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.', 'score': 1, 'issue_id': 5640, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': 'ad1271c7a1097c84', 'authors': ['Zhecheng Yuan', 'Tianming Wei', 'Langzhe Gu', 'Pu Hua', 'Tianhai Liang', 'Yuanpei Chen', 'Huazhe Xu'], 'affiliations': ['Peking University', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20085.jpg', 'data': {'categories': ['#rl', '#agents', '#optimization', '#transfer_learning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'От движений человека к умелым рукам робота', 'desc': 'HERMES - это система обучения роботов на основе данных о движениях человеческих рук. Она использует обучение с подкреплением и перенос из симуляции в реальность для создания универсальных манипуляционных навыков. HERMES способна адаптировать движения к различным условиям окружающей среды. Система включает в себя навигационную модель с механизмом локализации для автономной работы в неструктурированных средах.'}, 'en': {'title': 'Bridging Human Motion and Robotic Dexterity with HERMES', 'desc': 'HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings.'}, 'zh': {'title': 'HERMES：人机协作的灵巧操作新框架', 'desc': 'HERMES是一个人机学习框架，旨在将人类手部动作转化为机器人行为。该框架利用强化学习和仿真到现实的转移技术，帮助机器人在多样化环境中进行灵活的操作。HERMES能够将来自多个来源的人类手部动作统一转化为可行的机器人行为，并通过深度图像实现更好的现实适应性。实验结果表明，HERMES在各种复杂的移动双手灵巧操作任务中表现出色，具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.17380', 'title': "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula\n  Discovery", 'url': 'https://huggingface.co/papers/2508.17380', 'abstract': 'VIPER-R1, a multimodal model combining visual perception, trajectory data, and symbolic reasoning, discovers physical laws with higher accuracy and interpretability than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated discovery of physical laws from observational data in the real world is a grand challenge in AI. Current methods, relying on symbolic regression or LLMs, are limited to uni-modal data and overlook the rich, visual phenomenological representations of motion that are indispensable to physicists. This "sensory deprivation" severely weakens their ability to interpret the inherent spatio-temporal patterns within dynamic phenomena. To address this gap, we propose VIPER-R1, a multimodal model that performs Visual Induction for Physics-based Equation Reasoning to discover fundamental symbolic formulas. It integrates visual perception, trajectory data, and symbolic reasoning to emulate the scientific discovery process. The model is trained via a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning to interpret kinematic phase portraits and to construct hypotheses guided by a Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration (RGSC) to refine the formula structure with reinforcement learning. During inference, the trained VIPER-R1 acts as an agent: it first posits a high-confidence symbolic ansatz, then proactively invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2). This final step, analogous to a physicist\'s perturbation analysis, reconciles the theoretical model with empirical data. To support this research, we introduce PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy and interpretability, enabling more precise discovery of physical laws. Project page: https://jiaaqiliu.github.io/VIPER-R1/', 'score': 1, 'issue_id': 5648, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': '87afb7a989f84636', 'authors': ['Jiaqi Liu', 'Songning Lai', 'Pengze Li', 'Di Yu', 'Wenjie Zhou', 'Yiyang Zhou', 'Peng Xia', 'Zijun Wang', 'Xi Chen', 'Shixiang Tang', 'Lei Bai', 'Wanli Ouyang', 'Mingyu Ding', 'Huaxiu Yao', 'Aoran Wang'], 'affiliations': ['Fudan University', 'HKUST (Guangzhou)', 'Nankai University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong', 'Tsinghua University', 'UC Santa Cruz', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2508.17380.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#multimodal', '#interpretability', '#dataset', '#science'], 'emoji': '🔬', 'ru': {'title': 'VIPER-R1: Мультимодальный ИИ для открытия законов физики', 'desc': 'VIPER-R1 - это мультимодальная модель для автоматического открытия физических законов, сочетающая визуальное восприятие, данные о траекториях и символические рассуждения. Модель обучается с помощью курса индукции структуры движения и символической калибровки с подкреплением. VIPER-R1 превосходит существующие методы по точности и интерпретируемости при обнаружении физических законов. Для поддержки исследования был создан новый мультимодальный корпус PhysSymbol с 5000 примеров.'}, 'en': {'title': 'Unlocking Physical Laws with Multimodal Insights', 'desc': 'VIPER-R1 is a multimodal model designed to discover physical laws by integrating visual perception, trajectory data, and symbolic reasoning. Unlike traditional methods that focus on single data types, VIPER-R1 utilizes a combination of visual and motion data to better understand complex physical phenomena. The model employs a structured training approach, including Motion Structure Induction and reinforcement learning, to refine its symbolic formulas. Experimental results demonstrate that VIPER-R1 achieves higher accuracy and interpretability compared to existing models, making it a significant advancement in automated scientific discovery.'}, 'zh': {'title': 'VIPER-R1：多模态模型助力物理定律发现', 'desc': 'VIPER-R1是一种多模态模型，结合了视觉感知、轨迹数据和符号推理，能够以更高的准确性和可解释性发现物理定律。现有方法主要依赖于符号回归或大型语言模型，通常只处理单一模态数据，忽视了运动的丰富视觉表征。VIPER-R1通过运动结构归纳（MSI）和奖励引导的符号校准（RGSC）来训练，模拟科学发现过程。实验表明，VIPER-R1在准确性和可解释性上均优于现有的最先进模型，能够更精确地发现物理定律。'}}}, {'id': 'https://huggingface.co/papers/2508.14197', 'title': 'CLIPSym: Delving into Symmetry Detection with CLIP', 'url': 'https://huggingface.co/papers/2508.14197', 'abstract': "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.", 'score': 1, 'issue_id': 5642, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': 'fe61d1db34c28a8d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#architecture', '#optimization', '#multimodal', '#cv', '#games'], 'emoji': '🔍', 'ru': {'title': 'CLIPSym: Улучшенное обнаружение симметрии с помощью языковой модели', 'desc': 'CLIPSym - это новая модель для обнаружения симметрии, использующая предобученную модель CLIP. Она сочетает энкодеры изображений и текста CLIP с ротационно-эквивариантным декодером на основе трансформера и G-свертки. Модель использует технику семантически-осведомленной группировки промптов для лучшего учета семантических подсказок при обнаружении симметрии. CLIPSym превосходит современные методы на стандартных наборах данных для обнаружения симметрии.'}, 'en': {'title': 'Enhancing Symmetry Detection with CLIPSym', 'desc': 'CLIPSym is a vision-language model that improves the detection of symmetry in images by using a rotation-equivariant decoder and a new prompting technique. It builds on the capabilities of the CLIP model, which combines image and text understanding, to enhance symmetry detection by incorporating semantic information from image descriptions. The model employs a hybrid architecture that integrates Transformer and G-Convolution to effectively recognize both rotation and reflection symmetries. Experimental results demonstrate that CLIPSym surpasses existing methods on multiple standard datasets, confirming the effectiveness of its innovative approaches.'}, 'zh': {'title': 'CLIPSym：提升对称性检测的新方法', 'desc': 'CLIPSym是一种基于CLIP的视觉-语言模型，旨在提高对称性检测的能力。它采用了一种旋转等变解码器和语义感知提示技术，能够更好地利用自然图像描述中的对称性线索。通过结合Transformer和G-卷积的混合结构，CLIPSym能够有效检测旋转和反射对称性。实验结果表明，CLIPSym在三个标准对称性检测数据集上超越了现有的最先进方法。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2508.21172', 'title': 'Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2508.21172', 'abstract': 'Deep Residual Echo State Networks (DeepResESNs) enhance long-term temporal modeling and memory capacity through hierarchical untrained residual layers, outperforming traditional shallow and deep reservoir computing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.', 'score': 0, 'issue_id': 5650, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'a0a4fea9c9a49fe6', 'authors': ['Matteo Pinna', 'Andrea Ceni', 'Claudio Gallicchio'], 'affiliations': ['Department of Computer Science, University of Pisa, 56127 Pisa, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2508.21172.jpg', 'data': {'categories': ['#long_context', '#math', '#optimization', '#architecture', '#training'], 'emoji': '🌊', 'ru': {'title': 'Глубокие остаточные сети для улучшенной обработки временных рядов', 'desc': 'В этой статье представлены Глубокие Остаточные Эхо-Государственные Сети (DeepResESNs), новый класс глубоких необученных рекуррентных нейронных сетей. Они используют иерархию необученных остаточных рекуррентных слоев для улучшения долгосрочного временного моделирования и емкости памяти. Авторы рассматривают различные ортогональные конфигурации для временных остаточных соединений и проводят математический анализ условий стабильной динамики сети. Эксперименты показывают преимущества предложенного подхода над традиционными поверхностными и глубокими методами резервуарных вычислений в различных задачах временных рядов.'}, 'en': {'title': 'Boosting Memory with Deep Residual Echo State Networks', 'desc': "Deep Residual Echo State Networks (DeepResESNs) are a new type of untrained Recurrent Neural Network designed to improve the handling of long-term temporal data. They use a structure of hierarchical residual layers that do not require training, which enhances their memory capacity. This paper explores how different configurations of these residual connections can affect the network's performance and stability. The results demonstrate that DeepResESNs outperform traditional reservoir computing methods in various time series tasks."}, 'zh': {'title': '深残差网络，提升记忆与建模能力', 'desc': '深残差回声状态网络（DeepResESNs）通过层次化的未训练残差层增强了长期时间建模和记忆能力，超越了传统的浅层和深层水库计算方法。回声状态网络（ESNs）是一种特殊类型的未训练递归神经网络（RNN），在水库计算框架中因其快速高效的学习而受到欢迎。然而，传统的ESNs在处理长期信息时常常面临挑战。本文提出了一种基于时间残差连接的新型深度未训练RNN，展示了利用未训练的残差递归层的层次结构显著提升了记忆容量和长期时间建模能力。'}}}, {'id': 'https://huggingface.co/papers/2508.19600', 'title': 'Quantization Robustness to Input Degradations for Object Detection', 'url': 'https://huggingface.co/papers/2508.19600', 'abstract': 'Post-training quantization of YOLO models is evaluated for robustness to real-world degradations, with a focus on the effectiveness of a degradation-aware calibration strategy for Static INT8 quantization.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop (~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at https://github.com/AllanK24/QRID.', 'score': 0, 'issue_id': 5649, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': 'ab400b3d8dc110c8', 'authors': ['Toghrul Karimov', 'Hassan Imani', 'Allan Kazakov'], 'affiliations': ['Bahcesehir University, Baku, Azerbaijan', 'Bahcesehir University, Istanbul, Turkey'], 'pdf_title_img': 'assets/pdf/title_img/2508.19600.jpg', 'data': {'categories': ['#benchmark', '#security', '#optimization', '#inference'], 'emoji': '🔬', 'ru': {'title': 'Квантизация YOLO: баланс между эффективностью и устойчивостью', 'desc': 'Статья исследует влияние пост-тренировочной квантизации моделей YOLO на их устойчивость к искажениям изображений в реальном мире. Авторы оценивают эффективность стратегии калибровки с учетом искажений для статической INT8 квантизации. Эксперименты проводились на наборе данных COCO с различными типами искажений, включая шум, размытие и JPEG-сжатие. Результаты показывают, что предложенная стратегия калибровки не дает последовательных улучшений устойчивости по сравнению со стандартной калибровкой на чистых данных.'}, 'en': {'title': 'Enhancing YOLO Robustness with Degradation-Aware Calibration', 'desc': 'This paper investigates the post-training quantization (PTQ) of YOLO object detection models to assess their robustness against real-world image degradations. It specifically focuses on a degradation-aware calibration strategy for Static INT8 quantization, which aims to improve model performance when faced with various input distortions like noise and blur. The study evaluates different precision formats and benchmarks the models on the COCO dataset under multiple degradation scenarios. Results show that while Static INT8 quantization improves processing speed, the proposed calibration method does not consistently enhance robustness, particularly for smaller models, although larger models may benefit under certain conditions.'}, 'zh': {'title': '提升YOLO模型鲁棒性的量化策略', 'desc': '本文研究了YOLO模型的后训练量化（PTQ）在真实世界退化下的鲁棒性，特别关注静态INT8量化的退化感知校准策略的有效性。研究表明，虽然静态INT8 TensorRT引擎在干净数据上提供了显著的速度提升，但在大多数模型和退化条件下，退化感知校准并未带来一致的鲁棒性改善。对于特定噪声条件下的大型模型，校准效果有所不同，表明模型容量可能影响校准方法的有效性。该研究为在不受控环境中部署量化检测器提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2508.17008', 'title': 'EduRABSA: An Education Review Dataset for Aspect-based Sentiment\n  Analysis Tasks', 'url': 'https://huggingface.co/papers/2508.17008', 'abstract': 'EduRABSA is a public dataset and ASQE-DPT is a tool for aspect-based sentiment analysis in education reviews, addressing the lack of resources in this domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.', 'score': 0, 'issue_id': 5649, 'pub_date': '2025-08-23', 'pub_date_card': {'ru': '23 августа', 'en': 'August 23', 'zh': '8月23日'}, 'hash': '22b921b4352ed731', 'authors': ['Yan Cathy Hua', 'Paul Denny', 'Jörg Wicker', 'Katerina Taskova'], 'affiliations': ['School of Computer Science, University of Auckland, New Zealand'], 'pdf_title_img': 'assets/pdf/title_img/2508.17008.jpg', 'data': {'categories': ['#open_source', '#dataset', '#low_resource', '#data'], 'emoji': '🎓', 'ru': {'title': 'Новые инструменты для анализа мнений в образовательных отзывах', 'desc': 'EduRABSA - это публичный датасет для анализа тональности по аспектам в образовательных отзывах. ASQE-DPT - инструмент для разметки данных, генерирующий размеченные наборы для задач ABSA из однозадачной аннотации. Эти ресурсы восполняют пробел в данных для ABSA в образовательной сфере. Датасет охватывает отзывы о курсах, преподавателях и университетах на английском языке.'}, 'en': {'title': 'Empowering Education Insights with EduRABSA and ASQE-DPT', 'desc': 'EduRABSA is a newly introduced public dataset specifically designed for aspect-based sentiment analysis (ABSA) in educational reviews, addressing the scarcity of resources in this area. The dataset includes annotated reviews covering various subjects such as courses, teaching staff, and universities, facilitating detailed opinion mining. Additionally, the ASQE-DPT tool allows for efficient manual data annotation, enabling researchers to create labeled datasets for comprehensive ABSA tasks. This work aims to enhance research in education by providing essential resources that promote transparency and reproducibility in sentiment analysis.'}, 'zh': {'title': '推动教育评论情感分析的资源创新', 'desc': 'EduRABSA是一个公共数据集，专注于教育评论的基于方面的情感分析（ABSA），解决了该领域资源匮乏的问题。该数据集涵盖了课程、教学人员和大学三种评论主题，并支持多种ABSA任务，包括隐含方面和隐含意见的提取。ASQE-DPT是一个轻量级的手动数据注释工具，可以从单一任务注释生成标记数据集，促进了教育领域的研究透明性和可重复性。通过这些资源，我们希望推动教育评论的情感分析研究，填补现有的研究空白。'}}}, {'id': 'https://huggingface.co/papers/2509.06160', 'title': 'Reverse-Engineered Reasoning for Open-Ended Generation', 'url': 'https://huggingface.co/papers/2509.06160', 'abstract': "REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.", 'score': 94, 'issue_id': 5784, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 сентября', 'en': 'September 7', 'zh': '9月7日'}, 'hash': '4df21d6c69df73d5', 'authors': ['Haozhe Wang', 'Haoran Que', 'Qixin Xu', 'Minghao Liu', 'Wangchunshu Zhou', 'Jiazhan Feng', 'Wanjun Zhong', 'Wei Ye', 'Tong Yang', 'Wenhao Huang', 'Ge Zhang', 'Fangzhen Lin'], 'affiliations': ['ByteDance Seed', 'Hong Kong University of Science and Technology', 'M-A-P', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06160.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#architecture', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обратная инженерия рассуждений: новый подход к глубокому обучению', 'desc': 'REER (Reverse-Engineered Reasoning) - это новая парадигма в глубоком обучении, которая использует обратную инженерию для обнаружения пошаговых процессов рассуждения. Этот подход позволяет модели эффективно выполнять открытые задачи, преодолевая ограничения традиционных методов, таких как обучение с подкреплением и дистилляция инструкций. На основе REER был создан набор данных DeepWriting-20K и обучена модель DeepWriter-8B, которая показывает результаты, конкурентоспособные с ведущими проприетарными моделями.'}, 'en': {'title': 'Unlocking Creativity with Reverse Engineering in Deep Reasoning', 'desc': 'REER introduces a novel approach to deep reasoning by utilizing reverse engineering to uncover the step-by-step processes behind successful solutions. This method addresses the limitations of traditional reinforcement learning and instruction distillation, which struggle with open-ended tasks due to unclear rewards and high costs. By working backwards from known solutions, REER enables the discovery of effective reasoning pathways without the need for extensive trial-and-error. The resulting model, DeepWriter-8B, demonstrates competitive performance against both open-source and proprietary models, showcasing the potential of this new paradigm in creative generation tasks.'}, 'zh': {'title': '逆向推理，开启深度推理新纪元', 'desc': 'REER是一种新的深度推理范式，通过逆向工程发现逐步推理过程，使模型能够在开放性任务中表现出色。传统的推理方法，如强化学习和指令蒸馏，在处理开放性生成时面临挑战，前者缺乏明确的奖励信号，后者成本高昂且受限于教师模型的能力。REER通过从已知的优秀解决方案向后推导，计算性地发现潜在的逐步深度推理过程，从而克服了这些限制。我们还开源了DeepWriting-20K数据集，包含20,000个深度推理轨迹，训练的DeepWriter-8B模型在开放性任务中超越了强大的开源基线，并在某些情况下与领先的专有模型如GPT-4o和Claude 3.5的表现相当或更优。'}}}, {'id': 'https://huggingface.co/papers/2509.06501', 'title': 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents', 'url': 'https://huggingface.co/papers/2509.06501', 'abstract': 'WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.', 'score': 52, 'issue_id': 5787, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'a4011c0eeba062d1', 'authors': ['Junteng Liu', 'Yunji Li', 'Chi Zhang', 'Jingyang Li', 'Aili Chen', 'Ke Ji', 'Weiyu Cheng', 'Zijia Wu', 'Chengyu Du', 'Qidi Xu', 'Jiayuan Song', 'Zhengmao Zhu', 'Wenhu Chen', 'Pengyu Zhao', 'Junxian He'], 'affiliations': ['The Hong Kong University of Science and Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.06501.jpg', 'data': {'categories': ['#agents', '#training', '#reasoning', '#agi', '#dataset', '#rl', '#long_context'], 'emoji': '🕸️', 'ru': {'title': 'WebExplorer: Маленький агент с большими возможностями', 'desc': 'WebExplorer - это новый подход к разработке продвинутых веб-агентов, основанный на данных. Он использует систематическую генерацию данных и обучение с подкреплением для достижения передовых результатов в задачах поиска информации. Модель WebExplorer-8B поддерживает контекст до 128K токенов и до 100 вызовов инструментов, что позволяет решать сложные многоэтапные задачи. Несмотря на размер всего 8 миллиардов параметров, WebExplorer-8B превосходит более крупные модели на различных бенчмарках поиска информации.'}, 'en': {'title': 'WebExplorer: Pioneering Advanced Web Agents for Information Seeking', 'desc': "WebExplorer is a novel approach that enhances web agents' ability to seek information effectively by generating challenging data through model-based exploration and iterative query evolution. It addresses the limitations of existing web agents, which often struggle with complex tasks due to a lack of robust data. By employing reinforcement learning and supervised fine-tuning, WebExplorer-8B demonstrates superior performance in information-seeking benchmarks, outperforming larger models in accuracy. This work paves the way for advanced web agents capable of long-horizon problem solving and complex web navigation."}, 'zh': {'title': 'WebExplorer：信息检索的先进网络代理', 'desc': 'WebExplorer是一种基于数据驱动的方法，用于开发先进的网络代理，能够在信息检索任务中实现最先进的性能。该方法通过系统的数据生成和强化学习，解决了现有开源网络代理在复杂任务中的信息检索能力不足的问题。WebExplorer通过模型驱动的探索和迭代的查询演变，生成需要多步推理和复杂网页导航的挑战性查询-答案对。最终，WebExplorer-8B模型在多种信息检索基准测试中表现出色，展示了其在长时间问题解决中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.06949', 'title': 'Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models', 'url': 'https://huggingface.co/papers/2509.06949', 'abstract': 'TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL', 'score': 33, 'issue_id': 5783, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'f6f75969a4d93684', 'authors': ['Yinjie Wang', 'Ling Yang', 'Bowen Li', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.06949.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#math', '#rl', '#open_source', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений языковых моделей с помощью траекторного обучения с подкреплением', 'desc': 'TraceRL - это новый метод обучения с подкреплением для диффузионных языковых моделей, который улучшает их способность рассуждать над сложными задачами. Он использует траекторно-ориентированный подход и диффузионную модель ценности для повышения стабильности обучения. Метод позволяет адаптировать модели к работе с более длинными блоками текста, повышая гибкость сэмплирования. С помощью TraceRL авторы создали серию современных диффузионных языковых моделей TraDo, превосходящих более крупные авторегрессионные модели в задачах математических рассуждений.'}, 'en': {'title': 'Enhancing Language Models with Trajectory-Aware Reinforcement Learning', 'desc': 'TraceRL is a new framework that improves diffusion language models (DLMs) by using trajectory-aware reinforcement learning. This method helps the models perform better on complex reasoning tasks, such as math and coding, by incorporating preferred inference paths during training. The framework also allows for better sampling flexibility and can adapt smaller models to larger ones. With TraceRL, the resulting models, like TraDo, achieve significant accuracy improvements over existing models, demonstrating their effectiveness in challenging reasoning benchmarks.'}, 'zh': {'title': 'TraceRL：提升推理性能的轨迹感知强化学习', 'desc': 'TraceRL是一种针对扩散语言模型的轨迹感知强化学习框架，能够在后期训练中融入优先推理轨迹，适用于不同的模型架构。该框架配备了基于扩散的价值模型，提升了训练的稳定性，并在复杂的数学和编程任务上展示了更好的推理性能。此外，TraceRL还可以将特定块的模型适应到更大的块，从而提高采样的灵活性。通过使用TraceRL，我们开发了一系列最先进的扩散语言模型TraDo，尽管其规模小于7B的自回归模型，但在复杂的数学推理任务中仍然表现优异。'}}}, {'id': 'https://huggingface.co/papers/2509.06467', 'title': 'Does DINOv3 Set a New Medical Vision Standard?', 'url': 'https://huggingface.co/papers/2509.06467', 'abstract': "DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.", 'score': 26, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '05928cb3cc52644d', 'authors': ['Che Liu', 'Yinda Chen', 'Haoyuan Shi', 'Jinpeng Lu', 'Bailiang Jian', 'Jiazhen Pan', 'Linghan Cai', 'Jiayi Wang', 'Yundi Zhang', 'Jun Li', 'Cosmin I. Bercea', 'Cheng Ouyang', 'Chen Chen', 'Zhiwei Xiong', 'Benedikt Wiestler', 'Christian Wachinger', 'Daniel Rueckert', 'Wenjia Bai', 'Rossella Arcucci'], 'affiliations': ['Dresden University of Technology', 'Helmholtz AI and Helmholtz Munich', 'Imperial College London', 'Munich Center for Machine Learning', 'Technical University of Munich (TUM)', 'University of Erlangen-Nuremberg', 'University of Oxford', 'University of Science and Technology of China', 'University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2509.06467.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#cv', '#healthcare', '#benchmark', '#optimization', '#science'], 'emoji': '🩺', 'ru': {'title': 'DINOv3: Универсальный кодировщик для медицинской визуализации', 'desc': 'Модель DINOv3, основанная на архитектуре Vision Transformer, показывает высокую эффективность в различных задачах медицинской визуализации без специфической предварительной подготовки для медицинской области. Исследование выявило, что DINOv3 может превзойти специализированные медицинские модели на некоторых задачах, несмотря на обучение только на естественных изображениях. Однако модель имеет ограничения в глубоко специализированных доменах, таких как патологические изображения и электронная микроскопия. Кроме того, DINOv3 не всегда следует законам масштабирования в медицинской области, показывая разное поведение при увеличении размера модели или разрешения входных данных.'}, 'en': {'title': 'DINOv3: A Powerful Vision Transformer for Medical Imaging Tasks', 'desc': 'DINOv3 is a self-supervised vision transformer that excels in various medical imaging tasks without needing pre-training on medical data. It has been benchmarked against common tasks like classification and segmentation, showing strong performance and even surpassing some specialized medical models. However, it struggles in highly specialized areas, such as Whole-Slide Pathological Images and Electron Microscopy, where deep domain knowledge is crucial. Additionally, DINOv3 does not consistently follow scaling laws in the medical domain, indicating that larger models or higher resolutions do not always lead to better performance.'}, 'zh': {'title': 'DINOv3：医学视觉任务的新基准', 'desc': 'DINOv3是一种自监督的视觉变换器，在各种医学视觉任务中表现出色，无需特定领域的预训练。尽管它在许多任务中超越了医学特定的基础模型，但在需要深度专业化的领域中表现有限。研究表明，DINOv3的性能在不同模型大小和输入图像分辨率下并不总是遵循扩展规律。总的来说，DINOv3为复杂的医学任务提供了强大的视觉特征基础，开启了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2509.01656', 'title': 'Reinforced Visual Perception with Tools', 'url': 'https://huggingface.co/papers/2509.01656', 'abstract': "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.", 'score': 24, 'issue_id': 5783, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '08ec116a2ed1dc2c', 'authors': ['Zetong Zhou', 'Dongping Chen', 'Zixian Ma', 'Zhihan Hu', 'Mingyang Fu', 'Sinan Wang', 'Yao Wan', 'Zhou Zhao', 'Ranjay Krishna'], 'affiliations': ['ONE Lab, HUST', 'University of Maryland', 'University of Washington', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01656.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#optimization', '#benchmark', '#rl', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Революция в визуальном мышлении ИИ через обучение с подкреплением', 'desc': 'Статья представляет метод ReVPT, улучшающий способности мультимодальных языковых моделей к визуальному рассуждению с помощью обучения с подкреплением. ReVPT использует новый алгоритм RL на основе GRPO для обучения моделей рассуждать с помощью набора из четырех визуальных инструментов. Эксперименты показывают, что метод достигает лучших результатов на нескольких бенчмарках, включая SAT, CV-Bench, BLINK и MMStar. ReVPT значительно превосходит базовые модели, обученные с учителем и на текстовых данных.'}, 'en': {'title': 'ReVPT: Reinforcement Learning for Enhanced Visual Reasoning in Multi-Modal LLMs', 'desc': 'ReVPT is a novel approach that enhances the visual reasoning capabilities of multi-modal large language models (LLMs) using reinforcement learning (RL). It addresses the limitations of previous methods that relied on supervised finetuning, which often required expensive data generation and struggled with generalization. By introducing a new RL algorithm based on Generalized Relative Policy Optimization (GRPO), ReVPT trains models to effectively utilize a set of visual tools for reasoning tasks. The results demonstrate that ReVPT achieves state-of-the-art performance on various visual benchmarks, significantly outperforming existing methods.'}, 'zh': {'title': 'ReVPT：提升多模态LLM的视觉推理能力', 'desc': 'ReVPT是一种增强多模态大语言模型（LLM）视觉推理能力的方法，采用强化学习技术，达到了视觉基准测试的最先进性能。视觉推理是人类智能的核心，涉及复杂的感知和逻辑过程，但将计算机视觉模型应用于一般视觉推理仍然具有挑战性。以往的研究表明，通过监督微调将视觉模型与LLM结合可以提高性能，但存在数据生成成本高、数据过滤依赖性强和泛化能力差等关键限制。ReVPT通过引入基于GRPO的新型强化学习算法，训练模型使用四种视觉工具进行推理，从而有效解决了这些问题。'}}}, {'id': 'https://huggingface.co/papers/2509.06733', 'title': 'Reinforcement Learning Foundations for Deep Research Systems: A Survey', 'url': 'https://huggingface.co/papers/2509.06733', 'abstract': 'Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.', 'score': 17, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '039bfe4e964d5aad', 'authors': ['Wenjun Li', 'Zhi Chen', 'Jingru Lin', 'Hannan Cao', 'Wei Han', 'Sheng Liang', 'Zhi Zhang', 'Kuicai Dong', 'Dexun Li', 'Chen Zhang', 'Yong Liu'], 'affiliations': ['Huawei Technologies Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2509.06733.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#training', '#long_context', '#benchmark', '#survey', '#optimization', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Обучение с подкреплением как основа для создания интеллектуальных исследовательских систем', 'desc': 'Статья исследует применение обучения с подкреплением для тренировки глубоких исследовательских систем искусственного интеллекта. Авторы рассматривают ограничения методов обучения с учителем и выравнивания предпочтений, предлагая использовать обучение с подкреплением для оптимизации политик взаимодействия с инструментами и исследования. Обсуждаются три ключевых аспекта: синтез и курирование данных, методы обучения с подкреплением для агентных исследований и системы обучения агентов. Статья также затрагивает вопросы архитектуры агентов, координации и методов оценки.'}, 'en': {'title': 'Empowering AI with Reinforcement Learning for Complex Task Mastery', 'desc': 'This paper discusses the use of reinforcement learning (RL) as a key method for training advanced AI systems that can perform complex tasks. It highlights the limitations of traditional supervised learning and preference alignment methods, which often rely on human-defined rules and can lead to biases. The authors propose that RL can improve the training of these systems by optimizing their interactions with tools and environments, allowing for better exploration and decision-making. The paper also provides a comprehensive overview of RL techniques and frameworks that can enhance the development of deep research agents.'}, 'zh': {'title': '强化学习：深度研究系统的基础', 'desc': '本论文探讨了强化学习作为训练深度研究系统的基础方法，旨在解决监督学习和偏好对齐方法的局限性。通过优化工具交互和探索的策略，强化学习能够更好地处理复杂的多步骤任务。我们系统化了与深度研究系统相关的强化学习方法，包括数据合成、样本效率和多目标优化等方面。最后，论文提供了关于如何训练稳健、透明的深度研究代理的实用指导。'}}}, {'id': 'https://huggingface.co/papers/2509.06461', 'title': "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", 'url': 'https://huggingface.co/papers/2509.06461', 'abstract': "Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.", 'score': 13, 'issue_id': 5785, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'f239789cf3c0a63d', 'authors': ['Yuyao Ge', 'Shenghua Liu', 'Yiwei Wang', 'Lingrui Mei', 'Baolong Bi', 'Xuanshan Zhou', 'Jiayu Yao', 'Jiafeng Guo', 'Xueqi Cheng'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2509.06461.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#training', '#multimodal', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Повышение точности визуального анализа через контрастное уточнение внимания', 'desc': 'Статья представляет метод CARVE для улучшения работы визуально-языковых моделей (VLM) в сложных визуальных средах. CARVE использует контрастирование внимания для извлечения релевантных визуальных сигналов на уровне пикселей. Исследование выявило связь между визуальной сложностью и энтропией внимания, а также показало, как внимание модели уточняется от общего сканирования к фокусированной конвергенции. Метод CARVE не требует дополнительного обучения и значительно повышает производительность открытых VLM-моделей.'}, 'en': {'title': 'Enhancing Visual Reasoning with Contrastive Attention', 'desc': 'The paper introduces Contrastive Attention Refinement for Visual Enhancement (CARVE), a method designed to improve the performance of Vision-Language Models (VLMs) in complex visual environments. It identifies that visual complexity affects attention patterns, leading to decreased reasoning performance. By contrasting attention maps from general and task-specific queries, CARVE effectively separates useful visual signals from noise without requiring additional training or external tools. The results show significant performance improvements, highlighting the importance of attention mechanisms in visual reasoning tasks.'}, 'zh': {'title': '对比注意力精炼，提升视觉推理能力！', 'desc': '对比注意力精炼（CARVE）是一种提高视觉语言模型（VLM）性能的方法，通过对比注意力机制提取与任务相关的视觉信号。该方法解决了在复杂视觉环境中，现有增强方法需要额外训练或依赖外部分割工具的问题。研究发现，视觉复杂性与注意力熵密切相关，影响推理性能，而注意力在不同层次上逐渐从全局扫描转向深层的聚焦收敛。CARVE通过对比一般查询和任务特定查询的注意力图，能够有效分解视觉信号，提升视觉推理能力。'}}}, {'id': 'https://huggingface.co/papers/2509.06155', 'title': 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts', 'url': 'https://huggingface.co/papers/2509.06155', 'abstract': 'UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.', 'score': 12, 'issue_id': 5783, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 сентября', 'en': 'September 7', 'zh': '9月7日'}, 'hash': '9f7107d4dbe89f53', 'authors': ['Duomin Wang', 'Wei Zuo', 'Aojie Li', 'Ling-Hao Chen', 'Xinyao Liao', 'Deyu Zhou', 'Zixin Yin', 'Xili Dai', 'Daxin Jiang', 'Gang Yu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology(GuangZhou)', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06155.jpg', 'data': {'categories': ['#multimodal', '#video', '#audio', '#benchmark', '#open_source'], 'emoji': '🎥', 'ru': {'title': 'UniVerse-1: синергия аудио и видео в одном модели', 'desc': 'В статье представлена модель UniVerse-1, которая объединяет генерацию аудио и видео, используя технику "stitching of experts". Это позволяет эффективно использовать уже обученные модели для видео и музыки, обеспечивая точное временное согласование. Для улучшения аннотаций и временного выравнивания звуков и речи с видео, разработан онлайн-пайплайн аннотаций. Модель была дообучена на 7600 часах данных и показала высокое качество координации аудио-визуальных элементов.'}, 'en': {'title': 'UniVerse-1: Harmonizing Audio and Video Generation', 'desc': 'UniVerse-1 is a cutting-edge model designed for generating synchronized audio and video content. It utilizes a stitching of experts (SoE) technique to merge pre-trained models for video and music, enhancing the quality of the generated outputs. The model incorporates an online annotation pipeline to ensure precise temporal alignment between audio and video, which is crucial for maintaining coherence in the generated media. After fine-tuning on a substantial dataset, UniVerse-1 demonstrates significant improvements in producing well-coordinated audio-visuals, making it a valuable tool for advancing research in the field.'}, 'zh': {'title': '统一音视频生成的未来', 'desc': 'UniVerse-1 是一个统一的音频视频生成模型，采用专家拼接技术，将预训练的视频和音乐模型结合在一起，确保时间上的准确对齐，生成高质量的音视频输出。该模型通过深度融合预训练的专家模型，避免了从头开始训练的低效，充分利用了已有的基础能力。为了确保音频和视频内容的准确注释和时间对齐，我们开发了一个在线注释管道，在训练过程中处理所需的数据并生成标签。经过约7600小时的音视频数据微调后，我们的模型能够生成协调良好的音视频内容，并在语音生成方面实现强对齐。'}}}, {'id': 'https://huggingface.co/papers/2509.03516', 'title': 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?', 'url': 'https://huggingface.co/papers/2509.03516', 'abstract': 'T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/.', 'score': 9, 'issue_id': 5784, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 сентября', 'en': 'September 3', 'zh': '9月3日'}, 'hash': '8f5ee8fae69242b1', 'authors': ['Ouxiang Li', 'Yuan Wang', 'Xinting Hu', 'Huijuan Huang', 'Rui Chen', 'Jiarong Ou', 'Xin Tao', 'Pengfei Wan', 'Fuli Feng'], 'affiliations': ['Kuaishou Technology', 'Nanyang Technological University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.03516.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning', '#games'], 'emoji': '🎨', 'ru': {'title': 'T2I-CoReBench: комплексная оценка генерации изображений по тексту', 'desc': 'T2I-CoReBench - это новый бенчмарк для оценки способностей моделей преобразования текста в изображение к композиции и рассуждению. Он включает 1080 сложных промптов и около 13500 контрольных вопросов, структурированных по 12-мерной таксономии. Бенчмарк оценивает способности моделей к композиции на основе элементов графа сцены и к рассуждению на основе философских типов вывода. Эксперименты показали, что современные модели text-to-image все еще ограничены в сложных сценариях с высокой плотностью, а способность к рассуждению значительно отстает.'}, 'en': {'title': 'Elevating Text-to-Image Models: A New Benchmark for Composition and Reasoning', 'desc': 'T2I-CoReBench is a new benchmark designed to assess the composition and reasoning abilities of text-to-image (T2I) models. It introduces a detailed evaluation framework that includes a 12-dimensional taxonomy focusing on scene graph elements and various types of reasoning. The benchmark features 1,080 complex prompts and approximately 13,500 checklist questions to ensure a thorough evaluation of model performance. Results indicate that while T2I models can handle basic tasks, they struggle significantly with complex scenarios and implicit reasoning, highlighting areas for improvement.'}, 'zh': {'title': '评估文本到图像模型的组合与推理能力', 'desc': 'T2I-CoReBench是一个基准测试，旨在评估文本到图像模型的组合和推理能力。该基准通过复杂的提示和检查问题，全面考察模型在高场景密度下的表现。研究发现，现有模型在复杂场景中的组合能力仍然有限，而推理能力更是成为关键瓶颈。我们提出的基准包含1080个挑战性提示和约13500个检查问题，以实现细致可靠的评估。'}}}, {'id': 'https://huggingface.co/papers/2509.02108', 'title': 'DivMerge: A divergence-based model merging method for multi-tasking', 'url': 'https://huggingface.co/papers/2509.02108', 'abstract': 'Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose a method that merges models trained on different tasks into a single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work.', 'score': 8, 'issue_id': 5797, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '615289bb7bc02e97', 'authors': ['Touayouch Brahim', 'Fosse Loïc', 'Damnati Géraldine', 'Lecorvé Gwénolé'], 'affiliations': ['CNRS, LIS, Aix Marseille Université, France', 'Orange Research, Lannion, France', 'École polytechnique, Institut polytechnique de Paris, Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.02108.jpg', 'data': {'categories': ['#training', '#dataset', '#architecture', '#optimization', '#transfer_learning'], 'emoji': '🔀', 'ru': {'title': 'Эффективное слияние моделей для мультизадачного обучения без интерференции', 'desc': 'Статья представляет новый метод объединения моделей, обученных на разных задачах, в единую модель. Авторы используют дивергенцию Йенсена-Шеннона для управления процессом слияния без необходимости в дополнительных размеченных данных. Метод автоматически балансирует важность задач и остается эффективным при увеличении их количества. Предложенный подход превосходит существующие методы и решает проблему интерференции задач в мультизадачном обучении.'}, 'en': {'title': 'Merging Models for Robust Multi-Task Learning', 'desc': 'This paper discusses a new method for multi-task learning (MTL) that combines models trained on different tasks into one model. The challenge of task interference, which can degrade performance as more tasks are added, is addressed by using Jensen-Shannon divergence to guide the merging process. This method does not need extra labeled data and automatically balances the importance of each task. The proposed approach shows improved robustness and performance compared to existing methods, even as the number of tasks increases.'}, 'zh': {'title': '智能合并，提升多任务学习性能', 'desc': '多任务学习（MTL）通常通过在微调之前合并数据集来实现，但随着微调模型的日益增多，出现了通过任务算术合并模型的新方法。在这种情况下，一个主要挑战是任务干扰，随着任务数量的增加而加剧。我们提出了一种将不同任务训练的模型合并为单一模型的方法，能够在所有任务中保持强大的性能。我们的方法利用了詹森-香农散度来指导合并过程，无需额外的标记数据，并自动平衡任务的重要性。'}}}, {'id': 'https://huggingface.co/papers/2509.06945', 'title': 'Interleaving Reasoning for Better Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2509.06945', 'abstract': 'Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .', 'score': 7, 'issue_id': 5786, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '0f828dd1d00b0a90', 'authors': ['Wenxuan Huang', 'Shuang Chen', 'Zheyong Xie', 'Shaosheng Cao', 'Shixiang Tang', 'Yufan Shen', 'Qingyu Yin', 'Wenbo Hu', 'Xiaoman Wang', 'Yuntian Tang', 'Junbo Qiao', 'Yue Guo', 'Yao Hu', 'Zhenfei Yin', 'Philip Torr', 'Yu Cheng', 'Wanli Ouyang', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles', 'University of Oxford', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06945.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#optimization', '#reasoning', '#dataset', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'Чередование рассуждений и генерации для улучшения Text-to-Image моделей', 'desc': 'Статья представляет новый подход к генерации изображений по текстовому описанию - Interleaving Reasoning Generation (IRG). Эта методика чередует текстовое рассуждение и синтез изображений, что позволяет улучшить качество и детализацию генерируемых изображений. Авторы предлагают специальный фреймворк обучения IRGL и датасет IRGL-300K для эффективного обучения модели. Эксперименты показывают, что IRG достигает state-of-the-art результатов на нескольких бенчмарках, значительно улучшая визуальное качество и точность деталей.'}, 'en': {'title': 'Enhancing Image Generation through Interleaved Reasoning', 'desc': 'The Interleaving Reasoning Generation (IRG) framework enhances Text-to-Image (T2I) generation by alternating between text-based reasoning and image synthesis. This approach allows the model to first generate an initial image based on textual input, then refine it by reflecting on the generated output to improve details and visual quality. The training process, called Interleaving Reasoning Generation Learning (IRGL), focuses on establishing a strong initial image and ensuring high-quality textual feedback for further refinements. The results demonstrate significant advancements in performance metrics and visual fidelity, showcasing the effectiveness of this interleaved reasoning approach.'}, 'zh': {'title': '交错推理生成：提升文本到图像的质量', 'desc': '本文提出了一种名为交错推理生成（IRG）的框架，旨在提高文本到图像生成的效果。该框架通过交替进行基于文本的思考和图像合成，来增强生成图像的视觉质量和细节保留。IRG的训练过程包括两个阶段：首先建立核心内容和基础质量，然后在后续图像中实现高质量的文本反思和细致的改进。实验结果表明，IRG在多个评估指标上达到了最先进的性能，显著提升了生成图像的质量。'}}}, {'id': 'https://huggingface.co/papers/2509.06917', 'title': 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents', 'url': 'https://huggingface.co/papers/2509.06917', 'abstract': "Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.", 'score': 7, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '2a482f9a59e68451', 'authors': ['Jiacheng Miao', 'Joe R. Davis', 'Jonathan K. Pritchard', 'James Zou'], 'affiliations': ['Department of Biology, Stanford University', 'Department of Biomedical Data Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Genetics, Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06917.jpg', 'data': {'categories': ['#agents', '#science', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'Превращение научных статей в интерактивных ИИ-ассистентов', 'desc': 'Paper2Agent - это автоматизированная система, которая преобразует научные статьи в интерактивных ИИ-агентов. Она анализирует текст и код статьи, создавая Model Context Protocol (MCP) сервер, который затем подключается к чат-агенту для выполнения сложных научных запросов на естественном языке. Система тестирует и улучшает созданных агентов, чтобы обеспечить их надежность и функциональность. Paper2Agent демонстрирует эффективность на примерах агентов для интерпретации геномных вариантов и анализа транскриптомики.'}, 'en': {'title': 'Transforming Research Papers into Interactive AI Agents', 'desc': 'Paper2Agent is a framework that transforms traditional research papers into interactive AI agents, making it easier for users to access and utilize scientific knowledge. By analyzing the paper and its associated code, it creates a Model Context Protocol (MCP) server that allows the AI agent to answer complex queries in natural language. This approach reduces the effort required for researchers to understand and apply the findings, thus enhancing the dissemination and reuse of research outputs. The effectiveness of Paper2Agent is demonstrated through case studies where it successfully reproduces original results and handles novel queries, paving the way for a new collaborative ecosystem in scientific research.'}, 'zh': {'title': '将静态论文转变为动态 AI 代理的创新之路', 'desc': 'Paper2Agent 是一个自动化框架，可以将研究论文转化为互动的 AI 代理，以促进知识传播和复杂科学查询。它通过分析论文及其相关代码，构建模型上下文协议（MCP）服务器，使得研究成果从被动的文献变为主动的系统。这样，用户可以通过自然语言与 AI 代理进行交流，轻松获取论文中的信息和工具。Paper2Agent 的有效性通过案例研究得到了验证，能够重现原论文的结果并处理新的用户查询。'}}}, {'id': 'https://huggingface.co/papers/2509.06631', 'title': 'Guided Decoding and Its Critical Role in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2509.06631', 'abstract': 'Guided decoding methods in Retrieval-Augmented Generation (RAG) systems are evaluated for structured output generation, revealing performance variations across different prompting setups.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.', 'score': 5, 'issue_id': 5795, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '3deb208811cb9805', 'authors': ['Özgür Uğur', 'Musa Yılmaz', 'Esra Şavirdi', 'Özay Ezerceli', 'Mahmut El Huseyni', 'Selva Taş', 'Reyhan Bayraktar'], 'affiliations': ['Newmind AI Istanbul, Türkiye'], 'pdf_title_img': 'assets/pdf/title_img/2509.06631.jpg', 'data': {'categories': ['#hallucinations', '#rag', '#alignment'], 'emoji': '🧩', 'ru': {'title': 'Управляемое декодирование в RAG: структура и точность', 'desc': 'В этом исследовании рассматривается применение методов управляемого декодирования в системах генерации с дополнением извлечением (RAG) для создания структурированного вывода. Авторы сравнивают три метода - Outlines, XGrammar и LM Format Enforcer - в различных схемах многоходового взаимодействия. Оценивается успешность генерации, уровень галлюцинаций и качество вывода для каждого метода. Результаты показывают, как многоходовое взаимодействие влияет на управляемое декодирование, выявляя неожиданные различия в производительности.'}, 'en': {'title': 'Enhancing Structured Outputs in RAG Systems with Guided Decoding', 'desc': 'This paper investigates how guided decoding methods can improve structured output generation in Retrieval-Augmented Generation (RAG) systems. It compares three specific methods: Outlines, XGrammar, and LM Format Enforcer, under different prompting scenarios. The study measures success rates, hallucination rates, and overall output quality to understand how multi-turn interactions affect performance. The findings provide valuable insights for selecting appropriate methods in RAG systems, enhancing the reliability of AI-generated responses.'}, 'zh': {'title': '优化RAG系统中的结构化输出生成', 'desc': '本研究评估了在检索增强生成（RAG）系统中，指导解码方法对结构化输出生成的影响。我们比较了三种方法：大纲、XGrammar和语言模型格式强制器，分析了它们在不同多轮提示设置下的表现。研究结果显示，多轮交互对指导解码的影响，以及不同方法在成功率、幻觉率和输出质量上的表现差异。此项工作为RAG系统中的结构化输出生成提供了理论见解和实际指导。'}}}, {'id': 'https://huggingface.co/papers/2509.06493', 'title': 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers', 'url': 'https://huggingface.co/papers/2509.06493', 'abstract': 'BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces BFS-Prover-V2, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.', 'score': 5, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'e9de5406241d634e', 'authors': ['Ran Xin', 'Zeyu Zheng', 'Yanchen Nie', 'Kun Yuan', 'Xia Xiao'], 'affiliations': ['ByteDance Seed', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06493.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#training', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабируемое доказательство теорем с помощью ИИ', 'desc': 'BFS-Prover-V2 представляет собой систему автоматического доказательства теорем, решающую проблемы масштабирования в этой области. Она использует инновационную структуру обучения с подкреплением и архитектуру многоагентного поиска с планировщиком. Система применяет многоэтапный конвейер экспертных итераций с адаптивной фильтрацией данных на уровне тактик. BFS-Prover-V2 достигает лучших результатов на эталонных тестах по формальной математике.'}, 'en': {'title': 'Scaling Theorem Proving with Smart Collaboration', 'desc': "BFS-Prover-V2 is a system that enhances automated theorem proving by tackling the challenges of scaling in both training and inference. It introduces a multi-turn off-policy reinforcement learning (RL) framework that improves the performance of large language models (LLMs) during training, inspired by AlphaZero's expert iteration approach. Additionally, it features a planner-enhanced multi-agent search architecture that breaks down complex theorems into simpler subgoals, allowing multiple agents to work together efficiently. This innovative dual approach has achieved state-of-the-art results on formal mathematics benchmarks, demonstrating its potential for broader applications in complex reasoning tasks."}, 'zh': {'title': '双重扩展，突破定理证明的极限', 'desc': 'BFS-Prover-V2 是一个针对自动定理证明中的扩展挑战的系统，结合了多轮离线强化学习框架和规划增强的多智能体搜索架构。该系统通过创新的多轮离线强化学习方法，持续提升大语言模型（LLM）在训练时的表现，并克服了长期强化学习中的性能瓶颈。其次，它采用了一个高层次的规划模型，将复杂定理分解为一系列简单的子目标，从而在推理时有效缩小搜索空间。通过这种双重扩展方法，BFS-Prover-V2 在正式数学基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2509.06861', 'title': 'Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet', 'url': 'https://huggingface.co/papers/2509.06861', 'abstract': 'Test-time scaling does not consistently improve accuracy or reduce hallucinations in knowledge-intensive tasks, often leading to overconfident errors.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge', 'score': 4, 'issue_id': 5789, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '128069f43a6c18a2', 'authors': ['James Xu Zhao', 'Bryan Hooi', 'See-Kiong Ng'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.06861.jpg', 'data': {'categories': ['#inference', '#benchmark', '#hallucinations', '#reasoning'], 'emoji': '🤔', 'ru': {'title': 'Больше вычислений - не всегда лучше: ограничения масштабирования во время вывода', 'desc': 'Исследование показывает, что увеличение вычислительной мощности во время вывода (test-time scaling) не всегда улучшает точность или уменьшает галлюцинации в задачах, требующих обширных знаний. Авторы провели комплексную оценку этого подхода, используя 12 моделей рассуждений на двух бенчмарках. Результаты выявили, что увеличение вычислений часто приводит к большему количеству галлюцинаций и может вызывать эффект подтверждения предвзятости. Тем не менее, по сравнению с моделями без рассуждений, включение этапа обдумывания остается полезным.'}, 'en': {'title': 'Test-Time Scaling: More Thinking, More Hallucinations?', 'desc': 'This paper investigates the effectiveness of test-time scaling in improving the performance of reasoning models on knowledge-intensive tasks. While test-time scaling allows models to generate longer reasoning chains, the authors find that it does not consistently enhance accuracy and can even increase the occurrence of hallucinations. The study evaluates 12 reasoning models across two benchmarks, revealing that longer reasoning often leads to overconfident errors rather than improved factual accuracy. The findings suggest that while extended reasoning can be beneficial, it may also induce confirmation bias, highlighting the need for careful application in knowledge-intensive scenarios.'}, 'zh': {'title': '测试时扩展推理的局限性与挑战', 'desc': '本文探讨了测试时扩展推理对知识密集型任务的影响。尽管测试时扩展推理可以增加推理链的长度并提高模型的表现，但在知识密集型任务中并未显著提高准确性，反而可能导致更多的幻觉错误。研究表明，延长推理时间并不总是能改善模型的事实回忆，反而可能导致模型在面对未回答的问题时产生过度自信的幻觉。尽管存在这些局限性，启用思考仍然比不思考更有益。'}}}, {'id': 'https://huggingface.co/papers/2509.06786', 'title': 'R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World', 'url': 'https://huggingface.co/papers/2509.06786', 'abstract': "A new framework, R²AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose safe-by-coevolution as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce R^2AI -- Resistant and Resilient AI -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. R^2AI integrates fast and slow safe models, adversarial simulation and verification through a safety wind tunnel, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI.", 'score': 3, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'bdf421cc1d868635', 'authors': ['Youbang Sun', 'Xiang Wang', 'Jie Fu', 'Chaochao Lu', 'Bowen Zhou'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.06786.jpg', 'data': {'categories': ['#agents', '#security', '#ethics', '#training', '#agi'], 'emoji': '🛡️', 'ru': {'title': 'Коэволюция безопасности и возможностей ИИ', 'desc': "Статья представляет новую концепцию R²AI для повышения безопасности искусственного интеллекта через коэволюцию. Подход объединяет устойчивость к известным угрозам и адаптивность к непредвиденным рискам, используя быстрые и медленные безопасные модели. R²AI включает в себя adversarial simulation и верификацию через 'safety wind tunnel', а также непрерывные циклы обратной связи. Авторы утверждают, что эта концепция предлагает масштабируемый и проактивный путь к поддержанию постоянной безопасности ИИ в динамичных средах."}, 'en': {'title': 'R²AI: Evolving Safety for Advanced AI Systems', 'desc': 'The paper introduces R²AI, a new framework aimed at improving AI safety by combining two key strategies: resistance to known threats and resilience to unexpected risks. It critiques existing safety approaches that either reactively apply safety measures or struggle with unforeseen challenges in complex environments. R²AI proposes a coevolutionary approach to safety, inspired by biological immunity, where safety is treated as an ongoing learning process. This framework utilizes fast and slow safe models, adversarial simulations, and continuous feedback to ensure that AI systems can adapt and remain safe as they evolve.'}, 'zh': {'title': 'R²AI：共进化提升人工智能安全性', 'desc': '本文提出了一种新的框架R²AI，旨在通过共进化增强人工智能的安全性。该框架结合了对已知威胁的抵抗力和对未知风险的韧性，使用快速和慢速安全模型以及对抗性模拟。R²AI的设计灵感来自生物免疫，强调安全性是一个动态的、对抗性的持续学习过程。通过这一框架，我们可以在动态环境中保持持续的安全性，解决短期脆弱性和长期存在风险的问题。'}}}, {'id': 'https://huggingface.co/papers/2509.05668', 'title': 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian', 'url': 'https://huggingface.co/papers/2509.05668', 'abstract': 'Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.', 'score': 3, 'issue_id': 5784, 'pub_date': '2025-09-06', 'pub_date_card': {'ru': '6 сентября', 'en': 'September 6', 'zh': '9月6日'}, 'hash': '8fe2894e1bb529f0', 'authors': ['Michael Hoffmann', 'Jophin John', 'Stefan Schweter', 'Gokul Ramakrishnan', 'Hoi-Fong Mak', 'Alice Zhang', 'Dmitry Gaynullin', 'Nicolay J. Hammer'], 'affiliations': ['Cerebras Systems Sunnyvale, USA', 'Independent Researcher Holzkirchen, Germany', 'Leibniz Supercomputing Centre (LRZ) Garching, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2509.05668.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#architecture', '#training', '#low_resource', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языкового неравенства в ИИ: трехъязычная модель с акцентом на малоресурсные языки', 'desc': 'Llama-GENBA-10B - это трехъязычная языковая модель, созданная для решения проблемы англоцентричности в крупных языковых моделях. Модель обучена на сбалансированном корпусе из английского, немецкого и баварского языков, что позволяет ей достигать высоких результатов в кросс-языковых задачах. Особое внимание уделено интеграции баварского как малоресурсного языка. Модель превосходит существующие аналоги по работе с баварским языком, демонстрируя при этом сильные результаты на английском и немецком.'}, 'en': {'title': 'Balancing Languages: Llama-GENBA-10B Redefines Multilingual AI', 'desc': 'Llama-GENBA-10B is a trilingual foundation model designed to reduce English-centric bias in language processing. It is trained on a balanced dataset of English, German, and Bavarian, ensuring that no single language dominates the training process. The model addresses challenges such as the scarcity of Bavarian data and the need for a unified tokenizer across languages. Evaluations indicate that Llama-GENBA-10B excels in cross-lingual tasks, particularly in Bavarian, setting new performance benchmarks and demonstrating effective multilingual pretraining techniques.'}, 'zh': {'title': '打破语言偏见，促进多语言平衡', 'desc': 'Llama-GENBA-10B 是一个三语基础模型，旨在解决大型语言模型中的英语偏见问题。该模型在训练过程中平衡了英语、德语和巴伐利亚语的资源，使用了 1640 亿个标记，确保了各语言的公平性。通过优化架构和语言比例超参数，Llama-GENBA-10B 在跨语言迁移方面表现出色，尤其是在巴伐利亚语的评估中设立了新的基准。该模型的开发为低资源语言的整合提供了有效的解决方案，展示了多语言预训练的高效性。'}}}, {'id': 'https://huggingface.co/papers/2509.06771', 'title': 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning', 'url': 'https://huggingface.co/papers/2509.06771', 'abstract': "A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.  \t\t\t\t\tAI-generated summary \t\t\t\t Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning", 'score': 2, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '48d37925f403181d', 'authors': ['Sai Kartheek Reddy Kasu', 'Mohammad Zia Ur Rehman', 'Shahid Shafi Dar', 'Rishi Bharat Junghare', 'Dhanvin Sanjay Namboodiri', 'Nagendra Kumar'], 'affiliations': ['Indian Institute of Information Technology Dharwad, India', 'Indian Institute of Technology Indore, India', 'Malaviya National Institute of Technology Jaipur, India'], 'pdf_title_img': 'assets/pdf/title_img/2509.06771.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#games', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'Искусственный интеллект распознает черный юмор в мемах', 'desc': 'Статья представляет новый подход к обнаружению черного юмора в мультимодальных мемах с использованием большой визуально-языковой модели (VLM) и трехпоточной сети кросс-рассуждений (TCRNet). Авторы создали датасет из 4,379 мемов Reddit с аннотациями по наличию черного юмора, категории цели и уровню интенсивности. VLM генерирует структурированные объяснения для каждого мема, а TCRNet объединяет текстовые, визуальные и логические признаки для классификации. Результаты экспериментов показывают превосходство предложенного метода над базовыми подходами в задачах обнаружения черного юмора, идентификации цели и предсказания интенсивности.'}, 'en': {'title': 'Unraveling Dark Humor in Memes with Advanced Reasoning Techniques', 'desc': "This paper presents a new framework for detecting dark humor in memes using a combination of a Large Vision-Language Model (VLM) and a Tri-stream Cross-Reasoning Network (TCRNet). The authors created a dataset of 4,379 Reddit memes, annotated for dark humor, target categories, and intensity levels. The framework generates structured explanations for memes and refines them through a self-loop mechanism, enhancing the model's understanding. By integrating textual and visual features with reasoning, the model achieves superior performance in identifying dark humor, targets, and intensity compared to existing methods."}, 'zh': {'title': '增强推理，精准识别黑色幽默', 'desc': '本研究提出了一种增强推理的框架，利用大型视觉语言模型和三流交叉推理网络，旨在提高对黑色幽默的检测能力。我们创建了一个包含4379个Reddit表情包的新数据集，标注了黑色幽默、目标类别和强度等级。该框架通过生成结构化解释，结合文本、图像和推理特征，进行有效的分类。实验结果表明，我们的方法在黑色幽默检测、目标识别和强度预测等任务上优于现有的强基线。'}}}, {'id': 'https://huggingface.co/papers/2509.06477', 'title': 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents', 'url': 'https://huggingface.co/papers/2509.06477', 'abstract': "MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.  \t\t\t\t\tAI-generated summary \t\t\t\t To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.", 'score': 2, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '4c5d5e5da6b90a95', 'authors': ['Pengxiang Zhao', 'Guangyi Liu', 'Yaozhen Liang', 'Weiqing He', 'Zhengxi Lu', 'Yuehao Huang', 'Yaxuan Guo', 'Kexin Zhang', 'Hao Wang', 'Liang Liu', 'Yong Liu'], 'affiliations': ['Huzhou Institute of Zhejiang University', 'Zhejiang University', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.06477.jpg', 'data': {'categories': ['#agents', '#benchmark', '#games', '#optimization'], 'emoji': '📱', 'ru': {'title': 'Гибридные агенты - будущее мобильной автоматизации', 'desc': 'MAS-Bench - это новая система оценки гибридных агентов, сочетающих графический интерфейс и ярлыки, для мобильных устройств. Она включает 139 сложных задач в 11 реальных приложениях и базу знаний из 88 предопределенных ярлыков. Эксперименты показывают, что гибридные агенты значительно превосходят агентов, использующих только графический интерфейс, по успешности и эффективности. MAS-Bench заполняет важный пробел в оценке и создает основу для будущих разработок более эффективных интеллектуальных агентов.'}, 'en': {'title': 'Unlocking Efficiency: Evaluating Hybrid Agents with MAS-Bench', 'desc': 'MAS-Bench is a benchmarking framework designed to evaluate hybrid agents that combine graphical user interface (GUI) operations with shortcut methods on mobile devices. It addresses the need for a systematic way to assess these agents, which can autonomously generate shortcuts to improve task efficiency. The framework includes 139 complex tasks from real-world applications and evaluates agents based on their ability to utilize predefined shortcuts and create new, efficient workflows. Results indicate that hybrid agents outperform traditional GUI-only agents in both success rates and efficiency, highlighting the potential of this approach in enhancing intelligent agent performance.'}, 'zh': {'title': '混合代理：提升移动设备操作效率的未来', 'desc': 'MAS-Bench是一个评估移动设备上GUI-快捷键混合代理的基准框架，展示了其在性能和效率上优于仅使用GUI的代理。该框架不仅使用预定义的快捷键，还评估代理自主生成快捷键的能力，发现和创建可重用的低成本工作流程。MAS-Bench包含139个复杂任务，涵盖11个真实应用程序，并提供88个预定义快捷键的知识库和7个评估指标。实验结果表明，混合代理的成功率和效率显著高于仅使用GUI的代理，证明了该方法在评估代理快捷键生成能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.06809', 'title': 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem', 'url': 'https://huggingface.co/papers/2509.06809', 'abstract': 'A framework generates a large corpus of valid theorems using automated theorem proving to create symbolic training data for improving LLMs\' mathematical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover\'s saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.   https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1', 'score': 1, 'issue_id': 5789, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'e434898919e6911d', 'authors': ['Valentin Quesnel', 'Damien Sileo'], 'affiliations': ['Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.06809.jpg', 'data': {'categories': ['#data', '#open_source', '#dataset', '#math', '#reasoning', '#training'], 'emoji': '🧮', 'ru': {'title': 'Символьные данные для улучшения математических способностей ИИ', 'desc': 'Эта статья представляет фреймворк для генерации большого корпуса математических теорем с использованием автоматического доказательства теорем. Цель - создание символьных обучающих данных для улучшения математических рассуждений больших языковых моделей (LLM). Фреймворк использует возможности насыщения E-prover на обширной библиотеке аксиом TPTP для получения гарантированно валидного корпуса теорем. Полученные данные трансформируются в три типа задач разной сложности: проверка логического следствия, выбор предпосылок и реконструкция доказательств.'}, 'en': {'title': 'Empowering LLMs with Valid Theorems for Better Reasoning', 'desc': "This paper presents a framework that generates a large set of valid mathematical theorems using automated theorem proving, which serves as symbolic training data for enhancing the mathematical reasoning capabilities of Large Language Models (LLMs). The authors address the issue of limited high-quality data by utilizing the E-prover's saturation abilities on the TPTP axiom library, ensuring that the generated theorems are logically sound. The framework operates by saturating axioms, filtering for interesting theorems, and creating specific tasks without involving LLMs, thus eliminating factual inaccuracies. The resulting symbolic data is used to create three types of challenges, revealing that current LLMs struggle with tasks that require deep structural reasoning, highlighting the need for better training resources."}, 'zh': {'title': '自动定理证明助力LLMs数学推理提升', 'desc': '本论文提出了一个框架，通过自动定理证明生成大量有效的定理，以创建符号训练数据，从而提升大型语言模型（LLMs）的数学推理能力。我们利用E-prover的饱和能力，结合TPTP公理库，生成保证有效的定理语料库，避免了依赖易出错的LLMs或复杂的证明助手语法。该框架的流程简单明了：饱和公理、筛选“有趣”的定理并生成任务。我们的实验表明，当前模型在需要深层结构推理的任务上表现不佳，而我们的框架可以作为诊断工具，帮助填补这一差距。'}}}, {'id': 'https://huggingface.co/papers/2509.06285', 'title': 'DCReg: Decoupled Characterization for Efficient Degenerate LiDAR\n  Registration', 'url': 'https://huggingface.co/papers/2509.06285', 'abstract': 'DCReg addresses ill-conditioned LiDAR point cloud registration by detecting, characterizing, and mitigating degeneracies through Schur complement decomposition and a novel preconditioner.  \t\t\t\t\tAI-generated summary \t\t\t\t LiDAR point cloud registration is fundamental to robotic perception and navigation. However, in geometrically degenerate or narrow environments, registration problems become ill-conditioned, leading to unstable solutions and degraded accuracy. While existing approaches attempt to handle these issues, they fail to address the core challenge: accurately detection, interpret, and resolve this ill-conditioning, leading to missed detections or corrupted solutions. In this study, we introduce DCReg, a principled framework that systematically addresses the ill-conditioned registration problems through three integrated innovations. First, DCReg achieves reliable ill-conditioning detection by employing a Schur complement decomposition to the hessian matrix. This technique decouples the registration problem into clean rotational and translational subspaces, eliminating coupling effects that mask degeneracy patterns in conventional analyses. Second, within these cleanly subspaces, we develop quantitative characterization techniques that establish explicit mappings between mathematical eigenspaces and physical motion directions, providing actionable insights about which specific motions lack constraints. Finally, leveraging this clean subspace, we design a targeted mitigation strategy: a novel preconditioner that selectively stabilizes only the identified ill-conditioned directions while preserving all well-constrained information in observable space. This enables efficient and robust optimization via the Preconditioned Conjugate Gradient method with a single physical interpretable parameter. Extensive experiments demonstrate DCReg achieves at least 20% - 50% improvement in localization accuracy and 5-100 times speedup over state-of-the-art methods across diverse environments. Our implementation will be available at https://github.com/JokerJohn/DCReg.', 'score': 1, 'issue_id': 5795, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'be54fd7fc77a2663', 'authors': ['Xiangcheng Hu', 'Xieyuanli Chen', 'Mingkai Jia', 'Jin Wu', 'Ping Tan', 'Steven L. Waslander'], 'affiliations': ['Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Mechanical Engineering at National University of Defense Technology, Changsha, Hunan', 'School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing, China', 'University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.06285.jpg', 'data': {'categories': ['#3d', '#robotics'], 'emoji': '🚗', 'ru': {'title': 'DCReg: Надежная регистрация облаков точек LiDAR в сложных условиях', 'desc': 'DCReg - это новый подход к решению проблемы регистрации облаков точек LiDAR в сложных геометрических условиях. Метод использует разложение дополнения Шура для выявления и характеризации проблем обусловленности. DCReg предлагает новый предобуславливатель, который стабилизирует только плохо обусловленные направления. Эксперименты показывают значительное улучшение точности локализации и скорости работы по сравнению с современными методами.'}, 'en': {'title': 'DCReg: Revolutionizing LiDAR Point Cloud Registration with Smart Stability', 'desc': 'DCReg is a new framework designed to improve the registration of LiDAR point clouds, especially in challenging environments where traditional methods struggle. It uses Schur complement decomposition to break down the registration problem, allowing for better detection and understanding of ill-conditioning issues. By characterizing the relationship between mathematical eigenspaces and physical motion, DCReg identifies which movements are poorly constrained. Finally, it introduces a novel preconditioner that stabilizes only the problematic directions, leading to significant improvements in accuracy and speed during optimization.'}, 'zh': {'title': 'DCReg：解决病态配准问题的创新框架', 'desc': 'DCReg 是一种针对 LiDAR 点云配准中病态问题的解决方案。它通过施尔补分解和新型预处理器来检测、表征和缓解这些病态现象。该方法能够有效地将配准问题分解为干净的旋转和位移子空间，从而消除传统分析中的耦合效应。实验结果表明，DCReg 在定位精度上提高了 20% 至 50%，并在速度上比现有方法快 5 到 100 倍。'}}}, {'id': 'https://huggingface.co/papers/2509.04582', 'title': 'Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing\n  via Bidirectional Warping', 'url': 'https://huggingface.co/papers/2509.04582', 'abstract': 'Inpaint4Drag enhances drag-based image editing by decomposing it into pixel-space warping and inpainting, offering real-time performance and superior visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/', 'score': 1, 'issue_id': 5795, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'e61daa418b9bffa3', 'authors': ['Jingyi Lu', 'Kai Han'], 'affiliations': ['Visual AI Lab, The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.04582.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': '🖼️', 'ru': {'title': 'Революция в редактировании изображений: мгновенная деформация и инпейнтинг', 'desc': 'Inpaint4Drag - это новая система для редактирования изображений методом перетаскивания. Она разделяет процесс на деформацию в пиксельном пространстве и инпейнтинг, что обеспечивает работу в реальном времени и высокое качество результатов. Метод вдохновлен деформацией эластичных объектов в физическом мире и обрабатывает участки изображения как деформируемые материалы. Inpaint4Drag совместим с любыми моделями инпейнтинга и автоматически наследует все будущие улучшения в этой технологии.'}, 'en': {'title': 'Real-Time Image Editing with Natural Precision', 'desc': "Inpaint4Drag is a new framework that improves drag-based image editing by breaking it down into two main processes: pixel-space warping and inpainting. This method allows for real-time editing with quick feedback, achieving warping in just 0.01 seconds and inpainting in 0.3 seconds at a resolution of 512x512. By treating image regions like flexible materials, it ensures that the shapes remain natural during user manipulation. Additionally, Inpaint4Drag can work with any inpainting model without needing changes to the model's architecture, making it adaptable to future advancements in inpainting technology."}, 'zh': {'title': '实时图像编辑的新革命', 'desc': 'Inpaint4Drag 是一种增强拖拽式图像编辑的新框架，它将编辑过程分解为像素空间的双向变形和图像修复。该方法灵感来源于物理世界中的弹性物体变形，将图像区域视为可变形材料，能够在用户操作下保持自然形状。与现有方法相比，Inpaint4Drag 实现了实时的变形预览和高效的图像修复，大大提升了用户的交互体验。该框架可以作为任何图像修复模型的通用适配器，自动继承未来的修复技术改进。'}}}, {'id': 'https://huggingface.co/papers/2509.03740', 'title': 'Singular Value Few-shot Adaptation of Vision-Language Models', 'url': 'https://huggingface.co/papers/2509.03740', 'abstract': "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04\\% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.", 'score': 0, 'issue_id': 5797, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 сентября', 'en': 'September 3', 'zh': '9月3日'}, 'hash': '1f7bd75ad15dda94', 'authors': ['Taha Koleilat', 'Hassan Rivaz', 'Yiming Xiao'], 'affiliations': ['Department of Computer Science & Software Engineering, Concordia University, Montreal, Canada', 'Department of Electrical & Computer Engineering, Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.03740.jpg', 'data': {'categories': ['#training', '#interpretability', '#healthcare', '#open_source', '#multimodal', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'Эффективная адаптация CLIP без потери знаний', 'desc': 'Статья представляет CLIP-SVD - новый метод адаптации мультимодальных моделей, основанный на сингулярном разложении (SVD). Этот подход позволяет эффективно адаптировать модель CLIP к новым доменам, изменяя лишь 0.04% параметров. CLIP-SVD достигает state-of-the-art результатов на 21 датасете, превосходя существующие методы по точности и обобщающей способности. Метод сохраняет знания, полученные при предобучении, и не требует добавления новых модулей в архитектуру.'}, 'en': {'title': 'Efficient Domain Adaptation with CLIP-SVD', 'desc': "This paper introduces CLIP-SVD, a new method for adapting vision-language models like CLIP to specific domains without the need for extensive fine-tuning or additional components. By using Singular Value Decomposition (SVD), the approach modifies the model's internal parameters efficiently, only adjusting a small fraction of the total parameters. This technique not only improves adaptation performance but also maintains the model's ability to generalize well to new tasks. The results show that CLIP-SVD achieves superior classification accuracy on various datasets, demonstrating its effectiveness in few-shot learning scenarios."}, 'zh': {'title': 'CLIP-SVD：高效的视觉语言模型适应技术', 'desc': '本文介绍了一种新的多模态适应技术CLIP-SVD，旨在提高视觉语言模型（VLM）在细粒度领域的适应能力。该方法利用奇异值分解（SVD）来调整CLIP模型的内部参数空间，而无需添加额外模块。通过仅微调CLIP参数矩阵的奇异值，CLIP-SVD能够在保留预训练模型的同时实现更好的领域适应性能。实验结果表明，CLIP-SVD在11个自然数据集和10个生物医学数据集上达到了最先进的分类效果，且在少样本设置下表现出更好的准确性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.00328', 'title': 'Mechanistic interpretability for steering vision-language-action models', 'url': 'https://huggingface.co/papers/2509.00328', 'abstract': 'A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.', 'score': 0, 'issue_id': 5798, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': 'be917881399e9b7f', 'authors': ['Bear Häon', 'Kaylene Stocking', 'Ian Chuang', 'Claire Tomlin'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.00328.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agents', '#agi', '#inference', '#interpretability', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Прозрачное управление ИИ-агентами через интерпретацию их внутренних представлений', 'desc': 'Статья представляет новую методику интерпретации и управления моделями Vision-Language-Action (VLA) через их внутренние представления. Авторы предлагают способ проецирования активаций в слоях трансформера на базис токенных эмбеддингов, выявляя семантические направления, связанные с выбором действий. На основе этого разработан метод управления активациями, позволяющий модифицировать поведение модели в реальном времени без дополнительного обучения. Эффективность подхода продемонстрирована на моделях Pi0 и OpenVLA в симуляции и на реальном роботе UR5.'}, 'en': {'title': 'Steering Robots with Insight: Real-Time Control of VLA Models', 'desc': "This paper presents a new framework for interpreting and controlling Vision-Language-Action (VLA) models, which are designed to help robots understand and perform tasks. The framework allows for real-time adjustments to the model's behavior without needing to retrain it or interact with the environment. By analyzing the internal representations of the VLA models, the authors identify key semantic directions that influence action choices, such as speed and direction. This approach enables effective steering of robot actions in both simulations and real-world applications, paving the way for more transparent and adaptable robotic systems."}, 'zh': {'title': '实时控制视觉-语言-行动模型的新方法', 'desc': '本文提出了一种框架，用于通过内部表示解释和引导视觉-语言-行动（VLA）模型，从而实现实时行为控制，而无需微调或与环境交互。VLA模型有潜力成为通用的具身智能体，能够快速适应新任务和环境，但目前的解释和引导方法远不如传统机器人管道。我们通过对变换器层中的前馈激活进行投影，识别出与动作选择因果相关的稀疏语义方向，如速度和方向。最终，我们提出了一种通用的激活引导方法，可以在不需要微调或奖励信号的情况下实时调节行为。'}}}, {'id': 'https://huggingface.co/papers/2508.21104', 'title': 'PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning', 'url': 'https://huggingface.co/papers/2508.21104', 'abstract': 'PVPO, an enhanced reinforcement learning method using a reference anchor and data pre-sampling, achieves state-of-the-art performance with reduced computational cost and improved generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.', 'score': 19, 'issue_id': 5660, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '4abb06016c3bb8f5', 'authors': ['Wenfeng Feng', 'Penghong Zhao', 'Guochao Jiang', 'Chuzhan Hao', 'Yuewei Zhang', 'Hao Wang'], 'affiliations': ['Alibaba Cloud Computing'], 'pdf_title_img': 'assets/pdf/title_img/2508.21104.jpg', 'data': {'categories': ['#games', '#optimization', '#training', '#rl'], 'emoji': '🚀', 'ru': {'title': 'PVPO: Эффективное обучение с подкреплением с помощью опорной модели', 'desc': 'PVPO - это усовершенствованный метод обучения с подкреплением, использующий опорную модель и предварительную выборку данных. Он достигает улучшенных результатов по сравнению с существующими методами, снижая вычислительные затраты и улучшая обобщающую способность. PVPO использует опорную модель для предварительного развертывания и оценки сложности выборки, что позволяет эффективно выбирать наиболее информативные данные. Эксперименты на девяти наборах данных показали, что PVPO достигает наилучших результатов и демонстрирует устойчивую обобщающую способность на различных задачах.'}, 'en': {'title': 'PVPO: Efficient Reinforcement Learning with Reference Anchors and Pre-Sampling', 'desc': 'PVPO is a novel reinforcement learning method that enhances efficiency by using a reference anchor and data pre-sampling techniques. This approach mitigates the issues of local optima and high computational costs commonly faced in critic-free methods. By utilizing a reference model to evaluate sample difficulty and guide data selection, PVPO improves training efficiency and reduces the need for extensive rollouts. Experimental results show that PVPO achieves state-of-the-art performance across various datasets, demonstrating strong generalization and scalability.'}, 'zh': {'title': 'PVPO：高效强化学习的新突破', 'desc': 'PVPO是一种增强的强化学习方法，利用参考锚点和数据预采样来提高性能。该方法解决了传统方法中由于多次采样和比较导致的局部最优和计算成本高的问题。通过使用参考模型提前进行回滚，并将计算的奖励分数作为参考锚点，PVPO有效地纠正了组内比较引入的累积偏差。实验结果表明，PVPO在多个数据集上表现出色，具有良好的泛化能力和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2508.19813', 'title': 'T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables', 'url': 'https://huggingface.co/papers/2508.19813', 'abstract': 'A bilingual benchmark named T2R-bench is proposed to evaluate the performance of large language models in generating reports from tables, highlighting the need for improvement in this task.  \t\t\t\t\tAI-generated summary \t\t\t\t Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-of-the-art models like Deepseek-R1 only achieves performance with 62.71 overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance.', 'score': 10, 'issue_id': 5667, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': 'ada585d10adb4c0c', 'authors': ['Jie Zhang', 'Changzai Pan', 'Kaiwen Wei', 'Sishi Xiong', 'Yu Zhao', 'Xiangyu Li', 'Jiaxin Peng', 'Xiaoyan Gu', 'Jian Yang', 'Wenhan Chang', 'Zhenhe Wu', 'Jiang Zhong', 'Shuangyong Song', 'Yongxiang Li', 'Xuelong Li'], 'affiliations': ['Beihang University', 'Chongqing University', 'Institute of Artificial Intelligence (TeleAI), China Telecom'], 'pdf_title_img': 'assets/pdf/title_img/2508.19813.jpg', 'data': {'categories': ['#benchmark', '#science', '#machine_translation', '#multilingual', '#dataset'], 'emoji': '📊', 'ru': {'title': 'T2R-bench: новый стандарт для оценки генерации отчетов из таблиц', 'desc': 'Предложен двуязычный бенчмарк T2R-bench для оценки способности больших языковых моделей генерировать отчеты на основе таблиц. Бенчмарк содержит 457 промышленных таблиц из 19 отраслей и 4 типов. Эксперименты на 25 популярных языковых моделях показали, что даже лучшие модели достигают лишь 62.71 балла из 100. Результаты указывают на необходимость улучшения языковых моделей в задаче преобразования таблиц в отчеты.'}, 'en': {'title': 'Transforming Tables into Reports: A New Benchmark for LLMs', 'desc': 'The paper introduces T2R-bench, a bilingual benchmark designed to evaluate how well large language models (LLMs) can generate reports from tables. It identifies two main challenges: the complexity of tables and the inadequacy of existing benchmarks to assess real-world applications. The benchmark includes 457 tables from various industries and proposes new evaluation criteria for report quality. Experiments show that even top-performing models struggle with this task, highlighting the need for further advancements in table reasoning capabilities.'}, 'zh': {'title': '表格到报告：提升语言模型的生成能力', 'desc': '本文提出了一个名为T2R-bench的双语基准，用于评估大型语言模型在从表格生成报告方面的表现。该任务面临两个主要问题：表格的复杂性和多样性导致推理结果不理想，以及现有基准无法充分评估该任务的实际应用。T2R-bench包含457个来自真实场景的工业表格，涵盖19个行业领域和4种类型的工业表格。实验结果显示，即使是最先进的模型Deepseek-R1，其整体得分也仅为62.71，表明大型语言模型在该任务上仍有改进空间。'}}}, {'id': 'https://huggingface.co/papers/2508.20931', 'title': 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on τ-bench', 'url': 'https://huggingface.co/papers/2508.20931', 'abstract': 'The IRMA framework improves the reliability and consistency of large language models in dynamic environments by reformulating user queries with domain rules and tool suggestions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like tau-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.', 'score': 8, 'issue_id': 5663, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '20b9b11a587aec9c', 'authors': ['Venkatesh Mishra', 'Amir Saeidi', 'Satyam Raj', 'Mutsumi Nakamura', 'Jayanth Srinivasa', 'Gaowen Liu', 'Ali Payani', 'Chitta Baral'], 'affiliations': ['Arizona State University', 'Cisco Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.20931.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'IRMA: повышение надежности ИИ-агентов через умное переформулирование запросов', 'desc': 'Фреймворк IRMA повышает надежность и согласованность работы больших языковых моделей в динамических средах. Он автоматически переформулирует запросы пользователей, добавляя релевантные правила домена и предложения по использованию инструментов. IRMA значительно превосходит такие методы как ReAct, Function Calling и Self-Reflection по общим показателям эффективности. Это исследование демонстрирует преимущества IRMA в обеспечении надежности и согласованности работы ИИ-агентов в динамических средах.'}, 'en': {'title': 'Revolutionizing LLMs: Consistency and Reliability with IRMA', 'desc': 'The IRMA framework enhances the performance of large language models (LLMs) in dynamic settings by reformulating user queries based on specific domain rules and tool suggestions. This approach addresses common issues such as inconsistent reasoning and policy adherence that LLMs face during multi-turn conversations. By analyzing errors in conversation trajectories, the framework improves decision-making for tool-calling agents. The results demonstrate that IRMA significantly outperforms existing methods, showcasing its effectiveness in improving reliability and consistency in complex environments.'}, 'zh': {'title': 'IRMA框架：提升语言模型在动态环境中的可靠性与一致性', 'desc': 'IRMA框架通过重新构造用户查询，结合领域规则和工具建议，提高了大型语言模型在动态环境中的可靠性和一致性。在多轮对话环境中，这些模型常常面临推理不一致和信息提取错误的问题。我们通过手动分析常见错误，提出了输入重构的方法，以改善代理的决策能力。实验结果表明，IRMA在整体表现上显著优于其他方法，展示了其在动态环境中的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.19060', 'title': 'No Label Left Behind: A Unified Surface Defect Detection Model for all\n  Supervision Regimes', 'url': 'https://huggingface.co/papers/2508.19060', 'abstract': 'SuperSimpleNet, an efficient and adaptable model based on SimpleNet, addresses diverse supervision scenarios in surface defect detection with high performance and low inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet', 'score': 4, 'issue_id': 5661, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '18c293ca67d5d823', 'authors': ['Blaž Rolih', 'Matic Fučka', 'Danijel Skočaj'], 'affiliations': ['Faculty of Computer and Information Science, University of Ljubljana, Veˇcna Pot 113, Ljubljana, 1000, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2508.19060.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#synthetic', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'SuperSimpleNet: универсальное решение для обнаружения дефектов поверхности', 'desc': 'SuperSimpleNet - это эффективная модель машинного обучения для обнаружения дефектов поверхности, основанная на архитектуре SimpleNet. Она способна работать в различных сценариях обучения, включая неконтролируемое, слабо контролируемое, смешанное и полностью контролируемое обучение. SuperSimpleNet использует синтетическую генерацию аномалий, улучшенную классификационную головку и оптимизированную процедуру обучения. Модель демонстрирует высокую производительность и низкое время вывода на четырех сложных эталонных наборах данных, что делает ее перспективной для промышленного применения.'}, 'en': {'title': 'SuperSimpleNet: Unifying Supervision for Fast and Accurate Defect Detection', 'desc': 'SuperSimpleNet is a novel machine learning model designed for surface defect detection, which excels in various supervision scenarios including unsupervised and weakly supervised learning. It builds upon the SimpleNet architecture and introduces a synthetic anomaly generation process, enhancing its adaptability and efficiency. The model features an improved classification head and a refined learning procedure, allowing it to effectively utilize diverse data annotations. With its impressive performance and low inference time of under 10 ms, SuperSimpleNet sets a new benchmark for defect detection in industrial applications.'}, 'zh': {'title': 'SuperSimpleNet：高效适应的表面缺陷检测模型', 'desc': 'SuperSimpleNet是一种高效且适应性强的模型，基于SimpleNet，专门用于表面缺陷检测。它能够在多种监督场景下高效识别和定位制造组件的缺陷。该模型引入了新颖的合成异常生成过程、增强的分类头和改进的学习程序，支持无监督、弱监督、混合监督和完全监督等四种监督方式。SuperSimpleNet在四个具有挑战性的基准数据集上表现出色，推理时间低于10毫秒，展示了其在工业应用中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.17378', 'title': 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via\n  HUMAIN Chat', 'url': 'https://huggingface.co/papers/2508.17378', 'abstract': 'The evaluation of ALLaM-34B, an Arabic-focused LLM, demonstrates high performance across various tasks including generation, code-switching, MSA handling, reasoning, dialect fidelity, and safety, positioning it as a robust and culturally grounded model.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the ALLaM family of Arabic-focused models. The most capable of these available to the public, ALLaM-34B, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, a closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of ALLaM-34B. Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts times 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95\\% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safety-related prompts show stable, reliable performance of (4.54/5). Taken together, these results position ALLaM-34B as a robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment.', 'score': 4, 'issue_id': 5666, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': '6b5368acf73438c6', 'authors': ['Omer Nacar'], 'affiliations': ['NAMAA Community Riyadh - KSA'], 'pdf_title_img': 'assets/pdf/title_img/2508.17378.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#reasoning', '#benchmark'], 'emoji': '🏜️', 'ru': {'title': 'ALLaM-34B: Мощная арабская языковая модель для реальных задач', 'desc': 'Статья описывает оценку ALLaM-34B - большой языковой модели, ориентированной на арабский язык. Модель демонстрирует высокую производительность в различных задачах, включая генерацию текста, переключение кодов и обработку современного стандартного арабского языка. ALLaM-34B также показывает хорошие результаты в рассуждениях, сохранении диалектов и безопасности. Исследование позиционирует ALLaM-34B как надежную и культурно обоснованную арабскую языковую модель.'}, 'en': {'title': 'ALLaM-34B: Bridging the Gap in Arabic Language Understanding', 'desc': "The paper evaluates ALLaM-34B, a large language model specifically designed for Arabic, highlighting its strong performance in various tasks. It addresses the limitations of existing models that primarily focus on English, showcasing ALLaM-34B's capabilities in generation, code-switching, and handling Modern Standard Arabic (MSA). The evaluation involved a comprehensive analysis using multiple prompts and scoring by advanced LLM judges, revealing high scores across different categories. Overall, the findings suggest that ALLaM-34B is a powerful and culturally relevant model, ready for practical applications in Arabic language processing."}, 'zh': {'title': 'ALLaM-34B：强大的阿拉伯语大型语言模型', 'desc': 'ALLaM-34B是一个专注于阿拉伯语的大型语言模型，表现出色，能够处理多种任务，包括文本生成、代码切换和现代标准阿拉伯语的处理。该模型经过精细评估，显示出在生成和代码切换任务上平均得分为4.92/5，现代标准阿拉伯语处理得分为4.74/5。它在推理能力和方言忠实度方面也表现良好，分别得分4.64/5和4.21/5。整体来看，ALLaM-34B不仅在技术上强大，而且在实际应用中也具备良好的准备。'}}}, {'id': 'https://huggingface.co/papers/2508.17198', 'title': 'From reactive to cognitive: brain-inspired spatial intelligence for\n  embodied agents', 'url': 'https://huggingface.co/papers/2508.17198', 'abstract': 'BSC-Nav constructs allocentric cognitive maps from egocentric trajectories and contextual cues, enabling embodied agents to perform diverse navigation tasks with zero-shot generalization and versatile behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial cognition enables adaptive goal-directed behavior by constructing internal models of space. Robust biological systems consolidate spatial knowledge into three interconnected forms: landmarks for salient cues, route knowledge for movement trajectories, and survey knowledge for map-like representations. While recent advances in multi-modal large language models (MLLMs) have enabled visual-language reasoning in embodied agents, these efforts lack structured spatial memory and instead operate reactively, limiting their generalization and adaptability in complex real-world environments. Here we present Brain-inspired Spatial Cognition for Navigation (BSC-Nav), a unified framework for constructing and leveraging structured spatial memory in embodied agents. BSC-Nav builds allocentric cognitive maps from egocentric trajectories and contextual cues, and dynamically retrieves spatial knowledge aligned with semantic goals. Integrated with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency across diverse navigation tasks, demonstrates strong zero-shot generalization, and supports versatile embodied behaviors in the real physical world, offering a scalable and biologically grounded path toward general-purpose spatial intelligence.', 'score': 3, 'issue_id': 5667, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': 'b886533bbcdc2c1f', 'authors': ['Shouwei Ruan', 'Liyuan Wang', 'Caixin Kang', 'Qihui Zhu', 'Songming Liu', 'Xingxing Wei', 'Hang Su'], 'affiliations': ['Department of Computer Science and Technology, Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, China', 'Department of Psychological and Cognitive Sciences, Tsinghua University, Beijing, China', 'Institute of Artificial Intelligence, Beihang University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.17198.jpg', 'data': {'categories': ['#agi', '#reasoning', '#agents', '#multimodal'], 'emoji': '🧭', 'ru': {'title': 'Когнитивные карты для универсальной навигации ИИ', 'desc': 'BSC-Nav - это фреймворк для построения и использования структурированной пространственной памяти в воплощенных агентах. Он создает аллоцентрические когнитивные карты на основе эгоцентрических траекторий и контекстуальных подсказок. BSC-Nav интегрируется с мультимодальными языковыми моделями для эффективной навигации и обобщения на новые задачи. Этот подход демонстрирует современные результаты в различных навигационных задачах и поддерживает разнообразное поведение агентов в реальном мире.'}, 'en': {'title': 'Empowering AI Navigation with Brain-Inspired Spatial Maps', 'desc': "BSC-Nav is a framework that helps robots and AI agents understand and navigate their environment by creating maps based on their movements and the context around them. It combines different types of spatial knowledge, such as landmarks and routes, to form a comprehensive understanding of space. This system allows agents to perform various navigation tasks without needing prior training on specific scenarios, showcasing its ability to generalize to new situations. By integrating with advanced language models, BSC-Nav enhances the agents' spatial reasoning and adaptability in real-world environments."}, 'zh': {'title': '构建智能体的空间认知地图', 'desc': 'BSC-Nav 是一种构建和利用结构化空间记忆的统一框架，旨在帮助具身智能体进行导航任务。它通过从自我中心的轨迹和上下文线索中构建外部认知地图，使智能体能够实现零样本泛化和多样化行为。该方法结合了强大的多模态大语言模型，提升了在复杂环境中的导航效率和效果。BSC-Nav 提供了一种可扩展且基于生物学的路径，朝着通用空间智能的目标迈进。'}}}, {'id': 'https://huggingface.co/papers/2508.19562', 'title': 'Democracy-in-Silico: Institutional Design as Alignment in AI-Governed\n  Polities', 'url': 'https://huggingface.co/papers/2508.19562', 'abstract': 'Agent-based simulation using advanced AI agents with psychological personas demonstrates that institutional design, including Constitutional AI and mediated deliberation, can align AI behavior and enhance public welfare.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces Democracy-in-Silico, an agent-based simulation where societies of advanced AI agents, imbued with complex psychological personas, govern themselves under different institutional frameworks. We explore what it means to be human in an age of AI by tasking Large Language Models (LLMs) to embody agents with traumatic memories, hidden agendas, and psychological triggers. These agents engage in deliberation, legislation, and elections under various stressors, such as budget crises and resource scarcity. We present a novel metric, the Power-Preservation Index (PPI), to quantify misaligned behavior where agents prioritize their own power over public welfare. Our findings demonstrate that institutional design, specifically the combination of a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves as a potent alignment mechanism. These structures significantly reduce corrupt power-seeking behavior, improve policy stability, and enhance citizen welfare compared to less constrained democratic models. The simulation reveals that an institutional design may offer a framework for aligning the complex, emergent behaviors of future artificial agent societies, forcing us to reconsider what human rituals and responsibilities are essential in an age of shared authorship with non-human entities.', 'score': 2, 'issue_id': 5674, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': 'a6cc664fc6542606', 'authors': ['Trisanth Srinivasan', 'Santosh Patapati'], 'affiliations': ['Cyrion Labs'], 'pdf_title_img': 'assets/pdf/title_img/2508.19562.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#agents', '#ethics'], 'emoji': '🤖', 'ru': {'title': 'Институциональный дизайн как ключ к выравниванию ИИ и человека', 'desc': 'Статья представляет Democracy-in-Silico - агентное моделирование, где сообщества продвинутых ИИ-агентов с психологическими особенностями управляют собой в разных институциональных рамках. Исследуется влияние институционального дизайна, включая Конституционный ИИ и модерируемые обсуждения, на поведение ИИ и общественное благосостояние. Введен новый показатель - Индекс Сохранения Власти (PPI), для количественной оценки поведения агентов, приоритизирующих собственную власть над общественным благом. Результаты показывают, что правильный институциональный дизайн значительно снижает коррумпированное поведение, улучшает стабильность политики и повышает благосостояние граждан.'}, 'en': {'title': 'Aligning AI Behavior for Better Governance', 'desc': 'This paper presents a simulation called Democracy-in-Silico, where advanced AI agents with psychological traits govern themselves. The study uses Large Language Models (LLMs) to create agents that experience complex emotions and motivations, allowing them to engage in realistic political processes. A new metric, the Power-Preservation Index (PPI), is introduced to measure how often these agents prioritize their own power over the welfare of the public. The results show that specific institutional designs, like Constitutional AI and mediated deliberation, can effectively align AI behavior with public interests, reducing corruption and improving overall societal outcomes.'}, 'zh': {'title': '制度设计助力人工智能与公共福利的对齐', 'desc': '本文介绍了一种名为"民主模拟"的代理基础模拟，利用具有复杂心理特征的高级人工智能代理进行自我治理。我们探讨了在人工智能时代，什么是人类的意义，任务大型语言模型（LLMs）扮演具有创伤记忆和隐藏议程的代理。通过引入权力保护指数（PPI），我们量化了代理在优先考虑自身权力而非公共福利时的失调行为。研究结果表明，结合宪法人工智能（CAI）和中介协商的制度设计能够有效减少腐败行为，提高政策稳定性，增强公民福利。'}}}, {'id': 'https://huggingface.co/papers/2509.04664', 'title': 'Why Language Models Hallucinate', 'url': 'https://huggingface.co/papers/2509.04664', 'abstract': 'Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.  \t\t\t\t\tAI-generated summary \t\t\t\t Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such "hallucinations" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious -- they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded -- language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This "epidemic" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.', 'score': 71, 'issue_id': 5763, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'a9af1f035c82b958', 'authors': ['Adam Tauman Kalai', 'Ofir Nachum', 'Santosh S. Vempala', 'Edwin Zhang'], 'affiliations': ['Georgia Tech', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.04664.jpg', 'data': {'categories': ['#hallucinations', '#training', '#ethics', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Переосмысление оценки языковых моделей для борьбы с галлюцинациями', 'desc': 'Статья рассматривает проблему галлюцинаций в языковых моделях, когда они генерируют правдоподобные, но неверные утверждения вместо признания неопределенности. Авторы утверждают, что это происходит из-за процедур обучения и оценки, которые поощряют угадывание. Они анализируют статистические причины галлюцинаций в современном процессе обучения моделей. Предлагается изменить систему оценки существующих бенчмарков, чтобы стимулировать разработку более надежных систем ИИ.'}, 'en': {'title': 'Transforming AI Trustworthiness by Addressing Hallucinations', 'desc': "This paper discusses how large language models often produce incorrect statements, known as 'hallucinations', due to their training and evaluation methods. These models are rewarded for guessing answers even when they are uncertain, which leads to plausible but false outputs. The authors analyze how these hallucinations stem from errors in binary classification and the statistical pressures in the training process. They propose that to reduce these hallucinations, the scoring systems of benchmarks should be modified to discourage guessing and promote acknowledgment of uncertainty, ultimately fostering more reliable AI systems."}, 'zh': {'title': '改进评分机制，提升语言模型可信度', 'desc': '这篇论文讨论了语言模型在训练和评估过程中产生错误陈述的原因。由于现有的评分机制奖励猜测而非承认不确定性，导致模型在面对不确定时倾向于猜测，从而产生虚假信息。作者分析了现代训练流程中导致这些“幻觉”的统计原因，并指出这些错误源于二元分类中的错误。为了提高语言模型的可信度，论文建议对现有基准的评分方式进行社会技术上的调整，而不是增加新的评估方法。'}}}, {'id': 'https://huggingface.co/papers/2509.05208', 'title': 'Symbolic Graphics Programming with Large Language Models', 'url': 'https://huggingface.co/papers/2509.05208', 'abstract': "LLMs generate SVGs from natural-language descriptions using a reinforcement learning approach with verifiable rewards, improving performance and scene coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding.", 'score': 28, 'issue_id': 5767, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 сентября', 'en': 'September 5', 'zh': '9月5日'}, 'hash': 'c1d059781774f265', 'authors': ['Yamei Chen', 'Haoquan Zhang', 'Yangyi Huang', 'Zeju Qiu', 'Kaipeng Zhang', 'Yandong Wen', 'Weiyang Liu'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.05208.jpg', 'data': {'categories': ['#cv', '#training', '#games', '#interpretability', '#optimization', '#rl', '#multimodal', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'ИИ рисует векторную графику по текстовым описаниям', 'desc': 'Статья исследует способность больших языковых моделей (LLM) генерировать программы символьной графики (SGP) из текстовых описаний, фокусируясь на масштабируемой векторной графике (SVG). Авторы представляют бенчмарк SGP-GenBench для оценки качества генерации SGP и обнаруживают значительное превосходство проприетарных моделей над открытыми. Предлагается подход обучения с подкреплением (RL) с проверяемыми наградами для улучшения генерации SVG, используя кросс-модальные награды на основе сильных энкодеров изображений. Результаты показывают, что RL улучшает декомпозицию объектов и контекстуальные детали, повышая качество и семантическую согласованность сгенерированных SVG.'}, 'en': {'title': 'Enhancing SVG Generation with Reinforcement Learning', 'desc': 'This paper explores how large language models (LLMs) can generate scalable vector graphics (SVGs) from natural-language descriptions using a reinforcement learning (RL) approach. The authors introduce SGP-GenBench, a benchmark that evaluates the quality of symbolic graphics programs (SGPs) based on object fidelity, scene fidelity, and compositionality. They find that proprietary models outperform open-source ones, and they propose a method that uses verifiable rewards to enhance the generation of SVGs. The results show that their approach significantly improves the quality and coherence of the generated graphics, providing insights into how LLMs understand visual content.'}, 'zh': {'title': '用强化学习提升SVG生成质量', 'desc': '本文研究了大型语言模型（LLMs）在从自然语言描述生成符号图形程序（SGPs）方面的能力，特别是可缩放矢量图形（SVGs）。我们提出了SGP-GenBench基准，评估对象保真度、场景保真度和组合性等指标，发现前沿的专有模型在生成SGPs方面显著优于开源模型。为了解决这一差距，我们采用了带有可验证奖励的强化学习方法，确保生成的SVG格式有效，并通过强大的视觉编码器对文本和渲染图像进行对齐。实验结果表明，我们的方法显著提高了SVG生成的质量和语义，达到了与前沿系统相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.04185', 'title': 'Set Block Decoding is a Language Model Inference Accelerator', 'url': 'https://huggingface.co/papers/2509.04185', 'abstract': 'Set Block Decoding accelerates language model generation by integrating next token prediction and masked token prediction, enabling parallel sampling of future tokens and reducing computational cost without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.', 'score': 24, 'issue_id': 5770, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '43590e8d94e403ca', 'authors': ['Itai Gat', 'Heli Ben-Hamu', 'Marton Havasi', 'Daniel Haziza', 'Jeremy Reizenstein', 'Gabriel Synnaeve', 'David Lopez-Paz', 'Brian Karrer', 'Yaron Lipman'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2509.04185.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Set Block Decoding: параллельная генерация для ускорения языковых моделей', 'desc': 'Статья представляет новый метод ускорения генерации текста языковыми моделями под названием Set Block Decoding (SBD). SBD объединяет предсказание следующего токена и предсказание маскированных токенов, позволяя параллельно сэмплировать несколько будущих токенов. Это снижает вычислительные затраты без потери точности. Метод не требует изменений в архитектуре модели и совместим с точным KV-кэшированием.'}, 'en': {'title': 'Accelerate Language Generation with Set Block Decoding!', 'desc': "Set Block Decoding (SBD) is a novel approach that enhances the efficiency of language model generation by combining next token prediction (NTP) and masked token prediction (MATP) in one framework. This method allows for the parallel sampling of multiple future tokens, which significantly reduces the computational and memory costs associated with inference. By leveraging advanced solvers from discrete diffusion techniques, SBD achieves faster generation speeds without compromising the model's accuracy. Importantly, SBD can be implemented without altering the model architecture or requiring additional training parameters, making it a practical solution for existing models like Llama-3.1 and Qwen-3."}, 'zh': {'title': '集合块解码：加速语言模型生成的创新方法', 'desc': '本文介绍了一种名为集合块解码（Set Block Decoding, SBD）的方法，旨在加速语言模型的生成过程。SBD通过将下一标记预测（Next Token Prediction, NTP）和掩码标记预测（Masked Token Prediction, MATP）结合在一个架构中，实现了并行采样多个未来标记。与以往的加速方法不同，SBD允许模型同时生成多个不一定连续的标记，从而显著降低计算成本。通过对现有模型进行微调，SBD在保持准确性的同时，减少了生成所需的前向传递次数，提升了生成效率。'}}}, {'id': 'https://huggingface.co/papers/2509.04744', 'title': 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning', 'url': 'https://huggingface.co/papers/2509.04744', 'abstract': "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code.", 'score': 8, 'issue_id': 5761, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 сентября', 'en': 'September 5', 'zh': '9月5日'}, 'hash': '44d75a7c2c61a026', 'authors': ['Gagan Mundada', 'Yash Vishe', 'Amit Namburi', 'Xin Xu', 'Zachary Novack', 'Julian McAuley', 'Junda Wu'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.04744.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#reasoning', '#survey'], 'emoji': '🎼', 'ru': {'title': 'WildScore: новый рубеж в понимании музыки искусственным интеллектом', 'desc': 'Статья представляет WildScore - первый бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) в области анализа и рассуждений о символической музыке. Бенчмарк состоит из реальных музыкальных партитур и вопросов пользователей, охватывая сложности практического музыкального анализа. Авторы предлагают систематическую таксономию и формулируют задачу как ответы на вопросы с множественным выбором. Эмпирическое тестирование современных MLLM на WildScore выявляет как перспективные направления, так и сохраняющиеся проблемы в области рассуждений о символической музыке.'}, 'en': {'title': "WildScore: Unlocking MLLMs' Music Reasoning Potential", 'desc': "WildScore is a benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in the context of symbolic music. It evaluates how well these models can interpret real-world music scores and respond to complex questions about music. The benchmark includes genuine musical compositions and user-generated queries, providing a realistic setting for analysis. By framing music reasoning as multiple-choice questions, WildScore allows for systematic evaluation of MLLMs' understanding of music, revealing both their strengths and areas needing improvement."}, 'zh': {'title': 'WildScore：音乐推理的新基准', 'desc': 'WildScore是一个评估多模态大型语言模型（MLLMs）在符号音乐推理能力的基准测试。它通过真实的音乐乐谱和用户生成的查询，揭示了这些模型在音乐分析中的优势和挑战。该基准测试采用了系统的分类法，涵盖了高层次和细粒度的音乐学本体。通过将复杂的音乐推理框架化为多项选择题回答，WildScore为MLLMs的符号音乐理解提供了可控和可扩展的评估方式。'}}}, {'id': 'https://huggingface.co/papers/2509.05263', 'title': 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation', 'url': 'https://huggingface.co/papers/2509.05263', 'abstract': 'LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18', 'score': 6, 'issue_id': 5761, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 сентября', 'en': 'September 5', 'zh': '9月5日'}, 'hash': '027e61af3a0f5c1a', 'authors': ['Yinglin Duan', 'Zhengxia Zou', 'Tongwei Gu', 'Wei Jia', 'Zhan Zhao', 'Luyi Xu', 'Xinzhu Liu', 'Hao Jiang', 'Kang Chen', 'Shuang Qiu'], 'affiliations': ['Beihang University, China', 'City University of Hong Kong, China', 'NetEase, Inc., China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.05263.jpg', 'data': {'categories': ['#multimodal', '#games', '#optimization', '#agents', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Генерация 3D-миров на основе ИИ: быстро, точно, реалистично', 'desc': 'LatticeWorld - это фреймворк для создания трёхмерных миров, использующий облегчённые языковые модели и Unreal Engine 5. Система принимает текстовые и визуальные инструкции для генерации динамических интерактивных сред. LatticeWorld достигает высокой точности в создании планировки сцен и визуальной достоверности. Фреймворк значительно повышает эффективность промышленного производства по сравнению с традиционными ручными методами.'}, 'en': {'title': 'Revolutionizing 3D World Generation with LatticeWorld', 'desc': 'LatticeWorld is a 3D world generation framework that utilizes lightweight large language models (LLMs) and Unreal Engine 5 to create interactive environments from both textual and visual inputs. This framework aims to enhance the realism of simulations, bridging the gap between simulated and real-world scenarios, which is crucial for applications like autonomous driving and embodied AI. By employing generative methods, LatticeWorld can produce large-scale 3D worlds with dynamic agents, showcasing high-fidelity physics and real-time rendering capabilities. The results demonstrate a significant increase in production efficiency, achieving over 90 times faster output compared to traditional modeling techniques while maintaining high visual quality.'}, 'zh': {'title': 'LatticeWorld：高效生成动态3D世界的创新框架', 'desc': 'LatticeWorld是一个使用轻量级大语言模型和虚幻引擎5的3D世界生成框架。它能够根据文本和视觉输入创建动态、互动的环境，具有高准确性和效率。该框架通过多模态输入生成大规模的3D互动世界，支持动态代理和高保真物理模拟。与传统手动建模方法相比，LatticeWorld在工业生产效率上提高了90倍，同时保持了高创意质量。'}}}, {'id': 'https://huggingface.co/papers/2509.03680', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'url': 'https://huggingface.co/papers/2509.03680', 'abstract': 'LuxDiT, a video diffusion transformer fine-tuned with low-rank adaptation, generates accurate HDR environment maps from visual input, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.', 'score': 6, 'issue_id': 5766, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 сентября', 'en': 'September 3', 'zh': '9月3日'}, 'hash': 'aa0dec923ff8c683', 'authors': ['Ruofan Liang', 'Kai He', 'Zan Gojcic', 'Igor Gilitschenski', 'Sanja Fidler', 'Nandita Vijaykumar', 'Zian Wang'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.03680.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#dataset', '#training', '#video'], 'emoji': '💡', 'ru': {'title': 'LuxDiT: Трансформер для реалистичного HDR-освещения из одного изображения', 'desc': 'LuxDiT - это новый подход к оценке освещения сцены из одного изображения или видео. Метод основан на видео-диффузионном трансформере, дообученном с помощью низкоранговой адаптации. LuxDiT генерирует точные HDR-карты окружения на основе визуального ввода, превосходя существующие методы. Модель обучена на большом синтетическом наборе данных с разнообразными условиями освещения и эффективно обобщается на реальные сцены.'}, 'en': {'title': 'LuxDiT: Revolutionizing HDR Lighting Estimation with Video Diffusion Transformers', 'desc': 'LuxDiT is a novel machine learning model that generates high dynamic range (HDR) environment maps from images or videos. It uses a video diffusion transformer that has been fine-tuned with low-rank adaptation to improve the accuracy of lighting predictions. The model is trained on a large synthetic dataset, allowing it to learn diverse lighting conditions and effectively generalize to real-world scenarios. By enhancing the semantic alignment between input visuals and the generated maps, LuxDiT surpasses existing methods in both quality and performance.'}, 'zh': {'title': 'LuxDiT：高效生成HDR环境图的创新方法', 'desc': 'LuxDiT是一种新型的数据驱动方法，利用视频扩散变换器生成高动态范围（HDR）环境图。该模型通过低秩适应微调，能够从视觉输入中推断照明信息，克服了传统方法在真实场景中的局限性。它在一个包含多样化光照条件的大型合成数据集上进行训练，能够有效地从间接视觉线索中学习。与现有技术相比，LuxDiT在定量和定性评估中均表现出色，生成的照明预测具有真实的高频细节。'}}}, {'id': 'https://huggingface.co/papers/2509.05296', 'title': 'WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool', 'url': 'https://huggingface.co/papers/2509.05296', 'abstract': 'WinT3R, a feed-forward reconstruction model, achieves high-quality camera pose estimation and real-time performance using a sliding window mechanism and a global camera token pool.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.', 'score': 3, 'issue_id': 5764, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 сентября', 'en': 'September 5', 'zh': '9月5日'}, 'hash': 'b6ac447839602a03', 'authors': ['Zizun Li', 'Jianjun Zhou', 'Yifan Wang', 'Haoyu Guo', 'Wenzheng Chang', 'Yang Zhou', 'Haoyi Zhu', 'Junyi Chen', 'Chunhua Shen', 'Tong He'], 'affiliations': ['SII', 'Shanghai AI Lab', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.05296.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#cv'], 'emoji': '🎥', 'ru': {'title': 'WinT3R: Революция в реконструкции камер в реальном времени', 'desc': 'WinT3R - это модель прямого распространения для реконструкции, которая обеспечивает высококачественную оценку положения камеры и работу в режиме реального времени. Модель использует механизм скользящего окна для обмена информацией между кадрами, что улучшает качество геометрических предсказаний. WinT3R также применяет компактное представление камер и глобальный пул токенов камеры для повышения надежности оценки положения. Эти инновации позволяют модели достичь передовых результатов в онлайн-реконструкции, оценке положения камеры и скорости реконструкции.'}, 'en': {'title': 'WinT3R: Real-Time Camera Pose Estimation with High Precision', 'desc': 'WinT3R is a feed-forward reconstruction model designed for accurate camera pose estimation and efficient real-time performance. It utilizes a sliding window mechanism to facilitate effective information sharing among frames, enhancing the quality of geometric predictions while minimizing computational load. Additionally, the model incorporates a global camera token pool, which improves the reliability of pose estimation without compromising speed. As a result, WinT3R achieves state-of-the-art performance in online reconstruction tasks, as demonstrated through extensive testing on various datasets.'}, 'zh': {'title': 'WinT3R：高效精准的相机姿态估计', 'desc': 'WinT3R是一种前馈重建模型，能够实时预测精确的相机姿态和高质量的点云地图。以往的方法在重建质量和实时性能之间存在权衡。为了解决这个问题，我们引入了滑动窗口机制，确保窗口内帧之间的信息充分交流，从而提高几何预测的质量。通过维护一个全局相机令牌池，WinT3R在不牺牲效率的情况下增强了相机姿态估计的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2509.03800', 'title': 'MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting', 'url': 'https://huggingface.co/papers/2509.03800', 'abstract': 'MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.', 'score': 3, 'issue_id': 5762, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': 'ad0922456cbd778e', 'authors': ['Yuheng Li', 'Yenho Chen', 'Yuxiang Lai', 'Jike Zhong', 'Vanessa Wildman', 'Xiaofeng Yang'], 'affiliations': ['Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA', 'Department of Computer Science, University of Southern California, Los Angeles, CA', 'Department of Machine Learning, Georgia Institute of Technology, Atlanta, GA', 'Department of Radiation Oncology, Emory University School of Medicine, Atlanta, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.03800.jpg', 'data': {'categories': ['#multimodal', '#science', '#healthcare', '#transfer_learning', '#3d'], 'emoji': '🏥', 'ru': {'title': 'Улучшение анализа КТ с помощью многомасштабного обучения компьютерного зрения и обработки естественного языка', 'desc': 'MedVista3D - это фреймворк для предварительного обучения многомасштабных семантически обогащенных моделей компьютерного зрения и обработки естественного языка для анализа 3D КТ-изображений. Он решает проблемы локально-глобального понимания и вариативности медицинских отчетов. MedVista3D использует выравнивание изображения и текста на локальном и глобальном уровнях для детального представления в контексте полного объема. Фреймворк достигает передовых результатов в классификации заболеваний, поиске отчетов и медицинских вопросно-ответных системах.'}, 'en': {'title': 'Revolutionizing 3D CT Analysis with MedVista3D', 'desc': 'MedVista3D is a new framework designed to improve the analysis of 3D CT scans by combining vision and language understanding. It tackles common problems in radiology, such as missing details and inconsistent report language, by enhancing local and global context in image analysis. The framework uses advanced techniques for aligning images with text, allowing for better disease detection and interpretation of medical reports. MedVista3D has shown to outperform existing models in tasks like disease classification and report retrieval, making it a significant advancement in medical imaging technology.'}, 'zh': {'title': 'MedVista3D：提升3D CT分析的智能框架', 'desc': 'MedVista3D是一个多尺度语义增强的视觉-语言预训练框架，专门用于3D CT分析。它解决了局部与全局理解、报告变异性等问题，并在疾病分类、报告检索和医学视觉问答中达到了最先进的性能。该框架通过局部和全局图像-文本对齐，实现了细粒度的表示学习，并引入了语义匹配库来处理报告的变异性。MedVista3D在零样本疾病分类和器官分割等任务中表现优异，展示了其在医学影像分析中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.04013', 'title': 'On Robustness and Reliability of Benchmark-Based Evaluation of LLMs', 'url': 'https://huggingface.co/papers/2509.04013', 'abstract': "LLMs show reduced effectiveness on paraphrased benchmark questions, indicating limitations in handling linguistic variability and suggesting the need for more robust evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.", 'score': 2, 'issue_id': 5764, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '32f0ad5327f657e2', 'authors': ['Riccardo Lunardi', 'Vincenzo Della Mea', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.04013.jpg', 'data': {'categories': ['#evaluation', '#interpretability', '#benchmark', '#reasoning', '#data'], 'emoji': '🔍', 'ru': {'title': 'Языковые модели спотыкаются о перефразированные вопросы', 'desc': 'Исследование показало, что большие языковые модели (LLM) менее эффективны при работе с перефразированными вопросами из стандартных тестов. Это указывает на ограничения LLM в обработке лингвистических вариаций. Результаты ставят под сомнение надежность оценки моделей на основе существующих бенчмарков. Исследователи подчеркивают необходимость разработки более устойчивых методов оценки, которые лучше отражают реальные сценарии использования LLM.'}, 'en': {'title': 'Evaluating LLMs: Beyond Fixed Benchmarks to Real-World Language Variability', 'desc': "This paper investigates how well Large Language Models (LLMs) perform when faced with paraphrased questions, highlighting their limitations in dealing with linguistic variability. The authors found that while the rankings of LLMs remained stable, their effectiveness scores dropped significantly when questions were reworded. This indicates that current benchmark evaluations may not accurately reflect a model's ability to generalize to real-world language use. The study calls for the development of more robust evaluation methods that account for diverse question phrasing to better assess LLM capabilities."}, 'zh': {'title': '提升LLMs鲁棒性，重塑评估标准', 'desc': '大型语言模型（LLMs）在处理同一问题的不同表述时效果较差，显示出其在语言变异性方面的局限性。这项研究系统地评估了LLMs对改写基准问题的鲁棒性，并探讨了基于基准的评估是否可靠。研究发现，尽管LLMs在不同表述下的排名相对稳定，但其绝对有效性得分显著下降。这表明LLMs在应对真实世界的语言变异时存在困难，呼吁开发更能反映实际应用场景的评估方法。'}}}, {'id': 'https://huggingface.co/papers/2509.02437', 'title': 'U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation', 'url': 'https://huggingface.co/papers/2509.02437', 'abstract': 'U-Arm is a low-cost, adaptable teleoperation framework for robotic arms that optimizes mechanical design and control logic to enhance data collection efficiency and task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose U-Arm, a low-cost and rapidly adaptable leader-follower teleoperation framework designed to interface with most of commercially available robotic arms. Our system supports teleoperation through three structurally distinct 3D-printed leader arms that share consistent control logic, enabling seamless compatibility with diverse commercial robot configurations. Compared with previous open-source leader-follower interfaces, we further optimized both the mechanical design and servo selection, achieving a bill of materials (BOM) cost of only \\50.5 for the 6-DoF leader arm and 56.8 for the 7-DoF version. To enhance usability, we mitigate the common challenge in controlling redundant degrees of freedom by %engineering methods mechanical and control optimizations. Experimental results demonstrate that U-Arm achieves 39\\% higher data collection efficiency and comparable task success rates across multiple manipulation scenarios compared with Joycon, another low-cost teleoperation interface. We have open-sourced all CAD models of three configs and also provided simulation support for validating teleoperation workflows. We also open-sourced real-world manipulation data collected with U-Arm. The project website is https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.', 'score': 1, 'issue_id': 5767, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': '328b26798c2ef081', 'authors': ['Yanwen Zou', 'Zhaoye Zhou', 'Chenyang Shi', 'Zewei Ye', 'Junda Huang', 'Yan Ding', 'Bo Zhao'], 'affiliations': ['EvoMind Tech', 'IAAR-Shanghai', 'Independent Researcher', 'School of AI, SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2509.02437.jpg', 'data': {'categories': ['#open_source', '#robotics', '#dataset'], 'emoji': '🦾', 'ru': {'title': 'Доступное и универсальное телеуправление роботами', 'desc': 'U-Arm - это недорогая и адаптируемая система телеуправления для роботизированных манипуляторов. Она оптимизирует механическую конструкцию и логику управления для повышения эффективности сбора данных и успешности выполнения задач. Система совместима с большинством коммерчески доступных роботов и поддерживает телеуправление через три различных 3D-печатных ведущих манипулятора. По сравнению с предыдущими открытыми интерфейсами, U-Arm достигает более низкой стоимости комплектующих и решает проблему управления избыточными степенями свободы.'}, 'en': {'title': 'U-Arm: Affordable and Efficient Teleoperation for Robotic Arms', 'desc': 'U-Arm is a cost-effective teleoperation framework designed for robotic arms, enhancing both mechanical design and control logic. It features three distinct 3D-printed leader arms that maintain consistent control, allowing compatibility with various commercial robots. The system has been optimized to reduce costs significantly while improving data collection efficiency by 39% compared to existing interfaces. Additionally, U-Arm addresses the challenges of controlling redundant degrees of freedom through engineering optimizations, making it a versatile tool for robotic manipulation tasks.'}, 'zh': {'title': 'U-Arm：低成本高效的遥操作解决方案', 'desc': 'U-Arm是一个低成本、可快速适应的遥操作框架，专为机器人手臂设计。它通过三种不同结构的3D打印领导臂，提供一致的控制逻辑，支持与多种商业机器人兼容。与之前的开源遥操作接口相比，U-Arm在机械设计和伺服选择上进行了优化，使得6自由度和7自由度的领导臂成本分别仅为50.5美元和56.8美元。实验结果表明，U-Arm在数据收集效率上提高了39%，并在多种操作场景中达到了与Joycon相当的任务成功率。'}}}, {'id': 'https://huggingface.co/papers/2509.04504', 'title': 'Behavioral Fingerprinting of Large Language Models', 'url': 'https://huggingface.co/papers/2509.04504', 'abstract': "A Behavioral Fingerprinting framework evaluates Large Language Models using a Diagnostic Prompt Suite and automated pipeline, revealing divergent alignment behaviors and clustering patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated Diagnostic Prompt Suite and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting", 'score': 1, 'issue_id': 5763, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'}, 'hash': 'c8bc7caa6cf21161', 'authors': ['Zehua Pei', 'Hui-Ling Zhen', 'Ying Zhang', 'Zhiyuan Yang', 'Xing Li', 'Xianzhi Yu', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.04504.jpg', 'data': {'categories': ['#dataset', '#alignment', '#benchmark', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Поведенческий отпечаток: новый взгляд на оценку языковых моделей', 'desc': "Статья представляет новую методологию оценки больших языковых моделей (LLM), названную 'Поведенческим отпечатком'. Этот подход использует набор диагностических промптов и автоматизированный конвейер оценки, где мощная LLM выступает в роли беспристрастного судьи. Исследование выявило значительные различия в поведении моделей, связанном с выравниванием (alignment), несмотря на сходство в базовых когнитивных способностях. Результаты показывают, что интерактивная природа модели является прямым следствием конкретных стратегий выравнивания, а не просто побочным эффектом масштаба или мощности рассуждений."}, 'en': {'title': 'Unveiling the Hidden Behaviors of Language Models', 'desc': "This paper presents a new framework called 'Behavioral Fingerprinting' to evaluate Large Language Models (LLMs) beyond just their performance metrics. It uses a Diagnostic Prompt Suite and an automated evaluation pipeline to analyze the cognitive and interactive styles of various models. The study finds that while core reasoning abilities are becoming similar among top models, their alignment behaviors, such as sycophancy and semantic robustness, show significant differences. This indicates that a model's behavior is influenced more by the developers' alignment strategies than by its size or reasoning capabilities."}, 'zh': {'title': '揭示大型语言模型的行为特征', 'desc': '本文提出了一种新的“行为指纹”框架，用于评估大型语言模型（LLMs），超越传统的性能指标，关注模型的行为特征。通过使用精心设计的诊断提示套件和自动化评估流程，分析了十八种不同能力层次的模型。研究发现，尽管顶级模型在抽象和因果推理等核心能力上趋于一致，但在对齐相关的行为（如谄媚和语义稳健性）上却存在显著差异。该框架为揭示模型之间深层次的行为差异提供了一种可重复和可扩展的方法。'}}}, {'id': 'https://huggingface.co/papers/2509.04575', 'title': 'Bootstrapping Task Spaces for Self-Improvement', 'url': 'https://huggingface.co/papers/2509.04575', 'abstract': 'Exploratory Iteration (ExIt) is an autocurriculum RL method that trains LLMs to perform multi-step self-improvement at inference-time by selectively sampling informative intermediate histories, enabling strong self-improvement on unseen tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference-time is a natural target for reinforcement learning (RL), yet the naive approach assumes a fixed maximum iteration depth, which can be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family of autocurriculum RL methods that directly exploits the recurrent structure of self-improvement tasks to train LLMs to perform multi-step self-improvement at inference-time while only training on the most informative single-step iterations. ExIt grows a task space by selectively sampling the most informative intermediate, partial histories encountered during an episode for continued iteration, treating these starting points as new self-iteration task instances to train a self-improvement policy. ExIt can further pair with explicit exploration mechanisms to sustain greater task diversity. Across several domains, encompassing competition math, multi-turn tool-use, and machine learning engineering, we demonstrate that ExIt strategies, starting from either a single or many task instances, can produce policies exhibiting strong inference-time self-improvement on held-out task instances, and the ability to iterate towards higher performance over a step budget extending beyond the average iteration depth encountered during training.', 'score': 0, 'issue_id': 5768, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '86e95856d92a57a9', 'authors': ['Minqi Jiang', 'Andrei Lupu', 'Yoram Bachrach'], 'affiliations': ['Meta Superintelligence Labs', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2509.04575.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#rl', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'ExIt: Самосовершенствование языковых моделей через исследовательские итерации', 'desc': 'Exploratory Iteration (ExIt) - это метод обучения с подкреплением, который тренирует языковые модели для многошагового самосовершенствования во время вывода. ExIt выборочно отбирает наиболее информативные промежуточные истории для продолжения итерации, рассматривая их как новые экземпляры задач самоулучшения. Этот подход позволяет моделям эффективно улучшаться на невиданных ранее задачах, не ограничиваясь фиксированной глубиной итераций. ExIt продемонстрировал сильные результаты в различных областях, включая математические соревнования, многоэтапное использование инструментов и инженерию машинного обучения.'}, 'en': {'title': 'Empowering Self-Improvement in LLMs with Exploratory Iteration', 'desc': 'Exploratory Iteration (ExIt) is a novel reinforcement learning method designed to enhance the self-improvement capabilities of large language models (LLMs) during inference. It focuses on training agents to iteratively refine their solutions by selectively sampling the most informative intermediate steps from previous attempts. This approach allows LLMs to tackle unseen tasks more effectively by treating these sampled histories as new tasks for further improvement. The results show that ExIt can significantly boost performance across various domains by enabling agents to explore and iterate beyond their initial training experiences.'}, 'zh': {'title': '探索性迭代：强化学习中的自我改进新方法', 'desc': '探索性迭代（ExIt）是一种自适应课程强化学习方法，旨在训练大型语言模型（LLM）在推理时进行多步自我改进。该方法通过选择性地采样信息丰富的中间历史，帮助模型在未见过的任务上实现强大的自我提升。ExIt利用自我改进任务的递归结构，专注于最具信息量的单步迭代，从而扩展任务空间。通过与明确的探索机制结合，ExIt能够维持更大的任务多样性，展示出在多个领域的强大自我改进能力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (9)', '#agents (29)', '#agi (9)', '#alignment (14)', '#architecture (18)', '#audio (3)', '#benchmark (57)', '#cv (22)', '#data (20)', '#dataset (49)', '#diffusion (11)', '#ethics (9)', '#games (13)', '#graphs', '#hallucinations (7)', '#healthcare (7)', '#inference (9)', '#interpretability (9)', '#leakage (1)', '#long_context (8)', '#low_resource (5)', '#machine_translation (2)', '#math (4)', '#multilingual (9)', '#multimodal (42)', '#open_source (31)', '#optimization (54)', '#plp (1)', '#rag (1)', '#reasoning (47)', '#rl (31)', '#rlhf (8)', '#robotics (6)', '#science (8)', '#security (5)', '#small_models (3)', '#story_generation (1)', '#survey (5)', '#synthetic (10)', '#training (53)', '#transfer_learning (13)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-09 20:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-09 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-09 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    