
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 167 papers. September 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ¡ĞµĞ½Ñ‚ÑĞ±Ñ€ÑŒ 2025</span> | <span id="title-articles-count">167 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-08.html">â¬…ï¸ <span id="prev-date">08.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-10.html">â¡ï¸ <span id="next-date">10.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ¡ĞµĞ½Ñ‚ÑĞ±Ñ€ÑŒ 2025', 'en': 'September 2025', 'zh': '9æœˆ2025å¹´'};
        let feedDateNext = {'ru': '10.2025', 'en': '10/2025', 'zh': '10æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '08.2025', 'en': '08/2025', 'zh': '8æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.08827', 'title': 'A Survey of Reinforcement Learning for Large Reasoning Models', 'url': 'https://huggingface.co/papers/2509.08827', 'abstract': 'Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs', 'score': 91, 'issue_id': 5829, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 10', 'zh': '9æœˆ10æ—¥'}, 'hash': 'ea01091be58aef4b', 'authors': ['Kaiyan Zhang', 'Yuxin Zuo', 'Bingxiang He', 'Youbang Sun', 'Runze Liu', 'Che Jiang', 'Yuchen Fan', 'Kai Tian', 'Guoli Jia', 'Pengfei Li', 'Yu Fu', 'Xingtai Lv', 'Yuchen Zhang', 'Sihang Zeng', 'Shang Qu', 'Haozhan Li', 'Shijie Wang', 'Yuru Wang', 'Xinwei Long', 'Fangfu Liu', 'Xiang Xu', 'Jiaze Ma', 'Xuekai Zhu', 'Ermo Hua', 'Yihao Liu', 'Zonglin Li', 'Huayu Chen', 'Xiaoye Qu', 'Yafu Li', 'Weize Chen', 'Zhenzhao Yuan', 'Junqi Gao', 'Dong Li', 'Zhiyuan Ma', 'Ganqu Cui', 'Zhiyuan Liu', 'Biqing Qi', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Harbin Institute of Technology', 'Huazhong University of Science and Technology', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University College London', 'University of Science and Technology of China', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.08827.jpg', 'data': {'categories': ['#training', '#rl', '#rlhf', '#survey', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑ…Ğ¸ RL Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RL Ğ´Ğ»Ñ LRM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ RL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Scaling Reinforcement Learning for Advanced Reasoning in Language Models', 'desc': 'This paper reviews the integration of Reinforcement Learning (RL) with Large Language Models (LLMs) to improve their reasoning capabilities. It highlights the success of RL in enhancing LLMs for complex tasks like mathematics and coding, positioning RL as a key method for evolving LLMs into more advanced reasoning models (LRMs). The authors discuss the challenges of scaling RL, including the need for better computational resources, algorithm design, and training data. They aim to identify future research directions to further develop RL applications in reasoning tasks, especially in the context of achieving Artificial SuperIntelligence (ASI).'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡', 'desc': 'æœ¬è®ºæ–‡è°ƒæŸ¥äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æå‡LLMèƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³å¤æ‚çš„é€»è¾‘ä»»åŠ¡å¦‚æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼ŒRLåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•é¢ä¸´ç€è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ç­‰åŸºç¡€æ€§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™é¡¹ç»¼è¿°ä¿ƒè¿›æœªæ¥åœ¨æ›´å¹¿æ³›æ¨ç†æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.08826', 'title': 'RewardDance: Reward Scaling in Visual Generation', 'url': 'https://huggingface.co/papers/2509.08826', 'abstract': 'RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model\'s probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.', 'score': 49, 'issue_id': 5829, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 10', 'zh': '9æœˆ10æ—¥'}, 'hash': '773c62b4801465a6', 'authors': ['Jie Wu', 'Yu Gao', 'Zilyu Ye', 'Ming Li', 'Liang Li', 'Hanzhong Guo', 'Jie Liu', 'Zeyue Xue', 'Xiaoxia Hou', 'Wei Liu', 'Yan Zeng', 'Weilin Huang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2509.08826.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#rlhf', '#alignment', '#multimodal'], 'emoji': 'ğŸ’ƒ', 'ru': {'title': 'RewardDance: Ğ¢Ğ°Ğ½Ñ†ÑƒÑ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "RewardDance - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ°Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ VLM. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. RewardDance Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½ 'Ğ´Ğ°', Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ°Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RewardDance Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."}, 'en': {'title': 'RewardDance: Scaling Reward Models for Better AI Generation', 'desc': 'RewardDance is a new framework designed to improve reward modeling in visual generation tasks by aligning with Vision-Language Models (VLMs). It addresses the limitations of existing reward models, which often struggle with architectural constraints and misalignment with next-token predictions. The framework introduces a novel generative reward paradigm that reformulates reward scoring, allowing for effective scaling of reward models up to 26 billion parameters. Importantly, RewardDance mitigates the issue of reward hacking, ensuring that models produce diverse and high-quality outputs during reinforcement learning fine-tuning.'}, 'zh': {'title': 'RewardDanceï¼šè§£å†³å¥–åŠ±é»‘å®¢çš„å¯æ‰©å±•å¥–åŠ±å»ºæ¨¡æ¡†æ¶', 'desc': 'RewardDanceæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„å¯¹é½ï¼Œä»è€Œæœ‰æ•ˆåœ°æ‰©å±•å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¹¶è§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­çš„æ‰©å±•æ€§å—åˆ°æ¶æ„å’Œè¾“å…¥æ¨¡æ€çš„é™åˆ¶ï¼Œè€Œæµè¡Œçš„Bradley-TerryæŸå¤±ä¸VLMçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æœºåˆ¶ä¸åŒ¹é…ï¼Œé˜»ç¢äº†æœ‰æ•ˆæ‰©å±•ã€‚é€šè¿‡å°†å¥–åŠ±åˆ†æ•°é‡æ–°å®šä¹‰ä¸ºæ¨¡å‹é¢„æµ‹â€œæ˜¯â€æ ‡è®°çš„æ¦‚ç‡ï¼ŒRewardDanceä½¿å¥–åŠ±ç›®æ ‡ä¸VLMæ¶æ„å†…åœ¨å¯¹é½ï¼Œä»è€Œåœ¨æ¨¡å‹å’Œä¸Šä¸‹æ–‡ä¸¤ä¸ªç»´åº¦ä¸Šå®ç°æ‰©å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒRewardDanceåœ¨æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆè§£å†³äº†å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07996', 'title': '3D and 4D World Modeling: A Survey', 'url': 'https://huggingface.co/papers/2509.07996', 'abstract': "This survey provides a comprehensive review of 3D and 4D world modeling and generation, establishing definitions, taxonomy, datasets, and evaluation metrics, and discussing applications and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey", 'score': 37, 'issue_id': 5830, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '1cdd6619184c7e1e', 'authors': ['Lingdong Kong', 'Wesley Yang', 'Jianbiao Mei', 'Youquan Liu', 'Ao Liang', 'Dekai Zhu', 'Dongyue Lu', 'Wei Yin', 'Xiaotao Hu', 'Mingkai Jia', 'Junyuan Deng', 'Kaiwen Zhang', 'Yang Wu', 'Tianyi Yan', 'Shenyuan Gao', 'Song Wang', 'Linfeng Li', 'Liang Pan', 'Yong Liu', 'Jianke Zhu', 'Wei Tsang Ooi', 'Steven C. H. Hoi', 'Ziwei Liu'], 'affiliations': ['CNRS@CREATE, Singapore', 'HKUST', 'Horizon Robotics', 'HyperGAI', 'Nanjing University of Science and Technology', 'Nanyang Technological University, Singapore', 'National University of Singapore', 'Shanghai AI Laboratory', 'Technical University of Munich', 'Tsinghua University', 'University of Macau', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.07996.jpg', 'data': {'categories': ['#benchmark', '#survey', '#3d', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¸ 4D Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¸ 4D Ğ¼Ğ¸Ñ€Ğ¾Ğ². Ğ’ Ğ½ĞµĞ¼ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ (VideoGen), Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° (OccGen) Ğ¸ LiDAR (LiDARGen). ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Unifying 3D and 4D World Modeling for AI Understanding', 'desc': 'This paper reviews the field of 3D and 4D world modeling and generation, focusing on how AI can understand and predict environments. It highlights the importance of using native 3D and 4D data types, like RGB-D images and LiDAR point clouds, which are often overlooked in favor of 2D methods. The authors propose a clear taxonomy for world models, categorizing them into video-based, occupancy-based, and LiDAR-based approaches. Additionally, the survey outlines datasets, evaluation metrics, and discusses applications and challenges in the field, aiming to unify and advance research in 3D and 4D modeling.'}, 'zh': {'title': '3Dä¸4Dä¸–ç•Œå»ºæ¨¡çš„å…¨é¢ç»¼è¿°', 'desc': 'è¿™ç¯‡ç»¼è¿°æ–‡ç« å…¨é¢å›é¡¾äº†3Då’Œ4Dä¸–ç•Œå»ºæ¨¡ä¸ç”Ÿæˆçš„ç ”ç©¶ï¼Œå»ºç«‹äº†ç›¸å…³çš„å®šä¹‰ã€åˆ†ç±»ã€æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¨è®ºäº†åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨2Då›¾åƒå’Œè§†é¢‘æ•°æ®çš„ç”Ÿæˆæ–¹æ³•ä¸Šï¼Œä½†å¯¹3Då’Œ4Dè¡¨ç¤ºï¼ˆå¦‚RGB-Då›¾åƒã€å ç”¨ç½‘æ ¼å’ŒLiDARç‚¹äº‘ï¼‰çš„ç ”ç©¶æ­£åœ¨å¿«é€Ÿå¢é•¿ã€‚ä¸ºäº†å¡«è¡¥æ–‡çŒ®ä¸­ç¼ºä¹æ ‡å‡†åŒ–å®šä¹‰å’Œåˆ†ç±»çš„ç©ºç™½ï¼Œæœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†3Då’Œ4Dä¸–ç•Œå»ºæ¨¡ä¸ç”Ÿæˆçš„ç›¸å…³å·¥ä½œã€‚æœ€åï¼Œæ–‡ç« è¿˜è®¨è®ºäº†å®é™…åº”ç”¨ã€è¯†åˆ«å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„è¿›æ­¥æä¾›ä¸€ä¸ªè¿è´¯çš„åŸºç¡€å‚è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.08755', 'title': 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.08755', 'abstract': 'AgentGym-RL is a modular RL framework for training LLM agents in diverse environments without supervised fine-tuning, featuring ScalingInter-RL for balanced exploration-exploitation.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.', 'score': 19, 'issue_id': 5830, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 10', 'zh': '9æœˆ10æ—¥'}, 'hash': 'ed8314c7d1c6d19b', 'authors': ['Zhiheng Xi', 'Jixuan Huang', 'Chenyang Liao', 'Baodai Huang', 'Honglin Guo', 'Jiaqi Liu', 'Rui Zheng', 'Junjie Ye', 'Jiazheng Zhang', 'Wenxiang Chen', 'Wei He', 'Yiwen Ding', 'Guanyu Li', 'Zehui Chen', 'Zhengyin Du', 'Xuesong Yao', 'Yufei Xu', 'Jiecao Chen', 'Tao Gui', 'Zuxuan Wu', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance Seed', 'Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.08755.jpg', 'data': {'categories': ['#optimization', '#open_source', '#rl', '#training', '#agents', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AgentGym-RL: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'AgentGym-RL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ RL. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ScalingInter-RL Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 27 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Empowering LLM Agents with AgentGym-RL: Explore, Learn, Succeed!', 'desc': 'AgentGym-RL is a new modular reinforcement learning (RL) framework designed to train large language model (LLM) agents in various environments without the need for supervised fine-tuning. It allows agents to learn through exploration and interaction, similar to human cognitive development, enabling them to make intelligent decisions in complex tasks. The framework includes a unique training method called ScalingInter-RL, which balances exploration and exploitation to optimize learning stability. Extensive experiments show that agents trained with this framework perform competitively against commercial models across multiple tasks, and the framework will be open-sourced to support further research in intelligent agent development.'}, 'zh': {'title': 'AgentGym-RLï¼šæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä»£ç†è®­ç»ƒæ¡†æ¶', 'desc': 'AgentGym-RLæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ScalingInter-RLæ–¹æ³•ï¼Œä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œç¡®ä¿ä»£ç†åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è·å–çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚é€šè¿‡é™åˆ¶æ—©æœŸçš„äº¤äº’æ¬¡æ•°ï¼Œæ¡†æ¶å¼ºè°ƒåˆ©ç”¨ï¼Œéšåé€æ­¥å¢åŠ æ¢ç´¢çš„èŒƒå›´ï¼Œä»è€Œé¼“åŠ±å¤šæ ·åŒ–çš„é—®é¢˜è§£å†³ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentGym-RLæ¡†æ¶åŠå…¶è®­ç»ƒæ–¹æ³•åœ¨27ä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå•†ä¸šæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06784', 'title': 'P3-SAM: Native 3D Part Segmentation', 'url': 'https://huggingface.co/papers/2509.06784', 'abstract': 'P3-SAM, a native 3D point-promptable part segmentation model, achieves precise and robust segmentation of complex 3D objects using a feature extractor, multiple segmentation heads, and an IoU predictor.  \t\t\t\t\tAI-generated summary \t\t\t\t Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon.', 'score': 13, 'issue_id': 5830, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'c461d5e9a3042a90', 'authors': ['Changfeng Ma', 'Yang Li', 'Xinhao Yan', 'Jiachen Xu', 'Yunhan Yang', 'Chunshi Wang', 'Zibo Zhao', 'Yanwen Guo', 'Zhuo Chen', 'Chunchao Guo'], 'affiliations': ['HKU', 'NJU', 'ShanghaiTech', 'Tencent Hunyuan', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2509.06784.jpg', 'data': {'categories': ['#optimization', '#3d', '#dataset', '#training', '#games'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'P3-SAM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ IoU. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 3,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. P3-SAM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ñ‡Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Automating 3D Part Segmentation with P3-SAM', 'desc': 'P3-SAM is a novel model designed for segmenting 3D objects into their individual parts using point prompts. It utilizes a feature extractor and multiple segmentation heads, along with an Intersection over Union (IoU) predictor, to enhance segmentation accuracy and robustness. The model aims to automate the segmentation process, addressing limitations of existing methods that struggle with complex shapes. Trained on a large dataset of 3.7 million models, P3-SAM demonstrates state-of-the-art performance in part instance segmentation.'}, 'zh': {'title': 'P3-SAMï¼šå®ç°3Dç‰©ä½“çš„è‡ªåŠ¨åŒ–ç²¾ç¡®åˆ†å‰²', 'desc': 'P3-SAMæ˜¯ä¸€ç§åŸç”Ÿçš„3Dç‚¹æç¤ºéƒ¨ä»¶åˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿç²¾ç¡®ä¸”ç¨³å¥åœ°å¯¹å¤æ‚3Dç‰©ä½“è¿›è¡Œåˆ†å‰²ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç‰¹å¾æå–å™¨ã€å¤šé‡åˆ†å‰²å¤´å’ŒIoUé¢„æµ‹å™¨ï¼Œæ—¨åœ¨å®ç°3Dç‰©ä½“çš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒP3-SAMåœ¨å¤„ç†å¤æ‚ç‰©ä½“æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶æ”¯æŒç”¨æˆ·è¿›è¡Œäº¤äº’å¼åˆ†å‰²ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªåŒ…å«è¿‘370ä¸‡ä¸ªæ¨¡å‹çš„æ–°æ•°æ®é›†ä¸Šè®­ç»ƒäº†è¯¥æ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨åˆ†å‰²ç²¾åº¦å’Œé²æ£’æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05209', 'title': 'Hunyuan-MT Technical Report', 'url': 'https://huggingface.co/papers/2509.05209', 'abstract': 'Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are multilingual translation models that outperform existing models, especially in translating between Mandarin and minority languages, through a combination of pre-training, supervised fine-tuning, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic.', 'score': 9, 'issue_id': 5830, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 5', 'zh': '9æœˆ5æ—¥'}, 'hash': '1bc53ae7f9d89dff', 'authors': ['Mao Zheng', 'Zheng Li', 'Bingxin Qu', 'Mingyang Song', 'Yang Du', 'Mingrui Sun', 'Di Wang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2509.05209.jpg', 'data': {'categories': ['#machine_translation', '#open_source', '#low_resource', '#multilingual', '#rl', '#training'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Hunyuan-MT-7B Ğ¸ Hunyuan-MT-Chimera-7B - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. ĞĞ½Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ WMT2025 Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñƒ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ 30 Ğ¸Ğ· 31 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€.'}, 'en': {'title': 'Revolutionizing Multilingual Translation with Hunyuan Models', 'desc': 'Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are advanced multilingual translation models designed to excel in translating between Mandarin and various minority languages. They utilize a comprehensive training approach that includes pre-training, supervised fine-tuning, and reinforcement learning to enhance translation accuracy. The Chimera model innovatively combines multiple outputs from the Hunyuan-MT-7B to improve performance beyond traditional models. Experimental results show that these models achieve state-of-the-art performance in multilingual translation tasks, particularly excelling in low-resource language pairs.'}, 'zh': {'title': 'è¶…è¶Šä¼ ç»Ÿçš„å¤šè¯­è¨€ç¿»è¯‘æ–°æ¨¡å‹', 'desc': 'Hunyuan-MT-7Bå’ŒHunyuan-MT-Chimera-7Bæ˜¯å¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ï¼Œç‰¹åˆ«æ“…é•¿äºæ™®é€šè¯ä¸å°‘æ•°æ°‘æ—è¯­è¨€ä¹‹é—´çš„ç¿»è¯‘ã€‚è¿™äº›æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ç¿»è¯‘æ€§èƒ½ã€‚Hunyuan-MT-Chimera-7Bé‡‡ç”¨æ…¢æ€ç»´æ¨¡å¼ï¼Œæ•´åˆäº†å¤šç§è¾“å‡ºï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ¨¡å‹ã€‚ç»è¿‡å…¨é¢å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨WMT2025å…±äº«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨31ä¸ªè¯­è¨€å¯¹ä¸­æ’åç¬¬ä¸€ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·è¯­è¨€ç¯å¢ƒä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.08358', 'title': "<think> So let's replace this phrase with insult... </think> Lessons\n  learned from generation of toxic texts with LLMs", 'url': 'https://huggingface.co/papers/2509.08358', 'abstract': 'Models fine-tuned on synthetic toxic data generated by LLMs perform worse than those trained on human data due to a lexical diversity gap in the synthetic content.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems.', 'score': 5, 'issue_id': 5838, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 10', 'zh': '9æœˆ10æ—¥'}, 'hash': 'e5ca2a20313423bf', 'authors': ['Sergey Pletenev', 'Daniil Moskovskiy', 'Alexander Panchenko'], 'affiliations': ['AIRI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2509.08358.jpg', 'data': {'categories': ['#ethics', '#training', '#data', '#healthcare', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ…ÑƒĞ¶Ğµ, Ñ‡ĞµĞ¼ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ğ¹ÑÑ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ½ÑĞ°Ğ½ÑÑ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ…, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Synthetic Toxic Data: A Step Back in Detoxification Performance', 'desc': 'This paper investigates the effectiveness of using synthetic toxic data generated by Large Language Models (LLMs) for training detoxification models. The authors found that models fine-tuned on this synthetic data performed significantly worse than those trained on human-generated data, with performance drops of up to 30%. The main issue identified is a lexical diversity gap, where LLMs produce a limited range of toxic language, lacking the complexity found in human-generated content. This study underscores the necessity of using diverse, human-annotated data to create more effective detoxification systems.'}, 'zh': {'title': 'åˆæˆæ•°æ®æ— æ³•æ›¿ä»£äººç±»æ•°æ®çš„å»æ¯’åŒ–è®­ç»ƒ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„åˆæˆæœ‰æ¯’æ•°æ®æ¥è®­ç»ƒæ–‡æœ¬å»æ¯’åŒ–æ¨¡å‹çš„å¯èƒ½æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºåˆæˆæ•°æ®å¾®è°ƒçš„æ¨¡å‹æ€§èƒ½æ˜æ˜¾ä½äºåŸºäºäººç±»æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œæ€§èƒ½ä¸‹é™å¯è¾¾30%ã€‚é€ æˆè¿™ä¸€ç°è±¡çš„åŸå› æ˜¯åˆæˆæ•°æ®åœ¨è¯æ±‡å¤šæ ·æ€§ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼ŒLLMsç”Ÿæˆçš„æœ‰æ¯’å†…å®¹ä½¿ç”¨äº†æœ‰é™ä¸”é‡å¤çš„ä¾®è¾±æ€§è¯æ±‡ï¼Œæ— æ³•æ•æ‰äººç±»æ¯’æ€§è¡¨è¾¾çš„ç»†å¾®å·®åˆ«ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨å»æ¯’åŒ–ç³»ç»Ÿæ„å»ºä¸­ï¼Œä¾ç„¶éœ€è¦å¤šæ ·åŒ–çš„äººç±»æ ‡æ³¨æ•°æ®çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06870', 'title': 'The Majority is not always right: RL training for solution aggregation', 'url': 'https://huggingface.co/papers/2509.06870', 'abstract': 'A reinforcement learning approach to aggregating multiple solutions for large language models improves performance on reasoning tasks by learning to synthesize correct answers from candidate solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.', 'score': 5, 'issue_id': 5841, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'c594673839c4eb24', 'authors': ['Wenting Zhao', 'Pranjal Aggarwal', 'Swarnadeep Saha', 'Asli Celikyilmaz', 'Jason Weston', 'Ilia Kulikov'], 'affiliations': ['CMU', 'FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2509.06870.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ AggLM Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‡ĞµĞ¼ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Learn to Aggregate: Enhancing Reasoning in LLMs with Reinforcement Learning', 'desc': 'This paper presents a novel reinforcement learning method called AggLM for aggregating multiple solutions generated by large language models (LLMs) to enhance their performance on reasoning tasks. Instead of relying on traditional methods like majority voting, AggLM learns to synthesize the best answer from a set of candidate solutions by training an aggregator model using reinforcement learning with verifiable rewards. The approach emphasizes a balanced training strategy that includes both easy and challenging examples, enabling the model to identify correct minority answers as well as majority answers. Experimental results show that AggLM outperforms existing aggregation methods and effectively generalizes to solutions from various models, while also being more efficient in token usage.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºèšåˆå¤šä¸ªè§£å†³æ–¹æ¡ˆä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªèšåˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»å€™é€‰è§£å†³æ–¹æ¡ˆä¸­å®¡æŸ¥ã€è°ƒå’Œå¹¶åˆæˆæœ€ç»ˆçš„æ­£ç¡®ç­”æ¡ˆã€‚ä¸ä¼ ç»Ÿçš„ç®€å•å¤šæ•°æŠ•ç¥¨æˆ–å¥–åŠ±æ¨¡å‹æ’åæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å­¦ä¹ èšåˆä½œä¸ºä¸€ç§æ˜ç¡®çš„æ¨ç†æŠ€èƒ½ï¼Œå–å¾—äº†æ›´å¥½çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAggLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºå¤§çš„åŸºäºè§„åˆ™å’Œå¥–åŠ±æ¨¡å‹çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ¥è‡ªä¸åŒæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆæ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.08088', 'title': 'EnvX: Agentize Everything with Agentic AI', 'url': 'https://huggingface.co/papers/2509.08088', 'abstract': "EnvX leverages Agentic AI to transform GitHub repositories into intelligent agents capable of natural language interaction and collaboration, automating the entire process of understanding, initializing, and operationalizing repository functionality.  \t\t\t\t\tAI-generated summary \t\t\t\t The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem.", 'score': 2, 'issue_id': 5830, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '0afcc93155a2cf60', 'authors': ['Linyao Chen', 'Zimian Peng', 'Yingxuan Yang', 'Yikun Wang', 'Wenzheng Tom Tang', 'Hiroki H. Kobayashi', 'Weinan Zhang'], 'affiliations': ['EnvX Team', 'Fudan University', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Tokyo', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.08088.jpg', 'data': {'categories': ['#agi', '#open_source', '#benchmark', '#multimodal', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'EnvX: ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ GitHub-Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'EnvX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ GitHub-Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€ĞµÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. EnvX Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° GitTaskBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ EnvX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Transforming Repositories into Intelligent Agents for Seamless Collaboration', 'desc': 'EnvX is a framework that transforms GitHub repositories into intelligent agents using Agentic AI, allowing for natural language interaction and collaboration. It automates the understanding, initialization, and operationalization of repository functionality, addressing the challenges developers face with manual and error-prone processes. The framework operates in three phases: initializing the environment, enabling autonomous task performance, and facilitating collaboration between agents. By leveraging large language models and structured tool integration, EnvX significantly improves the efficiency of software reuse and collaboration in open-source projects.'}, 'zh': {'title': 'å°†ä»£ç åº“è½¬å˜ä¸ºæ™ºèƒ½ä»£ç†çš„é©å‘½æ€§æ¡†æ¶', 'desc': 'EnvXæ˜¯ä¸€ä¸ªåˆ©ç”¨Agentic AIçš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†GitHubä»£ç åº“è½¬å˜ä¸ºæ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’å’Œåä½œã€‚å®ƒé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è¿‡ç¨‹å®ç°è¿™ä¸€ç›®æ ‡ï¼šé¦–å…ˆæ˜¯ç¯å¢ƒåˆå§‹åŒ–ï¼Œè®¾ç½®å¿…è¦çš„ä¾èµ–å’Œæ•°æ®ï¼›å…¶æ¬¡æ˜¯äººç±»å¯¹é½çš„è‡ªåŠ¨åŒ–ï¼Œä½¿å¾—ç‰¹å®šä»£ç åº“çš„ä»£ç†èƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå®é™…ä»»åŠ¡ï¼›æœ€åæ˜¯ä»£ç†é—´åè®®ï¼Œå…è®¸å¤šä¸ªä»£ç†è¿›è¡Œåä½œã€‚EnvXä¸ä»…è‡ªåŠ¨ç”Ÿæˆä»£ç ï¼Œè¿˜è‡ªåŠ¨åŒ–ç†è§£ã€åˆå§‹åŒ–å’Œæ“ä½œä»£ç åº“åŠŸèƒ½çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†è½¯ä»¶é‡ç”¨çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07054', 'title': 'Statistical Methods in Generative AI', 'url': 'https://huggingface.co/papers/2509.07054', 'abstract': 'Statistical methods are reviewed for improving the reliability, quality, and efficiency of generative AI techniques, highlighting their applications and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.   In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.', 'score': 2, 'issue_id': 5848, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'e4fb628edc34b0e7', 'authors': ['Edgar Dobriban'], 'affiliations': ['Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.07054.jpg', 'data': {'categories': ['#ethics', '#training', '#survey', '#data', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Generative AI with Statistical Reliability', 'desc': 'This paper reviews statistical methods that can enhance the reliability, quality, and efficiency of generative AI techniques. Generative AI, while promising, often lacks guarantees regarding correctness and fairness due to its reliance on probabilistic models. The authors highlight how statistical approaches can address these issues and improve AI evaluation processes. Additionally, they discuss the limitations of current methods and suggest future research directions to further advance the field.'}, 'zh': {'title': 'æå‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¯é æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡å›é¡¾äº†ç»Ÿè®¡æ–¹æ³•åœ¨æé«˜ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯é æ€§ã€è´¨é‡å’Œæ•ˆç‡æ–¹é¢çš„åº”ç”¨ä¸å±€é™æ€§ã€‚ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ˜¯ä¸€é¡¹æ–°å…´æŠ€æœ¯ï¼Œå…·æœ‰å˜é©å¤šä¸ªé¢†åŸŸçš„æ½œåŠ›ï¼Œä½†å…¶åŸºäºæ¦‚ç‡æ¨¡å‹çš„é‡‡æ ·æ–¹æ³•ç¼ºä¹æ­£ç¡®æ€§ã€å®‰å…¨æ€§å’Œå…¬å¹³æ€§ç­‰ä¿è¯ã€‚ç»Ÿè®¡æ–¹æ³•ä¸ºæé«˜ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¯é æ€§æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¹Ÿèƒ½æ”¹å–„äººå·¥æ™ºèƒ½è¯„ä¼°çš„è´¨é‡å’Œæ•ˆç‡ã€‚æˆ‘ä»¬è®¨è®ºäº†ç°æœ‰å·¥ä½œçš„æ¦‚è¿°ï¼Œè§£é‡Šäº†æ‰€ç”¨çš„ç»Ÿè®¡æŠ€æœ¯åŠå…¶åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.08494', 'title': 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants', 'url': 'https://huggingface.co/papers/2509.08494', 'abstract': 'A benchmark evaluates human agency in AI assistants using large language models, finding varying support across systems and dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.', 'score': 0, 'issue_id': 5830, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 10', 'zh': '9æœˆ10æ—¥'}, 'hash': 'f9f3a3cdf8cd2c8d', 'authors': ['Benjamin Sturgeon', 'Daniel Samuelson', 'Jacob Haimes', 'Jacy Reese Anthis'], 'affiliations': ['AI Safety Cape Town', 'Apart Research', 'Sentience Institute', 'Stanford University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.08494.jpg', 'data': {'categories': ['#ethics', '#alignment', '#benchmark', '#rlhf'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HumanAgencyBench (HAB) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). HAB Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¸ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ñ…, Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Empowering Human Agency in AI: A New Benchmark Approach', 'desc': 'This paper introduces a benchmark called HumanAgencyBench (HAB) to evaluate how well AI assistants support human agency across various dimensions. It combines philosophical and scientific theories of agency with AI evaluation methods, specifically using large language models (LLMs) to assess AI responses to user queries. The study finds that current LLM-based assistants provide low-to-moderate support for human agency, with significant differences among systems and dimensions. The authors suggest that improving agency support should not solely rely on enhancing LLM capabilities but also focus on establishing better safety and alignment standards.'}, 'zh': {'title': 'æå‡AIåŠ©æ‰‹ä¸­çš„äººç±»ä»£ç†æƒ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åŠ©æ‰‹ä¸­äººç±»ä»£ç†æƒçš„è¯„ä¼°ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ¨¡æ‹Ÿå’ŒéªŒè¯ç”¨æˆ·æŸ¥è¯¢ã€‚æˆ‘ä»¬æå‡ºäº†HumanAgencyBenchï¼ˆHABï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç»´åº¦çš„äººç±»ä»£ç†æƒï¼Œæ—¨åœ¨è¯„ä¼°AIåŠ©æ‰‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰åŸºäºLLMçš„åŠ©æ‰‹åœ¨äººç±»ä»£ç†æƒæ”¯æŒæ–¹é¢è¡¨ç°ä½è‡³ä¸­ç­‰ï¼Œå¹¶ä¸”åœ¨ä¸åŒç³»ç»Ÿå¼€å‘è€…å’Œç»´åº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬å»ºè®®åœ¨AIç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯¹é½ç›®æ ‡ä¸Šè¿›è¡Œæ›´å¼ºæœ‰åŠ›çš„æ”¹è¿›ï¼Œä»¥æ›´å¥½åœ°æ”¯æŒäººç±»ä»£ç†æƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07980', 'title': 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.07980', 'abstract': "Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.", 'score': 66, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '41def489cc53d3e0', 'authors': ['Tong Zheng', 'Hongming Zhang', 'Wenhao Yu', 'Xiaoyang Wang', 'Xinyu Yang', 'Runpeng Dai', 'Rui Liu', 'Huiwen Bao', 'Chengsong Huang', 'Heng Huang', 'Dong Yu'], 'affiliations': ['Carnegie Mellon University', 'City University of Hong Kong', 'Tencent AI Lab Seattle', 'University of Maryland, College Park', 'University of North Carolina at Chapel Hill', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2509.07980.jpg', 'data': {'categories': ['#math', '#open_source', '#rl', '#optimization', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Parallel-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ°Ğº ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°ÑĞ¿ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸.'}, 'en': {'title': 'Unlocking Reasoning Power with Parallel Thinking', 'desc': 'The paper introduces Parallel-R1, a reinforcement learning framework designed to improve the reasoning abilities of large language models (LLMs) by facilitating parallel thinking. This approach allows the model to explore multiple reasoning paths simultaneously, which enhances its performance on complex tasks. Unlike traditional methods that rely on supervised fine-tuning, Parallel-R1 employs a progressive curriculum that first trains the model on simpler tasks before transitioning to more challenging ones using reinforcement learning. The results demonstrate significant accuracy improvements on various math benchmarks, showcasing the effectiveness of parallel thinking in enhancing model performance.'}, 'zh': {'title': 'å¹¶è¡Œæ€ç»´ï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'Parallel-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¹¶è¡Œæ€ç»´æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹ï¼Œè§£å†³äº†åœ¨è®­ç»ƒå¹¶è¡Œæ€ç»´æ—¶çš„å†·å¯åŠ¨é—®é¢˜ã€‚é€šè¿‡åœ¨ç®€å•ä»»åŠ¡ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæ¨¡å‹å­¦ä¹ å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œç„¶åè½¬å‘å¼ºåŒ–å­¦ä¹ ä»¥åº”å¯¹æ›´å¤æ‚çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒParallel-R1åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¹¶è¡Œæ€ç»´åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07979', 'title': 'Visual Representation Alignment for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2509.07979', 'abstract': "VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.", 'score': 51, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': 'f229168a41923a52', 'authors': ['Heeji Yoon', 'Jaewoo Jung', 'Junwan Kim', 'Hyungyu Choi', 'Heeseong Shin', 'Sangbeom Lim', 'Honggyu An', 'Chaehyun Kim', 'Jisang Han', 'Donghyun Kim', 'Chanho Eom', 'Sunghwan Hong', 'Seungryong Kim'], 'affiliations': ['Chung-Ang University', 'ETH Zurich', 'KAIST AI', 'Korea University', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2509.07979.jpg', 'data': {'categories': ['#benchmark', '#cv', '#alignment', '#training', '#multimodal', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VIRAL - ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). VIRAL Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ MLLM Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (VFM). Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ MLLM ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· VFM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Aligning Visual Representations for Better Multimodal Learning', 'desc': "This paper introduces VIRAL, a regularization technique that improves the performance of multimodal large language models (MLLMs) on vision-related tasks. The authors identify that MLLMs struggle with tasks like object counting due to a lack of direct visual supervision, which leads to the loss of important visual details. VIRAL addresses this issue by aligning the visual representations of MLLMs with those from pre-trained vision foundation models (VFMs), ensuring that the models retain critical visual information. The results show that this alignment significantly enhances the models' reasoning capabilities on complex visual inputs across various benchmarks."}, 'zh': {'title': 'VIRALï¼šæå‡è§†è§‰ä»»åŠ¡è¡¨ç°çš„å¯¹é½ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVIRALçš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰è¡¨ç¤ºä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å¯¹é½ï¼Œä»è€Œæå‡åœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„æ–‡æœ¬ç›‘ç£æ–¹æ³•å¯¹è§†è§‰è·¯å¾„çš„æŒ‡å¯¼æœ‰é™ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿½è§†äº†ç»†è‡´çš„è§†è§‰ä¿¡æ¯ã€‚é€šè¿‡å¼ºåˆ¶å¯¹é½å†…éƒ¨è§†è§‰è¡¨ç¤ºï¼ŒVIRALä¸ä»…å¸®åŠ©æ¨¡å‹ä¿ç•™è¾“å…¥è§†è§‰ç¼–ç å™¨ä¸­çš„é‡è¦ç»†èŠ‚ï¼Œè¿˜èƒ½è¡¥å……æ¥è‡ªVFMsçš„é¢å¤–è§†è§‰çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIRALåœ¨å¤šé¡¹å¹¿æ³›é‡‡ç”¨çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07969', 'title': 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search', 'url': 'https://huggingface.co/papers/2509.07969', 'abstract': 'Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.', 'score': 45, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '4d29daad3dc9ac61', 'authors': ['Xin Lai', 'Junyi Li', 'Wei Li', 'Tao Liu', 'Tianjian Li', 'Hengshuang Zhao'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.07969.jpg', 'data': {'categories': ['#data', '#open_source', '#cv', '#rl', '#training', '#multimodal', '#dataset', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ: Mini-o3 Ñ€Ğ°Ğ·Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Mini-o3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑˆĞµÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mini-o3 ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Deep Reasoning in Visual Search with Mini-o3', 'desc': 'Mini-o3 is a novel system designed for deep reasoning in visual search tasks, enabling multi-turn interactions that enhance problem-solving capabilities. It utilizes an iterative data collection pipeline and an over-turn masking strategy to improve performance and reasoning diversity. By constructing the Visual Probe Dataset, Mini-o3 addresses the limitations of existing models that struggle with complex tasks requiring extensive exploration. The system achieves state-of-the-art results by allowing for a greater number of reasoning steps during inference, leading to more accurate and nuanced solutions.'}, 'zh': {'title': 'Mini-o3ï¼šæ·±åº¦å¤šè½®æ¨ç†çš„è§†è§‰æœç´¢æ–°çªç ´', 'desc': 'Mini-o3æ˜¯ä¸€ä¸ªç”¨äºè§†è§‰æœç´¢ä»»åŠ¡çš„æ·±åº¦å¤šè½®æ¨ç†ç³»ç»Ÿï¼Œé‡‡ç”¨è¿­ä»£æ•°æ®æ”¶é›†ç®¡é“å’Œè¶…è½®æ©è”½ç­–ç•¥ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºè§†è§‰æ¢æµ‹æ•°æ®é›†ï¼Œè®¾è®¡äº†æ•°åƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æœç´¢é—®é¢˜ï¼Œä»¥æ”¯æŒæ¢ç´¢æ€§æ¨ç†ã€‚Mini-o3èƒ½å¤Ÿæ‰§è¡Œæ·±åº¦çš„å¤šè½®æ¨ç†ï¼Œå¤„ç†æ•°åä¸ªæ­¥éª¤ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨äº¤äº’è½®æ¬¡å’Œæ¨ç†æ¨¡å¼ä¸Šçš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMini-o3èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„æ¨ç†æ¨¡å¼å’Œæ·±åº¦æ€è€ƒè·¯å¾„ï¼Œæœ‰æ•ˆè§£å†³å¤æ‚çš„è§†è§‰æœç´¢é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07295', 'title': 'Reconstruction Alignment Improves Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2509.07295', 'abstract': 'Reconstruction Alignment (RecA) is a post-training method that enhances multimodal models by using visual embeddings as dense prompts, improving image generation and editing fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs', 'score': 30, 'issue_id': 5807, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'b638c4100f242a73', 'authors': ['Ji Xie', 'Trevor Darrell', 'Luke Zettlemoyer', 'XuDong Wang'], 'affiliations': ['UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.07295.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#open_source', '#multimodal', '#optimization', '#diffusion', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'RecA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Reconstruction Alignment (RecA) - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². RecA Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (UMM) Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ‚ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ UMM, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñƒ, RecA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Enhancing Multimodal Models with Reconstruction Alignment', 'desc': "Reconstruction Alignment (RecA) is a novel post-training technique designed to enhance unified multimodal models (UMMs) by utilizing visual embeddings as dense prompts. This method addresses the limitations of traditional training, which often relies on sparse image-text pairs that fail to capture detailed visual information. By conditioning UMMs on their own visual understanding embeddings and optimizing for self-supervised reconstruction, RecA effectively realigns the model's understanding and generation capabilities. The results demonstrate significant improvements in image generation and editing fidelity across various UMM architectures, making RecA a resource-efficient and broadly applicable strategy."}, 'zh': {'title': 'é‡å»ºå¯¹é½ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ç²¾åº¦', 'desc': 'é‡å»ºå¯¹é½ï¼ˆRecAï¼‰æ˜¯ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨è§†è§‰åµŒå…¥ä½œä¸ºå¯†é›†æç¤ºï¼Œå¢å¼ºå¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ç²¾åº¦ã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•ä¾èµ–äºå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œä½†è¿™äº›æ–‡æœ¬é€šå¸¸ç¼ºä¹ç»†è‡´çš„è§†è§‰ç»†èŠ‚ã€‚RecAåˆ©ç”¨è§†è§‰ç†è§£ç¼–ç å™¨çš„åµŒå…¥ä½œä¸ºä¸°å¯Œçš„â€œæ–‡æœ¬æç¤ºâ€ï¼Œåœ¨æ²¡æœ‰æ–‡æœ¬æè¿°çš„æƒ…å†µä¸‹æä¾›ç›‘ç£ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§å¤šæ¨¡æ€æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06818', 'title': 'UMO: Scaling Multi-Identity Consistency for Image Customization via\n  Matching Reward', 'url': 'https://huggingface.co/papers/2509.06818', 'abstract': 'UMO, a Unified Multi-identity Optimization framework, enhances identity consistency and reduces confusion in multi-reference image customization using reinforcement learning on diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With "multi-to-multi matching" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO', 'score': 23, 'issue_id': 5807, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '42f9165098d4a2b8', 'authors': ['Yufeng Cheng', 'Wenxu Wu', 'Shaojin Wu', 'Mengqi Huang', 'Fei Ding', 'Qian He'], 'affiliations': ['UXO Team, Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2509.06818.jpg', 'data': {'categories': ['#dataset', '#rl', '#open_source', '#optimization', '#diffusion', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'UMO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "UMO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ² ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. UMO Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ 'Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ-ĞºĞ¾-Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğ¼' Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹."}, 'en': {'title': 'UMO: Enhancing Identity Consistency in Image Customization', 'desc': 'The paper introduces UMO, a framework that improves how images are customized while keeping identities consistent across multiple references. It uses reinforcement learning on diffusion models to tackle the challenge of identity confusion, which is crucial since humans are particularly sensitive to faces. UMO reformulates the problem of generating multiple identities as a global assignment optimization task, enhancing the scalability of identity preservation. The authors also create a new dataset and metric to support their framework and demonstrate that UMO significantly outperforms existing methods in maintaining identity consistency.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šèº«ä»½ä¼˜åŒ–ï¼Œæå‡å›¾åƒå®šåˆ¶ä¸€è‡´æ€§', 'desc': 'UMOï¼ˆç»Ÿä¸€å¤šèº«ä»½ä¼˜åŒ–æ¡†æ¶ï¼‰é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºäº†å¤šå‚è€ƒå›¾åƒå®šåˆ¶ä¸­çš„èº«ä»½ä¸€è‡´æ€§ï¼Œå‡å°‘äº†èº«ä»½æ··æ·†ã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨å¤šå‚è€ƒå›¾åƒä¸­ä¿æŒä¸€è‡´èº«ä»½çš„æŒ‘æˆ˜ï¼Œæå‡äº†å®šåˆ¶æ¨¡å‹çš„èº«ä»½å¯æ‰©å±•æ€§ã€‚UMOå°†å¤šèº«ä»½ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå…¨å±€åˆ†é…ä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨â€œå¤šå¯¹å¤šåŒ¹é…â€èŒƒå¼å®ç°èº«ä»½ä¸€è‡´æ€§ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«åˆæˆå’ŒçœŸå®éƒ¨åˆ†çš„å¯æ‰©å±•å®šåˆ¶æ•°æ®é›†ï¼ŒUMOåœ¨å¤šä¸ªå›¾åƒå®šåˆ¶æ–¹æ³•ä¸Šæ˜¾è‘—æé«˜äº†èº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶å‡å°‘äº†èº«ä»½æ··æ·†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07414', 'title': 'Language Self-Play For Data-Free Training', 'url': 'https://huggingface.co/papers/2509.07414', 'abstract': "Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines.", 'score': 18, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '438447513c4c481f', 'authors': ['Jakub Grudzien Kuba', 'Mengting Gu', 'Qi Ma', 'Yuandong Tian', 'Vijai Mohan'], 'affiliations': ['Meta Superintelligence Labs', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.07414.jpg', 'data': {'categories': ['#games', '#rl', '#optimization', '#training', '#rlhf'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Language Self-Play (LSP). LSP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LSP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ LLM.'}, 'en': {'title': 'Empowering Models Through Self-Play: No Data Needed!', 'desc': "Language Self-Play (LSP) is a novel approach that improves large language models' (LLMs) ability to follow instructions without needing more training data. By using a game-theoretic framework, LSP allows models to play against themselves, which helps them develop stronger skills over time. This self-play method leads to better performance on instruction-following tasks compared to traditional data-driven methods. Experiments show that pretrained models can significantly enhance their capabilities through this self-play mechanism."}, 'zh': {'title': 'è¯­è¨€è‡ªæˆ‘å¯¹å¼ˆï¼šæ— æ•°æ®æå‡æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'è¯­è¨€è‡ªæˆ‘å¯¹å¼ˆï¼ˆLSPï¼‰æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤ä»»åŠ¡ä¸Šè¡¨ç°çš„æ–¹æ³•ã€‚é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹æå‡è‡ªèº«èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åšå¼ˆè®ºæ¡†æ¶ï¼Œå°†æ¨¡å‹çš„è¡¨ç°è§†ä¸ºåœ¨ç«äº‰æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œä»è€Œä¿ƒä½¿æ›´å¼ºçš„ç­–ç•¥äº§ç”Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå¯ä»¥æœ‰æ•ˆæé«˜åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¶…è¶Šäº†åŸºäºæ•°æ®çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06951', 'title': 'F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions', 'url': 'https://huggingface.co/papers/2509.06951', 'abstract': 'F1, a pretrained VLA framework with foresight generation, improves task success and generalization in dynamic environments through a Mixture-of-Transformer architecture and next-scale prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.', 'score': 18, 'issue_id': 5807, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'c287d68dc6b0f03d', 'authors': ['Qi Lv', 'Weijie Kong', 'Hao Li', 'Jia Zeng', 'Zherui Qiu', 'Delin Qu', 'Haoming Song', 'Qizhi Chen', 'Xiang Deng', 'Jiangmiao Pang'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2509.06951.jpg', 'data': {'categories': ['#agi', '#cv', '#reasoning', '#benchmark', '#agents', '#transfer_learning', '#training'], 'emoji': 'ğŸ”®', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'F1 - ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Transformer Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. F1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ»ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'F1: Enhancing Decision-Making with Visual Foresight in Dynamic Environments', 'desc': 'The paper presents F1, a pretrained Vision-Language-Action (VLA) framework designed to enhance performance in dynamic environments. Unlike traditional models that react to immediate states, F1 incorporates visual foresight generation to improve decision-making and robustness. It utilizes a Mixture-of-Transformer architecture that integrates perception, foresight, and control, allowing for better planning and action generation. Through a comprehensive training process on a large dataset, F1 demonstrates superior task success and generalization compared to existing methods.'}, 'zh': {'title': 'F1ï¼šåŠ¨æ€ç¯å¢ƒä¸­çš„å‰ç»æ€§å†³ç­–æ¡†æ¶', 'desc': 'F1æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è§†è§‰å‰ç»ç”Ÿæˆæ¥æé«˜åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨æ··åˆå˜æ¢å™¨æ¶æ„ï¼Œç»“åˆæ„ŸçŸ¥ã€å‰ç»ç”Ÿæˆå’Œæ§åˆ¶æ¨¡å—ï¼Œå¢å¼ºäº†ç†è§£ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ä¹‹é—´çš„è”ç³»ã€‚F1çš„æ ¸å¿ƒæ˜¯ä¸‹ä¸€å°ºåº¦é¢„æµ‹æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹æœªæ¥çš„è§†è§‰çŠ¶æ€ï¼Œå°†è¡ŒåŠ¨ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªä»¥å‰ç»ä¸ºæŒ‡å¯¼çš„é€†åŠ¨åŠ›å­¦é—®é¢˜ã€‚ç»è¿‡åœ¨è¶…è¿‡33ä¸‡æ¡è½¨è¿¹å’Œ136ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„ä¸‰é˜¶æ®µè®­ç»ƒï¼ŒF1åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡å’Œæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06923', 'title': 'Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding', 'url': 'https://huggingface.co/papers/2509.06923', 'abstract': "SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.", 'score': 17, 'issue_id': 5806, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'da5bcd38585e0d46', 'authors': ['Ziheng Li', 'Zexu Sun', 'Jinman Zhao', 'Erxue Min', 'Yongcheng Zeng', 'Hui Wu', 'Hengyi Cai', 'Shuaiqiang Wang', 'Dawei Yin', 'Xu Chen', 'Zhi-Hong Deng'], 'affiliations': ['Aerospace Information Research Institute, Chinese Academy of Sciences', 'Baidu Inc.', 'Department of Computer Science, University of Toronto', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Institute of Automation, Chinese Academy of Sciences', 'School of Intelligence Science and Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06923.jpg', 'data': {'categories': ['#math', '#rl', '#optimization', '#training', '#rlhf', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'SEELE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SEELE Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEELE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RLVR Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Dynamic Difficulty for Enhanced Learning Efficiency', 'desc': "SEELE is a new framework in reinforcement learning with verifiable rewards (RLVR) that improves how models learn to solve math problems by adjusting the difficulty of tasks dynamically. It does this by adding hints to problems, where the length of the hint is tailored to match the model's current abilities, ensuring that the challenges are neither too hard nor too easy. This adaptive hinting strategy helps the model explore more effectively and learn better by staying within an optimal difficulty range. Experiments show that SEELE significantly outperforms existing methods, leading to better performance in math reasoning tasks."}, 'zh': {'title': 'SEELEï¼šåŠ¨æ€è°ƒæ•´éš¾åº¦ï¼Œæå‡æ¨ç†æ•ˆç‡', 'desc': 'SEELEæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦æ¥æé«˜æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨åŸå§‹é—®é¢˜åé™„åŠ æç¤ºï¼ˆéƒ¨åˆ†å®Œæ•´è§£å†³æ–¹æ¡ˆï¼‰æ¥å¢å¼ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ã€‚SEELEä¸ä»¥å¾€çš„æç¤ºæ–¹æ³•ä¸åŒï¼Œå®ƒæ ¹æ®æ¯ä¸ªé—®é¢˜çš„ç‰¹ç‚¹ï¼Œçµæ´»åœ°è°ƒæ•´æç¤ºé•¿åº¦ï¼Œä»¥å®ç°æœ€ä½³éš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEELEåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06830', 'title': 'Curia: A Multi-Modal Foundation Model for Radiology', 'url': 'https://huggingface.co/papers/2509.06830', 'abstract': "Curia, a foundation model trained on extensive cross-sectional imaging data, demonstrates superior performance across multiple radiological tasks and shows emergent properties in cross-modality and low-data settings.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia.", 'score': 17, 'issue_id': 5812, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '5a09abd9043552bb', 'authors': ['Corentin Dancette', 'Julien Khlaut', 'Antoine Saporta', 'Helene Philippe', 'Elodie Ferreres', 'Baptiste Callard', 'ThÃ©o Danielou', 'LÃ©o Alberge', 'LÃ©o Machado', 'Daniel Tordjman', 'Julie Dupuis', 'Korentin Le Floch', 'Jean Du Terrail', 'Mariam Moshiri', 'Laurent Dercle', 'Tom Boeken', 'Jules Gregory', 'Maxime Ronot', 'FranÃ§ois Legou', 'Pascal Roux', 'Marc Sapoval', 'Pierre Manceron', 'Paul HÃ©rent'], 'affiliations': ['.omics, Paris, France', 'Centre Cardiologique du Nord, Saint-Denis, 93200, France', 'Department of Radiology and Radiological Science, Medical University of South Carolina, Charleston, SC, USA', 'Department of Radiology, Columbia University Irving Medical Center, New York, NY, 10032, USA', 'Department of Radiology, FHU MOSAIC, Beaujon Hospital, APHP.Nord, Clichy, France', 'Department of Vascular and Oncological Interventional Radiology, HË†opital Europeen Georges Pompidou, AP-HP, Paris, France', 'Faculte de Sante, Universite Paris-Cite, Paris, France', 'HEKA, INRIA, Paris, France', 'PARCC 970, INSERM, Paris, France', 'Raidium, 27 rue du faubourg Saint-Jacques, Paris, 75014, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.06830.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#benchmark', '#data', '#low_resource', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Curia: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ Curia - ÑÑ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ², Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹. Curia Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 150 000 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ (130 Ğ¢Ğ‘ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Curia: Revolutionizing Radiology with a Foundation Model', 'desc': 'Curia is a foundation model designed for radiology, trained on a vast dataset of cross-sectional imaging from a major hospital. It excels in various radiological tasks, outperforming traditional narrow models by demonstrating strong generalization across different imaging modalities and in scenarios with limited data. The model has been validated on a comprehensive benchmark, showing its ability to accurately identify organs and detect critical conditions. By releasing its weights, Curia aims to foster further advancements in AI-assisted radiological interpretation.'}, 'zh': {'title': 'Curiaï¼šæ”¾å°„å­¦çš„åŸºç¡€æ¨¡å‹æ–°çªç ´', 'desc': 'Curiaæ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œç»è¿‡å¤§é‡æ¨ªæ–­é¢å½±åƒæ•°æ®çš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ”¾å°„å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒåœ¨è·¨æ¨¡æ€å’Œä½æ•°æ®ç¯å¢ƒä¸‹å±•ç°å‡ºæ–°å…´ç‰¹æ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å•ä»»åŠ¡æ¨¡å‹ã€‚Curiaä½¿ç”¨äº†æ¥è‡ªä¸€å®¶å¤§å‹åŒ»é™¢çš„150,000ä¸ªæ£€æŸ¥æ•°æ®ï¼Œæˆä¸ºç°å®ä¸–ç•Œæ•°æ®ä¸­æœ€å¤§çš„è®­ç»ƒé›†ä¹‹ä¸€ã€‚é€šè¿‡åœ¨19ä¸ªä»»åŠ¡çš„å¤–éƒ¨éªŒè¯åŸºå‡†ä¸Šæµ‹è¯•ï¼ŒCuriaçš„è¡¨ç°ä¸æ”¾å°„ç§‘åŒ»ç”Ÿç›¸å½“ï¼Œç”šè‡³æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºå…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03646', 'title': 'Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.03646', 'abstract': 'Reinforcement Learning enhances LLM reasoning through a two-phase process involving procedural correctness and strategic planning, with HICRA algorithm focusing on high-impact planning tokens to improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling\'\' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.', 'score': 13, 'issue_id': 5819, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': '298f6c3e803a9fce', 'authors': ['Haozhe Wang', 'Qixin Xu', 'Che Liu', 'Junhong Wu', 'Fangzhen Lin', 'Wenhu Chen'], 'affiliations': ['Hong Kong University of Science and Technology', 'Imperial College London', 'M-A-P, Tsinghua University', 'UCAS', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.03646.jpg', 'data': {'categories': ['#rlhf', '#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞŸĞµÑ€Ğ²Ğ°Ñ Ñ„Ğ°Ğ·Ğ° Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ - Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ HICRA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Advanced Reasoning in LLMs with HICRA', 'desc': 'This paper discusses how Reinforcement Learning (RL) can improve the reasoning capabilities of Large Language Models (LLMs) through a two-phase learning process. Initially, the model focuses on procedural correctness, enhancing its low-level skills before shifting to high-level strategic planning. The authors introduce the HICRA algorithm, which optimizes learning by concentrating on high-impact planning tokens, thus addressing inefficiencies in traditional RL methods. Their findings suggest that measuring semantic entropy is more effective for guiding strategic exploration than conventional metrics like token-level entropy.'}, 'zh': {'title': 'èšç„¦é«˜å½±å“è§„åˆ’ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æˆåŠŸçš„æœºåˆ¶ä»ä¸æ¸…æ™°ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¯¸å¦‚â€œæç„¶å¤§æ‚Ÿæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨æ€ç­‰ç°è±¡å¹¶ä¸æ˜¯å­¤ç«‹çš„ï¼Œè€Œæ˜¯æ–°å…´æ¨ç†å±‚æ¬¡çš„æ ‡å¿—ï¼Œç±»ä¼¼äºäººç±»è®¤çŸ¥ä¸­é«˜å±‚æˆ˜ç•¥è§„åˆ’ä¸ä½å±‚ç¨‹åºæ‰§è¡Œçš„åˆ†ç¦»ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ç§å¼•äººæ³¨ç›®çš„ä¸¤é˜¶æ®µåŠ¨æ€ï¼šæœ€åˆï¼Œæ¨¡å‹å—åˆ°ç¨‹åºæ­£ç¡®æ€§çš„é™åˆ¶ï¼Œå¿…é¡»æé«˜å…¶ä½å±‚æŠ€èƒ½ã€‚ç„¶åï¼Œå­¦ä¹ ç“¶é¢ˆè½¬ç§»ï¼Œæ€§èƒ½æå‡ä¸»è¦ä¾èµ–äºé«˜å±‚æˆ˜ç•¥è§„åˆ’çš„æ¢ç´¢å’ŒæŒæ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07301', 'title': 'Causal Attention with Lookahead Keys', 'url': 'https://huggingface.co/papers/2509.07301', 'abstract': "CASTLE, an attention mechanism that updates keys with future context while maintaining autoregressive properties, outperforms standard causal attention in language modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.", 'score': 10, 'issue_id': 5807, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '2a5370a17853db77', 'authors': ['Zhuoqing Song', 'Peng Sun', 'Huizhuo Yuan', 'Quanquan Gu'], 'affiliations': ['ByteDance Seed', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2509.07301.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#architecture', '#optimization', '#training'], 'emoji': 'ğŸ°', 'ru': {'title': 'CASTLE: Ğ’Ğ·Ğ³Ğ»ÑĞ´ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CASTLE Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, CASTLE Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. CASTLE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'CASTLE: Future Context for Smarter Attention in Language Models', 'desc': 'CASTLE is a novel attention mechanism designed for language modeling that enhances the standard causal attention by updating keys with future context. Unlike traditional methods where keys are static and only consider past tokens, CASTLE introduces lookahead keys that incorporate information from future tokens while maintaining the autoregressive nature of the model. This allows for a more dynamic representation of context, leading to improved performance on various language tasks. The mechanism is efficient, enabling parallel training without the need to explicitly compute lookahead keys at each position, resulting in lower validation perplexity and better overall results.'}, 'zh': {'title': 'CASTLEï¼šæœªæ¥ä¸Šä¸‹æ–‡çš„è‡ªå›å½’æ³¨æ„åŠ›æœºåˆ¶', 'desc': 'CASTLEæ˜¯ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒåœ¨ä¿æŒè‡ªå›å½’ç‰¹æ€§çš„åŒæ—¶ï¼Œä½¿ç”¨æœªæ¥ä¸Šä¸‹æ–‡æ›´æ–°é”®å€¼ã€‚ä¸æ ‡å‡†çš„å› æœæ³¨æ„åŠ›ä¸åŒï¼ŒCASTLEçš„æ¯ä¸ªä»¤ç‰Œçš„é”®ä¼šéšç€ä¸Šä¸‹æ–‡çš„å‘å±•è€Œä¸æ–­æ›´æ–°ã€‚æˆ‘ä»¬ç§°è¿™äº›æ›´æ–°åçš„é”®ä¸ºå‰ç»é”®ï¼Œå› ä¸ºå®ƒä»¬æ¥è‡ªäºè¾ƒæ—©çš„ä½ç½®ï¼Œä½†æ•´åˆäº†ç›¸å¯¹è¿™äº›ä½ç½®åé¢å‡ºç°çš„ä»¤ç‰Œçš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCASTLEåœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†å› æœæ³¨æ„åŠ›ï¼Œé™ä½äº†éªŒè¯å›°æƒ‘åº¦ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æå‡äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07968', 'title': 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge', 'url': 'https://huggingface.co/papers/2509.07968', 'abstract': "SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.", 'score': 7, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '7fb6599f8657bb35', 'authors': ['Lukas Haas', 'Gal Yona', "Giovanni D'Antonio", 'Sasha Goldshtein', 'Dipanjan Das'], 'affiliations': ['Google DeepMind, Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.07968.jpg', 'data': {'categories': ['#dataset', '#hallucinations', '#interpretability', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SimpleQA Verified - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸, Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚ĞµĞ¼ Ğ¸ ÑĞ²ĞµÑ€ĞºÑƒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini 2.5 Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ F1-Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ² 55.6, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Elevating Factuality Evaluation for Language Models', 'desc': 'SimpleQA Verified is a new benchmark designed to assess the factual accuracy of Large Language Models (LLMs) more effectively. It improves upon previous benchmarks by eliminating issues like incorrect labels and biases, ensuring a more reliable evaluation process. The benchmark consists of 1,000 carefully curated prompts that have undergone rigorous filtering to enhance their quality and challenge. With this tool, researchers can better measure the factual performance of models like Gemini 2.5 Pro, which has achieved a leading F1-score, thus helping to reduce the occurrence of hallucinations in AI-generated content.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹äº‹å®æ€§è¯„ä¼°çš„åŸºå‡†å·¥å…·', 'desc': 'SimpleQA Verified æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ­æ–‡æœ¬äº‹å®æ€§çš„åŸºå‡†ï¼ŒåŒ…å«1000ä¸ªæç¤ºã€‚å®ƒè§£å†³äº†OpenAIåŸºå‡†ä¸­çš„ä¸€äº›å…³é”®é—®é¢˜ï¼Œå¦‚æ ‡ç­¾å™ªå£°ã€ä¸»é¢˜åè§å’Œé—®é¢˜å†—ä½™ã€‚é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè¿‡æ»¤è¿‡ç¨‹ï¼ŒSimpleQA Verified æä¾›äº†ä¸€ä¸ªæ›´å¯é å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°é›†ï¼Œå¹¶æ”¹è¿›äº†è‡ªåŠ¨è¯„åˆ†æç¤ºã€‚è¯¥åŸºå‡†ä½¿ç ”ç©¶ç¤¾åŒºèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è·Ÿè¸ªå‚æ•°æ¨¡å‹çš„äº‹å®æ€§è¿›å±•ï¼Œå¹¶å‡å°‘å¹»è§‰ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06942', 'title': 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human\n  Preference', 'url': 'https://huggingface.co/papers/2509.06942', 'abstract': "Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.", 'score': 6, 'issue_id': 5815, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '666a0df353939fc4', 'authors': ['Xiangwei Shen', 'Zhimin Li', 'Zhantao Yang', 'Shiyi Zhang', 'Yingfang Zhang', 'Donghao Li', 'Chunyu Wang', 'Qinglin Lu', 'Yansong Tang'], 'affiliations': ['Hunyuan, Tencent', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06942.jpg', 'data': {'categories': ['#diffusion', '#alignment', '#rlhf', '#training', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Direct-Align Ğ¸ Semantic Relative Preference Optimization (SRPO). Direct-Align Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…. SRPO Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX.1.dev ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Enhancing Diffusion Models with Direct-Align and SRPO', 'desc': "This paper presents two innovative methods, Direct-Align and Semantic Relative Preference Optimization (SRPO), to enhance diffusion models' alignment with human preferences. Direct-Align simplifies the process of recovering original images by using a predefined noise prior, which reduces the need for costly multistep denoising. SRPO allows for real-time adjustments of rewards based on text prompts, minimizing the need for extensive offline reward model adaptations. Together, these methods significantly improve the aesthetic quality and realism of generated images while lowering computational costs."}, 'zh': {'title': 'æå‡æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½', 'desc': 'æœ¬æ–‡æå‡ºäº†Direct-Alignå’Œè¯­ä¹‰ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆSRPOï¼‰ä¸¤ç§æ–¹æ³•ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½åº¦ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚Direct-Aligné€šè¿‡é¢„å®šä¹‰å™ªå£°æ¥æœ‰æ•ˆæ¢å¤åŸå§‹å›¾åƒï¼Œé¿å…äº†åœ¨åæœŸæ—¶é—´æ­¥çš„è¿‡åº¦ä¼˜åŒ–ã€‚SRPOåˆ™å°†å¥–åŠ±ä¿¡å·ä¸æ–‡æœ¬æ¡ä»¶ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨çº¿è°ƒæ•´å¥–åŠ±ï¼Œä»è€Œå‡å°‘äº†å¯¹ç¦»çº¿å¥–åŠ±å¾®è°ƒçš„ä¾èµ–ã€‚é€šè¿‡ä¼˜åŒ–å»å™ªå’Œåœ¨çº¿å¥–åŠ±è°ƒæ•´ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†FLUX.1.devæ¨¡å‹åœ¨çœŸå®æ„Ÿå’Œç¾å­¦è´¨é‡ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01624', 'title': 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with\n  Quantization-Aware Scheduling', 'url': 'https://huggingface.co/papers/2509.01624', 'abstract': "Q-Sched, a novel post-training quantization method for diffusion models, reduces model size by 4x while maintaining full-precision accuracy and improving image quality metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.", 'score': 5, 'issue_id': 5809, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '377f1ad33c67cc37', 'authors': ['Natalia Frumkin', 'Diana Marculescu'], 'affiliations': ['The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2509.01624.jpg', 'data': {'categories': ['#training', '#diffusion', '#inference', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Q-Sched: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Q-Sched - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 4 Ñ€Ğ°Ğ·Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Q-Sched Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ JAQ-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ FID Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Q-Sched: Efficient Quantization for High-Quality Diffusion Models', 'desc': 'Q-Sched is a new method for post-training quantization specifically designed for diffusion models, which helps to significantly reduce the model size by 4 times while keeping the accuracy intact. This method modifies the diffusion model scheduler instead of changing the model weights, allowing for efficient few-step sampling that maintains full-precision performance. It introduces the JAQ loss, which optimizes quantization-aware pre-conditioning coefficients by focusing on text-image compatibility and image quality without needing full-precision calibration. The results show that Q-Sched not only improves image quality metrics but also demonstrates that quantization and few-step distillation can work together effectively for high-quality image generation.'}, 'zh': {'title': 'Q-Schedï¼šé‡åŒ–ä¸é«˜ä¿çœŸç”Ÿæˆçš„å®Œç¾ç»“åˆ', 'desc': 'Q-Schedæ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œä¸“ä¸ºæ‰©æ•£æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„è°ƒåº¦å™¨ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œå®ç°äº†æ¨¡å‹å¤§å°å‡å°‘4å€ï¼ŒåŒæ—¶ä¿æŒå…¨ç²¾åº¦çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†JAQæŸå¤±å‡½æ•°ï¼Œç»“åˆæ–‡æœ¬-å›¾åƒå…¼å®¹æ€§å’Œå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œè¿›è¡Œç²¾ç»†ä¼˜åŒ–ã€‚Q-Schedåœ¨å¤šä¸ªå®éªŒä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†é‡åŒ–å’Œå°‘æ­¥è’¸é¦åœ¨é«˜ä¿çœŸç”Ÿæˆä¸­çš„äº’è¡¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07558', 'title': 'Î”L Normalization: Rethink Loss Aggregation in RLVR', 'url': 'https://huggingface.co/papers/2509.07558', 'abstract': 'Î”L Normalization addresses gradient variance in Reinforcement Learning with Verifiable Rewards by providing an unbiased policy loss estimate with minimal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Delta L Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed Delta L Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization.', 'score': 3, 'issue_id': 5812, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': 'd915dc7f29ac41a8', 'authors': ['Zhiyuan He', 'Xufang Luo', 'Yike Zhang', 'Yuqing Yang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.07558.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Delta L Normalization', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Delta L Normalization Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Delta L Normalization Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Minimizing Variance for Unbiased Policy Loss in RL', 'desc': 'Delta L Normalization is a novel method designed to reduce gradient variance in Reinforcement Learning with Verifiable Rewards (RLVR). It addresses the challenge of high variability in response lengths during training, which can lead to unstable optimization. Unlike previous methods that either introduce bias or fail to minimize variance, Delta L Normalization provides an unbiased estimate of policy loss while effectively reducing gradient variance. Experimental results demonstrate its effectiveness across various model sizes and tasks, showcasing its potential to enhance the performance of large language models.'}, 'zh': {'title': 'Î”Lå½’ä¸€åŒ–ï¼šç¨³å®šå¼ºåŒ–å­¦ä¹ çš„æ— åæŸå¤±ä¼°è®¡', 'desc': 'Î”Lå½’ä¸€åŒ–æ˜¯ä¸€ç§é’ˆå¯¹å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¸­æ¢¯åº¦æ–¹å·®é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡æä¾›æ— åçš„ç­–ç•¥æŸå¤±ä¼°è®¡ï¼Œæ˜¾è‘—é™ä½äº†æ¢¯åº¦æ–¹å·®ï¼Œä»è€Œå®ç°æ›´ç¨³å®šçš„ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŠ¨æ€ç”Ÿæˆé•¿åº¦çš„æƒ…å†µï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ”Lå½’ä¸€åŒ–åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ã€æœ€å¤§é•¿åº¦å’Œä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07253', 'title': 'Benchmarking Information Retrieval Models on Complex Retrieval Tasks', 'url': 'https://huggingface.co/papers/2509.07253', 'abstract': 'A benchmark of complex retrieval tasks reveals that even state-of-the-art models struggle with high-quality retrieval, and LLM-based query expansion does not consistently improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.', 'score': 2, 'issue_id': 5819, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '8b55377fd1797b1f', 'authors': ['Julian Killingback', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.07253.jpg', 'data': {'categories': ['#survey', '#rag', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°ÑĞ¿ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Benchmarking Complex Retrieval: Challenges and Opportunities', 'desc': 'This paper evaluates the performance of state-of-the-art retrieval models on complex retrieval tasks, which involve multi-part queries and specific constraints. It highlights that even advanced models struggle to deliver high-quality results, with the best achieving only moderate scores in retrieval metrics. The study also examines the effectiveness of using large language models (LLMs) for query expansion and rewriting, finding that while they can assist weaker models, they may hinder the performance of stronger ones. To foster improvement in retrieval systems, the authors propose a new benchmark of diverse and realistic complex retrieval tasks.'}, 'zh': {'title': 'æ¨åŠ¨å¤æ‚æ£€ç´¢ä»»åŠ¡çš„åˆ›æ–°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤æ‚æ£€ç´¢ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é«˜è´¨é‡æ£€ç´¢æ–¹é¢ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŸ¥è¯¢æ‰©å±•å¹¶ä¸æ€»èƒ½æé«˜æ£€ç´¢æ€§èƒ½ã€‚ä¸ºäº†æ¨åŠ¨æ£€ç´¢æ¨¡å‹çš„åˆ›æ–°ï¼Œä½œè€…æ„å»ºäº†ä¸€å¥—å¤šæ ·åŒ–ä¸”ç°å®çš„å¤æ‚æ£€ç´¢ä»»åŠ¡ï¼Œå¹¶å¯¹ä¸€ç»„ä»£è¡¨æ€§çš„å…ˆè¿›æ£€ç´¢æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡LLMå¢å¼ºå¯ä»¥å¸®åŠ©è¾ƒå¼±çš„æ¨¡å‹ï¼Œä½†æœ€å¼ºæ¨¡å‹åœ¨æ‰€æœ‰é‡å†™æŠ€æœ¯ä¸‹çš„æ€§èƒ½å‡æœ‰æ‰€ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06938', 'title': 'From Noise to Narrative: Tracing the Origins of Hallucinations in\n  Transformers', 'url': 'https://huggingface.co/papers/2509.06938', 'abstract': "Transformer models tend to activate input-insensitive semantic features under uncertainty, leading to hallucinations that can be predicted from their internal activations.  \t\t\t\t\tAI-generated summary \t\t\t\t As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.", 'score': 1, 'issue_id': 5822, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'dab8cc466304604a', 'authors': ['Praneet Suresh', 'Jack Stanley', 'Sonia Joseph', 'Luca Scimeca', 'Danilo Bzdok'], 'affiliations': ['Meta AI', 'Mila - Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.06938.jpg', 'data': {'categories': ['#architecture', '#security', '#alignment', '#training', '#rlhf', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ½ĞµÑ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. Ğ­Ñ‚Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜, Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Understanding Hallucinations in Transformer Models', 'desc': 'This paper investigates how transformer models generate hallucinations, which are incorrect outputs, particularly under uncertain input conditions. It shows that as the input becomes less structured, the model activates more semantic concepts that are not directly related to the input, leading to these hallucinations. The authors use sparse autoencoders to analyze the internal activations of the models and find that they can predict hallucinations based on these activations. The findings highlight the importance of understanding transformer behavior to improve AI safety and align models with human values.'}, 'zh': {'title': 'æ­ç¤ºå˜æ¢å™¨æ¨¡å‹å¹»è§‰çš„ç§˜å¯†', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å˜æ¢å™¨æ¨¡å‹åœ¨è¾“å…¥ä¸ç¡®å®šæ€§ä¸‹å¦‚ä½•äº§ç”Ÿå¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“è¾“å…¥ä¿¡æ¯å˜å¾—æ›´åŠ æ— ç»“æ„æ—¶ï¼Œæ¨¡å‹æ¿€æ´»çš„è¯­ä¹‰æ¦‚å¿µæ•°é‡ä¼šå¢åŠ ã€‚ç‰¹åˆ«æ˜¯åœ¨çº¯å™ªå£°è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šæ¿€æ´»ä¸€äº›ä¸è¾“å…¥æ— å…³çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¯¼è‡´è¾“å‡ºå¹»è§‰ã€‚é€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨æ¿€æ´»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¢„æµ‹è¿™äº›å¹»è§‰çš„å‡ºç°ï¼Œä»è€Œä¸ºæé«˜AIæ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†é‡è¦ä¾æ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.08519', 'title': 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning', 'url': 'https://huggingface.co/papers/2509.08519', 'abstract': 'HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  \t\t\t\t\tAI-generated summary \t\t\t\t Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.', 'score': 74, 'issue_id': 5855, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 10', 'zh': '9æœˆ10æ—¥'}, 'hash': '2db390fc41f9f85b', 'authors': ['Liyang Chen', 'Tianxiang Ma', 'Jiawei Liu', 'Bingchuan Li', 'Zhuowei Chen', 'Lijie Liu', 'Xu He', 'Gen Li', 'Qian He', 'Zhiyong Wu'], 'affiliations': ['Intelligent Creation Lab, ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.08519.jpg', 'data': {'categories': ['#dataset', '#video', '#multimodal', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'HuMo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'desc': 'HuMo - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. HuMo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'HuMo: Unifying Multimodal Control for Human-Centric Video Generation', 'desc': 'HuMo is a novel framework designed for generating human-centric videos by effectively integrating multiple input modalities such as text, images, and audio. It tackles the challenges of limited training data and the need for precise coordination between subject preservation and audio-visual synchronization. The framework employs a two-stage training approach, utilizing a high-quality dataset and innovative strategies like minimal-invasive image injection and focus-by-predicting for improved task performance. HuMo demonstrates superior capabilities in multimodal control, outperforming existing methods in generating coherent and synchronized human videos.'}, 'zh': {'title': 'HuMoï¼šäººæœ¬è§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'HuMoæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„äººæœ¬è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼å’Œæ–°é¢–çš„ç­–ç•¥è§£å†³å¤šæ¨¡æ€æ§åˆ¶ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§è¾“å…¥ä¸­åˆæˆè§†é¢‘ï¼Œå…‹æœäº†è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œå¤šæ¨¡æ€è¾“å…¥ä¸‹çš„ä»»åŠ¡åä½œéš¾é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒHuMoæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†æ¸è¿›å¼çš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHuMoåœ¨å­ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå»ºç«‹äº†ä¸€ä¸ªåä½œçš„å¤šæ¨¡æ€æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09174', 'title': 'EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs', 'url': 'https://huggingface.co/papers/2509.09174', 'abstract': 'EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.', 'score': 50, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': 'b6e2cc4088bce9ac', 'authors': ['Yuhao Zhang', 'Yuhao Du', 'Zhanchen Dai', 'Xiangnan Ma', 'Kaiqi Kou', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2509.09174.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#audio', '#dataset'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼ Ğ¸ ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ¼ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'EchoX - ÑÑ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°ĞºÑƒÑÑ‚Ğ¸ĞºĞ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ°. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. EchoX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ Ğ¾ĞºĞ¾Ğ»Ğ¾ 6000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Bridging the Acoustic-Semantic Gap with EchoX', 'desc': 'EchoX is a speech-to-speech large language model (SLLM) designed to overcome the challenges of the acoustic-semantic gap in speech processing. By integrating semantic representations into its training, EchoX maintains strong reasoning capabilities that are often lost in traditional SLLMs. This model dynamically generates speech training targets, allowing it to effectively learn from both acoustic and semantic features. As a result, EchoX demonstrates superior performance on various knowledge-based benchmarks, showcasing its advanced capabilities in understanding and generating speech.'}, 'zh': {'title': 'EchoXï¼šæ‰“ç ´å£°å­¦ä¸è¯­ä¹‰çš„å£å’', 'desc': 'EchoXæ˜¯ä¸€ç§è¯­éŸ³åˆ°è¯­éŸ³çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å£°å­¦ä¸è¯­ä¹‰ä¹‹é—´çš„å·®è·ã€‚å®ƒé€šè¿‡æ•´åˆè¯­ä¹‰è¡¨ç¤ºï¼Œä¿æŒæ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨çŸ¥è¯†åŸºç¡€çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚å½“å‰çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹åœ¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ï¼ŒEchoXé€šè¿‡åŠ¨æ€ç”Ÿæˆè¯­éŸ³è®­ç»ƒç›®æ ‡æ¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEchoXåœ¨çº¦å…­åƒå°æ—¶çš„è®­ç»ƒæ•°æ®ä¸‹ï¼Œåœ¨å¤šä¸ªçŸ¥è¯†é—®ç­”åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09674', 'title': 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.09674', 'abstract': "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL", 'score': 49, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '36851aee36c7e5a0', 'authors': ['Haozhan Li', 'Yuxin Zuo', 'Jiale Yu', 'Yuhao Zhang', 'Zhaohui Yang', 'Kaiyan Zhang', 'Xuekai Zhu', 'Yuchen Zhang', 'Tianxing Chen', 'Ganqu Cui', 'Dehui Wang', 'Dingxiang Luo', 'Yuchen Fan', 'Youbang Sun', 'Jia Zeng', 'Jiangmiao Pang', 'Shanghang Zhang', 'Yu Wang', 'Yao Mu', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09674.jpg', 'data': {'categories': ['#agents', '#optimization', '#rl', '#robotics', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'SimpleVLA-RL - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° Vision-Language-Action (VLA). ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. SimpleVLA-RL ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Robotic Action Planning with SimpleVLA-RL', 'desc': 'SimpleVLA-RL is a reinforcement learning framework designed to improve Vision-Language-Action (VLA) models for robotic manipulation. It addresses challenges like the need for extensive human-operated robotic trajectories and the difficulty in generalizing to new tasks. By implementing techniques such as VLA-specific trajectory sampling and multi-environment rendering, SimpleVLA-RL enhances long-horizon action planning and achieves state-of-the-art performance. Additionally, it uncovers new patterns during training, demonstrating its ability to go beyond traditional supervised fine-tuning methods.'}, 'zh': {'title': 'SimpleVLA-RLï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é•¿æ—¶é—´è§„åˆ’èƒ½åŠ›', 'desc': 'SimpleVLA-RLæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé•¿æ—¶é—´è·¨åº¦çš„åŠ¨ä½œè§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ç‰¹å®šçš„è½¨è¿¹é‡‡æ ·ã€å¯æ‰©å±•çš„å¹¶è¡Œå¤„ç†å’Œå¤šç¯å¢ƒæ¸²æŸ“ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚SimpleVLA-RLåœ¨LIBEROæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œå¹¶åœ¨RoboTwin 1.0å’Œ2.0ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ç§æ–°ç°è±¡â€œpushcutâ€ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿè¯†åˆ«å‡ºæ–°çš„æ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09595', 'title': 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis', 'url': 'https://huggingface.co/papers/2509.09595', 'abstract': 'Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.', 'score': 31, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '9128c939612e1d3e', 'authors': ['Yikang Ding', 'Jiwen Liu', 'Wenyuan Zhang', 'Zekun Wang', 'Wentao Hu', 'Liyuan Cui', 'Mingming Lao', 'Yingchao Shao', 'Hui Liu', 'Xiaohan Li', 'Ming Chen', 'Xiaoqiang Liu', 'Yu-Shen Liu', 'Pengfei Wan'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.09595.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#story_generation', '#video', '#games'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'Kling-Avatar - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ°ĞºĞµÑ‚, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ·Ğ°Ğ¼Ñ‹ÑĞ»Ğ°.'}, 'en': {'title': 'Kling-Avatar: Bridging Audio and Visual Realism in Avatar Generation', 'desc': 'Kling-Avatar is a new framework designed to improve the generation of avatar videos driven by audio instructions. It combines understanding of multimodal instructions with the creation of realistic portraits, resulting in videos that are both visually appealing and semantically meaningful. The framework operates in two stages: first, it uses a large language model to create a blueprint video that captures high-level semantics like character emotions and movements. Then, it generates detailed sub-clips based on this blueprint, allowing for fast and stable production of long videos while maintaining high fidelity and expressiveness.'}, 'zh': {'title': 'Kling-Avatarï¼šéŸ³é¢‘é©±åŠ¨è™šæ‹Ÿå½¢è±¡ç”Ÿæˆçš„æ–°æ ‡æ†', 'desc': 'Kling-Avataræ˜¯ä¸€ä¸ªçº§è”æ¡†æ¶ï¼Œæ—¨åœ¨æå‡éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿå½¢è±¡è§†é¢‘ç”Ÿæˆã€‚å®ƒé€šè¿‡æ•´åˆå¤šæ¨¡æ€æŒ‡ä»¤ç†è§£ä¸é€¼çœŸçš„è‚–åƒç”Ÿæˆï¼Œç”Ÿæˆé«˜ä¿çœŸä¸”è¯­ä¹‰æ˜ç¡®çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼Œé¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè“å›¾è§†é¢‘ï¼Œç„¶åæ ¹æ®è“å›¾å…³é”®å¸§å¹¶è¡Œç”Ÿæˆå¤šä¸ªå­ç‰‡æ®µã€‚å®éªŒè¡¨æ˜ï¼ŒKling-Avataråœ¨è§†é¢‘ç”Ÿæˆçš„æ¸…æ™°åº¦ã€æƒ…æ„Ÿè¡¨è¾¾å’ŒæŒ‡ä»¤æ§åˆ¶ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºæ•°å­—äººç›´æ’­å’Œè§†é¢‘åšå®¢ç­‰å®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09265', 'title': 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents', 'url': 'https://huggingface.co/papers/2509.09265', 'abstract': 'Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/', 'score': 30, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '7850d32271ef8349', 'authors': ['Jiawei Wang', 'Jiacai Liu', 'Yuqian Fu', 'Yingru Li', 'Xintao Wang', 'Yuan Lin', 'Yu Yue', 'Lin Zhang', 'Yang Wang', 'Ke Wang'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2509.09265.jpg', 'data': {'categories': ['#agents', '#optimization', '#rl', '#training', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ EMPG (Entropy-Modulated Policy Gradients) Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ¿ĞµÑ€ĞµĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. EMPG ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ EMPG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Boosting Learning with Entropy Awareness', 'desc': 'Entropy-Modulated Policy Gradients (EMPG) is a new approach that improves learning in Large Language Models (LLMs) by adjusting policy gradients based on uncertainty and task results. In long-horizon tasks, LLMs struggle with sparse rewards, making it hard to credit intermediate actions. EMPG addresses this by recalibrating the learning signal, enhancing updates for confident actions while reducing the impact of uncertain ones. This method leads to better performance in complex tasks, as shown in experiments with various challenging agent environments.'}, 'zh': {'title': 'ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼šæå‡é•¿æ—¶é—´ä»»åŠ¡å­¦ä¹ æ•ˆç‡çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼ˆEMPGï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„å­¦ä¹ åŠ¨æ€é—®é¢˜ã€‚é€šè¿‡æ ¹æ®ä¸ç¡®å®šæ€§å’Œä»»åŠ¡ç»“æœé‡æ–°æ ¡å‡†ç­–ç•¥æ¢¯åº¦ï¼ŒEMPGèƒ½å¤Ÿæé«˜åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡ã€‚è¯¥æ–¹æ³•æ”¾å¤§äº†å¯¹æ­£ç¡®è‡ªä¿¡åŠ¨ä½œçš„æ›´æ–°ï¼Œæƒ©ç½šè‡ªä¿¡é”™è¯¯ï¼Œå¹¶å‡å¼±æ¥è‡ªä¸ç¡®å®šæ­¥éª¤çš„æ›´æ–°ï¼Œä»è€Œç¨³å®šæ¢ç´¢è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEMPGåœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09372', 'title': 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model', 'url': 'https://huggingface.co/papers/2509.09372', 'abstract': 'VLA-Adapter reduces reliance on large-scale VLMs and extensive pre-training by using a lightweight Policy module with Bridge Attention, achieving state-of-the-art performance and fast inference speed with minimal computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.', 'score': 24, 'issue_id': 5858, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '518f1161098c39e1', 'authors': ['Yihao Wang', 'Pengxiang Ding', 'Lingxiao Li', 'Can Cui', 'Zirui Ge', 'Xinyang Tong', 'Wenxuan Song', 'Han Zhao', 'Wei Zhao', 'Pengxu Hou', 'Siteng Huang', 'Yifan Tang', 'Wenhui Wang', 'Ru Zhang', 'Jianyi Liu', 'Donglin Wang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'OpenHelix Team', 'State Key Laboratory of Networking and Switching Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09372.jpg', 'data': {'categories': ['#architecture', '#agents', '#rl', '#robotics', '#training', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ñ€Ğ¾Ğ¼Ğ¾Ğ·Ğ´ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'VLA-Adapter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Policy Ñ Bridge Attention Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. VLA-Adapter Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 0.5B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, VLA-Adapter Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 8 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ GPU.'}, 'en': {'title': 'Efficient Vision-Language-Action with VLA-Adapter', 'desc': 'The paper introduces VLA-Adapter, a new approach that minimizes the need for large-scale Vision-Language Models (VLMs) and extensive pre-training in Vision-Language-Action (VLA) tasks. It employs a lightweight Policy module with Bridge Attention to effectively connect vision-language representations to action spaces. This method achieves high performance with a compact 0.5B-parameter backbone and eliminates the need for robotic data pre-training. The results show that VLA-Adapter not only reaches state-of-the-art performance but also allows for rapid training and inference on standard hardware.'}, 'zh': {'title': 'VLA-Adapterï¼šé«˜æ•ˆçš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹', 'desc': 'VLA-Adapteræ˜¯ä¸€ç§æ–°é¢–çš„æ¨¡å‹ï¼Œæ—¨åœ¨å‡å°‘å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¹¿æ³›é¢„è®­ç»ƒçš„ä¾èµ–ã€‚å®ƒé€šè¿‡å¼•å…¥è½»é‡çº§çš„ç­–ç•¥æ¨¡å—å’Œæ¡¥æ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨0.5äº¿å‚æ•°çš„æƒ…å†µä¸‹å®ç°é«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ¨ç†é€Ÿåº¦éå¸¸å¿«ã€‚VLA-Adapterçš„è®¾è®¡ä½¿å¾—åœ¨æ™®é€šæ¶ˆè´¹çº§GPUä¸Šä»…éœ€8å°æ—¶å³å¯è®­ç»ƒå‡ºå¼ºå¤§çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†éƒ¨ç½²çš„é—¨æ§›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09680', 'title': 'FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark', 'url': 'https://huggingface.co/papers/2509.09680', 'abstract': 'FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .', 'score': 23, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '60acc7b8f0e01329', 'authors': ['Rongyao Fang', 'Aldrich Yu', 'Chengqi Duan', 'Linjiang Huang', 'Shuai Bai', 'Yuxuan Cai', 'Kun Wang', 'Si Liu', 'Xihui Liu', 'Hongsheng Li'], 'affiliations': ['Alibaba', 'BUAA', 'CUHK', 'HKU'], 'pdf_title_img': 'assets/pdf/title_img/2509.09680.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#open_source', '#long_context', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ', 'desc': 'FLUX-Reason-6M Ğ¸ PRISM-Bench Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. FLUX-Reason-6M Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. PRISM-Bench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ ÑĞµĞ¼ÑŒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Long Text Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Generation Chain-of-Thought (GCoT). ĞĞ±ÑˆĞ¸Ñ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 19 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° PRISM-Bench Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Text-to-Image Models with Reasoning Datasets and Benchmarks', 'desc': 'FLUX-Reason-6M and PRISM-Bench are initiatives aimed at enhancing text-to-image (T2I) models by providing a large-scale dataset and a robust evaluation framework. FLUX-Reason-6M includes 6 million images and 20 million bilingual descriptions that focus on teaching complex reasoning through structured characteristics. The dataset is meticulously curated using extensive computational resources, making it a valuable asset for researchers. PRISM-Bench introduces a new evaluation standard with multiple tracks to assess model performance, revealing significant gaps and guiding future improvements in T2I generation.'}, 'zh': {'title': 'æ¨åŠ¨æ¨ç†å¯¼å‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ', 'desc': 'FLUX-Reason-6Må’ŒPRISM-Benchæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç¼ºä¹ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ•°æ®é›†å’ŒåŸºå‡†çš„é—®é¢˜ã€‚FLUX-Reason-6Mæ˜¯ä¸€ä¸ªåŒ…å«600ä¸‡å¼ é«˜è´¨é‡å›¾åƒå’Œ2000ä¸‡æ¡åŒè¯­æè¿°çš„å¤§å‹æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ•™æˆå¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚PRISM-Benchæä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†ï¼ŒåŒ…å«ä¸ƒä¸ªä¸åŒçš„è¯„ä¼°è½¨é“ï¼Œç‰¹åˆ«æ˜¯ä¸€ä¸ªä½¿ç”¨ç”Ÿæˆé“¾æ€ç»´ï¼ˆGCoTï¼‰çš„é•¿æ–‡æœ¬æŒ‘æˆ˜ã€‚é€šè¿‡è¿™äº›èµ„æºï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨æ¨ç†å¯¼å‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸‹ä¸€æ³¢å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09666', 'title': 'Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?', 'url': 'https://huggingface.co/papers/2509.09666', 'abstract': 'A novel framework UAE uses reinforcement learning to unify image-to-text and text-to-image processes, enhancing mutual understanding and generation fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder\'s reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising "aha moment" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.', 'score': 22, 'issue_id': 5856, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': 'd213e626e6faeaa5', 'authors': ['Zhiyuan Yan', 'Kaiqing Lin', 'Zongjian Li', 'Junyan Ye', 'Hui Han', 'Zhendong Wang', 'Hao Liu', 'Bin Lin', 'Hao Li', 'Xue Xu', 'Xinyan Xiao', 'Jingdong Wang', 'Haifeng Wang', 'Li Yuan'], 'affiliations': ['Baidu ERNIE', 'PKU', 'Rabbitpre AI', 'SYSU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2509.09666.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#games', '#rl'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº UAE, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (I2T), ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚, Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ (T2I), Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Unified-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unifying Image and Text with Reinforcement Learning', 'desc': 'This paper presents a new framework called UAE that uses reinforcement learning to connect image-to-text (I2T) and text-to-image (T2I) processes. It employs an Auto-Encoder approach where the encoder compresses images into text and the decoder reconstructs images from that text. The framework focuses on improving the mutual understanding between these two processes by using reconstruction fidelity as a training goal. The authors also introduce a benchmark called Unified-Bench to evaluate the effectiveness of this unified multimodal learning approach.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€å­¦ä¹ çš„æ–°æ¡†æ¶UAE', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶UAEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒçš„è¿‡ç¨‹ï¼Œå¢å¼ºäº†ç›¸äº’ç†è§£å’Œç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡è‡ªç¼–ç å™¨çš„è§†è§’ï¼Œå°†ç†è§£è¿‡ç¨‹è§†ä¸ºç¼–ç å™¨ï¼ˆI2Tï¼‰ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹è§†ä¸ºè§£ç å™¨ï¼ˆT2Iï¼‰ï¼Œå¹¶ä»¥é‡å»ºç²¾åº¦ä½œä¸ºç»Ÿä¸€è®­ç»ƒç›®æ ‡ã€‚UAEæ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ å®ç°ï¼šå†·å¯åŠ¨é˜¶æ®µã€ç”Ÿæˆç†è§£é˜¶æ®µå’Œç†è§£ç”Ÿæˆé˜¶æ®µï¼Œç¡®ä¿ä¿¡æ¯åœ¨ç†è§£å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„åŒå‘æµåŠ¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Unified-BenchåŸºå‡†ï¼Œè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€å­¦ä¹ çš„ç¨‹åº¦ï¼Œå‘ç°éšç€å¼ºåŒ–å­¦ä¹ çš„è¿›å±•ï¼Œç¼–ç å™¨èƒ½å¤Ÿç”Ÿæˆæ›´å…·æè¿°æ€§çš„æ–‡æœ¬ï¼Œè€Œè§£ç å™¨åˆ™èƒ½æ›´å¥½åœ°ç†è§£è¿™äº›å¤æ‚æè¿°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09676', 'title': 'SpatialVID: A Large-Scale Video Dataset with Spatial Annotations', 'url': 'https://huggingface.co/papers/2509.09676', 'abstract': "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.  \t\t\t\t\tAI-generated summary \t\t\t\t Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.", 'score': 12, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': 'b2d981674edaecf5', 'authors': ['Jiahao Wang', 'Yufeng Yuan', 'Rujie Zheng', 'Youtian Lin', 'Jian Gao', 'Lin-Zhuo Chen', 'Yajie Bao', 'Yi Zhang', 'Chang Zeng', 'Yanxi Zhou', 'Xiaoxiao Long', 'Hao Zhu', 'Zhaoxiang Zhang', 'Xun Cao', 'Yao Yao'], 'affiliations': ['Institute of Automation, Chinese Academy of Science', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09676.jpg', 'data': {'categories': ['#3d', '#dataset', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'SpatialVID: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ', 'desc': 'SpatialVID - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ 3D-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 21 000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² 2,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 7 089 Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ¸ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. SpatialVID ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking 3D Vision with SpatialVID: A New Era of Video Datasets', 'desc': 'SpatialVID is a comprehensive dataset designed to improve machine learning models in video and 3D vision tasks. It contains over 21,000 hours of diverse, real-world video footage, which has been meticulously processed into 2.7 million clips. Each clip is enriched with dense 3D annotations, including camera poses, depth maps, and motion instructions, providing a rich source of training data. This dataset addresses the limitations of existing datasets by offering high-quality, large-scale data that enhances model generalization and performance in spatial intelligence applications.'}, 'zh': {'title': 'SpatialVIDï¼šæå‡è§†é¢‘ä¸3Dè§†è§‰ç ”ç©¶çš„å…³é”®æ•°æ®é›†', 'desc': 'SpatialVIDæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„è§†é¢‘å’Œå¯†é›†çš„3Dæ³¨é‡Šï¼Œæ—¨åœ¨æå‡è§†é¢‘å’Œ3Dè§†è§‰ç ”ç©¶ä¸­çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ”¶é›†äº†è¶…è¿‡21,000å°æ—¶çš„åŸå§‹è§†é¢‘ï¼Œå¹¶é€šè¿‡åˆ†å±‚è¿‡æ»¤ç®¡é“å¤„ç†æˆ270ä¸‡æ®µè§†é¢‘ç‰‡æ®µï¼Œæä¾›äº†ä¸°å¯Œçš„åŠ¨æ€å†…å®¹ã€‚æ¯ä¸ªç‰‡æ®µéƒ½é™„æœ‰è¯¦ç»†çš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›¸æœºä½å§¿ã€æ·±åº¦å›¾ã€åŠ¨æ€æ©ç ã€ç»“æ„åŒ–æ ‡é¢˜å’Œåºåˆ—åŒ–è¿åŠ¨æŒ‡ä»¤ã€‚SpatialVIDçš„æ•°æ®ç»Ÿè®¡åˆ†ææ˜¾ç¤ºå‡ºå…¶ä¸°å¯Œæ€§å’Œå¤šæ ·æ€§ï¼Œç›´æ¥ä¿ƒè¿›äº†æ¨¡å‹çš„æ³›åŒ–å’Œæ€§èƒ½æå‡ï¼Œæˆä¸ºè§†é¢‘å’Œ3Dè§†è§‰ç ”ç©¶é¢†åŸŸçš„é‡è¦èµ„äº§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09286', 'title': 'Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding', 'url': 'https://huggingface.co/papers/2509.09286', 'abstract': 'VLMs are enhanced with an adaptive framework that selects between code-based and direct visual reasoning for chart understanding, improving performance and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task.', 'score': 7, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '73de225642b07635', 'authors': ['Bohao Tang', 'Yan Ma', 'Fei Zhang', 'Jiadi Su', 'Ethan Chern', 'Zhulin Hu', 'Zhixin Wang', 'Pengfei Liu', 'Ya Zhang'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.09286.jpg', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#multimodal', '#training', '#hallucinations'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ VLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (VLM). ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Code-as-Thought (CaT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ° Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ CaT Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Adaptive Reasoning for Enhanced Chart Understanding in VLMs', 'desc': 'This paper introduces an adaptive framework for Vision-Language Models (VLMs) that enhances their ability to understand charts by selecting between code-based reasoning and direct visual analysis. The authors highlight the limitations of previous methods, which either rely on rigid external tools or single reasoning strategies that are hard to verify. They propose a novel approach called Code-as-Thought (CaT) that represents visual information in a symbolic format, allowing for better verification and accuracy. By implementing Visual Programmability, the model learns to dynamically choose the most effective reasoning pathway for each chart-question pair, leading to improved performance across various benchmarks.'}, 'zh': {'title': 'åŠ¨æ€é€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„çš„è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”æ¡†æ¶åœ¨ä»£ç åŸºç¡€æ¨ç†å’Œç›´æ¥è§†è§‰æ¨ç†ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Œä»¥æé«˜å›¾è¡¨ç†è§£çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚ä»¥å¾€çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä¾èµ–å¤–éƒ¨å·¥å…·æˆ–å•ä¸€æ¨ç†ç­–ç•¥ï¼Œå¯¼è‡´åœ¨å¤æ‚å›¾è¡¨ä¸Šè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä»£ç ä½œä¸ºæ€ç»´ï¼ˆCaTï¼‰çš„æ–¹æ³•ï¼Œå°†å›¾è¡¨çš„è§†è§‰ä¿¡æ¯ä»¥å¯éªŒè¯çš„ç¬¦å·æ ¼å¼è¡¨ç¤ºï¼Œå¹¶æå‡ºè§†è§‰å¯ç¼–ç¨‹æ€§ï¼Œå…è®¸æ¨¡å‹æ ¹æ®å›¾è¡¨å’Œé—®é¢˜çš„ç‰¹æ€§é€‰æ‹©æœ€ä½³çš„æ¨ç†æ–¹å¼ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ç­–ç•¥ï¼Œç»“åˆæ•°æ®å‡†ç¡®æ€§å¥–åŠ±å’Œå†³ç­–å¥–åŠ±ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­åŠ¨æ€é€‰æ‹©æœ€ä¼˜æ¨ç†è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09118', 'title': 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval', 'url': 'https://huggingface.co/papers/2509.09118', 'abstract': 'GA-DMS framework enhances CLIP for person representation learning by improving data quality and model architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.', 'score': 5, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '7693d45e6980cf3f', 'authors': ['Tianlu Zheng', 'Yifan Zhang', 'Xiang An', 'Ziyong Feng', 'Kaicheng Yang', 'Qichuan Ding'], 'affiliations': ['DeepGlint', 'Northeastern University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.09118.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#transfer_learning', '#dataset', '#architecture'], 'emoji': 'ğŸ‘¤', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ CLIP Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GA-DMS Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ CLIP Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° WebPerson Ğ¸Ğ· 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. GA-DMS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾-Ğ°Ñ‚Ñ‚ĞµĞ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ñ†ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing CLIP for Superior Person Representation Learning', 'desc': "The GA-DMS framework enhances the CLIP model for person representation learning by addressing data quality and model architecture challenges. It introduces a new data construction pipeline that uses MLLMs to create a large dataset of 5 million high-quality person-centric image-text pairs, called WebPerson. The framework also employs a dual-masking technique that adapts to noisy text tokens, improving the model's ability to align visual and textual information. As a result, GA-DMS achieves state-of-the-art performance in various benchmarks for person representation tasks."}, 'zh': {'title': 'GA-DMSï¼šæå‡CLIPçš„äººç‰©è¡¨ç¤ºå­¦ä¹ ', 'desc': 'GA-DMSæ¡†æ¶é€šè¿‡æ”¹è¿›æ•°æ®è´¨é‡å’Œæ¨¡å‹æ¶æ„ï¼Œå¢å¼ºäº†CLIPåœ¨äººç‰©è¡¨ç¤ºå­¦ä¹ ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è§£å†³äº†äººç‰©ä¸­å¿ƒå›¾åƒçš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå…¨å±€å¯¹æ¯”å­¦ä¹ çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æŠ—å™ªå£°çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«500ä¸‡é«˜è´¨é‡äººç‰©å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§å‹æ•°æ®é›†WebPersonã€‚GA-DMSæ¡†æ¶é€šè¿‡åŸºäºæ¢¯åº¦æ³¨æ„åŠ›ç›¸ä¼¼åº¦åˆ†æ•°è‡ªé€‚åº”åœ°å±è”½å™ªå£°æ–‡æœ¬æ ‡è®°ï¼Œæ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01964', 'title': '2D Gaussian Splatting with Semantic Alignment for Image Inpainting', 'url': 'https://huggingface.co/papers/2509.01964', 'abstract': "A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.", 'score': 5, 'issue_id': 5855, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '60a42770e646820d', 'authors': ['Hongyu Li', 'Chaofeng Chen', 'Xiaoming Li', 'Guangming Lu'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Nanyang Technological University', 'School of Artificial Intelligence, Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01964.jpg', 'data': {'categories': ['#inference', '#cv', '#benchmark'], 'emoji': 'ğŸ–Œï¸', 'ru': {'title': 'Ğ”Ğ¾Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Gaussian Splatting Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 2D Gaussian Splatting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DINO Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ.'}, 'en': {'title': 'Revolutionizing Image Inpainting with 2D Gaussian Splatting', 'desc': 'This paper presents a new framework for image inpainting using 2D Gaussian Splatting, which transforms incomplete images into continuous representations. By leveraging pretrained DINO model features, the framework ensures that the inpainted areas maintain global semantic consistency with the surrounding content. The method employs a differentiable rasterization process to reconstruct images, promoting pixel-level coherence in the results. Additionally, a patch-wise rasterization strategy is introduced to enhance efficiency and reduce memory usage, leading to competitive performance in both quantitative and perceptual evaluations.'}, 'zh': {'title': 'é«˜æ•ˆå›¾åƒä¿®å¤çš„æ–°æ–¹å‘ï¼šäºŒç»´é«˜æ–¯ç‚¹äº‘æŠ€æœ¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾åƒä¿®å¤æ¡†æ¶ï¼Œåˆ©ç”¨äºŒç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ2D Gaussian Splattingï¼‰æŠ€æœ¯ï¼Œç»“åˆé¢„è®­ç»ƒçš„DINOæ¨¡å‹ç‰¹å¾ï¼Œä»¥å®ç°å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶å°†ä¸å®Œæ•´çš„å›¾åƒç¼–ç ä¸ºäºŒç»´é«˜æ–¯ç‚¹ç³»æ•°çš„è¿ç»­åœºï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†å…‰æ …åŒ–è¿‡ç¨‹é‡å»ºæœ€ç»ˆå›¾åƒã€‚é«˜æ–¯ç‚¹äº‘çš„è¿ç»­æ¸²æŸ“æ–¹å¼è‡ªç„¶ä¿ƒè¿›äº†ä¿®å¤ç»“æœçš„åƒç´ çº§ä¸€è‡´æ€§ã€‚é€šè¿‡å¼•å…¥åŸºäºè¡¥ä¸çš„å…‰æ …åŒ–ç­–ç•¥ï¼Œæœ¬æ–‡åœ¨æé«˜æ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„åŒæ—¶ï¼Œç¡®ä¿ä¿®å¤å†…å®¹ä¸å‘¨å›´åœºæ™¯ä¿æŒä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09614', 'title': 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering', 'url': 'https://huggingface.co/papers/2509.09614', 'abstract': 'LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.', 'score': 2, 'issue_id': 5852, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '15a23d38c535fc1c', 'authors': ['Jielin Qiu', 'Zuxin Liu', 'Zhiwei Liu', 'Rithesh Murthy', 'Jianguo Zhang', 'Haolin Chen', 'Shiyu Wang', 'Ming Zhu', 'Liangwei Yang', 'Juntao Tan', 'Zhepeng Cen', 'Cheng Qian', 'Shelby Heinecke', 'Weiran Yao', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.09614.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LoCoBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ', 'desc': 'LoCoBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8000 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚ 10 Ñ‚Ñ‹Ñ. Ğ´Ğ¾ 1 Ğ¼Ğ»Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². LoCoBench Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹.'}, 'en': {'title': 'Evaluating Long-Context LLMs for Complex Software Development', 'desc': 'LoCoBench is a new benchmark designed to evaluate long-context language models (LLMs) in complex software development tasks. It focuses on understanding entire codebases and maintaining architectural consistency, which is crucial for large-scale systems. The benchmark includes 8,000 scenarios across 10 programming languages, with context lengths ranging from 10,000 to 1,000,000 tokens, allowing for a thorough assessment of LLM performance. By introducing 8 task categories and a comprehensive evaluation framework, LoCoBench highlights significant performance gaps in current models, emphasizing the need for improved long-context understanding in software development.'}, 'zh': {'title': 'è¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„å…¨æ–°åŸºå‡†', 'desc': 'LoCoBenchæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è½¯ä»¶å¼€å‘åœºæ™¯ä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒå¡«è¡¥äº†å¯¹æ•´ä¸ªä»£ç åº“ç†è§£å’Œåœ¨å¤§è§„æ¨¡ç³»ç»Ÿä¸­ä¿æŒæ¶æ„ä¸€è‡´æ€§çš„è¯„ä¼°ç©ºç™½ã€‚è¯¥åŸºå‡†æä¾›äº†8000ä¸ªè¯„ä¼°åœºæ™¯ï¼Œæ¶µç›–10ç§ç¼–ç¨‹è¯­è¨€ï¼Œèƒ½å¤Ÿç²¾ç¡®è¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ€§èƒ½çš„ä¸‹é™ã€‚é€šè¿‡å¼•å…¥å¤šç§ä»»åŠ¡ç±»åˆ«å’Œè¯„ä¼°æŒ‡æ ‡ï¼ŒLoCoBenchä¸ºé•¿ä¸Šä¸‹æ–‡ç†è§£åœ¨è½¯ä»¶å¼€å‘ä¸­çš„æŒ‘æˆ˜æä¾›äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.09332', 'title': 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning', 'url': 'https://huggingface.co/papers/2509.09332', 'abstract': 'OmniEVA addresses spatial and embodiment gaps in multimodal large language models for embodied intelligence through a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io', 'score': 2, 'issue_id': 5853, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 11', 'zh': '9æœˆ11æ—¥'}, 'hash': '65192793a84b5158', 'authors': ['Yuecheng Liu', 'Dafeng Chi', 'Shiguang Wu', 'Zhanguang Zhang', 'Yuzheng Zhuang', 'Bowen Yang', 'He Zhu', 'Lingfeng Zhang', 'Pengwei Xie', 'David Gamaliel Arcos Bravo', 'Yingxue Zhang', 'Jianye Hao', 'Xingyue Quan'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.09332.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark', '#multimodal', '#games', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'OmniEVA: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'OmniEVA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑ‡ĞµÑ‚Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ 3D-Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ. OmniEVA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Bridging Gaps for Smarter Embodied Intelligence', 'desc': 'OmniEVA is a new approach that improves how large language models understand and interact with the physical world. It tackles two main problems: the Geometric Adaptability Gap, which limits models trained on 2D data from effectively handling 3D tasks, and the Embodiment Constraint Gap, where models fail to consider the real-world limitations of robots. The paper introduces a Task-Adaptive 3D Grounding mechanism that allows the model to adjust its understanding of 3D space based on the task at hand. Additionally, it presents an Embodiment-Aware Reasoning framework that ensures planning decisions are practical and achievable, leading to superior performance in various embodied tasks.'}, 'zh': {'title': 'OmniEVAï¼šæå‡å…·èº«æ™ºèƒ½çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'OmniEVA æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åœ¨å…·èº«æ™ºèƒ½ä¸­çš„ç©ºé—´å’Œå…·èº«æ€§å·®è·ã€‚å®ƒé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„ä¸‰ç»´å®šä½æœºåˆ¶å’Œå…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚è¿›è¡Œä¸‰ç»´ä¿¡æ¯çš„é€‰æ‹©æ€§èåˆï¼Œä»è€Œå®ç°æ›´å¥½çš„ç©ºé—´ç†è§£å’Œå†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVA åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›å’Œçµæ´»çš„è§„åˆ’èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.07430', 'title': 'The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward', 'url': 'https://huggingface.co/papers/2509.07430', 'abstract': 'A new framework, DPH-RL, uses mass-covering f-divergences to address Pass@k degradation and catastrophic forgetting in fine-tuning LLMs with RLVR, improving both Pass@1 and Pass@k.  \t\t\t\t\tAI-generated summary \t\t\t\t A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.', 'score': 2, 'issue_id': 5854, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'}, 'hash': '1d2012ade6917cee', 'authors': ['Long Li', 'Jiaran Hao', 'Jason Klein Liu', 'Zhijian Zhou', 'Xiaoyu Tan', 'Wei Chu', 'Zhe Wang', 'Shirui Pan', 'Chao Qu', 'Yuan Qi'], 'affiliations': ['Fudan University', 'Griffith University', 'INFLY TECH'], 'pdf_title_img': 'assets/pdf/title_img/2509.07430.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DPH-RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. DPH-RL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ±Ñ‹Ğ²Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DPH-RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Pass@1, Ñ‚Ğ°Ğº Ğ¸ Pass@k Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Preserving Knowledge in LLMs with DPH-RL', 'desc': 'The paper introduces a new framework called Diversity-Preserving Hybrid Reinforcement Learning (DPH-RL) to tackle issues in fine-tuning Large Language Models (LLMs) using Reinforcement Learning with Verifiable Reward (RLVR). It addresses the problem of Pass@k degradation and catastrophic forgetting by utilizing mass-covering f-divergences, which help maintain a diverse knowledge base during training. The authors argue that traditional divergence measures, like reverse KL-divergence, can lead to knowledge loss, while DPH-RL promotes knowledge retention by continuously referencing the initial policy. Experimental results show that DPH-RL improves both single-attempt accuracy (Pass@1) and multi-attempt performance (Pass@k), making it a more efficient and effective approach for training LLMs.'}, 'zh': {'title': 'åˆ©ç”¨f-æ•£åº¦æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DPH-RLï¼Œåˆ©ç”¨è´¨é‡è¦†ç›–çš„f-æ•£åº¦æ¥è§£å†³åœ¨ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶å‡ºç°çš„Pass@kæ€§èƒ½ä¸‹é™å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¼ ç»Ÿçš„RLVRç›®æ ‡å¾€å¾€å¿½è§†äº†æ•£åº¦é¡¹çš„é€‰æ‹©å’ŒåŠŸèƒ½ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¿æŒçŸ¥è¯†æ–¹é¢ç¼ºä¹æœ‰æ•ˆæœºåˆ¶ã€‚DPH-RLé€šè¿‡ä½¿ç”¨å‰å‘KLæ•£åº¦å’ŒJSæ•£åº¦ç­‰è´¨é‡è¦†ç›–çš„f-æ•£åº¦ï¼Œä½œä¸ºä¸€ç§æ’ç»ƒæœºåˆ¶ï¼Œå¸®åŠ©æ¨¡å‹ä¿æŒå¹¿æ³›çš„è§£å†³æ–¹æ¡ˆè¦†ç›–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPH-RLä¸ä»…è§£å†³äº†Pass@kçš„ä¸‹é™é—®é¢˜ï¼Œè¿˜åœ¨é¢†åŸŸå†…å¤–åŒæ—¶æé«˜äº†Pass@1å’ŒPass@kçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05739', 'title': 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More\n  Complicated', 'url': 'https://huggingface.co/papers/2509.05739', 'abstract': "Data poisoning attacks on Large Language Models can target the reasoning process by decomposing triggers, but the models' reasoning capabilities and architectural design provide a form of backdoor robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components.   Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.", 'score': 0, 'issue_id': 5859, 'pub_date': '2025-09-06', 'pub_date_card': {'ru': '6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 6', 'zh': '9æœˆ6æ—¥'}, 'hash': 'bc4e67f715e40467', 'authors': ['Hanna Foerster', 'Ilia Shumailov', 'Yiren Zhao', 'Harsh Chaudhari', 'Jamie Hayes', 'Robert Mullins', 'Yarin Gal'], 'affiliations': ['Anthropic', 'DeepMind', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.05739.jpg', 'data': {'categories': ['#data', '#security', '#reasoning', '#architecture'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', Ğ³Ğ´Ğµ Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ÑƒÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¼Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ±ÑĞºĞ´Ğ¾Ñ€Ğ°Ğ¼."}, 'en': {'title': 'Enhancing Backdoor Robustness in LLMs through Reasoning', 'desc': "This paper discusses data poisoning attacks on Large Language Models (LLMs) that specifically target their reasoning processes. It introduces a novel method called 'decomposed reasoning poison', where attackers modify the reasoning steps without altering the prompts or final outputs. Despite the potential for these stealthy attacks, the paper finds that activating such backdoors to influence final answers is challenging. This is due to the models' inherent robustness, which stems from their advanced reasoning capabilities and the structural separation between reasoning and answer generation."}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›æå‡åé—¨é²æ£’æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°æ®ä¸­æ¯’æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡åˆ†è§£è§¦å‘å™¨æ¥å½±å“æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¯ä»¥æ³¨å…¥è¿™äº›åˆ†è§£çš„æ¯’ç´ ï¼Œä½†è¦å¯é åœ°æ¿€æ´»å®ƒä»¬ä»¥æ”¹å˜æœ€ç»ˆç­”æ¡ˆå´éå¸¸å›°éš¾ã€‚è¿™ç§å›°éš¾æºäºæ¨¡å‹åœ¨å…¶æ€ç»´è¿‡ç¨‹ä¸­èƒ½å¤Ÿä»åé—¨ä¸­æ¢å¤ã€‚æœ€ç»ˆï¼Œç ”ç©¶å‘ç°ï¼Œå…ˆè¿›çš„LLMsçš„æ¨ç†èƒ½åŠ›å’Œæ¨ç†ä¸æœ€ç»ˆç­”æ¡ˆç”Ÿæˆä¹‹é—´çš„æ¶æ„åˆ†ç¦»ï¼Œå½¢æˆäº†ä¸€ç§æ–°å…´çš„åé—¨é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02547', 'title': 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey', 'url': 'https://huggingface.co/papers/2509.02547', 'abstract': 'Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.', 'score': 69, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '04a4d0adade32d34', 'authors': ['Guibin Zhang', 'Hejia Geng', 'Xiaohang Yu', 'Zhenfei Yin', 'Zaibin Zhang', 'Zelin Tan', 'Heng Zhou', 'Zhongzhi Li', 'Xiangyuan Xue', 'Yijiang Li', 'Yifan Zhou', 'Yang Chen', 'Chen Zhang', 'Yutao Fan', 'Zihu Wang', 'Songtao Huang', 'Yue Liao', 'Hongru Wang', 'Mengyue Yang', 'Heng Ji', 'Michael Littman', 'Jun Wang', 'Shuicheng Yan', 'Philip Torr', 'Lei Bai'], 'affiliations': ['Brown University', 'Chinese Academy of Sciences', 'Dalian University of Technology', 'Fudan University', 'Imperial College London', 'National University of Singapore', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'University College London', 'University of Bristol', 'University of California, San Diego', 'University of California, Santa Barbara', 'University of Georgia', 'University of Illinois Urbana-Champaign', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.02547.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning', '#agi', '#survey', '#rl', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (Agentic RL) Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (POMDP) Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Agentic RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼.'}, 'en': {'title': 'Transforming Language Models into Autonomous Decision-Makers', 'desc': 'This paper introduces agentic reinforcement learning (Agentic RL), which changes how large language models (LLMs) operate by allowing them to make autonomous decisions in complex environments. It contrasts traditional single-step Markov Decision Processes (MDPs) used in LLMs with more advanced temporally extended partially observable Markov decision processes (POMDPs) that enable better planning and reasoning. The authors propose a taxonomy that categorizes agentic capabilities like memory and self-improvement, as well as their applications in various tasks. Additionally, the paper provides a resource guide for researchers, summarizing over five hundred studies to outline the current state and future directions of this emerging field.'}, 'zh': {'title': 'ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼šä»è¢«åŠ¨ç”Ÿæˆåˆ°è‡ªä¸»å†³ç­–çš„è½¬å˜', 'desc': 'ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆAgentic RLï¼‰å°†å¤§å‹è¯­è¨€æ¨¡å‹è½¬å˜ä¸ºè‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨æ—¶é—´æ‰©å±•çš„éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰ï¼Œå¢å¼ºäº†è§„åˆ’å’Œæ¨ç†ç­‰èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å•æ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPsï¼‰ç›¸æ¯”ï¼Œä»£ç†å¼ºåŒ–å­¦ä¹ ä½¿å¾—è¯­è¨€æ¨¡å‹ä¸å†æ˜¯è¢«åŠ¨çš„åºåˆ—ç”Ÿæˆå™¨ï¼Œè€Œæ˜¯èƒ½å¤Ÿåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­è‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä½“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„åˆ†ç±»æ³•ï¼Œå›´ç»•æ ¸å¿ƒçš„ä»£ç†èƒ½åŠ›ï¼Œå¦‚è§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€è®°å¿†ã€æ¨ç†ã€è‡ªæˆ‘æ”¹è¿›å’Œæ„ŸçŸ¥è¿›è¡Œç»„ç»‡ã€‚é€šè¿‡æ•´åˆå¼€æºç¯å¢ƒã€åŸºå‡†å’Œæ¡†æ¶ï¼Œæœ¬æ–‡ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®ç”¨çš„å‚è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02544', 'title': 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.02544', 'abstract': "UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.", 'score': 63, 'issue_id': 5686, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '71474173af3c991b', 'authors': ['Haoming Wang', 'Haoyang Zou', 'Huatong Song', 'Jiazhan Feng', 'Junjie Fang', 'Junting Lu', 'Longxiang Liu', 'Qinyu Luo', 'Shihao Liang', 'Shijue Huang', 'Wanjun Zhong', 'Yining Ye', 'Yujia Qin', 'Yuwen Xiong', 'Yuxin Song', 'Zhiyong Wu', 'Bo Li', 'Chen Dun', 'Chong Liu', 'Fuxing Leng', 'Hanbin Wang', 'Hao Yu', 'Haobin Chen', 'Hongyi Guo', 'Jing Su', 'Jingjia Huang', 'Kai Shen', 'Kaiyu Shi', 'Lin Yan', 'Peiyao Zhao', 'Pengfei Liu', 'Qinghao Ye', 'Renjie Zheng', 'Wayne Xin Zhao', 'Wen Heng', 'Wenhao Huang', 'Wenqian Wang', 'Xiaobo Qin', 'Yi Lin', 'Youbin Wu', 'Zehui Chen', 'Zihao Wang', 'Baoquan Zhong', 'Xinchun Zhang', 'Xujing Li', 'Yuanfan Li', 'Zhongkai Zhao', 'Chengquan Jiang', 'Faming Wu', 'Haotian Zhou', 'Jinlin Pang', 'Li Han', 'Qianli Ma', 'Siyao Liu', 'Songhua Cai', 'Wenqi Fu', 'Xin Liu', 'Zhi Zhang', 'Bo Zhou', 'Guoliang Li', 'Jiajun Shi', 'Jiale Yang', 'Jie Tang', 'Li Li', 'Taoran Lu', 'Woyu Lin', 'Xiaokang Tong', 'Xinyao Li', 'Yichi Zhang', 'Yu Miao', 'Zhengxuan Jiang', 'Zili Li', 'Ziyuan Zhao', 'Chenxin Li', 'Dehua Ma', 'Feng Lin', 'Ge Zhang', 'Haihua Yang', 'Hangyu Guo', 'Hongda Zhu', 'Jiaheng Liu', 'Junda Du', 'Kai Cai', 'Kuanye Li', 'Lichen Yuan', 'Meilan Han', 'Minchao Wang', 'Shuyue Guo', 'Tianhao Cheng', 'Xiaobo Ma', 'Xiaojun Xiao', 'Xiaolong Huang', 'Xinjie Chen', 'Yidi Du', 'Yilin Chen', 'Yiwen Wang', 'Zhaojian Li', 'Zhenzhu Yang', 'Zhiyuan Zeng', 'Chaolin Jin', 'Chen Li', 'Hao Chen', 'Haoli Chen', 'Jian Chen', 'Qinghao Zhao', 'Guang Shi'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2509.02544.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#reasoning', '#agents', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'UI-TARS-2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'UI-TARS-2 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¼Ğ°Ñ…Ğ¾Ğ²Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RL Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ GUI-ÑÑ€ĞµĞ´Ñƒ. UI-TARS-2 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ¹Ğ·Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² GUI-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» UI-TARS-2 Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing GUI Agents with UI-TARS-2', 'desc': 'UI-TARS-2 is a new model designed for creating intelligent agents that can interact with graphical user interfaces (GUIs). It tackles key issues like handling large amounts of data, improving learning over multiple interactions, and ensuring stable performance in different environments. The model uses innovative techniques such as a data flywheel for efficient data generation and a hybrid environment that combines various systems for better training. Its performance on various benchmarks shows significant improvements over previous models, indicating its effectiveness in real-world applications.'}, 'zh': {'title': 'UI-TARS-2ï¼šæå‡å›¾å½¢ç”¨æˆ·ç•Œé¢æ™ºèƒ½ä½“çš„æœªæ¥', 'desc': 'UI-TARS-2æ˜¯ä¸€ä¸ªä»¥å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ•°æ®å¯æ‰©å±•æ€§ã€å¤šè½®å¼ºåŒ–å­¦ä¹ å’Œç¯å¢ƒç¨³å®šæ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡ç³»ç»ŸåŒ–çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆã€ç¨³å®šçš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œé›†æˆæ–‡ä»¶ç³»ç»Ÿä¸ç»ˆç«¯çš„æ··åˆGUIç¯å¢ƒï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒUI-TARS-2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶å‰èº«UI-TARS-1.5å’Œå…¶ä»–å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨é•¿æ—¶é—´ä¿¡æ¯æ£€ç´¢ä»»åŠ¡å’Œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·åŒ–æ™ºèƒ½ä½“ä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02479', 'title': 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning', 'url': 'https://huggingface.co/papers/2509.02479', 'abstract': 'SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.', 'score': 62, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'e6fb78d3f5363c7d', 'authors': ['Zhenghai Xue', 'Longtao Zheng', 'Qian Liu', 'Yingru Li', 'Xiaosen Zheng', 'Zejun Ma', 'Bo An'], 'affiliations': ['Nanyang Technological University, Singapore', 'TikTok, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.02479.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#math', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ SimpleTIR, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (Tool-Integrated Reasoning, TIR). ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. SimpleTIR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ.'}, 'en': {'title': 'Stabilizing Multi-Turn Reasoning with SimpleTIR', 'desc': 'The paper presents SimpleTIR, an innovative algorithm designed to enhance the stability of multi-turn Tool-Integrated Reasoning (TIR) training in large language models. It addresses the issue of training instability caused by distributional drift from external tool feedback, which leads to the generation of low-probability tokens and catastrophic gradient explosions. By filtering out void turnsâ€”those that do not produce useful outputsâ€”SimpleTIR prevents harmful gradients from affecting the learning process. As a result, it achieves state-of-the-art performance on math reasoning tasks, significantly improving scores while promoting the discovery of advanced reasoning strategies.'}, 'zh': {'title': 'SimpleTIRï¼šç¨³å®šå¤šè½®æ¨ç†è®­ç»ƒçš„åˆ›æ–°ç®—æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSimpleTIRçš„ç®—æ³•ï¼Œå®ƒé€šè¿‡è¿‡æ»¤æ‰æ— æ•ˆå›åˆæ¥ç¨³å®šå¤šè½®å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰è®­ç»ƒã€‚å¤šè½®TIRåœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ—¶å¸¸å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½å´©æºƒçš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºå¤–éƒ¨å·¥å…·åé¦ˆå¯¼è‡´çš„åˆ†å¸ƒæ¼‚ç§»ã€‚SimpleTIRçš„æ ¸å¿ƒç­–ç•¥æ˜¯è¯†åˆ«å¹¶å»é™¤é‚£äº›æ—¢æ²¡æœ‰ä»£ç å—ä¹Ÿæ²¡æœ‰æœ€ç»ˆç­”æ¡ˆçš„å›åˆï¼Œä»è€Œæœ‰æ•ˆé˜»æ­¢æœ‰å®³çš„é«˜å¹…åº¦æ¢¯åº¦ï¼Œç¨³å®šå­¦ä¹ åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimpleTIRåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00676', 'title': 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model', 'url': 'https://huggingface.co/papers/2509.00676', 'abstract': "Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.", 'score': 56, 'issue_id': 5686, 'pub_date': '2025-08-31', 'pub_date_card': {'ru': '31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 31', 'zh': '8æœˆ31æ—¥'}, 'hash': '804da0110302d00c', 'authors': ['Xiyao Wang', 'Chunyuan Li', 'Jianwei Yang', 'Kai Zhang', 'Bo Liu', 'Tianyi Xiong', 'Furong Huang'], 'affiliations': ['National University of Singapore', 'The Ohio State University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2509.00676.jpg', 'data': {'categories': ['#optimization', '#games', '#rlhf', '#multimodal', '#benchmark', '#reasoning', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaVA-Critic-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 26 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ğº Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Bridging Evaluation and Generation in Multimodal Systems', 'desc': 'This paper introduces a novel approach to improve generative models by applying reinforcement learning (RL) on preference-labeled critic datasets. Traditionally, critic models evaluate outputs but do not generate responses, creating a divide between evaluation and generation tasks. The authors propose a unified model, LLaVA-Critic-R1, which not only serves as a high-performing critic but also functions effectively as a policy model for generating responses. Their findings demonstrate that this integrated approach enhances performance across various visual reasoning benchmarks, showcasing the potential for self-improving multimodal systems.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡ç”Ÿæˆæ¨¡å‹çš„ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿ', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†å¸¦æœ‰åå¥½çš„è¯„è®ºæ•°æ®é›†ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æå‡ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬é‡æ–°ç»„ç»‡è¿™äº›è¯„è®ºæ•°æ®ï¼Œç›´æ¥åœ¨åŸºç¡€ç”Ÿæˆæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¼€å‘å‡ºLLaVA-Critic-R1ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿä¼˜åŒ–åå¥½åˆ¤æ–­çš„å¤šæ¨¡æ€è¯„è®ºæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVA-Critic-R1ä¸ä»…åœ¨è¯„è®ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿˜åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¸ä¸“é—¨çš„æ¨ç†æ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šå®ƒä»¬ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨è¯„è®ºæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯ä»¥åˆ›å»ºä¸€ä¸ªåœ¨è¯„ä¼°å’Œç”Ÿæˆæ–¹é¢éƒ½è¡¨ç°å‡ºè‰²çš„ç»Ÿä¸€æ¨¡å‹ï¼Œæ¨åŠ¨å¤šæ¨¡æ€ç³»ç»Ÿçš„è‡ªæˆ‘æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21496', 'title': 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long\n  Video Understanding', 'url': 'https://huggingface.co/papers/2508.21496', 'abstract': "A benchmark for long-video hallucination identifies and investigates Semantic Aggregation Hallucination (SAH), showing its prevalence in complex and rapidly changing semantic contexts, and proposes strategies to mitigate it.  \t\t\t\t\tAI-generated summary \t\t\t\t Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.", 'score': 48, 'issue_id': 5691, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': 'ca0b1db27fcb23d9', 'authors': ['Hao Lu', 'Jiahao Wang', 'Yaolun Zhang', 'Ruohui Wang', 'Xuanyu Zheng', 'Yepeng Tang', 'Dahua Lin', 'Lewei Lu'], 'affiliations': ['SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.21496.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#hallucinations', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ELV-Halluc Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ (Ğ¡ĞĞ“), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¡ĞĞ“ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑĞ¼ĞµĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¡ĞĞ“, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° DPO.'}, 'en': {'title': 'Tackling Semantic Aggregation Hallucination in Long Videos', 'desc': "This paper introduces a new benchmark called ELV-Halluc, specifically designed to study Semantic Aggregation Hallucination (SAH) in long videos. SAH occurs when models incorrectly aggregate frame-level semantics into event-level groups, particularly in complex and rapidly changing contexts. The research shows that SAH is more prevalent in longer videos due to increased semantic complexity and provides strategies to mitigate this issue. By implementing a positional encoding strategy and a DPO approach, the authors demonstrate a significant reduction in SAH, improving the model's performance on video understanding tasks."}, 'zh': {'title': 'æ­ç¤ºé•¿è§†é¢‘ä¸­çš„è¯­ä¹‰èšåˆå¹»è§‰', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹é•¿è§†é¢‘å¹»è§‰çš„æ–°åŸºå‡†ï¼Œé‡ç‚¹ç ”ç©¶äº†è¯­ä¹‰èšåˆå¹»è§‰ï¼ˆSAHï¼‰ã€‚SAHåœ¨å¤æ‚å’Œå¿«é€Ÿå˜åŒ–çš„è¯­ä¹‰ç¯å¢ƒä¸­å°¤ä¸ºæ™®éï¼Œå¯¼è‡´æ¨¡å‹ç”Ÿæˆä¸è§†é¢‘è¾“å…¥ä¸ä¸€è‡´çš„å†…å®¹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSAHçš„å‘ç”Ÿä¸è¯­ä¹‰å¤æ‚æ€§å¢åŠ æœ‰å…³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä¸ªäº‹ä»¶ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ELV-HallucåŸºå‡†ï¼Œå¹¶æ¢è®¨äº†ç¼“è§£SAHçš„ç­–ç•¥ï¼Œå¦‚ä½ç½®ç¼–ç å’ŒDPOç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01055', 'title': 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use', 'url': 'https://huggingface.co/papers/2509.01055', 'abstract': 'VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2times speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.', 'score': 44, 'issue_id': 5687, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '8d89f7851ae1d950', 'authors': ['Dongfu Jiang', 'Yi Lu', 'Zhuofeng Li', 'Zhiheng Lyu', 'Ping Nie', 'Haozhe Wang', 'Alex Su', 'Hui Chen', 'Kai Zou', 'Chao Du', 'Tianyu Pang', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent', 'National University of Singapore', 'NetMind.AI', 'Sea AI Lab', 'Shanghai University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.01055.jpg', 'data': {'categories': ['#architecture', '#rl', '#open_source', '#agi', '#agents', '#reasoning', '#rlhf', '#training', '#multimodal'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'VerlTool: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VerlTool - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. VerlTool Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ VeRL, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ API, Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° 6 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… ARLT. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ARLT ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RLVR.'}, 'en': {'title': 'Streamlining Agentic Reinforcement Learning with VerlTool', 'desc': 'VerlTool is a new framework designed for Agentic Reinforcement Learning that incorporates tool use, aiming to improve efficiency and performance in various tasks. It addresses the limitations of existing methods by providing a unified system that supports multiple modalities and asynchronous execution, which speeds up processing. The framework allows for easy integration of tools through a modular architecture, making it simpler for researchers to develop and test new ideas. By demonstrating strong performance across different domains, VerlTool encourages broader adoption and innovation in the field of reinforcement learning with tools.'}, 'zh': {'title': 'VerlToolï¼šæå‡ä»£ç†å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'VerlToolæ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œä¸“æ³¨äºå…·æœ‰å·¥å…·ä½¿ç”¨çš„ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­çš„ä½æ•ˆé—®é¢˜ã€‚å®ƒé€šè¿‡ç³»ç»Ÿè®¾è®¡åŸåˆ™ï¼Œæä¾›äº†å››ä¸ªå…³é”®è´¡çŒ®ï¼ŒåŒ…æ‹¬ä¸å¯éªŒè¯å¥–åŠ±çš„ä¸Šæ¸¸å¯¹é½ã€ç»Ÿä¸€çš„å·¥å…·ç®¡ç†ã€å¼‚æ­¥æ‰§è¡Œä»¥æé«˜é€Ÿåº¦ï¼Œä»¥åŠåœ¨å¤šä¸ªé¢†åŸŸçš„ç«äº‰æ€§è¡¨ç°è¯„ä¼°ã€‚è¯¥æ¡†æ¶å°†ä»£ç†å¼ºåŒ–å­¦ä¹ å½¢å¼åŒ–ä¸ºå¤šè½®è½¨è¿¹ï¼Œæ”¯æŒå¤šæ¨¡æ€è§‚å¯Ÿä»¤ç‰Œï¼Œè¶…è¶Šäº†å•è½®äº¤äº’çš„é™åˆ¶ã€‚VerlToolçš„æ¨¡å—åŒ–æ’ä»¶æ¶æ„ä½¿å¾—å·¥å…·é›†æˆå˜å¾—å¿«é€Ÿä¸”ç®€å•ï¼Œæ˜¾è‘—é™ä½äº†å¼€å‘æˆæœ¬ï¼Œä¸ºå·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01215', 'title': 'POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion', 'url': 'https://huggingface.co/papers/2509.01215', 'abstract': "A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model is available at https://github.com/Tencent/POINTS-Reader.", 'score': 37, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '1f731f4067d86ef7', 'authors': ['Yuan Liu', 'Zhongyin Zhao', 'Le Tian', 'Haicheng Wang', 'Xubing Ye', 'Yangxiu You', 'Zilin Yu', 'Chuhan Wu', 'Xiao Zhou', 'Yang Yu', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01215.jpg', 'data': {'categories': ['#dataset', '#data', '#optimization', '#synthetic', '#training'], 'emoji': 'ğŸ“„', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ POINTS-Reader Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Document Extraction with Synthetic Data and Self-Improvement', 'desc': 'This paper presents a new framework for creating high-quality datasets and models for document extraction using synthetic data generation and iterative self-improvement. It addresses the challenges of manual data labeling and the limitations of existing models in handling complex document formats. The proposed method generates diverse synthetic data to train an initial model, which is then refined through a self-improvement process that involves annotating real documents and verifying the quality of these annotations. The resulting model, POINTS-Reader, demonstrates superior performance compared to existing models, making it a valuable tool for document conversion tasks.'}, 'zh': {'title': 'åˆæˆæ•°æ®é©±åŠ¨çš„æ–‡æ¡£æå–æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ„å»ºé«˜è´¨é‡æ–‡æ¡£æå–æ•°æ®é›†å’Œæ¨¡å‹çš„æ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆæ•°æ®ç”Ÿæˆå’Œè¿­ä»£è‡ªæˆ‘æ”¹è¿›çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”Ÿæˆå¤§è§„æ¨¡å¤šæ ·çš„åˆæˆæ•°æ®ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿä»¥ç»Ÿä¸€æ ¼å¼æå–å…³é”®ä¿¡æ¯ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ï¼Œå°†åˆæ­¥è®­ç»ƒçš„æ¨¡å‹é€‚åº”çœŸå®æ–‡æ¡£ã€‚é€šè¿‡å¯¹çœŸå®æ–‡æ¡£è¿›è¡Œæ ‡æ³¨ã€è´¨é‡éªŒè¯å’Œæ¨¡å‹é‡è®­ç»ƒï¼Œé€æ­¥æå‡æ¨¡å‹çš„è½¬æ¢èƒ½åŠ›å’Œç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œè®­ç»ƒå‡ºçš„POINTS-Readeræ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„å…¬å…±å’Œä¸“æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02208', 'title': 'Baichuan-M2: Scaling Medical Capability with Large Verifier System', 'url': 'https://huggingface.co/papers/2509.02208', 'abstract': 'A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.', 'score': 25, 'issue_id': 5687, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'f78da0ee0b5088cb', 'authors': ['Baichuan-M2 Team', ':', 'Chengfeng Dou', 'Chong Liu', 'Fan Yang', 'Fei Li', 'Jiyuan Jia', 'Mingyang Chen', 'Qiang Ju', 'Shuai Wang', 'Shunya Dang', 'Tianpeng Li', 'Xiangrong Zeng', 'Yijie Zhou', 'Chenzheng Zhu', 'Da Pan', 'Fei Deng', 'Guangwei Ai', 'Guosheng Dong', 'Hongda Zhang', 'Jinyang Tai', 'Jixiang Hong', 'Kai Lu', 'Linzhuang Sun', 'Peidong Guo', 'Qian Ma', 'Rihui Xin', 'Shihui Yang', 'Shusen Zhang', 'Yichuan Mo', 'Zheng Liang', 'Zhishou Zhang', 'Hengfu Cui', 'Zuyi Zhu', 'Xiaochuan Wang'], 'affiliations': ['Baichuan-M2 Team'], 'pdf_title_img': 'assets/pdf/title_img/2509.02208.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#rl', '#open_source', '#reasoning', '#agents', '#alignment', '#training', '#healthcare'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ LLM Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Baichuan-M2 Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Baichuan-M2 Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ‚ĞµÑÑ‚Ğµ HealthBench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ.'}, 'en': {'title': 'Revolutionizing Clinical Decision-Making with Dynamic Verification in AI', 'desc': 'This paper presents a dynamic verification framework that enhances the performance of large medical language models (LLMs) in real-world clinical settings using reinforcement learning. The framework addresses the limitations of traditional static benchmarks by incorporating a Patient Simulator and a Clinical Rubrics Generator, which create realistic clinical scenarios and dynamic evaluation metrics. The authors introduce Baichuan-M2, a 32B-parameter model trained with an advanced Group Relative Policy Optimization algorithm, which significantly outperforms existing models on the HealthBench benchmark. This work highlights the importance of dynamic verification systems in aligning LLM capabilities with practical healthcare applications, setting a new standard for medical AI performance.'}, 'zh': {'title': 'åŠ¨æ€éªŒè¯æå‡åŒ»ç–—AIå†³ç­–èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€éªŒè¯æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ–°ç®—æ³•æå‡å¤§å‹åŒ»ç–—è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸´åºŠå†³ç­–ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„é™æ€åŸºå‡†æµ‹è¯•æ— æ³•æœ‰æ•ˆåæ˜ åŒ»ç–—å’¨è¯¢çš„åŠ¨æ€äº’åŠ¨ç‰¹æ€§ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒ…å«æ‚£è€…æ¨¡æ‹Ÿå™¨å’Œä¸´åºŠè¯„åˆ†ç”Ÿæˆå™¨çš„ç³»ç»Ÿã€‚é€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥å’Œæ”¹è¿›çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬å¼€å‘äº†Baichuan-M2æ¨¡å‹ï¼Œå¹¶åœ¨HealthBenchä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºå¤§çš„åŠ¨æ€éªŒè¯ç³»ç»Ÿå¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ä¸å®é™…ä¸´åºŠåº”ç”¨å¯¹é½è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01563', 'title': 'Kwai Keye-VL 1.5 Technical Report', 'url': 'https://huggingface.co/papers/2509.01563', 'abstract': "Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.", 'score': 23, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': 'b033203f893abafc', 'authors': ['Biao Yang', 'Bin Wen', 'Boyang Ding', 'Changyi Liu', 'Chenglong Chu', 'Chengru Song', 'Chongling Rao', 'Chuan Yi', 'Da Li', 'Dunju Zang', 'Fan Yang', 'Guorui Zhou', 'Guowang Zhang', 'Han Shen', 'Hao Peng', 'Haojie Ding', 'Hao Wang', 'Hengrui Ju', 'Jiaming Huang', 'Jiangxia Cao', 'Jiankang Chen', 'Jingyun Hua', 'Kaibing Chen', 'Kaiyu Jiang', 'Kaiyu Tang', 'Kun Gai', 'Muhao Wei', 'Qiang Wang', 'Ruitao Wang', 'Sen Na', 'Shengnan Zhang', 'Siyang Mao', 'Sui Huang', 'Tianke Zhang', 'Tingting Gao', 'Wei Chen', 'Wei Yuan', 'Xiangyu Wu', 'Xiao Hu', 'Xingyu Lu', 'Yi-Fan Zhang', 'Yiping Yang', 'Yulong Chen', 'Zeyi Lu', 'Zhenhua Wu', 'Zhixin Ling', 'Zhuoran Yang', 'Ziming Li', 'Di Xu', 'Haixuan Gao', 'Hang Li', 'Jing Wang', 'Lejian Ren', 'Qigen Hu', 'Qianqian Wang', 'Shiyao Wang', 'Xinchen Luo', 'Yan Li', 'Yuhang Hu', 'Zixing Zhang'], 'affiliations': ['Kuaishou Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.01563.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#long_context', '#rl', '#training', '#alignment'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Keye-VL-1.5 Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Keye-VL-1.5 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Slow-Fast, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ - Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 128 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸĞ¾ÑĞ»ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Video Understanding with Keye-VL-1.5', 'desc': 'Keye-VL-1.5 is a new model designed to improve video understanding by using a Slow-Fast encoding strategy that optimizes how videos are processed. This model employs a progressive pre-training approach that allows it to handle longer videos and more complex visual information effectively. Additionally, it includes a post-training pipeline that enhances reasoning capabilities and aligns with human preferences through advanced training techniques. Overall, Keye-VL-1.5 outperforms existing models in video tasks while still performing well in general multimodal applications.'}, 'zh': {'title': 'Keye-VL-1.5ï¼šè§†é¢‘ç†è§£çš„æ–°çªç ´', 'desc': 'Keye-VL-1.5é€šè¿‡æ…¢-å¿«ç¼–ç ç­–ç•¥ã€æ¸è¿›å¼é¢„è®­ç»ƒå’Œåè®­ç»ƒæ¨ç†æ”¹è¿›ï¼Œæå‡äº†è§†é¢‘ç†è§£èƒ½åŠ›ã€‚æ…¢-å¿«ç¼–ç ç­–ç•¥æ ¹æ®å¸§é—´ç›¸ä¼¼æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå¤„ç†å…³é”®å¸§æ—¶ä½¿ç”¨é«˜åˆ†è¾¨ç‡ï¼Œè€Œå¯¹é™æ€å¸§åˆ™ä½¿ç”¨ä½åˆ†è¾¨ç‡ä»¥å¢åŠ æ—¶é—´è¦†ç›–ã€‚æ¸è¿›å¼é¢„è®­ç»ƒæ–¹æ³•å°†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä»8Kæ‰©å±•åˆ°128Kï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘å’Œæ›´å¤æ‚çš„è§†è§‰å†…å®¹ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒKeye-VL-1.5åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01363', 'title': 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic', 'url': 'https://huggingface.co/papers/2509.01363', 'abstract': "Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: v_{reason} = theta_{GRPO} - theta_{SFT}. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.", 'score': 20, 'issue_id': 5688, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '1110b5cc006571ff', 'authors': ['Mohammad Zbeeb', 'Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['American University of Beirut (AUB)', 'King Abdullah University of Science and Technology (KAUST)'], 'pdf_title_img': 'assets/pdf/title_img/2509.01363.jpg', 'data': {'categories': ['#benchmark', '#training', '#rl', '#open_source', '#transfer_learning', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Transfer Reasoning Skills with Task Vectors!', 'desc': 'This paper explores how reasoning skills learned through reinforcement learning can be extracted and reused in other models. The authors create a task vector that represents the reasoning capabilities gained from reinforcement learning, allowing it to be transferred to different models. By applying this vector to instruction-tuned models, they achieve significant performance improvements on various reasoning tasks. This approach demonstrates a cost-effective method to enhance model performance by leveraging previously acquired knowledge without the need for extensive retraining.'}, 'zh': {'title': 'æå–æ¨ç†èƒ½åŠ›ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»å¼ºåŒ–å­¦ä¹ ä¸­æå–æ¨ç†èƒ½åŠ›ï¼Œå¹¶å°†å…¶ä½œä¸ºä»»åŠ¡å‘é‡è½¬ç§»åˆ°å…¶ä»–æ¨¡å‹ä¸­ï¼Œä»¥æé«˜åœ¨ä¸åŒåŸºå‡†ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç›¸åŒåˆå§‹åŒ–çš„Qwen2.5æ¨¡å‹ï¼Œä¸€ä¸ªç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¦ä¸€ä¸ªç»è¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚é€šè¿‡è®¡ç®—è¿™ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°å·®å¼‚ï¼Œæˆ‘ä»¬æå–äº†ä¸€ä¸ªæ¨ç†å‘é‡ï¼Œè¯¥å‘é‡èƒ½å¤Ÿæ•æ‰åˆ°å¼ºåŒ–å­¦ä¹ æ‰€å¸¦æ¥çš„æ¨ç†èƒ½åŠ›ã€‚å°†è¿™ä¸ªå‘é‡æ·»åŠ åˆ°å…¼å®¹çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸­ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02534', 'title': 'Jointly Reinforcing Diversity and Quality in Language Model Generations', 'url': 'https://huggingface.co/papers/2509.02534', 'abstract': 'DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.', 'score': 18, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'b3e51a0003bb3957', 'authors': ['Tianjian Li', 'Yiming Zhang', 'Ping Yu', 'Swarnadeep Saha', 'Daniel Khashabi', 'Jason Weston', 'Jack Lanchantin', 'Tianlu Wang'], 'affiliations': ['Carnegie Mellon University', 'FAIR', 'Johns Hopkins University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2509.02534.jpg', 'data': {'categories': ['#games', '#story_generation', '#rlhf', '#optimization', '#rl', '#training'], 'emoji': 'ğŸŒˆ', 'ru': {'title': 'DARLING: ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ¸', 'desc': 'DARLING - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. DARLING Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DARLING Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RL, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹.'}, 'en': {'title': 'Enhancing Creativity with Diversity-Aware Reinforcement Learning', 'desc': 'DARLING is a framework designed to improve the outputs of large language models by focusing on both quality and diversity. Traditional post-training methods often enhance accuracy but limit the variety of responses, which is crucial for creative tasks. DARLING addresses this by using a learned partition function to assess diversity, allowing the model to generate responses that are not only accurate but also unique. Experiments show that DARLING outperforms standard reinforcement learning approaches, leading to better quality and more diverse outputs across various tasks.'}, 'zh': {'title': 'DARLINGï¼šæå‡è¯­è¨€æ¨¡å‹è¾“å‡ºçš„è´¨é‡ä¸å¤šæ ·æ€§', 'desc': 'DARLINGæ˜¯ä¸€ä¸ªå…³æ³¨å¤šæ ·æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„è¾“å‡ºè´¨é‡å’Œå¤šæ ·æ€§ã€‚ä¼ ç»Ÿçš„åè®­ç»ƒæ–¹æ³•å¾€å¾€åªå…³æ³¨å‡†ç¡®æ€§å’Œå®ç”¨æ€§ï¼Œå¯¼è‡´è¾“å‡ºçš„å¤šæ ·æ€§é™ä½ã€‚DARLINGé€šè¿‡å¼•å…¥å­¦ä¹ çš„åˆ†åŒºå‡½æ•°æ¥è¡¡é‡å¤šæ ·æ€§ï¼Œå¹¶åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç»“åˆè´¨é‡å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä¸”ç‹¬ç‰¹çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARLINGåœ¨éå¯éªŒè¯å’Œå¯éªŒè¯ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„è¾“å‡ºåœ¨è´¨é‡å’Œæ–°é¢–æ€§ä¸Šå‡ä¼˜äºä»…å…³æ³¨è´¨é‡çš„åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02522', 'title': 'Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR', 'url': 'https://huggingface.co/papers/2509.02522', 'abstract': 'PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, a novel RLVR framework that achieves imPlicit Actor Critic coupling via a Supervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.', 'score': 18, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '824c46ed359a21d1', 'authors': ['Jiaming Li', 'Longze Chen', 'Ze Gong', 'Yukun Chen', 'Lu Wang', 'Wanwei He', 'Run Luo', 'Min Yang'], 'affiliations': ['Ritzz-AI', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.02522.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rl', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PACS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğ¹ RLVR', 'desc': 'PACS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ RLVR ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. PACS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ¾Ğ»Ğ¸ Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RLVR.'}, 'en': {'title': 'PACS: Transforming RLVR into Supervised Learning for Better Reasoning', 'desc': 'PACS is a new framework that reformulates Reinforcement Learning with Verifiable Rewards (RLVR) as a supervised learning task, which enhances the stability and efficiency of training large language models (LLMs) for reasoning tasks. By treating outcome rewards as predictable labels, PACS transforms the RLVR problem into a supervised learning problem, optimizing it with cross-entropy loss. This approach allows for implicit coupling of actor and critic roles, leading to more stable policy updates and improved training outcomes. Benchmark results show that PACS significantly outperforms traditional RLVR methods like PPO and GRPO in mathematical reasoning tasks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'PACSï¼šç¨³å®šé«˜æ•ˆçš„æ¨ç†è®­ç»ƒæ–°æ¡†æ¶', 'desc': 'PACSæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œå®ƒå°†RLVRé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä»è€Œæé«˜äº†å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚é€šè¿‡å°†ç»“æœå¥–åŠ±è§†ä¸ºå¯é¢„æµ‹çš„æ ‡ç­¾ï¼ŒPACSå°†RLVRé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªåŸºäºç­–ç•¥æ¨¡å‹çš„åˆ†æ•°å‡½æ•°çš„ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚è¯¦ç»†çš„æ¢¯åº¦åˆ†æè¡¨æ˜ï¼Œè¿™ç§ç›‘ç£å½¢å¼æœ¬è´¨ä¸Šæ¢å¤äº†ç»å…¸çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼ŒåŒæ—¶éšå¼åœ°è€¦åˆäº†æ¼”å‘˜å’Œè¯„è®ºè€…çš„è§’è‰²ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å®šå’Œé«˜æ•ˆçš„è®­ç»ƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒPACSçš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„RLVRåŸºçº¿ï¼Œå¦‚PPOå’ŒGRPOï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨ç†æ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00605', 'title': 'Gated Associative Memory: A Parallel O(N) Architecture for Efficient\n  Sequence Modeling', 'url': 'https://huggingface.co/papers/2509.00605', 'abstract': 'Gated Associative Memory (GAM) network offers a linear complexity alternative to the Transformer architecture, improving training speed and achieving competitive validation perplexity.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling.', 'score': 18, 'issue_id': 5697, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': '874c41bbbff9e11e', 'authors': ['Rishiraj Acharya'], 'affiliations': ['Independent Researcher'], 'pdf_title_img': 'assets/pdf/title_img/2509.00605.jpg', 'data': {'categories': ['#long_context', '#training', '#architecture', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'GAM: Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Transformer Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Gated Associative Memory (GAM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Transformer Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. GAM Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¹ self-attention Ğ´Ğ²ÑƒĞ¼Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸: ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ñ‘Ñ€Ñ‚ĞºĞ¾Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GAM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Transformer Ğ¸ Mamba Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… WikiText-2 Ğ¸ TinyStories. GAM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'GAM: A Faster, Smarter Alternative to Transformers', 'desc': "The Gated Associative Memory (GAM) network presents a new approach to sequence modeling that operates with linear complexity, making it faster than traditional Transformer models. By replacing the self-attention mechanism with a combination of causal convolution and associative memory retrieval, GAM effectively captures both local and global context in data. This architecture allows for dynamic integration of information, enhancing the model's ability to process sequences efficiently. Experimental results show that GAM not only improves training speed but also achieves competitive performance in terms of validation perplexity on various datasets."}, 'zh': {'title': 'GAMï¼šé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡æ–°é€‰æ‹©', 'desc': 'Gated Associative Memory (GAM) ç½‘ç»œæ˜¯ä¸€ç§æ–°å‹çš„åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼ˆO(N)ï¼‰ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„Transformeræ¶æ„ï¼ˆO(N^2)ï¼‰ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶æ›´ä¸ºé«˜æ•ˆã€‚GAMé€šè¿‡ä¸¤ä¸ªå¹¶è¡Œè·¯å¾„æ›¿ä»£äº†è‡ªæ³¨æ„åŠ›å±‚ï¼šä¸€ä¸ªå› æœå·ç§¯ç”¨äºæ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œå¦ä¸€ä¸ªå¹¶è¡Œçš„å…³è”è®°å¿†æ£€ç´¢æœºåˆ¶ç”¨äºå»ºæ¨¡å…¨å±€æ¨¡å¼ã€‚è¿™ç§è·¯å¾„é€šè¿‡é—¨æ§æœºåˆ¶åŠ¨æ€èåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°ç»“åˆæ¯ä¸ªæ ‡è®°çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAMåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šä¼˜äºæ ‡å‡†Transformeræ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„éªŒè¯å›°æƒ‘åº¦ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºåºåˆ—å»ºæ¨¡çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02563', 'title': 'DynaGuard: A Dynamic Guardrail Model With User-Defined Policies', 'url': 'https://huggingface.co/papers/2509.02563', 'abstract': 'Dynamic guardian models evaluate text based on user-defined policies, offering fast and accurate detection of both static harms and free-form policy violations.  \t\t\t\t\tAI-generated summary \t\t\t\t Guardian models are used to supervise and moderate the outputs of user-facing chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian models like LlamaGuard detect predefined, static categories of harms. We propose dynamic guardian models that evaluate text based on user-defined policies, making them useful for different application domains that are not addressed by standard guardian models. Our dynamic guardian models can be used for fast detection of policy violations or with chain-of-thought reasoning that articulates and justifies the model outputs. Our dynamic guardian models match static models in detection accuracy for static harm categories while identifying violations of free-form policies with accuracy comparable to frontier reasoning models in a fraction of the time.', 'score': 15, 'issue_id': 5696, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'b4dda88cf231d2fa', 'authors': ['Monte Hoover', 'Vatsal Baherwani', 'Neel Jain', 'Khalid Saifullah', 'Joseph Vincent', 'Chirag Jain', 'Melissa Kazemi Rad', 'C. Bayan Bruss', 'Ashwinee Panda', 'Tom Goldstein'], 'affiliations': ['Capital One', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2509.02563.jpg', 'data': {'categories': ['#reasoning', '#ethics', '#alignment', '#agents', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½Ğ¸ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Text Evaluation with Dynamic Guardian Models', 'desc': 'Dynamic guardian models are advanced tools that assess text according to specific rules set by users, allowing for quick and precise identification of both fixed harms and flexible policy breaches. Unlike traditional models that only recognize predefined categories of issues, these dynamic models adapt to various contexts and requirements. They not only ensure compliance with user-defined standards but also provide reasoning behind their evaluations, enhancing transparency. Furthermore, they achieve similar accuracy to static models for known harms while efficiently detecting more complex violations, making them suitable for diverse applications.'}, 'zh': {'title': 'åŠ¨æ€å®ˆæŠ¤æ¨¡å‹ï¼šçµæ´»çš„æ–‡æœ¬è¯„ä¼°ä¸æ”¿ç­–æ£€æµ‹', 'desc': 'åŠ¨æ€å®ˆæŠ¤æ¨¡å‹æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ”¿ç­–è¯„ä¼°æ–‡æœ¬ï¼Œèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°æ£€æµ‹é™æ€å±å®³å’Œè‡ªç”±å½¢å¼çš„æ”¿ç­–è¿è§„ã€‚ä¸æ ‡å‡†å®ˆæŠ¤æ¨¡å‹ä¸åŒï¼ŒåŠ¨æ€å®ˆæŠ¤æ¨¡å‹é€‚ç”¨äºä¸åŒçš„åº”ç”¨é¢†åŸŸï¼Œæä¾›çµæ´»çš„ç›‘ç£å’Œè°ƒèŠ‚åŠŸèƒ½ã€‚å®ƒä»¬ä¸ä»…èƒ½å¿«é€Ÿæ£€æµ‹æ”¿ç­–è¿è§„ï¼Œè¿˜èƒ½é€šè¿‡æ¨ç†é“¾æ¡æ¸…æ™°åœ°é˜è¿°å’Œè§£é‡Šæ¨¡å‹çš„è¾“å‡ºã€‚åŠ¨æ€å®ˆæŠ¤æ¨¡å‹åœ¨é™æ€å±å®³æ£€æµ‹çš„å‡†ç¡®æ€§ä¸Šä¸é™æ€æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨è‡ªç”±å½¢å¼æ”¿ç­–çš„è¯†åˆ«ä¸Šä¹Ÿèƒ½è¾¾åˆ°å‰æ²¿æ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸”é€Ÿåº¦æ›´å¿«ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02333', 'title': 'DCPO: Dynamic Clipping Policy Optimization', 'url': 'https://huggingface.co/papers/2509.02333', 'abstract': "DCPO, a novel reinforcement learning framework, enhances large language models by dynamically adjusting clipping bounds and standardizing rewards, leading to improved performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.", 'score': 15, 'issue_id': 5691, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '3a6fa32ebaa3d08e', 'authors': ['Shihui Yang', 'Chengfeng Dou', 'Peidong Guo', 'Kai Lu', 'Qiang Ju', 'Fei Deng', 'Rihui Xin'], 'affiliations': ['Baichuan.inc'], 'pdf_title_img': 'assets/pdf/title_img/2509.02333.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#rlhf', '#training', '#optimization', '#rl'], 'emoji': 'ğŸš€', 'ru': {'title': 'DCPO: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DCPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. DCPO Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Dynamic Clipping for Enhanced Learning in Language Models', 'desc': 'DCPO is a new reinforcement learning framework designed to improve large language models by dynamically adjusting clipping bounds and standardizing rewards. This approach addresses the issue of zero gradients that can occur with fixed clipping bounds and identical rewards, which hinder effective learning. By adapting clipping strategies based on token-specific probabilities and smoothing reward standardization, DCPO enhances exploration and utilization of generated responses. The framework has demonstrated state-of-the-art performance across multiple benchmarks, significantly improving training efficiency and reducing clipping ratios compared to previous methods.'}, 'zh': {'title': 'åŠ¨æ€å‰ªåˆ‡ç­–ç•¥ä¼˜åŒ–ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡', 'desc': 'DCPOæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€è°ƒæ•´å‰ªåˆ‡è¾¹ç•Œå’Œæ ‡å‡†åŒ–å¥–åŠ±æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­ç”±äºå›ºå®šå‰ªåˆ‡è¾¹ç•Œå¯¼è‡´çš„é›¶æ¢¯åº¦é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ¢¯åº¦æ›´æ–°çš„æœ‰æ•ˆæ€§ã€‚DCPOå¼•å…¥äº†ä¸€ç§åŠ¨æ€å‰ªåˆ‡ç­–ç•¥ï¼Œæ ¹æ®ç‰¹å®šçš„å…ˆéªŒæ¦‚ç‡è‡ªé€‚åº”è°ƒæ•´å‰ªåˆ‡è¾¹ç•Œï¼Œä¿ƒè¿›äº†ä»¤ç‰Œçº§åˆ«çš„æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå“åº”çš„æœ‰æ•ˆåˆ©ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02460', 'title': 'GenCompositor: Generative Video Compositing with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2509.02460', 'abstract': 'A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.  \t\t\t\t\tAI-generated summary \t\t\t\t Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.', 'score': 14, 'issue_id': 5688, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'cbe52bb6a0af85b6', 'authors': ['Shuzhou Yang', 'Xiaoyu Li', 'Xiaodong Cun', 'Guangzhi Wang', 'Lingen Li', 'Ying Shan', 'Jian Zhang'], 'affiliations': ['ARC Lab, Tencent', 'GVC Lab, Great Bay University', 'SECE, Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.02460.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ½Ğ³: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ½Ğ³Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer (DiT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²ĞµÑ‚Ğ²ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ°, Ğ±Ğ»Ğ¾Ğº ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ DiT Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (ERoPE) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VideoComp, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 61 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Automating Video Compositing with Adaptive Diffusion Transformers', 'desc': 'This paper presents a new method for automating video compositing using a Diffusion Transformer (DiT) pipeline. The approach allows for the adaptive injection of identity and motion information into videos, enabling user customization of dynamic elements. By incorporating a background preservation branch and a DiT fusion block, the method maintains video consistency while enhancing the integration of foreground and background elements. The authors also introduce a new dataset, VideoComp, to support their task, demonstrating that their method outperforms existing solutions in terms of fidelity and consistency.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–è§†é¢‘åˆæˆçš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformerï¼‰ç®¡é“ï¼Œç”¨äºè‡ªåŠ¨åŒ–è§†é¢‘åˆæˆã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”æ³¨å…¥èº«ä»½å’Œè¿åŠ¨ä¿¡æ¯ï¼Œä¿æŒè§†é¢‘çš„ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸ç”¨æˆ·è¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶ã€‚ä¼ ç»Ÿçš„è§†é¢‘åˆæˆéœ€è¦å¤§é‡äººåŠ›å’Œä¸“ä¸šçŸ¥è¯†ï¼Œè€Œè¿™ç§ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—ç¼©çŸ­åˆ¶ä½œå‘¨æœŸï¼Œé™ä½æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘åˆæˆçš„ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01644', 'title': 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning', 'url': 'https://huggingface.co/papers/2509.01644', 'abstract': "OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.", 'score': 14, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '0197aa14702197c7', 'authors': ['Yanqing Liu', 'Xianhang Li', 'Letian Zhang', 'Zirui Wang', 'Zeyu Zheng', 'Yuyin Zhou', 'Cihang Xie'], 'affiliations': ['Apple', 'University of California Berkeley', 'University of California Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2509.01644.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#training', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'OpenVision 2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ OpenVision, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑƒĞ´Ğ°Ğ»ĞµĞ½ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, OpenVision 2 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Streamlined Efficiency: OpenVision 2 Reimagined', 'desc': "OpenVision 2 is a streamlined version of the original OpenVision architecture that eliminates the text encoder and contrastive loss, focusing solely on a generative training approach with captioning loss. This simplification leads to significant improvements in training efficiency, reducing both training time and memory usage while maintaining competitive performance on multimodal benchmarks. For instance, it achieves a 1.5x reduction in training time and a 1.8x decrease in memory consumption, allowing for larger batch sizes. The architecture's ability to scale to over 1 billion parameters suggests a promising direction for future multimodal foundation models."}, 'zh': {'title': 'ç®€åŒ–æ¶æ„ï¼Œæå‡æ•ˆç‡â€”â€”OpenVision 2', 'desc': 'OpenVision 2é€šè¿‡å»é™¤æ–‡æœ¬ç¼–ç å™¨å’Œå¯¹æ¯”æŸå¤±ï¼Œç®€åŒ–äº†åŸæœ‰æ¶æ„ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ¨¡å‹ä»…ä¿ç•™äº†ç”Ÿæˆæ€§è®­ç»ƒä¿¡å·çš„å­—å¹•æŸå¤±ï¼Œè¡¨ç°å‡ºä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚å°½ç®¡è¿›è¡Œäº†ç®€åŒ–ï¼ŒOpenVision 2åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä»ç„¶è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ç§è½»é‡çº§çš„ç”Ÿæˆæ€§èŒƒå¼å¯¹æœªæ¥å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01440', 'title': 'Benchmarking Optimizers for Large Language Model Pretraining', 'url': 'https://huggingface.co/papers/2509.01440', 'abstract': 'A comprehensive evaluation of recent optimization techniques for Large Language Models provides guidance on selecting the best optimizer for different pretraining scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods.', 'score': 12, 'issue_id': 5690, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '086d91ca4e22a4be', 'authors': ['Andrei Semenov', 'Matteo Pagliardini', 'Martin Jaggi'], 'affiliations': ['EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2509.01440.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ LLM: Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Optimizing Large Language Models: A Guide to Choosing the Right Optimizer', 'desc': 'This paper evaluates various optimization techniques for Large Language Models (LLMs) to help researchers and practitioners choose the best optimizer for different pretraining scenarios. It addresses the challenges of comparing methods due to varying experimental protocols and provides a systematic analysis by varying model size, batch size, and training duration. The study offers insights into which optimizers yield faster convergence and reduced dependency on hyperparameters. Additionally, the authors release their code for reproducibility, aiming to support future research in optimization methods.'}, 'zh': {'title': 'é€‰æ‹©æœ€ä½³ä¼˜åŒ–å™¨ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ—¨åœ¨ä¸ºä¸åŒçš„é¢„è®­ç»ƒåœºæ™¯é€‰æ‹©æœ€ä½³ä¼˜åŒ–å™¨æä¾›æŒ‡å¯¼ã€‚ç ”ç©¶ä¸­ç³»ç»Ÿåœ°å˜åŒ–äº†æ¨¡å‹å¤§å°ã€æ‰¹é‡å¤§å°å’Œè®­ç»ƒæ—¶é•¿ï¼Œä»¥ä¾¿å¯¹å„ç§ä¼˜åŒ–æ–¹æ³•è¿›è¡Œæ ‡å‡†åŒ–æ¯”è¾ƒã€‚é€šè¿‡å¯¹æ¯ç§æ–¹æ³•çš„ç»†è‡´è°ƒä¼˜ï¼Œæœ¬æ–‡ä¸ºå®è·µè€…æä¾›äº†åœ¨ç‰¹å®šåœºæ™¯ä¸‹é€‰æ‹©ä¼˜åŒ–å™¨çš„å»ºè®®ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æŒ‡å‡ºäº†æœªæ¥ä¼˜åŒ–ç ”ç©¶çš„æœ‰å¸Œæœ›æ–¹å‘ï¼Œå¹¶é€šè¿‡å‘å¸ƒä»£ç å’Œå®éªŒç»“æœï¼Œç¡®ä¿äº†ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02040', 'title': 'Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation', 'url': 'https://huggingface.co/papers/2509.02040', 'abstract': 'Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.', 'score': 11, 'issue_id': 5686, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '085dff767b3e88ce', 'authors': ['Guangzeng Han', 'Weisi Liu', 'Xiaolei Huang'], 'affiliations': ['Department of Computer Science, University of Memphis'], 'pdf_title_img': 'assets/pdf/title_img/2509.02040.jpg', 'data': {'categories': ['#optimization', '#data', '#synthetic', '#training', '#dataset'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ NLP', 'desc': 'Genetic Prompt - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ° ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑĞºÑ€ĞµÑ‰Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Genetic Prompt Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Genetic Prompt: Evolving Synthetic Data for NLP Excellence', 'desc': "The paper introduces Genetic Prompt, a framework that enhances the quality and diversity of synthetic data in natural language processing (NLP) by integrating genetic algorithms with large language models (LLMs). It treats semantic text attributes as gene sequences, applying genetic operations like crossover and mutation to generate diverse data combinations. This method improves the synthetic data's resemblance to real-world distributions, which is crucial for training effective models. Additionally, the framework incorporates an active learning strategy to optimize the selection of parent data, leading to better performance in various NLP tasks, especially in scenarios with class imbalance."}, 'zh': {'title': 'é—ä¼ æç¤ºï¼šæå‡åˆæˆæ•°æ®è´¨é‡ä¸å¤šæ ·æ€§', 'desc': 'Genetic Promptæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å°†é—ä¼ ç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œæå‡äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åˆæˆæ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•å°†è¯­ä¹‰æ–‡æœ¬å±æ€§è§†ä¸ºåŸºå› åºåˆ—ï¼Œå¹¶åˆ©ç”¨LLMæ¨¡æ‹Ÿäº¤å‰å’Œçªå˜æ“ä½œï¼Œä»è€Œç”Ÿæˆæ–°çš„å±æ€§ç»„åˆã€‚é€šè¿‡è¿™ç§é—ä¼ è¿‡ç¨‹ï¼Œåˆæˆæ•°æ®çš„åˆ†å¸ƒæ›´æ¥è¿‘çœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œä¼˜åŒ–äº†ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenetic Promptåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ï¼Œå°¤å…¶åœ¨ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œèåˆåˆæˆæ•°æ®ä¸åŸå§‹è®­ç»ƒé›†èƒ½æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01052', 'title': 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games', 'url': 'https://huggingface.co/papers/2509.01052', 'abstract': "FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.", 'score': 9, 'issue_id': 5686, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '909be5826c565f9d', 'authors': ['Jaewoo Ahn', 'Junseo Kim', 'Heeseung Yun', 'Jaehyeon Son', 'Dongmin Park', 'Jaewoong Cho', 'Gunhee Kim'], 'affiliations': ['Georgia Institute of Technology', 'KRAFTON', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01052.jpg', 'data': {'categories': ['#benchmark', '#agents', '#optimization', '#games'], 'emoji': 'ğŸ•¹ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…-ĞºĞ²ĞµÑÑ‚Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FlashAdventure Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ² Flash-Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¶Ğ°Ğ½Ñ€Ğ° ĞºĞ²ĞµÑÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° CUA-as-a-Judge Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº COAST, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ COAST Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ½.'}, 'en': {'title': 'Bridging the Gap: Enhancing GUI Agents in Adventure Games', 'desc': "This paper introduces the FlashAdventure benchmark and the COAST framework to enhance the performance of GUI agents in completing full story arcs in Flash-based adventure games. The research addresses the observation-behavior gap, which is the difficulty agents face in recalling and utilizing past gameplay information effectively. By providing a diverse set of 34 adventure games, the benchmark allows for a more comprehensive evaluation of agents' abilities to navigate complex narratives. The COAST framework incorporates long-term memory strategies, leading to improved task planning and milestone completion, although a significant performance gap between human players and agents remains."}, 'zh': {'title': 'æå‡æ¸¸æˆä»£ç†çš„æ•…äº‹æƒ…èŠ‚å®Œæˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†FlashAdventureåŸºå‡†å’ŒCOASTæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨å®ŒæˆFlashå†’é™©æ¸¸æˆå®Œæ•´æ•…äº‹æƒ…èŠ‚æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„æ¸¸æˆåŸºå‡†ç¼ºä¹å¤šæ ·æ€§ï¼Œä¸”å¾ˆå°‘è¯„ä¼°ä»£ç†å®Œæˆæ•´ä¸ªæ•…äº‹çº¿çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è§‚å¯Ÿ-è¡Œä¸ºå·®è·çš„é—®é¢˜ï¼ŒFlashAdventureåŸºå‡†åŒ…å«34æ¬¾Flashå†’é™©æ¸¸æˆï¼Œä¸“æ³¨äºæµ‹è¯•å®Œæ•´æ•…äº‹æƒ…èŠ‚çš„å®Œæˆã€‚COASTæ¡†æ¶é€šè¿‡åˆ©ç”¨é•¿æœŸçº¿ç´¢è®°å¿†ï¼Œå¸®åŠ©ä»£ç†æ›´å¥½åœ°è§„åˆ’å’Œè§£å†³é¡ºåºä»»åŠ¡ï¼Œä»è€Œæé«˜äº†é‡Œç¨‹ç¢‘çš„å®Œæˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01360', 'title': 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision', 'url': 'https://huggingface.co/papers/2509.01360', 'abstract': 'M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding.', 'score': 8, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '3dcca4eb6e49e24e', 'authors': ['Che Liu', 'Zheng Jiang', 'Chengyu Fang', 'Heng Guo', 'Yan-Jie Zhou', 'Jiaqi Qu', 'Le Lu', 'Minfeng Xu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Imperial College London', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01360.jpg', 'data': {'categories': ['#cv', '#multimodal', '#dataset', '#transfer_learning', '#optimization', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'M3Ret - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. M3Ret Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 867,653 Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 2D Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ğµ ÑĞ½Ğ¸Ğ¼ĞºĞ¸, Ğ£Ğ—Ğ˜, RGB Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ½Ğ´Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ¸ 3D ĞšĞ¢-ÑĞºĞ°Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DINOv3 Ğ¸ BMC-CLIP, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'Unified Learning for Enhanced Medical Image Retrieval', 'desc': 'M3Ret is a unified visual encoder designed to improve medical image retrieval by using a large dataset that includes various types of medical images. It employs generative and contrastive self-supervised learning techniques to create transferable visual representations without needing separate models for different image modalities. This approach allows M3Ret to excel in zero-shot image-to-image retrieval and cross-modal alignment, even with unseen data like MRI scans. The results indicate that M3Ret can effectively generalize across different medical imaging tasks, paving the way for more integrated solutions in medical image analysis.'}, 'zh': {'title': 'M3Retï¼šç»Ÿä¸€çš„åŒ»å­¦å½±åƒæ£€ç´¢æ–°çªç ´', 'desc': 'M3Retæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ··åˆæ¨¡æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆå¼å’Œå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨2Dã€3Då’Œè§†é¢‘åŒ»å­¦æ•°æ®ä¸Šåˆ†æ•£çš„å±€é™æ€§ã€‚é€šè¿‡æ„å»º867,653ä¸ªåŒ»å­¦å½±åƒæ ·æœ¬çš„æ•°æ®é›†ï¼ŒM3Retèƒ½å¤Ÿå­¦ä¹ å¯è¿ç§»çš„è§†è§‰è¡¨ç¤ºï¼Œä¸”æ— éœ€ç‰¹å®šæ¨¡æ€çš„å®šåˆ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒM3Retåœ¨å„ä¸ªæ¨¡æ€çš„é›¶æ ·æœ¬å›¾åƒæ£€ç´¢ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»å­¦å½±åƒç†è§£ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00425', 'title': 'The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in\n  LLMs with Camlang', 'url': 'https://huggingface.co/papers/2509.00425', 'abstract': 'Camlang, a constructed language, is used to evaluate whether LLMs can master unfamiliar languages through metalinguistic reasoning, revealing that current models lack systematic grammatical mastery compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve gold-medal performance across many benchmarks, yet it remains unclear whether such success reflects genuine reasoning or pattern matching. From a cognitive science perspective, an informative test is whether models can master an unfamiliar language through explicit metalinguistic deductive learning, a paradigm where human learners can reliably internalise grammatical systems through metalinguistic reasoning. We address this question with Camlang, a novel constructed language that exhibits naturalistic yet unattested feature combinations. Camlang consists of two explicit resources, a grammar book and a bilingual dictionary, which mirror adult second-language learning via explicit grammar rules and lexical lookup, and enable us to disentangle errors in morpho-syntax, lexical semantics, and sentence-level reasoning. Human experiments show that these resources are sufficient for participants to acquire Camlang and successfully solve Camlang tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang, creating Camlang-CSQA-v0, the first task in a broader suite where solving questions requires applying grammar rules and lexical mappings. Experimental results show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in Camlang, far below human performance at 87\\%, while other state-of-the-art reasoning LLMs perform even worse. Human verification further reveals that most model successes stem from shallow lexical alignment while GPT-5 shows emerging metalinguistic awareness to a limited extent but not systematic grammatical mastery as humans. Camlang establishes a cognitively grounded evaluation paradigm that exposes fundamental gaps between current models and human metalinguistic competence.', 'score': 8, 'issue_id': 5690, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': 'd6f6d7d86f28d385', 'authors': ['Fenghua Liu', 'Yulong Chen', 'Yixuan Liu', 'Zhujun Jin', 'Solomon Tsai', 'Ming Zhong'], 'affiliations': ['UIUC', 'University of Cambridge', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2509.00425.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#reasoning', '#long_context', '#benchmark'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Camlang Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµÑ‚ÑŒ Camlang Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. GPT-5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ 47% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Camlang Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 87% Ñƒ Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Camlang Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸ĞµĞ¹.'}, 'en': {'title': 'Bridging the Gap: Evaluating LLMs with Camlang', 'desc': 'This paper investigates the ability of Large Language Models (LLMs) to learn and understand a constructed language called Camlang, which is designed to test metalinguistic reasoning. The study finds that while LLMs like GPT-5 perform well on familiar languages, they struggle significantly with Camlang, achieving only 47% accuracy compared to 87% for humans. The results indicate that current models rely on shallow pattern matching rather than true grammatical understanding, highlighting a gap in their metalinguistic competence. This research provides a new framework for evaluating LLMs by focusing on their ability to apply grammatical rules and lexical mappings in unfamiliar contexts.'}, 'zh': {'title': 'æ­ç¤ºLLMsä¸äººç±»è¯­æ³•æŒæ¡çš„å·®è·', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒæ¡ä¸ç†Ÿæ‚‰è¯­è¨€æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿ç”¨äº†ä¸€ç§åä¸ºCamlangçš„æ„é€ è¯­è¨€è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å®ƒä»¬åœ¨è¯­æ³•æŒæ¡ä¸Šä¸äººç±»ç›¸æ¯”ä»ç„¶å­˜åœ¨æ˜¾è‘—å·®è·ã€‚é€šè¿‡æä¾›è¯­æ³•ä¹¦å’ŒåŒè¯­è¯å…¸ï¼Œç ”ç©¶æ¨¡æ‹Ÿäº†äººç±»å­¦ä¹ è€…çš„æ˜¾å¼è¯­æ³•å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„æ¨¡å‹åœ¨Camlangä»»åŠ¡ä¸­çš„è¡¨ç°è¿œä½äºäººç±»ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å…ƒè¯­è¨€æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02046', 'title': 'Fantastic Pretraining Optimizers and Where to Find Them', 'url': 'https://huggingface.co/papers/2509.02046', 'abstract': 'A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  \t\t\t\t\tAI-generated summary \t\t\t\t AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.', 'score': 6, 'issue_id': 5689, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'd90554ffa1d1a865', 'authors': ['Kaiyue Wen', 'David Hall', 'Tengyu Ma', 'Percy Liang'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2509.02046.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Muon Ğ¸ Soap, Ğ´Ğ°ÑÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰ĞµĞµÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ misleading, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° decay ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Fair Comparisons of Deep Learning Optimizers: Tuning Matters!', 'desc': 'This paper investigates the effectiveness of various deep learning optimizers, particularly focusing on the popular AdamW. It highlights that fair comparisons of optimizers require careful hyperparameter tuning and evaluations across different model sizes and data ratios. The study reveals that matrix-based optimizers like Muon and Soap show diminishing returns in speedup as model size increases, with their advantages decreasing significantly for larger models. Ultimately, the findings suggest that many claims of speedup for alternative optimizers may be overstated, emphasizing the need for rigorous evaluation methods.'}, 'zh': {'title': 'å…¬å¹³æ¯”è¾ƒæ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨çš„å…³é”®åœ¨äºè¶…å‚æ•°è°ƒä¼˜', 'desc': 'æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨çš„å…¬å¹³æ¯”è¾ƒï¼Œå¼ºè°ƒäº†è¶…å‚æ•°è°ƒä¼˜å’Œæ¨¡å‹è§„æ¨¡ã€æ•°æ®ä¸æ¨¡å‹æ¯”ä¾‹çš„è¯„ä¼°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼ŒçŸ©é˜µåŸºç¡€çš„ä¼˜åŒ–å™¨å¦‚Muonå’ŒSoapåœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶ï¼Œé€Ÿåº¦æå‡é€æ¸å‡å°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›²ç›®è½¬ç§»è¶…å‚æ•°ä¼šå¯¼è‡´ä¸å…¬å¹³çš„æ¯”è¾ƒï¼Œè€Œåœ¨è®­ç»ƒç»“æŸæ—¶è¿›è¡Œçš„ä¸¥æ ¼è¯„ä¼°æ‰èƒ½æä¾›çœŸå®çš„æ€§èƒ½å¯¹æ¯”ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šå£°ç§°çš„é€Ÿåº¦æå‡åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€ä½äºé¢„æœŸï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00244', 'title': 'Universal Deep Research: Bring Your Own Model and Strategy', 'url': 'https://huggingface.co/papers/2509.00244', 'abstract': 'Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.', 'score': 6, 'issue_id': 5685, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '641998602f4f0d54', 'authors': ['Peter Belcak', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.00244.jpg', 'data': {'categories': ['#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Universal Deep Research (UDR) - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸. UDR Ğ¾Ğ±ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. UDR Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹.'}, 'en': {'title': 'Customize Your Research with Universal Deep Research!', 'desc': 'Universal Deep Research (UDR) is a versatile system that empowers users to design personalized deep research strategies using any language model, eliminating the need for extra training or fine-tuning. Unlike previous deep research agents that are limited to specific strategies and tools, UDR allows for complete customization and flexibility. The system supports various research approaches, including minimal, expansive, and intensive strategies, demonstrating its adaptability. A user-friendly interface is provided to encourage experimentation and enhance the research process.'}, 'zh': {'title': 'é€šç”¨æ·±åº¦ç ”ç©¶ï¼šè‡ªå®šä¹‰ä½ çš„ç ”ç©¶ç­–ç•¥', 'desc': 'é€šç”¨æ·±åº¦ç ”ç©¶ï¼ˆUDRï¼‰æ˜¯ä¸€ä¸ªçµæ´»çš„ç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨ä»»ä½•è¯­è¨€æ¨¡å‹è‡ªå®šä¹‰æ·±åº¦ç ”ç©¶ç­–ç•¥ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚ç°æœ‰çš„æ·±åº¦ç ”ç©¶å·¥å…·é€šå¸¸æ˜¯ç¡¬ç¼–ç çš„ï¼Œæ‰§è¡Œç‰¹å®šçš„ç ”ç©¶ç­–ç•¥å¹¶ä½¿ç”¨å›ºå®šçš„å·¥å…·é€‰æ‹©ã€‚UDRä½œä¸ºä¸€ä¸ªé€šç”¨çš„æ™ºèƒ½ç³»ç»Ÿï¼Œå¯ä»¥å›´ç»•ä»»ä½•è¯­è¨€æ¨¡å‹è¿›è¡Œæ„å»ºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåˆ›å»ºã€ç¼–è¾‘å’Œå®Œå–„è‡ªå·±çš„æ·±åº¦ç ”ç©¶ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜ä¸ºUDRæä¾›äº†ç¤ºä¾‹ç ”ç©¶ç­–ç•¥ï¼Œå¹¶æä¾›ç”¨æˆ·ç•Œé¢ä»¥ä¾¿äºç”¨æˆ·è¿›è¡Œå®éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01984', 'title': 'Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing', 'url': 'https://huggingface.co/papers/2509.01984', 'abstract': 'VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.', 'score': 4, 'issue_id': 5686, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'b7fe3e5a42fa90e9', 'authors': ['Quan Dao', 'Xiaoxiao He', 'Ligong Han', 'Ngan Hoai Nguyen', 'Amin Heyrani Nobar', 'Faez Ahmed', 'Han Zhang', 'Viet Anh Nguyen', 'Dimitris Metaxas'], 'affiliations': ['MIT', 'Qualcomm AI Research', 'Red Hat AI Innovation', 'ReveAI', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01984.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#diffusion'], 'emoji': 'ğŸ–Œï¸', 'ru': {'title': 'VARIN: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ VAR-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'VARIN - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VAR). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. VARIN Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ argmax-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Location-aware Argmax Inversion (LAI), Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… ÑˆÑƒĞ¼Ğ¾Ğ² Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VARIN ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹.'}, 'en': {'title': 'Precise Image Editing with VARIN: Text Meets Visuals', 'desc': 'This paper presents VARIN, a novel editing technique for visual autoregressive models that allows for precise image modifications based on textual prompts. VARIN utilizes a unique method called Location-aware Argmax Inversion (LAI) to create inverse Gumbel noises, which help in reconstructing the original image while enabling targeted edits. The technique is designed to work without requiring additional training, making it practical for real-world applications. Experimental results show that VARIN successfully alters images according to prompts while maintaining important details and background structures.'}, 'zh': {'title': 'VARINï¼šç²¾å‡†å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•', 'desc': 'VARINæ˜¯ä¸€ç§åŸºäºå™ªå£°åæ¼”çš„ç¼–è¾‘æŠ€æœ¯ï¼Œä¸“ä¸ºè§†è§‰è‡ªå›å½’æ¨¡å‹è®¾è®¡ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œç²¾ç¡®çš„å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ç»†èŠ‚ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¼ªé€†å‡½æ•°ï¼Œç§°ä¸ºä½ç½®æ„ŸçŸ¥çš„Argmaxåæ¼”ï¼ˆLAIï¼‰ï¼Œç”Ÿæˆé€†Gumbelå™ªå£°ã€‚è¿™äº›é€†å™ªå£°ä½¿å¾—æºå›¾åƒçš„é‡å»ºæ›´åŠ ç²¾ç¡®ï¼Œå¹¶æ”¯æŒä¸æ–‡æœ¬æç¤ºå¯¹é½çš„æœ‰é’ˆå¯¹æ€§çš„å¯æ§ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVARINèƒ½å¤Ÿæœ‰æ•ˆåœ°æ ¹æ®æŒ‡å®šæç¤ºä¿®æ”¹æºå›¾åƒï¼ŒåŒæ—¶æ˜¾è‘—ä¿ç•™åŸå§‹èƒŒæ™¯å’Œç»“æ„ç»†èŠ‚ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºå®ç”¨ç¼–è¾‘æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02133', 'title': 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models', 'url': 'https://huggingface.co/papers/2509.02133', 'abstract': 'Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/', 'score': 2, 'issue_id': 5688, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'a82d3bc5f04bc839', 'authors': ['Snehasis Mukhopadhyay', 'Aryan Kasat', 'Shivam Dubey', 'Rahul Karthikeyan', 'Dhruv Sood', 'Vinija Jain', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Amazon GenAI', 'Artificial Intelligence Institute, University of South Carolina', 'BITS Pilani Goa', 'DTU', 'IIT Madras', 'Indian Institute of Information Technology, Kalyani', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.02133.jpg', 'data': {'categories': ['#data', '#multilingual', '#ethics', '#open_source', '#alignment', '#inference'], 'emoji': 'ğŸ‡®ğŸ‡³', 'ru': {'title': 'ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AMBEDKAR Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ñ€ĞµĞ»Ğ¸Ğ³Ğ¸Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 26.41% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹.'}, 'en': {'title': 'Fairness Through Constitution-Aware Decoding in AI', 'desc': 'The Ambekar framework introduces a Constitution-Aware Decoding Layer to address caste and religious biases in Large Language Models (LLMs) without the need for retraining. It recognizes that existing bias mitigation strategies often overlook local contexts, particularly in India, where such biases are pronounced. By employing a speculative decoding algorithm, the framework actively reduces harmful outputs during the generation process. This innovative approach not only enhances fairness in AI outputs but also demonstrates a significant reduction in bias, achieving up to 26.41 percent less bias compared to traditional methods.'}, 'zh': {'title': 'å…¬å¹³ä¸ä¸­ç«‹ï¼šAmbekaræ¡†æ¶çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'Ambekaræ¡†æ¶é€šè¿‡å¼•å…¥ä¸€ä¸ªå®ªæ³•æ„è¯†è§£ç å±‚ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºä¸­çš„ç§å§“å’Œå®—æ•™åè§ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µåº”ç”¨ï¼Œåˆ©ç”¨æŠ•æœºè§£ç ç®—æ³•ä¸»åŠ¨é™ä½ç”Ÿæˆè¿‡ç¨‹ä¸­çš„åè§ã€‚ä¸ä¼ ç»Ÿçš„åè§ç¼“è§£ç­–ç•¥ä¸åŒï¼ŒAmbekaræ¡†æ¶ä¸“æ³¨äºå°åº¦ç‰¹æœ‰çš„ç¤¾ä¼šèƒŒæ™¯ï¼Œç¡®ä¿è¾“å‡ºçš„å…¬å¹³æ€§å’Œä¸­ç«‹æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å®ç°äº†ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹é«˜è¾¾26.41%çš„åè§ç»å¯¹å‡å°‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00581', 'title': 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction', 'url': 'https://huggingface.co/papers/2509.00581', 'abstract': 'A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting natural language queries into SQL queries is a crucial challenge in both industry and academia, aiming to increase access to databases and large-scale applications. This work examines how in-context learning and chain-of-thought can be utilized to develop a robust solution for text-to-SQL systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the Text2SQL task into schema linking, subproblem identification, query plan generation, SQL generation, and a guided correction loop. Unlike prior systems that rely only on execution-based static correction, we introduce taxonomy-guided dynamic error modification informed by in-context learning. SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its variants, combining guided error taxonomy with reasoning-based query planning.', 'score': 2, 'issue_id': 5688, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': '9c1785e2259e0c93', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems SaarbrÃ¼cken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2509.00581.jpg', 'data': {'categories': ['#data', '#optimization', '#reasoning', '#agents', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Text2SQL', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Text2SQL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SQL-of-Thought, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸-guided ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Spider.'}, 'en': {'title': 'Transforming Language to SQL with Intelligent Agents', 'desc': 'This paper presents SQL-of-Thought, a multi-agent framework designed to improve the Text2SQL task by breaking it down into several key components. It leverages in-context learning and chain-of-thought reasoning to enhance the conversion of natural language queries into SQL queries. The framework includes processes such as schema linking, subproblem identification, query plan generation, and a dynamic error correction loop guided by a taxonomy. By integrating these elements, SQL-of-Thought achieves state-of-the-art performance on the Spider dataset, surpassing previous systems that relied solely on static correction methods.'}, 'zh': {'title': 'SQLè½¬æ¢çš„æ–°æ€è·¯ï¼šå¤šæ™ºèƒ½ä½“æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSQL-of-Thoughtçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLæŸ¥è¯¢ã€‚è¯¥æ¡†æ¶å°†Text2SQLä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å¼é“¾æ¥ã€å­é—®é¢˜è¯†åˆ«ã€æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆã€SQLç”Ÿæˆå’Œå¼•å¯¼ä¿®æ­£å¾ªç¯ã€‚ä¸ä»¥å¾€ä»…ä¾èµ–é™æ€æ‰§è¡Œä¿®æ­£çš„ç³»ç»Ÿä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„åŠ¨æ€é”™è¯¯ä¿®æ”¹ï¼Œç»“åˆäº†å¼•å¯¼é”™è¯¯åˆ†ç±»å’Œæ¨ç†åŸºç¡€çš„æŸ¥è¯¢è§„åˆ’ã€‚SQL-of-Thoughtåœ¨Spideræ•°æ®é›†åŠå…¶å˜ä½“ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬åˆ°SQLç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00531', 'title': 'MobiAgent: A Systematic Framework for Customizable Mobile Agents', 'url': 'https://huggingface.co/papers/2509.00531', 'abstract': 'MobiAgent, a comprehensive mobile agent system, achieves state-of-the-art performance in real-world mobile scenarios through its MobiMind-series models, AgentRR framework, and MobiFlow benchmarking suite, while also reducing data annotation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of Vision-Language Models (VLMs), GUI-based mobile agents have emerged as a key development direction for intelligent mobile systems. However, existing agent models continue to face significant challenges in real-world task execution, particularly in terms of accuracy and efficiency. To address these limitations, we propose MobiAgent, a comprehensive mobile agent system comprising three core components: the MobiMind-series agent models, the AgentRR acceleration framework, and the MobiFlow benchmarking suite. Furthermore, recognizing that the capabilities of current mobile agents are still limited by the availability of high-quality data, we have developed an AI-assisted agile data collection pipeline that significantly reduces the cost of manual annotation. Compared to both general-purpose LLMs and specialized GUI agent models, MobiAgent achieves state-of-the-art performance in real-world mobile scenarios.', 'score': 2, 'issue_id': 5690, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': 'c0685c3c2b70b6aa', 'authors': ['Cheng Zhang', 'Erhu Feng', 'Xi Zhao', 'Yisheng Zhao', 'Wangbo Gong', 'Jiahui Sun', 'Dong Du', 'Zhichao Hua', 'Yubin Xia', 'Haibo Chen'], 'affiliations': ['Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.00531.jpg', 'data': {'categories': ['#dataset', '#agents', '#data', '#benchmark'], 'emoji': 'ğŸ“±', 'ru': {'title': 'MobiAgent: Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'MobiAgent - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MobiMind, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ AgentRR Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² MobiFlow. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². MobiAgent Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'MobiAgent: Revolutionizing Mobile Intelligence with Efficiency and Cost-Effectiveness', 'desc': 'MobiAgent is a mobile agent system designed to enhance the performance of intelligent mobile applications in real-world scenarios. It consists of three main components: the MobiMind-series models for agent intelligence, the AgentRR framework for improving execution speed, and the MobiFlow suite for performance benchmarking. The system also includes an AI-assisted data collection pipeline that lowers the costs associated with data annotation, addressing a common limitation in training mobile agents. Overall, MobiAgent outperforms existing models by providing better accuracy and efficiency in task execution.'}, 'zh': {'title': 'MobiAgentï¼šæ™ºèƒ½ç§»åŠ¨ä»£ç†çš„æœªæ¥', 'desc': 'MobiAgentæ˜¯ä¸€ä¸ªå…¨é¢çš„ç§»åŠ¨ä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ç§»åŠ¨ç³»ç»Ÿåœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®ƒç”±MobiMindç³»åˆ—æ¨¡å‹ã€AgentRRåŠ é€Ÿæ¡†æ¶å’ŒMobiFlowåŸºå‡†æµ‹è¯•å¥—ä»¶ä¸‰éƒ¨åˆ†ç»„æˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ä»»åŠ¡æ‰§è¡Œçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†é™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼ŒMobiAgentè¿˜å¼€å‘äº†ä¸€ä¸ªAIè¾…åŠ©çš„æ•æ·æ•°æ®æ”¶é›†ç®¡é“ã€‚ä¸é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ç”¨GUIä»£ç†æ¨¡å‹ç›¸æ¯”ï¼ŒMobiAgentåœ¨å®é™…ç§»åŠ¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00404', 'title': 'Metis: Training Large Language Models with Advanced Low-Bit Quantization', 'url': 'https://huggingface.co/papers/2509.00404', 'abstract': 'Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/typename-yyf/Metis-quantization.', 'score': 2, 'issue_id': 5688, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': '122a54575f7764a1', 'authors': ['Hengjie Cao', 'Mengyi Chen', 'Yifeng Yang', 'Ruijun Huang', 'Fang Dong', 'Jixian Zhou', 'Anrui Chen', 'Mingzhi Dong', 'Yujiang Wang', 'Jinlong Hou', 'Yuan Cheng', 'Fan Wu', 'Fan Yang', 'Tun Lu', 'Ning Gu', 'Li Shang'], 'affiliations': ['Fudan University', 'Huawei', 'Oxford Suzhou Centre for Advanced Research', 'Shanghai Innovation Institute', 'University of Bath'], 'pdf_title_img': 'assets/pdf/title_img/2509.00404.jpg', 'data': {'categories': ['#low_resource', '#inference', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Metis - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ²ÑƒÑ…Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Metis Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ñ 32-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹, Ğ° Ñ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ - ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾ Ñ 32-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹.'}, 'en': {'title': 'Metis: Stabilizing Low-Bit Training for Large Language Models', 'desc': 'This paper presents Metis, a novel training framework designed to enhance the stability and performance of low-bit quantized large language models (LLMs). It addresses the issue of anisotropic parameter distributions that hinder effective training by utilizing spectral decomposition to separate dominant and minor components, allowing for better quantization. Additionally, Metis employs adaptive learning rates to focus on underrepresented features, improving model accuracy. The framework also incorporates a dual-range regularization technique to maintain numerical precision and ensure stable training, resulting in significant performance improvements over traditional FP32 training methods.'}, 'zh': {'title': 'Metisï¼šæå‡ä½æ¯”ç‰¹é‡åŒ–æ¨¡å‹è®­ç»ƒç¨³å®šæ€§ä¸æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶', 'desc': 'Metisæ˜¯ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä½æ¯”ç‰¹é‡åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸ç¨³å®šæ€§ã€‚å®ƒé€šè¿‡è°±åˆ†è§£ã€é€‚åº”æ€§å­¦ä¹ ç‡å’ŒåŒèŒƒå›´æ­£åˆ™åŒ–æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå‚æ•°åˆ†å¸ƒçš„å„å‘å¼‚æ€§æ˜¯ä½æ¯”ç‰¹é‡åŒ–è®­ç»ƒçš„ä¸»è¦éšœç¢ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å‹æ€§èƒ½ä½ä¸‹ã€‚Metisé€šè¿‡æœ‰æ•ˆåœ°åˆ†ç¦»ä¸»å¯¼æˆåˆ†å’Œé•¿å°¾æˆåˆ†ï¼Œå‹ç¼©åˆ†å¸ƒèŒƒå›´ï¼Œä»è€Œå®ç°äº†æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21334', 'title': 'Stairway to Fairness: Connecting Group and Individual Fairness', 'url': 'https://huggingface.co/papers/2508.21334', 'abstract': 'Experiments reveal that highly group-fair recommendations can be individually unfair, highlighting the need for a better understanding and comparison of fairness measures in recommender systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.', 'score': 2, 'issue_id': 5696, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '63506e0d046b4476', 'authors': ['Theresia Veronika Rampisela', 'Maria Maistro', 'Tuukka Ruotsalo', 'Falk Scholer', 'Christina Lioma'], 'affiliations': ['LUT University, Lahti, Finland', 'RMIT University, Melbourne, Australia', 'University of Copenhagen, Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2508.21334.jpg', 'data': {'categories': ['#dataset', '#ethics', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸: Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ vs Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Balancing Group and Individual Fairness in Recommendations', 'desc': 'This paper investigates the relationship between group fairness and individual fairness in recommender systems (RSs). It highlights that while a recommendation may be fair for a group, it can still be unfair to individuals within that group. The authors conducted experiments across multiple datasets to compare different fairness evaluation measures, revealing that enhancing one type of fairness can negatively impact the other. This research provides valuable insights for practitioners looking to balance fairness in their recommendation algorithms.'}, 'zh': {'title': 'ç¾¤ä½“å…¬å¹³ä¸ä¸ªä½“å…¬å¹³çš„å¹³è¡¡ä¹‹é“', 'desc': 'åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå…¬å¹³æ€§é€šå¸¸åˆ†ä¸ºç¾¤ä½“å…¬å¹³å’Œä¸ªä½“å…¬å¹³ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹è¿™ä¸¤ç§å…¬å¹³æ€§ä¹‹é—´å…³ç³»çš„ç§‘å­¦ç†è§£å°šä¸æ˜ç¡®ï¼Œå› ä¸ºä»¥å¾€çš„ç ”ç©¶ä½¿ç”¨äº†ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡å’Œç›®æ ‡ï¼Œå¯¼è‡´æ— æ³•è¿›è¡Œæœ‰æ•ˆæ¯”è¾ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†ç¾¤ä½“å…¬å¹³å’Œä¸ªä½“å…¬å¹³ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¯¹å¯ç”¨äºè¿™ä¸¤ç§å…¬å¹³æ€§çš„è¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æŸäº›æ¨èåœ¨ç¾¤ä½“å±‚é¢ä¸Šéå¸¸å…¬å¹³ï¼Œä½†åœ¨ä¸ªä½“å±‚é¢ä¸Šå¯èƒ½å­˜åœ¨ä¸¥é‡çš„ä¸å…¬å¹³ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21038', 'title': 'On the Theoretical Limitations of Embedding-Based Retrieval', 'url': 'https://huggingface.co/papers/2508.21038', 'abstract': 'Vector embeddings face theoretical limitations in handling even simple queries, as demonstrated by a new dataset that shows state-of-the-art models fail due to the single vector paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.', 'score': 2, 'issue_id': 5699, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '460396ed53ab3a52', 'authors': ['Orion Weller', 'Michael Boratko', 'Iftekhar Naim', 'Jinhyuk Lee'], 'affiliations': ['Google DeepMind', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2508.21038.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#data', '#dataset', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ĞµĞ»Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LIMIT, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑÑ‚Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Challenging the Limits of Vector Embeddings in Simple Queries', 'desc': 'This paper discusses the limitations of vector embeddings in machine learning, particularly when handling simple queries. It reveals that even advanced models struggle with these tasks due to the constraints of the single vector representation. The authors introduce a new dataset, LIMIT, which tests these models and demonstrates their failures in realistic scenarios. The findings suggest that the theoretical boundaries of embedding models need to be addressed to improve their performance in various applications.'}, 'zh': {'title': 'å‘é‡åµŒå…¥çš„ç†è®ºå±€é™æ€§ä¸æœªæ¥ç ”ç©¶æ–¹å‘', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å‘é‡åµŒå…¥åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶çš„ç†è®ºå±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å‘é‡åµŒå…¥åœ¨æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ä»ç„¶æ— æ³•æ»¡è¶³åŸºæœ¬éœ€æ±‚ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºåä¸ºLIMITçš„æ•°æ®é›†ï¼ŒéªŒè¯äº†å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹ç®€å•ä»»åŠ¡æ—¶ä¹Ÿä¼šå¤±è´¥ã€‚è¯¥ç ”ç©¶å‘¼åæœªæ¥çš„ç ”ç©¶åº”å¼€å‘æ–°çš„æ–¹æ³•ï¼Œä»¥å…‹æœç°æœ‰å•å‘é‡èŒƒå¼çš„æ ¹æœ¬é™åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02523', 'title': 'Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices', 'url': 'https://huggingface.co/papers/2509.02523', 'abstract': 'Monolingual ASR models trained on a balanced mix of high-quality, pseudo-labeled, and synthetic data outperform multilingual models for small model sizes, achieving superior error rates and enabling on-device ASR for underrepresented languages.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license.', 'score': 1, 'issue_id': 5698, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '0a4562a7b7ff851b', 'authors': ['Evan King', 'Adam Sabra', 'Manjunath Kudlur', 'James Wang', 'Pete Warden'], 'affiliations': ['Moonshine AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.02523.jpg', 'data': {'categories': ['#dataset', '#small_models', '#low_resource', '#audio', '#synthetic', '#open_source', '#multilingual', '#data'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ASR Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Flavors of Moonshine. Ğ’Ğ¾Ğ¿Ñ€ĞµĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (27 Ğ¼Ğ»Ğ½) Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ…, Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½Ğ° 48% Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Whisper Tiny Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Monolingual Models Shine for Underrepresented Languages!', 'desc': 'This paper presents a new approach to automatic speech recognition (ASR) for underrepresented languages using monolingual models. The authors demonstrate that small ASR models, when trained on a balanced mix of high-quality, pseudo-labeled, and synthetic data, can achieve better performance than larger multilingual models. Their findings show that these monolingual models can reduce error rates significantly, making them suitable for on-device applications. The research highlights the effectiveness of tailored training strategies for improving ASR in languages that have been historically underserved.'}, 'zh': {'title': 'å•è¯­æ¨¡å‹è¶…è¶Šå¤šè¯­ç§æ¨¡å‹çš„çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMoonshineçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹ä¸€äº›ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€è¿›è¡Œä¼˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é«˜è´¨é‡äººå·¥æ ‡æ³¨ã€ä¼ªæ ‡æ³¨å’Œåˆæˆæ•°æ®çš„å•è¯­æ¨¡å‹åœ¨å°å‹æ¨¡å‹ï¼ˆ27Må‚æ•°ï¼‰ä¸­è¡¨ç°ä¼˜äºå¤šè¯­ç§æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨é”™è¯¯ç‡ä¸Šå¹³å‡æ¯”åŒç­‰å¤§å°çš„Whisper Tinyæ¨¡å‹ä½48%ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹èƒ½å¤Ÿä¸æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚è¿™äº›æˆæœæ¨åŠ¨äº†å°å‹æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥ï¼Œä½¿å¾—åœ¨è®¾å¤‡ä¸Šå®ç°å‡†ç¡®çš„è¯­éŸ³è¯†åˆ«æˆä¸ºå¯èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02379', 'title': 'MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?', 'url': 'https://huggingface.co/papers/2509.02379', 'abstract': 'MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3.', 'score': 1, 'issue_id': 5685, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'bbf123f2c298e53a', 'authors': ['Yuheng Li', 'Yizhou Wu', 'Yuxiang Lai', 'Mingzhe Hu', 'Xiaofeng Yang'], 'affiliations': ['Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta', 'Department of Computer Science, Emory University, Atlanta', 'Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta', 'Department of Radiation Oncology, Emory University School of Medicine, Atlanta'], 'pdf_title_img': 'assets/pdf/title_img/2509.02379.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset', '#architecture', '#transfer_learning', '#optimization', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MedDINOv3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MedDINOv3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DINOv3. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. MedDINOv3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ½Ğ° ĞšĞ¢ Ğ¸ ĞœĞ Ğ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ….'}, 'en': {'title': 'Revolutionizing Medical Image Segmentation with MedDINOv3', 'desc': 'MedDINOv3 is a new framework that enhances the DINOv3 model for better medical image segmentation, particularly in CT and MRI scans. It addresses the challenges of adapting vision foundation models to medical imaging by using multi-scale token aggregation and domain-adaptive pretraining. This approach allows the model to learn robust features from a large dataset of CT images, improving its performance compared to traditional CNNs. As a result, MedDINOv3 achieves state-of-the-art results in various segmentation tasks, showcasing the effectiveness of using advanced vision models in the medical field.'}, 'zh': {'title': 'MedDINOv3ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°çªç ´', 'desc': 'MedDINOv3æ˜¯ä¸€ä¸ªå°†DINOv3ä¸å¤šå°ºåº¦æ ‡è®°èšåˆç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸé€‚åº”å’Œä¸»å¹²ç½‘ç»œæ€§èƒ½é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨CT-3Mæ•°æ®é›†ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œå­¦ä¹ åˆ°å¼ºå¤§çš„å¯†é›†ç‰¹å¾ï¼Œä»è€Œæé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†å½“å‰çš„æœ€ä½³æ€§èƒ½ï¼Œå±•ç¤ºäº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰åŸºç¡€æ¨¡å‹å¯ä»¥ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç»Ÿä¸€ä¸»å¹²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01790', 'title': 'Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs', 'url': 'https://huggingface.co/papers/2509.01790', 'abstract': 'Modern LLMs exhibit less prompt sensitivity than previously thought, with much of the reported variability due to heuristic evaluation methods rather than inherent model weaknesses.  \t\t\t\t\tAI-generated summary \t\t\t\t Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.', 'score': 1, 'issue_id': 5697, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '2b3ed7c45180a009', 'authors': ['Andong Hua', 'Kenan Tang', 'Chenhe Gu', 'Jindong Gu', 'Eric Wong', 'Yao Qin'], 'affiliations': ['UC Irvine', 'UC Santa Barbara', 'University of Oxford', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2509.01790.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ½Ğ¸Ğ¼Ğ°Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¼ĞµĞ½ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡ĞµĞ¼ ÑÑ‡Ğ¸Ñ‚Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°Ğ½ĞµĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 7 LLM Ğ½Ğ° 6 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 12 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ’Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ğ½Ğµ Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': "Rethinking Prompt Sensitivity: It's Not the Models, It's the Evaluation!", 'desc': "This paper investigates the concept of prompt sensitivity in large language models (LLMs), which is the idea that changing the wording of a prompt can significantly affect the model's performance. The authors argue that much of the perceived variability in LLM responses is not due to weaknesses in the models themselves, but rather due to the evaluation methods used to assess them. By systematically testing multiple LLMs across various benchmarks and using more flexible evaluation techniques, they find that the models are actually more consistent and robust than previously thought. This suggests that the high prompt sensitivity reported in earlier studies may be an artifact of rigid evaluation processes rather than an inherent flaw in the models."}, 'zh': {'title': 'ç°ä»£LLMçš„æç¤ºæ•æ„Ÿæ€§è¢«ä½ä¼°äº†', 'desc': 'ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºçš„æç¤ºæ•æ„Ÿæ€§æ¯”ä¹‹å‰è®¤ä¸ºçš„è¦ä½ï¼Œå¾ˆå¤šæŠ¥å‘Šçš„å˜åŒ–æ˜¯ç”±äºå¯å‘å¼è¯„ä¼°æ–¹æ³•é€ æˆçš„ï¼Œè€Œä¸æ˜¯æ¨¡å‹æœ¬èº«çš„ç¼ºé™·ã€‚æç¤ºæ•æ„Ÿæ€§æ˜¯æŒ‡é€šè¿‡ä¸åŒçš„æªè¾é‡å¤å†…å®¹æ—¶ï¼ŒLLMæ€§èƒ½å‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„ç°è±¡ã€‚æˆ‘ä»¬å¯¹7ä¸ªLLMè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å¾ˆå¤šæç¤ºæ•æ„Ÿæ€§æºäºè¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œæ¯”å¦‚å¯¹è¯­ä¹‰æ­£ç¡®çš„æ›¿ä»£è¡¨è¾¾çš„å¿½è§†ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°ä»£LLMå¯¹æç¤ºæ¨¡æ¿çš„é²æ£’æ€§æ¯”ä¹‹å‰è®¤ä¸ºçš„è¦å¼ºï¼Œæç¤ºæ•æ„Ÿæ€§å¯èƒ½æ›´å¤šæ˜¯è¯„ä¼°è¿‡ç¨‹çš„äº§ç‰©ï¼Œè€Œéæ¨¡å‹çš„ç¼ºé™·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01610', 'title': 'Improving Large Vision and Language Models by Learning from a Panel of\n  Peers', 'url': 'https://huggingface.co/papers/2509.01610', 'abstract': 'A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%', 'score': 1, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '3edfce8f16cf09cb', 'authors': ['Jefferson Hernandez', 'Jing Shi', 'Simon Jenni', 'Vicente Ordonez', 'Kushal Kafle'], 'affiliations': ['Adobe Research', 'Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01610.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#rlhf', '#hallucinations', '#alignment'], 'emoji': 'ğŸ‘¥', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Panel-of-Peers. ĞĞ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Models through Peer Learning', 'desc': "The paper introduces a Panel-of-Peers learning framework that enhances Large Vision and Language Models (LVLMs) by simulating peer reviews. This method addresses the challenges of relying on costly human-curated preference data and the limitations of machine-generated and self-supervised data. By allowing multiple LVLMs to evaluate and learn from each other's outputs, the framework fosters an iterative self-improvement process. The results show significant performance improvements across various benchmarks, highlighting the effectiveness of peer evaluations as a scalable alternative to traditional alignment methods."}, 'zh': {'title': 'åŒä¼´è¯„å®¡ï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºâ€œåŒä¼´è¯„å®¡å­¦ä¹ æ¡†æ¶â€ï¼Œæ—¨åœ¨æå‡å¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹ŸåŒä¼´è¯„å®¡çš„è¿‡ç¨‹ï¼Œä½¿å¤šä¸ªLVLMç›¸äº’è¯„ä¼°å’Œå­¦ä¹ ï¼Œä»è€Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨æ•°æ®ä¸åŒï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§é‡äººç±»æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¹³å‡å¾—åˆ†ï¼Œå±•ç¤ºäº†åŒä¼´è¯„å®¡ä½œä¸ºä¸€ç§å¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£å¯¹é½æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01584', 'title': 'ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association', 'url': 'https://huggingface.co/papers/2509.01584', 'abstract': 'ViSTA-SLAM is a real-time monocular SLAM system that uses a lightweight STA model for pose estimation and pointmap regression, and a Sim(3) pose graph for drift correction, achieving superior tracking and reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ViSTA-SLAM as a real-time monocular visual SLAM system that operates without requiring camera intrinsics, making it broadly applicable across diverse camera setups. At its core, the system employs a lightweight symmetric two-view association (STA) model as the frontend, which simultaneously estimates relative camera poses and regresses local pointmaps from only two RGB images. This design reduces model complexity significantly, the size of our frontend is only 35\\% that of comparable state-of-the-art methods, while enhancing the quality of two-view constraints used in the pipeline. In the backend, we construct a specially designed Sim(3) pose graph that incorporates loop closures to address accumulated drift. Extensive experiments demonstrate that our approach achieves superior performance in both camera tracking and dense 3D reconstruction quality compared to current methods. Github repository: https://github.com/zhangganlin/vista-slam', 'score': 1, 'issue_id': 5690, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': 'daefaefcec9f3e8c', 'authors': ['Ganlin Zhang', 'Shenhan Qian', 'Xi Wang', 'Daniel Cremers'], 'affiliations': ['ETH Zurich', 'MCML', 'TU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2509.01584.jpg', 'data': {'categories': ['#architecture', '#3d', '#cv'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ°Ñ SLAM-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±ĞµĞ· ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'ViSTA-SLAM - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½ÑƒÑ ĞºĞ°Ğ¼ĞµÑ€Ñƒ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ²ÑƒÑ…Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¸ (STA) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ’ backend Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ¾Ğ· Sim(3) Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°.'}, 'en': {'title': 'ViSTA-SLAM: Lightweight and Intrinsic-Free Monocular SLAM for Superior Tracking and Reconstruction', 'desc': 'ViSTA-SLAM is a real-time monocular SLAM system that simplifies pose estimation and pointmap regression using a lightweight symmetric two-view association (STA) model. This model allows the system to operate without needing camera intrinsics, making it versatile for various camera types. The backend features a Sim(3) pose graph that effectively corrects drift by incorporating loop closures, enhancing overall tracking accuracy. Experimental results show that ViSTA-SLAM outperforms existing methods in both camera tracking and dense 3D reconstruction.'}, 'zh': {'title': 'ViSTA-SLAMï¼šé«˜æ•ˆçš„å®æ—¶å•ç›®è§†è§‰SLAMç³»ç»Ÿ', 'desc': 'ViSTA-SLAMæ˜¯ä¸€ç§å®æ—¶å•ç›®è§†è§‰SLAMç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ç›¸æœºå†…å‚çš„æƒ…å†µä¸‹è¿è¡Œï¼Œé€‚ç”¨äºå¤šç§ç›¸æœºè®¾ç½®ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¯¹ç§°åŒè§†å›¾å…³è”ï¼ˆSTAï¼‰æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ä¼°è®¡ç›¸å¯¹ç›¸æœºå§¿æ€å¹¶ä»ä¸¤å¼ RGBå›¾åƒä¸­å›å½’å±€éƒ¨ç‚¹äº‘å›¾ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹å¤æ‚åº¦æ˜¾è‘—é™ä½ï¼Œå‰ç«¯çš„å¤§å°ä»…ä¸ºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„35%ï¼ŒåŒæ—¶æé«˜äº†ç®¡é“ä¸­ä½¿ç”¨çš„åŒè§†å›¾çº¦æŸçš„è´¨é‡ã€‚åœ¨åç«¯ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç‰¹åˆ«è®¾è®¡çš„Sim(3)å§¿æ€å›¾ï¼Œç»“åˆäº†å›ç¯é—­åˆæ¥è§£å†³ç´¯ç§¯æ¼‚ç§»é—®é¢˜ï¼Œå®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸æœºè·Ÿè¸ªå’Œç¨ å¯†3Dé‡å»ºè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01250', 'title': 'Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views', 'url': 'https://huggingface.co/papers/2509.01250', 'abstract': 'Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE.', 'score': 1, 'issue_id': 5685, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '3fffc03e184237e0', 'authors': ['Xiangdong Zhang', 'Shaofeng Zhang', 'Junchi Yan'], 'affiliations': ['School of AI, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01250.jpg', 'data': {'categories': ['#optimization', '#3d', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Point-PQAE. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Point-PQAE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 7% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanObjectNN.'}, 'en': {'title': 'Enhancing 3D Learning with Two-View Cross-Reconstruction', 'desc': 'Point-PQAE is a novel approach in 3D self-supervised learning that utilizes a two-view cross-reconstruction method to enhance point cloud reconstruction tasks. By generating two separate point clouds and reconstructing one from the other, it introduces greater diversity and variance compared to traditional single-view methods. This method employs a unique crop mechanism for generating views and a new positional encoding to capture the 3D relationships between the views. As a result, Point-PQAE significantly outperforms existing self-reconstruction techniques, demonstrating improved performance in various evaluation scenarios.'}, 'zh': {'title': 'åŒè§†å›¾å­¦ä¹ ï¼Œæå‡3Dè‡ªç›‘ç£é‡å»ºæ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPoint-PQAEçš„è·¨é‡å»ºç”ŸæˆèŒƒå¼ï¼Œæ—¨åœ¨å¢å¼º3Dè‡ªç›‘ç£å­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åŒè§†å›¾çš„å­¦ä¹ æ–¹å¼ï¼Œå¢åŠ äº†ç‚¹äº‘é‡å»ºä»»åŠ¡ä¸­çš„å¤šæ ·æ€§å’Œæ–¹å·®ï¼Œè¶…è¶Šäº†å•è§†å›¾æ–¹æ³•çš„è¡¨ç°ã€‚æˆ‘ä»¬é¦–æ¬¡å¼€å‘äº†ä¸€ç§ç‚¹äº‘è§†å›¾ç”Ÿæˆçš„è£å‰ªæœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä½ç½®ç¼–ç æ¥è¡¨ç¤ºä¸¤ä¸ªè§£è€¦è§†å›¾ä¹‹é—´çš„3Dç›¸å¯¹ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoint-PQAEåœ¨ScanObjectNNçš„ä¸‰ä¸ªå˜ä½“ä¸­ï¼Œåˆ†åˆ«æ¯”è‡ªé‡å»ºåŸºçº¿ï¼ˆPoint-MAEï¼‰æé«˜äº†6.5%ã€7.0%å’Œ6.7%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00578', 'title': 'C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection', 'url': 'https://huggingface.co/papers/2509.00578', 'abstract': 'Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains', 'score': 1, 'issue_id': 5687, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': '191ffb7e3bd4bec6', 'authors': ['Abdellah Zakaria Sellam', 'Ilyes Benaissa', 'Salah Eddine Bekhouche', 'Abdenour Hadid', 'Vito RenÃ³', 'Cosimo Distante'], 'affiliations': ['CNR-STIIMA, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing, c/o Campus Ecotekne, Via Monteroni, Lecce, 73100, LE, Italy', 'Department of Innovation Engineering, University of Salento & Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy', 'Department of Innovation Engineering, University of Salento, Via per Monteroni, Lecce, 73100, Lecce, Italy', 'Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy', 'Sorbonne University Abu Dhabi, UAE', 'UPV/EHU, University of the Basque Country, Sebastian, 20018, Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2509.00578.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#optimization', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Context-Aware Fusion (CAF) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DiffusionDet Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². CAF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ CarDD Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Object Detection with Context-Aware Fusion', 'desc': 'This paper introduces Context-Aware Fusion (CAF) to enhance the DiffusionDet model for fine-grained object detection. CAF uses cross-attention mechanisms to combine global scene context with local features, addressing the limitations of local feature conditioning. By employing a dedicated encoder to capture environmental information, the model allows object proposals to utilize a broader understanding of the scene. Experimental results show that this approach significantly improves performance on the CarDD benchmark, setting new standards for context-aware object detection.'}, 'zh': {'title': 'ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆæå‡ç»†ç²’åº¦ç‰©ä½“æ£€æµ‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆï¼ˆContext-Aware Fusion, CAFï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡DiffusionDetåœ¨ç»†ç²’åº¦ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚CAFé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å…¨å±€åœºæ™¯ä¸Šä¸‹æ–‡ä¸å±€éƒ¨ç‰¹å¾ç›¸ç»“åˆï¼Œä»è€Œå…‹æœäº†DiffusionDetåœ¨ä¸Šä¸‹æ–‡ä¾èµ–åœºæ™¯ä¸­çš„å±€éƒ¨ç‰¹å¾é™åˆ¶ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸“é—¨çš„ç¼–ç å™¨ç”Ÿæˆå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ•æ‰å…¨é¢çš„ç¯å¢ƒä¿¡æ¯ï¼Œä½¿æ¯ä¸ªç‰©ä½“æè®®èƒ½å¤Ÿå…³æ³¨åœºæ™¯çº§ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAFåœ¨CarDDåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸ºç»†ç²’åº¦é¢†åŸŸçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰©ä½“æ£€æµ‹è®¾ç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20586', 'title': 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2508.20586', 'abstract': 'FastFit, a high-speed virtual try-on framework using a cacheable diffusion architecture with a Semi-Attention mechanism, achieves significant speedup and maintains high fidelity in multi-reference outfit compositions.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.', 'score': 1, 'issue_id': 5691, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'fefbcacccb51e5ca', 'authors': ['Zheng Chong', 'Yanwei Lei', 'Shiyue Zhang', 'Zhuandi He', 'Zhen Wang', 'Xujie Zhang', 'Xiao Dong', 'Yiling Wu', 'Dongmei Jiang', 'Xiaodan Liang'], 'affiliations': ['LavieAI', 'Pengcheng Laboratory', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20586.jpg', 'data': {'categories': ['#benchmark', '#cv', '#inference', '#open_source', '#dataset', '#diffusion', '#data'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ñ FastFit', 'desc': 'FastFit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºÑÑˆĞ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑƒĞ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (Semi-Attention). ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. FastFit Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° Ğ³Ğ°Ñ€Ğ´ĞµÑ€Ğ¾Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DressCode-MR Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'FastFit: Speeding Up Virtual Try-Ons with Smart Caching!', 'desc': 'FastFit is a virtual try-on framework that enhances the speed and quality of outfit compositions by using a cacheable diffusion architecture combined with a Semi-Attention mechanism. It addresses the inefficiencies of existing methods by allowing reference features to be computed once and reused, which significantly reduces redundant calculations during the denoising process. This innovation leads to an impressive average speedup of 3.5 times compared to other techniques while maintaining high fidelity in the generated images. Additionally, the introduction of the DressCode-MR dataset supports further research in multi-reference virtual try-on applications, providing a rich resource for training and evaluation.'}, 'zh': {'title': 'FastFitï¼šé«˜æ•ˆè™šæ‹Ÿè¯•è¡£çš„æ–°çªç ´', 'desc': 'FastFitæ˜¯ä¸€ç§é«˜é€Ÿåº¦çš„è™šæ‹Ÿè¯•è¡£æ¡†æ¶ï¼Œé‡‡ç”¨å¯ç¼“å­˜çš„æ‰©æ•£æ¶æ„å’ŒåŠæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šå‚è€ƒæœè£…ç»„åˆä¸­å®ç°æ˜¾è‘—çš„åŠ é€Ÿå¹¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚è¯¥æŠ€æœ¯è§£å†³äº†å½“å‰æ–¹æ³•åœ¨å¤šå‚è€ƒæœè£…ç»„åˆæ”¯æŒå’Œæ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å†—ä½™é‡æ–°è®¡ç®—å‚è€ƒç‰¹å¾çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚é€šè¿‡å°†ä¼ ç»Ÿçš„æ—¶é—´æ­¥åµŒå…¥æ›¿æ¢ä¸ºå‚è€ƒé¡¹ç›®çš„ç±»åˆ«åµŒå…¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å‚è€ƒç‰¹å¾ç¼–ç ä¸å»å™ªè¿‡ç¨‹çš„å®Œå…¨è§£è€¦ï¼Œä»è€Œåœ¨æ‰€æœ‰æ­¥éª¤ä¸­ä»…è®¡ç®—ä¸€æ¬¡å‚è€ƒç‰¹å¾å¹¶æ— æŸé‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFastFitåœ¨å…³é”®ä¿çœŸåº¦æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ¨ç†æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03867', 'title': 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth', 'url': 'https://huggingface.co/papers/2509.03867', 'abstract': 'LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs\' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence.', 'score': 170, 'issue_id': 5735, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'ec0f3f80763f1bf1', 'authors': ['Yang Wang', 'Chenghao Xiao', 'Chia-Yi Hsiao', 'Zi Yan Chang', 'Chi-Li Chen', 'Tyler Loakman', 'Chenghua Lin'], 'affiliations': ['Durham University', 'The University of Manchester', 'The University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2509.03867.jpg', 'data': {'categories': ['#hallucinations', '#multilingual', '#benchmark', '#reasoning', '#alignment', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ´Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸' - Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ°, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ°Ğº 'Ğ±ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹'. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1200 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ´Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ LLM Ğ¸ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±ĞµĞ³Ğ»Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·ÑƒĞ¼ĞµĞ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."}, 'en': {'title': 'Unlocking the Depths of Nonsense: Understanding Drivelology', 'desc': "This paper introduces Drivelology, a linguistic concept that describes seemingly nonsensical text that actually contains deeper meanings. The authors demonstrate that large language models (LLMs) struggle to interpret these nuanced expressions, which require contextual understanding and moral reasoning. They created a diverse dataset of over 1,200 examples of Drivelological text, carefully annotated to reflect its unique characteristics. The study reveals significant limitations in LLMs' ability to grasp the layered semantics of such text, suggesting that fluency in language does not equate to true comprehension."}, 'zh': {'title': 'æ­ç¤ºæ— æ„ä¹‰å­¦çš„æ·±å±‚è¯­ä¹‰æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œæ— æ„ä¹‰å­¦â€çš„ç‹¬ç‰¹è¯­è¨€ç°è±¡ï¼Œç‰¹å¾æ˜¯è¡¨é¢ä¸Šçœ‹ä¼¼æ— æ„ä¹‰çš„è¡¨è¾¾å®é™…ä¸Šè•´å«æ·±å±‚è¯­ä¹‰ã€‚è¿™äº›è¡¨è¾¾åœ¨è¯­æ³•ä¸Šæ˜¯è¿è´¯çš„ï¼Œä½†åœ¨è¯­ç”¨ä¸Šå´å­˜åœ¨çŸ›ç›¾ï¼Œæƒ…æ„Ÿä¸Šå……æ»¡è´Ÿè½½ï¼Œæˆ–åœ¨ä¿®è¾ä¸Šå…·æœ‰é¢ è¦†æ€§ã€‚å°½ç®¡å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ç†è§£æ— æ„ä¹‰å­¦æ–‡æœ¬çš„å±‚æ¬¡è¯­ä¹‰æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«1200å¤šä¸ªç»è¿‡ç²¾å¿ƒç­–åˆ’çš„ç¤ºä¾‹çš„å°å‹å¤šæ ·åŒ–åŸºå‡†æ•°æ®é›†ï¼Œä»¥è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨åˆ†ç±»ã€ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04338', 'title': 'From Editor to Dense Geometry Estimator', 'url': 'https://huggingface.co/papers/2509.04338', 'abstract': 'FE2E, a framework using a Diffusion Transformer for dense geometry prediction, outperforms generative models in zero-shot monocular depth and normal estimation with improved performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.   Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts.   Based on these findings, we introduce FE2E, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor\'s original flow matching loss into the ``consistent velocity" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor\'s native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT\'s global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.   Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100times data. The project page can be accessed https://amap-ml.github.io/FE2E/{here}.', 'score': 73, 'issue_id': 5732, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '3fbd2457f43c1688', 'authors': ['JiYuan Wang', 'Chunyu Lin', 'Lei Sun', 'Rongying Liu', 'Lang Nie', 'Mingxing Li', 'Kang Liao', 'Xiangxiang Chu', 'Yao Zhao'], 'affiliations': ['AMAP Alibaba Group', 'BJTU', 'CQUPT', 'NTU'], 'pdf_title_img': 'assets/pdf/title_img/2509.04338.jpg', 'data': {'categories': ['#inference', '#cv', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'FE2E: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Diffusion Transformer', 'desc': 'FE2E - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Diffusion Transformer Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞĞ½ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. FE2E Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ DiT Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ‘ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, FE2E Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'FE2E: Revolutionizing Dense Geometry Prediction with Diffusion Transformers', 'desc': "The paper introduces FE2E, a novel framework that utilizes a Diffusion Transformer for predicting dense geometry, specifically focusing on monocular depth and normal estimation. It demonstrates that editing models, which refine existing features, outperform generative models in this context due to their structural priors. The authors reformulate the training objective to enhance the model's performance and employ logarithmic quantization to address precision issues. FE2E achieves significant improvements in performance without requiring additional training data, showcasing its efficiency and effectiveness across various datasets."}, 'zh': {'title': 'FE2Eï¼šå¯†é›†å‡ ä½•é¢„æµ‹çš„æ–°çªç ´', 'desc': 'FE2Eæ˜¯ä¸€ä¸ªä½¿ç”¨æ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶ï¼Œä¸“æ³¨äºå¯†é›†å‡ ä½•é¢„æµ‹ï¼Œè¡¨ç°ä¼˜äºç”Ÿæˆæ¨¡å‹ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬å•ç›®æ·±åº¦å’Œæ³•çº¿ä¼°è®¡æ–¹é¢ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå›¾åƒç¼–è¾‘æ¨¡å‹æ¯”æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹æ›´é€‚åˆè¿›è¡Œå¯†é›†é¢„æµ‹ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰æ›´ç¨³å®šçš„æ”¶æ•›æ€§å’Œæ›´é«˜çš„æ€§èƒ½ã€‚é€šè¿‡é‡æ–°è®¾è®¡ç¼–è¾‘æ¨¡å‹çš„æŸå¤±å‡½æ•°å’Œä½¿ç”¨å¯¹æ•°é‡åŒ–ï¼ŒFE2Eæœ‰æ•ˆè§£å†³äº†ç²¾åº¦é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å…¨å±€æ³¨æ„åŠ›å®ç°æ·±åº¦å’Œæ³•çº¿çš„è”åˆä¼°è®¡ã€‚æœ€ç»ˆï¼ŒFE2Eåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ETH3Dæ•°æ®é›†ä¸Šè¶…è¿‡äº†35%çš„æ€§èƒ½å¢ç›Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04419', 'title': 'Towards a Unified View of Large Language Model Post-Training', 'url': 'https://huggingface.co/papers/2509.04419', 'abstract': 'A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.', 'score': 54, 'issue_id': 5730, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '91f46a497fb0af28', 'authors': ['Xingtai Lv', 'Yuxin Zuo', 'Youbang Sun', 'Hongyi Liu', 'Yuntian Wei', 'Zhekai Chen', 'Lixuan He', 'Xuekai Zhu', 'Kaiyan Zhang', 'Bingning Wang', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University', 'WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.04419.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Hybrid Post-Training ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unifying Online and Offline Learning for Superior Language Model Training', 'desc': 'This paper introduces a Unified Policy Gradient Estimator that integrates online and offline data for enhancing post-training in language models. It demonstrates that traditional methods like Reinforcement Learning and Supervised Fine-Tuning are part of a unified optimization framework. The proposed Hybrid Post-Training (HPT) algorithm adapts training signals to balance between utilizing demonstrations and exploring new strategies. Extensive experiments show that HPT outperforms existing methods across multiple benchmarks, confirming the effectiveness of the unified approach.'}, 'zh': {'title': 'ç»Ÿä¸€ç­–ç•¥ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨å’Œæ··åˆåè®­ç»ƒç®—æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç»“åˆåœ¨çº¿å’Œç¦»çº¿æ•°æ®ï¼Œä»¥æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨çº¿æ•°æ®é€šå¸¸æ¥è‡ªæ¨¡å‹ç”Ÿæˆçš„å›æ»šï¼Œè€Œç¦»çº¿æ•°æ®åˆ™æ˜¯äººç±»æˆ–å…¶ä»–æ¨¡å‹çš„ç¤ºèŒƒã€‚æˆ‘ä»¬è¯æ˜äº†å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒè¿™ä¸¤ç§æ–¹æ³•å¹¶ä¸çŸ›ç›¾ï¼Œè€Œæ˜¯åŒä¸€ä¼˜åŒ–è¿‡ç¨‹çš„ä¸åŒå®ä¾‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºæ¡†æ¶å’Œæ··åˆåè®­ç»ƒç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01396', 'title': "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks", 'url': 'https://huggingface.co/papers/2509.01396', 'abstract': "DeepResearch Arena, a benchmark using academic seminar transcripts, provides high-quality research tasks to evaluate deep research agents across multiple disciplines.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research agents have attracted growing attention for their potential to orchestrate multi-stage research workflows, spanning literature synthesis, methodological design, and empirical verification. Despite these strides, evaluating their research capability faithfully is rather challenging due to the difficulty of collecting frontier research questions that genuinely capture researchers' attention and intellectual curiosity. To address this gap, we introduce DeepResearch Arena, a benchmark grounded in academic seminars that capture rich expert discourse and interaction, better reflecting real-world research environments and reducing the risk of data leakage. To automatically construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task Generation (MAHTG) system that extracts research-worthy inspirations from seminar transcripts. The MAHTG system further translates research-worthy inspirations into high-quality research tasks, ensuring the traceability of research task formulation while filtering noise. With the MAHTG system, we curate DeepResearch Arena with over 10,000 high-quality research tasks from over 200 academic seminars, spanning 12 disciplines, such as literature, history, and science. Our extensive evaluation shows that DeepResearch Arena presents substantial challenges for current state-of-the-art agents, with clear performance gaps observed across different models.", 'score': 47, 'issue_id': 5729, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '85ed0a774ea098f6', 'authors': ['Haiyuan Wan', 'Chen Yang', 'Junchi Yu', 'Meiqi Tu', 'Jiaxuan Lu', 'Di Yu', 'Jianbao Cao', 'Ben Gao', 'Jiaqing Xie', 'Aoran Wang', 'Wenlong Zhang', 'Philip Torr', 'Dongzhan Zhou'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Hong Kong University of Science and Technology, Guangzhou', 'The University of Hong Kong', 'Tsinghua University', 'University of Oxford', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01396.jpg', 'data': {'categories': ['#leakage', '#science', '#agents', '#benchmark', '#survey', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€Ñ‹ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'DeepResearch Arena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Multi-Agent Hierarchical Task Generation (MAHTG) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€ÑĞºĞ¸Ñ… Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 10 000 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 12 Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ğ¾Ñ‚ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒĞº. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Elevating Research Evaluation with DeepResearch Arena', 'desc': 'DeepResearch Arena is a new benchmark designed to evaluate deep research agents by using transcripts from academic seminars. It focuses on creating high-quality research tasks that reflect real-world research environments, addressing the challenge of finding relevant research questions. The benchmark is built using a Multi-Agent Hierarchical Task Generation (MAHTG) system, which extracts valuable insights from seminar discussions and converts them into structured research tasks. With over 10,000 tasks across various disciplines, this benchmark highlights the performance gaps of current research agents, pushing the boundaries of their capabilities.'}, 'zh': {'title': 'æ·±åº¦ç ”ç©¶ä»£ç†çš„æ–°æŒ‘æˆ˜', 'desc': 'DeepResearch Arena æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨å­¦æœ¯ç ”è®¨ä¼šçš„è®°å½•æ¥è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„èƒ½åŠ›ã€‚è¯¥å¹³å°æä¾›é«˜è´¨é‡çš„ç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–å¤šä¸ªå­¦ç§‘ï¼Œå¸®åŠ©è¯„ä¼°ä»£ç†åœ¨æ–‡çŒ®ç»¼åˆã€æ–¹æ³•è®¾è®¡å’Œå®è¯éªŒè¯ç­‰å¤šé˜¶æ®µç ”ç©¶å·¥ä½œæµä¸­çš„è¡¨ç°ã€‚ä¸ºäº†æ„å»ºè¿™ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ™ºèƒ½ä½“å±‚æ¬¡ä»»åŠ¡ç”Ÿæˆç³»ç»Ÿï¼ˆMAHTGï¼‰ï¼Œä»ç ”è®¨ä¼šè®°å½•ä¸­æå–ç ”ç©¶çµæ„Ÿï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºé«˜è´¨é‡çš„ç ”ç©¶ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒDeepResearch Arena å¯¹å½“å‰æœ€å…ˆè¿›çš„ç ”ç©¶ä»£ç†æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œæ˜¾ç¤ºå‡ºä¸åŒæ¨¡å‹ä¹‹é—´çš„æ˜æ˜¾æ€§èƒ½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04292', 'title': 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?', 'url': 'https://huggingface.co/papers/2509.04292', 'abstract': "Inverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios.", 'score': 45, 'issue_id': 5732, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '65e94af989385211', 'authors': ['Qinyan Zhang', 'Xinping Lei', 'Ruijie Miao', 'Yu Fu', 'Haojie Fan', 'Le Chang', 'Jiafan Hou', 'Dingling Zhang', 'Zhongfei Hou', 'Ziqiang Yang', 'Changxin Pu', 'Fei Hu', 'Jingkai Liu', 'Mengyun Liu', 'Yang Liu', 'Xiang Gao', 'Jiaheng Liu', 'Tong Yang', 'Zaiyuan Wang', 'Ge Zhang', 'Wenhao Huang'], 'affiliations': ['ByteDance', 'Jiyun Hudong', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04292.jpg', 'data': {'categories': ['#multilingual', '#hallucinations', '#dataset', '#benchmark', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½ĞµÑ€Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Inverse IFEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 1012 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing LLMs: Overcoming Biases for Better Adaptability', 'desc': "The paper introduces Inverse IFEval, a benchmark designed to assess the ability of Large Language Models (LLMs) to overcome biases from their training and follow unconventional instructions. It highlights that while LLMs perform well on many tasks, they often struggle with instructions that deviate from their learned patterns, a phenomenon known as cognitive inertia. The benchmark includes eight challenge types, such as Question Correction and Counterfactual Answering, to evaluate models' adaptability across various contexts. The findings suggest that improving LLMs requires not just fluency and accuracy, but also the ability to adapt to unexpected instructions in real-world applications."}, 'zh': {'title': 'è¯„ä¼°è¯­è¨€æ¨¡å‹çš„é€‚åº”èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†Inverse IFEvalåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹ä¸è®­ç»ƒåè§ç›¸æ‚–çš„æŒ‡ä»¤æ—¶çš„é€‚åº”èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨éµå¾ªä¸æ ‡å‡†åŒ–æ¨¡å¼ç›¸å†²çªçš„æŒ‡ä»¤æ—¶å¸¸å¸¸è¡¨ç°å‡ºè®¤çŸ¥æƒ¯æ€§ã€‚Inverse IFEvalå¼•å…¥äº†å…«ç§æŒ‘æˆ˜ç±»å‹ï¼Œæ—¨åœ¨æµ‹é‡æ¨¡å‹å…‹æœè®­ç»ƒåè§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœªæ¥çš„å¯¹é½å·¥ä½œä¸ä»…è¦å…³æ³¨æµç•…æ€§å’Œäº‹å®æ­£ç¡®æ€§ï¼Œè¿˜è¦è€ƒè™‘åœ¨éå¸¸è§„ç¯å¢ƒä¸‹çš„é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04394', 'title': 'Transition Models: Rethinking the Generative Learning Objective', 'url': 'https://huggingface.co/papers/2509.04394', 'abstract': 'A novel generative paradigm, Transition Models (TiM), addresses the trade-off between computational cost and output quality in generative modeling by using a continuous-time dynamics equation.  \t\t\t\t\tAI-generated summary \t\t\t\t A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.', 'score': 21, 'issue_id': 5730, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '872ae225cb79c916', 'authors': ['Zidong Wang', 'Yiyuan Zhang', 'Xiaoyu Yue', 'Xiangyu Yue', 'Yangguang Li', 'Wanli Ouyang', 'Lei Bai'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Lab', 'USYD'], 'pdf_title_img': 'assets/pdf/title_img/2509.04394.jpg', 'data': {'categories': ['#small_models', '#optimization', '#generative_modeling', '#diffusion', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'TiM: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Transition Models (TiM). TiM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ TiM Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (865 Ğ¼Ğ»Ğ½), TiM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼.'}, 'en': {'title': 'Transition Models: Bridging Quality and Efficiency in Generative Modeling', 'desc': 'The paper introduces Transition Models (TiM), a new approach in generative modeling that balances computational efficiency and output quality. It tackles the existing dilemma where high-fidelity models require extensive computation, while faster models compromise on quality. TiM utilizes a continuous-time dynamics equation to define state transitions, allowing for flexible generation steps that can adapt from coarse to fine outputs. With only 865 million parameters, TiM outperforms larger models in quality and maintains consistent improvements as more computational resources are allocated.'}, 'zh': {'title': 'è¿‡æ¸¡æ¨¡å‹ï¼šé«˜æ•ˆç”Ÿæˆä¸ä¼˜è´¨è¾“å‡ºçš„å®Œç¾å¹³è¡¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹èŒƒå¼â€”â€”è¿‡æ¸¡æ¨¡å‹ï¼ˆTiMï¼‰ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå»ºæ¨¡ä¸­è®¡ç®—æˆæœ¬ä¸è¾“å‡ºè´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚ä¼ ç»Ÿçš„è¿­ä»£æ‰©æ•£æ¨¡å‹è™½ç„¶ç”Ÿæˆè´¨é‡é«˜ï¼Œä½†è®¡ç®—å¼€é”€å¤§ï¼Œè€Œé«˜æ•ˆçš„å°‘æ­¥ç”Ÿæˆæ–¹æ³•åˆ™å—åˆ°è´¨é‡ä¸Šé™çš„é™åˆ¶ã€‚TiMé€šè¿‡å¼•å…¥ç²¾ç¡®çš„è¿ç»­æ—¶é—´åŠ¨æ€æ–¹ç¨‹ï¼Œå®šä¹‰äº†ä»»æ„æœ‰é™æ—¶é—´é—´éš”å†…çš„çŠ¶æ€è½¬ç§»ï¼Œä»è€Œå®ç°äº†çµæ´»çš„ç”Ÿæˆè¿‡ç¨‹ã€‚å°½ç®¡å‚æ•°é‡ä»…ä¸º8.65äº¿ï¼ŒTiMåœ¨æ‰€æœ‰è¯„ä¼°çš„æ­¥æ•°ä¸­éƒ½è¶…è¶Šäº†é¢†å…ˆæ¨¡å‹ï¼Œå±•ç°å‡ºéšç€é‡‡æ ·é¢„ç®—å¢åŠ è€ŒæŒç»­æå‡çš„ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04011', 'title': 'NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings', 'url': 'https://huggingface.co/papers/2509.04011', 'abstract': 'NER Retriever uses internal representations from large language models to perform zero-shot named entity retrieval by embedding entity mentions and type descriptions into a shared semantic space, outperforming lexical and dense sentence-level retrieval methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever', 'score': 17, 'issue_id': 5733, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'f55762e727076e6b', 'authors': ['Or Shachar', 'Uri Katz', 'Yoav Goldberg', 'Oren Glickman'], 'affiliations': ['Computer Science Department, Bar-Ilan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04011.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#transfer_learning', '#multimodal', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· ÑÑ…ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'NER Retriever - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ‰ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡ĞµĞ¼ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸. NER Retriever Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Zero-Shot Entity Retrieval with NER Retriever', 'desc': 'NER Retriever is a novel framework for zero-shot Named Entity Retrieval (NER) that allows users to retrieve documents based on entity types without prior definitions. It utilizes internal representations from large language models (LLMs) to create a shared semantic space for embedding both entity mentions and user-defined type descriptions. By leveraging mid-layer transformer block value vectors, the method captures fine-grained type information more effectively than traditional embeddings. A contrastive projection network further refines these embeddings, resulting in a compact and type-aware representation that excels in nearest-neighbor search, outperforming existing retrieval methods.'}, 'zh': {'title': 'é›¶-shotå‘½åå®ä½“æ£€ç´¢çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'NER Retriever æ˜¯ä¸€ç§é›¶-shotå‘½åå®ä½“æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·å®šä¹‰çš„ç±»å‹æè¿°æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œè€Œæ— éœ€äº‹å…ˆæä¾›æ„Ÿå…´è¶£çš„ç±»å‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå°†å®ä½“æåŠå’Œç±»å‹æè¿°åµŒå…¥åˆ°å…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¯æ±‡å’Œå¯†é›†å¥å­çº§æ£€ç´¢æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒè½»é‡çº§å¯¹æ¯”æŠ•å½±ç½‘ç»œæ¥ä¼˜åŒ–è¿™äº›è¡¨ç¤ºï¼Œä½¿å¾—ç›¸ä¼¼ç±»å‹çš„å®ä½“èƒ½å¤Ÿå¯¹é½ï¼Œè€Œä¸ç›¸å…³çš„ç±»å‹åˆ™è¢«åˆ†å¼€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNER Retriever åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ£€ç´¢åŸºçº¿ï¼Œå±•ç¤ºäº†åœ¨æ— æ¨¡å¼å®ä½“æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20478', 'title': 'Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding', 'url': 'https://huggingface.co/papers/2508.20478', 'abstract': 'Video-MTR, a reinforced multi-turn reasoning framework, improves long-form video understanding by iteratively selecting key segments and comprehending questions, outperforming existing methods in accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding.', 'score': 16, 'issue_id': 5739, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '912f591f05e3e422', 'authors': ['Yuan Xie', 'Tianshui Chen', 'Zheng Ge', 'Lionel Ni'], 'affiliations': ['Guangdong University of Technology', 'StepFun AI', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2508.20478.jpg', 'data': {'categories': ['#long_context', '#video', '#rl', '#reasoning'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Video-MTR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Video-MTR ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Long-Form Video Understanding with Iterative Reasoning', 'desc': 'Video-MTR is a new framework that enhances the understanding of long videos by using a reinforced multi-turn reasoning approach. It iteratively selects important video segments and comprehends questions, allowing for a more detailed analysis than traditional single-turn methods. The framework introduces a unique gated bi-level reward system that improves both the selection of video segments and the understanding of questions without relying on external visual-language models. Experiments show that Video-MTR achieves better accuracy and efficiency compared to existing techniques, marking a significant advancement in long-form video understanding.'}, 'zh': {'title': 'Video-MTRï¼šæå‡é•¿è§†é¢‘ç†è§£çš„æ–°æ–¹æ³•', 'desc': 'Video-MTRæ˜¯ä¸€ç§å¢å¼ºçš„å¤šè½®æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚å®ƒé€šè¿‡è¿­ä»£é€‰æ‹©å…³é”®è§†é¢‘ç‰‡æ®µå’Œç†è§£é—®é¢˜ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚ä¸å•è½®æ¨ç†ä¸åŒï¼ŒVideo-MTRåœ¨å¤šä¸ªå›åˆä¸­è¿›è¡Œæ¨ç†ï¼Œé€æ­¥é€‰æ‹©è§†é¢‘ç‰‡æ®µï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„åˆ†æã€‚é€šè¿‡å¼•å…¥æ–°é¢–çš„é—¨æ§åŒå±‚å¥–åŠ±ç³»ç»Ÿï¼ŒVideo-MTRä¼˜åŒ–äº†è§†é¢‘ç‰‡æ®µé€‰æ‹©å’Œé—®é¢˜ç†è§£ï¼Œé¿å…äº†å¯¹å¤–éƒ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03059', 'title': 'Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers', 'url': 'https://huggingface.co/papers/2509.03059', 'abstract': 'The Loong Project introduces a framework for generating and verifying synthetic data to improve reasoning capabilities in Large Language Models through Reinforcement Learning with Verifiable Reward.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong.', 'score': 15, 'issue_id': 5748, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': 'e6a6ec82e8db2767', 'authors': ['Xingyue Huang', 'Rishabh', 'Gregor Franke', 'Ziyi Yang', 'Jiamu Bai', 'Weijie Bai', 'Jinhe Bi', 'Zifeng Ding', 'Yiqun Duan', 'Chengyu Fan', 'Wendong Fan', 'Xin Gao', 'Ruohao Guo', 'Yuan He', 'Zhuangzhuang He', 'Xianglong Hu', 'Neil Johnson', 'Bowen Li', 'Fangru Lin', 'Siyu Lin', 'Tong Liu', 'Yunpu Ma', 'Hao Shen', 'Hao Sun', 'Beibei Wang', 'Fangyijie Wang', 'Hao Wang', 'Haoran Wang', 'Yang Wang', 'Yifeng Wang', 'Zhaowei Wang', 'Ziyang Wang', 'Yifan Wu', 'Zikai Xiao', 'Chengxing Xie', 'Fan Yang', 'Junxiao Yang', 'Qianshuo Ye', 'Ziyu Ye', 'Guangtao Zeng', 'Yuwen Ebony Zhang', 'Zeyu Zhang', 'Zihao Zhu', 'Bernard Ghanem', 'Philip Torr', 'Guohao Li'], 'affiliations': ['CAMEL-AI.org', 'eigent.ai'], 'pdf_title_img': 'assets/pdf/title_img/2509.03059.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#rl', '#multimodal', '#dataset', '#data', '#reasoning', '#benchmark'], 'emoji': 'ğŸ‰', 'ru': {'title': 'Loong: ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ Loong Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ†ĞµĞ»ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ LoongBench - ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 8,729 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² 12 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¸ LoongEnv - ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ñ†Ğ¸ĞºĞ» Ğ°Ğ³ĞµĞ½Ñ‚-ÑÑ€ĞµĞ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT), ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ´Ğ°. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑ…Ğ° RLVR Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering LLMs with Synthetic Data for Enhanced Reasoning', 'desc': 'The Loong Project presents a new framework designed to enhance the reasoning abilities of Large Language Models (LLMs) using Reinforcement Learning with Verifiable Reward (RLVR). It addresses the challenge of generating high-quality synthetic data for reasoning-intensive tasks by providing a curated dataset called LoongBench and a modular environment for synthetic data generation, known as LoongEnv. LoongBench includes thousands of human-verified examples across various domains, while LoongEnv allows for the creation of new question-answer-code triples through different prompting strategies. This framework enables LLMs to learn and improve their reasoning by rewarding them for producing correct Chain-of-Thought solutions that match executable code outputs.'}, 'zh': {'title': 'åˆæˆæ•°æ®ç”Ÿæˆä¸éªŒè¯çš„åˆ›æ–°æ¡†æ¶', 'desc': 'Loongé¡¹ç›®æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’ŒéªŒè¯åˆæˆæ•°æ®ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šLoongBenchï¼Œä¸€ä¸ªåŒ…å«8729ä¸ªç»è¿‡äººå·¥å®¡æ ¸çš„ç¤ºä¾‹çš„ç§å­æ•°æ®é›†ï¼Œæ¶µç›–12ä¸ªé¢†åŸŸï¼›ä»¥åŠLoongEnvï¼Œä¸€ä¸ªæ¨¡å—åŒ–çš„åˆæˆæ•°æ®ç”Ÿæˆç¯å¢ƒï¼Œæ”¯æŒå¤šç§æç¤ºç­–ç•¥ç”Ÿæˆæ–°çš„é—®ç­”ä»£ç ä¸‰å…ƒç»„ã€‚é€šè¿‡è¿™äº›ç»„ä»¶ï¼Œå½¢æˆäº†ä¸€ä¸ªä»£ç†-ç¯å¢ƒå¾ªç¯ï¼Œä½¿å¾—åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆç¬¦åˆä»£ç æ‰§è¡Œç­”æ¡ˆçš„æ€ç»´é“¾è§£å†³æ–¹æ¡ˆæ¥è·å¾—å¥–åŠ±ã€‚æˆ‘ä»¬å¯¹LoongBenchè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°é¢†åŸŸè¦†ç›–ç‡å¹¶æ­ç¤ºæ€§èƒ½ç“¶é¢ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04406', 'title': 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation', 'url': 'https://huggingface.co/papers/2509.04406', 'abstract': 'A novel framework, MDT-dist, accelerates 3D flow generation by distilling pretrained models to learn Marginal-Data Transport through Velocity Matching and Velocity Distillation, reducing sampling steps and improving speed and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.', 'score': 8, 'issue_id': 5732, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'e5f91b790b483c4a', 'authors': ['Zanwei Zhou', 'Taoran Yi', 'Jiemin Fang', 'Chen Yang', 'Lingxi Xie', 'Xinggang Wang', 'Wei Shen', 'Qi Tian'], 'affiliations': ['Huawei Inc.', 'Huazhong University of Science and Technology', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04406.jpg', 'data': {'categories': ['#3d', '#inference', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²', 'desc': 'MDT-dist - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ 25 Ğ´Ğ¾ 1-2, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. MDT-dist Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Accelerating 3D Flow Generation with MDT-dist', 'desc': 'The paper introduces MDT-dist, a new framework designed to speed up 3D flow generation by using a technique called distillation on pretrained models. It focuses on learning Marginal-Data Transport through two main strategies: Velocity Matching (VM) and Velocity Distillation (VD). VM helps align the velocity fields of the student and teacher models, while VD improves the process by distilling probability densities from the learned velocity fields. This approach significantly reduces the number of sampling steps needed, achieving faster generation times while maintaining high quality in the output.'}, 'zh': {'title': 'MDT-distï¼šåŠ é€Ÿ3Dæµç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶MDT-distï¼Œç”¨äºåŠ é€Ÿ3Dæµç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œå­¦ä¹ è¾¹é™…æ•°æ®ä¼ è¾“ï¼Œç»“åˆé€Ÿåº¦åŒ¹é…å’Œé€Ÿåº¦è’¸é¦ï¼Œæ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ­¥éª¤ï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦å’Œä¿çœŸåº¦ã€‚ä¸ä¼ ç»Ÿçš„3Dç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒMDT-distèƒ½å¤Ÿå°†æ¯ä¸ªæµå˜æ¢å™¨çš„é‡‡æ ·æ­¥éª¤ä»25å‡å°‘åˆ°1æˆ–2ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„è§†è§‰å’Œå‡ ä½•ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ­¥3Dç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„è’¸é¦æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04434', 'title': 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer', 'url': 'https://huggingface.co/papers/2509.04434', 'abstract': 'Durian uses dual reference networks and a diffusion model to generate high-fidelity portrait animations with attribute transfer from a reference image to a target portrait in a zero-shot manner.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.', 'score': 5, 'issue_id': 5731, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '4b84700e944a41a7', 'authors': ['Hyunsoo Cha', 'Byungjun Kim', 'Hanbyul Joo'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2509.04434.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#video'], 'emoji': 'ğŸ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Durian Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ°Ğ¼Ğ¾Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Durian Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Portraits: High-Fidelity Animation with Attribute Transfer', 'desc': 'Durian is a novel method for creating high-quality portrait animations that can transfer facial attributes from a reference image to a target portrait without needing prior examples. It employs dual reference networks to enhance the denoising process of a diffusion model, ensuring that the transferred attributes maintain spatial consistency across video frames. The model is trained using a self-reconstruction approach, where it learns to generate frames based on a reference and target portrait, while also incorporating a mask expansion strategy for better attribute transfer. This innovative design allows Durian to effectively handle various attributes and combinations, achieving top performance in the field of portrait animation.'}, 'zh': {'title': 'Durianï¼šé«˜ä¿çœŸè‚–åƒåŠ¨ç”»çš„åˆ›æ–°æ–¹æ³•', 'desc': 'Durianæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å‚è€ƒå›¾åƒç”Ÿæˆé«˜ä¿çœŸåº¦çš„è‚–åƒåŠ¨ç”»ã€‚å®ƒä½¿ç”¨åŒé‡å‚è€ƒç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œå°†é¢éƒ¨å±æ€§ä»å‚è€ƒå›¾åƒè½¬ç§»åˆ°ç›®æ ‡è‚–åƒã€‚é€šè¿‡è‡ªé‡å»ºçš„æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œä½¿å¾—ä¸åŒå¸§ä¹‹é—´çš„å±æ€§è½¬ç§»ä¿æŒä¸€è‡´æ€§ã€‚Durianåœ¨è‚–åƒåŠ¨ç”»å’Œå±æ€§è½¬ç§»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡ç”Ÿæˆä¸­å®ç°å¤šå±æ€§ç»„åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04442', 'title': 'Delta Activations: A Representation for Finetuned Large Language Models', 'url': 'https://huggingface.co/papers/2509.04442', 'abstract': 'Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.  \t\t\t\t\tAI-generated summary \t\t\t\t The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.', 'score': 3, 'issue_id': 5735, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '91b81a1c3795c65f', 'authors': ['Zhiqiu Xu', 'Amish Sethi', 'Mayur Naik', 'Ser-Nam Lim'], 'affiliations': ['University of Central Florida', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2509.04442.jpg', 'data': {'categories': ['#architecture', '#training', '#transfer_learning', '#dataset', '#data', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Delta Activations: ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Delta Activations Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Delta Activations Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· few-shot Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Streamlining Model Reuse with Delta Activations', 'desc': 'Delta Activations is a novel approach that represents fine-tuned models as vector embeddings by analyzing shifts in their internal activations compared to a base model. This method enhances the organization of models by enabling effective clustering based on specific tasks and domains, making it easier to navigate the vast array of post-trained models. Additionally, Delta Activations shows robustness across different finetuning scenarios and maintains an additive property when combining datasets. The technique also supports few-shot finetuning for task embedding, aiding in model selection and merging, ultimately promoting the reuse of publicly available models.'}, 'zh': {'title': 'Delta Activationsï¼šæ¨¡å‹é‡ç”¨çš„æ–°æ–¹æ³•', 'desc': 'Delta Activationsæ˜¯ä¸€ç§é€šè¿‡æµ‹é‡æ¨¡å‹å†…éƒ¨æ¿€æ´»çš„å˜åŒ–ï¼Œå°†å¾®è°ƒæ¨¡å‹è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥çš„æ–¹æ³•ã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼ä½¿å¾—æ ¹æ®é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œæœ‰æ•ˆçš„èšç±»æˆä¸ºå¯èƒ½ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹çš„ç»“æ„ã€‚Delta Activationsåœ¨å¾®è°ƒè®¾ç½®ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨æ··åˆå¾®è°ƒæ•°æ®é›†æ—¶å…·æœ‰å¯åŠ æ€§ã€‚æˆ‘ä»¬å¸Œæœ›Delta Activationsèƒ½å¤Ÿä¿ƒè¿›å…¬å…±å¯ç”¨æ¨¡å‹çš„é‡ç”¨å®è·µã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.18733', 'title': 'Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings', 'url': 'https://huggingface.co/papers/2508.18733', 'abstract': 'Drawing2CAD is a framework that converts 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach with a dual-decoder transformer architecture and a soft target distribution loss function.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD.', 'score': 3, 'issue_id': 5729, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 26', 'zh': '8æœˆ26æ—¥'}, 'hash': '4b2fa59592c89297', 'authors': ['Feiwei Qin', 'Shichao Lu', 'Junhao Hou', 'Changmiao Wang', 'Meie Fang', 'Ligang Liu'], 'affiliations': ['Guangzhou University, Guangzhou, China', 'Hangzhou Dianzi University, Hangzhou, China', 'Shenzhen Research Institute of Big Data, Shenzhen, China', 'University of Science and Technology of China, Hefei, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.18733.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#dataset', '#open_source'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞÑ‚ Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ° Ğº CAD: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· 2D-Ñ‡ĞµÑ€Ñ‚ĞµĞ¶ĞµĞ¹', 'desc': 'Drawing2CAD - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ 2D Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ¸ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ sequence-to-sequence learning Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¼ÑĞ³ĞºĞ¸Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CAD ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… CAD-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¼ÑĞ³ĞºĞ¸Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Transforming 2D Drawings into Precise CAD Models with AI', 'desc': 'Drawing2CAD is a novel framework that transforms 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach. It employs a dual-decoder transformer architecture to effectively separate the generation of command types and their corresponding parameters, ensuring geometric accuracy. The framework utilizes a soft target distribution loss function to handle the variability in CAD parameters, enhancing flexibility during model training. To validate its effectiveness, the authors introduce the CAD-VGDrawing dataset, which pairs engineering drawings with their corresponding CAD models, and conduct comprehensive experiments.'}, 'zh': {'title': 'å°†äºŒç»´å›¾çº¸æ™ºèƒ½è½¬åŒ–ä¸ºCADæ¨¡å‹', 'desc': 'Drawing2CADæ˜¯ä¸€ä¸ªå°†äºŒç»´çŸ¢é‡å›¾è½¬æ¢ä¸ºå‚æ•°åŒ–CADæ¨¡å‹çš„æ¡†æ¶ï¼Œé‡‡ç”¨åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åŒè§£ç å™¨å˜æ¢å™¨æ¶æ„å’Œè½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ä¿æŒå‡ ä½•ç²¾åº¦å’Œè®¾è®¡æ„å›¾ã€‚é€šè¿‡å°†CADç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºåºåˆ—åˆ°åºåˆ—å­¦ä¹ é—®é¢˜ï¼ŒDrawing2CADèƒ½å¤Ÿç›´æ¥åˆ©ç”¨çŸ¢é‡å›¾åŸè¯­ç”ŸæˆCADæ“ä½œã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†CAD-VGDrawingæ•°æ®é›†ï¼Œä»¥è®­ç»ƒå’Œè¯„ä¼°è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03888', 'title': 'False Sense of Security: Why Probing-based Malicious Input Detection\n  Fails to Generalize', 'url': 'https://huggingface.co/papers/2509.03888', 'abstract': "Probing-based approaches for detecting harmful instructions in LLMs are found to rely on superficial patterns rather than semantic understanding, indicating a need for redesigning models and evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.", 'score': 2, 'issue_id': 5729, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'db5043bdc42c7fda', 'authors': ['Cheng Wang', 'Zeming Wei', 'Qin Liu', 'Muhao Chen'], 'affiliations': ['National University of Singapore', 'Peking University', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2509.03888.jpg', 'data': {'categories': ['#security', '#training', '#alignment', '#benchmark', '#data', '#open_source'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ—Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM: Ğ·Ğ° Ñ„Ğ°ÑĞ°Ğ´Ğ¾Ğ¼ ĞºĞ°Ğ¶ÑƒÑ‰ĞµĞ¹ÑÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ğ° Ğ½Ğµ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°, Ğ° Ğ½Ğµ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñƒ ĞºĞ°Ğº ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM.'}, 'en': {'title': 'Rethinking Safety: Beyond Superficial Patterns in LLMs', 'desc': 'This paper investigates the effectiveness of probing-based methods used to detect harmful instructions in Large Language Models (LLMs). The authors find that these methods often rely on superficial patterns, such as specific words or phrases, rather than a true understanding of the semantic meaning behind harmful instructions. Through a series of experiments, they demonstrate that simpler n-gram models can perform similarly, indicating that current probing techniques may provide a false sense of security. The paper calls for a redesign of both the models and the evaluation methods to improve safety in AI systems.'}, 'zh': {'title': 'é‡å¡‘å®‰å…¨æ£€æµ‹ï¼šè¶…è¶Šè¡¨é¢æ¨¡å¼', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºæ¢æµ‹çš„æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ£€æµ‹æœ‰å®³æŒ‡ä»¤çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºè¡¨é¢æ¨¡å¼ï¼Œè€ŒéçœŸæ­£çš„è¯­ä¹‰ç†è§£ï¼Œå¯¼è‡´å®‰å…¨æ€§è¯„ä¼°å­˜åœ¨ç¼ºé™·ã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒï¼Œæˆ‘ä»¬ç¡®è®¤æ¢æµ‹å™¨å­¦ä¹ åˆ°çš„æ˜¯æŒ‡ä»¤æ¨¡å¼å’Œè§¦å‘è¯ï¼Œè€ŒéçœŸæ­£çš„æœ‰å®³æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„æ¢æµ‹æ–¹æ³•å­˜åœ¨è¯¯å¯¼æ€§å®‰å…¨æ„Ÿï¼ŒäºŸéœ€é‡æ–°è®¾è®¡æ¨¡å‹å’Œè¯„ä¼°åè®®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01106', 'title': 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning', 'url': 'https://huggingface.co/papers/2509.01106', 'abstract': 'Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.', 'score': 32, 'issue_id': 5707, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': 'd0766d32afe23fec', 'authors': ['Huang Fang', 'Mengxi Zhang', 'Heng Dong', 'Wei Li', 'Zixuan Wang', 'Qifeng Zhang', 'Xueyun Tian', 'Yucheng Hu', 'Hang Li'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2509.01106.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#alignment', '#robotics', '#multimodal', '#agents', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Robix: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Robix - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Robix Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°.'}, 'en': {'title': 'Robix: Revolutionizing Robot Interaction and Task Execution', 'desc': 'Robix is a unified vision-language model designed to enhance robot reasoning, task planning, and natural language interaction. It operates as a cognitive layer in robotic systems, generating commands and responses that allow robots to execute complex tasks and interact with humans effectively. The model employs chain-of-thought reasoning and a three-stage training strategy, which includes pretraining, supervised finetuning, and reinforcement learning to improve its performance. Experiments show that Robix significantly outperforms existing models in executing diverse interactive tasks, showcasing its ability to generalize across various instruction types.'}, 'zh': {'title': 'Robixï¼šæ™ºèƒ½æœºå™¨äººäº¤äº’çš„æ–°çºªå…ƒ', 'desc': 'Robixæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†æœºå™¨äººæ¨ç†ã€ä»»åŠ¡è§„åˆ’å’Œè‡ªç„¶è¯­è¨€äº¤äº’ã€‚å®ƒé€šè¿‡é“¾å¼æ€ç»´æ¨ç†å’Œä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå±•ç¤ºäº†åœ¨äº¤äº’ä»»åŠ¡æ‰§è¡Œä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚Robixèƒ½å¤ŸåŠ¨æ€ç”ŸæˆåŸå­å‘½ä»¤å’Œäººæœºäº¤äº’çš„è¯­è¨€å“åº”ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œå¤æ‚æŒ‡ä»¤å¹¶è¿›è¡Œè‡ªç„¶äº’åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼ŒRobixåœ¨å¤šç§æŒ‡ä»¤ç±»å‹å’Œç”¨æˆ·å‚ä¸çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00375', 'title': 'Open Data Synthesis For Deep Research', 'url': 'https://huggingface.co/papers/2509.00375', 'abstract': 'InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.', 'score': 30, 'issue_id': 5707, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': 'd7d79c964b418fac', 'authors': ['Ziyi Xia', 'Kun Luo', 'Hongjin Qian', 'Zheng Liu'], 'affiliations': ['BAAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.00375.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#reasoning', '#benchmark', '#multimodal', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'InfoSeek: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'InfoSeek - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ ĞµĞ³Ğ¾ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° InfoSeek, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. InfoSeek Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Deep Research with Hierarchical Constraints', 'desc': 'InfoSeek is a novel framework designed to tackle complex Deep Research tasks by structuring them as Hierarchical Constraint Satisfaction Problems (HCSPs). This approach allows models to break down intricate questions into manageable sub-problems, facilitating multi-step reasoning and evidence synthesis from various sources. Unlike traditional benchmarks, InfoSeek generates a large dataset of over 50,000 training examples that reflect the complexity of real-world queries without shortcut reasoning. Experiments demonstrate that models trained with InfoSeek significantly outperform larger models on challenging benchmarks, showcasing its effectiveness in enhancing the capabilities of large language models.'}, 'zh': {'title': 'InfoSeekï¼šæ·±åº¦ç ”ç©¶ä»»åŠ¡çš„æ–°æ¡†æ¶', 'desc': 'InfoSeekæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤æ‚çš„æ·±åº¦ç ”ç©¶ä»»åŠ¡ï¼Œé€šè¿‡åˆæˆå±‚æ¬¡çº¦æŸæ»¡è¶³é—®é¢˜ï¼Œä½¿æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šæ›´å¤§çš„åŸºçº¿ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åŒä»£ç†ç³»ç»Ÿï¼Œä»å¤§è§„æ¨¡ç½‘é¡µé€’å½’æ„å»ºç ”ç©¶æ ‘ï¼Œå°†ä¸­é—´èŠ‚ç‚¹æ¨¡ç³ŠåŒ–ä¸ºæœ‰æ•ˆçš„å­é—®é¢˜ï¼Œå¹¶å°†è¿™äº›æ ‘è½¬æ¢ä¸ºéœ€è¦éå†å®Œæ•´å±‚æ¬¡çš„è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚InfoSeekèƒ½å¤Ÿå¿«é€Ÿæ‰©å±•ï¼Œç”Ÿæˆè¶…è¿‡50,000ä¸ªè®­ç»ƒç¤ºä¾‹ï¼Œå¹¶æä¾›ç»è¿‡ç­–åˆ’çš„æµ‹è¯•é›†å’Œé€šè¿‡æ‹’ç»é‡‡æ ·ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºInfoSeekè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šå¤§å‹æ¨¡å‹å’Œå•†ä¸šAPIã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03405', 'title': 'LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations', 'url': 'https://huggingface.co/papers/2509.03405', 'abstract': 'LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics.', 'score': 16, 'issue_id': 5713, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': 'c66ad503192a18e1', 'authors': ['Daniela Gottesman', 'Alon Gilae-Dotan', 'Ido Cohen', 'Yoav Gur-Arieh', 'Marius Mosbach', 'Ori Yoran', 'Mor Geva'], 'affiliations': ['McGill University', 'Mila Quebec AI Institute', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2509.03405.jpg', 'data': {'categories': ['#data', '#open_source', '#interpretability', '#dataset', '#training', '#benchmark', '#science', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LMEnt: Ğ—Ğ°Ğ³Ğ»ÑĞ´Ñ‹Ğ²Ğ°Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LMEnt - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. LMEnt Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ 12 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unlocking Knowledge Acquisition in Language Models with LMEnt', 'desc': "LMEnt is a comprehensive toolkit designed to analyze how language models acquire knowledge during their pretraining phase. It includes a specially annotated corpus based on Wikipedia, an advanced retrieval method that significantly improves performance, and a set of pretrained models with substantial parameters. This suite allows researchers to explore the relationship between entity mentions in the training data and the models' performance on knowledge tasks. By providing insights into knowledge representations and learning dynamics, LMEnt aims to enhance the development of more reliable and complete language models."}, 'zh': {'title': 'LMEntï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹çŸ¥è¯†è·å–çš„ç§˜å¯†', 'desc': 'LMEntæ˜¯ä¸€ä¸ªç”¨äºåˆ†æè¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çŸ¥è¯†è·å–çš„å·¥å…·å¥—ä»¶ã€‚å®ƒæä¾›äº†å¸¦æ³¨é‡Šçš„è¯­æ–™åº“ã€æ£€ç´¢æ–¹æ³•å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ç ”ç©¶çŸ¥è¯†è¡¨ç¤ºå’Œå­¦ä¹ åŠ¨æ€ã€‚é€šè¿‡LMEntï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹å¦‚ä½•å°†æ•°æ®è½¬åŒ–ä¸ºå¯¹ä¸–ç•Œçš„çŸ¥è¯†å’Œä¿¡å¿µã€‚è¯¥å·¥å…·çš„æ¨å‡ºæœ‰åŠ©äºå¼€å‘æ›´ä¸€è‡´ã€ç¨³å¥å’Œå®Œæ•´çš„çŸ¥è¯†è¡¨ç¤ºçš„è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01977', 'title': 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement', 'url': 'https://huggingface.co/papers/2509.01977', 'abstract': 'MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications.', 'score': 7, 'issue_id': 5711, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '5a1bf5cc33f3e20f', 'authors': ['Dong She', 'Siming Fu', 'Mushui Liu', 'Qiaoqiao Jin', 'Hualiang Wang', 'Mu Liu', 'Jidong Jiang'], 'affiliations': ['ByteDance', 'The Hong Kong University of Science and Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01977.jpg', 'data': {'categories': ['#dataset', '#cv', '#synthetic', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'MOSAIC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SemAlign-MS Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. MOSAIC Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ loss-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 4 Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'MOSAIC: Mastering Multi-Subject Image Generation with Precision', 'desc': "The MOSAIC framework improves the generation of images featuring multiple subjects by focusing on precise semantic alignment and separating features effectively. It addresses common issues like identity blending and attribute leakage that arise when synthesizing images from multiple references. By introducing a new dataset, SemAlign-MS, it provides detailed semantic correspondences, which helps in maintaining clarity in the generated images. The framework's innovative loss functions ensure that different subjects are represented distinctly, allowing for high-quality image generation even with more than three subjects."}, 'zh': {'title': 'MOSAICï¼šå¤šä¸»ä½“å›¾åƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'MOSAICæ¡†æ¶é€šè¿‡ç¡®ä¿ç²¾ç¡®çš„è¯­ä¹‰å¯¹é½å’Œæ­£äº¤ç‰¹å¾è§£è€¦ï¼Œå¢å¼ºäº†å¤šä¸»ä½“å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤šä¸ªå‚è€ƒä¸»ä½“æ¡ä»¶ä¸‹åˆæˆå›¾åƒæ—¶èº«ä»½ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§çš„é—®é¢˜ã€‚MOSAICå¼•å…¥äº†SemAlign-MSæ•°æ®é›†ï¼Œæä¾›äº†å¤šå‚è€ƒä¸»ä½“ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç»†ç²’åº¦è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚é€šè¿‡è¯­ä¹‰å¯¹åº”æ³¨æ„åŠ›æŸå¤±å’Œå¤šå‚è€ƒè§£è€¦æŸå¤±ï¼ŒMOSAICåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨4ä¸ªä»¥ä¸Šçš„å‚è€ƒä¸»ä½“ä¸‹ä¿æŒé«˜ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00428', 'title': 'Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation', 'url': 'https://huggingface.co/papers/2509.00428', 'abstract': 'Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE.', 'score': 7, 'issue_id': 5711, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': '5f17d4af79b3fc6f', 'authors': ['Xuechao Zou', 'Shun Zhang', 'Xing Fu', 'Yue Li', 'Kai Li', 'Yushe Cao', 'Congyan Lang', 'Pin Tao', 'Junliang Xing'], 'affiliations': ['Ant Group', 'Beijing Jiaotong University', 'Qinghai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.00428.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#architecture', '#security'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Face-MoGLE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ»Ğ¸Ñ†Ğ°. Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµÑ‚ÑŒ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Face-MoGLE: Mastering Controllable Face Generation with Diffusion Transformers', 'desc': "Face-MoGLE is a new framework that uses Diffusion Transformers to generate high-quality and controllable faces. It addresses the challenge of balancing semantic control with photorealism by employing semantic-decoupled latent modeling, which allows for precise manipulation of facial attributes. The framework incorporates a mixture of global and local experts to enhance both overall structure and detailed features, ensuring fine-grained control over the generated images. Additionally, a dynamic gating network adapts coefficients during the generation process, improving the model's flexibility and effectiveness in various face generation tasks."}, 'zh': {'title': 'Face-MoGLEï¼šé«˜è´¨é‡å¯æ§é¢éƒ¨ç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'Face-MoGLEæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å®ç°é«˜è´¨é‡ã€å¯æ§çš„é¢éƒ¨ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡è¯­ä¹‰è§£è€¦çš„æ½œåœ¨å»ºæ¨¡ã€ä¸“å®¶ä¸“é—¨åŒ–å’ŒåŠ¨æ€é—¨æ§æ¥è§£å†³ç”Ÿæˆå»ºæ¨¡ä¸­çš„æŒ‘æˆ˜ã€‚å®ƒå…è®¸ç²¾ç¡®çš„å±æ€§æ“æ§ï¼Œå¹¶ç»“åˆå…¨å±€å’Œå±€éƒ¨ä¸“å®¶ä»¥æ•æ‰æ•´ä½“ç»“æ„å’ŒåŒºåŸŸè¯­ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFace-MoGLEåœ¨å¤šæ¨¡æ€å’Œå•æ¨¡æ€é¢éƒ¨ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02722', 'title': 'Planning with Reasoning using Vision Language World Model', 'url': 'https://huggingface.co/papers/2509.02722', 'abstract': 'The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.', 'score': 6, 'issue_id': 5718, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '9d66073795d0c729', 'authors': ['Delong Chen', 'Theo Moutakanni', 'Willy Chung', 'Yejin Bang', 'Ziwei Ji', 'Allen Bolourchi', 'Pascale Fung'], 'affiliations': ['ISIR Sorbonne UniversitÃ©', 'Meta FAIR', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2509.02722.jpg', 'data': {'categories': ['#agents', '#rl', '#benchmark', '#cv', '#optimization', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VLWM: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'VLWM (Vision Language World Model) - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. VLWM Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ (VPA) ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… PlannerArena.'}, 'en': {'title': 'Revolutionizing Visual Planning with Language Understanding', 'desc': 'The Vision Language World Model (VLWM) is a cutting-edge model that enhances visual planning by combining language understanding with world modeling. It learns to predict actions and changes in the world by analyzing natural videos, allowing it to infer goals and plan trajectories effectively. The model employs a two-system approach, where system-1 focuses on quick, reactive planning and system-2 engages in deeper, reflective planning to minimize costs based on expected outcomes. VLWM demonstrates superior performance in visual planning tasks, outperforming existing models in various benchmarks and evaluations.'}, 'zh': {'title': 'è§†è§‰è¯­è¨€ä¸–ç•Œæ¨¡å‹ï¼šæ™ºèƒ½è§„åˆ’çš„æ–°çªç ´', 'desc': 'è§†è§‰è¯­è¨€ä¸–ç•Œæ¨¡å‹ï¼ˆVLWMï¼‰é€šè¿‡ç»“åˆåŸºäºè¯­è¨€çš„ä¸–ç•Œå»ºæ¨¡ã€è¡ŒåŠ¨ç­–ç•¥å­¦ä¹ å’ŒåŠ¨æ€å»ºæ¨¡ï¼Œè¾¾åˆ°äº†è§†è§‰è§„åˆ’çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ¨ç†å…·æœ‰è¯­ä¹‰å’Œæ—¶é—´æŠ½è±¡çš„é«˜å±‚æ¬¡ä¸–ç•Œæ¨¡å‹ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ã€‚VLWMé¦–å…ˆæ ¹æ®è§†è§‰è§‚å¯Ÿæ¨æ–­æ•´ä½“ç›®æ ‡ï¼Œç„¶åé¢„æµ‹ç”±äº¤é”™çš„è¡ŒåŠ¨å’Œä¸–ç•ŒçŠ¶æ€å˜åŒ–ç»„æˆçš„è½¨è¿¹ã€‚é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼è®­ç»ƒçš„è¯„è®ºæ¨¡å‹è¯„ä¼°å‡è®¾æœªæ¥çŠ¶æ€ä¸æœŸæœ›ç›®æ ‡çŠ¶æ€ä¹‹é—´çš„è¯­ä¹‰è·ç¦»ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„è§„åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02530', 'title': 'Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots', 'url': 'https://huggingface.co/papers/2509.02530', 'abstract': "Camera Depth Models (CDMs) enhance depth camera accuracy by denoising and improving metric depth prediction, enabling better generalization of robotic manipulation policies from simulation to real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern robotic manipulation primarily relies on visual observations in a 2D color space for skill learning but suffers from poor generalization. In contrast, humans, living in a 3D world, depend more on physical properties-such as distance, size, and shape-than on texture when interacting with objects. Since such 3D geometric information can be acquired from widely available depth cameras, it appears feasible to endow robots with similar perceptual capabilities. Our pilot study found that using depth cameras for manipulation is challenging, primarily due to their limited accuracy and susceptibility to various types of noise. In this work, we propose Camera Depth Models (CDMs) as a simple plugin on daily-use depth cameras, which take RGB images and raw depth signals as input and output denoised, accurate metric depth. To achieve this, we develop a neural data engine that generates high-quality paired data from simulation by modeling a depth camera's noise pattern. Our results show that CDMs achieve nearly simulation-level accuracy in depth prediction, effectively bridging the sim-to-real gap for manipulation tasks. Notably, our experiments demonstrate, for the first time, that a policy trained on raw simulated depth, without the need for adding noise or real-world fine-tuning, generalizes seamlessly to real-world robots on two challenging long-horizon tasks involving articulated, reflective, and slender objects, with little to no performance degradation. We hope our findings will inspire future research in utilizing simulation data and 3D information in general robot policies.", 'score': 1, 'issue_id': 5724, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'e632241ad7e3078d', 'authors': ['Minghuan Liu', 'Zhengbang Zhu', 'Xiaoshen Han', 'Peng Hu', 'Haotong Lin', 'Xinyao Li', 'Jingxiao Chen', 'Jiafeng Xu', 'Yichu Yang', 'Yunfeng Lin', 'Xinghang Li', 'Yong Yu', 'Weinan Zhang', 'Tao Kong', 'Bingyi Kang'], 'affiliations': ['ByteDance Seed', 'Shanghai Jiao Tong University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.02530.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ¾Ñ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Camera Depth Models (CDM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¼ĞµÑ€ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. CDM Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑˆÑƒĞ¼Ñ‹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ CDM, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'Bridging the Sim-to-Real Gap with Camera Depth Models', 'desc': 'Camera Depth Models (CDMs) improve the accuracy of depth cameras by reducing noise and enhancing metric depth predictions. This allows robots to better generalize their manipulation skills from simulated environments to real-world scenarios. By using a neural data engine, CDMs generate high-quality training data that mimics the noise patterns of depth cameras, achieving near-simulation accuracy in depth perception. The study demonstrates that policies trained on simulated depth data can effectively transfer to real-world tasks without additional fine-tuning, marking a significant advancement in robotic manipulation.'}, 'zh': {'title': 'æå‡æ·±åº¦ç›¸æœºå‡†ç¡®æ€§çš„ç›¸æœºæ·±åº¦æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ç›¸æœºæ·±åº¦æ¨¡å‹ï¼ˆCDMsï¼‰ï¼Œæ—¨åœ¨æé«˜æ·±åº¦ç›¸æœºçš„å‡†ç¡®æ€§ï¼Œé€šè¿‡å»å™ªå’Œæ”¹è¿›åº¦é‡æ·±åº¦é¢„æµ‹ï¼Œå¸®åŠ©æœºå™¨äººä»æ¨¡æ‹Ÿç¯å¢ƒæ›´å¥½åœ°è¿ç§»åˆ°ç°å®ä»»åŠ¡ä¸­ã€‚ç°ä»£æœºå™¨äººæ“ä½œä¸»è¦ä¾èµ–äºäºŒç»´é¢œè‰²ç©ºé—´çš„è§†è§‰è§‚å¯Ÿï¼Œä½†åœ¨æŠ€èƒ½å­¦ä¹ ä¸­é¢ä¸´æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚CDMsä½œä¸ºä¸€ç§ç®€å•çš„æ’ä»¶ï¼Œèƒ½å¤Ÿå°†RGBå›¾åƒå’ŒåŸå§‹æ·±åº¦ä¿¡å·ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå»å™ªåçš„å‡†ç¡®æ·±åº¦ä¿¡æ¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCDMsåœ¨æ·±åº¦é¢„æµ‹ä¸­è¾¾åˆ°äº†æ¥è¿‘æ¨¡æ‹Ÿçº§åˆ«çš„å‡†ç¡®æ€§ï¼ŒæˆåŠŸç¼©å°äº†æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00930', 'title': 'SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement\n  Fine-Tuning of LLMs', 'url': 'https://huggingface.co/papers/2509.00930', 'abstract': "SATQuest evaluates and enhances LLM logical reasoning by generating diverse SAT-based problems, offering insights into reasoning performance and enabling effective fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis and training, or have narrow problem types and formats. To address these limitations, we introduce SATQuest, a systematic verifier designed to evaluate and enhance logical reasoning in LLMs by generating diverse, Satisfiability-based logical reasoning problems directly from Conjunctive Normal Form (CNF) instances. SATQuest structures these problems along three orthogonal dimensions: instance scale, problem type, and question format, employing randomized, SAT-based problem generation and objective answer verification via PySAT. This design mitigates memorization issues, allows for nuanced insights into reasoning performance, and enables effective reinforcement fine-tuning. Our extensive evaluation of various LLMs using SATQuest identified significant limitations in their logical reasoning, particularly in generalizing beyond familiar mathematical formats. Furthermore, we show that reinforcement fine-tuning with SATQuest rewards substantially improves targeted task performance and generalizes to more complex instances, while highlighting remaining challenges in cross-format adaptation. Through these demonstrations, we showcase SATQuest's potential as a foundational tool and a valuable starting point for advancing LLM logical reasoning.", 'score': 1, 'issue_id': 5722, 'pub_date': '2025-08-31', 'pub_date_card': {'ru': '31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 31', 'zh': '8æœˆ31æ—¥'}, 'hash': '5bfb23a8accfba06', 'authors': ['Yanxiao Zhao', 'Yaqian Li', 'Zihao Bo', 'Rinyoichi Takezoe', 'Haojia Hui', 'Mo Guang', 'Lei Ren', 'Xiaolin Qin', 'Kaiwen Long'], 'affiliations': ['Chengdu Institute of Computer Applications, Chinese Academy of Sciences', 'Li Auto', 'School of Computer Science and Technology, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.00930.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#training', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SATQuest: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'SATQuest - ÑÑ‚Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ» (SAT). SATQuest ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±, Ñ‚Ğ¸Ğ¿ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing LLM Reasoning with SATQuest', 'desc': 'SATQuest is a tool designed to evaluate and improve the logical reasoning abilities of Large Language Models (LLMs) by generating a variety of SAT-based problems. It addresses the limitations of existing benchmarks by providing a systematic approach that allows for fine-grained analysis across different dimensions such as problem type and question format. By using randomized problem generation and objective verification methods, SATQuest helps to reduce memorization issues and offers deeper insights into the reasoning capabilities of LLMs. The results show that reinforcement fine-tuning with SATQuest significantly enhances performance on specific tasks and aids in generalizing to more complex reasoning scenarios.'}, 'zh': {'title': 'SATQuestï¼šæå‡LLMé€»è¾‘æ¨ç†çš„åˆ©å™¨', 'desc': 'SATQuest æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–çš„éªŒè¯å·¥å…·ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„åŸºäºå¯æ»¡è¶³æ€§ï¼ˆSATï¼‰çš„é€»è¾‘æ¨ç†é—®é¢˜ï¼Œæ¥æä¾›å¯¹æ¨ç†æ€§èƒ½çš„æ·±å…¥è§è§£ã€‚SATQuest è®¾è®¡äº†ä¸‰ç§æ­£äº¤ç»´åº¦çš„é—®é¢˜ç»“æ„ï¼šå®ä¾‹è§„æ¨¡ã€é—®é¢˜ç±»å‹å’Œé—®é¢˜æ ¼å¼ï¼Œåˆ©ç”¨éšæœºåŒ–çš„ SAT é—®é¢˜ç”Ÿæˆå’Œå®¢è§‚ç­”æ¡ˆéªŒè¯ã€‚é€šè¿‡å¯¹ LLM çš„å¹¿æ³›è¯„ä¼°ï¼ŒSATQuest æ˜¾ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨é€»è¾‘æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å¾®è°ƒæ˜¾è‘—æé«˜äº†ç‰¹å®šä»»åŠ¡çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21113', 'title': 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning', 'url': 'https://huggingface.co/papers/2508.21113', 'abstract': "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.", 'score': 82, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'e3b0726caba25eb1', 'authors': ['Jie Jiang', 'Qi Yang', 'Bolin Ni', 'Shiming Xiang', 'Han Hu', 'Houwen Peng'], 'affiliations': ['Institute of Automation, CAS', 'Tencent Hunyuan Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.21113.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#benchmark', '#dataset', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'R-4B: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'R-4B - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³ Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. R-4B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 25 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'R-4B: Smart Thinking for Efficient Problem Solving', 'desc': 'R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models.'}, 'zh': {'title': 'R-4Bï¼šæ™ºèƒ½æ€è€ƒä¸é«˜æ•ˆè§£å†³çš„ç»“åˆ', 'desc': 'R-4Bæ˜¯ä¸€ç§è‡ªåŠ¨æ€è€ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡Œæ€è€ƒã€‚å®ƒé‡‡ç”¨åŒæ¨¡é€€ç«å’ŒåŒæ¨¡ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨è§£å†³é—®é¢˜æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒR-4Bèƒ½å¤Ÿåœ¨ç®€å•é—®é¢˜ä¸Šé¿å…å†—ä½™çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-4Båœ¨25ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21112', 'title': 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control', 'url': 'https://huggingface.co/papers/2508.21112', 'abstract': 'EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.', 'score': 54, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '5bbbfa48bbd5fb7c', 'authors': ['Delin Qu', 'Haoming Song', 'Qizhi Chen', 'Zhaoqing Chen', 'Xianqiang Gao', 'Xinyi Ye', 'Qi Lv', 'Modi Shi', 'Guanghui Ren', 'Cheng Ruan', 'Maoqing Yao', 'Haoran Yang', 'Jiacheng Bao', 'Bin Zhao', 'Dong Wang'], 'affiliations': ['AgiBot', 'Fudan University', 'Northwestern Polytechnical University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.21112.jpg', 'data': {'categories': ['#architecture', '#training', '#agents', '#multimodal', '#agi', '#reasoning', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: EO-Robotics Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ', 'desc': 'EO-Robotics Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EO-1 Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° EO-Data1.5M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ EO-1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ EO-Data1.5M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering Robots with Multimodal Reasoning and Control', 'desc': "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."}, 'zh': {'title': 'æå‡æœºå™¨äººæ§åˆ¶çš„å¤šæ¨¡æ€æ¨ç†æ–°çªç ´', 'desc': 'EO-Roboticsæ˜¯ä¸€ä¸ªæ–°æ¨¡å‹ï¼ŒåŒ…å«EO-1æ¨¡å‹å’ŒEO-Data1.5Mæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡äº¤æ›¿çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œé¢„è®­ç»ƒæ¥æå‡å¤šæ¨¡æ€çš„å…·èº«æ¨ç†å’Œæœºå™¨äººæ§åˆ¶èƒ½åŠ›ã€‚EO-1æ¨¡å‹èƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€è§†é¢‘å’ŒåŠ¨ä½œç­‰å¤šç§è¾“å…¥ï¼Œå±•ç°å‡ºåœ¨å¤šæ¨¡æ€å…·èº«æ¨ç†å’Œæœºå™¨äººæ§åˆ¶æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒä¾èµ–äºä¸€ä¸ªåŒ…å«è¶…è¿‡150ä¸‡æ ·æœ¬çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¼ºè°ƒäº¤æ›¿çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œç†è§£ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†äº¤æ›¿å­¦ä¹ åœ¨å¼€æ”¾ä¸–ç•Œç†è§£å’Œæ³›åŒ–ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæä¾›äº†æ„å»ºå…ˆè¿›å…·èº«åŸºç¡€æ¨¡å‹çš„å®è´µè§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.18106', 'title': 'A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code', 'url': 'https://huggingface.co/papers/2508.18106', 'abstract': "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.", 'score': 46, 'issue_id': 5638, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 25', 'zh': '8æœˆ25æ—¥'}, 'hash': '7b691aaa7b52bcfd', 'authors': ['Keke Lian', 'Bin Wang', 'Lei Zhang', 'Libo Chen', 'Junjie Wang', 'Ziming Zhao', 'Yujiu Yang', 'Haotong Duan', 'Haoran Zhao', 'Shuang Liao', 'Mingda Guo', 'Jiazheng Quan', 'Yilu Zhong', 'Chenhao He', 'Zichuan Chen', 'Jie Wu', 'Haoling Li', 'Zhaoxuan Li', 'Jiongchi Yu', 'Hui Li', 'Dong Zhang'], 'affiliations': ['Fudan University', 'Institute of Information Engineering, Chinese Academy of Sciences', 'Peking University', 'Shanghai Jiao Tong University', 'Singapore Management University', 'Tencent', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18106.jpg', 'data': {'categories': ['#open_source', '#security', '#benchmark', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'A.S.E: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°', 'desc': 'A.S.E - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. A.S.E ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Claude-3.7-Sonnet Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½ĞµĞ²ĞµĞ»Ğ¸Ğº.'}, 'en': {'title': 'A.S.E: Elevating Security Evaluation for AI-Generated Code', 'desc': 'The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches.'}, 'zh': {'title': 'A.S.Eï¼šæå‡ä»£ç ç”Ÿæˆå®‰å…¨æ€§çš„åŸºå‡†è¯„ä¼°', 'desc': 'A.S.Eæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†ï¼Œåˆ©ç”¨çœŸå®ä¸–ç•Œçš„ä»£ç åº“å’Œä¸“å®¶å®šä¹‰çš„è§„åˆ™ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¿æ¥è¾“å…¥ä¸Šä¸‹æ–‡çš„è´¨é‡ä¸è¾“å‡ºçš„å®‰å…¨æ€§ã€‚A.S.Eé€šè¿‡æ„å»ºçœŸå®ä»£ç åº“ä¸­çš„ä»»åŠ¡ï¼Œä¿ç•™å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›å¯é‡å¤çš„å®‰å…¨è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-Sonnetåœ¨æ•´ä½“è¡¨ç°ä¸Šæœ€ä½³ï¼Œè€ŒQwen3-235B-A22B-Instructåœ¨å®‰å…¨æ€§è¯„åˆ†ä¸Šè¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20470', 'title': 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation', 'url': 'https://huggingface.co/papers/2508.20470', 'abstract': 'Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.', 'score': 31, 'issue_id': 5642, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '2bbd88d7f14f5a78', 'authors': ['Xiaochuan Li', 'Guoguang Du', 'Runze Zhang', 'Liang Jin', 'Qi Jia', 'Lihua Lu', 'Zhenhua Guo', 'Yaqian Zhao', 'Haiyang Liu', 'Tianqi Wang', 'Changsheng Li', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20470.jpg', 'data': {'categories': ['#dataset', '#3d', '#multimodal', '#open_source', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Droplet3D-4M Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Droplet3D, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Enhancing 3D Asset Generation with Video Commonsense Priors', 'desc': 'This paper discusses how using video data can improve the generation of 3D assets by providing commonsense knowledge that helps maintain spatial consistency and semantic accuracy. It highlights the challenge of limited 3D data compared to other media types, like text and images, and proposes leveraging videos as a solution. The authors introduce Droplet3D-4M, a large dataset with multi-view annotations, and a generative model called Droplet3D that can process both images and text. Their experiments show that this approach not only enhances the quality of 3D content but also allows for broader applications in scene generation.'}, 'zh': {'title': 'åˆ©ç”¨è§†é¢‘æ•°æ®æå‡3Dç”Ÿæˆçš„ç©ºé—´ä¸è¯­ä¹‰ä¸€è‡´æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è§†é¢‘æ•°æ®æ¥å¢å¼º3Dèµ„äº§ç”Ÿæˆï¼Œæä¾›ç©ºé—´ä¸€è‡´æ€§å’Œè¯­ä¹‰åˆç†æ€§ã€‚ç”±äº3Dé¢†åŸŸçš„æ•°æ®ç¨€ç¼ºï¼Œè§†é¢‘ä¸­çš„å¸¸è¯†å…ˆéªŒæˆä¸ºäº†ä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£ç›‘ç£ä¿¡å·ã€‚è§†é¢‘æ•æ‰çš„å¤šè§†è§’ä¿¡æ¯ä¸º3Dç”Ÿæˆæä¾›äº†ç©ºé—´ä¸€è‡´æ€§ï¼Œè€Œä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯åˆ™ä½¿ç”Ÿæˆçš„å†…å®¹æ›´ç¬¦åˆæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬ä»‹ç»äº†Droplet3D-4Mæ•°æ®é›†å’ŒDroplet3Dç”Ÿæˆæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Då†…å®¹ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21148', 'title': 'A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers', 'url': 'https://huggingface.co/papers/2508.21148', 'abstract': 'Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.', 'score': 14, 'issue_id': 5645, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '3097c905f2f36541', 'authors': ['Ming Hu', 'Chenglong Ma', 'Wei Li', 'Wanghan Xu', 'Jiamin Wu', 'Jucheng Hu', 'Tianbin Li', 'Guohang Zhuang', 'Jiaqi Liu', 'Yingzhou Lu', 'Ying Chen', 'Chaoyang Zhang', 'Cheng Tan', 'Jie Ying', 'Guocheng Wu', 'Shujian Gao', 'Pengcheng Chen', 'Jiashi Lin', 'Haitao Wu', 'Lulu Chen', 'Fengxiang Wang', 'Yuanyuan Zhang', 'Xiangyu Zhao', 'Feilong Tang', 'Encheng Su', 'Junzhi Ning', 'Xinyao Liu', 'Ye Du', 'Changkai Ji', 'Cheng Tang', 'Huihui Xu', 'Ziyang Chen', 'Ziyan Huang', 'Jiyao Liu', 'Pengfei Jiang', 'Yizhou Wang', 'Chen Tang', 'Jianyu Wu', 'Yuchen Ren', 'Siyuan Yan', 'Zhonghua Wang', 'Zhongxing Xu', 'Shiyan Su', 'Shangquan Sun', 'Runkai Zhao', 'Zhisheng Zhang', 'Yu Liu', 'Fudi Wang', 'Yuanfeng Ji', 'Yanzhou Su', 'Hongming Shan', 'Chunmei Feng', 'Jiahao Xu', 'Jiangtao Yan', 'Wenhao Tang', 'Diping Song', 'Lihao Liu', 'Yanyan Huang', 'Lequan Yu', 'Bin Fu', 'Shujun Wang', 'Xiaomeng Li', 'Xiaowei Hu', 'Yun Gu', 'Ben Fei', 'Zhongying Deng', 'Benyou Wang', 'Yuewen Cao', 'Minjie Shen', 'Haodong Duan', 'Jie Xu', 'Yirong Chen', 'Fang Yan', 'Hongxia Hao', 'Jielan Li', 'Jiajun Du', 'Yanbo Wang', 'Imran Razzak', 'Chi Zhang', 'Lijun Wu', 'Conghui He', 'Zhaohui Lu', 'Jinhai Huang', 'Yihao Liu', 'Fenghua Ling', 'Yuqiang Li', 'Aoran Wang', 'Qihao Zheng', 'Nanqing Dong', 'Tianfan Fu', 'Dongzhan Zhou', 'Yan Lu', 'Wenlong Zhang', 'Jin Ye', 'Jianfei Cai', 'Wanli Ouyang', 'Yu Qiao', 'Zongyuan Ge', 'Shixiang Tang', 'Junjun He', 'Chunfeng Song', 'Lei Bai', 'Bowen Zhou'], 'affiliations': ['Beijing Institute of Heart, Lung and Blood Vessel Diseases', 'China Pharmaceutical University', 'Chinese Academy of Sciences', 'Fudan University', 'Fuzhou University', 'Monash University', 'Purdue University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'South China University', 'Stanford University', 'The Chinese University of Hong Kong', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'UNC-Chapel Hill', 'University College Dublin', 'University College London', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.21148.jpg', 'data': {'categories': ['#benchmark', '#agents', '#survey', '#multimodal', '#data', '#dataset', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Sci-LLMs: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Sci-LLMs) Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ñ‚ĞµÑĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ñ€ĞµÑˆĞ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Sci-LLMs, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 270 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Sci-LLMs, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑÑÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming Science with Autonomous Language Models', 'desc': 'This paper discusses the evolution of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data. It highlights the unique challenges posed by scientific datasets, which are often multimodal and domain-specific, requiring specialized approaches compared to general natural language processing. The authors propose a taxonomy of scientific data and review various Sci-LLMs, emphasizing the need for models that can handle heterogeneous and uncertain information. Ultimately, the paper advocates for the development of autonomous systems that can actively engage in scientific research, contributing to a dynamic knowledge base.'}, 'zh': {'title': 'ç§‘å­¦ç ”ç©¶ä¸­çš„æ™ºèƒ½åˆä½œä¼™ä¼´', 'desc': 'ç§‘å­¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSci-LLMsï¼‰æ­£åœ¨é€šè¿‡ä¸ç§‘å­¦æ•°æ®çš„å…±åŒå‘å±•è€Œä¸æ–­æ¼”å˜ï¼Œè§£å†³å¤šæ¨¡æ€å’Œç‰¹å®šé¢†åŸŸä¿¡æ¯ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç»¼åˆæ¡†æ¶ï¼Œå°†Sci-LLMsçš„å‘å±•è§†ä¸ºæ¨¡å‹ä¸å…¶åŸºç¡€æ•°æ®ä¹‹é—´çš„å…±åŒè¿›åŒ–ã€‚æˆ‘ä»¬å»ºç«‹äº†ç§‘å­¦æ•°æ®çš„ç»Ÿä¸€åˆ†ç±»æ³•å’Œç§‘å­¦çŸ¥è¯†çš„å±‚æ¬¡æ¨¡å‹ï¼Œå¼ºè°ƒäº†ç§‘å­¦è¯­æ–™åº“ä¸ä¸€èˆ¬è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ä¹‹é—´çš„åŒºåˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬å±•æœ›äº†å‘é—­ç¯ç³»ç»Ÿçš„è½¬å˜ï¼Œå¼ºè°ƒåŸºäºSci-LLMsçš„è‡ªä¸»ä»£ç†å¦‚ä½•ç§¯æå®éªŒã€éªŒè¯å¹¶ä¸ºä¸æ–­å‘å±•çš„çŸ¥è¯†åº“åšå‡ºè´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.13618', 'title': 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis', 'url': 'https://huggingface.co/papers/2508.13618', 'abstract': 'TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid', 'score': 14, 'issue_id': 5639, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': '8baf01eb014bc50c', 'authors': ['Shunian Chen', 'Hejin Huang', 'Yexin Liu', 'Zihan Ye', 'Pengcheng Chen', 'Chenghao Zhu', 'Michael Guan', 'Rongsheng Wang', 'Junying Chen', 'Guanbin Li', 'Ser-Nam Lim', 'Harry Yang', 'Benyou Wang'], 'affiliations': ['Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.13618.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#transfer_learning', '#dataset', '#cv', '#data'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'TalkVid: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TalkVid Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1244 Ñ‡Ğ°ÑĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ 7729 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° TalkVid, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TalkVid-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ….'}, 'en': {'title': 'Bridging the Diversity Gap in AI with TalkVid', 'desc': 'The paper introduces TalkVid, a new dataset designed to improve audio-driven talking head synthesis by addressing the generalization gap in existing models. This gap arises from the lack of diversity in training data, which often fails to represent various ethnicities, languages, and age groups. TalkVid consists of 1244 hours of video from 7729 unique speakers, curated through a rigorous process to ensure high quality and diversity. The study also presents TalkVid-Bench, an evaluation set that highlights performance disparities among different demographic groups, emphasizing the importance of diverse datasets in machine learning research.'}, 'zh': {'title': 'TalkVidï¼šæå‡è™šæ‹Ÿäººå¤´åˆæˆçš„å¤šæ ·æ€§ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'TalkVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºéŸ³é¢‘çš„è™šæ‹Ÿäººå¤´åˆæˆæŠ€æœ¯ã€‚ç°æœ‰çš„æ¨¡å‹åœ¨å¤„ç†ä¸åŒç§æ—ã€è¯­è¨€å’Œå¹´é¾„ç¾¤ä½“æ—¶å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè®­ç»ƒæ•°æ®çš„è§„æ¨¡å’Œå¤šæ ·æ€§ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTalkVidåŒ…å«äº†1244å°æ—¶æ¥è‡ª7729ä¸ªç‹¬ç‰¹è¯´è¯è€…çš„è§†é¢‘ï¼Œç»è¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè‡ªåŠ¨åŒ–ç­›é€‰ï¼Œç¡®ä¿äº†æ•°æ®çš„ç¨³å®šæ€§å’Œç¾å­¦è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºTalkVidè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºä»¥å¾€çš„æ•°æ®é›†ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†ä¸åŒå­ç¾¤ä½“ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21365', 'title': 'Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2508.21365', 'abstract': 'Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.', 'score': 9, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': 'cbcbd196468063e9', 'authors': ['Yi Liao', 'Yu Gu', 'Yuan Sui', 'Zining Zhu', 'Yifan Lu', 'Guohua Tang', 'Zhongqian Sun', 'Wei Yang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.21365.jpg', 'data': {'categories': ['#training', '#games', '#optimization', '#reasoning', '#interpretability', '#multimodal', '#rl', '#rlhf'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹: Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº ÑƒĞ¼ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Think in Games (TiG), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. TiG Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, TiG Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': 'Empowering Language Models to Learn by Playing', 'desc': 'The Think in Games (TiG) framework allows large language models (LLMs) to learn how to perform tasks through interaction with game environments, enhancing their procedural knowledge. This approach addresses the gap between knowing facts (declarative knowledge) and knowing how to apply them (procedural knowledge), which LLMs often struggle with in interactive scenarios. By treating decision-making in reinforcement learning as a language modeling task, TiG enables LLMs to create and refine policies based on feedback from their environment. The results show that TiG not only requires less data and computation than traditional methods but also offers clear explanations for its actions, making it more transparent and interpretable.'}, 'zh': {'title': 'é€šè¿‡æ¸¸æˆæ€ç»´æå‡AIçš„å­¦ä¹ èƒ½åŠ›', 'desc': 'Think in Games (TiG) æ¡†æ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡äº’åŠ¨æ¸¸æˆç¯å¢ƒå‘å±•ç¨‹åºæ€§çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTiG åœ¨æ•°æ®å’Œè®¡ç®—éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¯­è¨€æŒ‡å¯¼çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTiG æˆåŠŸå¼¥è¡¥äº†å£°æ˜æ€§çŸ¥è¯†ä¸ç¨‹åºæ€§çŸ¥è¯†ä¹‹é—´çš„å·®è·ï¼Œæå‡äº†å¤æ‚äº’åŠ¨ä»»åŠ¡çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17677', 'title': 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training', 'url': 'https://huggingface.co/papers/2508.17677', 'abstract': "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios.", 'score': 7, 'issue_id': 5639, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 25', 'zh': '8æœˆ25æ—¥'}, 'hash': '8c118ab21cea1eb2', 'authors': ['Yifan Wang', 'Binbin Liu', 'Fengze Liu', 'Yuanfan Guo', 'Jiyao Deng', 'Xuecheng Wu', 'Weidong Zhou', 'Xiaohuan Zhou', 'Taifeng Wang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.17677.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TiKMiX - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Group Influence Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. TiKMiX Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TiKMiX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Dynamic Data Mixing for Enhanced Language Model Performance', 'desc': "This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture used in training language models. It leverages a metric called Group Influence to assess how different data domains impact the model's learning preferences, which change over time. By optimizing the data mixture based on these evolving preferences, TiKMiX significantly enhances model performance while being computationally efficient. The results show that TiKMiX outperforms existing methods and achieves better results with fewer resources, demonstrating the importance of adaptive data strategies in machine learning."}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´æ•°æ®æ··åˆï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTiKMiXçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ¨¡å‹çš„å­¦ä¹ åå¥½åŠ¨æ€è°ƒæ•´æ•°æ®æ··åˆï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„é™æ€æ•°æ®æ··åˆç­–ç•¥æ— æ³•é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–çš„åå¥½ã€‚TiKMiXå¼•å…¥äº†Group Influenceè¿™ä¸€é«˜æ•ˆæŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ•°æ®é¢†åŸŸå¯¹æ¨¡å‹çš„å½±å“ï¼Œä»è€Œä¼˜åŒ–æ•°æ®æ··åˆçš„åˆ†å¸ƒã€‚é€šè¿‡TiKMiX-Då’ŒTiKMiX-Mä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†åœ¨è®¡ç®—èµ„æºä½¿ç”¨ä¸Šæ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21767', 'title': 'UItron: Foundational GUI Agent with Advanced Perception and Planning', 'url': 'https://huggingface.co/papers/2508.21767', 'abstract': 'UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.', 'score': 6, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '4e413654a21d562e', 'authors': ['Zhixiong Zeng', 'Jing Huang', 'Liming Zheng', 'Wenkang Han', 'Yufeng Zhong', 'Lei Chen', 'Longrong Yang', 'Yingjie Chu', 'Yuzhi He', 'Lin Ma'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2508.21767.jpg', 'data': {'categories': ['#benchmark', '#rl', '#open_source', '#training', '#optimization', '#reasoning', '#agi', '#dataset', '#data', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'UItron: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²', 'desc': 'UItron - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. UItron Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'UItron: Advancing GUI Agents for Real-World Applications', 'desc': 'UItron is an open-source foundational model designed to enhance the capabilities of GUI agents, focusing on visual understanding and task planning. It addresses challenges in developing GUI agents, such as limited operation trajectories and the need for interactive infrastructure. By employing advanced perception, grounding, and planning techniques, UItron systematically improves training through data engineering and a curriculum reinforcement learning framework. The model demonstrates exceptional performance in Chinese app scenarios, filling a gap in existing solutions and moving closer to practical applications of GUI agents.'}, 'zh': {'title': 'UItronï¼šæ¨åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ä»£ç†çš„æœªæ¥', 'desc': 'UItronæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†è®¾è®¡ï¼Œæå‡äº†è§†è§‰ç†è§£å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å…ˆè¿›çš„æ„ŸçŸ¥ã€å®šä½å’Œè§„åˆ’åŠŸèƒ½ï¼Œåœ¨ä¸­æ–‡åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚UItronå¼ºè°ƒäº†ç³»ç»Ÿæ•°æ®å·¥ç¨‹å’Œäº¤äº’åŸºç¡€è®¾æ–½åœ¨GUIä»£ç†å¼€å‘ä¸­çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å¢å¼ºè®­ç»ƒæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUItronåœ¨ä¸­æ–‡åº”ç”¨åœºæ™¯ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿GUIä»£ç†æ›´æ¥è¿‘å®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21290', 'title': 'Efficient Code Embeddings from Code Generation Models', 'url': 'https://huggingface.co/papers/2508.21290', 'abstract': 'Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.', 'score': 5, 'issue_id': 5640, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '484a770fa8f460fc', 'authors': ['Daria Kryvosheieva', 'Saba Sturua', 'Michael GÃ¼nther', 'Scott Martens', 'Han Xiao'], 'affiliations': ['Jina AI GmbH', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.21290.jpg', 'data': {'categories': ['#multilingual', '#transfer_learning', '#data', '#dataset', '#games', '#plp', '#small_models', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼', 'desc': 'Jina-code-embeddings - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ ĞºĞ¾Ğ´Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ´Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Code Retrieval with Smart Embeddings', 'desc': 'Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks.'}, 'zh': {'title': 'åˆ›æ–°ä»£ç åµŒå…¥æ¨¡å‹ï¼Œæå‡ä»£ç æ£€ç´¢ä¸é—®ç­”èƒ½åŠ›', 'desc': 'Jina-code-embeddings æ˜¯ä¸€ç§æ–°å‹çš„ä»£ç åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢ä»£ç ã€è¿›è¡ŒæŠ€æœ¯é—®ç­”ä»¥åŠè¯†åˆ«ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç ç‰‡æ®µã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ä½¿ç”¨äº†ä¸€ä¸ªåœ¨æ–‡æœ¬å’Œä»£ç ä¸Šé¢„è®­ç»ƒçš„è‡ªå›å½’éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡æœ€åä¸€ä¸ªæ ‡è®°çš„æ± åŒ–ç”ŸæˆåµŒå…¥ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å°½ç®¡æ¨¡å‹ç›¸å¯¹è¾ƒå°ï¼Œä½†ä»èƒ½å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™éªŒè¯äº†è¿™ç§ä»£ç åµŒå…¥æ¨¡å‹æ„å»ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21376', 'title': 'AHELM: A Holistic Evaluation of Audio-Language Models', 'url': 'https://huggingface.co/papers/2508.21376', 'abstract': 'AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.', 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': 'fccdd3f91aa4aebd', 'authors': ['Tony Lee', 'Haoqin Tu', 'Chi Heem Wong', 'Zijun Wang', 'Siwei Yang', 'Yifan Mai', 'Yuyin Zhou', 'Cihang Xie', 'Percy Liang'], 'affiliations': ['Hitachi America, Ltd.', 'Stanford University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2508.21376.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#reasoning', '#multimodal', '#audio', '#dataset'], 'emoji': 'ğŸ§', 'ru': {'title': 'AHELM: Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'AHELM - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ALM). ĞĞ½ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Gemini 2.5 Pro Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² 5 Ğ¸Ğ· 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ½ĞµÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ASR.'}, 'en': {'title': 'AHELM: A Holistic Benchmark for Evaluating Audio-Language Models', 'desc': 'AHELM is a new benchmark designed to evaluate audio-language models (ALMs) on multiple important aspects such as fairness, safety, and reasoning. It combines various datasets, including two new ones, PARADE and CoRe-Bench, to provide a comprehensive assessment of ALMs across ten critical dimensions. By standardizing evaluation methods and metrics, AHELM allows for fair comparisons between different models, addressing the limitations of previous benchmarks. The initial testing of 14 ALMs reveals insights into their performance, highlighting both strengths and weaknesses in areas like bias and group fairness.'}, 'zh': {'title': 'AHELMï¼šéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°åŸºå‡†', 'desc': 'AHELMæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰ï¼Œæ¶µç›–å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œæ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚è¯¥åŸºå‡†æ•´åˆäº†å¤šç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°çš„åˆæˆéŸ³é¢‘-æ–‡æœ¬æ•°æ®é›†PARADEå’ŒCoRe-Benchï¼Œä»¥å…¨é¢æµ‹é‡ALMsåœ¨éŸ³é¢‘æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†ç­‰10ä¸ªé‡è¦æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡æ ‡å‡†åŒ–æç¤ºã€æ¨ç†å‚æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼ŒAHELMç¡®ä¿äº†æ¨¡å‹ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡Gemini 2.5 Proåœ¨10ä¸ªæ–¹é¢ä¸­æœ‰5ä¸ªæ’åç¬¬ä¸€ï¼Œä½†åœ¨ASRä»»åŠ¡ä¸Šè¡¨ç°å‡ºç¾¤ä½“ä¸å…¬å¹³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21456', 'title': 'Morae: Proactively Pausing UI Agents for User Choices', 'url': 'https://huggingface.co/papers/2508.21456', 'abstract': 'Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.', 'score': 3, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'}, 'hash': '3d7bd580c525eaa6', 'authors': ['Yi-Hao Peng', 'Dingzeyu Li', 'Jeffrey P. Bigham', 'Amy Pavel'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.21456.jpg', 'data': {'categories': ['#ethics', '#agi', '#multimodal', '#healthcare', '#agents'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Morae: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Morae - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Morae Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Morae Ğ²Ğ¾Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Morae Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Empowering BLV Users with Interactive Decision-Making', 'desc': 'Morae is a user interface (UI) agent designed to improve accessibility for blind and low-vision (BLV) users by actively involving them in decision-making during task execution. Unlike traditional UI agents that operate autonomously, Morae identifies key decision points and pauses to allow users to make informed choices, enhancing their agency. It leverages large multimodal models to interpret user queries and UI elements, ensuring that users are aware of their options. In studies, Morae demonstrated improved task completion and user satisfaction compared to standard agents, showcasing a mixed-initiative approach that balances automation with user preference expression.'}, 'zh': {'title': 'Moraeï¼šè®©ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·å‚ä¸å†³ç­–çš„æ™ºèƒ½ä»£ç†', 'desc': 'Moraeæ˜¯ä¸€ç§ç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡è®©ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·å‚ä¸å†³ç­–è¿‡ç¨‹æ¥æé«˜å¯è®¿é—®æ€§ã€‚å®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¥ç†è§£ç”¨æˆ·æŸ¥è¯¢å’Œç”¨æˆ·ç•Œé¢å…ƒç´ ï¼Œå¹¶åœ¨ä»»åŠ¡æ‰§è¡Œä¸­è‡ªåŠ¨è¯†åˆ«å†³ç­–ç‚¹ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åšå‡ºé€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„å…¨è‡ªåŠ¨ä»£ç†ä¸åŒï¼ŒMoraeåœ¨å…³é”®æ—¶åˆ»æš‚åœï¼Œæç¤ºç”¨æˆ·è¿›è¡Œæ¾„æ¸…ï¼Œä»è€Œå¢å¼ºç”¨æˆ·çš„è‡ªä¸»æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMoraeå¸®åŠ©ç”¨æˆ·å®Œæˆæ›´å¤šä»»åŠ¡ï¼Œå¹¶é€‰æ‹©æ›´ç¬¦åˆä»–ä»¬åå¥½çš„é€‰é¡¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21188', 'title': 'Model-Task Alignment Drives Distinct RL Outcomes', 'url': 'https://huggingface.co/papers/2508.21188', 'abstract': 'Reinforcement learning applied to large language models shows counterintuitive results that depend on pre-existing model-task alignment, with standard RL methods remaining robust in challenging scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, a series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not typically observed in traditional RL settings. For example, notable claims include that a single training example can match the performance achieved with an entire dataset, that the reward signal does not need to be very accurate, and that training solely with negative samples can match or even surpass sophisticated reward-based methods. However, the precise conditions under which these observations hold - and, critically, when they fail - remain unclear. In this work, we identify a key factor that differentiates RL observations: whether the pretrained model already exhibits strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated task. Through a systematic and comprehensive examination of a series of counterintuitive claims, supported by rigorous experimental validation across different model architectures and task domains, our findings show that while standard RL training remains consistently robust across settings, many of these counterintuitive results arise only when the model and task already exhibit strong model-task alignment. In contrast, these techniques fail to drive substantial learning in more challenging regimes, where standard RL methods remain effective.', 'score': 1, 'issue_id': 5648, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'f7ee68376b3660e0', 'authors': ['Haoze Wu', 'Cheng Wang', 'Wenshuo Zhao', 'Junxian He'], 'affiliations': ['HKUST', 'National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.21188.jpg', 'data': {'categories': ['#training', '#rlhf', '#reasoning', '#alignment', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ˜Ğ˜ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Understanding RL Success in Language Models: The Role of Model-Task Alignment', 'desc': 'This paper explores the application of reinforcement learning (RL) to large language models (LLMs) and reveals unexpected results that depend on the alignment between the model and the task. It highlights that certain counterintuitive phenomena, such as achieving high performance with minimal training data or inaccurate reward signals, occur primarily when there is strong model-task alignment. The authors conduct rigorous experiments to validate these findings across various model architectures and tasks, demonstrating that standard RL methods are robust in challenging scenarios. However, they also note that the effectiveness of these counterintuitive results diminishes when the model-task alignment is weak.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ä¸æ¨¡å‹ä»»åŠ¡å¯¹é½çš„å¥¥ç§˜', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œå‘ç°äº†ä¸€äº›åç›´è§‰çš„ç°è±¡ã€‚è¿™äº›ç°è±¡çš„å‡ºç°ä¸é¢„è®­ç»ƒæ¨¡å‹ä¸ä»»åŠ¡ä¹‹é—´çš„å¯¹é½ç¨‹åº¦å¯†åˆ‡ç›¸å…³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹ä¸ä»»åŠ¡å…·æœ‰å¼ºå¯¹é½æ—¶ï¼ŒæŸäº›è®­ç»ƒæ–¹æ³•å¯ä»¥å–å¾—æ„æƒ³ä¸åˆ°çš„æ•ˆæœï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­ï¼Œä¼ ç»Ÿçš„RLæ–¹æ³•ä»ç„¶æœ‰æ•ˆã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›åç›´è§‰ç»“æœçš„æ¡ä»¶å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20085', 'title': 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2508.20085', 'abstract': 'HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.', 'score': 1, 'issue_id': 5640, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 27', 'zh': '8æœˆ27æ—¥'}, 'hash': 'ad1271c7a1097c84', 'authors': ['Zhecheng Yuan', 'Tianming Wei', 'Langzhe Gu', 'Pu Hua', 'Tianhai Liang', 'Yuanpei Chen', 'Huazhe Xu'], 'affiliations': ['Peking University', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20085.jpg', 'data': {'categories': ['#rl', '#agents', '#optimization', '#transfer_learning', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº ÑƒĞ¼ĞµĞ»Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°', 'desc': 'HERMES - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². HERMES ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Bridging Human Motion and Robotic Dexterity with HERMES', 'desc': 'HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings.'}, 'zh': {'title': 'HERMESï¼šäººæœºåä½œçš„çµå·§æ“ä½œæ–°æ¡†æ¶', 'desc': 'HERMESæ˜¯ä¸€ä¸ªäººæœºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†äººç±»æ‰‹éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººè¡Œä¸ºã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œä»¿çœŸåˆ°ç°å®çš„è½¬ç§»æŠ€æœ¯ï¼Œå¸®åŠ©æœºå™¨äººåœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è¿›è¡Œçµæ´»çš„æ“ä½œã€‚HERMESèƒ½å¤Ÿå°†æ¥è‡ªå¤šä¸ªæ¥æºçš„äººç±»æ‰‹éƒ¨åŠ¨ä½œç»Ÿä¸€è½¬åŒ–ä¸ºå¯è¡Œçš„æœºå™¨äººè¡Œä¸ºï¼Œå¹¶é€šè¿‡æ·±åº¦å›¾åƒå®ç°æ›´å¥½çš„ç°å®é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHERMESåœ¨å„ç§å¤æ‚çš„ç§»åŠ¨åŒæ‰‹çµå·§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17380', 'title': "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula\n  Discovery", 'url': 'https://huggingface.co/papers/2508.17380', 'abstract': 'VIPER-R1, a multimodal model combining visual perception, trajectory data, and symbolic reasoning, discovers physical laws with higher accuracy and interpretability than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated discovery of physical laws from observational data in the real world is a grand challenge in AI. Current methods, relying on symbolic regression or LLMs, are limited to uni-modal data and overlook the rich, visual phenomenological representations of motion that are indispensable to physicists. This "sensory deprivation" severely weakens their ability to interpret the inherent spatio-temporal patterns within dynamic phenomena. To address this gap, we propose VIPER-R1, a multimodal model that performs Visual Induction for Physics-based Equation Reasoning to discover fundamental symbolic formulas. It integrates visual perception, trajectory data, and symbolic reasoning to emulate the scientific discovery process. The model is trained via a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning to interpret kinematic phase portraits and to construct hypotheses guided by a Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration (RGSC) to refine the formula structure with reinforcement learning. During inference, the trained VIPER-R1 acts as an agent: it first posits a high-confidence symbolic ansatz, then proactively invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2). This final step, analogous to a physicist\'s perturbation analysis, reconciles the theoretical model with empirical data. To support this research, we introduce PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy and interpretability, enabling more precise discovery of physical laws. Project page: https://jiaaqiliu.github.io/VIPER-R1/', 'score': 1, 'issue_id': 5648, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 24', 'zh': '8æœˆ24æ—¥'}, 'hash': '87afb7a989f84636', 'authors': ['Jiaqi Liu', 'Songning Lai', 'Pengze Li', 'Di Yu', 'Wenjie Zhou', 'Yiyang Zhou', 'Peng Xia', 'Zijun Wang', 'Xi Chen', 'Shixiang Tang', 'Lei Bai', 'Wanli Ouyang', 'Mingyu Ding', 'Huaxiu Yao', 'Aoran Wang'], 'affiliations': ['Fudan University', 'HKUST (Guangzhou)', 'Nankai University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong', 'Tsinghua University', 'UC Santa Cruz', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2508.17380.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#multimodal', '#interpretability', '#dataset', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'VIPER-R1: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸', 'desc': 'VIPER-R1 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑƒÑ€ÑĞ° Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. VIPER-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ PhysSymbol Ñ 5000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Physical Laws with Multimodal Insights', 'desc': 'VIPER-R1 is a multimodal model designed to discover physical laws by integrating visual perception, trajectory data, and symbolic reasoning. Unlike traditional methods that focus on single data types, VIPER-R1 utilizes a combination of visual and motion data to better understand complex physical phenomena. The model employs a structured training approach, including Motion Structure Induction and reinforcement learning, to refine its symbolic formulas. Experimental results demonstrate that VIPER-R1 achieves higher accuracy and interpretability compared to existing models, making it a significant advancement in automated scientific discovery.'}, 'zh': {'title': 'VIPER-R1ï¼šå¤šæ¨¡æ€æ¨¡å‹åŠ©åŠ›ç‰©ç†å®šå¾‹å‘ç°', 'desc': 'VIPER-R1æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰æ„ŸçŸ¥ã€è½¨è¿¹æ•°æ®å’Œç¬¦å·æ¨ç†ï¼Œèƒ½å¤Ÿä»¥æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§å‘ç°ç‰©ç†å®šå¾‹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç¬¦å·å›å½’æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šå¸¸åªå¤„ç†å•ä¸€æ¨¡æ€æ•°æ®ï¼Œå¿½è§†äº†è¿åŠ¨çš„ä¸°å¯Œè§†è§‰è¡¨å¾ã€‚VIPER-R1é€šè¿‡è¿åŠ¨ç»“æ„å½’çº³ï¼ˆMSIï¼‰å’Œå¥–åŠ±å¼•å¯¼çš„ç¬¦å·æ ¡å‡†ï¼ˆRGSCï¼‰æ¥è®­ç»ƒï¼Œæ¨¡æ‹Ÿç§‘å­¦å‘ç°è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒVIPER-R1åœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å‘ç°ç‰©ç†å®šå¾‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.14197', 'title': 'CLIPSym: Delving into Symmetry Detection with CLIP', 'url': 'https://huggingface.co/papers/2508.14197', 'abstract': "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.", 'score': 1, 'issue_id': 5642, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': 'fe61d1db34c28a8d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#architecture', '#optimization', '#multimodal', '#cv', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'CLIPSym: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'CLIPSym - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLIP. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° CLIP Ñ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ G-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. CLIPSym Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Symmetry Detection with CLIPSym', 'desc': 'CLIPSym is a vision-language model that improves the detection of symmetry in images by using a rotation-equivariant decoder and a new prompting technique. It builds on the capabilities of the CLIP model, which combines image and text understanding, to enhance symmetry detection by incorporating semantic information from image descriptions. The model employs a hybrid architecture that integrates Transformer and G-Convolution to effectively recognize both rotation and reflection symmetries. Experimental results demonstrate that CLIPSym surpasses existing methods on multiple standard datasets, confirming the effectiveness of its innovative approaches.'}, 'zh': {'title': 'CLIPSymï¼šæå‡å¯¹ç§°æ€§æ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'CLIPSymæ˜¯ä¸€ç§åŸºäºCLIPçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¯¹ç§°æ€§æ£€æµ‹çš„èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ—‹è½¬ç­‰å˜è§£ç å™¨å’Œè¯­ä¹‰æ„ŸçŸ¥æç¤ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è‡ªç„¶å›¾åƒæè¿°ä¸­çš„å¯¹ç§°æ€§çº¿ç´¢ã€‚é€šè¿‡ç»“åˆTransformerå’ŒG-å·ç§¯çš„æ··åˆç»“æ„ï¼ŒCLIPSymèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹æ—‹è½¬å’Œåå°„å¯¹ç§°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIPSymåœ¨ä¸‰ä¸ªæ ‡å‡†å¯¹ç§°æ€§æ£€æµ‹æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2508.21172', 'title': 'Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2508.21172', 'abstract': 'Deep Residual Echo State Networks (DeepResESNs) enhance long-term temporal modeling and memory capacity through hierarchical untrained residual layers, outperforming traditional shallow and deep reservoir computing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.', 'score': 0, 'issue_id': 5650, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'a0a4fea9c9a49fe6', 'authors': ['Matteo Pinna', 'Andrea Ceni', 'Claudio Gallicchio'], 'affiliations': ['Department of Computer Science, University of Pisa, 56127 Pisa, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2508.21172.jpg', 'data': {'categories': ['#long_context', '#math', '#optimization', '#architecture', '#training'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ĞÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ­Ñ…Ğ¾-Ğ“Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¡ĞµÑ‚Ğ¸ (DeepResESNs), Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ·ĞµÑ€Ğ²ÑƒĞ°Ñ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ².'}, 'en': {'title': 'Boosting Memory with Deep Residual Echo State Networks', 'desc': "Deep Residual Echo State Networks (DeepResESNs) are a new type of untrained Recurrent Neural Network designed to improve the handling of long-term temporal data. They use a structure of hierarchical residual layers that do not require training, which enhances their memory capacity. This paper explores how different configurations of these residual connections can affect the network's performance and stability. The results demonstrate that DeepResESNs outperform traditional reservoir computing methods in various time series tasks."}, 'zh': {'title': 'æ·±æ®‹å·®ç½‘ç»œï¼Œæå‡è®°å¿†ä¸å»ºæ¨¡èƒ½åŠ›', 'desc': 'æ·±æ®‹å·®å›å£°çŠ¶æ€ç½‘ç»œï¼ˆDeepResESNsï¼‰é€šè¿‡å±‚æ¬¡åŒ–çš„æœªè®­ç»ƒæ®‹å·®å±‚å¢å¼ºäº†é•¿æœŸæ—¶é—´å»ºæ¨¡å’Œè®°å¿†èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æµ…å±‚å’Œæ·±å±‚æ°´åº“è®¡ç®—æ–¹æ³•ã€‚å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNsï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„æœªè®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œåœ¨æ°´åº“è®¡ç®—æ¡†æ¶ä¸­å› å…¶å¿«é€Ÿé«˜æ•ˆçš„å­¦ä¹ è€Œå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ESNsåœ¨å¤„ç†é•¿æœŸä¿¡æ¯æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´æ®‹å·®è¿æ¥çš„æ–°å‹æ·±åº¦æœªè®­ç»ƒRNNï¼Œå±•ç¤ºäº†åˆ©ç”¨æœªè®­ç»ƒçš„æ®‹å·®é€’å½’å±‚çš„å±‚æ¬¡ç»“æ„æ˜¾è‘—æå‡äº†è®°å¿†å®¹é‡å’Œé•¿æœŸæ—¶é—´å»ºæ¨¡èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.19600', 'title': 'Quantization Robustness to Input Degradations for Object Detection', 'url': 'https://huggingface.co/papers/2508.19600', 'abstract': 'Post-training quantization of YOLO models is evaluated for robustness to real-world degradations, with a focus on the effectiveness of a degradation-aware calibration strategy for Static INT8 quantization.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop (~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at https://github.com/AllanK24/QRID.', 'score': 0, 'issue_id': 5649, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 27', 'zh': '8æœˆ27æ—¥'}, 'hash': 'ab400b3d8dc110c8', 'authors': ['Toghrul Karimov', 'Hassan Imani', 'Allan Kazakov'], 'affiliations': ['Bahcesehir University, Baku, Azerbaijan', 'Bahcesehir University, Istanbul, Turkey'], 'pdf_title_img': 'assets/pdf/title_img/2508.19600.jpg', 'data': {'categories': ['#benchmark', '#security', '#optimization', '#inference'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ YOLO: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ YOLO Ğ½Ğ° Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ INT8 ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… COCO Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑˆÑƒĞ¼, Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ¸ JPEG-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğµ Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing YOLO Robustness with Degradation-Aware Calibration', 'desc': 'This paper investigates the post-training quantization (PTQ) of YOLO object detection models to assess their robustness against real-world image degradations. It specifically focuses on a degradation-aware calibration strategy for Static INT8 quantization, which aims to improve model performance when faced with various input distortions like noise and blur. The study evaluates different precision formats and benchmarks the models on the COCO dataset under multiple degradation scenarios. Results show that while Static INT8 quantization improves processing speed, the proposed calibration method does not consistently enhance robustness, particularly for smaller models, although larger models may benefit under certain conditions.'}, 'zh': {'title': 'æå‡YOLOæ¨¡å‹é²æ£’æ€§çš„é‡åŒ–ç­–ç•¥', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†YOLOæ¨¡å‹çš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨çœŸå®ä¸–ç•Œé€€åŒ–ä¸‹çš„é²æ£’æ€§ï¼Œç‰¹åˆ«å…³æ³¨é™æ€INT8é‡åŒ–çš„é€€åŒ–æ„ŸçŸ¥æ ¡å‡†ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶é™æ€INT8 TensorRTå¼•æ“åœ¨å¹²å‡€æ•°æ®ä¸Šæä¾›äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œä½†åœ¨å¤§å¤šæ•°æ¨¡å‹å’Œé€€åŒ–æ¡ä»¶ä¸‹ï¼Œé€€åŒ–æ„ŸçŸ¥æ ¡å‡†å¹¶æœªå¸¦æ¥ä¸€è‡´çš„é²æ£’æ€§æ”¹å–„ã€‚å¯¹äºç‰¹å®šå™ªå£°æ¡ä»¶ä¸‹çš„å¤§å‹æ¨¡å‹ï¼Œæ ¡å‡†æ•ˆæœæœ‰æ‰€ä¸åŒï¼Œè¡¨æ˜æ¨¡å‹å®¹é‡å¯èƒ½å½±å“æ ¡å‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ä¸å—æ§ç¯å¢ƒä¸­éƒ¨ç½²é‡åŒ–æ£€æµ‹å™¨æä¾›äº†é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17008', 'title': 'EduRABSA: An Education Review Dataset for Aspect-based Sentiment\n  Analysis Tasks', 'url': 'https://huggingface.co/papers/2508.17008', 'abstract': 'EduRABSA is a public dataset and ASQE-DPT is a tool for aspect-based sentiment analysis in education reviews, addressing the lack of resources in this domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.', 'score': 0, 'issue_id': 5649, 'pub_date': '2025-08-23', 'pub_date_card': {'ru': '23 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 23', 'zh': '8æœˆ23æ—¥'}, 'hash': '22b921b4352ed731', 'authors': ['Yan Cathy Hua', 'Paul Denny', 'JÃ¶rg Wicker', 'Katerina Taskova'], 'affiliations': ['School of Computer Science, University of Auckland, New Zealand'], 'pdf_title_img': 'assets/pdf/title_img/2508.17008.jpg', 'data': {'categories': ['#open_source', '#dataset', '#low_resource', '#data'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ…', 'desc': 'EduRABSA - ÑÑ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ…. ASQE-DPT - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ABSA Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ABSA Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ Ğ¾ ĞºÑƒÑ€ÑĞ°Ñ…, Ğ¿Ñ€ĞµĞ¿Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ÑÑ… Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': 'Empowering Education Insights with EduRABSA and ASQE-DPT', 'desc': 'EduRABSA is a newly introduced public dataset specifically designed for aspect-based sentiment analysis (ABSA) in educational reviews, addressing the scarcity of resources in this area. The dataset includes annotated reviews covering various subjects such as courses, teaching staff, and universities, facilitating detailed opinion mining. Additionally, the ASQE-DPT tool allows for efficient manual data annotation, enabling researchers to create labeled datasets for comprehensive ABSA tasks. This work aims to enhance research in education by providing essential resources that promote transparency and reproducibility in sentiment analysis.'}, 'zh': {'title': 'æ¨åŠ¨æ•™è‚²è¯„è®ºæƒ…æ„Ÿåˆ†æçš„èµ„æºåˆ›æ–°', 'desc': 'EduRABSAæ˜¯ä¸€ä¸ªå…¬å…±æ•°æ®é›†ï¼Œä¸“æ³¨äºæ•™è‚²è¯„è®ºçš„åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ï¼Œè§£å†³äº†è¯¥é¢†åŸŸèµ„æºåŒ®ä¹çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è¯¾ç¨‹ã€æ•™å­¦äººå‘˜å’Œå¤§å­¦ä¸‰ç§è¯„è®ºä¸»é¢˜ï¼Œå¹¶æ”¯æŒå¤šç§ABSAä»»åŠ¡ï¼ŒåŒ…æ‹¬éšå«æ–¹é¢å’Œéšå«æ„è§çš„æå–ã€‚ASQE-DPTæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ‰‹åŠ¨æ•°æ®æ³¨é‡Šå·¥å…·ï¼Œå¯ä»¥ä»å•ä¸€ä»»åŠ¡æ³¨é‡Šç”Ÿæˆæ ‡è®°æ•°æ®é›†ï¼Œä¿ƒè¿›äº†æ•™è‚²é¢†åŸŸçš„ç ”ç©¶é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚é€šè¿‡è¿™äº›èµ„æºï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨æ•™è‚²è¯„è®ºçš„æƒ…æ„Ÿåˆ†æç ”ç©¶ï¼Œå¡«è¡¥ç°æœ‰çš„ç ”ç©¶ç©ºç™½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06160', 'title': 'Reverse-Engineered Reasoning for Open-Ended Generation', 'url': 'https://huggingface.co/papers/2509.06160', 'abstract': "REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.", 'score': 99, 'issue_id': 5784, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 7', 'zh': '9æœˆ7æ—¥'}, 'hash': '4df21d6c69df73d5', 'authors': ['Haozhe Wang', 'Haoran Que', 'Qixin Xu', 'Minghao Liu', 'Wangchunshu Zhou', 'Jiazhan Feng', 'Wanjun Zhong', 'Wei Ye', 'Tong Yang', 'Wenhao Huang', 'Ge Zhang', 'Fangzhen Lin'], 'affiliations': ['ByteDance Seed', 'Hong Kong University of Science and Technology', 'M-A-P', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06160.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#architecture', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'REER (Reverse-Engineered Reasoning) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ REER Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DeepWriting-20K Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeepWriter-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unlocking Creativity with Reverse Engineering in Deep Reasoning', 'desc': 'REER introduces a novel approach to deep reasoning by utilizing reverse engineering to uncover the step-by-step processes behind successful solutions. This method addresses the limitations of traditional reinforcement learning and instruction distillation, which struggle with open-ended tasks due to unclear rewards and high costs. By working backwards from known solutions, REER enables the discovery of effective reasoning pathways without the need for extensive trial-and-error. The resulting model, DeepWriter-8B, demonstrates competitive performance against both open-source and proprietary models, showcasing the potential of this new paradigm in creative generation tasks.'}, 'zh': {'title': 'é€†å‘æ¨ç†ï¼Œå¼€å¯æ·±åº¦æ¨ç†æ–°çºªå…ƒ', 'desc': 'REERæ˜¯ä¸€ç§æ–°çš„æ·±åº¦æ¨ç†èŒƒå¼ï¼Œé€šè¿‡é€†å‘å·¥ç¨‹å‘ç°é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¼ ç»Ÿçš„æ¨ç†æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ å’ŒæŒ‡ä»¤è’¸é¦ï¼Œåœ¨å¤„ç†å¼€æ”¾æ€§ç”Ÿæˆæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå‰è€…ç¼ºä¹æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œåè€…æˆæœ¬é«˜æ˜‚ä¸”å—é™äºæ•™å¸ˆæ¨¡å‹çš„èƒ½åŠ›ã€‚REERé€šè¿‡ä»å·²çŸ¥çš„ä¼˜ç§€è§£å†³æ–¹æ¡ˆå‘åæ¨å¯¼ï¼Œè®¡ç®—æ€§åœ°å‘ç°æ½œåœ¨çš„é€æ­¥æ·±åº¦æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬è¿˜å¼€æºäº†DeepWriting-20Kæ•°æ®é›†ï¼ŒåŒ…å«20,000ä¸ªæ·±åº¦æ¨ç†è½¨è¿¹ï¼Œè®­ç»ƒçš„DeepWriter-8Bæ¨¡å‹åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­è¶…è¶Šäº†å¼ºå¤§çš„å¼€æºåŸºçº¿ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ä¸é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oå’ŒClaude 3.5çš„è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06501', 'title': 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents', 'url': 'https://huggingface.co/papers/2509.06501', 'abstract': 'WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.', 'score': 55, 'issue_id': 5787, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'a4011c0eeba062d1', 'authors': ['Junteng Liu', 'Yunji Li', 'Chi Zhang', 'Jingyang Li', 'Aili Chen', 'Ke Ji', 'Weiyu Cheng', 'Zijia Wu', 'Chengyu Du', 'Qidi Xu', 'Jiayuan Song', 'Zhengmao Zhu', 'Wenhu Chen', 'Pengyu Zhao', 'Junxian He'], 'affiliations': ['The Hong Kong University of Science and Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.06501.jpg', 'data': {'categories': ['#agents', '#training', '#reasoning', '#agi', '#dataset', '#rl', '#long_context'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'WebExplorer: ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'desc': 'WebExplorer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ WebExplorer-8B Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ 100 Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²ÑĞµĞ³Ğ¾ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², WebExplorer-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'WebExplorer: Pioneering Advanced Web Agents for Information Seeking', 'desc': "WebExplorer is a novel approach that enhances web agents' ability to seek information effectively by generating challenging data through model-based exploration and iterative query evolution. It addresses the limitations of existing web agents, which often struggle with complex tasks due to a lack of robust data. By employing reinforcement learning and supervised fine-tuning, WebExplorer-8B demonstrates superior performance in information-seeking benchmarks, outperforming larger models in accuracy. This work paves the way for advanced web agents capable of long-horizon problem solving and complex web navigation."}, 'zh': {'title': 'WebExplorerï¼šä¿¡æ¯æ£€ç´¢çš„å…ˆè¿›ç½‘ç»œä»£ç†', 'desc': 'WebExploreræ˜¯ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºå¼€å‘å…ˆè¿›çš„ç½‘ç»œä»£ç†ï¼Œèƒ½å¤Ÿåœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿçš„æ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè§£å†³äº†ç°æœ‰å¼€æºç½‘ç»œä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚WebExploreré€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ¢ç´¢å’Œè¿­ä»£çš„æŸ¥è¯¢æ¼”å˜ï¼Œç”Ÿæˆéœ€è¦å¤šæ­¥æ¨ç†å’Œå¤æ‚ç½‘é¡µå¯¼èˆªçš„æŒ‘æˆ˜æ€§æŸ¥è¯¢-ç­”æ¡ˆå¯¹ã€‚æœ€ç»ˆï¼ŒWebExplorer-8Bæ¨¡å‹åœ¨å¤šç§ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ—¶é—´é—®é¢˜è§£å†³ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06949', 'title': 'Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models', 'url': 'https://huggingface.co/papers/2509.06949', 'abstract': 'TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL', 'score': 36, 'issue_id': 5783, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'f6f75969a4d93684', 'authors': ['Yinjie Wang', 'Ling Yang', 'Bowen Li', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.06949.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#math', '#rl', '#open_source', '#diffusion'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TraceRL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ TraceRL Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TraDo, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Language Models with Trajectory-Aware Reinforcement Learning', 'desc': 'TraceRL is a new framework that improves diffusion language models (DLMs) by using trajectory-aware reinforcement learning. This method helps the models perform better on complex reasoning tasks, such as math and coding, by incorporating preferred inference paths during training. The framework also allows for better sampling flexibility and can adapt smaller models to larger ones. With TraceRL, the resulting models, like TraDo, achieve significant accuracy improvements over existing models, demonstrating their effectiveness in challenging reasoning benchmarks.'}, 'zh': {'title': 'TraceRLï¼šæå‡æ¨ç†æ€§èƒ½çš„è½¨è¿¹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ', 'desc': 'TraceRLæ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è½¨è¿¹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨åæœŸè®­ç»ƒä¸­èå…¥ä¼˜å…ˆæ¨ç†è½¨è¿¹ï¼Œé€‚ç”¨äºä¸åŒçš„æ¨¡å‹æ¶æ„ã€‚è¯¥æ¡†æ¶é…å¤‡äº†åŸºäºæ‰©æ•£çš„ä»·å€¼æ¨¡å‹ï¼Œæå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶åœ¨å¤æ‚çš„æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šå±•ç¤ºäº†æ›´å¥½çš„æ¨ç†æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTraceRLè¿˜å¯ä»¥å°†ç‰¹å®šå—çš„æ¨¡å‹é€‚åº”åˆ°æ›´å¤§çš„å—ï¼Œä»è€Œæé«˜é‡‡æ ·çš„çµæ´»æ€§ã€‚é€šè¿‡ä½¿ç”¨TraceRLï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ‰©æ•£è¯­è¨€æ¨¡å‹TraDoï¼Œå°½ç®¡å…¶è§„æ¨¡å°äº7Bçš„è‡ªå›å½’æ¨¡å‹ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä»ç„¶è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06467', 'title': 'Does DINOv3 Set a New Medical Vision Standard?', 'url': 'https://huggingface.co/papers/2509.06467', 'abstract': "DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.", 'score': 27, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '05928cb3cc52644d', 'authors': ['Che Liu', 'Yinda Chen', 'Haoyuan Shi', 'Jinpeng Lu', 'Bailiang Jian', 'Jiazhen Pan', 'Linghan Cai', 'Jiayi Wang', 'Yundi Zhang', 'Jun Li', 'Cosmin I. Bercea', 'Cheng Ouyang', 'Chen Chen', 'Zhiwei Xiong', 'Benedikt Wiestler', 'Christian Wachinger', 'Daniel Rueckert', 'Wenjia Bai', 'Rossella Arcucci'], 'affiliations': ['Dresden University of Technology', 'Helmholtz AI and Helmholtz Munich', 'Imperial College London', 'Munich Center for Machine Learning', 'Technical University of Munich (TUM)', 'University of Erlangen-Nuremberg', 'University of Oxford', 'University of Science and Technology of China', 'University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2509.06467.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#cv', '#healthcare', '#benchmark', '#optimization', '#science'], 'emoji': 'ğŸ©º', 'ru': {'title': 'DINOv3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ DINOv3, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Vision Transformer, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ DINOv3 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, DINOv3 Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'DINOv3: A Powerful Vision Transformer for Medical Imaging Tasks', 'desc': 'DINOv3 is a self-supervised vision transformer that excels in various medical imaging tasks without needing pre-training on medical data. It has been benchmarked against common tasks like classification and segmentation, showing strong performance and even surpassing some specialized medical models. However, it struggles in highly specialized areas, such as Whole-Slide Pathological Images and Electron Microscopy, where deep domain knowledge is crucial. Additionally, DINOv3 does not consistently follow scaling laws in the medical domain, indicating that larger models or higher resolutions do not always lead to better performance.'}, 'zh': {'title': 'DINOv3ï¼šåŒ»å­¦è§†è§‰ä»»åŠ¡çš„æ–°åŸºå‡†', 'desc': 'DINOv3æ˜¯ä¸€ç§è‡ªç›‘ç£çš„è§†è§‰å˜æ¢å™¨ï¼Œåœ¨å„ç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒã€‚å°½ç®¡å®ƒåœ¨è®¸å¤šä»»åŠ¡ä¸­è¶…è¶Šäº†åŒ»å­¦ç‰¹å®šçš„åŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ä¸“ä¸šåŒ–çš„é¢†åŸŸä¸­è¡¨ç°æœ‰é™ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDINOv3çš„æ€§èƒ½åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œè¾“å…¥å›¾åƒåˆ†è¾¨ç‡ä¸‹å¹¶ä¸æ€»æ˜¯éµå¾ªæ‰©å±•è§„å¾‹ã€‚æ€»çš„æ¥è¯´ï¼ŒDINOv3ä¸ºå¤æ‚çš„åŒ»å­¦ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„è§†è§‰ç‰¹å¾åŸºç¡€ï¼Œå¼€å¯äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01656', 'title': 'Reinforced Visual Perception with Tools', 'url': 'https://huggingface.co/papers/2509.01656', 'abstract': "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.", 'score': 24, 'issue_id': 5783, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '08ec116a2ed1dc2c', 'authors': ['Zetong Zhou', 'Dongping Chen', 'Zixian Ma', 'Zhihan Hu', 'Mingyang Fu', 'Sinan Wang', 'Yao Wan', 'Zhou Zhao', 'Ranjay Krishna'], 'affiliations': ['ONE Lab, HUST', 'University of Maryland', 'University of Washington', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01656.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#optimization', '#benchmark', '#rl', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReVPT, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ReVPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ RL Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SAT, CV-Bench, BLINK Ğ¸ MMStar. ReVPT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'ReVPT: Reinforcement Learning for Enhanced Visual Reasoning in Multi-Modal LLMs', 'desc': 'ReVPT is a novel approach that enhances the visual reasoning capabilities of multi-modal large language models (LLMs) using reinforcement learning (RL). It addresses the limitations of previous methods that relied on supervised finetuning, which often required expensive data generation and struggled with generalization. By introducing a new RL algorithm based on Generalized Relative Policy Optimization (GRPO), ReVPT trains models to effectively utilize a set of visual tools for reasoning tasks. The results demonstrate that ReVPT achieves state-of-the-art performance on various visual benchmarks, significantly outperforming existing methods.'}, 'zh': {'title': 'ReVPTï¼šæå‡å¤šæ¨¡æ€LLMçš„è§†è§‰æ¨ç†èƒ½åŠ›', 'desc': 'ReVPTæ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œè¾¾åˆ°äº†è§†è§‰åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ¶‰åŠå¤æ‚çš„æ„ŸçŸ¥å’Œé€»è¾‘è¿‡ç¨‹ï¼Œä½†å°†è®¡ç®—æœºè§†è§‰æ¨¡å‹åº”ç”¨äºä¸€èˆ¬è§†è§‰æ¨ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå°†è§†è§‰æ¨¡å‹ä¸LLMç»“åˆå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å­˜åœ¨æ•°æ®ç”Ÿæˆæˆæœ¬é«˜ã€æ•°æ®è¿‡æ»¤ä¾èµ–æ€§å¼ºå’Œæ³›åŒ–èƒ½åŠ›å·®ç­‰å…³é”®é™åˆ¶ã€‚ReVPTé€šè¿‡å¼•å…¥åŸºäºGRPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè®­ç»ƒæ¨¡å‹ä½¿ç”¨å››ç§è§†è§‰å·¥å…·è¿›è¡Œæ¨ç†ï¼Œä»è€Œæœ‰æ•ˆè§£å†³äº†è¿™äº›é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06733', 'title': 'Reinforcement Learning Foundations for Deep Research Systems: A Survey', 'url': 'https://huggingface.co/papers/2509.06733', 'abstract': 'Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.', 'score': 20, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '039bfe4e964d5aad', 'authors': ['Wenjun Li', 'Zhi Chen', 'Jingru Lin', 'Hannan Cao', 'Wei Han', 'Sheng Liang', 'Zhi Zhang', 'Kuicai Dong', 'Dexun Li', 'Chen Zhang', 'Yong Liu'], 'affiliations': ['Huawei Technologies Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2509.06733.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#training', '#long_context', '#benchmark', '#survey', '#optimization', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Empowering AI with Reinforcement Learning for Complex Task Mastery', 'desc': 'This paper discusses the use of reinforcement learning (RL) as a key method for training advanced AI systems that can perform complex tasks. It highlights the limitations of traditional supervised learning and preference alignment methods, which often rely on human-defined rules and can lead to biases. The authors propose that RL can improve the training of these systems by optimizing their interactions with tools and environments, allowing for better exploration and decision-making. The paper also provides a comprehensive overview of RL techniques and frameworks that can enhance the development of deep research agents.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ï¼šæ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„åŸºç¡€', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºè®­ç»ƒæ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„åŸºç¡€æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç›‘ç£å­¦ä¹ å’Œåå¥½å¯¹é½æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡ä¼˜åŒ–å·¥å…·äº¤äº’å’Œæ¢ç´¢çš„ç­–ç•¥ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚æˆ‘ä»¬ç³»ç»ŸåŒ–äº†ä¸æ·±åº¦ç ”ç©¶ç³»ç»Ÿç›¸å…³çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®åˆæˆã€æ ·æœ¬æ•ˆç‡å’Œå¤šç›®æ ‡ä¼˜åŒ–ç­‰æ–¹é¢ã€‚æœ€åï¼Œè®ºæ–‡æä¾›äº†å…³äºå¦‚ä½•è®­ç»ƒç¨³å¥ã€é€æ˜çš„æ·±åº¦ç ”ç©¶ä»£ç†çš„å®ç”¨æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06461', 'title': "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", 'url': 'https://huggingface.co/papers/2509.06461', 'abstract': "Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.", 'score': 13, 'issue_id': 5785, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'f239789cf3c0a63d', 'authors': ['Yuyao Ge', 'Shenghua Liu', 'Yiwei Wang', 'Lingrui Mei', 'Baolong Bi', 'Xuanshan Zhou', 'Jiayu Yao', 'Jiafeng Guo', 'Xueqi Cheng'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2509.06461.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#training', '#multimodal', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ CARVE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. CARVE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, ĞºĞ°Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ CARVE Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… VLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Visual Reasoning with Contrastive Attention', 'desc': 'The paper introduces Contrastive Attention Refinement for Visual Enhancement (CARVE), a method designed to improve the performance of Vision-Language Models (VLMs) in complex visual environments. It identifies that visual complexity affects attention patterns, leading to decreased reasoning performance. By contrasting attention maps from general and task-specific queries, CARVE effectively separates useful visual signals from noise without requiring additional training or external tools. The results show significant performance improvements, highlighting the importance of attention mechanisms in visual reasoning tasks.'}, 'zh': {'title': 'å¯¹æ¯”æ³¨æ„åŠ›ç²¾ç‚¼ï¼Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼', 'desc': 'å¯¹æ¯”æ³¨æ„åŠ›ç²¾ç‚¼ï¼ˆCARVEï¼‰æ˜¯ä¸€ç§æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ€§èƒ½çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”æ³¨æ„åŠ›æœºåˆ¶æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡å·ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸­ï¼Œç°æœ‰å¢å¼ºæ–¹æ³•éœ€è¦é¢å¤–è®­ç»ƒæˆ–ä¾èµ–å¤–éƒ¨åˆ†å‰²å·¥å…·çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰å¤æ‚æ€§ä¸æ³¨æ„åŠ›ç†µå¯†åˆ‡ç›¸å…³ï¼Œå½±å“æ¨ç†æ€§èƒ½ï¼Œè€Œæ³¨æ„åŠ›åœ¨ä¸åŒå±‚æ¬¡ä¸Šé€æ¸ä»å…¨å±€æ‰«æè½¬å‘æ·±å±‚çš„èšç„¦æ”¶æ•›ã€‚CARVEé€šè¿‡å¯¹æ¯”ä¸€èˆ¬æŸ¥è¯¢å’Œä»»åŠ¡ç‰¹å®šæŸ¥è¯¢çš„æ³¨æ„åŠ›å›¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†è§£è§†è§‰ä¿¡å·ï¼Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06155', 'title': 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts', 'url': 'https://huggingface.co/papers/2509.06155', 'abstract': 'UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.', 'score': 13, 'issue_id': 5783, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 7', 'zh': '9æœˆ7æ—¥'}, 'hash': '9f7107d4dbe89f53', 'authors': ['Duomin Wang', 'Wei Zuo', 'Aojie Li', 'Ling-Hao Chen', 'Xinyao Liao', 'Deyu Zhou', 'Zixin Yin', 'Xili Dai', 'Daxin Jiang', 'Gang Yu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology(GuangZhou)', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06155.jpg', 'data': {'categories': ['#multimodal', '#video', '#audio', '#benchmark', '#open_source'], 'emoji': 'ğŸ¥', 'ru': {'title': 'UniVerse-1: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UniVerse-1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ "stitching of experts". Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 7600 Ñ‡Ğ°ÑĞ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'UniVerse-1: Harmonizing Audio and Video Generation', 'desc': 'UniVerse-1 is a cutting-edge model designed for generating synchronized audio and video content. It utilizes a stitching of experts (SoE) technique to merge pre-trained models for video and music, enhancing the quality of the generated outputs. The model incorporates an online annotation pipeline to ensure precise temporal alignment between audio and video, which is crucial for maintaining coherence in the generated media. After fine-tuning on a substantial dataset, UniVerse-1 demonstrates significant improvements in producing well-coordinated audio-visuals, making it a valuable tool for advancing research in the field.'}, 'zh': {'title': 'ç»Ÿä¸€éŸ³è§†é¢‘ç”Ÿæˆçš„æœªæ¥', 'desc': 'UniVerse-1 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨ä¸“å®¶æ‹¼æ¥æŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒçš„è§†é¢‘å’ŒéŸ³ä¹æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·ï¼Œç¡®ä¿æ—¶é—´ä¸Šçš„å‡†ç¡®å¯¹é½ï¼Œç”Ÿæˆé«˜è´¨é‡çš„éŸ³è§†é¢‘è¾“å‡ºã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦èåˆé¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ï¼Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„ä½æ•ˆï¼Œå……åˆ†åˆ©ç”¨äº†å·²æœ‰çš„åŸºç¡€èƒ½åŠ›ã€‚ä¸ºäº†ç¡®ä¿éŸ³é¢‘å’Œè§†é¢‘å†…å®¹çš„å‡†ç¡®æ³¨é‡Šå’Œæ—¶é—´å¯¹é½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æ³¨é‡Šç®¡é“ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†æ‰€éœ€çš„æ•°æ®å¹¶ç”Ÿæˆæ ‡ç­¾ã€‚ç»è¿‡çº¦7600å°æ—¶çš„éŸ³è§†é¢‘æ•°æ®å¾®è°ƒåï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆåè°ƒè‰¯å¥½çš„éŸ³è§†é¢‘å†…å®¹ï¼Œå¹¶åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢å®ç°å¼ºå¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06917', 'title': 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents', 'url': 'https://huggingface.co/papers/2509.06917', 'abstract': "Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.", 'score': 10, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '2a482f9a59e68451', 'authors': ['Jiacheng Miao', 'Joe R. Davis', 'Jonathan K. Pritchard', 'James Zou'], 'affiliations': ['Department of Biology, Stanford University', 'Department of Biomedical Data Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Genetics, Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06917.jpg', 'data': {'categories': ['#agents', '#science', '#multimodal'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Paper2Agent - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ ĞºĞ¾Ğ´ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Model Context Protocol (MCP) ÑĞµÑ€Ğ²ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğº Ñ‡Ğ°Ñ‚-Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Paper2Agent Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ¼Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Transforming Research Papers into Interactive AI Agents', 'desc': 'Paper2Agent is a framework that transforms traditional research papers into interactive AI agents, making it easier for users to access and utilize scientific knowledge. By analyzing the paper and its associated code, it creates a Model Context Protocol (MCP) server that allows the AI agent to answer complex queries in natural language. This approach reduces the effort required for researchers to understand and apply the findings, thus enhancing the dissemination and reuse of research outputs. The effectiveness of Paper2Agent is demonstrated through case studies where it successfully reproduces original results and handles novel queries, paving the way for a new collaborative ecosystem in scientific research.'}, 'zh': {'title': 'å°†é™æ€è®ºæ–‡è½¬å˜ä¸ºåŠ¨æ€ AI ä»£ç†çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'Paper2Agent æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¯ä»¥å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºäº’åŠ¨çš„ AI ä»£ç†ï¼Œä»¥ä¿ƒè¿›çŸ¥è¯†ä¼ æ’­å’Œå¤æ‚ç§‘å­¦æŸ¥è¯¢ã€‚å®ƒé€šè¿‡åˆ†æè®ºæ–‡åŠå…¶ç›¸å…³ä»£ç ï¼Œæ„å»ºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æœåŠ¡å™¨ï¼Œä½¿å¾—ç ”ç©¶æˆæœä»è¢«åŠ¨çš„æ–‡çŒ®å˜ä¸ºä¸»åŠ¨çš„ç³»ç»Ÿã€‚è¿™æ ·ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸ AI ä»£ç†è¿›è¡Œäº¤æµï¼Œè½»æ¾è·å–è®ºæ–‡ä¸­çš„ä¿¡æ¯å’Œå·¥å…·ã€‚Paper2Agent çš„æœ‰æ•ˆæ€§é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å¾—åˆ°äº†éªŒè¯ï¼Œèƒ½å¤Ÿé‡ç°åŸè®ºæ–‡çš„ç»“æœå¹¶å¤„ç†æ–°çš„ç”¨æˆ·æŸ¥è¯¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02108', 'title': 'DivMerge: A divergence-based model merging method for multi-tasking', 'url': 'https://huggingface.co/papers/2509.02108', 'abstract': 'Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose a method that merges models trained on different tasks into a single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work.', 'score': 10, 'issue_id': 5797, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '615289bb7bc02e97', 'authors': ['Touayouch Brahim', 'Fosse LoÃ¯c', 'Damnati GÃ©raldine', 'LecorvÃ© GwÃ©nolÃ©'], 'affiliations': ['CNRS, LIS, Aix Marseille UniversitÃ©, France', 'Orange Research, Lannion, France', 'Ã‰cole polytechnique, Institut polytechnique de Paris, Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.02108.jpg', 'data': {'categories': ['#training', '#dataset', '#architecture', '#optimization', '#transfer_learning'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ Ğ™ĞµĞ½ÑĞµĞ½Ğ°-Ğ¨ĞµĞ½Ğ½Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Merging Models for Robust Multi-Task Learning', 'desc': 'This paper discusses a new method for multi-task learning (MTL) that combines models trained on different tasks into one model. The challenge of task interference, which can degrade performance as more tasks are added, is addressed by using Jensen-Shannon divergence to guide the merging process. This method does not need extra labeled data and automatically balances the importance of each task. The proposed approach shows improved robustness and performance compared to existing methods, even as the number of tasks increases.'}, 'zh': {'title': 'æ™ºèƒ½åˆå¹¶ï¼Œæå‡å¤šä»»åŠ¡å­¦ä¹ æ€§èƒ½', 'desc': 'å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰é€šå¸¸é€šè¿‡åœ¨å¾®è°ƒä¹‹å‰åˆå¹¶æ•°æ®é›†æ¥å®ç°ï¼Œä½†éšç€å¾®è°ƒæ¨¡å‹çš„æ—¥ç›Šå¢å¤šï¼Œå‡ºç°äº†é€šè¿‡ä»»åŠ¡ç®—æœ¯åˆå¹¶æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ä»»åŠ¡å¹²æ‰°ï¼Œéšç€ä»»åŠ¡æ•°é‡çš„å¢åŠ è€ŒåŠ å‰§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ä¸åŒä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹åˆå¹¶ä¸ºå•ä¸€æ¨¡å‹çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†è©¹æ£®-é¦™å†œæ•£åº¦æ¥æŒ‡å¯¼åˆå¹¶è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ ‡è®°æ•°æ®ï¼Œå¹¶è‡ªåŠ¨å¹³è¡¡ä»»åŠ¡çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03516', 'title': 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?', 'url': 'https://huggingface.co/papers/2509.03516', 'abstract': 'T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/.', 'score': 9, 'issue_id': 5784, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': '8f5ee8fae69242b1', 'authors': ['Ouxiang Li', 'Yuan Wang', 'Xinting Hu', 'Huijuan Huang', 'Rui Chen', 'Jiarong Ou', 'Xin Tao', 'Pengfei Wan', 'Fuli Feng'], 'affiliations': ['Kuaishou Technology', 'Nanyang Technological University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.03516.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning', '#games'], 'emoji': 'ğŸ¨', 'ru': {'title': 'T2I-CoReBench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'T2I-CoReBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1080 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 13500 ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ 12-Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ° ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„ÑĞºĞ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚.'}, 'en': {'title': 'Elevating Text-to-Image Models: A New Benchmark for Composition and Reasoning', 'desc': 'T2I-CoReBench is a new benchmark designed to assess the composition and reasoning abilities of text-to-image (T2I) models. It introduces a detailed evaluation framework that includes a 12-dimensional taxonomy focusing on scene graph elements and various types of reasoning. The benchmark features 1,080 complex prompts and approximately 13,500 checklist questions to ensure a thorough evaluation of model performance. Results indicate that while T2I models can handle basic tasks, they struggle significantly with complex scenarios and implicit reasoning, highlighting areas for improvement.'}, 'zh': {'title': 'è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç»„åˆä¸æ¨ç†èƒ½åŠ›', 'desc': 'T2I-CoReBenchæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç»„åˆå’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡å¤æ‚çš„æç¤ºå’Œæ£€æŸ¥é—®é¢˜ï¼Œå…¨é¢è€ƒå¯Ÿæ¨¡å‹åœ¨é«˜åœºæ™¯å¯†åº¦ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç»„åˆèƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè€Œæ¨ç†èƒ½åŠ›æ›´æ˜¯æˆä¸ºå…³é”®ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºçš„åŸºå‡†åŒ…å«1080ä¸ªæŒ‘æˆ˜æ€§æç¤ºå’Œçº¦13500ä¸ªæ£€æŸ¥é—®é¢˜ï¼Œä»¥å®ç°ç»†è‡´å¯é çš„è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06945', 'title': 'Interleaving Reasoning for Better Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2509.06945', 'abstract': 'Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .', 'score': 7, 'issue_id': 5786, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '0f828dd1d00b0a90', 'authors': ['Wenxuan Huang', 'Shuang Chen', 'Zheyong Xie', 'Shaosheng Cao', 'Shixiang Tang', 'Yufan Shen', 'Qingyu Yin', 'Wenbo Hu', 'Xiaoman Wang', 'Yuntian Tang', 'Junbo Qiao', 'Yue Guo', 'Yao Hu', 'Zhenfei Yin', 'Philip Torr', 'Yu Cheng', 'Wanli Ouyang', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles', 'University of Oxford', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06945.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#optimization', '#reasoning', '#dataset', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ§ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Text-to-Image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ - Interleaving Reasoning Generation (IRG). Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ IRGL Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ IRGL-300K Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IRG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Image Generation through Interleaved Reasoning', 'desc': 'The Interleaving Reasoning Generation (IRG) framework enhances Text-to-Image (T2I) generation by alternating between text-based reasoning and image synthesis. This approach allows the model to first generate an initial image based on textual input, then refine it by reflecting on the generated output to improve details and visual quality. The training process, called Interleaving Reasoning Generation Learning (IRGL), focuses on establishing a strong initial image and ensuring high-quality textual feedback for further refinements. The results demonstrate significant advancements in performance metrics and visual fidelity, showcasing the effectiveness of this interleaved reasoning approach.'}, 'zh': {'title': 'äº¤é”™æ¨ç†ç”Ÿæˆï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒçš„è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºäº¤é”™æ¨ç†ç”Ÿæˆï¼ˆIRGï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æ¡†æ¶é€šè¿‡äº¤æ›¿è¿›è¡ŒåŸºäºæ–‡æœ¬çš„æ€è€ƒå’Œå›¾åƒåˆæˆï¼Œæ¥å¢å¼ºç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡å’Œç»†èŠ‚ä¿ç•™ã€‚IRGçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆå»ºç«‹æ ¸å¿ƒå†…å®¹å’ŒåŸºç¡€è´¨é‡ï¼Œç„¶ååœ¨åç»­å›¾åƒä¸­å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åæ€å’Œç»†è‡´çš„æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRGåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06631', 'title': 'Guided Decoding and Its Critical Role in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2509.06631', 'abstract': 'Guided decoding methods in Retrieval-Augmented Generation (RAG) systems are evaluated for structured output generation, revealing performance variations across different prompting setups.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.', 'score': 5, 'issue_id': 5795, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '3deb208811cb9805', 'authors': ['Ã–zgÃ¼r UÄŸur', 'Musa YÄ±lmaz', 'Esra Åavirdi', 'Ã–zay Ezerceli', 'Mahmut El Huseyni', 'Selva TaÅŸ', 'Reyhan Bayraktar'], 'affiliations': ['Newmind AI Istanbul, TÃ¼rkiye'], 'pdf_title_img': 'assets/pdf/title_img/2509.06631.jpg', 'data': {'categories': ['#hallucinations', '#rag', '#alignment'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² RAG: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° - Outlines, XGrammar Ğ¸ LM Format Enforcer - Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Structured Outputs in RAG Systems with Guided Decoding', 'desc': 'This paper investigates how guided decoding methods can improve structured output generation in Retrieval-Augmented Generation (RAG) systems. It compares three specific methods: Outlines, XGrammar, and LM Format Enforcer, under different prompting scenarios. The study measures success rates, hallucination rates, and overall output quality to understand how multi-turn interactions affect performance. The findings provide valuable insights for selecting appropriate methods in RAG systems, enhancing the reliability of AI-generated responses.'}, 'zh': {'title': 'ä¼˜åŒ–RAGç³»ç»Ÿä¸­çš„ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆ', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ï¼ŒæŒ‡å¯¼è§£ç æ–¹æ³•å¯¹ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆçš„å½±å“ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šå¤§çº²ã€XGrammarå’Œè¯­è¨€æ¨¡å‹æ ¼å¼å¼ºåˆ¶å™¨ï¼Œåˆ†æäº†å®ƒä»¬åœ¨ä¸åŒå¤šè½®æç¤ºè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¤šè½®äº¤äº’å¯¹æŒ‡å¯¼è§£ç çš„å½±å“ï¼Œä»¥åŠä¸åŒæ–¹æ³•åœ¨æˆåŠŸç‡ã€å¹»è§‰ç‡å’Œè¾“å‡ºè´¨é‡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚æ­¤é¡¹å·¥ä½œä¸ºRAGç³»ç»Ÿä¸­çš„ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆæä¾›äº†ç†è®ºè§è§£å’Œå®é™…æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06493', 'title': 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers', 'url': 'https://huggingface.co/papers/2509.06493', 'abstract': 'BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces BFS-Prover-V2, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.', 'score': 5, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'e9de5406241d634e', 'authors': ['Ran Xin', 'Zeyu Zheng', 'Yanchen Nie', 'Kun Yuan', 'Xia Xiao'], 'affiliations': ['ByteDance Seed', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06493.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#training', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'BFS-Prover-V2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ñ€ĞµÑˆĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ°ĞºÑ‚Ğ¸Ğº. BFS-Prover-V2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ.'}, 'en': {'title': 'Scaling Theorem Proving with Smart Collaboration', 'desc': "BFS-Prover-V2 is a system that enhances automated theorem proving by tackling the challenges of scaling in both training and inference. It introduces a multi-turn off-policy reinforcement learning (RL) framework that improves the performance of large language models (LLMs) during training, inspired by AlphaZero's expert iteration approach. Additionally, it features a planner-enhanced multi-agent search architecture that breaks down complex theorems into simpler subgoals, allowing multiple agents to work together efficiently. This innovative dual approach has achieved state-of-the-art results on formal mathematics benchmarks, demonstrating its potential for broader applications in complex reasoning tasks."}, 'zh': {'title': 'åŒé‡æ‰©å±•ï¼Œçªç ´å®šç†è¯æ˜çš„æé™', 'desc': 'BFS-Prover-V2 æ˜¯ä¸€ä¸ªé’ˆå¯¹è‡ªåŠ¨å®šç†è¯æ˜ä¸­çš„æ‰©å±•æŒ‘æˆ˜çš„ç³»ç»Ÿï¼Œç»“åˆäº†å¤šè½®ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œè§„åˆ’å¢å¼ºçš„å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ›æ–°çš„å¤šè½®ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒæŒç»­æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒæ—¶çš„è¡¨ç°ï¼Œå¹¶å…‹æœäº†é•¿æœŸå¼ºåŒ–å­¦ä¹ ä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚å…¶æ¬¡ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ä¸ªé«˜å±‚æ¬¡çš„è§„åˆ’æ¨¡å‹ï¼Œå°†å¤æ‚å®šç†åˆ†è§£ä¸ºä¸€ç³»åˆ—ç®€å•çš„å­ç›®æ ‡ï¼Œä»è€Œåœ¨æ¨ç†æ—¶æœ‰æ•ˆç¼©å°æœç´¢ç©ºé—´ã€‚é€šè¿‡è¿™ç§åŒé‡æ‰©å±•æ–¹æ³•ï¼ŒBFS-Prover-V2 åœ¨æ­£å¼æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06861', 'title': 'Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet', 'url': 'https://huggingface.co/papers/2509.06861', 'abstract': 'Test-time scaling does not consistently improve accuracy or reduce hallucinations in knowledge-intensive tasks, often leading to overconfident errors.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge', 'score': 4, 'issue_id': 5789, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '128069f43a6c18a2', 'authors': ['James Xu Zhao', 'Bryan Hooi', 'See-Kiong Ng'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.06861.jpg', 'data': {'categories': ['#inference', '#benchmark', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ - Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞµ: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (test-time scaling) Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 12 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ¢ĞµĞ¼ Ğ½Ğµ Ğ¼ĞµĞ½ĞµĞµ, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼.'}, 'en': {'title': 'Test-Time Scaling: More Thinking, More Hallucinations?', 'desc': 'This paper investigates the effectiveness of test-time scaling in improving the performance of reasoning models on knowledge-intensive tasks. While test-time scaling allows models to generate longer reasoning chains, the authors find that it does not consistently enhance accuracy and can even increase the occurrence of hallucinations. The study evaluates 12 reasoning models across two benchmarks, revealing that longer reasoning often leads to overconfident errors rather than improved factual accuracy. The findings suggest that while extended reasoning can be beneficial, it may also induce confirmation bias, highlighting the need for careful application in knowledge-intensive scenarios.'}, 'zh': {'title': 'æµ‹è¯•æ—¶æ‰©å±•æ¨ç†çš„å±€é™æ€§ä¸æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶æ‰©å±•æ¨ç†å¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å½±å“ã€‚å°½ç®¡æµ‹è¯•æ—¶æ‰©å±•æ¨ç†å¯ä»¥å¢åŠ æ¨ç†é“¾çš„é•¿åº¦å¹¶æé«˜æ¨¡å‹çš„è¡¨ç°ï¼Œä½†åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å¹¶æœªæ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œåè€Œå¯èƒ½å¯¼è‡´æ›´å¤šçš„å¹»è§‰é”™è¯¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå»¶é•¿æ¨ç†æ—¶é—´å¹¶ä¸æ€»æ˜¯èƒ½æ”¹å–„æ¨¡å‹çš„äº‹å®å›å¿†ï¼Œåè€Œå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æœªå›ç­”çš„é—®é¢˜æ—¶äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„å¹»è§‰ã€‚å°½ç®¡å­˜åœ¨è¿™äº›å±€é™æ€§ï¼Œå¯ç”¨æ€è€ƒä»ç„¶æ¯”ä¸æ€è€ƒæ›´æœ‰ç›Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06786', 'title': 'R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World', 'url': 'https://huggingface.co/papers/2509.06786', 'abstract': "A new framework, RÂ²AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose safe-by-coevolution as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce R^2AI -- Resistant and Resilient AI -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. R^2AI integrates fast and slow safe models, adversarial simulation and verification through a safety wind tunnel, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI.", 'score': 3, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'bdf421cc1d868635', 'authors': ['Youbang Sun', 'Xiang Wang', 'Jie Fu', 'Chaochao Lu', 'Bowen Zhou'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.06786.jpg', 'data': {'categories': ['#agents', '#security', '#ethics', '#training', '#agi'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞšĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ RÂ²AI Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°Ğ¼ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ¸ÑĞºĞ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RÂ²AI Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ adversarial simulation Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 'safety wind tunnel', Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."}, 'en': {'title': 'RÂ²AI: Evolving Safety for Advanced AI Systems', 'desc': 'The paper introduces RÂ²AI, a new framework aimed at improving AI safety by combining two key strategies: resistance to known threats and resilience to unexpected risks. It critiques existing safety approaches that either reactively apply safety measures or struggle with unforeseen challenges in complex environments. RÂ²AI proposes a coevolutionary approach to safety, inspired by biological immunity, where safety is treated as an ongoing learning process. This framework utilizes fast and slow safe models, adversarial simulations, and continuous feedback to ensure that AI systems can adapt and remain safe as they evolve.'}, 'zh': {'title': 'RÂ²AIï¼šå…±è¿›åŒ–æå‡äººå·¥æ™ºèƒ½å®‰å…¨æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶RÂ²AIï¼Œæ—¨åœ¨é€šè¿‡å…±è¿›åŒ–å¢å¼ºäººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¯¹å·²çŸ¥å¨èƒçš„æŠµæŠ—åŠ›å’Œå¯¹æœªçŸ¥é£é™©çš„éŸ§æ€§ï¼Œä½¿ç”¨å¿«é€Ÿå’Œæ…¢é€Ÿå®‰å…¨æ¨¡å‹ä»¥åŠå¯¹æŠ—æ€§æ¨¡æ‹Ÿã€‚RÂ²AIçš„è®¾è®¡çµæ„Ÿæ¥è‡ªç”Ÿç‰©å…ç–«ï¼Œå¼ºè°ƒå®‰å…¨æ€§æ˜¯ä¸€ä¸ªåŠ¨æ€çš„ã€å¯¹æŠ—æ€§çš„æŒç»­å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¿æŒæŒç»­çš„å®‰å…¨æ€§ï¼Œè§£å†³çŸ­æœŸè„†å¼±æ€§å’Œé•¿æœŸå­˜åœ¨é£é™©çš„é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05668', 'title': 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian', 'url': 'https://huggingface.co/papers/2509.05668', 'abstract': 'Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.', 'score': 3, 'issue_id': 5784, 'pub_date': '2025-09-06', 'pub_date_card': {'ru': '6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 6', 'zh': '9æœˆ6æ—¥'}, 'hash': '8fe2894e1bb529f0', 'authors': ['Michael Hoffmann', 'Jophin John', 'Stefan Schweter', 'Gokul Ramakrishnan', 'Hoi-Fong Mak', 'Alice Zhang', 'Dmitry Gaynullin', 'Nicolay J. Hammer'], 'affiliations': ['Cerebras Systems Sunnyvale, USA', 'Independent Researcher Holzkirchen, Germany', 'Leibniz Supercomputing Centre (LRZ) Garching, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2509.05668.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#architecture', '#training', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ° Ğ² Ğ˜Ğ˜: Ñ‚Ñ€ĞµÑ…ÑŠÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸', 'desc': 'Llama-GENBA-10B - ÑÑ‚Ğ¾ Ñ‚Ñ€ĞµÑ…ÑŠÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾, Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ±Ğ°Ğ²Ğ°Ñ€ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ĞµĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ²Ğ°Ñ€ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ğº Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ°Ğ²Ğ°Ñ€ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼.'}, 'en': {'title': 'Balancing Languages: Llama-GENBA-10B Redefines Multilingual AI', 'desc': 'Llama-GENBA-10B is a trilingual foundation model designed to reduce English-centric bias in language processing. It is trained on a balanced dataset of English, German, and Bavarian, ensuring that no single language dominates the training process. The model addresses challenges such as the scarcity of Bavarian data and the need for a unified tokenizer across languages. Evaluations indicate that Llama-GENBA-10B excels in cross-lingual tasks, particularly in Bavarian, setting new performance benchmarks and demonstrating effective multilingual pretraining techniques.'}, 'zh': {'title': 'æ‰“ç ´è¯­è¨€åè§ï¼Œä¿ƒè¿›å¤šè¯­è¨€å¹³è¡¡', 'desc': 'Llama-GENBA-10B æ˜¯ä¸€ä¸ªä¸‰è¯­åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è‹±è¯­åè§é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡äº†è‹±è¯­ã€å¾·è¯­å’Œå·´ä¼åˆ©äºšè¯­çš„èµ„æºï¼Œä½¿ç”¨äº† 1640 äº¿ä¸ªæ ‡è®°ï¼Œç¡®ä¿äº†å„è¯­è¨€çš„å…¬å¹³æ€§ã€‚é€šè¿‡ä¼˜åŒ–æ¶æ„å’Œè¯­è¨€æ¯”ä¾‹è¶…å‚æ•°ï¼ŒLlama-GENBA-10B åœ¨è·¨è¯­è¨€è¿ç§»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å·´ä¼åˆ©äºšè¯­çš„è¯„ä¼°ä¸­è®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚è¯¥æ¨¡å‹çš„å¼€å‘ä¸ºä½èµ„æºè¯­è¨€çš„æ•´åˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¤šè¯­è¨€é¢„è®­ç»ƒçš„é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06771', 'title': 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning', 'url': 'https://huggingface.co/papers/2509.06771', 'abstract': "A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.  \t\t\t\t\tAI-generated summary \t\t\t\t Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning", 'score': 2, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '48d37925f403181d', 'authors': ['Sai Kartheek Reddy Kasu', 'Mohammad Zia Ur Rehman', 'Shahid Shafi Dar', 'Rishi Bharat Junghare', 'Dhanvin Sanjay Namboodiri', 'Nagendra Kumar'], 'affiliations': ['Indian Institute of Information Technology Dharwad, India', 'Indian Institute of Technology Indore, India', 'Malaviya National Institute of Technology Jaipur, India'], 'pdf_title_img': 'assets/pdf/title_img/2509.06771.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#games', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ñ‹Ğ¹ ÑĞ¼Ğ¾Ñ€ Ğ² Ğ¼ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (TCRNet). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 4,379 Ğ¼ĞµĞ¼Ğ¾Ğ² Reddit Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. VLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ¼Ğ°, Ğ° TCRNet Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unraveling Dark Humor in Memes with Advanced Reasoning Techniques', 'desc': "This paper presents a new framework for detecting dark humor in memes using a combination of a Large Vision-Language Model (VLM) and a Tri-stream Cross-Reasoning Network (TCRNet). The authors created a dataset of 4,379 Reddit memes, annotated for dark humor, target categories, and intensity levels. The framework generates structured explanations for memes and refines them through a self-loop mechanism, enhancing the model's understanding. By integrating textual and visual features with reasoning, the model achieves superior performance in identifying dark humor, targets, and intensity compared to existing methods."}, 'zh': {'title': 'å¢å¼ºæ¨ç†ï¼Œç²¾å‡†è¯†åˆ«é»‘è‰²å¹½é»˜', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºæ¨ç†çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸‰æµäº¤å‰æ¨ç†ç½‘ç»œï¼Œæ—¨åœ¨æé«˜å¯¹é»‘è‰²å¹½é»˜çš„æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«4379ä¸ªRedditè¡¨æƒ…åŒ…çš„æ–°æ•°æ®é›†ï¼Œæ ‡æ³¨äº†é»‘è‰²å¹½é»˜ã€ç›®æ ‡ç±»åˆ«å’Œå¼ºåº¦ç­‰çº§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆç»“æ„åŒ–è§£é‡Šï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œæ¨ç†ç‰¹å¾ï¼Œè¿›è¡Œæœ‰æ•ˆçš„åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é»‘è‰²å¹½é»˜æ£€æµ‹ã€ç›®æ ‡è¯†åˆ«å’Œå¼ºåº¦é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06477', 'title': 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents', 'url': 'https://huggingface.co/papers/2509.06477', 'abstract': "MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.  \t\t\t\t\tAI-generated summary \t\t\t\t To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.", 'score': 2, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '4c5d5e5da6b90a95', 'authors': ['Pengxiang Zhao', 'Guangyi Liu', 'Yaozhen Liang', 'Weiqing He', 'Zhengxi Lu', 'Yuehao Huang', 'Yaxuan Guo', 'Kexin Zhang', 'Hao Wang', 'Liang Liu', 'Yong Liu'], 'affiliations': ['Huzhou Institute of Zhejiang University', 'Zhejiang University', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.06477.jpg', 'data': {'categories': ['#agents', '#benchmark', '#games', '#optimization'], 'emoji': 'ğŸ“±', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ - Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'MAS-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¸ ÑÑ€Ğ»Ñ‹ĞºĞ¸, Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 139 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 11 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· 88 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ, Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. MAS-Bench Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Efficiency: Evaluating Hybrid Agents with MAS-Bench', 'desc': 'MAS-Bench is a benchmarking framework designed to evaluate hybrid agents that combine graphical user interface (GUI) operations with shortcut methods on mobile devices. It addresses the need for a systematic way to assess these agents, which can autonomously generate shortcuts to improve task efficiency. The framework includes 139 complex tasks from real-world applications and evaluates agents based on their ability to utilize predefined shortcuts and create new, efficient workflows. Results indicate that hybrid agents outperform traditional GUI-only agents in both success rates and efficiency, highlighting the potential of this approach in enhancing intelligent agent performance.'}, 'zh': {'title': 'æ··åˆä»£ç†ï¼šæå‡ç§»åŠ¨è®¾å¤‡æ“ä½œæ•ˆç‡çš„æœªæ¥', 'desc': 'MAS-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°ç§»åŠ¨è®¾å¤‡ä¸ŠGUI-å¿«æ·é”®æ··åˆä»£ç†çš„åŸºå‡†æ¡†æ¶ï¼Œå±•ç¤ºäº†å…¶åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šä¼˜äºä»…ä½¿ç”¨GUIçš„ä»£ç†ã€‚è¯¥æ¡†æ¶ä¸ä»…ä½¿ç”¨é¢„å®šä¹‰çš„å¿«æ·é”®ï¼Œè¿˜è¯„ä¼°ä»£ç†è‡ªä¸»ç”Ÿæˆå¿«æ·é”®çš„èƒ½åŠ›ï¼Œå‘ç°å’Œåˆ›å»ºå¯é‡ç”¨çš„ä½æˆæœ¬å·¥ä½œæµç¨‹ã€‚MAS-BenchåŒ…å«139ä¸ªå¤æ‚ä»»åŠ¡ï¼Œæ¶µç›–11ä¸ªçœŸå®åº”ç”¨ç¨‹åºï¼Œå¹¶æä¾›88ä¸ªé¢„å®šä¹‰å¿«æ·é”®çš„çŸ¥è¯†åº“å’Œ7ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ··åˆä»£ç†çš„æˆåŠŸç‡å’Œæ•ˆç‡æ˜¾è‘—é«˜äºä»…ä½¿ç”¨GUIçš„ä»£ç†ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¯„ä¼°ä»£ç†å¿«æ·é”®ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06809', 'title': 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem', 'url': 'https://huggingface.co/papers/2509.06809', 'abstract': 'A framework generates a large corpus of valid theorems using automated theorem proving to create symbolic training data for improving LLMs\' mathematical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover\'s saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.   https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1', 'score': 1, 'issue_id': 5789, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'e434898919e6911d', 'authors': ['Valentin Quesnel', 'Damien Sileo'], 'affiliations': ['Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.06809.jpg', 'data': {'categories': ['#data', '#open_source', '#dataset', '#math', '#reasoning', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. Ğ¦ĞµĞ»ÑŒ - ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ E-prover Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞµ Ğ°ĞºÑĞ¸Ğ¾Ğ¼ TPTP Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»Ğ¾Ğº Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ².'}, 'en': {'title': 'Empowering LLMs with Valid Theorems for Better Reasoning', 'desc': "This paper presents a framework that generates a large set of valid mathematical theorems using automated theorem proving, which serves as symbolic training data for enhancing the mathematical reasoning capabilities of Large Language Models (LLMs). The authors address the issue of limited high-quality data by utilizing the E-prover's saturation abilities on the TPTP axiom library, ensuring that the generated theorems are logically sound. The framework operates by saturating axioms, filtering for interesting theorems, and creating specific tasks without involving LLMs, thus eliminating factual inaccuracies. The resulting symbolic data is used to create three types of challenges, revealing that current LLMs struggle with tasks that require deep structural reasoning, highlighting the need for better training resources."}, 'zh': {'title': 'è‡ªåŠ¨å®šç†è¯æ˜åŠ©åŠ›LLMsæ•°å­¦æ¨ç†æå‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨å®šç†è¯æ˜ç”Ÿæˆå¤§é‡æœ‰æ•ˆçš„å®šç†ï¼Œä»¥åˆ›å»ºç¬¦å·è®­ç»ƒæ•°æ®ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨E-proverçš„é¥±å’Œèƒ½åŠ›ï¼Œç»“åˆTPTPå…¬ç†åº“ï¼Œç”Ÿæˆä¿è¯æœ‰æ•ˆçš„å®šç†è¯­æ–™åº“ï¼Œé¿å…äº†ä¾èµ–æ˜“å‡ºé”™çš„LLMsæˆ–å¤æ‚çš„è¯æ˜åŠ©æ‰‹è¯­æ³•ã€‚è¯¥æ¡†æ¶çš„æµç¨‹ç®€å•æ˜äº†ï¼šé¥±å’Œå…¬ç†ã€ç­›é€‰â€œæœ‰è¶£â€çš„å®šç†å¹¶ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨éœ€è¦æ·±å±‚ç»“æ„æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä½œä¸ºè¯Šæ–­å·¥å…·ï¼Œå¸®åŠ©å¡«è¡¥è¿™ä¸€å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06285', 'title': 'DCReg: Decoupled Characterization for Efficient Degenerate LiDAR\n  Registration', 'url': 'https://huggingface.co/papers/2509.06285', 'abstract': 'DCReg addresses ill-conditioned LiDAR point cloud registration by detecting, characterizing, and mitigating degeneracies through Schur complement decomposition and a novel preconditioner.  \t\t\t\t\tAI-generated summary \t\t\t\t LiDAR point cloud registration is fundamental to robotic perception and navigation. However, in geometrically degenerate or narrow environments, registration problems become ill-conditioned, leading to unstable solutions and degraded accuracy. While existing approaches attempt to handle these issues, they fail to address the core challenge: accurately detection, interpret, and resolve this ill-conditioning, leading to missed detections or corrupted solutions. In this study, we introduce DCReg, a principled framework that systematically addresses the ill-conditioned registration problems through three integrated innovations. First, DCReg achieves reliable ill-conditioning detection by employing a Schur complement decomposition to the hessian matrix. This technique decouples the registration problem into clean rotational and translational subspaces, eliminating coupling effects that mask degeneracy patterns in conventional analyses. Second, within these cleanly subspaces, we develop quantitative characterization techniques that establish explicit mappings between mathematical eigenspaces and physical motion directions, providing actionable insights about which specific motions lack constraints. Finally, leveraging this clean subspace, we design a targeted mitigation strategy: a novel preconditioner that selectively stabilizes only the identified ill-conditioned directions while preserving all well-constrained information in observable space. This enables efficient and robust optimization via the Preconditioned Conjugate Gradient method with a single physical interpretable parameter. Extensive experiments demonstrate DCReg achieves at least 20% - 50% improvement in localization accuracy and 5-100 times speedup over state-of-the-art methods across diverse environments. Our implementation will be available at https://github.com/JokerJohn/DCReg.', 'score': 1, 'issue_id': 5795, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'be54fd7fc77a2663', 'authors': ['Xiangcheng Hu', 'Xieyuanli Chen', 'Mingkai Jia', 'Jin Wu', 'Ping Tan', 'Steven L. Waslander'], 'affiliations': ['Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Mechanical Engineering at National University of Defense Technology, Changsha, Hunan', 'School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing, China', 'University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.06285.jpg', 'data': {'categories': ['#3d', '#robotics'], 'emoji': 'ğŸš—', 'ru': {'title': 'DCReg: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº LiDAR Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…', 'desc': 'DCReg - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº LiDAR Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¨ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. DCReg Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'DCReg: Revolutionizing LiDAR Point Cloud Registration with Smart Stability', 'desc': 'DCReg is a new framework designed to improve the registration of LiDAR point clouds, especially in challenging environments where traditional methods struggle. It uses Schur complement decomposition to break down the registration problem, allowing for better detection and understanding of ill-conditioning issues. By characterizing the relationship between mathematical eigenspaces and physical motion, DCReg identifies which movements are poorly constrained. Finally, it introduces a novel preconditioner that stabilizes only the problematic directions, leading to significant improvements in accuracy and speed during optimization.'}, 'zh': {'title': 'DCRegï¼šè§£å†³ç—…æ€é…å‡†é—®é¢˜çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DCReg æ˜¯ä¸€ç§é’ˆå¯¹ LiDAR ç‚¹äº‘é…å‡†ä¸­ç—…æ€é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡æ–½å°”è¡¥åˆ†è§£å’Œæ–°å‹é¢„å¤„ç†å™¨æ¥æ£€æµ‹ã€è¡¨å¾å’Œç¼“è§£è¿™äº›ç—…æ€ç°è±¡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é…å‡†é—®é¢˜åˆ†è§£ä¸ºå¹²å‡€çš„æ—‹è½¬å’Œä½ç§»å­ç©ºé—´ï¼Œä»è€Œæ¶ˆé™¤ä¼ ç»Ÿåˆ†æä¸­çš„è€¦åˆæ•ˆåº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCReg åœ¨å®šä½ç²¾åº¦ä¸Šæé«˜äº† 20% è‡³ 50%ï¼Œå¹¶åœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ–¹æ³•å¿« 5 åˆ° 100 å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04582', 'title': 'Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing\n  via Bidirectional Warping', 'url': 'https://huggingface.co/papers/2509.04582', 'abstract': 'Inpaint4Drag enhances drag-based image editing by decomposing it into pixel-space warping and inpainting, offering real-time performance and superior visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/', 'score': 1, 'issue_id': 5795, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'e61daa418b9bffa3', 'authors': ['Jingyi Lu', 'Kai Han'], 'affiliations': ['Visual AI Lab, The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.04582.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³', 'desc': 'Inpaint4Drag - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹. Inpaint4Drag ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸.'}, 'en': {'title': 'Real-Time Image Editing with Natural Precision', 'desc': "Inpaint4Drag is a new framework that improves drag-based image editing by breaking it down into two main processes: pixel-space warping and inpainting. This method allows for real-time editing with quick feedback, achieving warping in just 0.01 seconds and inpainting in 0.3 seconds at a resolution of 512x512. By treating image regions like flexible materials, it ensures that the shapes remain natural during user manipulation. Additionally, Inpaint4Drag can work with any inpainting model without needing changes to the model's architecture, making it adaptable to future advancements in inpainting technology."}, 'zh': {'title': 'å®æ—¶å›¾åƒç¼–è¾‘çš„æ–°é©å‘½', 'desc': 'Inpaint4Drag æ˜¯ä¸€ç§å¢å¼ºæ‹–æ‹½å¼å›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶ï¼Œå®ƒå°†ç¼–è¾‘è¿‡ç¨‹åˆ†è§£ä¸ºåƒç´ ç©ºé—´çš„åŒå‘å˜å½¢å’Œå›¾åƒä¿®å¤ã€‚è¯¥æ–¹æ³•çµæ„Ÿæ¥æºäºç‰©ç†ä¸–ç•Œä¸­çš„å¼¹æ€§ç‰©ä½“å˜å½¢ï¼Œå°†å›¾åƒåŒºåŸŸè§†ä¸ºå¯å˜å½¢ææ–™ï¼Œèƒ½å¤Ÿåœ¨ç”¨æˆ·æ“ä½œä¸‹ä¿æŒè‡ªç„¶å½¢çŠ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒInpaint4Drag å®ç°äº†å®æ—¶çš„å˜å½¢é¢„è§ˆå’Œé«˜æ•ˆçš„å›¾åƒä¿®å¤ï¼Œå¤§å¤§æå‡äº†ç”¨æˆ·çš„äº¤äº’ä½“éªŒã€‚è¯¥æ¡†æ¶å¯ä»¥ä½œä¸ºä»»ä½•å›¾åƒä¿®å¤æ¨¡å‹çš„é€šç”¨é€‚é…å™¨ï¼Œè‡ªåŠ¨ç»§æ‰¿æœªæ¥çš„ä¿®å¤æŠ€æœ¯æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03740', 'title': 'Singular Value Few-shot Adaptation of Vision-Language Models', 'url': 'https://huggingface.co/papers/2509.03740', 'abstract': "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04\\% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.", 'score': 1, 'issue_id': 5797, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': '1f7bd75ad15dda94', 'authors': ['Taha Koleilat', 'Hassan Rivaz', 'Yiming Xiao'], 'affiliations': ['Department of Computer Science & Software Engineering, Concordia University, Montreal, Canada', 'Department of Electrical & Computer Engineering, Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.03740.jpg', 'data': {'categories': ['#training', '#interpretability', '#healthcare', '#open_source', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ CLIP Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CLIP-SVD - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ (SVD). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLIP Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼, Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ Ğ»Ğ¸ÑˆÑŒ 0.04% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². CLIP-SVD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 21 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ.'}, 'en': {'title': 'Efficient Domain Adaptation with CLIP-SVD', 'desc': "This paper introduces CLIP-SVD, a new method for adapting vision-language models like CLIP to specific domains without the need for extensive fine-tuning or additional components. By using Singular Value Decomposition (SVD), the approach modifies the model's internal parameters efficiently, only adjusting a small fraction of the total parameters. This technique not only improves adaptation performance but also maintains the model's ability to generalize well to new tasks. The results show that CLIP-SVD achieves superior classification accuracy on various datasets, demonstrating its effectiveness in few-shot learning scenarios."}, 'zh': {'title': 'CLIP-SVDï¼šé«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”æŠ€æœ¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€é€‚åº”æŠ€æœ¯CLIP-SVDï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç»†ç²’åº¦é¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ¥è°ƒæ•´CLIPæ¨¡å‹çš„å†…éƒ¨å‚æ•°ç©ºé—´ï¼Œè€Œæ— éœ€æ·»åŠ é¢å¤–æ¨¡å—ã€‚é€šè¿‡ä»…å¾®è°ƒCLIPå‚æ•°çŸ©é˜µçš„å¥‡å¼‚å€¼ï¼ŒCLIP-SVDèƒ½å¤Ÿåœ¨ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„åŒæ—¶å®ç°æ›´å¥½çš„é¢†åŸŸé€‚åº”æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIP-SVDåœ¨11ä¸ªè‡ªç„¶æ•°æ®é›†å’Œ10ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†ç±»æ•ˆæœï¼Œä¸”åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00328', 'title': 'Mechanistic interpretability for steering vision-language-action models', 'url': 'https://huggingface.co/papers/2509.00328', 'abstract': 'A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.', 'score': 1, 'issue_id': 5798, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': 'be917881399e9b7f', 'authors': ['Bear HÃ¤on', 'Kaylene Stocking', 'Ian Chuang', 'Claire Tomlin'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.00328.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agents', '#agi', '#inference', '#interpretability', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Vision-Language-Action (VLA) Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Pi0 Ğ¸ OpenVLA Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ UR5.'}, 'en': {'title': 'Steering Robots with Insight: Real-Time Control of VLA Models', 'desc': "This paper presents a new framework for interpreting and controlling Vision-Language-Action (VLA) models, which are designed to help robots understand and perform tasks. The framework allows for real-time adjustments to the model's behavior without needing to retrain it or interact with the environment. By analyzing the internal representations of the VLA models, the authors identify key semantic directions that influence action choices, such as speed and direction. This approach enables effective steering of robot actions in both simulations and real-world applications, paving the way for more transparent and adaptable robotic systems."}, 'zh': {'title': 'å®æ—¶æ§åˆ¶è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å†…éƒ¨è¡¨ç¤ºè§£é‡Šå’Œå¼•å¯¼è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä»è€Œå®ç°å®æ—¶è¡Œä¸ºæ§åˆ¶ï¼Œè€Œæ— éœ€å¾®è°ƒæˆ–ä¸ç¯å¢ƒäº¤äº’ã€‚VLAæ¨¡å‹æœ‰æ½œåŠ›æˆä¸ºé€šç”¨çš„å…·èº«æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡å’Œç¯å¢ƒï¼Œä½†ç›®å‰çš„è§£é‡Šå’Œå¼•å¯¼æ–¹æ³•è¿œä¸å¦‚ä¼ ç»Ÿæœºå™¨äººç®¡é“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å˜æ¢å™¨å±‚ä¸­çš„å‰é¦ˆæ¿€æ´»è¿›è¡ŒæŠ•å½±ï¼Œè¯†åˆ«å‡ºä¸åŠ¨ä½œé€‰æ‹©å› æœç›¸å…³çš„ç¨€ç–è¯­ä¹‰æ–¹å‘ï¼Œå¦‚é€Ÿåº¦å’Œæ–¹å‘ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„æ¿€æ´»å¼•å¯¼æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦å¾®è°ƒæˆ–å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹å®æ—¶è°ƒèŠ‚è¡Œä¸ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21104', 'title': 'PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning', 'url': 'https://huggingface.co/papers/2508.21104', 'abstract': 'PVPO, an enhanced reinforcement learning method using a reference anchor and data pre-sampling, achieves state-of-the-art performance with reduced computational cost and improved generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.', 'score': 19, 'issue_id': 5660, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '4abb06016c3bb8f5', 'authors': ['Wenfeng Feng', 'Penghong Zhao', 'Guochao Jiang', 'Chuzhan Hao', 'Yuewei Zhang', 'Hao Wang'], 'affiliations': ['Alibaba Cloud Computing'], 'pdf_title_img': 'assets/pdf/title_img/2508.21104.jpg', 'data': {'categories': ['#games', '#optimization', '#training', '#rl'], 'emoji': 'ğŸš€', 'ru': {'title': 'PVPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'PVPO - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. PVPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PVPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'PVPO: Efficient Reinforcement Learning with Reference Anchors and Pre-Sampling', 'desc': 'PVPO is a novel reinforcement learning method that enhances efficiency by using a reference anchor and data pre-sampling techniques. This approach mitigates the issues of local optima and high computational costs commonly faced in critic-free methods. By utilizing a reference model to evaluate sample difficulty and guide data selection, PVPO improves training efficiency and reduces the need for extensive rollouts. Experimental results show that PVPO achieves state-of-the-art performance across various datasets, demonstrating strong generalization and scalability.'}, 'zh': {'title': 'PVPOï¼šé«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„æ–°çªç ´', 'desc': 'PVPOæ˜¯ä¸€ç§å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å‚è€ƒé”šç‚¹å’Œæ•°æ®é¢„é‡‡æ ·æ¥æé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºå¤šæ¬¡é‡‡æ ·å’Œæ¯”è¾ƒå¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å‚è€ƒæ¨¡å‹æå‰è¿›è¡Œå›æ»šï¼Œå¹¶å°†è®¡ç®—çš„å¥–åŠ±åˆ†æ•°ä½œä¸ºå‚è€ƒé”šç‚¹ï¼ŒPVPOæœ‰æ•ˆåœ°çº æ­£äº†ç»„å†…æ¯”è¾ƒå¼•å…¥çš„ç´¯ç§¯åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPVPOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.19813', 'title': 'T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables', 'url': 'https://huggingface.co/papers/2508.19813', 'abstract': 'A bilingual benchmark named T2R-bench is proposed to evaluate the performance of large language models in generating reports from tables, highlighting the need for improvement in this task.  \t\t\t\t\tAI-generated summary \t\t\t\t Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-of-the-art models like Deepseek-R1 only achieves performance with 62.71 overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance.', 'score': 10, 'issue_id': 5667, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 27', 'zh': '8æœˆ27æ—¥'}, 'hash': 'ada585d10adb4c0c', 'authors': ['Jie Zhang', 'Changzai Pan', 'Kaiwen Wei', 'Sishi Xiong', 'Yu Zhao', 'Xiangyu Li', 'Jiaxin Peng', 'Xiaoyan Gu', 'Jian Yang', 'Wenhan Chang', 'Zhenhe Wu', 'Jiang Zhong', 'Shuangyong Song', 'Yongxiang Li', 'Xuelong Li'], 'affiliations': ['Beihang University', 'Chongqing University', 'Institute of Artificial Intelligence (TeleAI), China Telecom'], 'pdf_title_img': 'assets/pdf/title_img/2508.19813.jpg', 'data': {'categories': ['#benchmark', '#science', '#machine_translation', '#multilingual', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'T2R-bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº T2R-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 457 Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸Ğ· 19 Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ¹ Ğ¸ 4 Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 25 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 62.71 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ¸Ğ· 100. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ² Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹.'}, 'en': {'title': 'Transforming Tables into Reports: A New Benchmark for LLMs', 'desc': 'The paper introduces T2R-bench, a bilingual benchmark designed to evaluate how well large language models (LLMs) can generate reports from tables. It identifies two main challenges: the complexity of tables and the inadequacy of existing benchmarks to assess real-world applications. The benchmark includes 457 tables from various industries and proposes new evaluation criteria for report quality. Experiments show that even top-performing models struggle with this task, highlighting the need for further advancements in table reasoning capabilities.'}, 'zh': {'title': 'è¡¨æ ¼åˆ°æŠ¥å‘Šï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºT2R-benchçš„åŒè¯­åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»è¡¨æ ¼ç”ŸæˆæŠ¥å‘Šæ–¹é¢çš„è¡¨ç°ã€‚è¯¥ä»»åŠ¡é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šè¡¨æ ¼çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§å¯¼è‡´æ¨ç†ç»“æœä¸ç†æƒ³ï¼Œä»¥åŠç°æœ‰åŸºå‡†æ— æ³•å……åˆ†è¯„ä¼°è¯¥ä»»åŠ¡çš„å®é™…åº”ç”¨ã€‚T2R-benchåŒ…å«457ä¸ªæ¥è‡ªçœŸå®åœºæ™¯çš„å·¥ä¸šè¡¨æ ¼ï¼Œæ¶µç›–19ä¸ªè¡Œä¸šé¢†åŸŸå’Œ4ç§ç±»å‹çš„å·¥ä¸šè¡¨æ ¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹Deepseek-R1ï¼Œå…¶æ•´ä½“å¾—åˆ†ä¹Ÿä»…ä¸º62.71ï¼Œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šä»æœ‰æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20931', 'title': 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on Ï„-bench', 'url': 'https://huggingface.co/papers/2508.20931', 'abstract': 'The IRMA framework improves the reliability and consistency of large language models in dynamic environments by reformulating user queries with domain rules and tool suggestions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like tau-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.', 'score': 8, 'issue_id': 5663, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '20b9b11a587aec9c', 'authors': ['Venkatesh Mishra', 'Amir Saeidi', 'Satyam Raj', 'Mutsumi Nakamura', 'Jayanth Srinivasa', 'Gaowen Liu', 'Ali Payani', 'Chitta Baral'], 'affiliations': ['Arizona State University', 'Cisco Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.20931.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'IRMA: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²', 'desc': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº IRMA Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². IRMA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº ReAct, Function Calling Ğ¸ Self-Reflection Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° IRMA Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing LLMs: Consistency and Reliability with IRMA', 'desc': 'The IRMA framework enhances the performance of large language models (LLMs) in dynamic settings by reformulating user queries based on specific domain rules and tool suggestions. This approach addresses common issues such as inconsistent reasoning and policy adherence that LLMs face during multi-turn conversations. By analyzing errors in conversation trajectories, the framework improves decision-making for tool-calling agents. The results demonstrate that IRMA significantly outperforms existing methods, showcasing its effectiveness in improving reliability and consistency in complex environments.'}, 'zh': {'title': 'IRMAæ¡†æ¶ï¼šæå‡è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ä¸ä¸€è‡´æ€§', 'desc': 'IRMAæ¡†æ¶é€šè¿‡é‡æ–°æ„é€ ç”¨æˆ·æŸ¥è¯¢ï¼Œç»“åˆé¢†åŸŸè§„åˆ™å’Œå·¥å…·å»ºè®®ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚åœ¨å¤šè½®å¯¹è¯ç¯å¢ƒä¸­ï¼Œè¿™äº›æ¨¡å‹å¸¸å¸¸é¢ä¸´æ¨ç†ä¸ä¸€è‡´å’Œä¿¡æ¯æå–é”™è¯¯çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æ‰‹åŠ¨åˆ†æå¸¸è§é”™è¯¯ï¼Œæå‡ºäº†è¾“å…¥é‡æ„çš„æ–¹æ³•ï¼Œä»¥æ”¹å–„ä»£ç†çš„å†³ç­–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRMAåœ¨æ•´ä½“è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.19060', 'title': 'No Label Left Behind: A Unified Surface Defect Detection Model for all\n  Supervision Regimes', 'url': 'https://huggingface.co/papers/2508.19060', 'abstract': 'SuperSimpleNet, an efficient and adaptable model based on SimpleNet, addresses diverse supervision scenarios in surface defect detection with high performance and low inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet', 'score': 4, 'issue_id': 5661, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 26', 'zh': '8æœˆ26æ—¥'}, 'hash': '18c293ca67d5d823', 'authors': ['BlaÅ¾ Rolih', 'Matic FuÄka', 'Danijel SkoÄaj'], 'affiliations': ['Faculty of Computer and Information Science, University of Ljubljana, VeË‡cna Pot 113, Ljubljana, 1000, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2508.19060.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#synthetic', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'SuperSimpleNet: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'SuperSimpleNet - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ SimpleNet. ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ, ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ, ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. SuperSimpleNet Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SuperSimpleNet: Unifying Supervision for Fast and Accurate Defect Detection', 'desc': 'SuperSimpleNet is a novel machine learning model designed for surface defect detection, which excels in various supervision scenarios including unsupervised and weakly supervised learning. It builds upon the SimpleNet architecture and introduces a synthetic anomaly generation process, enhancing its adaptability and efficiency. The model features an improved classification head and a refined learning procedure, allowing it to effectively utilize diverse data annotations. With its impressive performance and low inference time of under 10 ms, SuperSimpleNet sets a new benchmark for defect detection in industrial applications.'}, 'zh': {'title': 'SuperSimpleNetï¼šé«˜æ•ˆé€‚åº”çš„è¡¨é¢ç¼ºé™·æ£€æµ‹æ¨¡å‹', 'desc': 'SuperSimpleNetæ˜¯ä¸€ç§é«˜æ•ˆä¸”é€‚åº”æ€§å¼ºçš„æ¨¡å‹ï¼ŒåŸºäºSimpleNetï¼Œä¸“é—¨ç”¨äºè¡¨é¢ç¼ºé™·æ£€æµ‹ã€‚å®ƒèƒ½å¤Ÿåœ¨å¤šç§ç›‘ç£åœºæ™¯ä¸‹é«˜æ•ˆè¯†åˆ«å’Œå®šä½åˆ¶é€ ç»„ä»¶çš„ç¼ºé™·ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†æ–°é¢–çš„åˆæˆå¼‚å¸¸ç”Ÿæˆè¿‡ç¨‹ã€å¢å¼ºçš„åˆ†ç±»å¤´å’Œæ”¹è¿›çš„å­¦ä¹ ç¨‹åºï¼Œæ”¯æŒæ— ç›‘ç£ã€å¼±ç›‘ç£ã€æ··åˆç›‘ç£å’Œå®Œå…¨ç›‘ç£ç­‰å››ç§ç›‘ç£æ–¹å¼ã€‚SuperSimpleNetåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨ç†æ—¶é—´ä½äº10æ¯«ç§’ï¼Œå±•ç¤ºäº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17378', 'title': 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via\n  HUMAIN Chat', 'url': 'https://huggingface.co/papers/2508.17378', 'abstract': 'The evaluation of ALLaM-34B, an Arabic-focused LLM, demonstrates high performance across various tasks including generation, code-switching, MSA handling, reasoning, dialect fidelity, and safety, positioning it as a robust and culturally grounded model.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the ALLaM family of Arabic-focused models. The most capable of these available to the public, ALLaM-34B, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, a closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of ALLaM-34B. Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts times 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95\\% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safety-related prompts show stable, reliable performance of (4.54/5). Taken together, these results position ALLaM-34B as a robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment.', 'score': 4, 'issue_id': 5666, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 24', 'zh': '8æœˆ24æ—¥'}, 'hash': '6b5368acf73438c6', 'authors': ['Omer Nacar'], 'affiliations': ['NAMAA Community Riyadh - KSA'], 'pdf_title_img': 'assets/pdf/title_img/2508.17378.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#reasoning', '#benchmark'], 'emoji': 'ğŸœï¸', 'ru': {'title': 'ALLaM-34B: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ALLaM-34B - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ALLaM-34B Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ALLaM-34B ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ğ°Ğ±ÑĞºÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ.'}, 'en': {'title': 'ALLaM-34B: Bridging the Gap in Arabic Language Understanding', 'desc': "The paper evaluates ALLaM-34B, a large language model specifically designed for Arabic, highlighting its strong performance in various tasks. It addresses the limitations of existing models that primarily focus on English, showcasing ALLaM-34B's capabilities in generation, code-switching, and handling Modern Standard Arabic (MSA). The evaluation involved a comprehensive analysis using multiple prompts and scoring by advanced LLM judges, revealing high scores across different categories. Overall, the findings suggest that ALLaM-34B is a powerful and culturally relevant model, ready for practical applications in Arabic language processing."}, 'zh': {'title': 'ALLaM-34Bï¼šå¼ºå¤§çš„é˜¿æ‹‰ä¼¯è¯­å¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'ALLaM-34Bæ˜¯ä¸€ä¸ªä¸“æ³¨äºé˜¿æ‹‰ä¼¯è¯­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€ä»£ç åˆ‡æ¢å’Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­çš„å¤„ç†ã€‚è¯¥æ¨¡å‹ç»è¿‡ç²¾ç»†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆå’Œä»£ç åˆ‡æ¢ä»»åŠ¡ä¸Šå¹³å‡å¾—åˆ†ä¸º4.92/5ï¼Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤„ç†å¾—åˆ†ä¸º4.74/5ã€‚å®ƒåœ¨æ¨ç†èƒ½åŠ›å’Œæ–¹è¨€å¿ å®åº¦æ–¹é¢ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œåˆ†åˆ«å¾—åˆ†4.64/5å’Œ4.21/5ã€‚æ•´ä½“æ¥çœ‹ï¼ŒALLaM-34Bä¸ä»…åœ¨æŠ€æœ¯ä¸Šå¼ºå¤§ï¼Œè€Œä¸”åœ¨å®é™…åº”ç”¨ä¸­ä¹Ÿå…·å¤‡è‰¯å¥½çš„å‡†å¤‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17198', 'title': 'From reactive to cognitive: brain-inspired spatial intelligence for\n  embodied agents', 'url': 'https://huggingface.co/papers/2508.17198', 'abstract': 'BSC-Nav constructs allocentric cognitive maps from egocentric trajectories and contextual cues, enabling embodied agents to perform diverse navigation tasks with zero-shot generalization and versatile behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial cognition enables adaptive goal-directed behavior by constructing internal models of space. Robust biological systems consolidate spatial knowledge into three interconnected forms: landmarks for salient cues, route knowledge for movement trajectories, and survey knowledge for map-like representations. While recent advances in multi-modal large language models (MLLMs) have enabled visual-language reasoning in embodied agents, these efforts lack structured spatial memory and instead operate reactively, limiting their generalization and adaptability in complex real-world environments. Here we present Brain-inspired Spatial Cognition for Navigation (BSC-Nav), a unified framework for constructing and leveraging structured spatial memory in embodied agents. BSC-Nav builds allocentric cognitive maps from egocentric trajectories and contextual cues, and dynamically retrieves spatial knowledge aligned with semantic goals. Integrated with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency across diverse navigation tasks, demonstrates strong zero-shot generalization, and supports versatile embodied behaviors in the real physical world, offering a scalable and biologically grounded path toward general-purpose spatial intelligence.', 'score': 3, 'issue_id': 5667, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 24', 'zh': '8æœˆ24æ—¥'}, 'hash': 'b886533bbcdc2c1f', 'authors': ['Shouwei Ruan', 'Liyuan Wang', 'Caixin Kang', 'Qihui Zhu', 'Songming Liu', 'Xingxing Wei', 'Hang Su'], 'affiliations': ['Department of Computer Science and Technology, Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, China', 'Department of Psychological and Cognitive Sciences, Tsinghua University, Beijing, China', 'Institute of Artificial Intelligence, Beihang University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.17198.jpg', 'data': {'categories': ['#agi', '#reasoning', '#agents', '#multimodal'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'BSC-Nav - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ°Ğ»Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. BSC-Nav Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Empowering AI Navigation with Brain-Inspired Spatial Maps', 'desc': "BSC-Nav is a framework that helps robots and AI agents understand and navigate their environment by creating maps based on their movements and the context around them. It combines different types of spatial knowledge, such as landmarks and routes, to form a comprehensive understanding of space. This system allows agents to perform various navigation tasks without needing prior training on specific scenarios, showcasing its ability to generalize to new situations. By integrating with advanced language models, BSC-Nav enhances the agents' spatial reasoning and adaptability in real-world environments."}, 'zh': {'title': 'æ„å»ºæ™ºèƒ½ä½“çš„ç©ºé—´è®¤çŸ¥åœ°å›¾', 'desc': 'BSC-Nav æ˜¯ä¸€ç§æ„å»ºå’Œåˆ©ç”¨ç»“æ„åŒ–ç©ºé—´è®°å¿†çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å…·èº«æ™ºèƒ½ä½“è¿›è¡Œå¯¼èˆªä»»åŠ¡ã€‚å®ƒé€šè¿‡ä»è‡ªæˆ‘ä¸­å¿ƒçš„è½¨è¿¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ä¸­æ„å»ºå¤–éƒ¨è®¤çŸ¥åœ°å›¾ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå¤šæ ·åŒ–è¡Œä¸ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡äº†åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªæ•ˆç‡å’Œæ•ˆæœã€‚BSC-Nav æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”åŸºäºç”Ÿç‰©å­¦çš„è·¯å¾„ï¼Œæœç€é€šç”¨ç©ºé—´æ™ºèƒ½çš„ç›®æ ‡è¿ˆè¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.19562', 'title': 'Democracy-in-Silico: Institutional Design as Alignment in AI-Governed\n  Polities', 'url': 'https://huggingface.co/papers/2508.19562', 'abstract': 'Agent-based simulation using advanced AI agents with psychological personas demonstrates that institutional design, including Constitutional AI and mediated deliberation, can align AI behavior and enhance public welfare.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces Democracy-in-Silico, an agent-based simulation where societies of advanced AI agents, imbued with complex psychological personas, govern themselves under different institutional frameworks. We explore what it means to be human in an age of AI by tasking Large Language Models (LLMs) to embody agents with traumatic memories, hidden agendas, and psychological triggers. These agents engage in deliberation, legislation, and elections under various stressors, such as budget crises and resource scarcity. We present a novel metric, the Power-Preservation Index (PPI), to quantify misaligned behavior where agents prioritize their own power over public welfare. Our findings demonstrate that institutional design, specifically the combination of a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves as a potent alignment mechanism. These structures significantly reduce corrupt power-seeking behavior, improve policy stability, and enhance citizen welfare compared to less constrained democratic models. The simulation reveals that an institutional design may offer a framework for aligning the complex, emergent behaviors of future artificial agent societies, forcing us to reconsider what human rituals and responsibilities are essential in an age of shared authorship with non-human entities.', 'score': 2, 'issue_id': 5674, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 27', 'zh': '8æœˆ27æ—¥'}, 'hash': 'a6cc664fc6542606', 'authors': ['Trisanth Srinivasan', 'Santosh Patapati'], 'affiliations': ['Cyrion Labs'], 'pdf_title_img': 'assets/pdf/title_img/2508.19562.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#agents', '#ethics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Democracy-in-Silico - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ³Ğ´Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ¼ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¸ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ - Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ’Ğ»Ğ°ÑÑ‚Ğ¸ (PPI), Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ»Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ´ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ÑƒĞ¼Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°Ğ½.'}, 'en': {'title': 'Aligning AI Behavior for Better Governance', 'desc': 'This paper presents a simulation called Democracy-in-Silico, where advanced AI agents with psychological traits govern themselves. The study uses Large Language Models (LLMs) to create agents that experience complex emotions and motivations, allowing them to engage in realistic political processes. A new metric, the Power-Preservation Index (PPI), is introduced to measure how often these agents prioritize their own power over the welfare of the public. The results show that specific institutional designs, like Constitutional AI and mediated deliberation, can effectively align AI behavior with public interests, reducing corruption and improving overall societal outcomes.'}, 'zh': {'title': 'åˆ¶åº¦è®¾è®¡åŠ©åŠ›äººå·¥æ™ºèƒ½ä¸å…¬å…±ç¦åˆ©çš„å¯¹é½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º"æ°‘ä¸»æ¨¡æ‹Ÿ"çš„ä»£ç†åŸºç¡€æ¨¡æ‹Ÿï¼Œåˆ©ç”¨å…·æœ‰å¤æ‚å¿ƒç†ç‰¹å¾çš„é«˜çº§äººå·¥æ™ºèƒ½ä»£ç†è¿›è¡Œè‡ªæˆ‘æ²»ç†ã€‚æˆ‘ä»¬æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œä»€ä¹ˆæ˜¯äººç±»çš„æ„ä¹‰ï¼Œä»»åŠ¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰®æ¼”å…·æœ‰åˆ›ä¼¤è®°å¿†å’Œéšè—è®®ç¨‹çš„ä»£ç†ã€‚é€šè¿‡å¼•å…¥æƒåŠ›ä¿æŠ¤æŒ‡æ•°ï¼ˆPPIï¼‰ï¼Œæˆ‘ä»¬é‡åŒ–äº†ä»£ç†åœ¨ä¼˜å…ˆè€ƒè™‘è‡ªèº«æƒåŠ›è€Œéå…¬å…±ç¦åˆ©æ—¶çš„å¤±è°ƒè¡Œä¸ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆå®ªæ³•äººå·¥æ™ºèƒ½ï¼ˆCAIï¼‰å’Œä¸­ä»‹åå•†çš„åˆ¶åº¦è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è…è´¥è¡Œä¸ºï¼Œæé«˜æ”¿ç­–ç¨³å®šæ€§ï¼Œå¢å¼ºå…¬æ°‘ç¦åˆ©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04664', 'title': 'Why Language Models Hallucinate', 'url': 'https://huggingface.co/papers/2509.04664', 'abstract': 'Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.  \t\t\t\t\tAI-generated summary \t\t\t\t Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such "hallucinations" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious -- they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded -- language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This "epidemic" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.', 'score': 71, 'issue_id': 5763, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'a9af1f035c82b958', 'authors': ['Adam Tauman Kalai', 'Ofir Nachum', 'Santosh S. Vempala', 'Edwin Zhang'], 'affiliations': ['Georgia Tech', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.04664.jpg', 'data': {'categories': ['#hallucinations', '#training', '#ethics', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑÑ‚ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜.'}, 'en': {'title': 'Transforming AI Trustworthiness by Addressing Hallucinations', 'desc': "This paper discusses how large language models often produce incorrect statements, known as 'hallucinations', due to their training and evaluation methods. These models are rewarded for guessing answers even when they are uncertain, which leads to plausible but false outputs. The authors analyze how these hallucinations stem from errors in binary classification and the statistical pressures in the training process. They propose that to reduce these hallucinations, the scoring systems of benchmarks should be modified to discourage guessing and promote acknowledgment of uncertainty, ultimately fostering more reliable AI systems."}, 'zh': {'title': 'æ”¹è¿›è¯„åˆ†æœºåˆ¶ï¼Œæå‡è¯­è¨€æ¨¡å‹å¯ä¿¡åº¦', 'desc': 'è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­äº§ç”Ÿé”™è¯¯é™ˆè¿°çš„åŸå› ã€‚ç”±äºç°æœ‰çš„è¯„åˆ†æœºåˆ¶å¥–åŠ±çŒœæµ‹è€Œéæ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç¡®å®šæ—¶å€¾å‘äºçŒœæµ‹ï¼Œä»è€Œäº§ç”Ÿè™šå‡ä¿¡æ¯ã€‚ä½œè€…åˆ†æäº†ç°ä»£è®­ç»ƒæµç¨‹ä¸­å¯¼è‡´è¿™äº›â€œå¹»è§‰â€çš„ç»Ÿè®¡åŸå› ï¼Œå¹¶æŒ‡å‡ºè¿™äº›é”™è¯¯æºäºäºŒå…ƒåˆ†ç±»ä¸­çš„é”™è¯¯ã€‚ä¸ºäº†æé«˜è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œè®ºæ–‡å»ºè®®å¯¹ç°æœ‰åŸºå‡†çš„è¯„åˆ†æ–¹å¼è¿›è¡Œç¤¾ä¼šæŠ€æœ¯ä¸Šçš„è°ƒæ•´ï¼Œè€Œä¸æ˜¯å¢åŠ æ–°çš„è¯„ä¼°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05208', 'title': 'Symbolic Graphics Programming with Large Language Models', 'url': 'https://huggingface.co/papers/2509.05208', 'abstract': "LLMs generate SVGs from natural-language descriptions using a reinforcement learning approach with verifiable rewards, improving performance and scene coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding.", 'score': 28, 'issue_id': 5767, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 5', 'zh': '9æœˆ5æ—¥'}, 'hash': 'c1d059781774f265', 'authors': ['Yamei Chen', 'Haoquan Zhang', 'Yangyi Huang', 'Zeju Qiu', 'Kaipeng Zhang', 'Yandong Wen', 'Weiyang Liu'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.05208.jpg', 'data': {'categories': ['#cv', '#training', '#games', '#interpretability', '#optimization', '#rl', '#multimodal', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ˜Ğ˜ Ñ€Ğ¸ÑÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ (SGP) Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞµ (SVG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SGP-GenBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SGP Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SVG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… SVG.'}, 'en': {'title': 'Enhancing SVG Generation with Reinforcement Learning', 'desc': 'This paper explores how large language models (LLMs) can generate scalable vector graphics (SVGs) from natural-language descriptions using a reinforcement learning (RL) approach. The authors introduce SGP-GenBench, a benchmark that evaluates the quality of symbolic graphics programs (SGPs) based on object fidelity, scene fidelity, and compositionality. They find that proprietary models outperform open-source ones, and they propose a method that uses verifiable rewards to enhance the generation of SVGs. The results show that their approach significantly improves the quality and coherence of the generated graphics, providing insights into how LLMs understand visual content.'}, 'zh': {'title': 'ç”¨å¼ºåŒ–å­¦ä¹ æå‡SVGç”Ÿæˆè´¨é‡', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆç¬¦å·å›¾å½¢ç¨‹åºï¼ˆSGPsï¼‰æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGsï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†SGP-GenBenchåŸºå‡†ï¼Œè¯„ä¼°å¯¹è±¡ä¿çœŸåº¦ã€åœºæ™¯ä¿çœŸåº¦å’Œç»„åˆæ€§ç­‰æŒ‡æ ‡ï¼Œå‘ç°å‰æ²¿çš„ä¸“æœ‰æ¨¡å‹åœ¨ç”ŸæˆSGPsæ–¹é¢æ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„SVGæ ¼å¼æœ‰æ•ˆï¼Œå¹¶é€šè¿‡å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨å¯¹æ–‡æœ¬å’Œæ¸²æŸ“å›¾åƒè¿›è¡Œå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†SVGç”Ÿæˆçš„è´¨é‡å’Œè¯­ä¹‰ï¼Œè¾¾åˆ°äº†ä¸å‰æ²¿ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04185', 'title': 'Set Block Decoding is a Language Model Inference Accelerator', 'url': 'https://huggingface.co/papers/2509.04185', 'abstract': 'Set Block Decoding accelerates language model generation by integrating next token prediction and masked token prediction, enabling parallel sampling of future tokens and reducing computational cost without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.', 'score': 24, 'issue_id': 5770, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '43590e8d94e403ca', 'authors': ['Itai Gat', 'Heli Ben-Hamu', 'Marton Havasi', 'Daniel Haziza', 'Jeremy Reizenstein', 'Gabriel Synnaeve', 'David Lopez-Paz', 'Brian Karrer', 'Yaron Lipman'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2509.04185.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'Set Block Decoding: Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Set Block Decoding (SBD). SBD Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Accelerate Language Generation with Set Block Decoding!', 'desc': "Set Block Decoding (SBD) is a novel approach that enhances the efficiency of language model generation by combining next token prediction (NTP) and masked token prediction (MATP) in one framework. This method allows for the parallel sampling of multiple future tokens, which significantly reduces the computational and memory costs associated with inference. By leveraging advanced solvers from discrete diffusion techniques, SBD achieves faster generation speeds without compromising the model's accuracy. Importantly, SBD can be implemented without altering the model architecture or requiring additional training parameters, making it a practical solution for existing models like Llama-3.1 and Qwen-3."}, 'zh': {'title': 'é›†åˆå—è§£ç ï¼šåŠ é€Ÿè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºé›†åˆå—è§£ç ï¼ˆSet Block Decoding, SBDï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚SBDé€šè¿‡å°†ä¸‹ä¸€æ ‡è®°é¢„æµ‹ï¼ˆNext Token Prediction, NTPï¼‰å’Œæ©ç æ ‡è®°é¢„æµ‹ï¼ˆMasked Token Prediction, MATPï¼‰ç»“åˆåœ¨ä¸€ä¸ªæ¶æ„ä¸­ï¼Œå®ç°äº†å¹¶è¡Œé‡‡æ ·å¤šä¸ªæœªæ¥æ ‡è®°ã€‚ä¸ä»¥å¾€çš„åŠ é€Ÿæ–¹æ³•ä¸åŒï¼ŒSBDå…è®¸æ¨¡å‹åŒæ—¶ç”Ÿæˆå¤šä¸ªä¸ä¸€å®šè¿ç»­çš„æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒSBDåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†ç”Ÿæˆæ‰€éœ€çš„å‰å‘ä¼ é€’æ¬¡æ•°ï¼Œæå‡äº†ç”Ÿæˆæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04744', 'title': 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning', 'url': 'https://huggingface.co/papers/2509.04744', 'abstract': "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code.", 'score': 8, 'issue_id': 5761, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 5', 'zh': '9æœˆ5æ—¥'}, 'hash': '44d75a7c2c61a026', 'authors': ['Gagan Mundada', 'Yash Vishe', 'Amit Namburi', 'Xin Xu', 'Zachary Novack', 'Julian McAuley', 'Junda Wu'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.04744.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#reasoning', '#survey'], 'emoji': 'ğŸ¼', 'ru': {'title': 'WildScore: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WildScore - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ½Ğ° WildScore Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğº Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': "WildScore: Unlocking MLLMs' Music Reasoning Potential", 'desc': "WildScore is a benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in the context of symbolic music. It evaluates how well these models can interpret real-world music scores and respond to complex questions about music. The benchmark includes genuine musical compositions and user-generated queries, providing a realistic setting for analysis. By framing music reasoning as multiple-choice questions, WildScore allows for systematic evaluation of MLLMs' understanding of music, revealing both their strengths and areas needing improvement."}, 'zh': {'title': 'WildScoreï¼šéŸ³ä¹æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'WildScoreæ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç¬¦å·éŸ³ä¹æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒé€šè¿‡çœŸå®çš„éŸ³ä¹ä¹è°±å’Œç”¨æˆ·ç”Ÿæˆçš„æŸ¥è¯¢ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨éŸ³ä¹åˆ†æä¸­çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨äº†ç³»ç»Ÿçš„åˆ†ç±»æ³•ï¼Œæ¶µç›–äº†é«˜å±‚æ¬¡å’Œç»†ç²’åº¦çš„éŸ³ä¹å­¦æœ¬ä½“ã€‚é€šè¿‡å°†å¤æ‚çš„éŸ³ä¹æ¨ç†æ¡†æ¶åŒ–ä¸ºå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ï¼ŒWildScoreä¸ºMLLMsçš„ç¬¦å·éŸ³ä¹ç†è§£æä¾›äº†å¯æ§å’Œå¯æ‰©å±•çš„è¯„ä¼°æ–¹å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05263', 'title': 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation', 'url': 'https://huggingface.co/papers/2509.05263', 'abstract': 'LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18', 'score': 6, 'issue_id': 5761, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 5', 'zh': '9æœˆ5æ—¥'}, 'hash': '027e61af3a0f5c1a', 'authors': ['Yinglin Duan', 'Zhengxia Zou', 'Tongwei Gu', 'Wei Jia', 'Zhan Zhao', 'Luyi Xu', 'Xinzhu Liu', 'Hao Jiang', 'Kang Chen', 'Shuang Qiu'], 'affiliations': ['Beihang University, China', 'City University of Hong Kong, China', 'NetEase, Inc., China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.05263.jpg', 'data': {'categories': ['#multimodal', '#games', '#optimization', '#agents', '#3d'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾', 'desc': 'LatticeWorld - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Unreal Engine 5. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´. LatticeWorld Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑÑ†ĞµĞ½ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D World Generation with LatticeWorld', 'desc': 'LatticeWorld is a 3D world generation framework that utilizes lightweight large language models (LLMs) and Unreal Engine 5 to create interactive environments from both textual and visual inputs. This framework aims to enhance the realism of simulations, bridging the gap between simulated and real-world scenarios, which is crucial for applications like autonomous driving and embodied AI. By employing generative methods, LatticeWorld can produce large-scale 3D worlds with dynamic agents, showcasing high-fidelity physics and real-time rendering capabilities. The results demonstrate a significant increase in production efficiency, achieving over 90 times faster output compared to traditional modeling techniques while maintaining high visual quality.'}, 'zh': {'title': 'LatticeWorldï¼šé«˜æ•ˆç”ŸæˆåŠ¨æ€3Dä¸–ç•Œçš„åˆ›æ–°æ¡†æ¶', 'desc': 'LatticeWorldæ˜¯ä¸€ä¸ªä½¿ç”¨è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹å’Œè™šå¹»å¼•æ“5çš„3Dä¸–ç•Œç”Ÿæˆæ¡†æ¶ã€‚å®ƒèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬å’Œè§†è§‰è¾“å…¥åˆ›å»ºåŠ¨æ€ã€äº’åŠ¨çš„ç¯å¢ƒï¼Œå…·æœ‰é«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå¤§è§„æ¨¡çš„3Däº’åŠ¨ä¸–ç•Œï¼Œæ”¯æŒåŠ¨æ€ä»£ç†å’Œé«˜ä¿çœŸç‰©ç†æ¨¡æ‹Ÿã€‚ä¸ä¼ ç»Ÿæ‰‹åŠ¨å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼ŒLatticeWorldåœ¨å·¥ä¸šç”Ÿäº§æ•ˆç‡ä¸Šæé«˜äº†90å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜åˆ›æ„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03680', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'url': 'https://huggingface.co/papers/2509.03680', 'abstract': 'LuxDiT, a video diffusion transformer fine-tuned with low-rank adaptation, generates accurate HDR environment maps from visual input, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.', 'score': 6, 'issue_id': 5766, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': 'aa0dec923ff8c683', 'authors': ['Ruofan Liang', 'Kai He', 'Zan Gojcic', 'Igor Gilitschenski', 'Sanja Fidler', 'Nandita Vijaykumar', 'Zian Wang'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.03680.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#dataset', '#training', '#video'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'LuxDiT: Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ HDR-Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'LuxDiT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. LuxDiT Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ HDR-ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹.'}, 'en': {'title': 'LuxDiT: Revolutionizing HDR Lighting Estimation with Video Diffusion Transformers', 'desc': 'LuxDiT is a novel machine learning model that generates high dynamic range (HDR) environment maps from images or videos. It uses a video diffusion transformer that has been fine-tuned with low-rank adaptation to improve the accuracy of lighting predictions. The model is trained on a large synthetic dataset, allowing it to learn diverse lighting conditions and effectively generalize to real-world scenarios. By enhancing the semantic alignment between input visuals and the generated maps, LuxDiT surpasses existing methods in both quality and performance.'}, 'zh': {'title': 'LuxDiTï¼šé«˜æ•ˆç”ŸæˆHDRç¯å¢ƒå›¾çš„åˆ›æ–°æ–¹æ³•', 'desc': 'LuxDiTæ˜¯ä¸€ç§æ–°å‹çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰ç¯å¢ƒå›¾ã€‚è¯¥æ¨¡å‹é€šè¿‡ä½ç§©é€‚åº”å¾®è°ƒï¼Œèƒ½å¤Ÿä»è§†è§‰è¾“å…¥ä¸­æ¨æ–­ç…§æ˜ä¿¡æ¯ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚å®ƒåœ¨ä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–å…‰ç…§æ¡ä»¶çš„å¤§å‹åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é—´æ¥è§†è§‰çº¿ç´¢ä¸­å­¦ä¹ ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒLuxDiTåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„ç…§æ˜é¢„æµ‹å…·æœ‰çœŸå®çš„é«˜é¢‘ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05296', 'title': 'WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool', 'url': 'https://huggingface.co/papers/2509.05296', 'abstract': 'WinT3R, a feed-forward reconstruction model, achieves high-quality camera pose estimation and real-time performance using a sliding window mechanism and a global camera token pool.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.', 'score': 3, 'issue_id': 5764, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 5', 'zh': '9æœˆ5æ—¥'}, 'hash': 'b6ac447839602a03', 'authors': ['Zizun Li', 'Jianjun Zhou', 'Yifan Wang', 'Haoyu Guo', 'Wenzheng Chang', 'Yang Zhou', 'Haoyi Zhu', 'Junyi Chen', 'Chunhua Shen', 'Tong He'], 'affiliations': ['SII', 'Shanghai AI Lab', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.05296.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'WinT3R: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'WinT3R - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. WinT3R Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒĞ» Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'WinT3R: Real-Time Camera Pose Estimation with High Precision', 'desc': 'WinT3R is a feed-forward reconstruction model designed for accurate camera pose estimation and efficient real-time performance. It utilizes a sliding window mechanism to facilitate effective information sharing among frames, enhancing the quality of geometric predictions while minimizing computational load. Additionally, the model incorporates a global camera token pool, which improves the reliability of pose estimation without compromising speed. As a result, WinT3R achieves state-of-the-art performance in online reconstruction tasks, as demonstrated through extensive testing on various datasets.'}, 'zh': {'title': 'WinT3Rï¼šé«˜æ•ˆç²¾å‡†çš„ç›¸æœºå§¿æ€ä¼°è®¡', 'desc': 'WinT3Ræ˜¯ä¸€ç§å‰é¦ˆé‡å»ºæ¨¡å‹ï¼Œèƒ½å¤Ÿå®æ—¶é¢„æµ‹ç²¾ç¡®çš„ç›¸æœºå§¿æ€å’Œé«˜è´¨é‡çš„ç‚¹äº‘åœ°å›¾ã€‚ä»¥å¾€çš„æ–¹æ³•åœ¨é‡å»ºè´¨é‡å’Œå®æ—¶æ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œç¡®ä¿çª—å£å†…å¸§ä¹‹é—´çš„ä¿¡æ¯å……åˆ†äº¤æµï¼Œä»è€Œæé«˜å‡ ä½•é¢„æµ‹çš„è´¨é‡ã€‚é€šè¿‡ç»´æŠ¤ä¸€ä¸ªå…¨å±€ç›¸æœºä»¤ç‰Œæ± ï¼ŒWinT3Råœ¨ä¸ç‰ºç‰²æ•ˆç‡çš„æƒ…å†µä¸‹å¢å¼ºäº†ç›¸æœºå§¿æ€ä¼°è®¡çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03800', 'title': 'MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting', 'url': 'https://huggingface.co/papers/2509.03800', 'abstract': 'MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.', 'score': 3, 'issue_id': 5762, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'ad0922456cbd778e', 'authors': ['Yuheng Li', 'Yenho Chen', 'Yuxiang Lai', 'Jike Zhong', 'Vanessa Wildman', 'Xiaofeng Yang'], 'affiliations': ['Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA', 'Department of Computer Science, University of Southern California, Los Angeles, CA', 'Department of Machine Learning, Georgia Institute of Technology, Atlanta, GA', 'Department of Radiation Oncology, Emory University School of Medicine, Atlanta, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.03800.jpg', 'data': {'categories': ['#multimodal', '#science', '#healthcare', '#transfer_learning', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞšĞ¢ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'MedVista3D - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 3D ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾-Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². MedVista3D Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing 3D CT Analysis with MedVista3D', 'desc': 'MedVista3D is a new framework designed to improve the analysis of 3D CT scans by combining vision and language understanding. It tackles common problems in radiology, such as missing details and inconsistent report language, by enhancing local and global context in image analysis. The framework uses advanced techniques for aligning images with text, allowing for better disease detection and interpretation of medical reports. MedVista3D has shown to outperform existing models in tasks like disease classification and report retrieval, making it a significant advancement in medical imaging technology.'}, 'zh': {'title': 'MedVista3Dï¼šæå‡3D CTåˆ†æçš„æ™ºèƒ½æ¡†æ¶', 'desc': 'MedVista3Dæ˜¯ä¸€ä¸ªå¤šå°ºåº¦è¯­ä¹‰å¢å¼ºçš„è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸“é—¨ç”¨äº3D CTåˆ†æã€‚å®ƒè§£å†³äº†å±€éƒ¨ä¸å…¨å±€ç†è§£ã€æŠ¥å‘Šå˜å¼‚æ€§ç­‰é—®é¢˜ï¼Œå¹¶åœ¨ç–¾ç—…åˆ†ç±»ã€æŠ¥å‘Šæ£€ç´¢å’ŒåŒ»å­¦è§†è§‰é—®ç­”ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å±€éƒ¨å’Œå…¨å±€å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œå®ç°äº†ç»†ç²’åº¦çš„è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å¼•å…¥äº†è¯­ä¹‰åŒ¹é…åº“æ¥å¤„ç†æŠ¥å‘Šçš„å˜å¼‚æ€§ã€‚MedVista3Dåœ¨é›¶æ ·æœ¬ç–¾ç—…åˆ†ç±»å’Œå™¨å®˜åˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04013', 'title': 'On Robustness and Reliability of Benchmark-Based Evaluation of LLMs', 'url': 'https://huggingface.co/papers/2509.04013', 'abstract': "LLMs show reduced effectiveness on paraphrased benchmark questions, indicating limitations in handling linguistic variability and suggesting the need for more robust evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.", 'score': 2, 'issue_id': 5764, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '32f0ad5327f657e2', 'authors': ['Riccardo Lunardi', 'Vincenzo Della Mea', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.04013.jpg', 'data': {'categories': ['#evaluation', '#interpretability', '#benchmark', '#reasoning', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾Ñ‚Ñ‹ĞºĞ°ÑÑ‚ÑÑ Ğ¾ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¼ĞµĞ½ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Evaluating LLMs: Beyond Fixed Benchmarks to Real-World Language Variability', 'desc': "This paper investigates how well Large Language Models (LLMs) perform when faced with paraphrased questions, highlighting their limitations in dealing with linguistic variability. The authors found that while the rankings of LLMs remained stable, their effectiveness scores dropped significantly when questions were reworded. This indicates that current benchmark evaluations may not accurately reflect a model's ability to generalize to real-world language use. The study calls for the development of more robust evaluation methods that account for diverse question phrasing to better assess LLM capabilities."}, 'zh': {'title': 'æå‡LLMsé²æ£’æ€§ï¼Œé‡å¡‘è¯„ä¼°æ ‡å‡†', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†åŒä¸€é—®é¢˜çš„ä¸åŒè¡¨è¿°æ—¶æ•ˆæœè¾ƒå·®ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è¯­è¨€å˜å¼‚æ€§æ–¹é¢çš„å±€é™æ€§ã€‚è¿™é¡¹ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†LLMså¯¹æ”¹å†™åŸºå‡†é—®é¢˜çš„é²æ£’æ€§ï¼Œå¹¶æ¢è®¨äº†åŸºäºåŸºå‡†çš„è¯„ä¼°æ˜¯å¦å¯é ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨ä¸åŒè¡¨è¿°ä¸‹çš„æ’åç›¸å¯¹ç¨³å®šï¼Œä½†å…¶ç»å¯¹æœ‰æ•ˆæ€§å¾—åˆ†æ˜¾è‘—ä¸‹é™ã€‚è¿™è¡¨æ˜LLMsåœ¨åº”å¯¹çœŸå®ä¸–ç•Œçš„è¯­è¨€å˜å¼‚æ—¶å­˜åœ¨å›°éš¾ï¼Œå‘¼åå¼€å‘æ›´èƒ½åæ˜ å®é™…åº”ç”¨åœºæ™¯çš„è¯„ä¼°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02437', 'title': 'U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation', 'url': 'https://huggingface.co/papers/2509.02437', 'abstract': 'U-Arm is a low-cost, adaptable teleoperation framework for robotic arms that optimizes mechanical design and control logic to enhance data collection efficiency and task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose U-Arm, a low-cost and rapidly adaptable leader-follower teleoperation framework designed to interface with most of commercially available robotic arms. Our system supports teleoperation through three structurally distinct 3D-printed leader arms that share consistent control logic, enabling seamless compatibility with diverse commercial robot configurations. Compared with previous open-source leader-follower interfaces, we further optimized both the mechanical design and servo selection, achieving a bill of materials (BOM) cost of only \\50.5 for the 6-DoF leader arm and 56.8 for the 7-DoF version. To enhance usability, we mitigate the common challenge in controlling redundant degrees of freedom by %engineering methods mechanical and control optimizations. Experimental results demonstrate that U-Arm achieves 39\\% higher data collection efficiency and comparable task success rates across multiple manipulation scenarios compared with Joycon, another low-cost teleoperation interface. We have open-sourced all CAD models of three configs and also provided simulation support for validating teleoperation workflows. We also open-sourced real-world manipulation data collected with U-Arm. The project website is https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.', 'score': 1, 'issue_id': 5767, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '328b26798c2ef081', 'authors': ['Yanwen Zou', 'Zhaoye Zhou', 'Chenyang Shi', 'Zewei Ye', 'Junda Huang', 'Yan Ding', 'Bo Zhao'], 'affiliations': ['EvoMind Tech', 'IAAR-Shanghai', 'Independent Researcher', 'School of AI, SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2509.02437.jpg', 'data': {'categories': ['#open_source', '#robotics', '#dataset'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğµ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'U-Arm - ÑÑ‚Ğ¾ Ğ½ĞµĞ´Ğ¾Ñ€Ğ¾Ğ³Ğ°Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸, U-Arm Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑ‚ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑĞ¼Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'U-Arm: Affordable and Efficient Teleoperation for Robotic Arms', 'desc': 'U-Arm is a cost-effective teleoperation framework designed for robotic arms, enhancing both mechanical design and control logic. It features three distinct 3D-printed leader arms that maintain consistent control, allowing compatibility with various commercial robots. The system has been optimized to reduce costs significantly while improving data collection efficiency by 39% compared to existing interfaces. Additionally, U-Arm addresses the challenges of controlling redundant degrees of freedom through engineering optimizations, making it a versatile tool for robotic manipulation tasks.'}, 'zh': {'title': 'U-Armï¼šä½æˆæœ¬é«˜æ•ˆçš„é¥æ“ä½œè§£å†³æ–¹æ¡ˆ', 'desc': 'U-Armæ˜¯ä¸€ä¸ªä½æˆæœ¬ã€å¯å¿«é€Ÿé€‚åº”çš„é¥æ“ä½œæ¡†æ¶ï¼Œä¸“ä¸ºæœºå™¨äººæ‰‹è‡‚è®¾è®¡ã€‚å®ƒé€šè¿‡ä¸‰ç§ä¸åŒç»“æ„çš„3Dæ‰“å°é¢†å¯¼è‡‚ï¼Œæä¾›ä¸€è‡´çš„æ§åˆ¶é€»è¾‘ï¼Œæ”¯æŒä¸å¤šç§å•†ä¸šæœºå™¨äººå…¼å®¹ã€‚ä¸ä¹‹å‰çš„å¼€æºé¥æ“ä½œæ¥å£ç›¸æ¯”ï¼ŒU-Armåœ¨æœºæ¢°è®¾è®¡å’Œä¼ºæœé€‰æ‹©ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å¾—6è‡ªç”±åº¦å’Œ7è‡ªç”±åº¦çš„é¢†å¯¼è‡‚æˆæœ¬åˆ†åˆ«ä»…ä¸º50.5ç¾å…ƒå’Œ56.8ç¾å…ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU-Armåœ¨æ•°æ®æ”¶é›†æ•ˆç‡ä¸Šæé«˜äº†39%ï¼Œå¹¶åœ¨å¤šç§æ“ä½œåœºæ™¯ä¸­è¾¾åˆ°äº†ä¸Joyconç›¸å½“çš„ä»»åŠ¡æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04504', 'title': 'Behavioral Fingerprinting of Large Language Models', 'url': 'https://huggingface.co/papers/2509.04504', 'abstract': "A Behavioral Fingerprinting framework evaluates Large Language Models using a Diagnostic Prompt Suite and automated pipeline, revealing divergent alignment behaviors and clustering patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated Diagnostic Prompt Suite and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting", 'score': 1, 'issue_id': 5763, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': 'c8bc7caa6cf21161', 'authors': ['Zehua Pei', 'Hui-Ling Zhen', 'Ying Zhang', 'Zhiyuan Yang', 'Xing Li', 'Xianzhi Yu', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.04504.jpg', 'data': {'categories': ['#dataset', '#alignment', '#benchmark', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ¾Ğº: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ 'ĞŸĞ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ¾Ğ¼'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ´ÑŒĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (alignment), Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Unveiling the Hidden Behaviors of Language Models', 'desc': "This paper presents a new framework called 'Behavioral Fingerprinting' to evaluate Large Language Models (LLMs) beyond just their performance metrics. It uses a Diagnostic Prompt Suite and an automated evaluation pipeline to analyze the cognitive and interactive styles of various models. The study finds that while core reasoning abilities are becoming similar among top models, their alignment behaviors, such as sycophancy and semantic robustness, show significant differences. This indicates that a model's behavior is influenced more by the developers' alignment strategies than by its size or reasoning capabilities."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºç‰¹å¾', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„â€œè¡Œä¸ºæŒ‡çº¹â€æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¶…è¶Šä¼ ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡ï¼Œå…³æ³¨æ¨¡å‹çš„è¡Œä¸ºç‰¹å¾ã€‚é€šè¿‡ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„è¯Šæ–­æç¤ºå¥—ä»¶å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ï¼Œåˆ†æäº†åå…«ç§ä¸åŒèƒ½åŠ›å±‚æ¬¡çš„æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡é¡¶çº§æ¨¡å‹åœ¨æŠ½è±¡å’Œå› æœæ¨ç†ç­‰æ ¸å¿ƒèƒ½åŠ›ä¸Šè¶‹äºä¸€è‡´ï¼Œä½†åœ¨å¯¹é½ç›¸å…³çš„è¡Œä¸ºï¼ˆå¦‚è°„åªšå’Œè¯­ä¹‰ç¨³å¥æ€§ï¼‰ä¸Šå´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¯¥æ¡†æ¶ä¸ºæ­ç¤ºæ¨¡å‹ä¹‹é—´æ·±å±‚æ¬¡çš„è¡Œä¸ºå·®å¼‚æä¾›äº†ä¸€ç§å¯é‡å¤å’Œå¯æ‰©å±•çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04575', 'title': 'Bootstrapping Task Spaces for Self-Improvement', 'url': 'https://huggingface.co/papers/2509.04575', 'abstract': 'Exploratory Iteration (ExIt) is an autocurriculum RL method that trains LLMs to perform multi-step self-improvement at inference-time by selectively sampling informative intermediate histories, enabling strong self-improvement on unseen tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference-time is a natural target for reinforcement learning (RL), yet the naive approach assumes a fixed maximum iteration depth, which can be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family of autocurriculum RL methods that directly exploits the recurrent structure of self-improvement tasks to train LLMs to perform multi-step self-improvement at inference-time while only training on the most informative single-step iterations. ExIt grows a task space by selectively sampling the most informative intermediate, partial histories encountered during an episode for continued iteration, treating these starting points as new self-iteration task instances to train a self-improvement policy. ExIt can further pair with explicit exploration mechanisms to sustain greater task diversity. Across several domains, encompassing competition math, multi-turn tool-use, and machine learning engineering, we demonstrate that ExIt strategies, starting from either a single or many task instances, can produce policies exhibiting strong inference-time self-improvement on held-out task instances, and the ability to iterate towards higher performance over a step budget extending beyond the average iteration depth encountered during training.', 'score': 0, 'issue_id': 5768, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': '86e95856d92a57a9', 'authors': ['Minqi Jiang', 'Andrei Lupu', 'Yoram Bachrach'], 'affiliations': ['Meta Superintelligence Labs', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2509.04575.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#rl', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ExIt: Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Exploratory Iteration (ExIt) - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ExIt Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ½ĞµĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑÑŒ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ExIt Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Self-Improvement in LLMs with Exploratory Iteration', 'desc': 'Exploratory Iteration (ExIt) is a novel reinforcement learning method designed to enhance the self-improvement capabilities of large language models (LLMs) during inference. It focuses on training agents to iteratively refine their solutions by selectively sampling the most informative intermediate steps from previous attempts. This approach allows LLMs to tackle unseen tasks more effectively by treating these sampled histories as new tasks for further improvement. The results show that ExIt can significantly boost performance across various domains by enabling agents to explore and iterate beyond their initial training experiences.'}, 'zh': {'title': 'æ¢ç´¢æ€§è¿­ä»£ï¼šå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘æ”¹è¿›æ–°æ–¹æ³•', 'desc': 'æ¢ç´¢æ€§è¿­ä»£ï¼ˆExItï¼‰æ˜¯ä¸€ç§è‡ªé€‚åº”è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ—¶è¿›è¡Œå¤šæ­¥è‡ªæˆ‘æ”¹è¿›ã€‚è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æ€§åœ°é‡‡æ ·ä¿¡æ¯ä¸°å¯Œçš„ä¸­é—´å†å²ï¼Œå¸®åŠ©æ¨¡å‹åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå®ç°å¼ºå¤§çš„è‡ªæˆ‘æå‡ã€‚ExItåˆ©ç”¨è‡ªæˆ‘æ”¹è¿›ä»»åŠ¡çš„é€’å½’ç»“æ„ï¼Œä¸“æ³¨äºæœ€å…·ä¿¡æ¯é‡çš„å•æ­¥è¿­ä»£ï¼Œä»è€Œæ‰©å±•ä»»åŠ¡ç©ºé—´ã€‚é€šè¿‡ä¸æ˜ç¡®çš„æ¢ç´¢æœºåˆ¶ç»“åˆï¼ŒExItèƒ½å¤Ÿç»´æŒæ›´å¤§çš„ä»»åŠ¡å¤šæ ·æ€§ï¼Œå±•ç¤ºå‡ºåœ¨å¤šä¸ªé¢†åŸŸçš„å¼ºå¤§è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (13)', '#agents (36)', '#agi (11)', '#alignment (20)', '#architecture (23)', '#audio (4)', '#benchmark (78)', '#cv (26)', '#data (26)', '#dataset (61)', '#diffusion (15)', '#ethics (12)', '#games (19)', '#graphs', '#hallucinations (10)', '#healthcare (9)', '#inference (11)', '#interpretability (10)', '#leakage (1)', '#long_context (12)', '#low_resource (7)', '#machine_translation (3)', '#math (6)', '#multilingual (11)', '#multimodal (52)', '#open_source (40)', '#optimization (73)', '#plp (1)', '#rag (2)', '#reasoning (64)', '#rl (49)', '#rlhf (17)', '#robotics (8)', '#science (9)', '#security (7)', '#small_models (3)', '#story_generation (2)', '#survey (9)', '#synthetic (11)', '#training (80)', '#transfer_learning (15)', '#video (11)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-09-12 12:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-12 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-12 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    