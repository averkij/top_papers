
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 55 papers. December 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Декабрь 2024</span> | <span id="title-articles-count">55 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2024-11.html">⬅️ <span id="prev-date">11.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-01.html">➡️ <span id="next-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Декабрь 2024', 'en': 'December 2024', 'zh': '12月2024年'};
        let feedDateNext = {'ru': '01.2025', 'en': '01/2025', 'zh': '1月2025年'};
        let feedDatePrev = {'ru': '11.2024', 'en': '11/2024', 'zh': '11月2024年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.01824', 'title': 'X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models', 'url': 'https://huggingface.co/papers/2412.01824', 'abstract': "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", 'score': 37, 'issue_id': 911, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '6b7c2d6555d8df8d', 'authors': ['Zeyi Sun', 'Ziyang Chu', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuanjun Xiong', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'MThreads AI', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.01824.jpg', 'data': {'categories': ['#agi', '#cv', '#games', '#optimization', '#training', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'X-Prompt: универсальная модель для генерации изображений с контекстным обучением', 'desc': 'Статья представляет X-Prompt - авторегрессивную мультимодальную модель для генерации изображений. Модель использует контекстное обучение, что позволяет ей решать как знакомые, так и новые задачи генерации изображений. X-Prompt эффективно сжимает признаки из контекстных примеров и поддерживает длинные последовательности токенов контекста. Эксперименты подтверждают способность модели решать разнообразные задачи генерации изображений, включая ранее не встречавшиеся.'}, 'en': {'title': 'X-Prompt: Unlocking Image Generation with In-Context Learning', 'desc': 'This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges.'}, 'zh': {'title': 'X-Prompt：提升图像生成的上下文学习能力', 'desc': '本文介绍了一种名为X-Prompt的自回归大规模视觉语言模型，旨在提升图像生成任务的表现。X-Prompt通过利用上下文中的示例，能够在已知和未知的图像生成任务中实现竞争力的性能。该模型采用了专门的设计，能够有效压缩上下文示例中的重要特征，从而支持更长的上下文序列并提高对未知任务的泛化能力。通过统一的训练任务，X-Prompt在文本和图像预测方面表现出色，验证了其在多样化图像生成任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.00131', 'title': 'Open-Sora Plan: Open-Source Large Video Generation Model', 'url': 'https://huggingface.co/papers/2412.00131', 'abstract': 'We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.', 'score': 19, 'issue_id': 911, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': 'fa9b0009de797ca2', 'authors': ['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.00131.jpg', 'data': {'categories': ['#open_source', '#data', '#inference', '#training', '#video'], 'emoji': '🎬', 'ru': {'title': 'Открытая модель для генерации высококачественного видео', 'desc': 'Проект Open-Sora Plan представляет собой открытую модель генерации видео высокого разрешения на основе различных пользовательских входных данных. Модель включает в себя вейвлет-поточный вариационный автоэнкодер, совместный денойзер изображений и видео, а также различные контроллеры условий. Разработаны стратегии для эффективного обучения и вывода, а также предложен многомерный конвейер курирования данных. Проект достиг впечатляющих результатов в генерации видео как в качественных, так и в количественных оценках.'}, 'en': {'title': 'Empowering High-Resolution Video Generation with Open-Sora Plan', 'desc': 'The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field.'}, 'zh': {'title': '开放源代码，生成高质量视频的未来', 'desc': 'Open-Sora计划是一个开源项目，旨在基于用户输入生成高分辨率的长时视频。该项目包含多个组件，如小波流变分自编码器和联合图像-视频去噪器，支持整个视频生成过程。我们还设计了多种高效的训练和推理策略，并提出了多维数据策划管道，以获取高质量数据。通过这些高效的设计，Open-Sora计划在视频生成的定性和定量评估中取得了令人印象深刻的结果。'}}}, {'id': 'https://huggingface.co/papers/2412.00927', 'title': 'VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation', 'url': 'https://huggingface.co/papers/2412.00927', 'abstract': 'Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.', 'score': 17, 'issue_id': 912, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': 'b4065e6e554dea31', 'authors': ['Weiming Ren', 'Huan Yang', 'Jie Min', 'Cong Wei', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.00927.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset', '#data', '#long_context', '#benchmark', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Синтез данных для обучения ИИ пониманию сложных видео', 'desc': 'VISTA - это система для улучшения понимания длинных и высокого разрешения видео большими мультимодальными моделями. Она синтезирует новые видео, комбинируя существующие пространственно и временно, и создает вопросно-ответные пары к ним. На основе VISTA создан датасет VISTA-400K, который позволил улучшить результаты моделей на 3.3% в задачах понимания длинных видео. Также авторы представили первый бенчмарк HRVideoBench для оценки понимания видео высокого разрешения.'}, 'en': {'title': 'Enhancing Video Understanding with VISTA: A Data-Centric Approach', 'desc': "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."}, 'zh': {'title': 'VISTA：提升视频理解的新方法', 'desc': '当前的大型多模态模型在处理长时长或高分辨率视频时面临重大挑战，主要是由于缺乏高质量的数据集。为了解决这个问题，我们提出了VISTA，一个简单而有效的视频时空增强框架，能够从现有的视频-字幕数据集中合成长时长和高分辨率的视频指令对。VISTA通过空间和时间的组合，创建新的合成视频，并生成与这些新合成视频相关的问题-答案对。通过在我们的数据上微调各种视频多模态模型，平均提高了3.3%的性能，进一步验证了我们框架的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.18499', 'title': 'GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation', 'url': 'https://huggingface.co/papers/2411.18499', 'abstract': 'Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.', 'score': 16, 'issue_id': 914, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'bec1bf0079934886', 'authors': ['Pengfei Zhou', 'Xiaopeng Peng', 'Jiajun Song', 'Chuanhao Li', 'Zhaopan Xu', 'Yue Yang', 'Ziyao Guo', 'Hao Zhang', 'Yuqi Lin', 'Yefei He', 'Lirui Zhao', 'Shuo Liu', 'Tianhua Li', 'Yuxuan Xie', 'Xiaojun Chang', 'Yu Qiao', 'Wenqi Shao', 'Kaipeng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.18499.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#games'], 'emoji': '🔀', 'ru': {'title': 'Новый бенчмарк для оценки мультимодальной генерации', 'desc': 'Статья представляет новый набор данных GATE OpenING для оценки мультимодальных языковых моделей в задачах генерации чередующегося текстово-визуального контента. OpenING включает 5400 аннотированных примеров из 56 реальных задач. Авторы также предлагают модель IntJudge для оценки генерации открытого типа, которая превосходит GPT-based оценщиков на 11.34%. Эксперименты показывают, что существующие методы генерации чередующегося контента имеют значительный потенциал для улучшения.'}, 'en': {'title': 'Bridging the Gap in Multimodal Generation with OpenING!', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators.'}, 'zh': {'title': '推动多模态生成的基准与评估', 'desc': '多模态大型语言模型（MLLMs）在视觉理解和生成任务上取得了显著进展。然而，生成交错的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。为了解决这一问题，我们引入了GATE OpenING（OpenING），这是一个包含5400个高质量人类标注实例的综合基准，涵盖56个真实世界任务。我们的研究还提出了IntJudge，一个用于评估开放式多模态生成方法的评估模型，显示出与人类判断的高一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.01819', 'title': 'Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2412.01819', 'abstract': 'This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.', 'score': 15, 'issue_id': 914, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '25cf4512f592aea4', 'authors': ['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.01819.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#video', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Switti: быстрый и эффективный генератор изображений по тексту', 'desc': 'Статья представляет Switti - трансформер для генерации изображений по текстовому описанию. Авторы предлагают архитектурные модификации для улучшения сходимости и производительности авторегрессионных моделей. На основе наблюдения о слабой зависимости карт внимания от предыдущих масштабов, разработана неавторегрессионная версия, обеспечивающая ускорение и снижение потребления памяти. Исследование также показывает, что отключение guidance на высоких разрешениях дополнительно ускоряет генерацию и улучшает детализацию.'}, 'en': {'title': 'Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers', 'desc': 'This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods.'}, 'zh': {'title': 'Switti：加速文本到图像生成的变换器', 'desc': '本文介绍了Switti，一种用于文本到图像生成的尺度变换器。我们从现有的下一尺度预测自回归模型出发，探索其在T2I生成中的应用，并提出架构修改以提高收敛性和整体性能。研究发现，我们的预训练尺度自回归模型的自注意力图对前一尺度的依赖性较弱，因此我们提出了一种非自回归的替代方案，能够实现约11%的采样加速和更低的内存使用，同时生成质量略有提升。此外，我们发现高分辨率尺度下的无分类器引导通常是不必要的，甚至可能会降低性能。'}}}, {'id': 'https://huggingface.co/papers/2412.00154', 'title': 'o1-Coder: an o1 Replication for Coding', 'url': 'https://huggingface.co/papers/2412.00154', 'abstract': "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .", 'score': 14, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '4a9b72e0b9a6ba64', 'authors': ['Yuxiang Zhang', 'Shangxi Wu', 'Yuqi Yang', 'Jiangming Shu', 'Jinlin Xiao', 'Chao Kong', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.00154.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'O1-CODER: Усиление ИИ для программирования через Систему-2 мышления', 'desc': 'Технический отчет представляет O1-CODER - попытку воспроизвести модель o1 от OpenAI для задач программирования. Модель интегрирует обучение с подкреплением и метод Монте-Карло для улучшения способностей мышления Системы-2. Фреймворк включает обучение генератора тестовых случаев, использование MCTS для генерации кода с процессами рассуждения, и итеративную доводку модели политики. Отчет также рассматривает возможности и проблемы развертывания подобных o1 моделей в реальных приложениях.'}, 'en': {'title': 'Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking', 'desc': "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."}, 'zh': {'title': 'O1-CODER：提升编码任务的智能模型', 'desc': '本文介绍了O1-CODER，这是一个旨在复制OpenAI的o1模型，专注于编码任务的技术报告。该模型结合了强化学习（RL）和蒙特卡洛树搜索（MCTS），以增强其系统2思维能力。框架中包括训练测试用例生成器（TCG）以进行标准化代码测试，利用MCTS生成带有推理过程的代码数据，并迭代微调策略模型，初步生成伪代码，随后生成完整代码。报告还讨论了在实际应用中部署类似o1模型的机遇和挑战，建议转向系统2范式，并强调环境状态更新的重要性。'}}}, {'id': 'https://huggingface.co/papers/2411.18671', 'title': 'TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video', 'url': 'https://huggingface.co/papers/2411.18671', 'abstract': 'In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.', 'score': 13, 'issue_id': 909, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'f99e1015e9222dc6', 'authors': ['Jinyuan Qu', 'Hongyang Li', 'Shilong Liu', 'Tianhe Ren', 'Zhaoyang Zeng', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18671.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#video', '#cv', '#optimization'], 'emoji': '👁️', 'ru': {'title': 'Контекстное внимание для надежного отслеживания точек в видео', 'desc': 'TAPTRv3 - это улучшенная версия TAPTRv2 для более надежного отслеживания точек в длинных видео. Система использует пространственный и временной контекст для повышения качества запроса признаков. Введены два новых механизма: Context-aware Cross-Attention (CCA) для улучшения пространственного запроса и Visibility-aware Long-Temporal Attention (VLTA) для временного запроса. TAPTRv3 значительно превосходит предыдущую версию и достигает наилучших результатов на сложных наборах данных.'}, 'en': {'title': 'Enhancing Video Point Tracking with TAPTRv3', 'desc': 'TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets.'}, 'zh': {'title': 'TAPTRv3：长视频点跟踪的新突破', 'desc': '本文介绍了TAPTRv3，这是在TAPTRv2基础上开发的，旨在提高长视频中的点跟踪鲁棒性。TAPTRv2是一个简单的类似DETR的框架，可以准确跟踪现实视频中的任意点，而无需成本体积。TAPTRv3通过利用空间和时间上下文来改善特征查询，从而在长视频中实现更稳健的跟踪。我们提出了上下文感知交叉注意力（CCA）和可见性感知长时间注意力（VLTA），显著提升了特征查询的质量，超越了TAPTRv2，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.00100', 'title': 'Steering Rectified Flow Models in the Vector Field for Controlled Image Generation', 'url': 'https://huggingface.co/papers/2412.00100', 'abstract': 'Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.', 'score': 12, 'issue_id': 911, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '5fceae277a0e3d85', 'authors': ['Maitreya Patel', 'Song Wen', 'Dimitris N. Metaxas', 'Yezhou Yang'], 'affiliations': ['Arizona State University', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00100.jpg', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#diffusion', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'FlowChef: Управление векторным полем для эффективной генерации изображений', 'desc': 'Статья представляет новый подход к управляемой генерации изображений под названием FlowChef. Он основан на использовании динамики векторного поля в ректифицированных потоковых моделях (RFM) для эффективного управления траекторией шумоподавления. FlowChef позволяет решать задачи управляемой генерации изображений, линейных обратных задач и редактирования изображений без дополнительного обучения или инверсии. Результаты показывают, что FlowChef значительно превосходит существующие методы по производительности, требованиям к памяти и времени вычислений.'}, 'en': {'title': 'FlowChef: Revolutionizing Controlled Image Generation with Efficiency', 'desc': 'This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field.'}, 'zh': {'title': 'FlowChef：高效的受控图像生成新方法', 'desc': '扩散模型在生成真实感图像、图像编辑和解决逆问题方面表现出色，但修正流模型在这些任务中仍未得到充分探索。现有的基于扩散模型的方法通常需要额外的训练，缺乏对预训练潜在模型的泛化能力，并且在性能上表现不佳，计算资源消耗大。本文提出FlowChef，通过有效引导去噪轨迹，利用向量场的特性，实现了受控图像生成任务，且无需额外训练或复杂的反向传播。实验结果表明，FlowChef在性能、内存和时间需求上显著优于基线方法，达到了新的最先进结果。'}}}, {'id': 'https://huggingface.co/papers/2412.01199', 'title': 'TinyFusion: Diffusion Transformers Learned Shallow', 'url': 'https://huggingface.co/papers/2412.01199', 'abstract': 'Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.', 'score': 11, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '7d1de7010c3fabd7', 'authors': ['Gongfan Fang', 'Kunjun Li', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.01199.jpg', 'data': {'categories': ['#inference', '#diffusion', '#training', '#optimization', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'TinyFusion: Эффективная обрезка диффузионных трансформеров без потери качества', 'desc': 'TinyFusion - это метод обрезки глубины для уменьшения количества параметров в диффузионных трансформерах. Он использует дифференцируемую технику сэмплирования для обучаемой обрезки и оптимизирует производительность модели после дообучения. TinyFusion превосходит существующие методы обрезки и хорошо обобщается на различные архитектуры. Эксперименты показывают, что метод позволяет создать компактный диффузионный трансформер с двукратным ускорением при сохранении высокого качества генерации изображений.'}, 'en': {'title': 'TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers', 'desc': 'This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics.'}, 'zh': {'title': 'TinyFusion：高效剪枝，提升扩散变换器性能', 'desc': '本论文提出了一种名为TinyFusion的深度剪枝方法，旨在减少扩散变换器中的冗余层，从而降低推理开销。我们的方法通过端到端学习实现剪枝，并确保剪枝后的模型在微调后能够恢复强大的性能。TinyFusion引入了一种可微分采样技术，使得剪枝过程可学习，并与共同优化的参数结合，以模拟未来的微调效果。实验结果表明，TinyFusion在扩散变换器的层剪枝方面优于现有的方法，展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2412.00568', 'title': 'The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning', 'url': 'https://huggingface.co/papers/2412.00568', 'abstract': 'Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.', 'score': 10, 'issue_id': 923, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': '1352e219e54ac56e', 'authors': ['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno Régaldo-Saint Blancard', 'François Rozet', 'Liam H. Parker', 'Miles Cranmer', 'Shirley Ho'], 'affiliations': ['CEA DAM', 'Cornell University', 'Flatiron Institute', 'Los Alamos National Laboratory', 'New York University', 'Polymathic AI', 'Princeton University', 'Rutgers University', 'University of California, Berkeley', 'University of Cambridge', 'University of Colorado, Boulder', 'University of Liège', 'University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2412.00568.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'Well: масштабный бенчмарк для суррогатных моделей в физическом моделировании', 'desc': "Статья представляет новый набор данных под названием 'Well' для оценки суррогатных моделей машинного обучения в области численного моделирования. 'Well' содержит 15 ТБ данных из 16 наборов, охватывающих различные области физики, включая биологические системы, гидродинамику и астрофизику. Авторы предоставляют унифицированный интерфейс PyTorch для обучения и оценки моделей на этих данных. Набор данных призван помочь исследователям в разработке более эффективных методов машинного обучения для ускорения рабочих процессов, основанных на симуляциях."}, 'en': {'title': 'Unlocking Complex Simulations with the Well: A New Dataset for Machine Learning', 'desc': 'This paper presents a large-scale collection of datasets called the Well, designed to enhance machine learning surrogate models for simulation-based workflows. The Well contains 15TB of data across 16 diverse datasets, covering various physical systems such as fluid dynamics and biological systems. By providing a unified PyTorch interface, the Well facilitates the training and evaluation of machine learning models on these complex datasets. The authors also introduce example baselines to demonstrate the unique challenges posed by the intricate dynamics represented in the Well.'}, 'zh': {'title': '机器学习加速仿真：探索"Well"数据集', 'desc': '本文介绍了一种基于机器学习的替代模型，旨在加速基于仿真的工作流程。我们提出了一个名为"Well"的大规模数据集，包含多种时空物理系统的数值仿真数据，总计15TB，涵盖生物系统、流体动力学、声散射等多个领域。该数据集为研究人员提供了丰富的资源，以评估新方法的有效性，并可单独使用或作为更广泛基准套件的一部分。为了方便使用，我们提供了统一的PyTorch接口，帮助训练和评估模型，并展示了新的挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.00174', 'title': 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters', 'url': 'https://huggingface.co/papers/2412.00174', 'abstract': "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.", 'score': 10, 'issue_id': 913, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '5a30d9c535229ab0', 'authors': ['Jianping Jiang', 'Weiye Xiao', 'Zhengyu Lin', 'Huaizhong Zhang', 'Tianxiang Ren', 'Yang Gao', 'Zhiqian Lin', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.00174.jpg', 'data': {'categories': ['#synthetic', '#games', '#dataset', '#agents', '#3d', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Создание социально умных 3D персонажей для виртуальной реальности', 'desc': 'В статье представлен SOLAMI - первый сквозной фреймворк для социального моделирования зрения-языка-действия (VLA) для иммерсивного взаимодействия с 3D автономными персонажами. Фреймворк включает в себя унифицированную архитектуру социального VLA для генерации мультимодальных ответов на основе пользовательского ввода. Для решения проблемы нехватки данных авторы создали синтетический набор данных SynMSI, используя существующие наборы данных о движении. Также разработан VR-интерфейс для иммерсивного взаимодействия пользователей с персонажами.'}, 'en': {'title': 'Empowering 3D Characters with Social Intelligence through SOLAMI', 'desc': 'This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency.'}, 'zh': {'title': '赋予3D角色社交智能的创新框架', 'desc': '本文介绍了SOLAMI，这是第一个端到端的社会视觉-语言-动作（VLA）建模框架，旨在与3D自主角色进行沉浸式互动。SOLAMI从三个方面构建3D自主角色：首先，提出了统一的社会VLA架构，根据用户的多模态输入生成多模态响应（语音和动作），以驱动角色进行社交互动。其次，介绍了SynMSI，这是一个合成的多模态社交互动数据集，通过自动化流程生成，解决了数据稀缺的问题。最后，开发了一个虚拟现实接口，使用户能够与这些角色进行沉浸式互动，实验结果表明，该框架能够提供更精确和自然的角色响应。'}}}, {'id': 'https://huggingface.co/papers/2412.01822', 'title': 'VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models', 'url': 'https://huggingface.co/papers/2412.01822', 'abstract': 'The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate "verbalizers" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs\' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.', 'score': 9, 'issue_id': 916, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '0c3ac9e6ce3f4cd2', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yu-Chiang Frank Wang', 'Yong Man Ro', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01822.jpg', 'data': {'categories': ['#dataset', '#small_models', '#architecture', '#transfer_learning', '#optimization', '#training', '#benchmark', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Эффективное визуально-языковое обучение через послойную вербализацию', 'desc': "Исследователи представили новое семейство моделей визуально-языкового обучения под названием VLsI, которое фокусируется на эффективности без ущерба для точности. VLsI использует уникальный процесс послойной дистилляции, вводя промежуточные 'вербализаторы', которые отображают признаки из каждого слоя в пространство естественного языка. Этот подход позволяет меньшим моделям гибко согласовываться с процессами рассуждений более крупных моделей. Авторы продемонстрировали значительное улучшение производительности VLsI по сравнению с GPT-4V на десяти сложных визуально-языковых тестах без необходимости увеличения размера модели или изменения архитектуры."}, 'en': {'title': 'Efficient Vision-Language Models with Layer-wise Distillation', 'desc': "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."}, 'zh': {'title': '高效视觉语言模型的创新之路', 'desc': '本文提出了一种新的视觉语言模型（VLM）家族，称为VLsI，旨在提高模型的效率而不牺牲准确性。VLsI采用了一种独特的层级蒸馏过程，通过引入中间的“语言化器”，将每一层的特征映射到自然语言空间，使得较小的VLM能够灵活地与较大VLM的推理过程对齐。该方法有效缓解了输出模仿中常见的训练不稳定性，并通过对齐小型VLM的层级进展与大型VLM的层级进展，超越了典型的最终层调优。我们在十个具有挑战性的视觉语言基准上验证了VLsI，取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2412.01064', 'title': 'FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2412.01064', 'abstract': 'With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '1978e3ecf42cac0c', 'authors': ['Taekyung Ki', 'Dongchan Min', 'Gyoungsu Chae'], 'affiliations': ['DeepBrain AI Inc.', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.01064.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal', '#video', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Генерация реалистичных видео с говорящим портретом на основе аудио и потоковых моделей', 'desc': 'Статья представляет FLOAT - метод генерации видео с говорящим портретом на основе аудио, используя генеративную модель сопоставления потоков. Авторы переносят генеративное моделирование из пиксельного латентного пространства в пространство движения, что позволяет эффективно создавать согласованные во времени движения. Метод включает предиктор векторного поля на основе трансформера с покадровым механизмом обусловливания. FLOAT также поддерживает усиление эмоций на основе речи, позволяя естественно добавлять выразительные движения.'}, 'en': {'title': 'FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency', 'desc': 'This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity.'}, 'zh': {'title': 'FLOAT：高效音频驱动的人像视频生成', 'desc': '本论文提出了一种名为FLOAT的音频驱动人像视频生成方法，基于流匹配生成模型。我们将生成建模从基于像素的潜在空间转移到学习的运动潜在空间，从而实现时间一致的运动设计。该方法引入了基于变换器的向量场预测器，并采用简单有效的逐帧条件机制。实验结果表明，我们的方法在视觉质量、运动保真度和效率方面优于现有的音频驱动人像生成方法。'}}}, {'id': 'https://huggingface.co/papers/2411.18933', 'title': 'Efficient Track Anything', 'url': 'https://huggingface.co/papers/2411.18933', 'abstract': 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '86ff7f63f69fa104', 'authors': ['Yunyang Xiong', 'Chong Zhou', 'Xiaoyu Xiang', 'Lemeng Wu', 'Chenchen Zhu', 'Zechun Liu', 'Saksham Suri', 'Balakrishnan Varadarajan', 'Ramya Akula', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Bilge Soran', 'Vikas Chandra'], 'affiliations': ['Meta AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18933.jpg', 'data': {'categories': ['#video', '#small_models', '#benchmark', '#optimization', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная сегментация видео на мобильных устройствах', 'desc': 'EfficientTAMs - это облегченные модели для сегментации и отслеживания объектов в видео. Они основаны на использовании простого Vision Transformer (ViT) в качестве энкодера изображений и эффективного модуля памяти. EfficientTAMs показывают результаты, сравнимые с SAM 2, но работают в 2 раза быстрее и имеют в 2,4 раза меньше параметров. На мобильных устройствах, таких как iPhone 15 Pro Max, EfficientTAMs могут выполнять сегментацию объектов в видео со скоростью около 10 кадров в секунду.'}, 'en': {'title': 'EfficientTAMs: Lightweight Video Segmentation for Mobile Devices', 'desc': 'The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms.'}, 'zh': {'title': '高效视频物体分割，轻量化模型新选择', 'desc': 'Segment Anything Model 2（SAM 2）是一种强大的视频物体分割和跟踪工具。为了提高性能，SAM 2使用了多阶段图像编码器和记忆机制，但其计算复杂性限制了在移动设备上的应用。为了解决这个问题，我们提出了高效的跟踪模型EfficientTAMs，它使用轻量级的视觉变换器（ViT）作为图像编码器，并引入高效的记忆模块，从而降低了计算复杂性。我们的EfficientTAMs在多个视频分割基准测试中表现出色，能够在移动设备上以合理的质量进行视频物体分割。'}}}, {'id': 'https://huggingface.co/papers/2411.17459', 'title': 'WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model', 'url': 'https://huggingface.co/papers/2411.17459', 'abstract': 'Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.', 'score': 7, 'issue_id': 909, 'pub_date': '2024-11-26', 'pub_date_card': {'ru': '26 ноября', 'en': 'November 26', 'zh': '11月26日'}, 'hash': '9b68162718865b81', 'authors': ['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan'], 'affiliations': ['Peking University', 'Peng Cheng Laboratory', 'Rabbitpre Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2411.17459.jpg', 'data': {'categories': ['#diffusion', '#data', '#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': '🌊', 'ru': {'title': 'Эффективное кодирование видео с помощью вейвлетов', 'desc': 'Статья представляет новый метод кодирования видео под названием Wavelet Flow VAE (WF-VAE). Этот автоэнкодер использует многоуровневое вейвлет-преобразование для эффективного кодирования низкочастотной информации в видео. WF-VAE решает проблему вычислительных ограничений при обработке видео высокого разрешения и большой длительности в латентных видео-диффузионных моделях (LVDM). Метод также включает технику Causal Cache для сохранения целостности латентного пространства при поблочной обработке.'}, 'en': {'title': 'Efficient Video Encoding with Wavelet Flow VAE', 'desc': 'The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs.'}, 'zh': {'title': '小波流VAE：高效视频编码的新方法', 'desc': '视频变分自编码器（VAE）将视频编码为低维潜在空间，是大多数潜在视频扩散模型（LVDMs）的关键组成部分，能够降低模型训练成本。然而，随着生成视频的分辨率和时长增加，视频VAE的编码成本成为训练LVDMs的瓶颈。此外，大多数LVDMs采用的块状推理方法在处理长时长视频时可能导致潜在空间的不连续性。为了解决计算瓶颈，我们提出了小波流VAE（WF-VAE），通过多级小波变换有效编码视频的关键信息，并引入因果缓存方法以保持潜在空间的完整性。'}}}, {'id': 'https://huggingface.co/papers/2412.01316', 'title': 'Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation', 'url': 'https://huggingface.co/papers/2412.01316', 'abstract': 'We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.', 'score': 6, 'issue_id': 910, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '5bca597a08ec0a14', 'authors': ['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang'], 'affiliations': ['01.AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.01316.jpg', 'data': {'categories': ['#video', '#dataset', '#architecture', '#diffusion', '#long_context'], 'emoji': '🎬', 'ru': {'title': 'Presto: Революция в генерации длинных видео с помощью ИИ', 'desc': 'Presto - это новая модель диффузии видео, разработанная для генерации 15-секундных видеороликов с долгосрочной согласованностью и богатым содержанием. Модель использует стратегию Segmented Cross-Attention (SCA), которая разделяет скрытые состояния на сегменты вдоль временного измерения, позволяя каждому сегменту перекрестно обращаться к соответствующей подписи. Для обучения модели был создан датасет LongTake-HD, содержащий 261 тысячу видео с богатым содержанием и согласованностью сценариев. Эксперименты показывают, что Presto превосходит существующие методы генерации видео по показателям семантической оценки и степени динамичности.'}, 'en': {'title': 'Presto: Revolutionizing Video Generation with Long-Range Coherence', 'desc': 'Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation.'}, 'zh': {'title': 'Presto：生成长时间一致性视频的新方法', 'desc': '我们介绍了一种新的视频扩散模型Presto，旨在生成具有长时间一致性和丰富内容的15秒视频。为了解决在长时间内保持场景多样性的挑战，我们提出了一种分段交叉注意力(SCA)策略，该策略将隐藏状态沿时间维度分段，使每个段能够与相应的子标题进行交叉关注。SCA不需要额外的参数，可以无缝集成到现有的基于DiT的架构中。我们的实验表明，Presto在视频生成方面显著优于现有的最先进方法，提升了内容丰富性和长距离一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.01800', 'title': 'PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos', 'url': 'https://huggingface.co/papers/2412.01800', 'abstract': 'Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.', 'score': 4, 'issue_id': 917, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '43cc035146493599', 'authors': ['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peking University', 'Sun Yat-sen University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.01800.jpg', 'data': {'categories': ['#dataset', '#video', '#open_source', '#optimization', '#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': '🎮', 'ru': {'title': 'PhysVLM: Обучение видеоязыковых моделей физическому здравому смыслу через игровые глитчи', 'desc': 'Исследователи представили новый бенчмарк PhysGame для оценки понимания физических закономерностей в видеоязыковых моделях (Video LLM) на основе геймплейных видео с глитчами. Они создали набор данных PhysInstruct для обучения моделей физическому здравому смыслу и набор PhysDPO для оптимизации предпочтений. На базе этих наборов данных была разработана модель PhysVLM, показавшая лучшие результаты как на специализированном бенчмарке PhysGame, так и на общих тестах понимания видео.'}, 'en': {'title': 'Enhancing Video LLMs with Physical Commonsense Understanding', 'desc': 'This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models.'}, 'zh': {'title': '提升视频模型的物理常识理解能力', 'desc': '本文介绍了一种新颖的基准测试PhysGame，用于评估视频大语言模型在游戏视频中对物理常识的理解能力。游戏视频中常常出现违反物理常识的故障，这使得它们成为评估模型能力的有效数据源。我们还创建了PhysInstruct和PhysDPO两个数据集，以帮助模型学习物理常识并优化其偏好。通过这些数据集，我们提出了PhysVLM，一个增强物理知识的视频大语言模型，并在多个基准测试中展示了其优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.19939', 'title': 'VLSBench: Unveiling Visual Leakage in Multimodal Safety', 'url': 'https://huggingface.co/papers/2411.19939', 'abstract': 'Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench', 'score': 4, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '03d8c636cf8c9486', 'authors': ['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2411.19939.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#leakage', '#open_source', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Новый взгляд на безопасность мультимодальных ИИ-моделей', 'desc': 'Статья рассматривает проблему безопасности мультимодальных больших языковых моделей (MLLM). Авторы обнаружили феномен утечки визуальной информации о безопасности (VSIL) в существующих мультимодальных тестах безопасности. Для решения этой проблемы они создали новый набор данных VLSBench, содержащий 2400 пар изображение-текст без VSIL. Эксперименты показали, что VLSBench представляет значительную сложность для современных MLLM и демонстрирует необходимость мультимодальной настройки для сценариев без VSIL.'}, 'en': {'title': 'Unveiling Safety in Multimodal Models: The VSIL Challenge', 'desc': 'This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it.'}, 'zh': {'title': '多模态安全性的新挑战与解决方案', 'desc': '这篇论文探讨了多模态大型语言模型（MLLMs）在安全性方面的挑战。研究发现，文本去学习可以与使用图像-文本对训练的模型在安全性表现上相当。作者指出，现有的多模态安全基准存在视觉安全信息泄漏（VSIL）问题，这使得模型能够轻易拒绝敏感的文本-图像查询。为了解决这个问题，研究者构建了一个新的多模态视觉无泄漏安全基准（VLSBench），以更好地评估模型在没有VSIL情况下的安全性。'}}}, {'id': 'https://huggingface.co/papers/2412.00947', 'title': 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information', 'url': 'https://huggingface.co/papers/2412.00947', 'abstract': 'Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.', 'score': 4, 'issue_id': 911, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': '7135f8936b0d3ee8', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Sarkar Snigdha Sarathi Das', 'Ranran Haoran Zhang', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00947.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#cv', '#synthetic', '#training'], 'emoji': '👁️', 'ru': {'title': 'VisOnlyQA: новый путь к улучшению визуального восприятия AI', 'desc': 'Исследователи представили новый датасет VisOnlyQA для оценки визуального восприятия больших визуально-языковых моделей (LVLM). Датасет фокусируется на геометрической и числовой информации в научных изображениях, позволяя анализировать способности моделей к тонкому визуальному восприятию. Эксперименты показали, что даже передовые LVLM, такие как GPT-4o и Gemini 1.5 Pro, плохо справляются с задачами VisOnlyQA, в то время как люди демонстрируют почти идеальные результаты. Исследование подчеркивает необходимость улучшения как обучающих данных, так и архитектур моделей для повышения качества визуального восприятия LVLM.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs with VisOnlyQA', 'desc': 'This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models.'}, 'zh': {'title': '提升视觉感知，助力大型视觉语言模型', 'desc': '本研究介绍了一个新的数据集VisOnlyQA，旨在直接评估大型视觉语言模型（LVLMs）在科学图形中几何和数值信息问题上的视觉感知能力。该数据集包含1200个多项选择题，涵盖四类图形的12个任务，帮助分析LVLMs对细粒度视觉信息的理解。实验结果显示，评估的20个LVLMs在视觉感知任务上的表现较差，而人类的表现几乎完美。通过合成训练数据进行微调可以提升LVLMs的视觉感知能力，但改进效果有限，且更强的语言模型能够提高视觉感知能力。'}}}, {'id': 'https://huggingface.co/papers/2412.00176', 'title': 'Art-Free Generative Models: Art Creation Without Graphic Art Knowledge', 'url': 'https://huggingface.co/papers/2412.00176', 'abstract': 'We explore the question: "How much prior art knowledge is needed to create art?" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.', 'score': 3, 'issue_id': 921, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '41f372dd1c030fbd', 'authors': ['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba'], 'affiliations': ['MIT', 'Northeastern University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00176.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic', '#games', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект творит шедевры с минимальным обучением', 'desc': 'Исследователи изучают вопрос о необходимом объеме предварительных знаний об искусстве для его создания. Они предлагают модель генерации изображений по тексту, обученную без доступа к контенту, связанному с искусством. Затем авторы представляют простой, но эффективный метод обучения адаптера для создания искусства, используя лишь несколько примеров выбранных художественных стилей. Эксперименты показывают, что искусство, созданное с помощью этого метода, воспринимается пользователями на уровне, сравнимом с произведениями моделей, обученных на больших наборах данных, богатых искусством.'}, 'en': {'title': 'Creating Art with Minimal Prior Knowledge', 'desc': "This paper investigates the necessity of prior art knowledge in generating art through a text-to-image model. The authors propose a novel approach that trains the model without any art-related content, relying instead on a small number of examples from specific artistic styles to create an 'art adapter'. Their experiments reveal that the art produced by this method is perceived similarly to that generated by models trained on extensive art datasets. Additionally, they employ data attribution techniques to demonstrate how both artistic and non-artistic examples influence the development of new artistic styles."}, 'zh': {'title': '创造艺术无需丰富的艺术知识', 'desc': '我们探讨了创造艺术需要多少先前的艺术知识。为此，我们提出了一种文本到图像生成模型，该模型在没有艺术相关内容的情况下进行训练。我们还引入了一种简单有效的方法，仅使用少量选定艺术风格的示例来学习艺术适配器。实验结果表明，使用我们的方法生成的艺术作品在用户眼中与在大型艺术丰富数据集上训练的模型生成的艺术作品相当。'}}}, {'id': 'https://huggingface.co/papers/2412.01250', 'title': 'Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input', 'url': 'https://huggingface.co/papers/2412.01250', 'abstract': 'Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.', 'score': 3, 'issue_id': 915, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '8b768de35e4c5114', 'authors': ['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang'], 'affiliations': ['Fondazione Bruno Kessler', 'Polytechnic of Turin', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Verona'], 'pdf_title_img': 'assets/pdf/title_img/2412.01250.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#reasoning', '#agents'], 'emoji': '🗺️', 'ru': {'title': 'Интерактивная навигация робота с уточнением цели у человека', 'desc': 'Статья представляет новую задачу навигации по инстансам объектов - Collaborative Instance Navigation (CoIN), где агент взаимодействует с человеком во время навигации для уточнения целевого объекта. Предложен метод AIUTA, использующий Vision Language Models и Large Language Models для обработки визуальной информации и генерации вопросов. AIUTA включает модули Self-Questioner для самоанализа наблюдений и Interaction Trigger для определения необходимости задать вопрос пользователю. Авторы также представили бенчмарк CoIN-Bench для оценки подобных систем.'}, 'en': {'title': 'Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration', 'desc': 'This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods.'}, 'zh': {'title': '协作实例导航：让机器更懂人类指令', 'desc': '现有的实例目标导航任务通常需要用户提供详细的描述，但在现实中，这种要求往往不切实际。为了解决这个问题，我们提出了一种新的任务，称为协作实例导航（CoIN），通过动态的代理-用户互动来解决导航中的不确定性。我们的方法，代理-用户互动与不确定性意识（AIUTA），结合了视觉语言模型和大型语言模型的能力，能够在导航过程中主动与用户对话。通过引入CoIN-Bench基准，我们的AIUTA方法在实例导航中表现出色，展示了处理用户输入的灵活性。'}}}, {'id': 'https://huggingface.co/papers/2411.19799', 'title': 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge', 'url': 'https://huggingface.co/papers/2411.19799', 'abstract': 'The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.', 'score': 3, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '7b0cbdd28adecf2c', 'authors': ['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam', 'Perttu Isotalo', 'Maral Jabbarishiviari', 'Börje F. Karlsson', 'Eldar Khalilov', 'Christopher Klamm', 'Fajri Koto', 'Dominik Krzemiński', 'Gabriel Adriano de Melo', 'Syrielle Montariol', 'Yiyang Nan', 'Joel Niklaus', 'Jekaterina Novikova', 'Johan Samir Obando Ceron', 'Debjit Paul', 'Esther Ploeger', 'Jebish Purbey', 'Swati Rajwal', 'Selvan Sunitha Ravi', 'Sara Rydell', 'Roshan Santhosh', 'Drishti Sharma', 'Marjana Prifti Skenduli', 'Arshia Soltani Moakhar', 'Bardia Soltani Moakhar', 'Ran Tamir', 'Ayush Kumar Tarun', 'Azmine Toushik Wasi', 'Thenuka Ovin Weerasinghe', 'Serhan Yilmaz', 'Mike Zhang', 'Imanol Schlag', 'Marzieh Fadaee', 'Sara Hooker', 'Antoine Bosselut'], 'affiliations': ['Cohere For AI', 'Cohere For AI Community', 'EPFL', 'ETH Zurich', 'Swiss AI Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2411.19799.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#benchmark', '#reasoning'], 'emoji': '🌍', 'ru': {'title': 'Глобальная оценка многоязычных ИИ-моделей в локальных контекстах', 'desc': 'Статья посвящена проблеме неравномерной эффективности больших языковых моделей (LLM) для разных языков. Авторы создали набор данных INCLUDE из 197,243 пар вопросов и ответов на 44 языках для оценки многоязычных LLM. Этот ресурс основан на местных экзаменационных материалах и охватывает различные региональные контексты. INCLUDE позволяет оценивать знания и способности к рассуждению многоязычных LLM в реальных языковых средах их применения.'}, 'en': {'title': 'Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs', 'desc': 'This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge.'}, 'zh': {'title': '提升多语言模型的实际应用能力', 'desc': '这篇论文讨论了大型语言模型（LLM）在不同语言之间的性能差异，这影响了它们在许多地区的有效应用。为了克服多语言LLM开发中的瓶颈，作者构建了一个包含197,243个问答对的评估套件，来源于当地考试材料。这个新资源INCLUDE是一个全面的知识和推理中心基准，涵盖44种书面语言，旨在评估多语言LLM在实际语言环境中的表现。通过这种方式，研究希望提升生成性人工智能工具在不同社区的经济和社会价值。'}}}, {'id': 'https://huggingface.co/papers/2412.01821', 'title': 'World-consistent Video Diffusion with Explicit 3D Modeling', 'url': 'https://huggingface.co/papers/2412.01821', 'abstract': 'Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.', 'score': 2, 'issue_id': 922, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '843891601e85de06', 'authors': ['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu'], 'affiliations': ['Apple', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.01821.jpg', 'data': {'categories': ['#3d', '#benchmark', '#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': '3D-согласованная генерация видео с помощью диффузионных моделей', 'desc': 'Статья представляет новую модель World-consistent Video Diffusion (WVD) для генерации 3D-согласованного контента. WVD использует XYZ-изображения для явного 3D-контроля в процессе диффузии. Модель обучается совместному распределению RGB и XYZ кадров, что позволяет решать различные задачи, включая генерацию 3D из одного изображения и создание видео с контролем камеры. WVD демонстрирует конкурентоспособные результаты на нескольких эталонных тестах.'}, 'en': {'title': 'Unifying 3D Consistency in Video Generation with WVD', 'desc': 'This paper introduces World-consistent Video Diffusion (WVD), a new framework that enhances video and image generation by incorporating 3D supervision. WVD uses XYZ images to provide global 3D coordinates for each pixel, allowing for more accurate and consistent 3D content generation. The framework employs a diffusion transformer to learn the relationship between RGB and XYZ frames, enabling tasks like generating new RGB frames from 3D data. Overall, WVD offers a versatile solution for various tasks, achieving strong performance across benchmarks with a single pretrained model.'}, 'zh': {'title': '统一3D一致性的视频生成新框架', 'desc': '最近，扩散模型在图像和视频生成方面取得了显著进展，能够在单帧和多帧上下文中实现逼真的视觉合成。然而，这些模型在高效且明确地生成3D一致内容方面仍然存在挑战。为了解决这个问题，我们提出了世界一致视频扩散（WVD），这是一个新颖的框架，利用XYZ图像进行明确的3D监督，编码每个图像像素的全局3D坐标。WVD通过灵活的修补策略支持多任务适应性，能够从真实的RGB估计XYZ帧，或沿指定的相机轨迹生成新的RGB帧。'}}}, {'id': 'https://huggingface.co/papers/2412.00319', 'title': 'Improving speaker verification robustness with synthetic emotional utterances', 'url': 'https://huggingface.co/papers/2412.00319', 'abstract': 'A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.   To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': 'e9e71338ed876bc0', 'authors': ['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke'], 'affiliations': ['Amazon Alexa AI, USA', 'Amazon Business, USA', 'Amazon GenAI, USA', 'Amazon Search Experience Science, USA', 'Amazon Web Services, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00319.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#audio'], 'emoji': '🎭', 'ru': {'title': 'CycleGAN для аугментации эмоциональной речи улучшает верификацию говорящего', 'desc': 'Статья предлагает новый подход к верификации говорящего с использованием CycleGAN для аугментации данных эмоциональной речи. Авторы синтезируют эмоциональные речевые сегменты для каждого конкретного диктора, сохраняя при этом уникальную голосовую идентичность. Эксперименты показывают, что модели, обученные на расширенном наборе данных, превосходят базовые модели в задаче верификации говорящих в сценариях с эмоциональной речью. Предложенный метод снижает равную ошибку (EER) до 3.64% относительно базовой линии.'}, 'en': {'title': 'Enhancing Speaker Verification with Synthetic Emotional Data', 'desc': "This paper discusses a speaker verification (SV) system that authenticates speakers based on their voice. A major challenge for these systems is accurately verifying speakers when they express different emotions, as most existing models struggle with emotional speech. The authors propose using a CycleGAN framework to create synthetic emotional speech data, which helps improve the training of SV models. Their results show that incorporating this augmented data significantly enhances the models' performance, reducing error rates in emotional speech verification."}, 'zh': {'title': '情感语音验证的新突破', 'desc': '本研究提出了一种说话人验证系统，旨在确认特定语音样本是否来自特定说话者。该系统面临的主要挑战是如何在不同情感状态下保持一致的性能。为了克服这一问题，我们采用CycleGAN框架进行数据增强，合成每个说话者的情感语音片段，同时保留其独特的声音特征。实验结果表明，使用合成情感数据训练的模型在情感语音验证任务中表现优于基线模型，错误率降低了3.64%。'}}}, {'id': 'https://huggingface.co/papers/2412.00869', 'title': 'Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting', 'url': 'https://huggingface.co/papers/2412.00869', 'abstract': 'Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': '6816796631155a8c', 'authors': ['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'IIIT Hyderabad, India', 'Meta, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00869.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели все еще не мастера аналогий', 'desc': 'Эта статья посвящена исследованию способности больших языковых моделей (LLM) решать пропорциональные аналогии. Авторы создали набор данных из 15 000 вопросов с множественным выбором для завершения аналогий. Они оценили производительность современных LLM в различных настройках промптов с дополнительными знаниями. Результаты показывают, что даже лучшие модели достигают точности лишь 55%, и целевые знания помогают больше, чем примеры или структурированная информация.'}, 'en': {'title': 'Enhancing Analogy Completion in LLMs with Targeted Knowledge', 'desc': 'This paper focuses on the challenge of solving proportional analogies, which are important for assessing cognitive abilities. The authors present a new dataset containing 15,000 multiple-choice questions designed for this purpose. They evaluate how well current Large Language Models (LLMs) perform on these analogies, especially when given different types of knowledge prompts. The findings reveal that while LLMs struggle with these tasks, targeted knowledge prompts significantly improve their performance compared to other types of knowledge.'}, 'zh': {'title': '提升类比能力，知识是关键！', 'desc': '类比是认知的重要部分，比例类比由四个术语组成，常用于评估语言和认知能力。本文介绍了一个包含15,000个多项选择题的比例类比完成数据集，并评估了当前大型语言模型（LLMs）在不同知识增强提示设置下的表现。研究发现，尽管模型经过大量训练，解决比例类比仍然具有挑战性，最佳模型的准确率仅为55%。特别是，提供针对性的知识比提供示例或结构化知识更能帮助模型完成比例类比。'}}}, {'id': 'https://huggingface.co/papers/2412.01408', 'title': 'Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning', 'url': 'https://huggingface.co/papers/2412.01408', 'abstract': 'Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.', 'score': 1, 'issue_id': 918, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'f4c5e5e0485d3969', 'authors': ['Aditya Narayan Sankaran', 'Reza Farahbaksh', 'Noel Crespi'], 'affiliations': ['SAMOVAR, Télécom SudParis Institut Polytechnique de Paris 91120 Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.01408.jpg', 'data': {'categories': ['#low_resource', '#interpretability', '#dataset', '#multilingual', '#audio', '#training'], 'emoji': '🎙️', 'ru': {'title': 'Борьба с оскорблениями в аудио: мало данных, много языков', 'desc': 'Исследование посвящено обнаружению оскорбительного контента в аудио на языках с ограниченными ресурсами, в частности на индийских языках, с использованием Few Shot Learning (FSL). Авторы применяют предобученные аудио-представления из моделей Wav2Vec и Whisper в рамках Model-Agnostic Meta-Learning (MAML) для классификации оскорбительного языка в 10 языках. Эксперименты проводились с различными размерами выборок (50-200) для оценки влияния ограниченных данных на производительность. Исследование демонстрирует способность предобученных моделей к обобщению в условиях ограниченных ресурсов и предоставляет ценные insights для обнаружения оскорбительного языка в многоязычном контексте.'}, 'en': {'title': 'Empowering Low-Resource Languages: Detecting Abuse in Audio with Few Shot Learning', 'desc': 'This paper focuses on detecting abusive language in audio, especially in languages with limited resources, like many Indian languages. It uses Few Shot Learning (FSL) techniques with pre-trained audio models such as Wav2Vec and Whisper to improve detection accuracy. The authors apply the Model-Agnostic Meta-Learning (MAML) framework to classify abusive content across 10 different languages, even with minimal training data. Their findings demonstrate how well pre-trained models can generalize in low-resource settings, providing insights for better abuse detection in multilingual environments.'}, 'zh': {'title': '利用预训练模型提升低资源环境下的辱骂内容检测', 'desc': '本研究探讨了在低资源环境中，特别是音频模式下，检测在线辱骂内容的潜力。我们使用预训练的音频表示，结合少量学习（Few Shot Learning），在印度语言中进行辱骂语言的检测。通过利用Wav2Vec和Whisper等模型的强大表示，我们在ADIMA数据集上进行跨语言的辱骂检测。研究表明，预训练模型在低资源场景中的泛化能力，为多语言环境中的辱骂语言检测提供了有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2411.19477', 'title': 'A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models', 'url': 'https://huggingface.co/papers/2411.19477', 'abstract': 'We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.', 'score': 1, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '6057902d5fcbe3f4', 'authors': ['Yanxi Chen', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2411.19477.jpg', 'data': {'categories': ['#training', '#benchmark', '#architecture', '#optimization'], 'emoji': '🏆', 'ru': {'title': 'Масштабируемый алгоритм улучшения точности больших языковых моделей', 'desc': 'Авторы предлагают двухэтапный алгоритм, который демонстрирует масштабируемый закон для вычислений больших языковых моделей (LLM) во время тестирования. Алгоритм сначала генерирует N кандидатов решений, а затем выбирает лучшее с помощью многораундового турнира на выбывание. Теоретически доказано, что вероятность ошибки алгоритма экспоненциально уменьшается с увеличением N и K. Эмпирические результаты на сложном бенчмарке MMLU-Pro подтверждают эффективность предложенного метода.'}, 'en': {'title': 'Efficient Solution Selection for Large Language Models', 'desc': 'This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark.'}, 'zh': {'title': '高效选择：两阶段算法优化大语言模型计算', 'desc': '我们提出了一种通用的两阶段算法，能够在大语言模型（LLMs）的测试时间计算中实现可证明的扩展规律。该算法首先生成N个候选解，然后通过多轮淘汰赛选择最佳解，每对候选解比较K次，只有胜者进入下一轮。该算法的最简实现仅需使用黑箱LLM，无需外部验证器或奖励模型，总共需要N次(K + 1)高度可并行的LLM调用来解决输入问题。理论证明表明，假设生成的候选解正确的概率为p_{gen} > 0，且正确与错误解的比较能以概率p_{comp} > 0.5识别出正确的胜者，则该算法的失败概率随着N和K的增加呈指数级下降。'}}}, {'id': 'https://huggingface.co/papers/2412.02259', 'title': 'VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation', 'url': 'https://huggingface.co/papers/2412.02259', 'abstract': 'Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.', 'score': 21, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '5a007f38be3e3ba7', 'authors': ['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Peking University', 'University of Central Florida', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02259.jpg', 'data': {'categories': ['#video', '#story_generation', '#games'], 'emoji': '🎬', 'ru': {'title': 'VGoT: Новый уровень в генерации многокадровых видео с сохранением логики повествования', 'desc': 'Статья представляет новый подход к генерации многокадровых видео под названием VideoGen-of-Thought (VGoT). Эта архитектура разделяет процесс на несколько этапов: генерация сценария, создание ключевых кадров, генерация отдельных сцен и механизм сглаживания. VGoT использует встраивания для сохранения идентичности персонажей между сценами и применяет межкадровое сглаживание для обеспечения визуальной согласованности. Эксперименты показывают, что VGoT превосходит существующие методы в создании качественных и связных многокадровых видео.'}, 'en': {'title': 'Revolutionizing Multi-Shot Video Generation with VGoT', 'desc': 'The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots.'}, 'zh': {'title': '多镜头视频生成的新突破', 'desc': '当前的视频生成模型在生成短片方面表现出色，但在创建多镜头、电影般的视频时仍然存在困难。现有模型通常只针对单镜头目标进行训练，因此在保持逻辑故事线和视觉一致性方面显得不足。为此，我们提出了VideoGen-of-Thought（VGoT），这是一种专门为多镜头视频生成设计的协作和无训练架构。VGoT通过脚本生成、关键帧生成和镜头级视频生成等模块化步骤，确保了合理的叙事设计和跨镜头的一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.19943', 'title': "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability", 'url': 'https://huggingface.co/papers/2411.19943', 'abstract': "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.", 'score': 21, 'issue_id': 933, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'aaf523f6bd9412e3', 'authors': ['Zicheng Lin', 'Tian Liang', 'Jiahao Xu', 'Xing Wang', 'Ruilin Luo', 'Chufan Shi', 'Siheng Li', 'Yujiu Yang', 'Zhaopeng Tu'], 'affiliations': ['Tsinghua University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.19943.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#benchmark', '#alignment'], 'emoji': '🔍', 'ru': {'title': 'Повышение точности рассуждений LLM путем выявления критических токенов', 'desc': "Исследование посвящено влиянию отдельных токенов на результаты рассуждений в больших языковых моделях (LLM). Авторы обнаружили существование 'критических токенов', которые приводят к неправильным траекториям рассуждений. Они предложили новый метод cDPO для автоматического распознавания и обучения с учетом критических токенов в процессе выравнивания модели. Экспериментальные результаты на бенчмарках GSM8K и MATH500 с использованием моделей Llama-3 и deepseek-math продемонстрировали эффективность предложенного подхода."}, 'en': {'title': 'Enhancing Reasoning in LLMs by Identifying Critical Tokens', 'desc': "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."}, 'zh': {'title': '识别关键token，提升推理准确性', 'desc': '大型语言模型（LLMs）在推理任务中表现出色，能够通过自回归的方式生成推理过程。本文探讨了单个token对推理任务最终结果的影响，发现存在“关键token”，这些token会导致错误的推理轨迹。我们提出了一种新方法cDPO，旨在自动识别关键token并在对齐过程中进行token级奖励。通过对比正负模型的生成可能性，我们能够识别出在错误轨迹中导致错误结果的关键token，从而提高模型的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2412.01981', 'title': 'Free Process Rewards without Process Labels', 'url': 'https://huggingface.co/papers/2412.01981', 'abstract': "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.", 'score': 15, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '13434e4f301a0d88', 'authors': ['Lifan Yuan', 'Wendi Li', 'Huayu Chen', 'Ganqu Cui', 'Ning Ding', 'Kaiyan Zhang', 'Bowen Zhou', 'Zhiyuan Liu', 'Hao Peng'], 'affiliations': ['Huazhong University of Science and Technology', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.01981.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#reasoning', '#data', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение PRM без пошаговой разметки', 'desc': 'Статья представляет новый подход к обучению процессуальных моделей вознаграждения (PRM) в машинном обучении. Авторы показывают, что неявную PRM можно получить без дополнительных затрат, просто обучая модель вознаграждения за результат (ORM) на более дешевых метках уровня ответов. Эксперименты на датасете MATH демонстрируют, что предложенный метод превосходит сильный бейзлайн на основе MCTS, используя менее 1/38 обучающих данных. Исследование также выявляет, что масштабирование инструкций и ответов улучшает производительность неявной PRM.'}, 'en': {'title': 'Unlocking Efficient Training for Process Reward Models', 'desc': 'This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples.'}, 'zh': {'title': '隐式过程奖励模型：高效训练的新思路', 'desc': '本文提出了一种新的过程奖励模型（PRM），与传统的结果奖励模型（ORM）不同，PRM能够逐步评估推理过程，提供更细致的奖励。然而，训练PRM需要在每个中间步骤都有标注，这在数据收集上面临挑战。我们展示了通过训练ORM并使用响应级别的标签，可以在没有额外成本的情况下获得隐式PRM。实验结果表明，隐式PRM在数据效率和性能上优于传统方法，尤其是在数据稀缺的情况下。'}}}, {'id': 'https://huggingface.co/papers/2412.02611', 'title': 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?', 'url': 'https://huggingface.co/papers/2412.02611', 'abstract': 'Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.', 'score': 11, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': 'f63565048b4948b4', 'authors': ['Kaixiong Gong', 'Kaituo Feng', 'Bohao Li', 'Yibing Wang', 'Mofan Cheng', 'Shijia Yang', 'Jiaming Han', 'Benyou Wang', 'Yutong Bai', 'Zhuoran Yang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Stanford University', 'UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02611.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#interpretability', '#multimodal', '#games'], 'emoji': '🎧', 'ru': {'title': 'Слышат ли ИИ-модели то, что видят?', 'desc': 'Статья представляет новый тест DeafTest и бенчмарк AV-Odyssey Bench для оценки мультимодальных больших языковых моделей (MLLM) в задачах аудио-визуального понимания. Исследователи обнаружили, что современные MLLM часто не справляются с простыми задачами определения громкости и высоты звуков. Бенчмарк включает 4,555 задач с текстом, изображениями и аудио, требующих эффективного использования обоих модальностей. Авторы провели тестирование ряда закрытых и открытых моделей, выявив ограничения текущих систем для дальнейшего улучшения сбора данных и разработки моделей.'}, 'en': {'title': 'Unveiling the Limits of Multimodal Models with AV-Odyssey Bench', 'desc': 'This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation.'}, 'zh': {'title': '揭示多模态模型的局限性', 'desc': '最近，多模态大型语言模型（MLLMs）如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其在视觉和音频方面的能力。尽管这些模型在多种音频-视觉应用中表现出色，但我们的DeafTest显示，MLLMs在一些人类认为简单的任务上常常表现不佳，例如判断两个声音哪个更响和哪个音调更高。为此，我们提出了AV-Odyssey Bench，这是一个全面的音频-视觉基准，旨在评估这些MLLMs是否真正理解音频-视觉信息。该基准包含4555个精心设计的问题，要求模型有效利用视觉和音频输入中的线索，以准确推断答案。'}}}, {'id': 'https://huggingface.co/papers/2412.02114', 'title': 'OmniCreator: Self-Supervised Unified Generation with Universal Editing', 'url': 'https://huggingface.co/papers/2412.02114', 'abstract': 'We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.', 'score': 7, 'issue_id': 939, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '62bf26709baf7f97', 'authors': ['Haodong Chen', 'Lan Wang', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKUST', 'MSU', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2412.02114.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#dataset', '#video', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Универсальный генератор и редактор медиа по текстовому запросу', 'desc': 'OmniCreator - это новая система для генерации и редактирования изображений и видео на основе текстовых запросов. Она обучается самостоятельно, используя пары текст-видео, и устанавливает семантические связи между видео и текстом. OmniCreator может как генерировать новый контент по текстовому описанию, так и редактировать существующие видео и изображения без ограничений. Авторы также представили новый набор данных OmniBench-99 для оценки моделей генеративного редактирования видео.'}, 'en': {'title': 'OmniCreator: Unified Text-Prompted Image and Video Generation and Editing', 'desc': 'OmniCreator is a new framework that allows for the generation and editing of both images and videos using text prompts. It learns to understand the relationship between text and video through self-supervised learning, using original text-video pairs as input. During use, it can either generate new videos based on text prompts or edit existing videos while maintaining fidelity to the provided text. Additionally, the framework is evaluated using a new dataset called OmniBench-99, which helps assess the performance of generative video editing models.'}, 'zh': {'title': 'OmniCreator：统一的图像与视频生成与编辑框架', 'desc': 'OmniCreator是一个新颖的框架，能够在一个平台上进行文本提示的统一生成（图像+视频）和编辑。它通过自监督学习，利用原始的文本-视频对作为条件，同时使用相同的视频作为去噪目标，学习视频与文本之间的语义对应关系。在推理阶段，OmniCreator能够根据文本提示和视频生成忠实于两者的目标，实现无约束的通用编辑效果。我们还引入了OmniBench-99数据集，以全面评估生成视频编辑模型的性能，实验结果表明OmniCreator在所有模型中表现出显著的优势。'}}}, {'id': 'https://huggingface.co/papers/2412.02632', 'title': 'Scaling Image Tokenizers with Grouped Spherical Quantization', 'url': 'https://huggingface.co/papers/2412.02632', 'abstract': 'Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.', 'score': 5, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '60eda94a31cded90', 'authors': ['Jiangtao Wang', 'Zhen Qin', 'Yifan Zhang', 'Vincent Tao Hu', 'Björn Ommer', 'Rania Briq', 'Stefan Kesselheim'], 'affiliations': ['CompVis @ LMU Munich', 'Jülich Supercomputing Centre', 'TapTap', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02632.jpg', 'data': {'categories': ['#training', '#inference', '#cv', '#data'], 'emoji': '🔍', 'ru': {'title': 'GSQ: Эффективная токенизация изображений на сферической поверхности', 'desc': 'В статье представлен новый метод токенизации изображений - Групповая Сферическая Квантизация (GSQ). GSQ использует сферическую инициализацию кодовой книги и регуляризацию поиска для ограничения латентного пространства кодовой книги сферической поверхностью. Авторы провели эмпирический анализ стратегий обучения токенизаторов изображений и показали, что GSQ-GAN превосходит современные методы по качеству реконструкции при меньшем количестве итераций обучения. Исследование масштабируемости GSQ выявило различное поведение при высоких и низких уровнях пространственного сжатия, подчеркивая сложности представления высокоразмерных латентных пространств.'}, 'en': {'title': 'Efficient Image Processing with Grouped Spherical Quantization', 'desc': "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."}, 'zh': {'title': '分组球面量化：高效的视觉标记器新方法', 'desc': '本文介绍了一种新的视觉标记器方法，称为分组球面量化（GSQ），旨在解决传统方法中的一些问题。GSQ通过球面代码本初始化和查找正则化，限制了代码本潜在空间在球面上的分布。我们的实证分析表明，GSQ-GAN在重建质量上优于现有的最先进方法，并且训练迭代次数更少。研究还系统地考察了GSQ在潜在维度、代码本大小和压缩比等方面的扩展行为，揭示了高维潜在空间表示的挑战。'}}}, {'id': 'https://huggingface.co/papers/2411.19067', 'title': 'MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation', 'url': 'https://huggingface.co/papers/2411.19067', 'abstract': "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.", 'score': 4, 'issue_id': 934, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '74d4a17af3574a5d', 'authors': ['Minhyun Lee', 'Seungho Lee', 'Song Park', 'Dongyoon Han', 'Byeongho Heo', 'Hyunjung Shim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19067.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#training'], 'emoji': '🎭', 'ru': {'title': 'Маскирование для улучшения сегментации изображений по описанию', 'desc': 'Статья посвящена задаче сегментации изображений по текстовому описанию (RIS). Авторы предлагают новый метод обучения под названием MaskRIS, который использует маскирование изображений и текста. Этот подход улучшает устойчивость модели к окклюзиям, неполной информации и языковым сложностям. Эксперименты показывают, что MaskRIS превосходит существующие методы как при полностью контролируемом, так и при слабо контролируемом обучении.'}, 'en': {'title': 'Enhancing Referring Image Segmentation with Masking Techniques', 'desc': 'Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets.'}, 'zh': {'title': 'Masked Referring Image Segmentation：提升图像分割性能的新方法', 'desc': '引用图像分割（RIS）是一种先进的视觉-语言任务，旨在根据自由形式的文本描述识别和分割图像中的对象。本文探讨了有效的数据增强技术，并提出了一种新的训练框架，称为Masked Referring Image Segmentation（MaskRIS）。研究表明，传统的图像增强方法在RIS中效果不佳，而简单的随机遮罩显著提升了RIS的性能。MaskRIS结合了图像和文本遮罩，并采用了失真感知上下文学习（DCL），从而提高了模型对遮挡、不完整信息和语言复杂性的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2412.01292', 'title': 'LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences', 'url': 'https://huggingface.co/papers/2412.01292', 'abstract': "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.", 'score': 4, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'e8f8ddd05e13e9ef', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Junyan Li', 'Shuailei Ma', 'Xinyu Sun', 'Tianhang Xiang', 'Yinjie Lei', 'Mingkui Tan', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'Northeastern University', 'Pazhou Laboratory', 'Sichuan University', 'South China University of Technology', 'Tencent Robotics X', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.01292.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#3d', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Умное понимание 3D-сцен с LSceneLLM', 'desc': 'Исследование 3D Vision-Language Models (3D-VLMs) важно для развития воплощенного AI в 3D-сценах, таких как визуальная навигация и ответы на вопросы. Из-за высокой плотности визуальных признаков в больших 3D-сценах сложно точно определить важную для задачи информацию. Предлагается адаптивная структура LSceneLLM, которая автоматически определяет важные области, используя визуальные предпочтения LLM для разных задач. Эксперименты показывают, что наш метод превосходит существующие методы в понимании больших сцен и улучшает результаты при интеграции в существующие 3D-VLMs.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with LSceneLLM', 'desc': 'This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods.'}, 'zh': {'title': '自适应3D视觉语言模型，提升场景理解能力', 'desc': '3D视觉语言模型（3D-VLMs）的研究越来越受到关注，这对在3D场景中发展具身人工智能至关重要，如视觉导航和具身问答。由于3D场景中视觉特征的高密度，准确定位与任务相关的视觉信息变得具有挑战性。现有方法尝试对所有对象进行分割，并将其特征视为场景表示，但这些与任务无关的对象特征包含大量冗余信息和缺失的细节。为了解决这些问题，我们提出了LSceneLLM，一个自适应框架，通过利用大语言模型（LLM）对不同任务的视觉偏好，自动识别与任务相关的区域，并通过可插拔的场景放大模块捕捉聚焦区域的细粒度细节。'}}}, {'id': 'https://huggingface.co/papers/2412.02592', 'title': 'OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2412.02592', 'abstract': "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench", 'score': 3, 'issue_id': 937, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '91dbac114744b1e9', 'authors': ['Junyuan Zhang', 'Qintong Zhang', 'Bin Wang', 'Linke Ouyang', 'Zichen Wen', 'Ying Li', 'Ka-Ho Chow', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Beihang University', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of HongKong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02592.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#multimodal', '#benchmark', '#rag', '#optimization', '#survey'], 'emoji': '🔍', 'ru': {'title': 'OHRBench: раскрывая влияние OCR на системы RAG', 'desc': 'OHRBench - это первый бенчмарк для оценки влияния оптического распознавания символов (OCR) на системы генерации с извлечением информации (RAG). Он включает 350 неструктурированных PDF-документов из шести реальных областей применения RAG, а также вопросы и ответы, созданные на основе мультимодальных элементов документов. Исследователи выделяют два основных типа шума OCR: семантический и форматный, и применяют возмущения для создания структурированных данных с различной степенью каждого типа шума. Результаты показывают уязвимость систем RAG к ошибкам OCR и потенциал использования мультимодальных языковых моделей (VLM) без OCR в системах RAG.'}, 'en': {'title': "Enhancing RAG: Understanding OCR's Impact with OHRBench", 'desc': 'This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems.'}, 'zh': {'title': '揭示OCR对RAG系统的影响', 'desc': '本论文介绍了OHRBench，这是第一个用于理解光学字符识别（OCR）对检索增强生成（RAG）系统影响的基准。研究发现，OCR在处理非结构化PDF文档时会引入语义噪声和格式噪声，导致知识库质量下降。通过对350个真实世界应用领域的文档进行评估，结果显示现有的OCR解决方案无法有效构建高质量的知识库。最后，论文探讨了在RAG系统中使用视觉语言模型（VLMs）而不依赖OCR的潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.19542', 'title': 'A dynamic parallel method for performance optimization on hybrid CPUs', 'url': 'https://huggingface.co/papers/2411.19542', 'abstract': 'The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.', 'score': 3, 'issue_id': 936, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '27226211eddf71d4', 'authors': ['Luo Yu', 'Liu Yucheng', 'Shen Haihao'], 'affiliations': ['Intel Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2411.19542.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение инференса ИИ на гибридных CPU: балансировка для максимальной производительности', 'desc': 'Статья представляет динамический метод параллельных вычислений для гибридных процессоров, оптимизирующий инференс больших языковых моделей (LLM). Авторы обнаружили, что существующие фреймворки для инференса ИИ не учитывают неравномерные возможности ядер в гибридных CPU, что приводит к низкой производительности. Предложенный метод балансирует нагрузку между ядрами перед началом параллельной работы, что значительно повышает эффективность инференса LLM. В результате, инструмент Neural Speed достиг более 90% использования пропускной способности памяти на двух гибридных процессорах Intel.'}, 'en': {'title': 'Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs', 'desc': 'The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs.'}, 'zh': {'title': '动态平衡，提升AI推理性能！', 'desc': 'AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，目前的AI推理框架忽视了混合CPU的不平衡硬件能力，导致推理性能低下。为了解决这个问题，我们提出了一种动态并行方法，显著提高了混合CPU的LLM推理性能，通过在并行工作开始之前平衡每个核心的工作负载。该方法使Neural Speed在两款混合Intel CPU上实现了超过90%的内存带宽。'}}}, {'id': 'https://huggingface.co/papers/2412.01558', 'title': 'VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval', 'url': 'https://huggingface.co/papers/2412.01558', 'abstract': 'Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .', 'score': 1, 'issue_id': 935, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '12235c4ebf26fe4a', 'authors': ['Dhiman Paul', 'Md Rizwan Parvez', 'Nabeel Mohammed', 'Shafin Rahman'], 'affiliations': ['Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh', 'Qatar Computing Research Institute (QCRI), Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2412.01558.jpg', 'data': {'categories': ['#video', '#games', '#synthetic', '#architecture', '#benchmark', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'VideoLights: Новый подход к анализу ключевых моментов видео с помощью продвинутых нейросетевых архитектур', 'desc': 'Статья представляет VideoLights - новую систему для обнаружения ключевых моментов видео и поиска по ним. Авторы предлагают улучшенные методы выравнивания видео и текста, двунаправленное слияние модальностей и механизм обратной связи между задачами. Они также вводят адаптивные функции потерь и используют большие мультимодальные модели для улучшения представления данных. Эксперименты показывают, что VideoLights превосходит существующие методы на нескольких наборах данных.'}, 'en': {'title': 'Enhancing Video-Text Integration with VideoLights', 'desc': 'This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis.'}, 'zh': {'title': 'VideoLights：提升视频与文本分析的全新框架', 'desc': '本论文提出了一种名为VideoLights的视频高亮检测和时刻检索框架，旨在解决现有模型在视频与文本对齐和跨任务动态方面的不足。我们引入了卷积投影和特征精炼模块，以提高视频和文本特征的对齐效果，并采用双向跨模态融合网络来增强查询感知的片段表示。通过单向联合任务反馈机制，我们能够提升两个任务之间的相关性，同时引入硬正负损失以改善学习效果。实验结果表明，VideoLights在多个基准数据集上表现出色，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.18478', 'title': 'Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS', 'url': 'https://huggingface.co/papers/2411.18478', 'abstract': "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%).", 'score': 21, 'issue_id': 890, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '05890d0739faa85c', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Department of Automation, Tsinghua University', 'Beijing National Research Center for Information Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.18478.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'HiAR-ICL: Абстрактное мышление для языковых моделей', 'desc': "HiAR-ICL - это новый подход к обучению больших языковых моделей, который фокусируется на абстрактных паттернах мышления вместо конкретных примеров. Метод использует пять базовых действий рассуждения для построения цепочек мышления. Применяя поиск Монте-Карло, HiAR-ICL создает 'карточки мыслей' для руководства выводами. Эксперименты показали, что HiAR-ICL достигает лучших результатов на бенчмарке MATH, превосходя GPT-4 и Claude 3.5."}, 'en': {'title': 'Revolutionizing Mathematical Reasoning with HiAR-ICL', 'desc': "This paper introduces HiAR-ICL, a new approach to In-context Learning (ICL) that enhances large language models' ability to perform complex mathematical reasoning tasks. Traditional ICL relies heavily on the quality of examples and often requires human input, which can be limiting. HiAR-ICL shifts the focus from specific examples to abstract reasoning patterns, utilizing five atomic reasoning actions to create structured reasoning chains. The method employs Monte Carlo Tree Search to explore reasoning paths and develop a cognitive complexity framework that matches problems with suitable thought cards, achieving superior performance on the MATH benchmark."}, 'zh': {'title': '高层次自动推理：超越传统上下文学习的局限性', 'desc': '本文提出了一种新的高层次自动推理范式HiAR-ICL，旨在解决传统上下文学习在复杂数学推理任务中的局限性。HiAR-ICL通过引入五种基本推理动作，转变了对具体示例的依赖，强调抽象思维模式的重要性。该方法利用蒙特卡洛树搜索探索推理路径，并构建思维卡片以指导后续推理。实验结果表明，HiAR-ICL在MATH基准测试中取得了79.6%的准确率，超越了其他先进模型。'}}}, {'id': 'https://huggingface.co/papers/2411.19930', 'title': 'On Domain-Specific Post-Training for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.19930', 'abstract': 'Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.', 'score': 18, 'issue_id': 885, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '5d14749b38f15e60', 'authors': ['Daixuan Cheng', 'Shaohan Huang', 'Ziyu Zhu', 'Xintong Zhang', 'Wayne Xin Zhao', 'Zhongzhi Luan', 'Bo Dai', 'Zhenliang Zhang'], 'affiliations': ['Beihang University', 'Beijing Institute of Technology', 'Renmin University of China', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19930.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#open_source', '#multimodal', '#synthetic'], 'emoji': '🔬', 'ru': {'title': 'Адаптация мультимодальных ИИ к специализированным областям', 'desc': 'Статья исследует адаптацию мультимодальных больших языковых моделей (MLLM) к специфическим доменам. Авторы разработали синтезатор визуальных инструкций, использующий доменные пары изображение-подпись для генерации разнообразных задач. Они применили одноэтапный процесс обучения для улучшения разнообразия задач при доменной постобработке. Эксперименты проводились в биомедицинской и пищевой областях с использованием различных MLLM, демонстрируя эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques', 'desc': 'This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area.'}, 'zh': {'title': '提升多模态模型的领域适应性', 'desc': '近年来，通用多模态大型语言模型（MLLMs）迅速发展。然而，将通用MLLMs适应于特定领域，如科学和工业应用，仍然较少被探索。本文系统研究了MLLMs的领域适应性，重点在于数据合成、训练流程和任务评估。我们开发了一种视觉指令合成器，能够有效生成多样化的视觉指令任务，从而提升MLLMs在特定领域的表现。'}}}, {'id': 'https://huggingface.co/papers/2411.19189', 'title': 'Video Depth without Video Models', 'url': 'https://huggingface.co/papers/2411.19189', 'abstract': 'Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.', 'score': 16, 'issue_id': 889, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '1fc611a9a44595a1', 'authors': ['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.19189.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'RollingDepth: Революция в оценке глубины видео с помощью LDM', 'desc': 'Данная статья представляет новый подход к оценке глубины видео под названием RollingDepth. Модель основана на латентной диффузионной модели (LDM) для одиночных изображений и включает два ключевых компонента: оценщик глубины для коротких видеофрагментов и алгоритм регистрации для сборки фрагментов в целостное видео. RollingDepth эффективно обрабатывает длинные видео и превосходит по точности как специализированные оценщики глубины видео, так и высокопроизводительные модели для отдельных кадров. Этот метод решает проблемы временной непрерывности и изменений диапазона глубины при движении камеры.'}, 'en': {'title': 'Transforming Monocular Videos into Accurate 3D Depth with RollingDepth', 'desc': 'This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences.'}, 'zh': {'title': '将单图像深度估计提升为视频深度估计的创新之路', 'desc': '视频深度估计通过推断每帧的密集深度，将单目视频片段提升为3D。最近，单图像深度估计的进展激发了对视频深度的关注，但简单地将单图像深度估计器应用于每帧会忽略时间连续性，导致闪烁和深度范围的突然变化。我们提出了一种名为RollingDepth的模型，它结合了多帧深度估计和优化的注册算法，能够有效处理长视频并提供更准确的深度视频。该模型在性能上超越了专用视频深度估计器和高性能单帧模型。'}}}, {'id': 'https://huggingface.co/papers/2411.19146', 'title': 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs', 'url': 'https://huggingface.co/papers/2411.19146', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.", 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': 'b33bb17742a81e99', 'authors': ['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Itay Levy', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.19146.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': '🧩', 'ru': {'title': 'Ускорение LLM без потери качества: революция в эффективности ИИ', 'desc': 'Представлена система Puzzle для ускорения инференса больших языковых моделей (LLM) на конкретном оборудовании при сохранении их возможностей. Используя нейроархитектурный поиск (NAS) и блочную локальную дистилляцию знаний (BLD), Puzzle оптимизирует модели с десятками миллиардов параметров под аппаратные ограничения. На примере модели Nemotron-51B, полученной из Llama-3.1-70B-Instruct, демонстрируется 2.17-кратное ускорение инференса при сохранении 98.4% возможностей исходной модели. Это показывает, что производительность инференса, а не только количество параметров, должна определять выбор модели.'}, 'en': {'title': 'Optimizing Large Language Models for Efficient Inference', 'desc': "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."}, 'zh': {'title': '高效推理，强大模型的新范式', 'desc': '大型语言模型（LLMs）在推理方面表现出色，但高计算成本限制了它们的应用。我们提出了Puzzle框架，通过神经架构搜索（NAS）在特定硬件上加速LLM推理，同时保持其能力。该框架利用块状局部知识蒸馏（BLD）进行并行架构探索，并采用混合整数规划进行精确约束优化。通过Nemotron-51B模型，我们展示了在单个NVIDIA H100 GPU上实现2.17倍推理吞吐量提升的实际效果，同时保留了98.4%的原始模型能力。'}}}, {'id': 'https://huggingface.co/papers/2411.19108', 'title': "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model", 'url': 'https://huggingface.co/papers/2411.19108', 'abstract': 'As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.', 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '02a6c2edf156e9d3', 'authors': ['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.19108.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': '⏱️', 'ru': {'title': 'Умное кэширование для быстрой генерации видео', 'desc': 'Статья представляет новый подход к ускорению диффузионных моделей для генерации видео под названием TeaCache. Метод оценивает различия между выходными данными модели на разных временных шагах, используя входные данные и встраивания временных шагов. TeaCache применяет стратегию масштабирования для уточнения оценок различий и использует их для кэширования выходных данных. Эксперименты показывают, что TeaCache достигает ускорения до 4,41 раза по сравнению с Open-Sora-Plan при незначительном снижении качества визуализации.'}, 'en': {'title': 'Accelerating Video Generation with Smart Caching', 'desc': 'This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods.'}, 'zh': {'title': '提升视频生成速度的新方法：TeaCache', 'desc': '本研究提出了一种新的缓存方法，称为时间步嵌入感知缓存（TeaCache），旨在提高视频生成中的扩散模型的推理速度。传统方法通过在均匀选择的时间步缓存模型输出，但忽略了不同时间步之间输出差异的不均匀性。TeaCache通过调节噪声输入，利用时间步嵌入来更好地近似模型输出的差异，从而优化缓存选择。实验结果表明，TeaCache在保持视觉质量的同时，推理速度提高了4.41倍。'}}}, {'id': 'https://huggingface.co/papers/2411.19324', 'title': 'Trajectory Attention for Fine-grained Video Motion Control', 'url': 'https://huggingface.co/papers/2411.19324', 'abstract': 'Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.', 'score': 9, 'issue_id': 885, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '02a266f597ae69e7', 'authors': ['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Sensetime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19324.jpg', 'data': {'categories': ['#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль движения камеры в генеративных видеомоделях с помощью trajectory attention', 'desc': "Статья представляет новый подход под названием 'trajectory attention' для точного контроля движения камеры в генеративных видеомоделях. Метод выполняет внимание вдоль доступных траекторий пикселей, что позволяет более точно внедрять информацию о траектории в процесс генерации видео. Trajectory attention работает как вспомогательная ветвь наряду с традиционным временным вниманием, обеспечивая как точный контроль движения, так и возможность генерации нового контента. Эксперименты показывают значительные улучшения в точности и согласованности на больших расстояниях при сохранении высокого качества генерации."}, 'en': {'title': 'Enhancing Video Generation with Trajectory Attention', 'desc': 'This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks.'}, 'zh': {'title': '轨迹注意力：精确控制视频生成中的相机运动', 'desc': '本论文介绍了一种新颖的轨迹注意力机制，用于视频生成中的相机运动控制。与现有方法相比，我们的方法能够更精确地处理运动控制，并有效地结合了轨迹信息。通过将轨迹注意力作为辅助分支与传统时间注意力结合，我们的方法在生成新内容的同时，确保了运动控制的精确性。实验结果表明，该方法在图像和视频的相机运动控制中显著提高了精度和长距离一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.18552', 'title': 'FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion', 'url': 'https://huggingface.co/papers/2411.18552', 'abstract': 'Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads.', 'score': 8, 'issue_id': 892, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'dd1bf99b66f1b34d', 'authors': ['Haosen Yang', 'Adrian Bulat', 'Isma Hadji', 'Hai X. Pham', 'Xiatian Zhu', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University, UK', 'Samsung AI Center, Cambridge, UK', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2411.18552.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Гибкое масштабирование диффузионных моделей без переобучения', 'desc': 'Эта статья представляет новый метод под названием Fam diffusion для улучшения работы диффузионных моделей при изменении разрешения изображений. Авторы предлагают два модуля: Frequency Modulation (FM) для улучшения глобальной структурной согласованности и Attention Modulation (AM) для повышения согласованности локальных текстурных паттернов. Метод легко интегрируется в любую модель латентной диффузии и не требует дополнительного обучения. Результаты показывают эффективность метода в устранении структурных и локальных артефактов, демонстрируя при этом лучшую производительность по сравнению с существующими подходами.'}, 'en': {'title': 'Flexible Image Generation with Fam Diffusion', 'desc': 'This paper presents a novel approach to enhance diffusion models for image generation, allowing them to work effectively at various resolutions without retraining. The proposed Fam diffusion method introduces two key modules: Frequency Modulation (FM) for improving global structure consistency and Attention Modulation (AM) for refining local texture patterns. These modules address common issues like repetitive patterns and structural distortions that occur when using scaled resolutions. The method integrates seamlessly into existing latent diffusion models, demonstrating state-of-the-art performance with minimal latency overheads and improved image quality.'}, 'zh': {'title': '灵活分辨率下的高质量图像生成', 'desc': '扩散模型在生成高质量图像方面表现出色，但仅在训练时使用的分辨率下有效。在不同的分辨率下推理会导致重复模式和结构失真。我们提出了两个简单的模块，频率调制（FM）模块和注意力调制（AM）模块，来解决这些问题。我们的Fam扩散方法可以无缝集成到任何潜在扩散模型中，无需额外训练，并且在处理结构和局部伪影方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2411.19527', 'title': 'DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding', 'url': 'https://huggingface.co/papers/2411.19527', 'abstract': 'Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this "discord" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: https://whwjdqls.github.io/discord.github.io/.', 'score': 8, 'issue_id': 891, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'b1fc0d8f7ba13620', 'authors': ['Jungbin Cho', 'Junwan Kim', 'Jisoo Kim', 'Minseo Kim', 'Mingu Kang', 'Sungeun Hong', 'Tae-Hyun Oh', 'Youngjae Yu'], 'affiliations': ['POSTECH', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19527.jpg', 'data': {'categories': ['#video', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'DisCoRD: Мост между дискретной эффективностью и непрерывным реализмом в генерации движений', 'desc': 'Статья представляет новый метод DisCoRD для генерации движений человека. Метод объединяет преимущества дискретных и непрерывных подходов, используя дискретные токены и непрерывное декодирование через выпрямленный поток. DisCoRD применяет итеративное уточнение в непрерывном пространстве для захвата тонких динамических характеристик движения. Результаты показывают, что метод достигает наилучших показателей по метрике FID на датасетах HumanML3D и KIT-ML.'}, 'en': {'title': 'Bridging the Gap: DisCoRD for Smooth Human Motion Generation', 'desc': 'This paper addresses the challenges of generating human motion using machine learning models, particularly the limitations of discrete quantization methods like VQ-VAEs. It introduces a new method called DisCoRD, which stands for Discrete Tokens to Continuous Motion via Rectified Flow Decoding. DisCoRD effectively converts discrete motion tokens into smooth continuous motion by using an iterative refinement process in the continuous space. The results show that DisCoRD outperforms existing methods, achieving state-of-the-art performance metrics on benchmark datasets, thus providing a solution that balances discrete efficiency with continuous realism.'}, 'zh': {'title': '打破离散与连续的界限，提升运动生成自然性', 'desc': '人类运动是连续和动态的，这给生成模型带来了很大挑战。尽管离散量化方法（如VQ-VAEs）占主导地位，但它们在表达能力和帧噪声方面存在局限。我们提出了一种新方法DisCoRD，通过修正流解码将离散运动标记解码为连续运动，解决了离散和连续表示之间的矛盾。DisCoRD在连续空间中进行迭代优化，捕捉细微动态，确保运动更加平滑自然，且与任何基于离散的框架兼容。'}}}, {'id': 'https://huggingface.co/papers/2411.19950', 'title': 'AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos', 'url': 'https://huggingface.co/papers/2411.19950', 'abstract': 'We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets', 'score': 5, 'issue_id': 888, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '9f7d2daec9cb311d', 'authors': ['Yuze He', 'Wang Zhao', 'Shaohui Liu', 'Yubin Hu', 'Yushi Bai', 'Yu-Hui Wen', 'Yong-Jin Liu'], 'affiliations': ['Beijing Jiaotong University', 'ETH Zurich', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19950.jpg', 'data': {'categories': ['#3d'], 'emoji': '📐', 'ru': {'title': 'AlphaTablets: революция в представлении 3D плоскостей', 'desc': 'В статье представлен AlphaTablets - новый подход к представлению трехмерных плоскостей в виде прямоугольников с альфа-каналами. Это позволяет сочетать преимущества существующих 2D и 3D представлений, обеспечивая точное и гибкое моделирование 3D плоскостей. Авторы разработали дифференцируемую растеризацию для эффективной визуализации 3D плоскостей и предложили новый алгоритм 3D реконструкции плоскостей из монокулярных видео. Эксперименты на наборе данных ScanNet показали превосходные результаты в задаче 3D реконструкции плоскостей.'}, 'en': {'title': 'AlphaTablets: Revolutionizing 3D Plane Representation', 'desc': 'AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset.'}, 'zh': {'title': 'AlphaTablets：3D平面重建的新方法', 'desc': '我们介绍了AlphaTablets，这是一种新颖且通用的3D平面表示方法，具有连续的3D表面和精确的边界划分。通过将3D平面表示为带有alpha通道的矩形，AlphaTablets结合了当前2D和3D平面表示的优点，实现了3D平面的准确、一致和灵活建模。我们在AlphaTablets的基础上推导出可微分光栅化技术，以高效地将3D平面渲染为图像，并提出了一种新颖的自下而上的单目视频3D平面重建管道。通过迭代优化和合并，我们能够重建出完整且准确的3D平面，具有坚实的表面和清晰的边界。'}}}, {'id': 'https://huggingface.co/papers/2411.19460', 'title': 'Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing', 'url': 'https://huggingface.co/papers/2411.19460', 'abstract': 'With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.', 'score': 5, 'issue_id': 886, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'b96751a3db484750', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.19460.jpg', 'data': {'categories': ['#architecture', '#long_context', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Эффективная обработка длинных видео с помощью Video-Ma^2mba', 'desc': 'Статья представляет Video-Ma^2mba - новую архитектуру для обработки длинных видеопоследовательностей, использующую модели пространства состояний вместо механизмов внимания. Это позволяет линейно масштабировать большие мультимодальные модели (LMM) по времени и памяти. Авторы также вводят метод мультиосевого градиентного чекпойнтинга (MA-GC) для повышения эффективности использования памяти. Эмпирические исследования показывают, что Video-Ma^2mba может обрабатывать длинные видеопоследовательности на одном GPU, улучшая точность и релевантность ответов в задачах понимания длинных видео.'}, 'en': {'title': 'Revolutionizing Long Video Processing with Linear Scalability', 'desc': 'The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models.'}, 'zh': {'title': '高效处理长视频序列的新方法', 'desc': '随着视频数据规模和复杂性的增加，处理长视频序列面临着显著的挑战。我们提出了一种新架构Video-Ma^2mba，它在Mamba-2框架中引入了状态空间模型（SSMs），替代了传统的注意力机制，从而使得大规模多模态模型（LMMs）在时间和内存需求上实现线性扩展。我们还引入了多轴梯度检查点（MA-GC）方法，优化内存管理，仅保留必要的激活信息，显著降低了内存占用。实验结果表明，Video-Ma^2mba能够在单个GPU上处理相当于数百万个标记或超过两小时的连续视频序列，提升了长视频理解任务的准确性和相关性。'}}}, {'id': 'https://huggingface.co/papers/2411.19865', 'title': 'Reverse Thinking Makes LLMs Stronger Reasoners', 'url': 'https://huggingface.co/papers/2411.19865', 'abstract': "Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.", 'score': 4, 'issue_id': 899, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '8f066f57ddff0ae8', 'authors': ['Justin Chih-Yao Chen', 'Zifeng Wang', 'Hamid Palangi', 'Rujun Han', 'Sayna Ebrahimi', 'Long Le', 'Vincent Perot', 'Swaroop Mishra', 'Mohit Bansal', 'Chen-Yu Lee', 'Tomas Pfister'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.19865.jpg', 'data': {'categories': ['#data', '#training', '#transfer_learning', '#small_models', '#dataset', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'RevThink: Усиление LLM обратным мышлением', 'desc': 'Статья представляет новый метод под названием Reverse-Enhanced Thinking (RevThink) для улучшения рассуждений больших языковых моделей (LLM). RevThink использует обратное мышление, дополняя набор данных структурированными прямыми и обратными рассуждениями от модели-учителя. Метод включает три задачи обучения: генерация прямых рассуждений, обратных вопросов и обратных рассуждений. Эксперименты показали значительное улучшение производительности на различных наборах данных по сравнению с базовыми методами.'}, 'en': {'title': 'Empowering LLMs with Reverse Reasoning for Enhanced Performance', 'desc': 'This paper introduces Reverse-Enhanced Thinking (RevThink), a framework designed to improve Large Language Models (LLMs) by enabling them to perform reverse reasoning. RevThink enhances reasoning performance by augmenting datasets with structured forward and backward reasoning examples, allowing models to learn from both directions. The framework employs multi-task learning objectives to train a smaller student model, focusing on generating forward reasoning, backward questions, and backward reasoning. Experimental results show significant improvements in reasoning tasks, demonstrating the effectiveness and efficiency of RevThink in enhancing model performance with limited data.'}, 'zh': {'title': '反向思维：提升推理能力的新方法', 'desc': '反向思维在人的推理中起着重要作用。本文提出了一种名为反向增强思维（RevThink）的框架，旨在使大型语言模型（LLMs）能够进行反向推理。该框架通过数据增强和学习目标来实现，收集结构化的前向和后向推理数据。实验结果表明，RevThink在多个数据集上显著提高了模型的推理性能，并展示了良好的样本效率和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.19638', 'title': 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification', 'url': 'https://huggingface.co/papers/2411.19638', 'abstract': "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.", 'score': 4, 'issue_id': 889, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '98bf5f113194343b', 'authors': ['Taja Kuzman', 'Nikola Ljubešić'], 'affiliations': ['Department of Knowledge Technologies, Jožef Stefan Institute, 1000 Ljubljana, Slovenia', 'Jožef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia', 'University of Ljubljana, 1000 Ljubljana, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2411.19638.jpg', 'data': {'categories': ['#machine_translation', '#training', '#low_resource', '#multilingual', '#dataset'], 'emoji': '📰', 'ru': {'title': 'Эффективная многоязычная классификация новостей без ручной разметки', 'desc': "Статья представляет новый подход к многоязычной классификации новостей с использованием модели 'учитель-ученик'. Большая языковая модель GPT выступает в роли учителя, автоматически аннотируя новостные статьи на четырех языках. Модели-ученики на основе BERT обучаются на этих данных, достигая высокой производительности при меньших вычислительных затратах. Исследование показывает эффективность метода для многоязычной и кросс-языковой классификации новостей."}, 'en': {'title': 'Empowering Multilingual News Classification with Teacher-Student LLMs', 'desc': 'This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages.'}, 'zh': {'title': '多语言新闻分类的新方法', 'desc': '随着在线新闻数量的不断增加，按主题对新闻进行分类变得至关重要。本文提出了一种基于大型语言模型的教师-学生框架，用于开发多语言新闻分类模型，且无需手动数据标注。教师模型使用生成预训练变换器（GPT）自动标注新闻文章，展示出在多种语言上的高零样本性能。通过微调较小的BERT类学生模型，这些模型在相对较少的训练实例下也能达到与教师模型相当的高性能。'}}}, {'id': 'https://huggingface.co/papers/2411.18673', 'title': 'AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.18673', 'abstract': 'Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.', 'score': 4, 'issue_id': 886, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '1ea35d3552a278a3', 'authors': ['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov'], 'affiliations': ['SFU', 'Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2411.18673.jpg', 'data': {'categories': ['#dataset', '#architecture', '#diffusion', '#games', '#3d', '#training', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Прецизионное управление 3D-камерой в генеративных видеомоделях', 'desc': 'Эта статья представляет новый подход к управлению 3D-камерой в генеративных видеомоделях. Авторы анализируют движение камеры с фундаментальной точки зрения и предлагают несколько улучшений, включая оптимизацию графиков обучения и тестирования, ограничение внедрения условий камеры в определенные слои архитектуры и использование специально подобранного набора данных для обучения. Результатом является архитектура Advanced 3D Camera Control (AC3D), которая обеспечивает более точное управление камерой без ущерба для качества синтеза видео. Модель AC3D достигает нового уровня генеративного видеомоделирования с управлением камерой.'}, 'en': {'title': 'Precision in 3D Camera Control for Enhanced Video Generation', 'desc': 'This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling.'}, 'zh': {'title': '先进的3D相机控制，提升视频生成质量', 'desc': '本研究分析了3D相机控制在文本到视频模型中的应用，发现相机运动对视频生成质量有显著影响。我们提出了一种新的训练和测试姿态调节策略，以提高训练收敛速度和视频的视觉质量。通过对无条件视频扩散变换器的表示进行探测，我们发现相机姿态估计在模型内部隐式执行，因此我们限制了相机条件的注入，以减少对其他视频特征的干扰。最终，我们设计了先进的3D相机控制架构（AC3D），成为具有相机控制的生成视频建模的新一代模型。'}}}, {'id': 'https://huggingface.co/papers/2411.19842', 'title': 'Scaling Transformers for Low-Bitrate High-Quality Speech Coding', 'url': 'https://huggingface.co/papers/2411.19842', 'abstract': 'The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of 400 or 700 bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.', 'score': 3, 'issue_id': 900, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '23e49aedef71b878', 'authors': ['Julian D Parker', 'Anton Smirnov', 'Jordi Pons', 'CJ Carr', 'Zack Zukowski', 'Zach Evans', 'Xubo Liu'], 'affiliations': ['Stability AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.19842.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#audio', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Трансформеры покоряют токенизацию речи', 'desc': 'Данная статья описывает новый подход к токенизации речи с использованием нейронных аудиокодеков. Авторы представляют модель на основе трансформера с большим количеством параметров и применением гибкого квантования FSQ. Эта архитектура позволяет достичь высокого качества речи при крайне низких битрейтах в 400-700 бит/с. Результаты значительно превосходят существующие базовые модели как в объективных, так и в субъективных тестах.'}, 'en': {'title': 'Transforming Speech Quality with Scalable Neural Models', 'desc': 'This paper discusses the importance of tokenizing speech using neural audio codec models in AI systems. It highlights a new approach that utilizes a large-scale transformer architecture combined with Finite Scalar Quantization (FSQ) to improve speech quality. The proposed method achieves impressive results, delivering high-quality speech at very low bit-rates of 400 or 700 bits-per-second. The models developed in this study significantly outperform existing methods in both objective measures and subjective evaluations.'}, 'zh': {'title': '通过扩展变换器实现高质量低比特率语音标记化', 'desc': '本论文探讨了使用神经音频编解码模型对语音进行标记化的重要性。传统的标记化模型通常采用低参数量的架构，依赖于强的归纳偏置。我们展示了通过扩展变换器架构并应用灵活的有限标量量化（FSQ）瓶颈，可以在极低的比特率下实现最先进的语音质量。训练后的模型在客观和主观测试中均显著超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2411.18664', 'title': 'Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling', 'url': 'https://huggingface.co/papers/2411.18664', 'abstract': 'Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.', 'score': 2, 'issue_id': 900, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '3576f88bbf3e4567', 'authors': ['Junha Hyung', 'Kinam Kim', 'Susung Hong', 'Min-Jung Kim', 'Jaegul Choo'], 'affiliations': ['KAIST AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.18664.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#3d', '#training', '#video'], 'emoji': '🎬', 'ru': {'title': 'STG: Улучшение качества видео без потери разнообразия', 'desc': 'Статья представляет новый метод улучшения качества генерации видео с помощью диффузионных моделей - Spatiotemporal Skip Guidance (STG). STG не требует дополнительного обучения и использует самовозмущение для создания слабой модели. Метод избирательно пропускает пространственно-временные слои, что позволяет улучшить качество сэмплов без ущерба для разнообразия и динамики. STG превосходит существующие методы, такие как CFG и автогайденс, не требуя при этом дополнительных моделей или обучения.'}, 'en': {'title': 'Enhancing Video Diffusion with Spatiotemporal Skip Guidance', 'desc': 'This paper presents Spatiotemporal Skip Guidance (STG), a novel method for improving video diffusion models without requiring additional training or external models. STG enhances the quality of generated videos by using a self-perturbation technique that simulates a weak model through selective skipping of spatiotemporal layers. This approach allows for better sample quality while maintaining diversity and dynamic motion, addressing the limitations of existing methods like CFG. The authors demonstrate that STG is an efficient and effective guidance technique that enhances the performance of transformer-based video diffusion models.'}, 'zh': {'title': '时空跳跃引导：提升视频扩散模型的高效方法', 'desc': '扩散模型已成为生成高质量图像、视频和3D内容的强大工具。虽然采样引导技术如CFG可以提高质量，但会降低多样性和动态性。自引导方法虽然可以缓解这些问题，但需要额外的弱模型训练，限制了其在大规模模型中的实用性。我们提出了时空跳跃引导（STG），这是一种简单的无训练采样引导方法，旨在增强基于变换器的视频扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2411.18092', 'title': 'Training Noise Token Pruning', 'url': 'https://huggingface.co/papers/2411.18092', 'abstract': "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to Rate-Distortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNT's advantages over previous pruning methods.", 'score': 1, 'issue_id': 905, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '570d01745d1c7f3d', 'authors': ['Mingxing Rao', 'Bohan Jiang', 'Daniel Moyer'], 'affiliations': ['Vanderbilt University, Nashville, TN 37235, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.18092.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#cv', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Плавная обрезка трансформеров с помощью шума', 'desc': 'В этой работе представлен метод обрезки трансформеров для компьютерного зрения под названием Training Noise Token (TNT). Метод заменяет дискретное отбрасывание токенов на добавление непрерывного шума, что обеспечивает плавную оптимизацию при обучении, сохраняя при этом преимущества дискретного отбрасывания при развертывании. Авторы приводят теоретические связи с литературой по скорости-искажению и эмпирические оценки на наборе данных ImageNet с использованием архитектур ViT и DeiT. Результаты демонстрируют преимущества TNT перед предыдущими методами обрезки.'}, 'en': {'title': 'Smooth Optimization with TNT Pruning for Vision Transformers', 'desc': 'This paper introduces a novel approach called Training Noise Token (TNT) Pruning for vision transformers, which enhances the training process by allowing continuous additive noise instead of strictly dropping tokens. This method enables smoother optimization during training while still benefiting from the computational efficiency of discrete token dropping during deployment. The authors establish theoretical links to Rate-Distortion theory, which helps to understand the trade-offs involved in token pruning. Empirical results on the ImageNet dataset show that TNT Pruning outperforms existing pruning techniques when applied to ViT and DeiT architectures.'}, 'zh': {'title': '训练噪声标记剪枝：优化与效率的结合', 'desc': '本文提出了一种用于视觉变换器的训练噪声标记（TNT）剪枝方法。我们的方法将离散的标记丢弃条件放宽为连续的加性噪声，从而在训练中实现平滑优化，同时在部署环境中保留离散丢弃的计算优势。我们还提供了与率失真文献的理论联系，并在ImageNet数据集上使用ViT和DeiT架构进行了实证评估，展示了TNT相较于之前剪枝方法的优势。'}}}, {'id': 'https://huggingface.co/papers/2411.18665', 'title': 'SpotLight: Shadow-Guided Object Relighting via Diffusion', 'url': 'https://huggingface.co/papers/2411.18665', 'abstract': 'Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting.', 'score': 1, 'issue_id': 903, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '1b31caf705bc142d', 'authors': ['Frédéric Fortier-Chouinard', 'Zitian Zhang', 'Louis-Etienne Messier', 'Mathieu Garon', 'Anand Bhattad', 'Jean-François Lalonde'], 'affiliations': ['Depix Technologies', 'Toyota Technological Institute at Chicago', 'Universite Laval'], 'pdf_title_img': 'assets/pdf/title_img/2411.18665.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion'], 'emoji': '💡', 'ru': {'title': 'Контроль освещения в нейронном рендеринге через тени объектов', 'desc': 'Статья описывает метод SpotLight для контролируемого освещения объектов в нейронном рендеринге с использованием диффузионных моделей. Авторы показывают, что добавление только тени объекта в предобученный нейронный рендерер позволяет точно затенять объект в соответствии с желаемым положением источника света. Метод SpotLight не требует дополнительного обучения и может использоваться с существующими нейронными рендерерами. Согласно проведенному исследованию, SpotLight превосходит существующие диффузионные модели, специально разработанные для перестановки освещения.'}, 'en': {'title': 'SpotLight: Control Lighting with Shadows in Neural Rendering', 'desc': "This paper introduces SpotLight, a method that enhances neural rendering by allowing precise control over lighting for virtual objects in images. It achieves this by enabling users to specify the desired shadows of the object, which the diffusion model uses to accurately shade the object based on the light's position. SpotLight integrates seamlessly with existing pre-trained diffusion-based neural renderers, requiring no additional training. The results demonstrate significant improvements in object compositing, both in quantitative metrics and user perception, compared to traditional diffusion models designed for relighting."}, 'zh': {'title': 'SpotLight：精准控制虚拟物体光照的创新方法', 'desc': '本论文介绍了一种名为SpotLight的方法，利用扩散模型进行虚拟物体的重光照。与传统的物理渲染器不同，SpotLight通过仅指定物体的阴影来实现精确的光照控制。我们的方法能够在不需要额外训练的情况下，准确地根据所需的光源位置为物体上色，并与背景图像和谐融合。实验结果表明，SpotLight在物体合成效果上优于现有的扩散模型，得到了用户研究的支持。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (8)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (17)', '#audio (4)', '#benchmark (20)', '#cv (11)', '#data (8)', '#dataset (23)', '#diffusion (15)', '#ethics (1)', '#games (8)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (9)', '#interpretability (5)', '#leakage (1)', '#long_context (4)', '#low_resource (4)', '#machine_translation (1)', '#math (3)', '#multilingual (3)', '#multimodal (16)', '#open_source (9)', '#optimization (23)', '#plp', '#rag (1)', '#reasoning (9)', '#rl (1)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models (3)', '#story_generation (1)', '#survey (2)', '#synthetic (7)', '#training (24)', '#transfer_learning (3)', '#video (20)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-04 10:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-04 10:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-04 10:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    