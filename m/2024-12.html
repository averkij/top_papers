
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 104 papers. December 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Декабрь 2024</span> | <span id="title-articles-count">104 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2024-11.html">⬅️ <span id="prev-date">11.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-01.html">➡️ <span id="next-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Декабрь 2024', 'en': 'December 2024', 'zh': '12月2024年'};
        let feedDateNext = {'ru': '01.2025', 'en': '01/2025', 'zh': '1月2025年'};
        let feedDatePrev = {'ru': '11.2024', 'en': '11/2024', 'zh': '11月2024年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.04467', 'title': 'VisionZip: Longer is Better but Not Necessary in Vision Language Models', 'url': 'https://huggingface.co/papers/2412.04467', 'abstract': 'Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .', 'score': 48, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '5539efbb0d3e8e80', 'authors': ['Senqiao Yang', 'Yukang Chen', 'Zhuotao Tian', 'Chengyao Wang', 'Jingyao Li', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HITSZ', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2412.04467.jpg', 'data': {'categories': ['#inference', '#interpretability', '#multimodal', '#optimization', '#cv'], 'emoji': '🗜️', 'ru': {'title': 'VisionZip: Сжимаем визуальные токены, ускоряем ИИ', 'desc': 'VisionZip - это новый метод, который улучшает эффективность визуально-языковых моделей путем выбора наиболее информативных визуальных токенов. Он решает проблему избыточности токенов, генерируемых популярными кодировщиками изображений, такими как CLIP и SigLIP. VisionZip значительно повышает производительность и скорость вывода моделей, особенно в задачах понимания изображений и видео, а также в многоходовых диалогах. Авторы призывают сообщество сосредоточиться на извлечении лучших визуальных признаков, а не просто на увеличении длины токенов.'}, 'en': {'title': 'Streamlining Visual Tokens for Enhanced Efficiency in Vision-Language Models', 'desc': 'This paper presents VisionZip, a method designed to reduce redundancy in visual tokens used in vision-language models. By selecting only the most informative tokens, VisionZip improves computational efficiency without sacrificing performance. The method shows significant improvements in both model inference speed and overall accuracy, outperforming previous state-of-the-art techniques by at least 5%. The authors emphasize the importance of optimizing visual feature extraction rather than simply increasing token length.'}, 'zh': {'title': 'VisionZip：高效减少视觉标记冗余的创新方法', 'desc': '最近，视觉语言模型的进展通过增加视觉标记的长度来提高性能，但这也显著增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在显著的冗余。为了解决这个问题，我们提出了VisionZip，这是一种简单而有效的方法，可以选择一组信息丰富的标记输入到语言模型中，从而减少视觉标记的冗余，提高效率，同时保持模型性能。实验结果表明，VisionZip在几乎所有设置中比之前的最先进方法提高了至少5%的性能，并显著提升了模型推理速度。'}}}, {'id': 'https://huggingface.co/papers/2412.04455', 'title': 'Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection', 'url': 'https://huggingface.co/papers/2412.04455', 'abstract': 'Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as a unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuracy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and a real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments.', 'score': 30, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'a00b427eb116e4bf', 'authors': ['Enshen Zhou', 'Qi Su', 'Cheng Chi', 'Zhizheng Zhang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'He Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University', 'School of Software, Beihang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04455.jpg', 'data': {'categories': ['#agents', '#robotics', '#optimization', '#cv', '#security'], 'emoji': '🤖', 'ru': {'title': 'Код как монитор: умное обнаружение ошибок для роботов', 'desc': 'В статье представлен метод Code-as-Monitor (CaM), использующий визуально-языковую модель для обнаружения и предотвращения ошибок в робототехнических системах. CaM формулирует задачи как набор пространственно-временных ограничений и использует сгенерированный код для их оценки в режиме реального времени. Метод вводит элементы ограничений для абстрагирования связанных сущностей, что упрощает отслеживание и облегчает визуальное программирование. Эксперименты показывают, что CaM превосходит базовые методы по успешности и времени выполнения задач в различных условиях.'}, 'en': {'title': 'Revolutionizing Robotic Failure Detection with Code-as-Monitor', 'desc': 'This paper presents Code-as-Monitor (CaM), a new method for detecting and preventing failures in robotic systems that operate in unpredictable environments. CaM uses a vision-language model (VLM) to handle both reactive and proactive failure detection by treating these tasks as spatio-temporal constraint satisfaction problems. The method improves monitoring accuracy and efficiency by introducing compact geometric elements that represent constraints, making it easier to track and manage them visually. Experimental results demonstrate that CaM significantly outperforms existing methods, achieving higher success rates and faster execution times in both simulated and real-world scenarios.'}, 'zh': {'title': '智能监控：提升机器人系统的故障检测与预防', 'desc': '本文提出了一种名为Code-as-Monitor（CaM）的新方法，用于自动检测和预防闭环机器人系统中的开放集故障。该方法利用视觉-语言模型（VLM）将反应性和主动性故障检测任务统一为时空约束满足问题。通过生成的代码进行实时监控，CaM在准确性和效率上都有显著提升。实验结果表明，CaM在严重干扰下的成功率提高了28.7%，执行时间减少了31.8%。'}}}, {'id': 'https://huggingface.co/papers/2412.04454', 'title': 'Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction', 'url': 'https://huggingface.co/papers/2412.04454', 'abstract': 'Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/.', 'score': 23, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'a088657cce2c618c', 'authors': ['Yiheng Xu', 'Zekun Wang', 'Junli Wang', 'Dunjie Lu', 'Tianbao Xie', 'Amrita Saha', 'Doyen Sahoo', 'Tao Yu', 'Caiming Xiong'], 'affiliations': ['Salesforce Research', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.04454.jpg', 'data': {'categories': ['#open_source', '#games', '#multimodal', '#agents', '#training', '#reasoning', '#dataset'], 'emoji': '🖥️', 'ru': {'title': 'Aguvis: Автономный ГИП-агент на чистом компьютерном зрении', 'desc': 'Авторы представляют Aguvis - унифицированную систему для автономных агентов графического интерфейса пользователя (ГИП), работающую на основе компьютерного зрения. Система использует наблюдения на основе изображений и привязку инструкций на естественном языке к визуальным элементам, что обеспечивает кросс-платформенную генерализацию. Aguvis включает явное планирование и рассуждение для улучшения навигации в сложных цифровых средах. Эксперименты показывают, что Aguvis превосходит существующие методы в офлайн и онлайн сценариях, достигая полной автономности без использования внешних моделей.'}, 'en': {'title': 'Aguvis: Revolutionizing Autonomous GUI Interaction with Pure Vision', 'desc': 'This paper presents Aguvis, a novel framework designed for autonomous GUI agents that operates purely on visual inputs. Unlike previous methods that depend on textual representations, Aguvis utilizes image-based observations and natural language instructions to interact with GUI elements. The framework incorporates explicit planning and reasoning, allowing it to effectively navigate and perform tasks in complex digital environments. Through extensive testing, Aguvis demonstrates superior performance compared to existing methods, marking a significant advancement in the development of fully autonomous GUI agents.'}, 'zh': {'title': 'Aguvis：完全自主的视觉GUI代理', 'desc': '本论文介绍了一种名为Aguvis的框架，旨在自动化图形用户界面（GUI）任务。该框架基于纯视觉的方法，能够在不同平台上操作，克服了传统方法的局限性。Aguvis通过图像观察和自然语言指令的结合，增强了模型的规划和推理能力，使其能够独立导航和与复杂数字环境互动。实验结果表明，Aguvis在离线和在线场景中均超越了现有的最先进方法，成为首个完全自主的纯视觉GUI代理。'}}}, {'id': 'https://huggingface.co/papers/2412.03895', 'title': 'A Noise is Worth Diffusion Guidance', 'url': 'https://huggingface.co/papers/2412.03895', 'abstract': "Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruct high-quality images without guidance, we focus on the initial noise of the denoising pipeline. By mapping Gaussian noise to `guidance-free noise', we uncover that small low-magnitude low-frequency components significantly enhance the denoising process, removing the need for guidance and thus improving both inference throughput and memory. Expanding on this, we propose \\ours, a novel method that replaces guidance methods with a single refinement of the initial noise. This refined noise enables high-quality image generation without guidance, within the same diffusion pipeline. Our noise-refining model leverages efficient noise-space learning, achieving rapid convergence and strong performance with just 50K text-image pairs. We validate its effectiveness across diverse metrics and analyze how refined noise can eliminate the need for guidance. See our project page: https://cvlab-kaist.github.io/NoiseRefine/.", 'score': 19, 'issue_id': 985, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '1e3bc318f7457d8c', 'authors': ['Donghoon Ahn', 'Jiwon Kang', 'Sanghyun Lee', 'Jaewon Min', 'Minjae Kim', 'Wooseok Jang', 'Hyoungwon Cho', 'Sayak Paul', 'SeonHwa Kim', 'Eunju Cha', 'Kyong Hwan Jin', 'Seungryong Kim'], 'affiliations': ['Hugging Face', 'KAIST', 'Korea University', 'Sookmyung Womens University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03895.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': '🎨', 'ru': {'title': 'Генерация изображений без наведения: революция в диффузионных моделях', 'desc': 'Статья представляет новый метод генерации изображений с помощью диффузионных моделей без использования техник наведения. Авторы предлагают подход NoiseRefine, который заменяет методы наведения однократным уточнением начального шума. Этот метод позволяет получать качественные изображения без наведения, используя тот же диффузионный конвейер. Модель уточнения шума эффективно обучается на пространстве шумов, достигая быстрой сходимости всего на 50 тысячах пар текст-изображение.'}, 'en': {'title': 'Guidance-Free Image Generation with Noise Refinement', 'desc': 'This paper investigates the necessity of guidance methods in diffusion models for image generation. It reveals that high-quality images can be produced by refining the initial noise in the denoising process, eliminating the reliance on techniques like classifier-free guidance. The authors introduce a new method called \textit{NoiseRefine}, which enhances the denoising process by focusing on low-magnitude low-frequency components of noise. Their approach demonstrates improved efficiency in image generation, achieving strong results with fewer training samples and without the need for additional guidance methods.'}, 'zh': {'title': '无指导生成高质量图像的新方法', 'desc': '扩散模型在生成高质量图像方面表现出色，但目前的扩散模型在没有指导方法的情况下难以生成可靠的图像。我们发现，通过扩散反演获得的噪声可以在没有指导的情况下重建高质量图像，因此我们关注去噪流程中的初始噪声。我们提出了一种新方法\textit{ours}，通过对初始噪声进行单次精炼，替代了传统的指导方法，从而在同一扩散流程中实现高质量图像生成。我们的模型利用高效的噪声空间学习，快速收敛并在仅使用50K文本-图像对的情况下取得了良好的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.04424', 'title': 'Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion', 'url': 'https://huggingface.co/papers/2412.04424', 'abstract': 'We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2\'s visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose "depth-breath fusion (DBFusion)" to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VL\'s visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. https://github.com/JiuhaiChen/Florence-VL', 'score': 17, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '18a7fac6be50215d', 'authors': ['Jiuhai Chen', 'Jianwei Yang', 'Haiping Wu', 'Dianqi Li', 'Jianfeng Gao', 'Tianyi Zhou', 'Bin Xiao'], 'affiliations': ['Microsoft Research', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2412.04424.jpg', 'data': {'categories': ['#alignment', '#training', '#benchmark', '#hallucinations', '#open_source', '#architecture', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Florence-VL: Новый уровень понимания изображений для языковых моделей', 'desc': 'Florence-VL представляет собой новое семейство мультимодальных больших языковых моделей (MLLM) с улучшенными визуальными представлениями, созданными с помощью Florence-2 - генеративной базовой модели компьютерного зрения. В отличие от широко используемых трансформеров CLIP, Florence-2 способна захватывать различные уровни и аспекты визуальных характеристик. Авторы предлагают новую архитектуру слияния признаков и инновационный рецепт обучения, эффективно интегрирующий визуальные характеристики Florence-2 в предобученные языковые модели. Florence-VL достигает значительных улучшений по сравнению с существующими передовыми MLLM в различных мультимодальных и ориентированных на зрение тестах.'}, 'en': {'title': 'Florence-VL: Bridging Vision and Language with Depth-Breath Fusion', 'desc': 'Florence-VL is a new type of multimodal large language model that enhances visual understanding using advanced features from the Florence-2 vision model. Unlike traditional models that rely on contrastive learning, Florence-2 captures a wider range of visual details, making it adaptable for various tasks. The model employs a unique feature-fusion method called depth-breath fusion (DBFusion) to combine visual information from different levels and prompts effectively. As a result, Florence-VL outperforms existing models in multiple benchmarks related to vision and language tasks, and its training methods are available for further research.'}, 'zh': {'title': 'Florence-VL：多模态语言模型的新突破', 'desc': 'Florence-VL是一种新型的多模态大型语言模型，结合了Florence-2生成的丰富视觉表示。与传统的对比学习训练的CLIP风格视觉变换器不同，Florence-2能够捕捉不同层次和方面的视觉特征，更加灵活地适应各种下游任务。我们提出了一种新颖的特征融合架构和创新的训练方案，有效地将Florence-2的视觉特征整合到预训练的语言模型中。Florence-VL在多种多模态和视觉中心基准测试中显著提升了性能，展示了其在视觉-语言对齐方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2412.01339', 'title': 'Negative Token Merging: Image-based Adversarial Feature Guidance', 'url': 'https://huggingface.co/papers/2412.01339', 'abstract': 'Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual elements like copyrighted characters. In this paper, for the first time we explore an alternate modality in this direction by performing adversarial guidance directly using visual features from a reference image or other images in a batch. In particular, we introduce negative token merging (NegToMe), a simple but effective training-free approach which performs adversarial guidance by selectively pushing apart matching semantic features (between reference and output generation) during the reverse diffusion process. When used w.r.t. other images in the same batch, we observe that NegToMe significantly increases output diversity (racial, gender, visual) without sacrificing output image quality. Similarly, when used w.r.t. a reference copyrighted asset, NegToMe helps reduce visual similarity with copyrighted content by 34.57%. NegToMe is simple to implement using just few-lines of code, uses only marginally higher (<4%) inference times and generalizes to different diffusion architectures like Flux, which do not natively support the use of a separate negative prompt. Code is available at https://negtome.github.io', 'score': 15, 'issue_id': 982, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '2cfa8e0ec05b9ae1', 'authors': ['Jaskirat Singh', 'Lindsey Li', 'Weijia Shi', 'Ranjay Krishna', 'Yejin Choi', 'Pang Wei Koh', 'Michael F. Cohen', 'Stephen Gould', 'Liang Zheng', 'Luke Zettlemoyer'], 'affiliations': ['Allen Institute for AI', 'Australian National University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.01339.jpg', 'data': {'categories': ['#training', '#cv', '#ethics', '#multimodal', '#security', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'NegToMe: визуальное руководство для диффузионных моделей', 'desc': 'Данная статья представляет новый метод под названием NegToMe (negative token merging) для улучшения генерации изображений с помощью диффузионных моделей. В отличие от традиционного подхода с использованием текстовых негативных промптов, NegToMe применяет визуальные признаки референсных изображений для направленного изменения генерации. Метод позволяет увеличить разнообразие выходных изображений и уменьшить их сходство с защищенными авторским правом материалами. NegToMe не требует дополнительного обучения модели и может быть легко реализован с минимальными затратами вычислительных ресурсов.'}, 'en': {'title': 'Enhancing Image Diversity and Copyright Safety with NegToMe', 'desc': 'This paper introduces a new method called Negative Token Merging (NegToMe) for adversarial guidance in image generation. Unlike traditional methods that rely solely on text prompts, NegToMe uses visual features from reference images to steer the output away from unwanted concepts. The approach selectively separates matching semantic features during the reverse diffusion process, enhancing the diversity of generated images while maintaining quality. Additionally, it effectively reduces visual similarity to copyrighted content, making it a practical tool for creators.'}, 'zh': {'title': '通过视觉特征实现对抗引导的创新方法', 'desc': '本文提出了一种新的对抗引导方法，称为负令牌合并（NegToMe），通过直接使用参考图像的视觉特征来推动输出特征远离不希望的概念。与仅使用文本进行对抗引导相比，这种方法能够更好地捕捉复杂的视觉概念，并有效避免版权角色等不希望的视觉元素。实验表明，NegToMe显著提高了输出的多样性，同时保持了图像质量，并且在减少与版权内容的视觉相似性方面表现出色。该方法实现简单，代码量少，且对不同的扩散架构具有良好的通用性。'}}}, {'id': 'https://huggingface.co/papers/2412.03679', 'title': 'Evaluating Language Models as Synthetic Data Generators', 'url': 'https://huggingface.co/papers/2412.03679', 'abstract': "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.", 'score': 15, 'issue_id': 980, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '2b80634d3f712590', 'authors': ['Seungone Kim', 'Juyoung Suk', 'Xiang Yue', 'Vijay Viswanathan', 'Seongyun Lee', 'Yizhong Wang', 'Kiril Gashteovski', 'Carolin Lawrence', 'Sean Welleck', 'Graham Neubig'], 'affiliations': ['Carnegie Mellon University', 'KAIST AI', 'NEC Laboratories Europe', 'Ss. Cyril and Methodius University of Skopje', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.03679.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#data'], 'emoji': '🧠', 'ru': {'title': 'AgoraBench: Новый взгляд на генерацию данных языковыми моделями', 'desc': 'В статье представлен AgoraBench - новый бенчмарк для оценки способности языковых моделей генерировать синтетические данные. Исследователи провели масштабный эксперимент, сгенерировав 1,26 миллиона обучающих примеров с помощью 6 различных ЯМ и обучив 99 студенческих моделей. Результаты показали, что разные ЯМ имеют свои сильные стороны в генерации данных, и эта способность не всегда коррелирует с навыками решения задач. Авторы также выявили ключевые факторы, влияющие на качество генерируемых данных, включая формат вывода и выбор модели.'}, 'en': {'title': 'Unlocking the Power of Language Models in Data Generation', 'desc': "This paper introduces AgoraBench, a benchmark designed to evaluate the data generation capabilities of various language models (LMs). It highlights the importance of synthetic data in enhancing the performance of LMs, especially in post-training scenarios. The study synthesizes a large dataset using six different LMs and trains 99 student models to analyze their data generation strengths and weaknesses. Key findings indicate that while some LMs excel in creating new problems, others are better at refining existing ones, and that data generation quality is influenced by several intrinsic features rather than just the LM's problem-solving skills."}, 'zh': {'title': '评估语言模型数据生成能力的新基准', 'desc': '随着合成数据在语言模型后期训练中的使用增加，语言模型生成高质量数据的能力变得与直接解决问题的能力同样重要。以往的研究主要集中在开发有效的数据生成方法，但缺乏对不同语言模型作为数据生成器的系统比较。为了解决这个问题，我们提出了AgoraBench，一个提供标准化设置和指标的基准，用于评估语言模型的数据生成能力。我们的研究发现，不同语言模型在数据生成方面具有独特的优势，并且数据生成能力与解决问题的能力并不总是相关，而是与数据质量的多个内在特征有关。'}}}, {'id': 'https://huggingface.co/papers/2412.01506', 'title': 'Structured 3D Latents for Scalable and Versatile 3D Generation', 'url': 'https://huggingface.co/papers/2412.01506', 'abstract': 'We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.', 'score': 14, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'c35e5586d464b27c', 'authors': ['Jianfeng Xiang', 'Zelong Lv', 'Sicheng Xu', 'Yu Deng', 'Ruicheng Wang', 'Bowen Zhang', 'Dong Chen', 'Xin Tong', 'Jiaolong Yang'], 'affiliations': ['Microsoft Research', 'Tsinghua University', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2412.01506.jpg', 'data': {'categories': ['#3d', '#dataset', '#training', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'Универсальная генерация 3D-объектов с помощью структурированного латентного представления', 'desc': 'Статья представляет новый метод генерации 3D-объектов высокого качества. Основой метода является унифицированное структурированное латентное представление (SLAT), позволяющее декодировать различные выходные форматы. SLAT интегрирует разреженную 3D-сетку с плотными мультиракурсными визуальными признаками, извлеченными из мощной фундаментальной модели компьютерного зрения. Для генерации 3D-объектов используются трансформеры с выпрямленным потоком, адаптированные для SLAT, обученные на большом наборе данных из 500 тысяч разнообразных объектов.'}, 'en': {'title': 'Revolutionizing 3D Asset Creation with SLAT!', 'desc': 'This paper presents a new method for creating high-quality 3D assets using a unified Structured LATent (SLAT) representation. The SLAT allows for flexible decoding into various formats like Radiance Fields, 3D Gaussians, and meshes by combining a sparse 3D grid with detailed visual features from a vision model. The authors utilize rectified flow transformers designed for SLAT, training models with up to 2 billion parameters on a large dataset of 500,000 diverse 3D objects. The results show significant improvements in quality and versatility, enabling local 3D editing and output format selection that previous models could not achieve.'}, 'zh': {'title': '灵活高效的3D资产生成新方法', 'desc': '我们提出了一种新颖的3D生成方法，用于多功能和高质量的3D资产创建。该方法的核心是统一的结构化潜在(SLAT)表示，能够解码为不同的输出格式，如辐射场、3D高斯和网格。通过将稀疏的3D网格与从强大的视觉基础模型中提取的密集多视角视觉特征相结合，我们全面捕捉了结构（几何）和纹理（外观）信息，同时在解码过程中保持灵活性。我们的模型使用针对SLAT的整流流变换器进行3D生成，训练了高达20亿参数的模型，生成的结果在文本或图像条件下的质量显著超过现有方法。'}}}, {'id': 'https://huggingface.co/papers/2412.03632', 'title': 'MV-Adapter: Multi-view Consistent Image Generation Made Easy', 'url': 'https://huggingface.co/papers/2412.03632', 'abstract': 'Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to optimization difficulties and scarce high-quality 3D data. In this paper, we propose the first adapter-based solution for multi-view image generation, and introduce MV-Adapter, a versatile plug-and-play adapter that enhances T2I models and their derivatives without altering the original network structure or feature space. By updating fewer parameters, MV-Adapter enables efficient training and preserves the prior knowledge embedded in pre-trained models, mitigating overfitting risks. To efficiently model the 3D geometric knowledge within the adapter, we introduce innovative designs that include duplicated self-attention layers and parallel attention architecture, enabling the adapter to inherit the powerful priors of the pre-trained models to model the novel 3D knowledge. Moreover, we present a unified condition encoder that seamlessly integrates camera parameters and geometric information, facilitating applications such as text- and image-based 3D generation and texturing. MV-Adapter achieves multi-view generation at 768 resolution on Stable Diffusion XL (SDXL), and demonstrates adaptability and versatility. It can also be extended to arbitrary view generation, enabling broader applications. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its efficiency, adaptability and versatility.', 'score': 12, 'issue_id': 981, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'fe0891c026484abc', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Haoran Wang', 'Ran Yi', 'Lizhuang Ma', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['School of Software, Beihang University', 'Shanghai Jiao Tong University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03632.jpg', 'data': {'categories': ['#training', '#synthetic', '#optimization', '#architecture', '#cv', '#3d'], 'emoji': '🖼️', 'ru': {'title': 'MV-Adapter: Эффективная генерация многоракурсных изображений без переобучения', 'desc': 'Статья представляет MV-Adapter - первое адаптерное решение для генерации многоракурсных изображений. Этот плагин улучшает модели текст-в-изображение без изменения их структуры, сохраняя предобученные знания и снижая риск переобучения. MV-Adapter использует инновационные подходы, включая дублированные слои самовнимания и параллельную архитектуру внимания, для эффективного моделирования 3D-геометрии. Решение достигает высокого качества генерации многоракурсных изображений с разрешением 768 пикселей на основе Stable Diffusion XL.'}, 'en': {'title': 'Efficient Multi-View Image Generation with MV-Adapter', 'desc': 'This paper introduces MV-Adapter, a novel adapter-based approach for multi-view image generation that avoids invasive changes to pre-trained text-to-image (T2I) models. By updating fewer parameters, MV-Adapter enhances the efficiency of training while preserving the knowledge from pre-trained models, thus reducing the risk of overfitting. The design incorporates advanced features like duplicated self-attention layers and a unified condition encoder to effectively integrate 3D geometric information. MV-Adapter not only achieves high-quality multi-view generation but also allows for broader applications in arbitrary view generation.'}, 'zh': {'title': '高效多视角图像生成的新标准', 'desc': '现有的多视角图像生成方法通常需要对预训练的文本到图像模型进行大幅修改，并且需要完全微调，这导致了高计算成本和图像质量下降。本文提出了一种基于适配器的多视角图像生成解决方案，MV-Adapter是一种可插拔的适配器，可以增强文本到图像模型，而无需改变原始网络结构。通过更新更少的参数，MV-Adapter实现了高效训练，并保留了预训练模型中的先验知识，降低了过拟合风险。我们还引入了统一的条件编码器，能够无缝整合相机参数和几何信息，促进文本和图像基础的3D生成和纹理处理。'}}}, {'id': 'https://huggingface.co/papers/2412.04431', 'title': 'Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2412.04431', 'abstract': 'We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.', 'score': 9, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'f297f288187d4cc4', 'authors': ['Jian Han', 'Jinlai Liu', 'Yi Jiang', 'Bin Yan', 'Yuqi Zhang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.04431.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#multimodal', '#cv', '#benchmark', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Infinity: новый уровень генерации изображений с бесконечным словарем', 'desc': 'Представлена модель Infinity - битовая визуальная авторегрессионная модель для генерации фотореалистичных изображений по текстовому описанию. Infinity использует бесконечный словарь токенов и механизм битовой самокоррекции, что значительно улучшает качество генерации. Модель превосходит ведущие диффузионные модели по ряду метрик, включая GenEval и ImageReward. Infinity генерирует качественное изображение 1024x1024 за 0.8 секунды, что делает ее самой быстрой моделью text-to-image на данный момент.'}, 'en': {'title': 'Infinity: Redefining Text-to-Image Generation with Infinite Vocabulary', 'desc': 'Infinity is a novel Bitwise Visual AutoRegressive Model designed to create high-resolution, photorealistic images based on language instructions. It introduces an infinite-vocabulary tokenizer and classifier, along with a bitwise self-correction mechanism, which enhances the detail and quality of generated images. By scaling both the tokenizer and transformer sizes, Infinity significantly improves upon traditional visual autoregressive models. It sets new performance records in text-to-image generation, outperforming leading diffusion models and achieving faster generation times without additional optimization.'}, 'zh': {'title': 'Infinity：无限可能的视觉生成模型', 'desc': '本文介绍了Infinity，一种基于位的视觉自回归模型，能够根据语言指令生成高分辨率、逼真的图像。Infinity在位元令牌预测框架下重新定义了视觉自回归模型，采用无限词汇量的令牌器和分类器，以及位元自我校正机制，显著提升了生成能力和细节表现。通过理论上将令牌器的词汇量扩展到无限，并同时扩展变换器的规模，我们的方法相比传统的VAR模型释放了强大的扩展能力。Infinity在自回归文本到图像模型中创下新纪录，超越了顶级扩散模型，如SD3-Medium和SDXL。'}}}, {'id': 'https://huggingface.co/papers/2412.04146', 'title': 'AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models', 'url': 'https://huggingface.co/papers/2412.04146', 'abstract': 'Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results.', 'score': 8, 'issue_id': 989, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'ed058640f1a96005', 'authors': ['Xinghui Li', 'Qichao Sun', 'Pengze Zhang', 'Fulong Ye', 'Zhichao Liao', 'Wanquan Feng', 'Songtao Zhao', 'Qian He'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04146.jpg', 'data': {'categories': ['#diffusion', '#games', '#cv', '#multimodal'], 'emoji': '👚', 'ru': {'title': 'AnyDressing: Виртуальная примерка любой комбинации одежды с сохранением деталей', 'desc': 'Статья представляет новый метод AnyDressing для виртуальной примерки одежды на основе диффузионных моделей. Метод состоит из двух основных сетей: GarmentsNet для извлечения деталей одежды и DressingNet для генерации изображений. В GarmentsNet используется модуль Garment-Specific Feature Extractor для эффективного кодирования текстур одежды. DressingNet применяет механизм Dressing-Attention и стратегию Instance-Level Garment Localization Learning для точного наложения элементов одежды.'}, 'en': {'title': 'AnyDressing: Revolutionizing Multi-Garment Image Generation', 'desc': 'This paper introduces AnyDressing, a novel method for Multi-Garment Virtual Dressing that enhances image generation from text and image prompts. It features two main networks: GarmentsNet, which extracts detailed clothing features, and DressingNet, which generates customized images while maintaining text-image consistency. The method includes a Garment-Specific Feature Extractor to efficiently encode garment textures and an adaptive Dressing-Attention mechanism to accurately place multi-garment features. Overall, AnyDressing improves the diversity and controllability of synthesized images, achieving state-of-the-art performance in garment-centric image generation.'}, 'zh': {'title': 'AnyDressing：多服装虚拟穿衣的新方法', 'desc': '本文提出了一种新的多服装虚拟穿衣任务，并提出了AnyDressing方法，以支持任意组合的服装和个性化文本提示。AnyDressing包含两个主要网络：GarmentsNet用于提取服装细节特征，DressingNet用于生成定制图像。我们设计了高效的服装特征提取模块，避免了服装混淆，同时提高了网络效率。此外，DressingNet中的自适应注意机制和实例级服装定位学习策略，确保了多服装特征的准确注入，从而提升了生成图像的多样性和一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.04315', 'title': 'Densing Law of LLMs', 'url': 'https://huggingface.co/papers/2412.04315', 'abstract': "Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments, and the scaling trend is becoming increasingly unsustainable. This paper introduces the concept of ``capacity density'' as a new metric to evaluate the quality of the LLMs across different scales and describes the trend of LLMs in terms of both effectiveness and efficiency. To calculate the capacity density of a given target LLM, we first introduce a set of reference models and develop a scaling law to predict the downstream performance of these reference models based on their parameter sizes. We then define the effective parameter size of the target LLM as the parameter size required by a reference model to achieve equivalent performance, and formalize the capacity density as the ratio of the effective parameter size to the actual parameter size of the target LLM. Capacity density provides a unified framework for assessing both model effectiveness and efficiency. Our further analysis of recent open-source base LLMs reveals an empirical law (the densing law)that the capacity density of LLMs grows exponentially over time. More specifically, using some widely used benchmarks for evaluation, the capacity density of LLMs doubles approximately every three months. The law provides new perspectives to guide future LLM development, emphasizing the importance of improving capacity density to achieve optimal results with minimal computational overhead.", 'score': 8, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '4a5189762d711a19', 'authors': ['Chaojun Xiao', 'Jie Cai', 'Weilin Zhao', 'Guoyang Zeng', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['ModelBest Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04315.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#optimization', '#architecture', '#inference'], 'emoji': '📈', 'ru': {'title': 'Плотность мощности: новый путь к эффективным языковым моделям', 'desc': "Статья представляет новую метрику 'плотность мощности' для оценки качества больших языковых моделей (LLM) разного масштаба. Авторы вводят понятие эффективного размера параметров модели и формулируют плотность мощности как отношение эффективного размера к фактическому. Анализ современных LLM выявил эмпирический закон экспоненциального роста плотности мощности со временем. Предложенный подход предоставляет новый взгляд на развитие LLM, подчеркивая важность повышения плотности мощности для достижения оптимальных результатов при минимальных вычислительных затратах."}, 'en': {'title': 'Maximizing Performance with Minimal Resources: The Capacity Density Revolution', 'desc': "This paper discusses the challenges of training and using large language models (LLMs) as they grow in size. It introduces a new metric called 'capacity density' to evaluate LLMs based on their effectiveness and efficiency. The authors develop a scaling law to predict how well different models perform relative to their size and define capacity density as the ratio of effective parameter size to actual parameter size. Their findings suggest that the capacity density of LLMs is increasing rapidly, which could guide future improvements in model design to optimize performance while reducing resource use."}, 'zh': {'title': '提升容量密度，优化大型语言模型的效率与效果', 'desc': '大型语言模型（LLMs）在人工智能领域取得了重要进展，但随着模型规模的增加，训练和推理的效率面临巨大挑战。本文提出了“容量密度”这一新指标，用于评估不同规模LLMs的质量，并描述了LLMs在有效性和效率方面的趋势。通过引入一组参考模型并开发缩放法则，本文计算了目标LLM的有效参数大小，并将容量密度定义为有效参数大小与实际参数大小的比率。我们的分析表明，LLMs的容量密度每三个月大约翻倍，为未来的LLM开发提供了新的视角，强调了提高容量密度的重要性。'}}}, {'id': 'https://huggingface.co/papers/2412.04280', 'title': 'HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing', 'url': 'https://huggingface.co/papers/2412.04280', 'abstract': 'We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. HumanEdit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. With meticulously curation, HumanEdit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. The dataset includes six distinct types of editing instructions: Action, Add, Counting, Relation, Remove, and Replace, encompassing a broad spectrum of real-world scenarios. All images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. Furthermore, HumanEdit offers comprehensive diversity and high-resolution 1024 times 1024 content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. With the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release HumanEdit at https://huggingface.co/datasets/BryanW/HumanEdit.', 'score': 7, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '4467ff5ceea9cb1d', 'authors': ['Jinbin Bai', 'Wei Chow', 'Ling Yang', 'Xiangtai Li', 'Juncheng Li', 'Hanwang Zhang', 'Shuicheng Yan'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Peking University', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.04280.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#data'], 'emoji': '🎨', 'ru': {'title': 'HumanEdit: Редактирование изображений с человеческим подходом', 'desc': 'HumanEdit - это набор данных для редактирования изображений на основе текстовых инструкций, созданный с участием людей. Он включает 5751 изображение с масками и инструкциями шести типов: действие, добавление, подсчет, отношение, удаление и замена. Датасет отличается высоким качеством и разнообразием, так как создавался с привлечением аннотаторов и администраторов. HumanEdit предназначен для обучения и оценки моделей машинного обучения в задачах редактирования изображений.'}, 'en': {'title': 'HumanEdit: Elevating Image Editing with Human-Centric Instructions', 'desc': 'HumanEdit is a newly created dataset aimed at improving instruction-guided image editing by incorporating significant human feedback. Unlike previous datasets that lacked sufficient human input, HumanEdit utilizes human annotators to create data pairs and provide valuable feedback, ensuring alignment with human preferences. The dataset consists of 5,751 high-resolution images and includes six types of editing instructions, allowing for a wide range of image manipulation tasks. By offering detailed instructions and masks, HumanEdit sets a new standard for versatility and accuracy in image editing research.'}, 'zh': {'title': 'HumanEdit：精准多样的图像编辑数据集', 'desc': 'HumanEdit是一个高质量的人类奖励数据集，专门用于指导图像编辑。与以往的大规模编辑数据集不同，HumanEdit通过人类注释者构建数据对，并由管理员提供反馈，确保数据与人类偏好的对齐。该数据集包含5751张图像，经过2500多个小时的人工努力，涵盖六种不同类型的编辑指令，支持多样化的图像编辑任务。HumanEdit为未来的研究提供了一个新的基准，推动图像编辑领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2412.04378', 'title': 'Discriminative Fine-tuning of LVLMs', 'url': 'https://huggingface.co/papers/2412.04378', 'abstract': 'Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework\'s components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.', 'score': 6, 'issue_id': 989, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '6d05f8d3eedf64ed', 'authors': ['Yassine Ouali', 'Adrian Bulat', 'Alexandros Xenos', 'Anestis Zaganidis', 'Ioannis Maniadis Metaxas', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University of London', 'Samsung AI Cambridge', 'Technical University of Iasi'], 'pdf_title_img': 'assets/pdf/title_img/2412.04378.jpg', 'data': {'categories': ['#architecture', '#cv', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#training'], 'emoji': '🔍', 'ru': {'title': 'Объединение лучшего из двух миров: дискриминативные LVLM с улучшенным пониманием языка', 'desc': 'В статье предлагается новый подход к обучению больших визуально-языковых моделей (LVLM) для дискриминативных задач. Авторы комбинируют сильные стороны контрастивных моделей типа CLIP и генеративных LVLM, создавая модель с улучшенным пониманием языка и способностью к композиционному рассуждению. Предложенный метод включает специальную схему обучения с контрастивными и предиктивными потерями, а также эффективную адаптацию параметров. Результаты показывают значительное улучшение по сравнению с современными моделями типа CLIP в задачах поиска изображений по тексту и композиционности.'}, 'en': {'title': 'Unlocking Discriminative Power in Vision-Language Models', 'desc': 'This paper introduces a new training method for Large Vision-Language Models (LVLMs) that enhances their ability to understand and discriminate between images and text. By combining contrastive learning with next-token prediction, the model achieves better performance in tasks requiring detailed vision-language reasoning. The authors also present a parameter-efficient adaptation technique that utilizes soft prompting and LoRA adapters, making the model more efficient. Overall, this work demonstrates significant improvements over existing models like CLIP, particularly in image-text retrieval and compositional understanding.'}, 'zh': {'title': '结合优势，提升视觉语言模型的区分能力', 'desc': '本文提出了一种新的训练方法，用于对大型视觉语言模型（LVLMs）进行区分性微调，从而提高其在图像-文本任务中的表现。我们的方法结合了对比学习和下一个标记预测损失，使得模型能够更好地理解语言并进行图像-文本的区分。通过使用可变长度和粒度的图像-文本对进行训练，我们的框架展示了其各个组成部分的必要性。最终，我们的方法在标准的图像-文本检索基准上显著超越了现有的CLIP类模型。'}}}, {'id': 'https://huggingface.co/papers/2412.02142', 'title': 'Personalized Multimodal Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2412.02142', 'abstract': 'Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.', 'score': 6, 'issue_id': 982, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': 'a34ccd5a86ab7840', 'authors': ['Junda Wu', 'Hanjia Lyu', 'Yu Xia', 'Zhehao Zhang', 'Joe Barrow', 'Ishita Kumar', 'Mehrnoosh Mirtaheri', 'Hongjie Chen', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Tong Yu', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Nesreen K. Ahmed', 'Yu Wang', 'Xiang Chen', 'Hanieh Deilamsalehy', 'Namyong Park', 'Sungchul Kim', 'Huanrui Yang', 'Subrata Mitra', 'Zhengmian Hu', 'Nedim Lipka', 'Dang Nguyen', 'Yue Zhao', 'Jiebo Luo', 'Julian McAuley'], 'affiliations': ['Adobe Research', 'Cisco Research', 'Dartmouth College', 'Meta AI', 'University of Arizona', 'University of California, San Diego', 'University of Maryland', 'University of Massachusetts at Amherst', 'University of Oregon', 'University of Rochester', 'University of Southern California', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.02142.jpg', 'data': {'categories': ['#training', '#survey', '#multimodal', '#architecture', '#dataset', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Персонализация мультимодальных ИИ: от архитектуры до применения', 'desc': 'В статье представлен обзор персонализированных мультимодальных больших языковых моделей (МLLM). Авторы предлагают таксономию методов персонализации МLLM и обсуждают их архитектуру, методы обучения и применения. Рассматриваются задачи и метрики оценки персонализированных МLLM, а также наборы данных для их тестирования. В завершение обозначены ключевые открытые проблемы в этой области.'}, 'en': {'title': 'Personalizing Multimodal Language Models for Enhanced User Experience', 'desc': 'This paper surveys personalized multimodal large language models (MLLMs), which integrate various data types like text, images, and audio for improved task performance. It introduces a taxonomy to categorize personalization techniques, detailing their architectures and training methods. The authors also discuss how these techniques can be adapted or combined, emphasizing their benefits and rationale. Additionally, the paper reviews existing personalization tasks, evaluation metrics, and relevant datasets, while identifying key challenges in the field.'}, 'zh': {'title': '个性化多模态大型语言模型的未来之路', 'desc': '多模态大型语言模型（MLLMs）在整合文本、图像和音频等多种数据模态方面表现出色，能够高效完成复杂任务。本文对个性化多模态大型语言模型进行了全面的调查，重点介绍了其架构、训练方法和应用。我们提出了一种直观的分类法，用于对个性化MLLMs的技术进行分类，并讨论了相应的技术。最后，我们总结了现有研究中调查的个性化任务及其评估指标，并指出了未来的挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.04062', 'title': 'ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality', 'url': 'https://huggingface.co/papers/2412.04062', 'abstract': "In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining.", 'score': 6, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '44d216e3fa2fb345', 'authors': ['Yefei He', 'Feng Chen', 'Yuanyu He', 'Shaoxuan He', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.04062.jpg', 'data': {'categories': ['#optimization', '#training', '#cv'], 'emoji': '🚀', 'ru': {'title': 'ZipAR: Ускоряем генерацию изображений параллельным декодированием', 'desc': 'ZipAR - это новый подход к ускорению авторегрессионной генерации изображений без дополнительного обучения модели. Метод основан на наблюдении, что пространственно удаленные области изображения имеют минимальную взаимозависимость. ZipAR позволяет параллельно декодировать токены, соответствующие смежным областям, что значительно сокращает количество прямых проходов модели. Эксперименты показали, что ZipAR может уменьшить число прямых проходов на 91% для модели Emu3-Gen без переобучения.'}, 'en': {'title': 'Accelerate Image Generation with Parallel Decoding!', 'desc': "This paper introduces ZipAR, a new framework designed to speed up the process of generating images using auto-regressive models without the need for retraining. It leverages the observation that images have local structures, allowing for the parallel decoding of visual tokens in adjacent regions. By implementing a 'next-set prediction' approach, ZipAR can decode multiple tokens at once, significantly cutting down the number of forward passes needed to create an image. Experiments show that this method can reduce forward passes by up to 91%, greatly enhancing generation efficiency."}, 'zh': {'title': 'ZipAR：加速自回归视觉生成的高效解码框架', 'desc': '本文提出了一种名为ZipAR的框架，旨在加速自回归视觉生成。该框架不需要重新训练，能够实现并行解码，利用图像的局部结构特性。通过在列维度上并行解码空间相邻区域的视觉标记，ZipAR显著减少了生成图像所需的前向传播次数。实验结果表明，ZipAR在Emu3-Gen模型上可以将前向传播次数减少多达91%。'}}}, {'id': 'https://huggingface.co/papers/2412.01820', 'title': 'Towards Universal Soccer Video Understanding', 'url': 'https://huggingface.co/papers/2412.01820', 'abstract': 'As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research.', 'score': 6, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'fa9f7e4132ee5026', 'authors': ['Jiayuan Rao', 'Haoning Wu', 'Hao Jiang', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Alibaba Group, China', 'CMIC, Shanghai Jiao Tong University, China', 'School of Artificial Intelligence, Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01820.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#video'], 'emoji': '⚽', 'ru': {'title': 'MatchVision: революция в компьютерном зрении для футбола', 'desc': 'В этой статье представлена многомодальная модель MatchVision для анализа футбольных видео. Авторы создали крупнейший датасет SoccerReplay-1988, содержащий видео и аннотации 1988 полных матчей. MatchVision использует пространственно-временную информацию из видео и превосходит существующие модели в задачах классификации событий, генерации комментариев и распознавания нарушений. Эксперименты показали значительное улучшение производительности по сравнению с предыдущими подходами.'}, 'en': {'title': 'Revolutionizing Soccer Video Analysis with MatchVision', 'desc': 'This paper presents a new framework for understanding soccer videos using machine learning. It introduces SoccerReplay-1988, a large dataset containing videos and annotations from nearly 2,000 soccer matches, which helps in training models. The authors also develop MatchVision, a visual-language model that processes spatiotemporal data from soccer videos and performs well in tasks like event classification and commentary generation. The results show that MatchVision outperforms existing models, setting a new standard for research in sports video analysis.'}, 'zh': {'title': '足球视频理解的新标准', 'desc': '这篇论文旨在开发一个全面的多模态框架，用于理解足球视频。我们引入了SoccerReplay-1988，这是迄今为止最大的多模态足球数据集，包含1988场完整比赛的视频和详细注释。我们还提出了足球领域的首个视觉-语言基础模型MatchVision，能够利用足球视频中的时空信息，并在多个下游任务中表现出色。通过广泛的实验和消融研究，MatchVision在事件分类、评论生成和多视角犯规识别等任务上均表现出最先进的性能，显示了我们提出的数据和模型的优越性。'}}}, {'id': 'https://huggingface.co/papers/2412.03304', 'title': 'Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation', 'url': 'https://huggingface.co/papers/2412.03304', 'abstract': 'Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artifacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global-MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global-MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.', 'score': 5, 'issue_id': 988, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'f9677fd07f780c7b', 'authors': ['Shivalika Singh', 'Angelika Romanou', 'Clémentine Fourrier', 'David I. Adelani', 'Jian Gang Ngui', 'Daniel Vila-Suero', 'Peerat Limkonchotiwat', 'Kelly Marchisio', 'Wei Qi Leong', 'Yosephine Susanto', 'Raymond Ng', 'Shayne Longpre', 'Wei-Yin Ko', 'Madeline Smith', 'Antoine Bosselut', 'Alice Oh', 'Andre F. T. Martins', 'Leshem Choshen', 'Daphne Ippolito', 'Enzo Ferrante', 'Marzieh Fadaee', 'Beyza Ermis', 'Sara Hooker'], 'affiliations': ['AI Singapore', 'CONICET & Universidad de Buenos Aires', 'Carnegie Mellon University', 'Cohere', 'Cohere For AI', 'EPFL', 'Hugging Face', 'Instituto Superior Técnico, Universidade de Lisboa', 'Instituto de Telecomunicações', 'KAIST', 'MIT', 'MIT, MIT-IBM Watson AI Lab', 'Mila, McGill University & Canada CIFAR AI Chair', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.03304.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multilingual', '#low_resource', '#machine_translation'], 'emoji': '🌍', 'ru': {'title': 'Преодоление культурных барьеров в многоязычной оценке языковых моделей', 'desc': 'Статья рассматривает проблему культурных предубеждений в многоязычных наборах данных для оценки языковых моделей. Авторы анализируют влияние этих предубеждений на эффективность датасетов как глобальных бенчмарков, особенно в контексте переведенного набора данных MMLU. Исследование показывает, что 28% вопросов в MMLU требуют культурно-специфических знаний, а 84.9% вопросов по географии сосредоточены на Северной Америке и Европе. В результате работы был создан улучшенный датасет Global-MMLU, охватывающий 42 языка и учитывающий культурные особенности.'}, 'en': {'title': 'Bridging Cultural Gaps in Multilingual Machine Learning', 'desc': 'This paper addresses the cultural biases found in multilingual datasets, which hinder their effectiveness as global benchmarks for machine learning models. It highlights how these biases arise not only from language differences but also from the cultural context needed to understand questions, particularly in datasets like MMLU. The authors demonstrate that relying solely on machine translation can lead to significant distortions in model performance and evaluation rankings, especially for questions requiring cultural or geographic knowledge. To combat these issues, they introduce Global-MMLU, a refined dataset that includes culturally sensitive annotations and improved translation quality, enabling more accurate evaluations across 42 languages.'}, 'zh': {'title': '解决多语言数据集中的文化偏见', 'desc': '这篇论文探讨了多语言数据集中存在的文化偏见对全球基准的影响。这些偏见不仅源于语言，还涉及到理解问题所需的文化知识，降低了翻译数据集的实用性。研究表明，许多问题需要文化敏感的知识，尤其是关于地理知识的问题，主要集中在北美和欧洲地区。为了解决这些问题，作者发布了Global-MMLU，一个改进的多语言评估集，涵盖42种语言，并通过专业和社区注释者验证翻译质量，评估文化偏见。'}}}, {'id': 'https://huggingface.co/papers/2412.01169', 'title': 'OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows', 'url': 'https://huggingface.co/papers/2412.01169', 'abstract': 'We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows.', 'score': 5, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '7f3df6f7d4733664', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Zichun Liao', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01169.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#training', '#optimization', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'OmniFlow: универсальная модель для мультимодальной генерации', 'desc': 'OmniFlow - это новая генеративная модель, разработанная для задач генерации любого типа данных в любой другой тип, например текст-в-изображение или аудио-в-изображение. Модель развивает фреймворк выпрямленного потока (rectified flow) для работы с совместным распределением нескольких модальностей. OmniFlow превосходит предыдущие модели в широком спектре задач генерации. Авторы предлагают новую архитектуру на основе MMDiT из Stable Diffusion 3, а также изучают оптимальные параметры трансформеров с выпрямленным потоком для генерации аудио и текста.'}, 'en': {'title': 'OmniFlow: Bridging Modalities for Any-to-Any Generation', 'desc': 'OmniFlow is a new generative model that can create outputs across different types of data, like turning text into images or audio. It builds on the rectified flow (RF) framework, allowing it to understand and generate multiple types of data together. This model not only improves performance on tasks like text-to-image and text-to-audio synthesis but also introduces a way for users to control how different data types relate to each other in the generated results. Additionally, it features a unique architecture that enhances existing models and provides insights into optimizing generative tasks across various modalities.'}, 'zh': {'title': 'OmniFlow：多模态生成的灵活解决方案', 'desc': 'OmniFlow是一种新型生成模型，旨在处理任意到任意的生成任务，如文本到图像、文本到音频和音频到图像的合成。它在文本到图像模型中改进了修正流（RF）框架，以处理多种模态的联合分布。OmniFlow在多种任务上超越了之前的任意到任意模型，提供了灵活的模态对齐控制机制。我们的研究还探讨了修正流变换器在大规模音频和文本生成中的设计选择，为优化不同模态的性能提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2412.04106', 'title': 'MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities', 'url': 'https://huggingface.co/papers/2412.04106', 'abstract': 'Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities.', 'score': 5, 'issue_id': 980, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '7b2720f9a6c27027', 'authors': ['Haoning Wu', 'Ziheng Zhao', 'Ya Zhang', 'Weidi Xie', 'Yanfeng Wang'], 'affiliations': ['CMIC, Shanghai Jiao Tong University, China', 'School of Artificial Intelligence, Shanghai Jiao Tong University, China', 'Shanghai AI Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.04106.jpg', 'data': {'categories': ['#diffusion', '#healthcare', '#synthetic', '#cv', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Генерация синтетических МРТ для обучения сегментации без разметки', 'desc': 'В этой статье представлен новый подход к синтезу медицинских изображений для неаннотированных модальностей с использованием генеративных моделей. Авторы создали большой датасет MedGen-1M, содержащий радиологические изображения с текстовыми описаниями и частичной разметкой органов. Они разработали диффузионную модель MRGen для генерации МРТ-изображений на основе текстовых подсказок и масок. Эксперименты показали эффективность этого подхода для обучения моделей сегментации на неаннотированных модальностях МРТ.'}, 'en': {'title': 'Generating Synthetic Data for Medical Image Segmentation', 'desc': 'This paper presents a novel approach to medical image segmentation by using generative models to create synthetic data for modalities that lack mask annotations. The authors introduce a large dataset called MedGen-1M, which includes radiology images with modality labels and some organ mask annotations, facilitating research in medical image generation. They propose a diffusion-based data engine named MRGen that generates MR images based on text prompts and available masks, allowing for the training of segmentation models on unannotated data. Extensive experiments demonstrate the effectiveness of MRGen in synthesizing training samples and improving segmentation performance across various MRI modalities.'}, 'zh': {'title': '利用生成模型推动医学图像分割的创新', 'desc': '这篇论文探讨了在医学图像分割中使用生成模型的新方法，特别是针对未标注模态的数据合成。研究者们收集并整理了一个大型的放射学图像-文本数据集MedGen-1M，包含模态标签、属性、区域和器官信息，以及部分器官的掩膜注释。论文中提出了一种基于扩散的生成数据引擎MRGen，可以根据文本提示和掩膜生成缺乏掩膜注释的MR图像，从而训练未标注模态的分割模型。通过广泛的实验，结果表明该数据引擎能够有效合成训练样本，推动MRI分割向未标注模态的扩展。'}}}, {'id': 'https://huggingface.co/papers/2412.04139', 'title': 'Monet: Mixture of Monosemantic Experts for Transformers', 'url': 'https://huggingface.co/papers/2412.04139', 'abstract': 'Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance} mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust} model behavior. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Monet.', 'score': 4, 'issue_id': 982, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '9853123764b31006', 'authors': ['Jungwoo Park', 'Young Jin Ahn', 'Kee-Eung Kim', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04139.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#open_source', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Прозрачные большие языковые модели: расшифровка внутренних вычислений с помощью моносемантических экспертов', 'desc': 'Статья представляет новую архитектуру Mixture of Monosemantic Experts for Transformers (Monet) для улучшения интерпретируемости больших языковых моделей. Monet внедряет разреженное обучение словаря непосредственно в предобучение моделей типа Mixture-of-Experts. Этот подход позволяет масштабировать количество экспертов до 262,144 на слой, при этом общее количество параметров растет пропорционально квадратному корню числа экспертов. Анализ показывает взаимоисключающее распределение знаний между экспертами и демонстрирует параметрические знания, инкапсулированные в отдельных экспертах.'}, 'en': {'title': 'Unlocking LLMs: Expert Knowledge for Safer AI', 'desc': "This paper addresses the challenge of understanding how large language models (LLMs) work internally, particularly to align them with human values and reduce harmful outputs. The authors introduce a new architecture called Mixture of Monosemantic Experts for Transformers (Monet), which improves upon previous methods by integrating sparse dictionary learning directly into the model's training process. Monet allows for a large number of specialized experts, enhancing the model's ability to manage different types of knowledge while maintaining overall performance. The findings suggest that this approach not only improves interpretability but also enables better control over the model's behavior regarding various domains and toxicity levels."}, 'zh': {'title': '提升大型语言模型的可解释性与性能', 'desc': '理解大型语言模型（LLMs）的内部计算对于使其与人类价值观对齐至关重要，并防止生成有害内容。然而，机制可解释性受到多义性的阻碍，即单个神经元对多个无关概念的响应。我们提出了一种新的架构，称为单义专家混合模型（Monet），它将稀疏字典学习直接融入端到端的专家预训练中，从而提高了模型的性能。我们的分析表明，专家之间的知识是相互独立的，并且Monet允许在不同领域、语言和毒性减轻方面进行知识操作，而不会降低整体性能。'}}}, {'id': 'https://huggingface.co/papers/2411.19574', 'title': 'KV Shifting Attention Enhances Language Modeling', 'url': 'https://huggingface.co/papers/2411.19574', 'abstract': "The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attention. In order to more efficiently implement the ability of the model's induction, we revisit the induction heads mechanism and proposed a KV shifting attention. We theoretically prove that the KV shifting attention reducing the model's requirements for the depth and width of the induction heads mechanism. Our experimental results demonstrate that KV shifting attention is beneficial to learning induction heads and language modeling, which lead to better performance or faster convergence from toy models to the pre-training models with more than 10 B parameters.", 'score': 3, 'issue_id': 987, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'feb0a592d740fe9b', 'authors': ['Mingyu Xu', 'Wei Cheng', 'Bingning Wang', 'Weipeng Chen'], 'affiliations': ['Baichuan Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2411.19574.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности индукционных механизмов в языковых моделях', 'desc': 'Эта статья исследует механизм индукционных головок в больших языковых моделях, основанных на трансформерах. Авторы предлагают новый метод под названием KV shifting attention для более эффективной реализации индукционных способностей модели. Теоретически доказано, что этот метод снижает требования к глубине и ширине механизма индукционных головок. Экспериментальные результаты показывают, что KV shifting attention улучшает обучение индукционным головкам и языковое моделирование, приводя к лучшей производительности или более быстрой сходимости.'}, 'en': {'title': 'Enhancing Induction Heads with KV Shifting Attention', 'desc': "This paper discusses improvements in large language models that use a decode-only transformer structure, focusing on their in-context learning (ICL) abilities. The authors highlight the importance of the induction heads mechanism, which traditionally requires multiple layers of attention. They introduce a new approach called KV shifting attention, which simplifies the induction heads mechanism by reducing the model's depth and width requirements. Experimental results show that this new attention method enhances the learning of induction heads and improves language modeling performance, leading to faster convergence in models with over 10 billion parameters."}, 'zh': {'title': '提升归纳能力的KV位移注意力机制', 'desc': '当前的大型语言模型主要基于解码结构的变换器，具有很强的上下文学习能力。人们普遍认为，这种能力的重要基础是归纳头机制，而该机制至少需要两层注意力。为了更有效地实现模型的归纳能力，我们重新审视了归纳头机制，并提出了一种KV位移注意力。我们的理论证明表明，KV位移注意力降低了模型对归纳头机制的深度和宽度要求，实验结果显示其在学习归纳头和语言建模方面具有优势。'}}}, {'id': 'https://huggingface.co/papers/2412.04003', 'title': 'Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement', 'url': 'https://huggingface.co/papers/2412.04003', 'abstract': 'Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.', 'score': 2, 'issue_id': 985, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '435ec5749dcb2e12', 'authors': ['Lingfeng Ming', 'Bo Zeng', 'Chenyang Lyu', 'Tianqi Shi', 'Yu Zhao', 'Xue Yang', 'Yefeng Liu', 'Yiyu Wang', 'Linlong Xu', 'Yangyang Liu', 'Xiaohu Zhao', 'Hao Wang', 'Heng Liu', 'Hao Zhou', 'Huifeng Yin', 'Zifu Shang', 'Haijun Li', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2412.04003.jpg', 'data': {'categories': ['#training', '#machine_translation', '#dataset', '#low_resource', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Marco-LLM: Преодоление языкового барьера в мире ИИ', 'desc': 'Marco-LLM - это новая многоязычная модель больших языковых моделей (LLM), разработанная для улучшения производительности в задачах с низкоресурсными языками. Модель была обучена на большом объеме многоязычных данных с использованием архитектуры Qwen2. Marco-LLM показала значительные улучшения по сравнению с современными LLM на различных многоязычных тестах, включая MMMLU, AGIEval, Belebele и другие. Кроме того, модель продемонстрировала существенный прогресс в задачах машинного перевода между любыми языками.'}, 'en': {'title': 'Bridging Language Gaps with Marco-LLM', 'desc': 'This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds.'}, 'zh': {'title': 'Marco-LLM：打破语言壁垒的多语言模型', 'desc': '大型语言模型（LLMs）在近年来取得了显著进展，但其优秀表现主要集中在主要世界语言上，尤其是英语。为了改善低资源语言的多语言任务表现，我们提出了Marco-LLM，这是一个针对跨语言增强的大规模多语言训练模型。我们收集了大量低资源语言的多语言数据，并使用Qwen2模型进行了广泛的持续预训练。Marco-LLM在多项多语言基准测试中表现出显著的改进，尤其在任意对任意的机器翻译任务中，展示了其多语言模型的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.03704', 'title': 'Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension', 'url': 'https://huggingface.co/papers/2412.03704', 'abstract': "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.", 'score': 1, 'issue_id': 993, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'ec4d70e89a11baa5', 'authors': ['Wang Xiyao', 'Yang Zhengyuan', 'Li Linjie', 'Lu Hongjin', 'Xu Yuancheng', 'Lin Chung-Ching Lin', 'Lin Kevin', 'Huang Furong', 'Wang Lijuan'], 'affiliations': ['Microsoft', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2412.03704.jpg', 'data': {'categories': ['#optimization', '#inference', '#cv', '#hallucinations', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'VisVM: Улучшение VLM через оптимизацию вывода', 'desc': 'Статья представляет Vision Value Model (VisVM) - модель, улучшающую качество ответов моделей компьютерного зрения и обработки естественного языка (VLM) путем оптимизации процесса вывода. VisVM оценивает качество генерируемых предложений и прогнозирует качество последующих, что позволяет избежать галлюцинаций и недостаточной детализации. Эксперименты показывают, что использование VisVM значительно улучшает способность VLM генерировать подробные описания с меньшим количеством ошибок. Более того, самообучение модели с использованием VisVM улучшает производительность VLM на различных мультимодальных бенчмарках.'}, 'en': {'title': 'Enhancing Visual Comprehension in VLMs with Vision Value Model', 'desc': 'This paper introduces the Vision Value Model (VisVM), which enhances the response quality of vision-language models (VLMs) during inference. VisVM evaluates not only the quality of the current generated sentence but also predicts the quality of future sentences, providing a long-term value assessment. By guiding the search process, VisVM helps VLMs avoid generating sentences that lack detail or contain inaccuracies, leading to more descriptive and accurate outputs. Experimental results show that using VisVM significantly improves the generation of captions with richer visual details and reduces hallucinations, paving the way for self-improving VLMs through self-training.'}, 'zh': {'title': '提升视觉语言模型响应质量的关键', 'desc': '尽管视觉语言模型（VLMs）取得了显著进展，但在推理时计算能力的提升方面仍缺乏有效的方法。本文提出了一种视觉价值模型（VisVM），它可以指导VLM在推理时的搜索，以生成更具视觉理解的响应。VisVM不仅评估当前搜索步骤生成句子的质量，还预测后续句子的质量，从而提供长期价值。实验结果表明，VisVM引导的搜索显著提高了VLM生成描述性标题的能力，减少了幻觉现象，显示出自我提升VLM的潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.04262', 'title': 'SynFinTabs: A Dataset of Synthetic Financial Tables for Information and Table Extraction', 'url': 'https://huggingface.co/papers/2412.04262', 'abstract': 'Table extraction from document images is a challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source code. However, there are significant layout and typographical differences between tables found across scientific, financial, and other domains. Current datasets often lack the words, and their positions, contained within the tables, instead relying on unreliable OCR to extract these features for training modern machine learning models on natural language processing tasks. Therefore, there is a need for a more general method of obtaining labelled data. We present SynFinTabs, a large-scale, labelled dataset of synthetic financial tables. Our hope is that our method of generating these synthetic tables is transferable to other domains. To demonstrate the effectiveness of our dataset in training models to extract information from table images, we create FinTabQA, a layout large language model trained on an extractive question-answering task. We test our model using real-world financial tables and compare it to a state-of-the-art generative model and discuss the results. We make the dataset, model, and dataset generation code publicly available.', 'score': 0, 'issue_id': 994, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '3fef2d0f6d8b7c5c', 'authors': ['Ethan Bradley', 'Muhammad Roman', 'Karen Rafferty', 'Barry Devereux'], 'affiliations': ['School of Electronics, Electrical Engineering and Computer Science, Queens University Belfast, Belfast, UK'], 'pdf_title_img': 'assets/pdf/title_img/2412.04262.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#synthetic', '#transfer_learning', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'Синтетические данные открывают новые возможности для анализа финансовых таблиц', 'desc': 'Статья представляет SynFinTabs - крупномасштабный набор данных синтетических финансовых таблиц. Авторы разработали метод генерации синтетических таблиц, который может быть применен и в других областях. На основе этого датасета обучена модель FinTabQA - языковая модель для извлечения информации из изображений таблиц. Модель тестировалась на реальных финансовых таблицах и сравнивалась с современными генеративными моделями. Датасет, модель и код для генерации данных находятся в открытом доступе.'}, 'en': {'title': 'Unlocking Financial Insights with Synthetic Table Data', 'desc': 'This paper addresses the challenge of table extraction from document images, particularly in the financial domain where labeled data is scarce. It introduces SynFinTabs, a large-scale dataset of synthetic financial tables designed to improve the training of machine learning models for this task. The authors also present FinTabQA, a layout large language model specifically trained for extractive question-answering on table images. By comparing their model with existing state-of-the-art generative models, they demonstrate the effectiveness of their synthetic dataset and make all resources publicly available for further research.'}, 'zh': {'title': '合成金融表格数据集的创新应用', 'desc': '本文讨论了从文档图像中提取表格的挑战，尤其是在缺乏标注数据的情况下。现有的数据集主要集中在科学表格上，忽视了金融等其他领域的表格布局和排版差异。为了解决这个问题，作者提出了SynFinTabs，一个大规模的合成金融表格标注数据集，旨在为不同领域提供可转移的标注数据生成方法。通过创建FinTabQA模型，作者展示了该数据集在训练信息提取模型方面的有效性，并与最先进的生成模型进行了比较。'}}}, {'id': 'https://huggingface.co/papers/2412.04448', 'title': 'MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation', 'url': 'https://huggingface.co/papers/2412.04448', 'abstract': 'Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos remain significant challenges. To address these challenges, we propose Memory-guided EMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Our approach is built around two key modules: (1) a memory-guided temporal module, which enhances long-term identity consistency and motion smoothness by developing memory states to store information from a longer past context to guide temporal modeling via linear attention; and (2) an emotion-aware audio module, which replaces traditional cross attention with multi-modal attention to enhance audio-video interaction, while detecting emotions from audio to refine facial expressions via emotion adaptive layer norm. Extensive quantitative and qualitative results demonstrate that MEMO generates more realistic talking videos across diverse image and audio types, outperforming state-of-the-art methods in overall quality, audio-lip synchronization, identity consistency, and expression-emotion alignment.', 'score': 0, 'issue_id': 993, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'cb01c778a4be31bd', 'authors': ['Longtao Zheng', 'Yifan Zhang', 'Hanzhong Guo', 'Jiachun Pan', 'Zhenxiong Tan', 'Jiahao Lu', 'Chuanxin Tang', 'Bo An', 'Shuicheng Yan'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.04448.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#video', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'MEMO: Реалистичная анимация портретов с учетом памяти и эмоций', 'desc': 'Статья представляет новый подход к генерации видео с говорящими портретами на основе аудио под названием MEMO (Memory-guided EMOtion-aware diffusion). Ключевые компоненты MEMO включают модуль памяти для улучшения долгосрочной согласованности личности и плавности движений, а также эмоционально-адаптивный аудиомодуль для улучшения взаимодействия аудио и видео. MEMO использует линейное внимание и мультимодальное внимание для повышения качества генерации. Результаты экспериментов показывают превосходство MEMO над современными методами в общем качестве, синхронизации губ с аудио, согласованности личности и соответствии выражений эмоциям.'}, 'en': {'title': 'MEMO: Revolutionizing Audio-Driven Talking Video Generation', 'desc': 'This paper introduces Memory-guided EMOtion-aware diffusion (MEMO), a novel approach for generating talking videos that are synchronized with audio. MEMO addresses key challenges such as audio-lip synchronization and maintaining consistent identities over time. It utilizes a memory-guided temporal module to enhance motion smoothness and identity consistency by leveraging past information. Additionally, an emotion-aware audio module improves the interaction between audio and video, ensuring that facial expressions align with detected emotions, resulting in high-quality, expressive talking videos.'}, 'zh': {'title': '记忆引导的情感感知扩散：生成更真实的对话视频', 'desc': '最近视频扩散模型的进展为基于音频的真实感对话视频生成开辟了新潜力。然而，实现无缝的音频与嘴唇同步、保持长期身份一致性以及生成自然的音频对齐表情仍然是重大挑战。为了解决这些问题，我们提出了记忆引导的情感感知扩散（MEMO），这是一种端到端的音频驱动肖像动画方法，旨在生成身份一致且富有表现力的对话视频。我们的方案围绕两个关键模块构建：记忆引导的时间模块和情感感知音频模块，显著提升了视频生成的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.04363', 'title': 'Challenges in Trustworthy Human Evaluation of Chatbots', 'url': 'https://huggingface.co/papers/2412.04363', 'abstract': 'Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.', 'score': 0, 'issue_id': 992, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '4fa16cf75a2af540', 'authors': ['Wenting Zhao', 'Alexander M. Rush', 'Tanya Goyal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.04363.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#hallucinations', '#alignment', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'Ахиллесова пята краудсорсинговых бенчмарков ИИ', 'desc': 'Статья рассматривает проблемы открытых платформ для оценки языковых моделей, таких как Chatbot Arena. Авторы выявляют три источника некачественных аннотаций, способных исказить рейтинги моделей. Показано, что всего 10% недобросовестных оценок могут изменить позиции моделей в рейтинге на 5 мест. Обсуждаются сложности обеспечения высокого качества разметки данных людьми.'}, 'en': {'title': 'Guarding Against Bad Votes: Ensuring Trustworthy LLM Rankings', 'desc': 'This paper examines the challenges of maintaining high-quality annotations in community-driven platforms that evaluate large language models (LLMs). It identifies three main sources of poor annotations: apathetic users who lack motivation to provide accurate feedback, adversarial users who intentionally skew results, and other unspecified factors. The authors reveal that even a small percentage of low-quality votes can significantly impact the rankings of models on leaderboards. The paper concludes by highlighting the ongoing challenges in ensuring reliable human annotations for accurate model evaluation.'}, 'zh': {'title': '确保高质量注释，提升模型排名的信任度', 'desc': '本文探讨了开放社区平台（如Chatbot Arena）在收集用户偏好数据时的挑战。我们发现，低质量的注释来源主要有三种，包括冷漠的用户和恶意行为者，这些都可能影响大型语言模型（LLM）的排名。研究表明，仅10%的低质量投票就能使模型在排行榜上变化多达5个名次。最后，文章讨论了确保高质量人类注释的开放性挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.01824', 'title': 'X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models', 'url': 'https://huggingface.co/papers/2412.01824', 'abstract': "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", 'score': 37, 'issue_id': 911, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '6b7c2d6555d8df8d', 'authors': ['Zeyi Sun', 'Ziyang Chu', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuanjun Xiong', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'MThreads AI', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.01824.jpg', 'data': {'categories': ['#agi', '#cv', '#games', '#optimization', '#training', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'X-Prompt: универсальная модель для генерации изображений с контекстным обучением', 'desc': 'Статья представляет X-Prompt - авторегрессивную мультимодальную модель для генерации изображений. Модель использует контекстное обучение, что позволяет ей решать как знакомые, так и новые задачи генерации изображений. X-Prompt эффективно сжимает признаки из контекстных примеров и поддерживает длинные последовательности токенов контекста. Эксперименты подтверждают способность модели решать разнообразные задачи генерации изображений, включая ранее не встречавшиеся.'}, 'en': {'title': 'X-Prompt: Unlocking Image Generation with In-Context Learning', 'desc': 'This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges.'}, 'zh': {'title': 'X-Prompt：提升图像生成的上下文学习能力', 'desc': '本文介绍了一种名为X-Prompt的自回归大规模视觉语言模型，旨在提升图像生成任务的表现。X-Prompt通过利用上下文中的示例，能够在已知和未知的图像生成任务中实现竞争力的性能。该模型采用了专门的设计，能够有效压缩上下文示例中的重要特征，从而支持更长的上下文序列并提高对未知任务的泛化能力。通过统一的训练任务，X-Prompt在文本和图像预测方面表现出色，验证了其在多样化图像生成任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.00131', 'title': 'Open-Sora Plan: Open-Source Large Video Generation Model', 'url': 'https://huggingface.co/papers/2412.00131', 'abstract': 'We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.', 'score': 19, 'issue_id': 911, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': 'fa9b0009de797ca2', 'authors': ['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.00131.jpg', 'data': {'categories': ['#open_source', '#data', '#inference', '#training', '#video'], 'emoji': '🎬', 'ru': {'title': 'Открытая модель для генерации высококачественного видео', 'desc': 'Проект Open-Sora Plan представляет собой открытую модель генерации видео высокого разрешения на основе различных пользовательских входных данных. Модель включает в себя вейвлет-поточный вариационный автоэнкодер, совместный денойзер изображений и видео, а также различные контроллеры условий. Разработаны стратегии для эффективного обучения и вывода, а также предложен многомерный конвейер курирования данных. Проект достиг впечатляющих результатов в генерации видео как в качественных, так и в количественных оценках.'}, 'en': {'title': 'Empowering High-Resolution Video Generation with Open-Sora Plan', 'desc': 'The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field.'}, 'zh': {'title': '开放源代码，生成高质量视频的未来', 'desc': 'Open-Sora计划是一个开源项目，旨在基于用户输入生成高分辨率的长时视频。该项目包含多个组件，如小波流变分自编码器和联合图像-视频去噪器，支持整个视频生成过程。我们还设计了多种高效的训练和推理策略，并提出了多维数据策划管道，以获取高质量数据。通过这些高效的设计，Open-Sora计划在视频生成的定性和定量评估中取得了令人印象深刻的结果。'}}}, {'id': 'https://huggingface.co/papers/2412.00927', 'title': 'VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation', 'url': 'https://huggingface.co/papers/2412.00927', 'abstract': 'Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.', 'score': 17, 'issue_id': 912, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': 'b4065e6e554dea31', 'authors': ['Weiming Ren', 'Huan Yang', 'Jie Min', 'Cong Wei', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.00927.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset', '#data', '#long_context', '#benchmark', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Синтез данных для обучения ИИ пониманию сложных видео', 'desc': 'VISTA - это система для улучшения понимания длинных и высокого разрешения видео большими мультимодальными моделями. Она синтезирует новые видео, комбинируя существующие пространственно и временно, и создает вопросно-ответные пары к ним. На основе VISTA создан датасет VISTA-400K, который позволил улучшить результаты моделей на 3.3% в задачах понимания длинных видео. Также авторы представили первый бенчмарк HRVideoBench для оценки понимания видео высокого разрешения.'}, 'en': {'title': 'Enhancing Video Understanding with VISTA: A Data-Centric Approach', 'desc': "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."}, 'zh': {'title': 'VISTA：提升视频理解的新方法', 'desc': '当前的大型多模态模型在处理长时长或高分辨率视频时面临重大挑战，主要是由于缺乏高质量的数据集。为了解决这个问题，我们提出了VISTA，一个简单而有效的视频时空增强框架，能够从现有的视频-字幕数据集中合成长时长和高分辨率的视频指令对。VISTA通过空间和时间的组合，创建新的合成视频，并生成与这些新合成视频相关的问题-答案对。通过在我们的数据上微调各种视频多模态模型，平均提高了3.3%的性能，进一步验证了我们框架的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.18499', 'title': 'GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation', 'url': 'https://huggingface.co/papers/2411.18499', 'abstract': 'Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.', 'score': 16, 'issue_id': 914, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'bec1bf0079934886', 'authors': ['Pengfei Zhou', 'Xiaopeng Peng', 'Jiajun Song', 'Chuanhao Li', 'Zhaopan Xu', 'Yue Yang', 'Ziyao Guo', 'Hao Zhang', 'Yuqi Lin', 'Yefei He', 'Lirui Zhao', 'Shuo Liu', 'Tianhua Li', 'Yuxuan Xie', 'Xiaojun Chang', 'Yu Qiao', 'Wenqi Shao', 'Kaipeng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.18499.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#games'], 'emoji': '🔀', 'ru': {'title': 'Новый бенчмарк для оценки мультимодальной генерации', 'desc': 'Статья представляет новый набор данных GATE OpenING для оценки мультимодальных языковых моделей в задачах генерации чередующегося текстово-визуального контента. OpenING включает 5400 аннотированных примеров из 56 реальных задач. Авторы также предлагают модель IntJudge для оценки генерации открытого типа, которая превосходит GPT-based оценщиков на 11.34%. Эксперименты показывают, что существующие методы генерации чередующегося контента имеют значительный потенциал для улучшения.'}, 'en': {'title': 'Bridging the Gap in Multimodal Generation with OpenING!', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators.'}, 'zh': {'title': '推动多模态生成的基准与评估', 'desc': '多模态大型语言模型（MLLMs）在视觉理解和生成任务上取得了显著进展。然而，生成交错的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。为了解决这一问题，我们引入了GATE OpenING（OpenING），这是一个包含5400个高质量人类标注实例的综合基准，涵盖56个真实世界任务。我们的研究还提出了IntJudge，一个用于评估开放式多模态生成方法的评估模型，显示出与人类判断的高一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.01819', 'title': 'Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2412.01819', 'abstract': 'This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.', 'score': 15, 'issue_id': 914, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '25cf4512f592aea4', 'authors': ['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.01819.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#video', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Switti: быстрый и эффективный генератор изображений по тексту', 'desc': 'Статья представляет Switti - трансформер для генерации изображений по текстовому описанию. Авторы предлагают архитектурные модификации для улучшения сходимости и производительности авторегрессионных моделей. На основе наблюдения о слабой зависимости карт внимания от предыдущих масштабов, разработана неавторегрессионная версия, обеспечивающая ускорение и снижение потребления памяти. Исследование также показывает, что отключение guidance на высоких разрешениях дополнительно ускоряет генерацию и улучшает детализацию.'}, 'en': {'title': 'Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers', 'desc': 'This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods.'}, 'zh': {'title': 'Switti：加速文本到图像生成的变换器', 'desc': '本文介绍了Switti，一种用于文本到图像生成的尺度变换器。我们从现有的下一尺度预测自回归模型出发，探索其在T2I生成中的应用，并提出架构修改以提高收敛性和整体性能。研究发现，我们的预训练尺度自回归模型的自注意力图对前一尺度的依赖性较弱，因此我们提出了一种非自回归的替代方案，能够实现约11%的采样加速和更低的内存使用，同时生成质量略有提升。此外，我们发现高分辨率尺度下的无分类器引导通常是不必要的，甚至可能会降低性能。'}}}, {'id': 'https://huggingface.co/papers/2412.00154', 'title': 'o1-Coder: an o1 Replication for Coding', 'url': 'https://huggingface.co/papers/2412.00154', 'abstract': "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .", 'score': 14, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '4a9b72e0b9a6ba64', 'authors': ['Yuxiang Zhang', 'Shangxi Wu', 'Yuqi Yang', 'Jiangming Shu', 'Jinlin Xiao', 'Chao Kong', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.00154.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'O1-CODER: Усиление ИИ для программирования через Систему-2 мышления', 'desc': 'Технический отчет представляет O1-CODER - попытку воспроизвести модель o1 от OpenAI для задач программирования. Модель интегрирует обучение с подкреплением и метод Монте-Карло для улучшения способностей мышления Системы-2. Фреймворк включает обучение генератора тестовых случаев, использование MCTS для генерации кода с процессами рассуждения, и итеративную доводку модели политики. Отчет также рассматривает возможности и проблемы развертывания подобных o1 моделей в реальных приложениях.'}, 'en': {'title': 'Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking', 'desc': "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."}, 'zh': {'title': 'O1-CODER：提升编码任务的智能模型', 'desc': '本文介绍了O1-CODER，这是一个旨在复制OpenAI的o1模型，专注于编码任务的技术报告。该模型结合了强化学习（RL）和蒙特卡洛树搜索（MCTS），以增强其系统2思维能力。框架中包括训练测试用例生成器（TCG）以进行标准化代码测试，利用MCTS生成带有推理过程的代码数据，并迭代微调策略模型，初步生成伪代码，随后生成完整代码。报告还讨论了在实际应用中部署类似o1模型的机遇和挑战，建议转向系统2范式，并强调环境状态更新的重要性。'}}}, {'id': 'https://huggingface.co/papers/2411.18671', 'title': 'TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video', 'url': 'https://huggingface.co/papers/2411.18671', 'abstract': 'In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.', 'score': 13, 'issue_id': 909, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'f99e1015e9222dc6', 'authors': ['Jinyuan Qu', 'Hongyang Li', 'Shilong Liu', 'Tianhe Ren', 'Zhaoyang Zeng', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18671.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#video', '#cv', '#optimization'], 'emoji': '👁️', 'ru': {'title': 'Контекстное внимание для надежного отслеживания точек в видео', 'desc': 'TAPTRv3 - это улучшенная версия TAPTRv2 для более надежного отслеживания точек в длинных видео. Система использует пространственный и временной контекст для повышения качества запроса признаков. Введены два новых механизма: Context-aware Cross-Attention (CCA) для улучшения пространственного запроса и Visibility-aware Long-Temporal Attention (VLTA) для временного запроса. TAPTRv3 значительно превосходит предыдущую версию и достигает наилучших результатов на сложных наборах данных.'}, 'en': {'title': 'Enhancing Video Point Tracking with TAPTRv3', 'desc': 'TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets.'}, 'zh': {'title': 'TAPTRv3：长视频点跟踪的新突破', 'desc': '本文介绍了TAPTRv3，这是在TAPTRv2基础上开发的，旨在提高长视频中的点跟踪鲁棒性。TAPTRv2是一个简单的类似DETR的框架，可以准确跟踪现实视频中的任意点，而无需成本体积。TAPTRv3通过利用空间和时间上下文来改善特征查询，从而在长视频中实现更稳健的跟踪。我们提出了上下文感知交叉注意力（CCA）和可见性感知长时间注意力（VLTA），显著提升了特征查询的质量，超越了TAPTRv2，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.00100', 'title': 'Steering Rectified Flow Models in the Vector Field for Controlled Image Generation', 'url': 'https://huggingface.co/papers/2412.00100', 'abstract': 'Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.', 'score': 12, 'issue_id': 911, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '5fceae277a0e3d85', 'authors': ['Maitreya Patel', 'Song Wen', 'Dimitris N. Metaxas', 'Yezhou Yang'], 'affiliations': ['Arizona State University', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00100.jpg', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#diffusion', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'FlowChef: Управление векторным полем для эффективной генерации изображений', 'desc': 'Статья представляет новый подход к управляемой генерации изображений под названием FlowChef. Он основан на использовании динамики векторного поля в ректифицированных потоковых моделях (RFM) для эффективного управления траекторией шумоподавления. FlowChef позволяет решать задачи управляемой генерации изображений, линейных обратных задач и редактирования изображений без дополнительного обучения или инверсии. Результаты показывают, что FlowChef значительно превосходит существующие методы по производительности, требованиям к памяти и времени вычислений.'}, 'en': {'title': 'FlowChef: Revolutionizing Controlled Image Generation with Efficiency', 'desc': 'This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field.'}, 'zh': {'title': 'FlowChef：高效的受控图像生成新方法', 'desc': '扩散模型在生成真实感图像、图像编辑和解决逆问题方面表现出色，但修正流模型在这些任务中仍未得到充分探索。现有的基于扩散模型的方法通常需要额外的训练，缺乏对预训练潜在模型的泛化能力，并且在性能上表现不佳，计算资源消耗大。本文提出FlowChef，通过有效引导去噪轨迹，利用向量场的特性，实现了受控图像生成任务，且无需额外训练或复杂的反向传播。实验结果表明，FlowChef在性能、内存和时间需求上显著优于基线方法，达到了新的最先进结果。'}}}, {'id': 'https://huggingface.co/papers/2412.01199', 'title': 'TinyFusion: Diffusion Transformers Learned Shallow', 'url': 'https://huggingface.co/papers/2412.01199', 'abstract': 'Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.', 'score': 11, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '7d1de7010c3fabd7', 'authors': ['Gongfan Fang', 'Kunjun Li', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.01199.jpg', 'data': {'categories': ['#inference', '#diffusion', '#training', '#optimization', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'TinyFusion: Эффективная обрезка диффузионных трансформеров без потери качества', 'desc': 'TinyFusion - это метод обрезки глубины для уменьшения количества параметров в диффузионных трансформерах. Он использует дифференцируемую технику сэмплирования для обучаемой обрезки и оптимизирует производительность модели после дообучения. TinyFusion превосходит существующие методы обрезки и хорошо обобщается на различные архитектуры. Эксперименты показывают, что метод позволяет создать компактный диффузионный трансформер с двукратным ускорением при сохранении высокого качества генерации изображений.'}, 'en': {'title': 'TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers', 'desc': 'This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics.'}, 'zh': {'title': 'TinyFusion：高效剪枝，提升扩散变换器性能', 'desc': '本论文提出了一种名为TinyFusion的深度剪枝方法，旨在减少扩散变换器中的冗余层，从而降低推理开销。我们的方法通过端到端学习实现剪枝，并确保剪枝后的模型在微调后能够恢复强大的性能。TinyFusion引入了一种可微分采样技术，使得剪枝过程可学习，并与共同优化的参数结合，以模拟未来的微调效果。实验结果表明，TinyFusion在扩散变换器的层剪枝方面优于现有的方法，展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2412.00568', 'title': 'The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning', 'url': 'https://huggingface.co/papers/2412.00568', 'abstract': 'Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.', 'score': 10, 'issue_id': 923, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': '1352e219e54ac56e', 'authors': ['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno Régaldo-Saint Blancard', 'François Rozet', 'Liam H. Parker', 'Miles Cranmer', 'Shirley Ho'], 'affiliations': ['CEA DAM', 'Cornell University', 'Flatiron Institute', 'Los Alamos National Laboratory', 'New York University', 'Polymathic AI', 'Princeton University', 'Rutgers University', 'University of California, Berkeley', 'University of Cambridge', 'University of Colorado, Boulder', 'University of Liège', 'University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2412.00568.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'Well: масштабный бенчмарк для суррогатных моделей в физическом моделировании', 'desc': "Статья представляет новый набор данных под названием 'Well' для оценки суррогатных моделей машинного обучения в области численного моделирования. 'Well' содержит 15 ТБ данных из 16 наборов, охватывающих различные области физики, включая биологические системы, гидродинамику и астрофизику. Авторы предоставляют унифицированный интерфейс PyTorch для обучения и оценки моделей на этих данных. Набор данных призван помочь исследователям в разработке более эффективных методов машинного обучения для ускорения рабочих процессов, основанных на симуляциях."}, 'en': {'title': 'Unlocking Complex Simulations with the Well: A New Dataset for Machine Learning', 'desc': 'This paper presents a large-scale collection of datasets called the Well, designed to enhance machine learning surrogate models for simulation-based workflows. The Well contains 15TB of data across 16 diverse datasets, covering various physical systems such as fluid dynamics and biological systems. By providing a unified PyTorch interface, the Well facilitates the training and evaluation of machine learning models on these complex datasets. The authors also introduce example baselines to demonstrate the unique challenges posed by the intricate dynamics represented in the Well.'}, 'zh': {'title': '机器学习加速仿真：探索"Well"数据集', 'desc': '本文介绍了一种基于机器学习的替代模型，旨在加速基于仿真的工作流程。我们提出了一个名为"Well"的大规模数据集，包含多种时空物理系统的数值仿真数据，总计15TB，涵盖生物系统、流体动力学、声散射等多个领域。该数据集为研究人员提供了丰富的资源，以评估新方法的有效性，并可单独使用或作为更广泛基准套件的一部分。为了方便使用，我们提供了统一的PyTorch接口，帮助训练和评估模型，并展示了新的挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.00174', 'title': 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters', 'url': 'https://huggingface.co/papers/2412.00174', 'abstract': "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.", 'score': 10, 'issue_id': 913, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '5a30d9c535229ab0', 'authors': ['Jianping Jiang', 'Weiye Xiao', 'Zhengyu Lin', 'Huaizhong Zhang', 'Tianxiang Ren', 'Yang Gao', 'Zhiqian Lin', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.00174.jpg', 'data': {'categories': ['#synthetic', '#games', '#dataset', '#agents', '#3d', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Создание социально умных 3D персонажей для виртуальной реальности', 'desc': 'В статье представлен SOLAMI - первый сквозной фреймворк для социального моделирования зрения-языка-действия (VLA) для иммерсивного взаимодействия с 3D автономными персонажами. Фреймворк включает в себя унифицированную архитектуру социального VLA для генерации мультимодальных ответов на основе пользовательского ввода. Для решения проблемы нехватки данных авторы создали синтетический набор данных SynMSI, используя существующие наборы данных о движении. Также разработан VR-интерфейс для иммерсивного взаимодействия пользователей с персонажами.'}, 'en': {'title': 'Empowering 3D Characters with Social Intelligence through SOLAMI', 'desc': 'This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency.'}, 'zh': {'title': '赋予3D角色社交智能的创新框架', 'desc': '本文介绍了SOLAMI，这是第一个端到端的社会视觉-语言-动作（VLA）建模框架，旨在与3D自主角色进行沉浸式互动。SOLAMI从三个方面构建3D自主角色：首先，提出了统一的社会VLA架构，根据用户的多模态输入生成多模态响应（语音和动作），以驱动角色进行社交互动。其次，介绍了SynMSI，这是一个合成的多模态社交互动数据集，通过自动化流程生成，解决了数据稀缺的问题。最后，开发了一个虚拟现实接口，使用户能够与这些角色进行沉浸式互动，实验结果表明，该框架能够提供更精确和自然的角色响应。'}}}, {'id': 'https://huggingface.co/papers/2412.01822', 'title': 'VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models', 'url': 'https://huggingface.co/papers/2412.01822', 'abstract': 'The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate "verbalizers" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs\' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.', 'score': 9, 'issue_id': 916, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '0c3ac9e6ce3f4cd2', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yu-Chiang Frank Wang', 'Yong Man Ro', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01822.jpg', 'data': {'categories': ['#dataset', '#small_models', '#architecture', '#transfer_learning', '#optimization', '#training', '#benchmark', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Эффективное визуально-языковое обучение через послойную вербализацию', 'desc': "Исследователи представили новое семейство моделей визуально-языкового обучения под названием VLsI, которое фокусируется на эффективности без ущерба для точности. VLsI использует уникальный процесс послойной дистилляции, вводя промежуточные 'вербализаторы', которые отображают признаки из каждого слоя в пространство естественного языка. Этот подход позволяет меньшим моделям гибко согласовываться с процессами рассуждений более крупных моделей. Авторы продемонстрировали значительное улучшение производительности VLsI по сравнению с GPT-4V на десяти сложных визуально-языковых тестах без необходимости увеличения размера модели или изменения архитектуры."}, 'en': {'title': 'Efficient Vision-Language Models with Layer-wise Distillation', 'desc': "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."}, 'zh': {'title': '高效视觉语言模型的创新之路', 'desc': '本文提出了一种新的视觉语言模型（VLM）家族，称为VLsI，旨在提高模型的效率而不牺牲准确性。VLsI采用了一种独特的层级蒸馏过程，通过引入中间的“语言化器”，将每一层的特征映射到自然语言空间，使得较小的VLM能够灵活地与较大VLM的推理过程对齐。该方法有效缓解了输出模仿中常见的训练不稳定性，并通过对齐小型VLM的层级进展与大型VLM的层级进展，超越了典型的最终层调优。我们在十个具有挑战性的视觉语言基准上验证了VLsI，取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2412.01064', 'title': 'FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2412.01064', 'abstract': 'With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '1978e3ecf42cac0c', 'authors': ['Taekyung Ki', 'Dongchan Min', 'Gyoungsu Chae'], 'affiliations': ['DeepBrain AI Inc.', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.01064.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal', '#video', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Генерация реалистичных видео с говорящим портретом на основе аудио и потоковых моделей', 'desc': 'Статья представляет FLOAT - метод генерации видео с говорящим портретом на основе аудио, используя генеративную модель сопоставления потоков. Авторы переносят генеративное моделирование из пиксельного латентного пространства в пространство движения, что позволяет эффективно создавать согласованные во времени движения. Метод включает предиктор векторного поля на основе трансформера с покадровым механизмом обусловливания. FLOAT также поддерживает усиление эмоций на основе речи, позволяя естественно добавлять выразительные движения.'}, 'en': {'title': 'FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency', 'desc': 'This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity.'}, 'zh': {'title': 'FLOAT：高效音频驱动的人像视频生成', 'desc': '本论文提出了一种名为FLOAT的音频驱动人像视频生成方法，基于流匹配生成模型。我们将生成建模从基于像素的潜在空间转移到学习的运动潜在空间，从而实现时间一致的运动设计。该方法引入了基于变换器的向量场预测器，并采用简单有效的逐帧条件机制。实验结果表明，我们的方法在视觉质量、运动保真度和效率方面优于现有的音频驱动人像生成方法。'}}}, {'id': 'https://huggingface.co/papers/2411.18933', 'title': 'Efficient Track Anything', 'url': 'https://huggingface.co/papers/2411.18933', 'abstract': 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '86ff7f63f69fa104', 'authors': ['Yunyang Xiong', 'Chong Zhou', 'Xiaoyu Xiang', 'Lemeng Wu', 'Chenchen Zhu', 'Zechun Liu', 'Saksham Suri', 'Balakrishnan Varadarajan', 'Ramya Akula', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Bilge Soran', 'Vikas Chandra'], 'affiliations': ['Meta AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18933.jpg', 'data': {'categories': ['#video', '#small_models', '#benchmark', '#optimization', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная сегментация видео на мобильных устройствах', 'desc': 'EfficientTAMs - это облегченные модели для сегментации и отслеживания объектов в видео. Они основаны на использовании простого Vision Transformer (ViT) в качестве энкодера изображений и эффективного модуля памяти. EfficientTAMs показывают результаты, сравнимые с SAM 2, но работают в 2 раза быстрее и имеют в 2,4 раза меньше параметров. На мобильных устройствах, таких как iPhone 15 Pro Max, EfficientTAMs могут выполнять сегментацию объектов в видео со скоростью около 10 кадров в секунду.'}, 'en': {'title': 'EfficientTAMs: Lightweight Video Segmentation for Mobile Devices', 'desc': 'The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms.'}, 'zh': {'title': '高效视频物体分割，轻量化模型新选择', 'desc': 'Segment Anything Model 2（SAM 2）是一种强大的视频物体分割和跟踪工具。为了提高性能，SAM 2使用了多阶段图像编码器和记忆机制，但其计算复杂性限制了在移动设备上的应用。为了解决这个问题，我们提出了高效的跟踪模型EfficientTAMs，它使用轻量级的视觉变换器（ViT）作为图像编码器，并引入高效的记忆模块，从而降低了计算复杂性。我们的EfficientTAMs在多个视频分割基准测试中表现出色，能够在移动设备上以合理的质量进行视频物体分割。'}}}, {'id': 'https://huggingface.co/papers/2411.17459', 'title': 'WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model', 'url': 'https://huggingface.co/papers/2411.17459', 'abstract': 'Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.', 'score': 7, 'issue_id': 909, 'pub_date': '2024-11-26', 'pub_date_card': {'ru': '26 ноября', 'en': 'November 26', 'zh': '11月26日'}, 'hash': '9b68162718865b81', 'authors': ['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan'], 'affiliations': ['Peking University', 'Peng Cheng Laboratory', 'Rabbitpre Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2411.17459.jpg', 'data': {'categories': ['#diffusion', '#data', '#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': '🌊', 'ru': {'title': 'Эффективное кодирование видео с помощью вейвлетов', 'desc': 'Статья представляет новый метод кодирования видео под названием Wavelet Flow VAE (WF-VAE). Этот автоэнкодер использует многоуровневое вейвлет-преобразование для эффективного кодирования низкочастотной информации в видео. WF-VAE решает проблему вычислительных ограничений при обработке видео высокого разрешения и большой длительности в латентных видео-диффузионных моделях (LVDM). Метод также включает технику Causal Cache для сохранения целостности латентного пространства при поблочной обработке.'}, 'en': {'title': 'Efficient Video Encoding with Wavelet Flow VAE', 'desc': 'The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs.'}, 'zh': {'title': '小波流VAE：高效视频编码的新方法', 'desc': '视频变分自编码器（VAE）将视频编码为低维潜在空间，是大多数潜在视频扩散模型（LVDMs）的关键组成部分，能够降低模型训练成本。然而，随着生成视频的分辨率和时长增加，视频VAE的编码成本成为训练LVDMs的瓶颈。此外，大多数LVDMs采用的块状推理方法在处理长时长视频时可能导致潜在空间的不连续性。为了解决计算瓶颈，我们提出了小波流VAE（WF-VAE），通过多级小波变换有效编码视频的关键信息，并引入因果缓存方法以保持潜在空间的完整性。'}}}, {'id': 'https://huggingface.co/papers/2412.01316', 'title': 'Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation', 'url': 'https://huggingface.co/papers/2412.01316', 'abstract': 'We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.', 'score': 6, 'issue_id': 910, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '5bca597a08ec0a14', 'authors': ['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang'], 'affiliations': ['01.AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.01316.jpg', 'data': {'categories': ['#video', '#dataset', '#architecture', '#diffusion', '#long_context'], 'emoji': '🎬', 'ru': {'title': 'Presto: Революция в генерации длинных видео с помощью ИИ', 'desc': 'Presto - это новая модель диффузии видео, разработанная для генерации 15-секундных видеороликов с долгосрочной согласованностью и богатым содержанием. Модель использует стратегию Segmented Cross-Attention (SCA), которая разделяет скрытые состояния на сегменты вдоль временного измерения, позволяя каждому сегменту перекрестно обращаться к соответствующей подписи. Для обучения модели был создан датасет LongTake-HD, содержащий 261 тысячу видео с богатым содержанием и согласованностью сценариев. Эксперименты показывают, что Presto превосходит существующие методы генерации видео по показателям семантической оценки и степени динамичности.'}, 'en': {'title': 'Presto: Revolutionizing Video Generation with Long-Range Coherence', 'desc': 'Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation.'}, 'zh': {'title': 'Presto：生成长时间一致性视频的新方法', 'desc': '我们介绍了一种新的视频扩散模型Presto，旨在生成具有长时间一致性和丰富内容的15秒视频。为了解决在长时间内保持场景多样性的挑战，我们提出了一种分段交叉注意力(SCA)策略，该策略将隐藏状态沿时间维度分段，使每个段能够与相应的子标题进行交叉关注。SCA不需要额外的参数，可以无缝集成到现有的基于DiT的架构中。我们的实验表明，Presto在视频生成方面显著优于现有的最先进方法，提升了内容丰富性和长距离一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.01800', 'title': 'PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos', 'url': 'https://huggingface.co/papers/2412.01800', 'abstract': 'Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.', 'score': 4, 'issue_id': 917, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '43cc035146493599', 'authors': ['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peking University', 'Sun Yat-sen University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.01800.jpg', 'data': {'categories': ['#dataset', '#video', '#open_source', '#optimization', '#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': '🎮', 'ru': {'title': 'PhysVLM: Обучение видеоязыковых моделей физическому здравому смыслу через игровые глитчи', 'desc': 'Исследователи представили новый бенчмарк PhysGame для оценки понимания физических закономерностей в видеоязыковых моделях (Video LLM) на основе геймплейных видео с глитчами. Они создали набор данных PhysInstruct для обучения моделей физическому здравому смыслу и набор PhysDPO для оптимизации предпочтений. На базе этих наборов данных была разработана модель PhysVLM, показавшая лучшие результаты как на специализированном бенчмарке PhysGame, так и на общих тестах понимания видео.'}, 'en': {'title': 'Enhancing Video LLMs with Physical Commonsense Understanding', 'desc': 'This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models.'}, 'zh': {'title': '提升视频模型的物理常识理解能力', 'desc': '本文介绍了一种新颖的基准测试PhysGame，用于评估视频大语言模型在游戏视频中对物理常识的理解能力。游戏视频中常常出现违反物理常识的故障，这使得它们成为评估模型能力的有效数据源。我们还创建了PhysInstruct和PhysDPO两个数据集，以帮助模型学习物理常识并优化其偏好。通过这些数据集，我们提出了PhysVLM，一个增强物理知识的视频大语言模型，并在多个基准测试中展示了其优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.19939', 'title': 'VLSBench: Unveiling Visual Leakage in Multimodal Safety', 'url': 'https://huggingface.co/papers/2411.19939', 'abstract': 'Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench', 'score': 4, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '03d8c636cf8c9486', 'authors': ['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2411.19939.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#leakage', '#open_source', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Новый взгляд на безопасность мультимодальных ИИ-моделей', 'desc': 'Статья рассматривает проблему безопасности мультимодальных больших языковых моделей (MLLM). Авторы обнаружили феномен утечки визуальной информации о безопасности (VSIL) в существующих мультимодальных тестах безопасности. Для решения этой проблемы они создали новый набор данных VLSBench, содержащий 2400 пар изображение-текст без VSIL. Эксперименты показали, что VLSBench представляет значительную сложность для современных MLLM и демонстрирует необходимость мультимодальной настройки для сценариев без VSIL.'}, 'en': {'title': 'Unveiling Safety in Multimodal Models: The VSIL Challenge', 'desc': 'This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it.'}, 'zh': {'title': '多模态安全性的新挑战与解决方案', 'desc': '这篇论文探讨了多模态大型语言模型（MLLMs）在安全性方面的挑战。研究发现，文本去学习可以与使用图像-文本对训练的模型在安全性表现上相当。作者指出，现有的多模态安全基准存在视觉安全信息泄漏（VSIL）问题，这使得模型能够轻易拒绝敏感的文本-图像查询。为了解决这个问题，研究者构建了一个新的多模态视觉无泄漏安全基准（VLSBench），以更好地评估模型在没有VSIL情况下的安全性。'}}}, {'id': 'https://huggingface.co/papers/2412.00947', 'title': 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information', 'url': 'https://huggingface.co/papers/2412.00947', 'abstract': 'Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.', 'score': 4, 'issue_id': 911, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': '7135f8936b0d3ee8', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Sarkar Snigdha Sarathi Das', 'Ranran Haoran Zhang', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00947.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#cv', '#synthetic', '#training'], 'emoji': '👁️', 'ru': {'title': 'VisOnlyQA: новый путь к улучшению визуального восприятия AI', 'desc': 'Исследователи представили новый датасет VisOnlyQA для оценки визуального восприятия больших визуально-языковых моделей (LVLM). Датасет фокусируется на геометрической и числовой информации в научных изображениях, позволяя анализировать способности моделей к тонкому визуальному восприятию. Эксперименты показали, что даже передовые LVLM, такие как GPT-4o и Gemini 1.5 Pro, плохо справляются с задачами VisOnlyQA, в то время как люди демонстрируют почти идеальные результаты. Исследование подчеркивает необходимость улучшения как обучающих данных, так и архитектур моделей для повышения качества визуального восприятия LVLM.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs with VisOnlyQA', 'desc': 'This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models.'}, 'zh': {'title': '提升视觉感知，助力大型视觉语言模型', 'desc': '本研究介绍了一个新的数据集VisOnlyQA，旨在直接评估大型视觉语言模型（LVLMs）在科学图形中几何和数值信息问题上的视觉感知能力。该数据集包含1200个多项选择题，涵盖四类图形的12个任务，帮助分析LVLMs对细粒度视觉信息的理解。实验结果显示，评估的20个LVLMs在视觉感知任务上的表现较差，而人类的表现几乎完美。通过合成训练数据进行微调可以提升LVLMs的视觉感知能力，但改进效果有限，且更强的语言模型能够提高视觉感知能力。'}}}, {'id': 'https://huggingface.co/papers/2412.00176', 'title': 'Art-Free Generative Models: Art Creation Without Graphic Art Knowledge', 'url': 'https://huggingface.co/papers/2412.00176', 'abstract': 'We explore the question: "How much prior art knowledge is needed to create art?" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.', 'score': 3, 'issue_id': 921, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '41f372dd1c030fbd', 'authors': ['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba'], 'affiliations': ['MIT', 'Northeastern University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00176.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic', '#games', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект творит шедевры с минимальным обучением', 'desc': 'Исследователи изучают вопрос о необходимом объеме предварительных знаний об искусстве для его создания. Они предлагают модель генерации изображений по тексту, обученную без доступа к контенту, связанному с искусством. Затем авторы представляют простой, но эффективный метод обучения адаптера для создания искусства, используя лишь несколько примеров выбранных художественных стилей. Эксперименты показывают, что искусство, созданное с помощью этого метода, воспринимается пользователями на уровне, сравнимом с произведениями моделей, обученных на больших наборах данных, богатых искусством.'}, 'en': {'title': 'Creating Art with Minimal Prior Knowledge', 'desc': "This paper investigates the necessity of prior art knowledge in generating art through a text-to-image model. The authors propose a novel approach that trains the model without any art-related content, relying instead on a small number of examples from specific artistic styles to create an 'art adapter'. Their experiments reveal that the art produced by this method is perceived similarly to that generated by models trained on extensive art datasets. Additionally, they employ data attribution techniques to demonstrate how both artistic and non-artistic examples influence the development of new artistic styles."}, 'zh': {'title': '创造艺术无需丰富的艺术知识', 'desc': '我们探讨了创造艺术需要多少先前的艺术知识。为此，我们提出了一种文本到图像生成模型，该模型在没有艺术相关内容的情况下进行训练。我们还引入了一种简单有效的方法，仅使用少量选定艺术风格的示例来学习艺术适配器。实验结果表明，使用我们的方法生成的艺术作品在用户眼中与在大型艺术丰富数据集上训练的模型生成的艺术作品相当。'}}}, {'id': 'https://huggingface.co/papers/2412.01250', 'title': 'Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input', 'url': 'https://huggingface.co/papers/2412.01250', 'abstract': 'Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.', 'score': 3, 'issue_id': 915, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '8b768de35e4c5114', 'authors': ['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang'], 'affiliations': ['Fondazione Bruno Kessler', 'Polytechnic of Turin', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Verona'], 'pdf_title_img': 'assets/pdf/title_img/2412.01250.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#reasoning', '#agents'], 'emoji': '🗺️', 'ru': {'title': 'Интерактивная навигация робота с уточнением цели у человека', 'desc': 'Статья представляет новую задачу навигации по инстансам объектов - Collaborative Instance Navigation (CoIN), где агент взаимодействует с человеком во время навигации для уточнения целевого объекта. Предложен метод AIUTA, использующий Vision Language Models и Large Language Models для обработки визуальной информации и генерации вопросов. AIUTA включает модули Self-Questioner для самоанализа наблюдений и Interaction Trigger для определения необходимости задать вопрос пользователю. Авторы также представили бенчмарк CoIN-Bench для оценки подобных систем.'}, 'en': {'title': 'Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration', 'desc': 'This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods.'}, 'zh': {'title': '协作实例导航：让机器更懂人类指令', 'desc': '现有的实例目标导航任务通常需要用户提供详细的描述，但在现实中，这种要求往往不切实际。为了解决这个问题，我们提出了一种新的任务，称为协作实例导航（CoIN），通过动态的代理-用户互动来解决导航中的不确定性。我们的方法，代理-用户互动与不确定性意识（AIUTA），结合了视觉语言模型和大型语言模型的能力，能够在导航过程中主动与用户对话。通过引入CoIN-Bench基准，我们的AIUTA方法在实例导航中表现出色，展示了处理用户输入的灵活性。'}}}, {'id': 'https://huggingface.co/papers/2411.19799', 'title': 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge', 'url': 'https://huggingface.co/papers/2411.19799', 'abstract': 'The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.', 'score': 3, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '7b0cbdd28adecf2c', 'authors': ['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam', 'Perttu Isotalo', 'Maral Jabbarishiviari', 'Börje F. Karlsson', 'Eldar Khalilov', 'Christopher Klamm', 'Fajri Koto', 'Dominik Krzemiński', 'Gabriel Adriano de Melo', 'Syrielle Montariol', 'Yiyang Nan', 'Joel Niklaus', 'Jekaterina Novikova', 'Johan Samir Obando Ceron', 'Debjit Paul', 'Esther Ploeger', 'Jebish Purbey', 'Swati Rajwal', 'Selvan Sunitha Ravi', 'Sara Rydell', 'Roshan Santhosh', 'Drishti Sharma', 'Marjana Prifti Skenduli', 'Arshia Soltani Moakhar', 'Bardia Soltani Moakhar', 'Ran Tamir', 'Ayush Kumar Tarun', 'Azmine Toushik Wasi', 'Thenuka Ovin Weerasinghe', 'Serhan Yilmaz', 'Mike Zhang', 'Imanol Schlag', 'Marzieh Fadaee', 'Sara Hooker', 'Antoine Bosselut'], 'affiliations': ['Cohere For AI', 'Cohere For AI Community', 'EPFL', 'ETH Zurich', 'Swiss AI Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2411.19799.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#benchmark', '#reasoning'], 'emoji': '🌍', 'ru': {'title': 'Глобальная оценка многоязычных ИИ-моделей в локальных контекстах', 'desc': 'Статья посвящена проблеме неравномерной эффективности больших языковых моделей (LLM) для разных языков. Авторы создали набор данных INCLUDE из 197,243 пар вопросов и ответов на 44 языках для оценки многоязычных LLM. Этот ресурс основан на местных экзаменационных материалах и охватывает различные региональные контексты. INCLUDE позволяет оценивать знания и способности к рассуждению многоязычных LLM в реальных языковых средах их применения.'}, 'en': {'title': 'Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs', 'desc': 'This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge.'}, 'zh': {'title': '提升多语言模型的实际应用能力', 'desc': '这篇论文讨论了大型语言模型（LLM）在不同语言之间的性能差异，这影响了它们在许多地区的有效应用。为了克服多语言LLM开发中的瓶颈，作者构建了一个包含197,243个问答对的评估套件，来源于当地考试材料。这个新资源INCLUDE是一个全面的知识和推理中心基准，涵盖44种书面语言，旨在评估多语言LLM在实际语言环境中的表现。通过这种方式，研究希望提升生成性人工智能工具在不同社区的经济和社会价值。'}}}, {'id': 'https://huggingface.co/papers/2412.01821', 'title': 'World-consistent Video Diffusion with Explicit 3D Modeling', 'url': 'https://huggingface.co/papers/2412.01821', 'abstract': 'Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.', 'score': 2, 'issue_id': 922, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '843891601e85de06', 'authors': ['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu'], 'affiliations': ['Apple', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.01821.jpg', 'data': {'categories': ['#3d', '#benchmark', '#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': '3D-согласованная генерация видео с помощью диффузионных моделей', 'desc': 'Статья представляет новую модель World-consistent Video Diffusion (WVD) для генерации 3D-согласованного контента. WVD использует XYZ-изображения для явного 3D-контроля в процессе диффузии. Модель обучается совместному распределению RGB и XYZ кадров, что позволяет решать различные задачи, включая генерацию 3D из одного изображения и создание видео с контролем камеры. WVD демонстрирует конкурентоспособные результаты на нескольких эталонных тестах.'}, 'en': {'title': 'Unifying 3D Consistency in Video Generation with WVD', 'desc': 'This paper introduces World-consistent Video Diffusion (WVD), a new framework that enhances video and image generation by incorporating 3D supervision. WVD uses XYZ images to provide global 3D coordinates for each pixel, allowing for more accurate and consistent 3D content generation. The framework employs a diffusion transformer to learn the relationship between RGB and XYZ frames, enabling tasks like generating new RGB frames from 3D data. Overall, WVD offers a versatile solution for various tasks, achieving strong performance across benchmarks with a single pretrained model.'}, 'zh': {'title': '统一3D一致性的视频生成新框架', 'desc': '最近，扩散模型在图像和视频生成方面取得了显著进展，能够在单帧和多帧上下文中实现逼真的视觉合成。然而，这些模型在高效且明确地生成3D一致内容方面仍然存在挑战。为了解决这个问题，我们提出了世界一致视频扩散（WVD），这是一个新颖的框架，利用XYZ图像进行明确的3D监督，编码每个图像像素的全局3D坐标。WVD通过灵活的修补策略支持多任务适应性，能够从真实的RGB估计XYZ帧，或沿指定的相机轨迹生成新的RGB帧。'}}}, {'id': 'https://huggingface.co/papers/2412.00319', 'title': 'Improving speaker verification robustness with synthetic emotional utterances', 'url': 'https://huggingface.co/papers/2412.00319', 'abstract': 'A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.   To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': 'e9e71338ed876bc0', 'authors': ['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke'], 'affiliations': ['Amazon Alexa AI, USA', 'Amazon Business, USA', 'Amazon GenAI, USA', 'Amazon Search Experience Science, USA', 'Amazon Web Services, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00319.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#audio'], 'emoji': '🎭', 'ru': {'title': 'CycleGAN для аугментации эмоциональной речи улучшает верификацию говорящего', 'desc': 'Статья предлагает новый подход к верификации говорящего с использованием CycleGAN для аугментации данных эмоциональной речи. Авторы синтезируют эмоциональные речевые сегменты для каждого конкретного диктора, сохраняя при этом уникальную голосовую идентичность. Эксперименты показывают, что модели, обученные на расширенном наборе данных, превосходят базовые модели в задаче верификации говорящих в сценариях с эмоциональной речью. Предложенный метод снижает равную ошибку (EER) до 3.64% относительно базовой линии.'}, 'en': {'title': 'Enhancing Speaker Verification with Synthetic Emotional Data', 'desc': "This paper discusses a speaker verification (SV) system that authenticates speakers based on their voice. A major challenge for these systems is accurately verifying speakers when they express different emotions, as most existing models struggle with emotional speech. The authors propose using a CycleGAN framework to create synthetic emotional speech data, which helps improve the training of SV models. Their results show that incorporating this augmented data significantly enhances the models' performance, reducing error rates in emotional speech verification."}, 'zh': {'title': '情感语音验证的新突破', 'desc': '本研究提出了一种说话人验证系统，旨在确认特定语音样本是否来自特定说话者。该系统面临的主要挑战是如何在不同情感状态下保持一致的性能。为了克服这一问题，我们采用CycleGAN框架进行数据增强，合成每个说话者的情感语音片段，同时保留其独特的声音特征。实验结果表明，使用合成情感数据训练的模型在情感语音验证任务中表现优于基线模型，错误率降低了3.64%。'}}}, {'id': 'https://huggingface.co/papers/2412.00869', 'title': 'Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting', 'url': 'https://huggingface.co/papers/2412.00869', 'abstract': 'Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': '6816796631155a8c', 'authors': ['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'IIIT Hyderabad, India', 'Meta, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00869.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели все еще не мастера аналогий', 'desc': 'Эта статья посвящена исследованию способности больших языковых моделей (LLM) решать пропорциональные аналогии. Авторы создали набор данных из 15 000 вопросов с множественным выбором для завершения аналогий. Они оценили производительность современных LLM в различных настройках промптов с дополнительными знаниями. Результаты показывают, что даже лучшие модели достигают точности лишь 55%, и целевые знания помогают больше, чем примеры или структурированная информация.'}, 'en': {'title': 'Enhancing Analogy Completion in LLMs with Targeted Knowledge', 'desc': 'This paper focuses on the challenge of solving proportional analogies, which are important for assessing cognitive abilities. The authors present a new dataset containing 15,000 multiple-choice questions designed for this purpose. They evaluate how well current Large Language Models (LLMs) perform on these analogies, especially when given different types of knowledge prompts. The findings reveal that while LLMs struggle with these tasks, targeted knowledge prompts significantly improve their performance compared to other types of knowledge.'}, 'zh': {'title': '提升类比能力，知识是关键！', 'desc': '类比是认知的重要部分，比例类比由四个术语组成，常用于评估语言和认知能力。本文介绍了一个包含15,000个多项选择题的比例类比完成数据集，并评估了当前大型语言模型（LLMs）在不同知识增强提示设置下的表现。研究发现，尽管模型经过大量训练，解决比例类比仍然具有挑战性，最佳模型的准确率仅为55%。特别是，提供针对性的知识比提供示例或结构化知识更能帮助模型完成比例类比。'}}}, {'id': 'https://huggingface.co/papers/2412.01408', 'title': 'Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning', 'url': 'https://huggingface.co/papers/2412.01408', 'abstract': 'Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.', 'score': 1, 'issue_id': 918, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'f4c5e5e0485d3969', 'authors': ['Aditya Narayan Sankaran', 'Reza Farahbaksh', 'Noel Crespi'], 'affiliations': ['SAMOVAR, Télécom SudParis Institut Polytechnique de Paris 91120 Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.01408.jpg', 'data': {'categories': ['#low_resource', '#interpretability', '#dataset', '#multilingual', '#audio', '#training'], 'emoji': '🎙️', 'ru': {'title': 'Борьба с оскорблениями в аудио: мало данных, много языков', 'desc': 'Исследование посвящено обнаружению оскорбительного контента в аудио на языках с ограниченными ресурсами, в частности на индийских языках, с использованием Few Shot Learning (FSL). Авторы применяют предобученные аудио-представления из моделей Wav2Vec и Whisper в рамках Model-Agnostic Meta-Learning (MAML) для классификации оскорбительного языка в 10 языках. Эксперименты проводились с различными размерами выборок (50-200) для оценки влияния ограниченных данных на производительность. Исследование демонстрирует способность предобученных моделей к обобщению в условиях ограниченных ресурсов и предоставляет ценные insights для обнаружения оскорбительного языка в многоязычном контексте.'}, 'en': {'title': 'Empowering Low-Resource Languages: Detecting Abuse in Audio with Few Shot Learning', 'desc': 'This paper focuses on detecting abusive language in audio, especially in languages with limited resources, like many Indian languages. It uses Few Shot Learning (FSL) techniques with pre-trained audio models such as Wav2Vec and Whisper to improve detection accuracy. The authors apply the Model-Agnostic Meta-Learning (MAML) framework to classify abusive content across 10 different languages, even with minimal training data. Their findings demonstrate how well pre-trained models can generalize in low-resource settings, providing insights for better abuse detection in multilingual environments.'}, 'zh': {'title': '利用预训练模型提升低资源环境下的辱骂内容检测', 'desc': '本研究探讨了在低资源环境中，特别是音频模式下，检测在线辱骂内容的潜力。我们使用预训练的音频表示，结合少量学习（Few Shot Learning），在印度语言中进行辱骂语言的检测。通过利用Wav2Vec和Whisper等模型的强大表示，我们在ADIMA数据集上进行跨语言的辱骂检测。研究表明，预训练模型在低资源场景中的泛化能力，为多语言环境中的辱骂语言检测提供了有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2411.19477', 'title': 'A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models', 'url': 'https://huggingface.co/papers/2411.19477', 'abstract': 'We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.', 'score': 1, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '6057902d5fcbe3f4', 'authors': ['Yanxi Chen', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2411.19477.jpg', 'data': {'categories': ['#training', '#benchmark', '#architecture', '#optimization'], 'emoji': '🏆', 'ru': {'title': 'Масштабируемый алгоритм улучшения точности больших языковых моделей', 'desc': 'Авторы предлагают двухэтапный алгоритм, который демонстрирует масштабируемый закон для вычислений больших языковых моделей (LLM) во время тестирования. Алгоритм сначала генерирует N кандидатов решений, а затем выбирает лучшее с помощью многораундового турнира на выбывание. Теоретически доказано, что вероятность ошибки алгоритма экспоненциально уменьшается с увеличением N и K. Эмпирические результаты на сложном бенчмарке MMLU-Pro подтверждают эффективность предложенного метода.'}, 'en': {'title': 'Efficient Solution Selection for Large Language Models', 'desc': 'This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark.'}, 'zh': {'title': '高效选择：两阶段算法优化大语言模型计算', 'desc': '我们提出了一种通用的两阶段算法，能够在大语言模型（LLMs）的测试时间计算中实现可证明的扩展规律。该算法首先生成N个候选解，然后通过多轮淘汰赛选择最佳解，每对候选解比较K次，只有胜者进入下一轮。该算法的最简实现仅需使用黑箱LLM，无需外部验证器或奖励模型，总共需要N次(K + 1)高度可并行的LLM调用来解决输入问题。理论证明表明，假设生成的候选解正确的概率为p_{gen} > 0，且正确与错误解的比较能以概率p_{comp} > 0.5识别出正确的胜者，则该算法的失败概率随着N和K的增加呈指数级下降。'}}}, {'id': 'https://huggingface.co/papers/2412.02259', 'title': 'VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation', 'url': 'https://huggingface.co/papers/2412.02259', 'abstract': 'Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.', 'score': 43, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '5a007f38be3e3ba7', 'authors': ['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Peking University', 'University of Central Florida', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02259.jpg', 'data': {'categories': ['#video', '#story_generation', '#games'], 'emoji': '🎬', 'ru': {'title': 'VGoT: Новый уровень в генерации многокадровых видео с сохранением логики повествования', 'desc': 'Статья представляет новый подход к генерации многокадровых видео под названием VideoGen-of-Thought (VGoT). Эта архитектура разделяет процесс на несколько этапов: генерация сценария, создание ключевых кадров, генерация отдельных сцен и механизм сглаживания. VGoT использует встраивания для сохранения идентичности персонажей между сценами и применяет межкадровое сглаживание для обеспечения визуальной согласованности. Эксперименты показывают, что VGoT превосходит существующие методы в создании качественных и связных многокадровых видео.'}, 'en': {'title': 'Revolutionizing Multi-Shot Video Generation with VGoT', 'desc': 'The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots.'}, 'zh': {'title': '多镜头视频生成的新突破', 'desc': '当前的视频生成模型在生成短片方面表现出色，但在创建多镜头、电影般的视频时仍然存在困难。现有模型通常只针对单镜头目标进行训练，因此在保持逻辑故事线和视觉一致性方面显得不足。为此，我们提出了VideoGen-of-Thought（VGoT），这是一种专门为多镜头视频生成设计的协作和无训练架构。VGoT通过脚本生成、关键帧生成和镜头级视频生成等模块化步骤，确保了合理的叙事设计和跨镜头的一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.19943', 'title': "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability", 'url': 'https://huggingface.co/papers/2411.19943', 'abstract': "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.", 'score': 33, 'issue_id': 933, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'aaf523f6bd9412e3', 'authors': ['Zicheng Lin', 'Tian Liang', 'Jiahao Xu', 'Xing Wang', 'Ruilin Luo', 'Chufan Shi', 'Siheng Li', 'Yujiu Yang', 'Zhaopeng Tu'], 'affiliations': ['Tsinghua University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.19943.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#benchmark', '#alignment'], 'emoji': '🔍', 'ru': {'title': 'Повышение точности рассуждений LLM путем выявления критических токенов', 'desc': "Исследование посвящено влиянию отдельных токенов на результаты рассуждений в больших языковых моделях (LLM). Авторы обнаружили существование 'критических токенов', которые приводят к неправильным траекториям рассуждений. Они предложили новый метод cDPO для автоматического распознавания и обучения с учетом критических токенов в процессе выравнивания модели. Экспериментальные результаты на бенчмарках GSM8K и MATH500 с использованием моделей Llama-3 и deepseek-math продемонстрировали эффективность предложенного подхода."}, 'en': {'title': 'Enhancing Reasoning in LLMs by Identifying Critical Tokens', 'desc': "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."}, 'zh': {'title': '识别关键token，提升推理准确性', 'desc': '大型语言模型（LLMs）在推理任务中表现出色，能够通过自回归的方式生成推理过程。本文探讨了单个token对推理任务最终结果的影响，发现存在“关键token”，这些token会导致错误的推理轨迹。我们提出了一种新方法cDPO，旨在自动识别关键token并在对齐过程中进行token级奖励。通过对比正负模型的生成可能性，我们能够识别出在错误轨迹中导致错误结果的关键token，从而提高模型的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2412.01928', 'title': 'MALT: Improving Reasoning with Multi-Agent LLM Training', 'url': 'https://huggingface.co/papers/2412.01928', 'abstract': 'Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step toward "Multi-agent LLM training" (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model\'s specialized capabilities as part of a joint sequential system. We evaluate our approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches.', 'score': 19, 'issue_id': 943, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '980ef49924f6a484', 'authors': ['Sumeet Ramesh Motwani', 'Chandler Smith', 'Rocktim Jyoti Das', 'Markian Rybchuk', 'Philip H. S. Torr', 'Ivan Laptev', 'Fabio Pizzati', 'Ronald Clark', 'Christian Schroeder de Witt'], 'affiliations': ['University of Oxford', 'Cooperative AI Foundation', 'MBZUAI', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.01928.jpg', 'data': {'categories': ['#math', '#training', '#synthetic', '#agents', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Совместное обучение LLM для решения сложных задач рассуждения', 'desc': "Статья представляет новый подход к обучению больших языковых моделей (LLM) для совместной работы над сложными задачами рассуждения. Авторы предлагают метод 'Мульти-агентного обучения LLM' (MALT), использующий последовательную setup с гетерогенными LLM в специализированных ролях: генератор, верификатор и модель уточнения. Процесс включает генерацию синтетических данных на основе расширения траекторий и стратегию распределения кредита, управляемую совместными результатами. Эксперименты показывают значительные улучшения производительности на задачах математического и здравого рассуждения по сравнению с базовыми моделями."}, 'en': {'title': 'Unlocking Collaborative Intelligence in LLMs', 'desc': 'This paper introduces a novel approach called Multi-agent LLM training (MALT) aimed at enhancing collaboration among large language models (LLMs) for solving complex reasoning tasks. The authors propose a structured setup where different LLMs take on specialized roles—such as generator, verifier, and refinement model—to iteratively tackle problems. They implement a synthetic data generation process and a credit assignment strategy that rewards models based on their joint performance, allowing them to learn from both successful and unsuccessful attempts. The results show significant performance improvements on various reasoning benchmarks, highlighting the potential of cooperative multi-agent systems in advancing LLM capabilities.'}, 'zh': {'title': '多智能体协作训练，提升推理能力！', 'desc': '本文探讨了多智能体大语言模型（LLM）训练的潜力，旨在提高模型在复杂问题上的协作能力。我们提出了一种多智能体设置，模型分为生成器、验证器和精炼模型，协同解决推理问题。通过轨迹扩展的合成数据生成和基于联合结果的奖励策略，我们的模型能够自主提升各自的专业能力。实验结果表明，使用MALT方法的Llama 3.1 8B模型在数学和常识推理任务上取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2412.01981', 'title': 'Free Process Rewards without Process Labels', 'url': 'https://huggingface.co/papers/2412.01981', 'abstract': "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.", 'score': 18, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '13434e4f301a0d88', 'authors': ['Lifan Yuan', 'Wendi Li', 'Huayu Chen', 'Ganqu Cui', 'Ning Ding', 'Kaiyan Zhang', 'Bowen Zhou', 'Zhiyuan Liu', 'Hao Peng'], 'affiliations': ['Huazhong University of Science and Technology', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.01981.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#reasoning', '#data', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение PRM без пошаговой разметки', 'desc': 'Статья представляет новый подход к обучению процессуальных моделей вознаграждения (PRM) в машинном обучении. Авторы показывают, что неявную PRM можно получить без дополнительных затрат, просто обучая модель вознаграждения за результат (ORM) на более дешевых метках уровня ответов. Эксперименты на датасете MATH демонстрируют, что предложенный метод превосходит сильный бейзлайн на основе MCTS, используя менее 1/38 обучающих данных. Исследование также выявляет, что масштабирование инструкций и ответов улучшает производительность неявной PRM.'}, 'en': {'title': 'Unlocking Efficient Training for Process Reward Models', 'desc': 'This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples.'}, 'zh': {'title': '隐式过程奖励模型：高效训练的新思路', 'desc': '本文提出了一种新的过程奖励模型（PRM），与传统的结果奖励模型（ORM）不同，PRM能够逐步评估推理过程，提供更细致的奖励。然而，训练PRM需要在每个中间步骤都有标注，这在数据收集上面临挑战。我们展示了通过训练ORM并使用响应级别的标签，可以在没有额外成本的情况下获得隐式PRM。实验结果表明，隐式PRM在数据效率和性能上优于传统方法，尤其是在数据稀缺的情况下。'}}}, {'id': 'https://huggingface.co/papers/2412.02611', 'title': 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?', 'url': 'https://huggingface.co/papers/2412.02611', 'abstract': 'Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.', 'score': 17, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': 'f63565048b4948b4', 'authors': ['Kaixiong Gong', 'Kaituo Feng', 'Bohao Li', 'Yibing Wang', 'Mofan Cheng', 'Shijia Yang', 'Jiaming Han', 'Benyou Wang', 'Yutong Bai', 'Zhuoran Yang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Stanford University', 'UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02611.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#interpretability', '#multimodal', '#games'], 'emoji': '🎧', 'ru': {'title': 'Слышат ли ИИ-модели то, что видят?', 'desc': 'Статья представляет новый тест DeafTest и бенчмарк AV-Odyssey Bench для оценки мультимодальных больших языковых моделей (MLLM) в задачах аудио-визуального понимания. Исследователи обнаружили, что современные MLLM часто не справляются с простыми задачами определения громкости и высоты звуков. Бенчмарк включает 4,555 задач с текстом, изображениями и аудио, требующих эффективного использования обоих модальностей. Авторы провели тестирование ряда закрытых и открытых моделей, выявив ограничения текущих систем для дальнейшего улучшения сбора данных и разработки моделей.'}, 'en': {'title': 'Unveiling the Limits of Multimodal Models with AV-Odyssey Bench', 'desc': 'This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation.'}, 'zh': {'title': '揭示多模态模型的局限性', 'desc': '最近，多模态大型语言模型（MLLMs）如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其在视觉和音频方面的能力。尽管这些模型在多种音频-视觉应用中表现出色，但我们的DeafTest显示，MLLMs在一些人类认为简单的任务上常常表现不佳，例如判断两个声音哪个更响和哪个音调更高。为此，我们提出了AV-Odyssey Bench，这是一个全面的音频-视觉基准，旨在评估这些MLLMs是否真正理解音频-视觉信息。该基准包含4555个精心设计的问题，要求模型有效利用视觉和音频输入中的线索，以准确推断答案。'}}}, {'id': 'https://huggingface.co/papers/2412.02114', 'title': 'OmniCreator: Self-Supervised Unified Generation with Universal Editing', 'url': 'https://huggingface.co/papers/2412.02114', 'abstract': 'We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.', 'score': 12, 'issue_id': 939, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '62bf26709baf7f97', 'authors': ['Haodong Chen', 'Lan Wang', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKUST', 'MSU', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2412.02114.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#dataset', '#video', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Универсальный генератор и редактор медиа по текстовому запросу', 'desc': 'OmniCreator - это новая система для генерации и редактирования изображений и видео на основе текстовых запросов. Она обучается самостоятельно, используя пары текст-видео, и устанавливает семантические связи между видео и текстом. OmniCreator может как генерировать новый контент по текстовому описанию, так и редактировать существующие видео и изображения без ограничений. Авторы также представили новый набор данных OmniBench-99 для оценки моделей генеративного редактирования видео.'}, 'en': {'title': 'OmniCreator: Unified Text-Prompted Image and Video Generation and Editing', 'desc': 'OmniCreator is a new framework that allows for the generation and editing of both images and videos using text prompts. It learns to understand the relationship between text and video through self-supervised learning, using original text-video pairs as input. During use, it can either generate new videos based on text prompts or edit existing videos while maintaining fidelity to the provided text. Additionally, the framework is evaluated using a new dataset called OmniBench-99, which helps assess the performance of generative video editing models.'}, 'zh': {'title': 'OmniCreator：统一的图像与视频生成与编辑框架', 'desc': 'OmniCreator是一个新颖的框架，能够在一个平台上进行文本提示的统一生成（图像+视频）和编辑。它通过自监督学习，利用原始的文本-视频对作为条件，同时使用相同的视频作为去噪目标，学习视频与文本之间的语义对应关系。在推理阶段，OmniCreator能够根据文本提示和视频生成忠实于两者的目标，实现无约束的通用编辑效果。我们还引入了OmniBench-99数据集，以全面评估生成视频编辑模型的性能，实验结果表明OmniCreator在所有模型中表现出显著的优势。'}}}, {'id': 'https://huggingface.co/papers/2411.19655', 'title': 'Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS', 'url': 'https://huggingface.co/papers/2411.19655', 'abstract': 'After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.   Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.   To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.   Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.', 'score': 11, 'issue_id': 947, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'd1255811f49c640e', 'authors': ['Alessandro Scirè', 'Andrei Stefan Bejgu', 'Simone Tedeschi', 'Karim Ghonim', 'Federico Martelli', 'Roberto Navigli'], 'affiliations': ['Babelscape, Italy', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2411.19655.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#multimodal', '#machine_translation', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'LLM-Oasis: Новый стандарт для оценки фактологической точности языковых моделей', 'desc': 'Эта статья представляет LLM-Oasis - крупнейший ресурс для обучения оценщиков фактологической точности больших языковых моделей (LLM). LLM-Oasis создан путем извлечения утверждений из Википедии, фальсификации части этих утверждений и генерации пар фактических и нефактических текстов. Эксперименты показывают, что LLM-Oasis представляет значительную сложность для современных LLM, при этом GPT-4 достигает точности до 60% в предложенной задаче оценки фактологической точности. Ресурс имеет потенциал для стимулирования будущих исследований в этой области.'}, 'en': {'title': 'LLM-Oasis: A New Frontier for Factuality Evaluation in Language Models', 'desc': 'This paper discusses the advancements in Natural Language Generation (NLG) tasks due to Large Language Models (LLMs), while also addressing the issue of hallucinations, which are inaccuracies in generated content. It introduces LLM-Oasis, a comprehensive resource designed to train evaluators for assessing the factuality of LLM outputs. LLM-Oasis is created by extracting claims from Wikipedia, falsifying some, and generating pairs of factual and unfactual texts, validated by human annotators. The experiments show that LLM-Oasis poses a significant challenge to current LLMs, indicating its potential to enhance future research in factuality evaluation.'}, 'zh': {'title': '推动事实性评估的未来研究', 'desc': '本文介绍了大型语言模型（LLMs）在自然语言生成任务中的应用，尤其是在文本摘要和机器翻译方面的进展。然而，LLMs 仍然会产生虚假信息，即与事实不符的内容，因此评估 LLMs 的事实性变得非常重要。为了解决这一问题，本文提出了 LLM-Oasis，这是一个用于训练端到端事实性评估器的最大资源。通过从维基百科提取声明并生成真实与虚假的文本对，LLM-Oasis 为事实性评估系统的基准测试提供了重要的数据集。'}}}, {'id': 'https://huggingface.co/papers/2412.02632', 'title': 'Scaling Image Tokenizers with Grouped Spherical Quantization', 'url': 'https://huggingface.co/papers/2412.02632', 'abstract': 'Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.', 'score': 9, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '60eda94a31cded90', 'authors': ['Jiangtao Wang', 'Zhen Qin', 'Yifan Zhang', 'Vincent Tao Hu', 'Björn Ommer', 'Rania Briq', 'Stefan Kesselheim'], 'affiliations': ['CompVis @ LMU Munich', 'Jülich Supercomputing Centre', 'TapTap', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02632.jpg', 'data': {'categories': ['#training', '#inference', '#cv', '#data'], 'emoji': '🔍', 'ru': {'title': 'GSQ: Эффективная токенизация изображений на сферической поверхности', 'desc': 'В статье представлен новый метод токенизации изображений - Групповая Сферическая Квантизация (GSQ). GSQ использует сферическую инициализацию кодовой книги и регуляризацию поиска для ограничения латентного пространства кодовой книги сферической поверхностью. Авторы провели эмпирический анализ стратегий обучения токенизаторов изображений и показали, что GSQ-GAN превосходит современные методы по качеству реконструкции при меньшем количестве итераций обучения. Исследование масштабируемости GSQ выявило различное поведение при высоких и низких уровнях пространственного сжатия, подчеркивая сложности представления высокоразмерных латентных пространств.'}, 'en': {'title': 'Efficient Image Processing with Grouped Spherical Quantization', 'desc': "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."}, 'zh': {'title': '分组球面量化：高效的视觉标记器新方法', 'desc': '本文介绍了一种新的视觉标记器方法，称为分组球面量化（GSQ），旨在解决传统方法中的一些问题。GSQ通过球面代码本初始化和查找正则化，限制了代码本潜在空间在球面上的分布。我们的实证分析表明，GSQ-GAN在重建质量上优于现有的最先进方法，并且训练迭代次数更少。研究还系统地考察了GSQ在潜在维度、代码本大小和压缩比等方面的扩展行为，揭示了高维潜在空间表示的挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.02592', 'title': 'OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2412.02592', 'abstract': "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench", 'score': 8, 'issue_id': 937, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '91dbac114744b1e9', 'authors': ['Junyuan Zhang', 'Qintong Zhang', 'Bin Wang', 'Linke Ouyang', 'Zichen Wen', 'Ying Li', 'Ka-Ho Chow', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Beihang University', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of HongKong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02592.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#multimodal', '#benchmark', '#rag', '#optimization', '#survey'], 'emoji': '🔍', 'ru': {'title': 'OHRBench: раскрывая влияние OCR на системы RAG', 'desc': 'OHRBench - это первый бенчмарк для оценки влияния оптического распознавания символов (OCR) на системы генерации с извлечением информации (RAG). Он включает 350 неструктурированных PDF-документов из шести реальных областей применения RAG, а также вопросы и ответы, созданные на основе мультимодальных элементов документов. Исследователи выделяют два основных типа шума OCR: семантический и форматный, и применяют возмущения для создания структурированных данных с различной степенью каждого типа шума. Результаты показывают уязвимость систем RAG к ошибкам OCR и потенциал использования мультимодальных языковых моделей (VLM) без OCR в системах RAG.'}, 'en': {'title': "Enhancing RAG: Understanding OCR's Impact with OHRBench", 'desc': 'This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems.'}, 'zh': {'title': '揭示OCR对RAG系统的影响', 'desc': '本论文介绍了OHRBench，这是第一个用于理解光学字符识别（OCR）对检索增强生成（RAG）系统影响的基准。研究发现，OCR在处理非结构化PDF文档时会引入语义噪声和格式噪声，导致知识库质量下降。通过对350个真实世界应用领域的文档进行评估，结果显示现有的OCR解决方案无法有效构建高质量的知识库。最后，论文探讨了在RAG系统中使用视觉语言模型（VLMs）而不依赖OCR的潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.01292', 'title': 'LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences', 'url': 'https://huggingface.co/papers/2412.01292', 'abstract': "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.", 'score': 7, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'e8f8ddd05e13e9ef', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Junyan Li', 'Shuailei Ma', 'Xinyu Sun', 'Tianhang Xiang', 'Yinjie Lei', 'Mingkui Tan', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'Northeastern University', 'Pazhou Laboratory', 'Sichuan University', 'South China University of Technology', 'Tencent Robotics X', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.01292.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#3d', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Умное понимание 3D-сцен с LSceneLLM', 'desc': 'Исследование 3D Vision-Language Models (3D-VLMs) важно для развития воплощенного AI в 3D-сценах, таких как визуальная навигация и ответы на вопросы. Из-за высокой плотности визуальных признаков в больших 3D-сценах сложно точно определить важную для задачи информацию. Предлагается адаптивная структура LSceneLLM, которая автоматически определяет важные области, используя визуальные предпочтения LLM для разных задач. Эксперименты показывают, что наш метод превосходит существующие методы в понимании больших сцен и улучшает результаты при интеграции в существующие 3D-VLMs.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with LSceneLLM', 'desc': 'This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods.'}, 'zh': {'title': '自适应3D视觉语言模型，提升场景理解能力', 'desc': '3D视觉语言模型（3D-VLMs）的研究越来越受到关注，这对在3D场景中发展具身人工智能至关重要，如视觉导航和具身问答。由于3D场景中视觉特征的高密度，准确定位与任务相关的视觉信息变得具有挑战性。现有方法尝试对所有对象进行分割，并将其特征视为场景表示，但这些与任务无关的对象特征包含大量冗余信息和缺失的细节。为了解决这些问题，我们提出了LSceneLLM，一个自适应框架，通过利用大语言模型（LLM）对不同任务的视觉偏好，自动识别与任务相关的区域，并通过可插拔的场景放大模块捕捉聚焦区域的细粒度细节。'}}}, {'id': 'https://huggingface.co/papers/2412.02700', 'title': 'Motion Prompting: Controlling Video Generation with Motion Trajectories', 'url': 'https://huggingface.co/papers/2412.02700', 'abstract': 'Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/', 'score': 5, 'issue_id': 949, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': '6adffbc375f9f4f5', 'authors': ['Daniel Geng', 'Charles Herrmann', 'Junhwa Hur', 'Forrester Cole', 'Serena Zhang', 'Tobias Pfaff', 'Tatiana Lopez-Guevara', 'Carl Doersch', 'Yusuf Aytar', 'Michael Rubinstein', 'Chen Sun', 'Oliver Wang', 'Andrew Owens', 'Deqing Sun'], 'affiliations': ['Brown University', 'Google DeepMind', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2412.02700.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#games'], 'emoji': '🎥', 'ru': {'title': 'Управление движением открывает новые горизонты в генерации видео', 'desc': "Статья представляет новый подход к генерации видео с использованием управления движением. Авторы разработали модель, которая может генерировать видео на основе разреженных или плотных траекторий движения, называемых 'motion prompts'. Эта гибкая система позволяет контролировать движение камеры, объектов и общую динамику сцены. Модель демонстрирует впечатляющие результаты в различных приложениях, включая перенос движения и редактирование изображений, а также проявляет способность к реалистичной физике."}, 'en': {'title': 'Empowering Video Generation with Flexible Motion Prompts', 'desc': 'This paper presents a novel approach to video generation by using motion prompts, which are flexible representations of motion trajectories. Unlike traditional models that rely solely on text prompts, this method allows for the encoding of various types of motion, including object-specific and global scene movements. The authors introduce a technique called motion prompt expansion, enabling users to convert high-level requests into detailed motion trajectories. The results indicate that this approach not only enhances video generation but also allows for realistic interactions and behaviors within the generated content.'}, 'zh': {'title': '运动提示：视频生成的新方式', 'desc': '本论文提出了一种新的视频生成模型，利用运动轨迹作为控制手段，克服了传统文本提示在动态动作和时间组合上的局限性。我们引入了一种灵活的运动提示表示，可以编码任意数量的轨迹，包括特定物体或全局场景的运动。用户可以直接指定稀疏轨迹，我们还展示了如何将高层次的用户请求转化为详细的半稀疏运动提示。通过多种应用展示了我们方法的多样性，包括相机和物体运动控制、与图像的交互、运动转移和图像编辑。'}}}, {'id': 'https://huggingface.co/papers/2411.19542', 'title': 'A dynamic parallel method for performance optimization on hybrid CPUs', 'url': 'https://huggingface.co/papers/2411.19542', 'abstract': 'The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.', 'score': 5, 'issue_id': 936, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '27226211eddf71d4', 'authors': ['Luo Yu', 'Liu Yucheng', 'Shen Haihao'], 'affiliations': ['Intel Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2411.19542.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение инференса ИИ на гибридных CPU: балансировка для максимальной производительности', 'desc': 'Статья представляет динамический метод параллельных вычислений для гибридных процессоров, оптимизирующий инференс больших языковых моделей (LLM). Авторы обнаружили, что существующие фреймворки для инференса ИИ не учитывают неравномерные возможности ядер в гибридных CPU, что приводит к низкой производительности. Предложенный метод балансирует нагрузку между ядрами перед началом параллельной работы, что значительно повышает эффективность инференса LLM. В результате, инструмент Neural Speed достиг более 90% использования пропускной способности памяти на двух гибридных процессорах Intel.'}, 'en': {'title': 'Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs', 'desc': 'The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs.'}, 'zh': {'title': '动态平衡，提升AI推理性能！', 'desc': 'AIPC概念越来越受欢迎，越来越多的混合CPU将在客户端设备上运行AI模型。然而，目前的AI推理框架忽视了混合CPU的不平衡硬件能力，导致推理性能低下。为了解决这个问题，我们提出了一种动态并行方法，显著提高了混合CPU的LLM推理性能，通过在并行工作开始之前平衡每个核心的工作负载。该方法使Neural Speed在两款混合Intel CPU上实现了超过90%的内存带宽。'}}}, {'id': 'https://huggingface.co/papers/2411.19067', 'title': 'MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation', 'url': 'https://huggingface.co/papers/2411.19067', 'abstract': "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.", 'score': 5, 'issue_id': 934, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '74d4a17af3574a5d', 'authors': ['Minhyun Lee', 'Seungho Lee', 'Song Park', 'Dongyoon Han', 'Byeongho Heo', 'Hyunjung Shim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19067.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#training'], 'emoji': '🎭', 'ru': {'title': 'Маскирование для улучшения сегментации изображений по описанию', 'desc': 'Статья посвящена задаче сегментации изображений по текстовому описанию (RIS). Авторы предлагают новый метод обучения под названием MaskRIS, который использует маскирование изображений и текста. Этот подход улучшает устойчивость модели к окклюзиям, неполной информации и языковым сложностям. Эксперименты показывают, что MaskRIS превосходит существующие методы как при полностью контролируемом, так и при слабо контролируемом обучении.'}, 'en': {'title': 'Enhancing Referring Image Segmentation with Masking Techniques', 'desc': 'Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets.'}, 'zh': {'title': 'Masked Referring Image Segmentation：提升图像分割性能的新方法', 'desc': '引用图像分割（RIS）是一种先进的视觉-语言任务，旨在根据自由形式的文本描述识别和分割图像中的对象。本文探讨了有效的数据增强技术，并提出了一种新的训练框架，称为Masked Referring Image Segmentation（MaskRIS）。研究表明，传统的图像增强方法在RIS中效果不佳，而简单的随机遮罩显著提升了RIS的性能。MaskRIS结合了图像和文本遮罩，并采用了失真感知上下文学习（DCL），从而提高了模型对遮挡、不完整信息和语言复杂性的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2412.00239', 'title': 'Generating a Low-code Complete Workflow via Task Decomposition and RAG', 'url': 'https://huggingface.co/papers/2412.00239', 'abstract': 'AI technologies are moving rapidly from research to production. With the popularity of Foundation Models (FMs) that generate text, images, and video, AI-based systems are increasing their complexity. Compared to traditional AI-based software, systems employing FMs, or GenAI-based systems, are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across GenAI applications. Our first contribution is to formalize two techniques, Task Decomposition and Retrieval-Augmented Generation (RAG), as design patterns for GenAI-based systems. We discuss their trade-offs in terms of software quality attributes and comment on alternative approaches. We recommend to AI practitioners to consider these techniques not only from a scientific perspective but also from the standpoint of desired engineering properties such as flexibility, maintainability, safety, and security. As a second contribution, we describe our industry experience applying Task Decomposition and RAG to build a complex real-world GenAI application for enterprise users: Workflow Generation. The task of generating workflows entails generating a specific plan using data from the system environment, taking as input a user requirement. As these two patterns affect the entire AI development cycle, we explain how they impacted the dataset creation, model training, model evaluation, and deployment phases.', 'score': 2, 'issue_id': 948, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '44bf29d0fbeafbc3', 'authors': ['Orlando Marquez Ayala', 'Patrice Béchard'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2412.00239.jpg', 'data': {'categories': ['#optimization', '#training', '#rag', '#dataset', '#security', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Паттерны проектирования для систем генеративного ИИ: от теории к практике', 'desc': 'Статья посвящена формализации двух техник - декомпозиции задач и генерации с дополнением извлечением информации (RAG) - как паттернов проектирования для систем на основе генеративного ИИ. Авторы обсуждают компромиссы этих паттернов с точки зрения атрибутов качества программного обеспечения и рекомендуют рассматривать их не только с научной точки зрения, но и с позиции желаемых инженерных свойств. В статье также описывается опыт применения этих паттернов для создания сложного приложения генеративного ИИ для корпоративных пользователей - генерации рабочих процессов. Авторы объясняют, как эти паттерны повлияли на весь цикл разработки ИИ, включая создание датасетов, обучение и оценку моделей, а также развертывание.'}, 'en': {'title': 'Streamlining GenAI Development with Design Patterns', 'desc': 'This paper discusses the challenges of designing AI systems that use Foundation Models (FMs) due to their complexity and versatility. It introduces two design patterns, Task Decomposition and Retrieval-Augmented Generation (RAG), which can help streamline the development of GenAI applications. The authors analyze the trade-offs of these techniques in relation to software quality attributes like flexibility and security. Additionally, they share their practical experience in applying these patterns to create a Workflow Generation application for enterprise users, highlighting their influence on various stages of the AI development cycle.'}, 'zh': {'title': '设计模式助力生成AI系统的开发', 'desc': '随着基础模型（FMs）在文本、图像和视频生成中的普及，AI系统的复杂性不断增加。与传统的AI软件相比，基于生成AI（GenAI）的系统在设计上更具挑战性，因此需要记录最佳实践，称为设计模式。本文首次将任务分解和检索增强生成（RAG）形式化为GenAI系统的设计模式，并讨论它们在软件质量属性方面的权衡。我们还分享了在企业用户的复杂GenAI应用——工作流生成中的实际经验，说明这些模式如何影响整个AI开发周期。'}}}, {'id': 'https://huggingface.co/papers/2412.01558', 'title': 'VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval', 'url': 'https://huggingface.co/papers/2412.01558', 'abstract': 'Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .', 'score': 2, 'issue_id': 935, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '12235c4ebf26fe4a', 'authors': ['Dhiman Paul', 'Md Rizwan Parvez', 'Nabeel Mohammed', 'Shafin Rahman'], 'affiliations': ['Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh', 'Qatar Computing Research Institute (QCRI), Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2412.01558.jpg', 'data': {'categories': ['#video', '#games', '#synthetic', '#architecture', '#benchmark', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'VideoLights: Новый подход к анализу ключевых моментов видео с помощью продвинутых нейросетевых архитектур', 'desc': 'Статья представляет VideoLights - новую систему для обнаружения ключевых моментов видео и поиска по ним. Авторы предлагают улучшенные методы выравнивания видео и текста, двунаправленное слияние модальностей и механизм обратной связи между задачами. Они также вводят адаптивные функции потерь и используют большие мультимодальные модели для улучшения представления данных. Эксперименты показывают, что VideoLights превосходит существующие методы на нескольких наборах данных.'}, 'en': {'title': 'Enhancing Video-Text Integration with VideoLights', 'desc': 'This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis.'}, 'zh': {'title': 'VideoLights：提升视频与文本分析的全新框架', 'desc': '本论文提出了一种名为VideoLights的视频高亮检测和时刻检索框架，旨在解决现有模型在视频与文本对齐和跨任务动态方面的不足。我们引入了卷积投影和特征精炼模块，以提高视频和文本特征的对齐效果，并采用双向跨模态融合网络来增强查询感知的片段表示。通过单向联合任务反馈机制，我们能够提升两个任务之间的相关性，同时引入硬正负损失以改善学习效果。实验结果表明，VideoLights在多个基准数据集上表现出色，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.18478', 'title': 'Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS', 'url': 'https://huggingface.co/papers/2411.18478', 'abstract': "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%).", 'score': 21, 'issue_id': 890, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '05890d0739faa85c', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Department of Automation, Tsinghua University', 'Beijing National Research Center for Information Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.18478.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'HiAR-ICL: Абстрактное мышление для языковых моделей', 'desc': "HiAR-ICL - это новый подход к обучению больших языковых моделей, который фокусируется на абстрактных паттернах мышления вместо конкретных примеров. Метод использует пять базовых действий рассуждения для построения цепочек мышления. Применяя поиск Монте-Карло, HiAR-ICL создает 'карточки мыслей' для руководства выводами. Эксперименты показали, что HiAR-ICL достигает лучших результатов на бенчмарке MATH, превосходя GPT-4 и Claude 3.5."}, 'en': {'title': 'Revolutionizing Mathematical Reasoning with HiAR-ICL', 'desc': "This paper introduces HiAR-ICL, a new approach to In-context Learning (ICL) that enhances large language models' ability to perform complex mathematical reasoning tasks. Traditional ICL relies heavily on the quality of examples and often requires human input, which can be limiting. HiAR-ICL shifts the focus from specific examples to abstract reasoning patterns, utilizing five atomic reasoning actions to create structured reasoning chains. The method employs Monte Carlo Tree Search to explore reasoning paths and develop a cognitive complexity framework that matches problems with suitable thought cards, achieving superior performance on the MATH benchmark."}, 'zh': {'title': '高层次自动推理：超越传统上下文学习的局限性', 'desc': '本文提出了一种新的高层次自动推理范式HiAR-ICL，旨在解决传统上下文学习在复杂数学推理任务中的局限性。HiAR-ICL通过引入五种基本推理动作，转变了对具体示例的依赖，强调抽象思维模式的重要性。该方法利用蒙特卡洛树搜索探索推理路径，并构建思维卡片以指导后续推理。实验结果表明，HiAR-ICL在MATH基准测试中取得了79.6%的准确率，超越了其他先进模型。'}}}, {'id': 'https://huggingface.co/papers/2411.19930', 'title': 'On Domain-Specific Post-Training for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.19930', 'abstract': 'Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.', 'score': 18, 'issue_id': 885, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '5d14749b38f15e60', 'authors': ['Daixuan Cheng', 'Shaohan Huang', 'Ziyu Zhu', 'Xintong Zhang', 'Wayne Xin Zhao', 'Zhongzhi Luan', 'Bo Dai', 'Zhenliang Zhang'], 'affiliations': ['Beihang University', 'Beijing Institute of Technology', 'Renmin University of China', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19930.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#open_source', '#multimodal', '#synthetic'], 'emoji': '🔬', 'ru': {'title': 'Адаптация мультимодальных ИИ к специализированным областям', 'desc': 'Статья исследует адаптацию мультимодальных больших языковых моделей (MLLM) к специфическим доменам. Авторы разработали синтезатор визуальных инструкций, использующий доменные пары изображение-подпись для генерации разнообразных задач. Они применили одноэтапный процесс обучения для улучшения разнообразия задач при доменной постобработке. Эксперименты проводились в биомедицинской и пищевой областях с использованием различных MLLM, демонстрируя эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques', 'desc': 'This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area.'}, 'zh': {'title': '提升多模态模型的领域适应性', 'desc': '近年来，通用多模态大型语言模型（MLLMs）迅速发展。然而，将通用MLLMs适应于特定领域，如科学和工业应用，仍然较少被探索。本文系统研究了MLLMs的领域适应性，重点在于数据合成、训练流程和任务评估。我们开发了一种视觉指令合成器，能够有效生成多样化的视觉指令任务，从而提升MLLMs在特定领域的表现。'}}}, {'id': 'https://huggingface.co/papers/2411.19189', 'title': 'Video Depth without Video Models', 'url': 'https://huggingface.co/papers/2411.19189', 'abstract': 'Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.', 'score': 16, 'issue_id': 889, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '1fc611a9a44595a1', 'authors': ['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.19189.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'RollingDepth: Революция в оценке глубины видео с помощью LDM', 'desc': 'Данная статья представляет новый подход к оценке глубины видео под названием RollingDepth. Модель основана на латентной диффузионной модели (LDM) для одиночных изображений и включает два ключевых компонента: оценщик глубины для коротких видеофрагментов и алгоритм регистрации для сборки фрагментов в целостное видео. RollingDepth эффективно обрабатывает длинные видео и превосходит по точности как специализированные оценщики глубины видео, так и высокопроизводительные модели для отдельных кадров. Этот метод решает проблемы временной непрерывности и изменений диапазона глубины при движении камеры.'}, 'en': {'title': 'Transforming Monocular Videos into Accurate 3D Depth with RollingDepth', 'desc': 'This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences.'}, 'zh': {'title': '将单图像深度估计提升为视频深度估计的创新之路', 'desc': '视频深度估计通过推断每帧的密集深度，将单目视频片段提升为3D。最近，单图像深度估计的进展激发了对视频深度的关注，但简单地将单图像深度估计器应用于每帧会忽略时间连续性，导致闪烁和深度范围的突然变化。我们提出了一种名为RollingDepth的模型，它结合了多帧深度估计和优化的注册算法，能够有效处理长视频并提供更准确的深度视频。该模型在性能上超越了专用视频深度估计器和高性能单帧模型。'}}}, {'id': 'https://huggingface.co/papers/2411.19146', 'title': 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs', 'url': 'https://huggingface.co/papers/2411.19146', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.", 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': 'b33bb17742a81e99', 'authors': ['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Itay Levy', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.19146.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': '🧩', 'ru': {'title': 'Ускорение LLM без потери качества: революция в эффективности ИИ', 'desc': 'Представлена система Puzzle для ускорения инференса больших языковых моделей (LLM) на конкретном оборудовании при сохранении их возможностей. Используя нейроархитектурный поиск (NAS) и блочную локальную дистилляцию знаний (BLD), Puzzle оптимизирует модели с десятками миллиардов параметров под аппаратные ограничения. На примере модели Nemotron-51B, полученной из Llama-3.1-70B-Instruct, демонстрируется 2.17-кратное ускорение инференса при сохранении 98.4% возможностей исходной модели. Это показывает, что производительность инференса, а не только количество параметров, должна определять выбор модели.'}, 'en': {'title': 'Optimizing Large Language Models for Efficient Inference', 'desc': "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."}, 'zh': {'title': '高效推理，强大模型的新范式', 'desc': '大型语言模型（LLMs）在推理方面表现出色，但高计算成本限制了它们的应用。我们提出了Puzzle框架，通过神经架构搜索（NAS）在特定硬件上加速LLM推理，同时保持其能力。该框架利用块状局部知识蒸馏（BLD）进行并行架构探索，并采用混合整数规划进行精确约束优化。通过Nemotron-51B模型，我们展示了在单个NVIDIA H100 GPU上实现2.17倍推理吞吐量提升的实际效果，同时保留了98.4%的原始模型能力。'}}}, {'id': 'https://huggingface.co/papers/2411.19108', 'title': "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model", 'url': 'https://huggingface.co/papers/2411.19108', 'abstract': 'As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.', 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '02a6c2edf156e9d3', 'authors': ['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.19108.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': '⏱️', 'ru': {'title': 'Умное кэширование для быстрой генерации видео', 'desc': 'Статья представляет новый подход к ускорению диффузионных моделей для генерации видео под названием TeaCache. Метод оценивает различия между выходными данными модели на разных временных шагах, используя входные данные и встраивания временных шагов. TeaCache применяет стратегию масштабирования для уточнения оценок различий и использует их для кэширования выходных данных. Эксперименты показывают, что TeaCache достигает ускорения до 4,41 раза по сравнению с Open-Sora-Plan при незначительном снижении качества визуализации.'}, 'en': {'title': 'Accelerating Video Generation with Smart Caching', 'desc': 'This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods.'}, 'zh': {'title': '提升视频生成速度的新方法：TeaCache', 'desc': '本研究提出了一种新的缓存方法，称为时间步嵌入感知缓存（TeaCache），旨在提高视频生成中的扩散模型的推理速度。传统方法通过在均匀选择的时间步缓存模型输出，但忽略了不同时间步之间输出差异的不均匀性。TeaCache通过调节噪声输入，利用时间步嵌入来更好地近似模型输出的差异，从而优化缓存选择。实验结果表明，TeaCache在保持视觉质量的同时，推理速度提高了4.41倍。'}}}, {'id': 'https://huggingface.co/papers/2411.19324', 'title': 'Trajectory Attention for Fine-grained Video Motion Control', 'url': 'https://huggingface.co/papers/2411.19324', 'abstract': 'Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.', 'score': 9, 'issue_id': 885, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '02a266f597ae69e7', 'authors': ['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Sensetime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19324.jpg', 'data': {'categories': ['#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль движения камеры в генеративных видеомоделях с помощью trajectory attention', 'desc': "Статья представляет новый подход под названием 'trajectory attention' для точного контроля движения камеры в генеративных видеомоделях. Метод выполняет внимание вдоль доступных траекторий пикселей, что позволяет более точно внедрять информацию о траектории в процесс генерации видео. Trajectory attention работает как вспомогательная ветвь наряду с традиционным временным вниманием, обеспечивая как точный контроль движения, так и возможность генерации нового контента. Эксперименты показывают значительные улучшения в точности и согласованности на больших расстояниях при сохранении высокого качества генерации."}, 'en': {'title': 'Enhancing Video Generation with Trajectory Attention', 'desc': 'This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks.'}, 'zh': {'title': '轨迹注意力：精确控制视频生成中的相机运动', 'desc': '本论文介绍了一种新颖的轨迹注意力机制，用于视频生成中的相机运动控制。与现有方法相比，我们的方法能够更精确地处理运动控制，并有效地结合了轨迹信息。通过将轨迹注意力作为辅助分支与传统时间注意力结合，我们的方法在生成新内容的同时，确保了运动控制的精确性。实验结果表明，该方法在图像和视频的相机运动控制中显著提高了精度和长距离一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.18552', 'title': 'FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion', 'url': 'https://huggingface.co/papers/2411.18552', 'abstract': 'Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads.', 'score': 8, 'issue_id': 892, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'dd1bf99b66f1b34d', 'authors': ['Haosen Yang', 'Adrian Bulat', 'Isma Hadji', 'Hai X. Pham', 'Xiatian Zhu', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University, UK', 'Samsung AI Center, Cambridge, UK', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2411.18552.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Гибкое масштабирование диффузионных моделей без переобучения', 'desc': 'Эта статья представляет новый метод под названием Fam diffusion для улучшения работы диффузионных моделей при изменении разрешения изображений. Авторы предлагают два модуля: Frequency Modulation (FM) для улучшения глобальной структурной согласованности и Attention Modulation (AM) для повышения согласованности локальных текстурных паттернов. Метод легко интегрируется в любую модель латентной диффузии и не требует дополнительного обучения. Результаты показывают эффективность метода в устранении структурных и локальных артефактов, демонстрируя при этом лучшую производительность по сравнению с существующими подходами.'}, 'en': {'title': 'Flexible Image Generation with Fam Diffusion', 'desc': 'This paper presents a novel approach to enhance diffusion models for image generation, allowing them to work effectively at various resolutions without retraining. The proposed Fam diffusion method introduces two key modules: Frequency Modulation (FM) for improving global structure consistency and Attention Modulation (AM) for refining local texture patterns. These modules address common issues like repetitive patterns and structural distortions that occur when using scaled resolutions. The method integrates seamlessly into existing latent diffusion models, demonstrating state-of-the-art performance with minimal latency overheads and improved image quality.'}, 'zh': {'title': '灵活分辨率下的高质量图像生成', 'desc': '扩散模型在生成高质量图像方面表现出色，但仅在训练时使用的分辨率下有效。在不同的分辨率下推理会导致重复模式和结构失真。我们提出了两个简单的模块，频率调制（FM）模块和注意力调制（AM）模块，来解决这些问题。我们的Fam扩散方法可以无缝集成到任何潜在扩散模型中，无需额外训练，并且在处理结构和局部伪影方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2411.19527', 'title': 'DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding', 'url': 'https://huggingface.co/papers/2411.19527', 'abstract': 'Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this "discord" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: https://whwjdqls.github.io/discord.github.io/.', 'score': 8, 'issue_id': 891, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'b1fc0d8f7ba13620', 'authors': ['Jungbin Cho', 'Junwan Kim', 'Jisoo Kim', 'Minseo Kim', 'Mingu Kang', 'Sungeun Hong', 'Tae-Hyun Oh', 'Youngjae Yu'], 'affiliations': ['POSTECH', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19527.jpg', 'data': {'categories': ['#video', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'DisCoRD: Мост между дискретной эффективностью и непрерывным реализмом в генерации движений', 'desc': 'Статья представляет новый метод DisCoRD для генерации движений человека. Метод объединяет преимущества дискретных и непрерывных подходов, используя дискретные токены и непрерывное декодирование через выпрямленный поток. DisCoRD применяет итеративное уточнение в непрерывном пространстве для захвата тонких динамических характеристик движения. Результаты показывают, что метод достигает наилучших показателей по метрике FID на датасетах HumanML3D и KIT-ML.'}, 'en': {'title': 'Bridging the Gap: DisCoRD for Smooth Human Motion Generation', 'desc': 'This paper addresses the challenges of generating human motion using machine learning models, particularly the limitations of discrete quantization methods like VQ-VAEs. It introduces a new method called DisCoRD, which stands for Discrete Tokens to Continuous Motion via Rectified Flow Decoding. DisCoRD effectively converts discrete motion tokens into smooth continuous motion by using an iterative refinement process in the continuous space. The results show that DisCoRD outperforms existing methods, achieving state-of-the-art performance metrics on benchmark datasets, thus providing a solution that balances discrete efficiency with continuous realism.'}, 'zh': {'title': '打破离散与连续的界限，提升运动生成自然性', 'desc': '人类运动是连续和动态的，这给生成模型带来了很大挑战。尽管离散量化方法（如VQ-VAEs）占主导地位，但它们在表达能力和帧噪声方面存在局限。我们提出了一种新方法DisCoRD，通过修正流解码将离散运动标记解码为连续运动，解决了离散和连续表示之间的矛盾。DisCoRD在连续空间中进行迭代优化，捕捉细微动态，确保运动更加平滑自然，且与任何基于离散的框架兼容。'}}}, {'id': 'https://huggingface.co/papers/2411.19950', 'title': 'AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos', 'url': 'https://huggingface.co/papers/2411.19950', 'abstract': 'We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets', 'score': 5, 'issue_id': 888, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '9f7d2daec9cb311d', 'authors': ['Yuze He', 'Wang Zhao', 'Shaohui Liu', 'Yubin Hu', 'Yushi Bai', 'Yu-Hui Wen', 'Yong-Jin Liu'], 'affiliations': ['Beijing Jiaotong University', 'ETH Zurich', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19950.jpg', 'data': {'categories': ['#3d'], 'emoji': '📐', 'ru': {'title': 'AlphaTablets: революция в представлении 3D плоскостей', 'desc': 'В статье представлен AlphaTablets - новый подход к представлению трехмерных плоскостей в виде прямоугольников с альфа-каналами. Это позволяет сочетать преимущества существующих 2D и 3D представлений, обеспечивая точное и гибкое моделирование 3D плоскостей. Авторы разработали дифференцируемую растеризацию для эффективной визуализации 3D плоскостей и предложили новый алгоритм 3D реконструкции плоскостей из монокулярных видео. Эксперименты на наборе данных ScanNet показали превосходные результаты в задаче 3D реконструкции плоскостей.'}, 'en': {'title': 'AlphaTablets: Revolutionizing 3D Plane Representation', 'desc': 'AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset.'}, 'zh': {'title': 'AlphaTablets：3D平面重建的新方法', 'desc': '我们介绍了AlphaTablets，这是一种新颖且通用的3D平面表示方法，具有连续的3D表面和精确的边界划分。通过将3D平面表示为带有alpha通道的矩形，AlphaTablets结合了当前2D和3D平面表示的优点，实现了3D平面的准确、一致和灵活建模。我们在AlphaTablets的基础上推导出可微分光栅化技术，以高效地将3D平面渲染为图像，并提出了一种新颖的自下而上的单目视频3D平面重建管道。通过迭代优化和合并，我们能够重建出完整且准确的3D平面，具有坚实的表面和清晰的边界。'}}}, {'id': 'https://huggingface.co/papers/2411.19460', 'title': 'Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing', 'url': 'https://huggingface.co/papers/2411.19460', 'abstract': 'With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.', 'score': 5, 'issue_id': 886, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': 'b96751a3db484750', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.19460.jpg', 'data': {'categories': ['#architecture', '#long_context', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Эффективная обработка длинных видео с помощью Video-Ma^2mba', 'desc': 'Статья представляет Video-Ma^2mba - новую архитектуру для обработки длинных видеопоследовательностей, использующую модели пространства состояний вместо механизмов внимания. Это позволяет линейно масштабировать большие мультимодальные модели (LMM) по времени и памяти. Авторы также вводят метод мультиосевого градиентного чекпойнтинга (MA-GC) для повышения эффективности использования памяти. Эмпирические исследования показывают, что Video-Ma^2mba может обрабатывать длинные видеопоследовательности на одном GPU, улучшая точность и релевантность ответов в задачах понимания длинных видео.'}, 'en': {'title': 'Revolutionizing Long Video Processing with Linear Scalability', 'desc': 'The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models.'}, 'zh': {'title': '高效处理长视频序列的新方法', 'desc': '随着视频数据规模和复杂性的增加，处理长视频序列面临着显著的挑战。我们提出了一种新架构Video-Ma^2mba，它在Mamba-2框架中引入了状态空间模型（SSMs），替代了传统的注意力机制，从而使得大规模多模态模型（LMMs）在时间和内存需求上实现线性扩展。我们还引入了多轴梯度检查点（MA-GC）方法，优化内存管理，仅保留必要的激活信息，显著降低了内存占用。实验结果表明，Video-Ma^2mba能够在单个GPU上处理相当于数百万个标记或超过两小时的连续视频序列，提升了长视频理解任务的准确性和相关性。'}}}, {'id': 'https://huggingface.co/papers/2411.19865', 'title': 'Reverse Thinking Makes LLMs Stronger Reasoners', 'url': 'https://huggingface.co/papers/2411.19865', 'abstract': "Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.", 'score': 4, 'issue_id': 899, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '8f066f57ddff0ae8', 'authors': ['Justin Chih-Yao Chen', 'Zifeng Wang', 'Hamid Palangi', 'Rujun Han', 'Sayna Ebrahimi', 'Long Le', 'Vincent Perot', 'Swaroop Mishra', 'Mohit Bansal', 'Chen-Yu Lee', 'Tomas Pfister'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.19865.jpg', 'data': {'categories': ['#data', '#training', '#transfer_learning', '#small_models', '#dataset', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'RevThink: Усиление LLM обратным мышлением', 'desc': 'Статья представляет новый метод под названием Reverse-Enhanced Thinking (RevThink) для улучшения рассуждений больших языковых моделей (LLM). RevThink использует обратное мышление, дополняя набор данных структурированными прямыми и обратными рассуждениями от модели-учителя. Метод включает три задачи обучения: генерация прямых рассуждений, обратных вопросов и обратных рассуждений. Эксперименты показали значительное улучшение производительности на различных наборах данных по сравнению с базовыми методами.'}, 'en': {'title': 'Empowering LLMs with Reverse Reasoning for Enhanced Performance', 'desc': 'This paper introduces Reverse-Enhanced Thinking (RevThink), a framework designed to improve Large Language Models (LLMs) by enabling them to perform reverse reasoning. RevThink enhances reasoning performance by augmenting datasets with structured forward and backward reasoning examples, allowing models to learn from both directions. The framework employs multi-task learning objectives to train a smaller student model, focusing on generating forward reasoning, backward questions, and backward reasoning. Experimental results show significant improvements in reasoning tasks, demonstrating the effectiveness and efficiency of RevThink in enhancing model performance with limited data.'}, 'zh': {'title': '反向思维：提升推理能力的新方法', 'desc': '反向思维在人的推理中起着重要作用。本文提出了一种名为反向增强思维（RevThink）的框架，旨在使大型语言模型（LLMs）能够进行反向推理。该框架通过数据增强和学习目标来实现，收集结构化的前向和后向推理数据。实验结果表明，RevThink在多个数据集上显著提高了模型的推理性能，并展示了良好的样本效率和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.19638', 'title': 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification', 'url': 'https://huggingface.co/papers/2411.19638', 'abstract': "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.", 'score': 4, 'issue_id': 889, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '98bf5f113194343b', 'authors': ['Taja Kuzman', 'Nikola Ljubešić'], 'affiliations': ['Department of Knowledge Technologies, Jožef Stefan Institute, 1000 Ljubljana, Slovenia', 'Jožef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia', 'University of Ljubljana, 1000 Ljubljana, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2411.19638.jpg', 'data': {'categories': ['#machine_translation', '#training', '#low_resource', '#multilingual', '#dataset'], 'emoji': '📰', 'ru': {'title': 'Эффективная многоязычная классификация новостей без ручной разметки', 'desc': "Статья представляет новый подход к многоязычной классификации новостей с использованием модели 'учитель-ученик'. Большая языковая модель GPT выступает в роли учителя, автоматически аннотируя новостные статьи на четырех языках. Модели-ученики на основе BERT обучаются на этих данных, достигая высокой производительности при меньших вычислительных затратах. Исследование показывает эффективность метода для многоязычной и кросс-языковой классификации новостей."}, 'en': {'title': 'Empowering Multilingual News Classification with Teacher-Student LLMs', 'desc': 'This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages.'}, 'zh': {'title': '多语言新闻分类的新方法', 'desc': '随着在线新闻数量的不断增加，按主题对新闻进行分类变得至关重要。本文提出了一种基于大型语言模型的教师-学生框架，用于开发多语言新闻分类模型，且无需手动数据标注。教师模型使用生成预训练变换器（GPT）自动标注新闻文章，展示出在多种语言上的高零样本性能。通过微调较小的BERT类学生模型，这些模型在相对较少的训练实例下也能达到与教师模型相当的高性能。'}}}, {'id': 'https://huggingface.co/papers/2411.18673', 'title': 'AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.18673', 'abstract': 'Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.', 'score': 4, 'issue_id': 886, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '1ea35d3552a278a3', 'authors': ['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov'], 'affiliations': ['SFU', 'Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2411.18673.jpg', 'data': {'categories': ['#dataset', '#architecture', '#diffusion', '#games', '#3d', '#training', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Прецизионное управление 3D-камерой в генеративных видеомоделях', 'desc': 'Эта статья представляет новый подход к управлению 3D-камерой в генеративных видеомоделях. Авторы анализируют движение камеры с фундаментальной точки зрения и предлагают несколько улучшений, включая оптимизацию графиков обучения и тестирования, ограничение внедрения условий камеры в определенные слои архитектуры и использование специально подобранного набора данных для обучения. Результатом является архитектура Advanced 3D Camera Control (AC3D), которая обеспечивает более точное управление камерой без ущерба для качества синтеза видео. Модель AC3D достигает нового уровня генеративного видеомоделирования с управлением камерой.'}, 'en': {'title': 'Precision in 3D Camera Control for Enhanced Video Generation', 'desc': 'This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling.'}, 'zh': {'title': '先进的3D相机控制，提升视频生成质量', 'desc': '本研究分析了3D相机控制在文本到视频模型中的应用，发现相机运动对视频生成质量有显著影响。我们提出了一种新的训练和测试姿态调节策略，以提高训练收敛速度和视频的视觉质量。通过对无条件视频扩散变换器的表示进行探测，我们发现相机姿态估计在模型内部隐式执行，因此我们限制了相机条件的注入，以减少对其他视频特征的干扰。最终，我们设计了先进的3D相机控制架构（AC3D），成为具有相机控制的生成视频建模的新一代模型。'}}}, {'id': 'https://huggingface.co/papers/2411.19842', 'title': 'Scaling Transformers for Low-Bitrate High-Quality Speech Coding', 'url': 'https://huggingface.co/papers/2411.19842', 'abstract': 'The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of 400 or 700 bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.', 'score': 3, 'issue_id': 900, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '23e49aedef71b878', 'authors': ['Julian D Parker', 'Anton Smirnov', 'Jordi Pons', 'CJ Carr', 'Zack Zukowski', 'Zach Evans', 'Xubo Liu'], 'affiliations': ['Stability AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.19842.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#audio', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Трансформеры покоряют токенизацию речи', 'desc': 'Данная статья описывает новый подход к токенизации речи с использованием нейронных аудиокодеков. Авторы представляют модель на основе трансформера с большим количеством параметров и применением гибкого квантования FSQ. Эта архитектура позволяет достичь высокого качества речи при крайне низких битрейтах в 400-700 бит/с. Результаты значительно превосходят существующие базовые модели как в объективных, так и в субъективных тестах.'}, 'en': {'title': 'Transforming Speech Quality with Scalable Neural Models', 'desc': 'This paper discusses the importance of tokenizing speech using neural audio codec models in AI systems. It highlights a new approach that utilizes a large-scale transformer architecture combined with Finite Scalar Quantization (FSQ) to improve speech quality. The proposed method achieves impressive results, delivering high-quality speech at very low bit-rates of 400 or 700 bits-per-second. The models developed in this study significantly outperform existing methods in both objective measures and subjective evaluations.'}, 'zh': {'title': '通过扩展变换器实现高质量低比特率语音标记化', 'desc': '本论文探讨了使用神经音频编解码模型对语音进行标记化的重要性。传统的标记化模型通常采用低参数量的架构，依赖于强的归纳偏置。我们展示了通过扩展变换器架构并应用灵活的有限标量量化（FSQ）瓶颈，可以在极低的比特率下实现最先进的语音质量。训练后的模型在客观和主观测试中均显著超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2411.18664', 'title': 'Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling', 'url': 'https://huggingface.co/papers/2411.18664', 'abstract': 'Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.', 'score': 2, 'issue_id': 900, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '3576f88bbf3e4567', 'authors': ['Junha Hyung', 'Kinam Kim', 'Susung Hong', 'Min-Jung Kim', 'Jaegul Choo'], 'affiliations': ['KAIST AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.18664.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#3d', '#training', '#video'], 'emoji': '🎬', 'ru': {'title': 'STG: Улучшение качества видео без потери разнообразия', 'desc': 'Статья представляет новый метод улучшения качества генерации видео с помощью диффузионных моделей - Spatiotemporal Skip Guidance (STG). STG не требует дополнительного обучения и использует самовозмущение для создания слабой модели. Метод избирательно пропускает пространственно-временные слои, что позволяет улучшить качество сэмплов без ущерба для разнообразия и динамики. STG превосходит существующие методы, такие как CFG и автогайденс, не требуя при этом дополнительных моделей или обучения.'}, 'en': {'title': 'Enhancing Video Diffusion with Spatiotemporal Skip Guidance', 'desc': 'This paper presents Spatiotemporal Skip Guidance (STG), a novel method for improving video diffusion models without requiring additional training or external models. STG enhances the quality of generated videos by using a self-perturbation technique that simulates a weak model through selective skipping of spatiotemporal layers. This approach allows for better sample quality while maintaining diversity and dynamic motion, addressing the limitations of existing methods like CFG. The authors demonstrate that STG is an efficient and effective guidance technique that enhances the performance of transformer-based video diffusion models.'}, 'zh': {'title': '时空跳跃引导：提升视频扩散模型的高效方法', 'desc': '扩散模型已成为生成高质量图像、视频和3D内容的强大工具。虽然采样引导技术如CFG可以提高质量，但会降低多样性和动态性。自引导方法虽然可以缓解这些问题，但需要额外的弱模型训练，限制了其在大规模模型中的实用性。我们提出了时空跳跃引导（STG），这是一种简单的无训练采样引导方法，旨在增强基于变换器的视频扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2411.18092', 'title': 'Training Noise Token Pruning', 'url': 'https://huggingface.co/papers/2411.18092', 'abstract': "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to Rate-Distortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNT's advantages over previous pruning methods.", 'score': 1, 'issue_id': 905, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '570d01745d1c7f3d', 'authors': ['Mingxing Rao', 'Bohan Jiang', 'Daniel Moyer'], 'affiliations': ['Vanderbilt University, Nashville, TN 37235, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.18092.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#cv', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Плавная обрезка трансформеров с помощью шума', 'desc': 'В этой работе представлен метод обрезки трансформеров для компьютерного зрения под названием Training Noise Token (TNT). Метод заменяет дискретное отбрасывание токенов на добавление непрерывного шума, что обеспечивает плавную оптимизацию при обучении, сохраняя при этом преимущества дискретного отбрасывания при развертывании. Авторы приводят теоретические связи с литературой по скорости-искажению и эмпирические оценки на наборе данных ImageNet с использованием архитектур ViT и DeiT. Результаты демонстрируют преимущества TNT перед предыдущими методами обрезки.'}, 'en': {'title': 'Smooth Optimization with TNT Pruning for Vision Transformers', 'desc': 'This paper introduces a novel approach called Training Noise Token (TNT) Pruning for vision transformers, which enhances the training process by allowing continuous additive noise instead of strictly dropping tokens. This method enables smoother optimization during training while still benefiting from the computational efficiency of discrete token dropping during deployment. The authors establish theoretical links to Rate-Distortion theory, which helps to understand the trade-offs involved in token pruning. Empirical results on the ImageNet dataset show that TNT Pruning outperforms existing pruning techniques when applied to ViT and DeiT architectures.'}, 'zh': {'title': '训练噪声标记剪枝：优化与效率的结合', 'desc': '本文提出了一种用于视觉变换器的训练噪声标记（TNT）剪枝方法。我们的方法将离散的标记丢弃条件放宽为连续的加性噪声，从而在训练中实现平滑优化，同时在部署环境中保留离散丢弃的计算优势。我们还提供了与率失真文献的理论联系，并在ImageNet数据集上使用ViT和DeiT架构进行了实证评估，展示了TNT相较于之前剪枝方法的优势。'}}}, {'id': 'https://huggingface.co/papers/2411.18665', 'title': 'SpotLight: Shadow-Guided Object Relighting via Diffusion', 'url': 'https://huggingface.co/papers/2411.18665', 'abstract': 'Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting.', 'score': 1, 'issue_id': 903, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '1b31caf705bc142d', 'authors': ['Frédéric Fortier-Chouinard', 'Zitian Zhang', 'Louis-Etienne Messier', 'Mathieu Garon', 'Anand Bhattad', 'Jean-François Lalonde'], 'affiliations': ['Depix Technologies', 'Toyota Technological Institute at Chicago', 'Universite Laval'], 'pdf_title_img': 'assets/pdf/title_img/2411.18665.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion'], 'emoji': '💡', 'ru': {'title': 'Контроль освещения в нейронном рендеринге через тени объектов', 'desc': 'Статья описывает метод SpotLight для контролируемого освещения объектов в нейронном рендеринге с использованием диффузионных моделей. Авторы показывают, что добавление только тени объекта в предобученный нейронный рендерер позволяет точно затенять объект в соответствии с желаемым положением источника света. Метод SpotLight не требует дополнительного обучения и может использоваться с существующими нейронными рендерерами. Согласно проведенному исследованию, SpotLight превосходит существующие диффузионные модели, специально разработанные для перестановки освещения.'}, 'en': {'title': 'SpotLight: Control Lighting with Shadows in Neural Rendering', 'desc': "This paper introduces SpotLight, a method that enhances neural rendering by allowing precise control over lighting for virtual objects in images. It achieves this by enabling users to specify the desired shadows of the object, which the diffusion model uses to accurately shade the object based on the light's position. SpotLight integrates seamlessly with existing pre-trained diffusion-based neural renderers, requiring no additional training. The results demonstrate significant improvements in object compositing, both in quantitative metrics and user perception, compared to traditional diffusion models designed for relighting."}, 'zh': {'title': 'SpotLight：精准控制虚拟物体光照的创新方法', 'desc': '本论文介绍了一种名为SpotLight的方法，利用扩散模型进行虚拟物体的重光照。与传统的物理渲染器不同，SpotLight通过仅指定物体的阴影来实现精确的光照控制。我们的方法能够在不需要额外训练的情况下，准确地根据所需的光源位置为物体上色，并与背景图像和谐融合。实验结果表明，SpotLight在物体合成效果上优于现有的扩散模型，得到了用户研究的支持。'}}}, {'id': 'https://huggingface.co/papers/2412.03555', 'title': 'PaliGemma 2: A Family of Versatile VLMs for Transfer', 'url': 'https://huggingface.co/papers/2412.03555', 'abstract': 'PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.', 'score': 55, 'issue_id': 964, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '12d0d9bcc8060099', 'authors': ['Andreas Steiner', 'André Susano Pinto', 'Michael Tschannen', 'Daniel Keysers', 'Xiao Wang', 'Yonatan Bitton', 'Alexey Gritsenko', 'Matthias Minderer', 'Anthony Sherbondy', 'Shangbang Long', 'Siyang Qin', 'Reeve Ingle', 'Emanuele Bugliarello', 'Sahar Kazemzadeh', 'Thomas Mesnard', 'Ibrahim Alabdulmohsin', 'Lucas Beyer', 'Xiaohua Zhai'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2412.03555.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#training', '#cv', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'PaliGemma 2: Новый уровень мультимодального ИИ', 'desc': 'PaliGemma 2 - это улучшенная версия открытой мультимодальной модели PaliGemma, основанная на семействе языковых моделей Gemma 2. Модель сочетает визуальный энкодер SigLIP-So400m с рядом моделей Gemma 2 разных размеров, от 2B до 27B параметров. Обучение проводилось на изображениях разного разрешения (224px, 448px и 896px) в несколько этапов для приобретения широких знаний. PaliGemma 2 демонстрирует отличные результаты на различных задачах, включая распознавание структуры таблиц, молекулярных структур, нотных записей, а также генерацию подробных описаний изображений и радиологических отчетов.'}, 'en': {'title': 'PaliGemma 2: Advancing Vision-Language Understanding', 'desc': 'PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning.'}, 'zh': {'title': 'PaliGemma 2：视觉与语言的完美结合', 'desc': 'PaliGemma 2 是基于 Gemma 2 语言模型家族的 PaliGemma 开放视觉语言模型的升级版。我们结合了 SigLIP-So400m 视觉编码器和不同规模的 Gemma 2 模型，进行多阶段训练，以提高模型的知识迁移能力。通过在三种分辨率下训练，我们能够研究影响迁移性能的因素，如学习率，并分析任务类型、模型大小和分辨率之间的关系。PaliGemma 2 扩展了迁移任务的数量和范围，涵盖了多种光学字符识别相关任务，并在这些任务上取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2412.02687', 'title': 'SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance', 'url': 'https://huggingface.co/papers/2412.02687', 'abstract': "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.", 'score': 55, 'issue_id': 961, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 декабря', 'en': 'December 3', 'zh': '12月3日'}, 'hash': 'd766bad745d5f322', 'authors': ['Viet Nguyen', 'Anh Nguyen', 'Trung Dao', 'Khoi Nguyen', 'Cuong Pham', 'Toan Tran', 'Anh Tran'], 'affiliations': ['Posts & Telecom. Inst. of Tech.', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.02687.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization', '#inference', '#training', '#benchmark', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Повышение стабильности и гибкости одношаговых диффузионных моделей', 'desc': 'Статья представляет SNOOPI - новый фреймворк для улучшения одношаговых диффузионных моделей генерации изображений. Авторы предлагают метод PG-SB для повышения стабильности обучения путем использования случайного масштаба бесклассификаторного руководства. Также вводится метод NASA для интеграции негативных промптов через кросс-внимание. Эксперименты показывают значительное улучшение базовых моделей по различным метрикам, достигая нового рекорда HPSv2 в 31.08 для одношаговых диффузионных моделей.'}, 'en': {'title': 'SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance', 'desc': 'This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08.'}, 'zh': {'title': 'SNOOPI：提升一步扩散模型的稳定性与生成质量', 'desc': '本论文提出了一种新框架SNOOPI，旨在解决现有一步扩散模型的局限性。我们通过Proper Guidance-SwiftBrush (PG-SB)方法增强了训练的稳定性，采用随机尺度的无分类器引导策略。我们还提出了一种无训练的方法Negative-Away Steer Attention (NASA)，通过交叉注意力将负提示集成到一步扩散模型中，以抑制生成图像中的不必要元素。实验结果表明，我们的方法在多个指标上显著提高了基线模型的性能，创造了一步扩散模型的新标杆。'}}}, {'id': 'https://huggingface.co/papers/2412.03552', 'title': 'Imagine360: Immersive 360 Video Generation from Perspective Anchor', 'url': 'https://huggingface.co/papers/2412.03552', 'abstract': '360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.', 'score': 23, 'issue_id': 958, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '90dc986cabb575af', 'authors': ['Jing Tan', 'Shuai Yang', 'Tong Wu', 'Jingwen He', 'Yuwei Guo', 'Ziwei Liu', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03552.jpg', 'data': {'categories': ['#cv', '#video', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Погружение в 360°: от обычного видео к панорамному опыту', 'desc': 'Статья представляет Imagine360 - первую систему для генерации 360-градусных видео из обычных перспективных видео. Система использует двухветвевую архитектуру с модулями шумоподавления для перспективного и панорамного видео, а также антиподальную маску для захвата дальних зависимостей движения. Предложены решения для адаптации к изменениям угла обзора во входных видео. Эксперименты показывают превосходное качество графики и согласованность движения по сравнению с существующими методами.'}, 'en': {'title': 'Transforming Perspective Videos into Immersive 360° Experiences', 'desc': 'The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos.'}, 'zh': {'title': 'Imagine360：个性化沉浸式360度视频创作的未来', 'desc': '360度视频提供了一种超沉浸式体验，让观众可以从全方位探索动态场景。为实现更友好和个性化的360度视频内容创作，我们提出了Imagine360，这是首个将标准视角视频转换为360度视频的框架。Imagine360通过有限的360度视频数据学习细致的球面视觉和运动模式，采用双分支设计来提供局部和全局约束。实验表明，Imagine360在图形质量和运动一致性方面优于现有的360度视频生成方法。'}}}, {'id': 'https://huggingface.co/papers/2412.03515', 'title': 'Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2412.03515', 'abstract': 'Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.', 'score': 21, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '6e733cf9c0a1b851', 'authors': ['Shengyuan Zhang', 'An Zhao', 'Ling Yang', 'Zejian Li', 'Chenye Meng', 'Haoran Xu', 'Tianrun Chen', 'AnYang Wei', 'Perry Pengyun GU', 'Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03515.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#3d', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'Быстрое и качественное завершение 3D LiDAR-сцен для автономных транспортных средств', 'desc': 'Статья представляет новый метод дистилляции для моделей завершения 3D LiDAR-сцен под названием ScoreLiDAR. Этот метод позволяет значительно ускорить процесс семплирования при сохранении высокого качества завершения сцены. Авторы также вводят новую Структурную Потерю, которая помогает дистиллированной модели лучше улавливать геометрическую структуру 3D LiDAR-сцены. Эксперименты показывают, что ScoreLiDAR ускоряет время завершения более чем в 5 раз и превосходит современные модели завершения 3D LiDAR-сцен.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with ScoreLiDAR', 'desc': "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."}, 'zh': {'title': '高效3D LiDAR场景补全的新方法', 'desc': '扩散模型因其强大的训练稳定性和高质量的场景补全而被应用于3D LiDAR场景补全。然而，慢速采样速度限制了基于扩散的场景补全模型的实际应用，因为自动驾驶车辆需要高效感知周围环境。本文提出了一种新颖的蒸馏方法，称为ScoreLiDAR，旨在实现高效且高质量的场景补全。通过引入结构损失，ScoreLiDAR能够更好地捕捉3D LiDAR场景的几何结构，同时显著加快了每帧的补全时间。'}}}, {'id': 'https://huggingface.co/papers/2412.03069', 'title': 'TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2412.03069', 'abstract': "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.", 'score': 18, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '820e62e1bd498d55', 'authors': ['Liao Qu', 'Huichao Zhang', 'Yiheng Liu', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Daniel K. Du', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.03069.jpg', 'data': {'categories': ['#multimodal', '#cv', '#architecture'], 'emoji': '🔀', 'ru': {'title': 'TokenFlow: единый токенизатор для понимания и генерации изображений', 'desc': 'TokenFlow - это новый универсальный токенизатор изображений, объединяющий задачи мультимодального понимания и генерации. Он использует инновационную архитектуру с двойным кодбуком, которая разделяет обучение семантических и пиксельных признаков, сохраняя их выравнивание через общий механизм отображения. TokenFlow превосходит существующие модели в задачах понимания, достигая улучшения на 7.2% по сравнению с LLaVA-1.5 13B. Модель также показывает высокие результаты в реконструкции изображений и авторегрессивной генерации, достигая показателей на уровне современных моделей.'}, 'en': {'title': 'TokenFlow: Bridging Understanding and Generation in Image Processing', 'desc': 'TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics.'}, 'zh': {'title': 'TokenFlow：多模态理解与生成的桥梁', 'desc': '本文介绍了一种新颖的图像标记器TokenFlow，它弥合了多模态理解与生成之间的差距。研究表明，理解和生成任务需要不同粒度的视觉信息，传统的单一重建目标向量量化编码器无法有效处理这一问题。TokenFlow通过创新的双代码本架构，解耦了语义和像素级特征学习，同时通过共享映射机制保持它们的对齐。实验结果表明，TokenFlow在多项任务中表现优越，首次证明离散视觉输入在理解性能上超越了LLaVA-1.5 13B。'}}}, {'id': 'https://huggingface.co/papers/2412.00493', 'title': 'Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2412.00493', 'abstract': "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.", 'score': 14, 'issue_id': 964, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': '10c214b548697656', 'authors': ['Duo Zheng', 'Shijia Huang', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.00493.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#3d', '#optimization', '#training'], 'emoji': '🎥', 'ru': {'title': 'Video-3D LLM: Прорыв в понимании трехмерных сцен', 'desc': 'Статья представляет новую модель Video-3D LLM для понимания трехмерных сцен. Модель рассматривает 3D-сцены как динамические видео и использует 3D-позиционное кодирование для лучшего соответствия видеопредставлений реальным пространственным контекстам. Авторы применили технику выборки с максимальным покрытием для оптимизации баланса между вычислительными затратами и эффективностью. Эксперименты показывают, что модель достигает наилучших результатов на нескольких эталонных тестах по пониманию 3D-сцен.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Video-3D LLM', 'desc': 'This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding.'}, 'zh': {'title': '提升3D场景理解的创新模型', 'desc': '这篇论文介绍了一种新型的多模态大语言模型（MLLM），称为Video-3D LLM，旨在提高3D场景理解能力。传统的MLLM主要基于2D数据训练，导致它们在处理3D环境时存在局限性。通过将3D场景视为动态视频，并引入3D位置编码，Video-3D LLM能够更准确地对齐视频表示与现实世界的空间上下文。此外，论文还提出了一种最大覆盖采样技术，以优化计算成本和性能效率之间的平衡。'}}}, {'id': 'https://huggingface.co/papers/2412.03205', 'title': 'U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs', 'url': 'https://huggingface.co/papers/2412.03205', 'abstract': "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.   To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release mu-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.   The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on mu-MATH.", 'score': 13, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '8df63a02d444d462', 'authors': ['Konstantin Chernyshev', 'Vitaliy Polshkov', 'Ekaterina Artemova', 'Alex Myasnikov', 'Vlad Stepanov', 'Alexei Miasnikov', 'Sergei Tilga'], 'affiliations': ['Gradarius', 'Stevens Institute of Technology', 'Toloka AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.03205.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#math', '#science', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'U-MATH: новый рубеж в оценке математических способностей ИИ', 'desc': 'Авторы представляют новый бенчмарк U-MATH для оценки математических способностей языковых моделей (LLM). Он содержит 1100 задач университетского уровня по шести основным предметам, включая 20% мультимодальных задач. Для оценки решений используется специально обученная языковая модель, для чего был создан датасет mu-MATH. Эксперименты показали, что современные LLM достигают точности лишь 63% на текстовых и 45% на визуальных задачах U-MATH, а лучшая модель-оценщик имеет F1-меру 80% на mu-MATH.'}, 'en': {'title': 'U-MATH: Elevating Math Evaluation for LLMs', 'desc': 'This paper presents U-MATH, a new benchmark designed to evaluate the mathematical skills of large language models (LLMs) using 1,100 open-ended university-level problems. The benchmark covers six core subjects and includes multimodal tasks, which incorporate visual elements, addressing the limitations of existing evaluations. To assess the performance of LLMs on these problems, the authors introduce mu-MATH, a dataset specifically for evaluating the correctness of solutions generated by LLMs. The results indicate that LLMs struggle with these tasks, achieving only 63% accuracy on text-based problems and 45% on visual ones, highlighting the need for further advancements in LLM capabilities.'}, 'zh': {'title': 'U-MATH：提升LLMs数学能力评估的新基准', 'desc': '目前对大型语言模型（LLMs）数学技能的评估存在局限性，现有基准测试相对较小，主要集中在基础和高中问题上，且缺乏主题多样性。此外，任务中视觉元素的包含仍然未得到充分探索。为了解决这些问题，我们引入了U-MATH，这是一个包含1100个未发表的开放式大学级问题的新基准，涵盖六个核心学科，其中20%的问题为多模态问题。我们的研究表明，LLMs在文本任务上的最高准确率仅为63%，而在视觉问题上的准确率更低，仅为45%。'}}}, {'id': 'https://huggingface.co/papers/2412.03517', 'title': 'NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images', 'url': 'https://huggingface.co/papers/2412.03517', 'abstract': 'Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.', 'score': 13, 'issue_id': 960, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '9d51bf0b60be344b', 'authors': ['Lingen Li', 'Zhaoyang Zhang', 'Yaowei Li', 'Jiale Xu', 'Xiaoyu Li', 'Wenbo Hu', 'Weihao Cheng', 'Jinwei Gu', 'Tianfan Xue', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03517.jpg', 'data': {'categories': ['#optimization', '#3d', '#diffusion', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Синтез новых ракурсов без явного выравнивания видов', 'desc': 'NVComposer - это новый подход к синтезу новых ракурсов, который устраняет необходимость во внешнем выравнивании видов. Он использует двухпоточную модель диффузии для одновременной генерации целевых ракурсов и позиций камер. Метод включает модуль выравнивания признаков с учетом геометрии, который извлекает геометрические закономерности из плотных стерео моделей во время обучения. Эксперименты показывают, что NVComposer достигает наилучших результатов в задачах генеративного многоракурсного синтеза новых видов.'}, 'en': {'title': 'NVComposer: Generating Novel Views Without External Alignment', 'desc': 'This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis.'}, 'zh': {'title': 'NVComposer：无须外部对齐的生成新视图合成', 'desc': '最近生成模型的进展显著提升了多视图数据的新的视图合成（NVS）能力。然而，现有方法依赖于外部的多视图对齐过程，如显式的姿态估计或预重建，这限制了它们的灵活性和可访问性，尤其是在视图之间重叠不足或遮挡时对齐不稳定的情况下。本文提出了NVComposer，这是一种新颖的方法，消除了对显式外部对齐的需求。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系，从而在生成多视图NVS任务中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.01106', 'title': 'One Shot, One Talk: Whole-body Talking Avatar from a Single Image', 'url': 'https://huggingface.co/papers/2412.01106', 'abstract': 'Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.', 'score': 12, 'issue_id': 957, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '13d96f9bb346e344', 'authors': ['Jun Xiang', 'Yudong Guo', 'Leipeng Hu', 'Boyang Guo', 'Yancheng Yuan', 'Juyong Zhang'], 'affiliations': ['The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01106.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#diffusion', '#3d'], 'emoji': '🤖', 'ru': {'title': 'Реалистичный говорящий аватар из одного фото', 'desc': 'Статья описывает новый метод создания реалистичных аватаров, способных говорить и двигаться, на основе всего одного изображения. Авторы используют диффузионные модели для генерации псевдо-видео кадров, которые служат обучающими данными. Они предлагают гибридное представление аватара, сочетающее 3D гауссовы сплаты и полигональную сетку. Метод позволяет точно контролировать жесты и мимику аватара, преодолевая ограничения существующих подходов.'}, 'en': {'title': 'From One Image to a Lifelike Talking Avatar!', 'desc': "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."}, 'zh': {'title': '从单张图像生成全身会说话的虚拟头像', 'desc': '本文提出了一种从单张图像构建全身会说话的虚拟头像的方法。我们解决了复杂动态建模和对新手势与表情的泛化这两个关键问题。通过使用姿态引导的图像到视频扩散模型，我们生成了不完美的视频帧作为伪标签，以实现无缝泛化。实验结果表明，我们的方法能够从单张图像创建出逼真、可精确动画和富有表现力的全身虚拟头像。'}}}, {'id': 'https://huggingface.co/papers/2412.02030', 'title': 'NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training', 'url': 'https://huggingface.co/papers/2412.02030', 'abstract': 'We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.', 'score': 11, 'issue_id': 966, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '4c749ff913210111', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2412.02030.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#architecture', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'NitroFusion: Революция в одношаговой генерации изображений', 'desc': 'NitroFusion - это новый подход к одношаговой диффузии, использующий динамическую состязательную структуру для генерации высококачественных изображений. Метод применяет большой пул специализированных дискриминаторов, каждый из которых фокусируется на определенном аспекте качества изображения на разных уровнях шума. NitroFusion включает механизмы обновления дискриминаторов для предотвращения переобучения и глобально-локальные головки дискриминаторов для многомасштабной оценки качества. Подход позволяет гибко выбирать от 1 до 4 шагов денойзинга, обеспечивая компромисс между качеством и скоростью.'}, 'en': {'title': 'NitroFusion: Fast and High-Quality Image Generation with Dynamic Discriminators', 'desc': 'NitroFusion is a new method for generating images quickly while maintaining high quality. It uses a dynamic adversarial framework with multiple specialized discriminator heads that focus on different quality aspects, similar to art critics. This approach allows for better feedback during the generation process, leading to improved fidelity in one-step generation. Additionally, NitroFusion offers flexible options for users to balance speed and quality by adjusting the number of denoising steps used.'}, 'zh': {'title': 'NitroFusion：高效与高质量生成的完美结合', 'desc': 'NitroFusion是一种全新的单步扩散生成方法，通过动态对抗框架实现高质量生成。与传统的单步方法相比，NitroFusion在生成质量上有显著提升，尽管单步方法在速度上具有优势。该方法利用多个专业的判别器组，针对不同的噪声水平提供多样化的反馈，从而提高生成的保真度。通过灵活的部署机制，用户可以根据需要在1到4个去噪步骤之间动态选择，实现质量与速度的平衡。'}}}, {'id': 'https://huggingface.co/papers/2411.19103', 'title': 'VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.19103', 'abstract': "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.", 'score': 11, 'issue_id': 964, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '4507a3a2ac0bc8b5', 'authors': ['Jeongho Ju', 'Daeyoung Kim', 'SunYoung Park', 'Youngjune Kim'], 'affiliations': ['NC Research, NCSOFT'], 'pdf_title_img': 'assets/pdf/title_img/2411.19103.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#training', '#low_resource'], 'emoji': '🌏', 'ru': {'title': 'VARCO-VISION: Прорыв в двуязычном компьютерном зрении', 'desc': 'В этой статье представлена новая мультиязычная модель компьютерного зрения VARCO-VISION для корейского и английского языков. Авторы применили пошаговую стратегию обучения, позволяющую модели усваивать как лингвистическую, так и визуальную информацию. VARCO-VISION демонстрирует высокую производительность в различных задачах, требующих двуязычного понимания и генерации текста и изображений. Модель также способна выполнять задачи локализации объектов, референции и оптического распознавания символов, что расширяет ее потенциальное применение в реальных сценариях.'}, 'en': {'title': 'VARCO-VISION: Bridging Korean and English through Vision-Language Learning', 'desc': 'This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area.'}, 'zh': {'title': 'VARCO-VISION：双语视觉语言模型的新里程碑', 'desc': '本文介绍了一种开源的韩英视觉语言模型VARCO-VISION。我们采用逐步训练策略，使模型能够同时学习语言和视觉信息，同时保留基础模型的知识。与同类模型相比，VARCO-VISION在双语图像文本理解和生成能力方面表现出色。该模型还具备定位、引用和光学字符识别（OCR）功能，扩展了其在现实场景中的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.03558', 'title': 'MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation', 'url': 'https://huggingface.co/papers/2412.03558', 'abstract': 'This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.', 'score': 10, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '5e1a4c1e1017e7af', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Xingqiao An', 'Yunhan Yang', 'Yangguang Li', 'Zi-Xin Zou', 'Ding Liang', 'Xihui Liu', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03558.jpg', 'data': {'categories': ['#cv', '#synthetic', '#diffusion', '#training', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'MIDI: Революционный подход к генерации 3D-сцен из одного изображения', 'desc': 'Статья представляет MIDI - новый подход к генерации трехмерных сцен из одного изображения. MIDI использует предобученные модели генерации 3D-объектов и расширяет их до многоэкземплярных диффузионных моделей, позволяя одновременно генерировать несколько 3D-объектов с точными пространственными отношениями. Ключевой особенностью является новый механизм многоэкземплярного внимания, который эффективно учитывает взаимодействия между объектами и пространственную согласованность непосредственно в процессе генерации. MIDI демонстрирует передовые результаты в генерации сцен из изображений, что подтверждается оценками на синтетических данных, реальных сценах и стилизованных изображениях.'}, 'en': {'title': 'MIDI: Revolutionizing 3D Scene Generation from Single Images', 'desc': 'This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes.'}, 'zh': {'title': 'MIDI：从单图像生成3D场景的新方法', 'desc': '本文介绍了一种名为MIDI的新方法，用于从单张图像生成组合3D场景。与现有依赖重建或检索技术的方法不同，MIDI扩展了预训练的图像到3D对象生成模型，采用多实例扩散模型，实现了多个3D实例的同时生成。MIDI的核心是一个新颖的多实例注意机制，能够有效捕捉对象间的交互和空间一致性，简化了生成过程。该方法在图像到场景生成方面表现出色，经过合成数据、真实场景数据和文本到图像扩散模型生成的风格化场景图像的评估验证。'}}}, {'id': 'https://huggingface.co/papers/2412.03439', 'title': 'CleanDIFT: Diffusion Features without Noise', 'url': 'https://huggingface.co/papers/2412.03439', 'abstract': 'Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.', 'score': 9, 'issue_id': 963, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'cd474064bf17503a', 'authors': ['Nick Stracke', 'Stefan Andreas Baumann', 'Kolja Bauer', 'Frank Fundel', 'Björn Ommer'], 'affiliations': ['CompVis @ LMU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2412.03439.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Улучшение семантических признаков диффузионных моделей без шума', 'desc': 'Исследователи обнаружили, что внутренние признаки, извлекаемые из предобученных диффузионных моделей, являются мощными семантическими дескрипторами для различных задач. Однако добавление шума к изображениям перед их обработкой моделью критически влияет на полезность этих признаков. Авторы предлагают метод легковесной неконтролируемой донастройки, позволяющий получать качественные семантические признаки без шума. Эти признаки значительно превосходят предыдущие подходы по эффективности в различных задачах при меньших вычислительных затратах.'}, 'en': {'title': 'Unlocking Noise-Free Semantic Features from Diffusion Models', 'desc': 'This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient.'}, 'zh': {'title': '无噪声的高质量语义特征提取', 'desc': '最近，大规模预训练扩散模型的内部特征被确立为强大的语义描述符，适用于多种下游任务。通常，这些特征需要在图像中添加噪声后才能提取，因为模型在处理几乎没有噪声的图像时，提供的特征效果不佳。我们发现噪声对特征的有效性有重要影响，且通过不同随机噪声的集成无法解决这个问题。为此，我们提出了一种轻量级的无监督微调方法，使扩散模型能够提供高质量、无噪声的语义特征，显著超越了之前的扩散特征。'}}}, {'id': 'https://huggingface.co/papers/2412.03085', 'title': 'Mimir: Improving Video Diffusion Models for Precise Text Understanding', 'url': 'https://huggingface.co/papers/2412.03085', 'abstract': 'Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'a065164e5fdadf2c', 'authors': ['Shuai Tan', 'Biao Gong', 'Yutong Feng', 'Kecheng Zheng', 'Dandan Zheng', 'Shuwei Shi', 'Yujun Shen', 'Jingdong Chen', 'Ming Yang'], 'affiliations': ['Ant Group', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03085.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training'], 'emoji': '🎬', 'ru': {'title': 'Mimir: Улучшение генерации видео с помощью больших языковых моделей', 'desc': "Эта статья представляет Mimir - новый подход к генерации видео на основе текста. Авторы предлагают использовать большие языковые модели (LLM) для улучшения понимания текста и воображения в процессе генерации. Ключевой элемент Mimir - специальный 'token fuser', который объединяет выходы текстовых энкодеров и LLM. Результаты показывают, что Mimir эффективен в создании качественных видео с хорошим пониманием текста, особенно для коротких описаний и динамичных сцен."}, 'en': {'title': 'Mimir: Bridging Text Understanding and Video Generation', 'desc': 'This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements.'}, 'zh': {'title': 'Mimir：提升文本到视频生成的智能', 'desc': '本文提出了一种名为Mimir的端到端训练框架，用于文本到视频生成（T2V）。该框架通过精心设计的令牌融合器，解决了文本编码器与大型语言模型（LLMs）之间的特征分布差距。Mimir能够充分利用学习到的视频先验，同时增强LLMs在文本理解方面的能力。实验结果表明，Mimir在生成高质量视频时，尤其在处理短文本和动态变化时，表现出色。'}}}, {'id': 'https://huggingface.co/papers/2412.03565', 'title': 'Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning', 'url': 'https://huggingface.co/papers/2412.03565', 'abstract': 'Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.', 'score': 5, 'issue_id': 967, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '72af31b504d0aac1', 'authors': ['Wujian Peng', 'Lingchen Meng', 'Yitong Chen', 'Yiweng Xie', 'Yang Liu', 'Tao Gui', 'Hang Xu', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.03565.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#alignment', '#dataset', '#training'], 'emoji': '🔍', 'ru': {'title': 'Улучшение понимания экземпляров в LMM с помощью явных визуальных подсказок', 'desc': 'Статья представляет Inst-IT - решение для улучшения понимания экземпляров в больших мультимодальных моделях (LMM) с помощью явных визуальных подсказок. Авторы разработали автоматизированный конвейер аннотаций с использованием GPT-4 для извлечения информации на уровне экземпляров из изображений и видео. Inst-IT включает в себя эталонный тест для диагностики мультимодального понимания на уровне экземпляров, большой набор данных для обучения с инструкциями и парадигму непрерывного обучения. Экспериментальные результаты показывают значительное улучшение как в понимании экземпляров, так и в общем понимании изображений и видео.'}, 'en': {'title': 'Enhancing Instance Understanding in Multimodal Models with Inst-IT', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding specific instances within images and videos, despite their overall comprehension abilities. It introduces a new automated annotation pipeline that uses GPT-4o to extract detailed instance-level information through explicit visual prompts. The authors propose a method called Inst-IT, which enhances LMMs' instance understanding by utilizing instruction tuning with a focus on spatial-temporal elements. Experimental results indicate that Inst-IT significantly improves both instance-level understanding and general image and video comprehension across various benchmarks."}, 'zh': {'title': '提升实例理解能力的创新方法', 'desc': '大型多模态模型（LMMs）在指令调优方面取得了显著突破，但在实例级理解上仍然存在挑战。实例级理解关注特定元素，这对于深入理解图像和视频至关重要。我们提出了一种自动注释管道，利用GPT-4o通过明确的视觉提示提取实例级信息。基于此，我们开发了Inst-IT，通过明确的视觉提示指令调优来增强LMMs的实例理解能力，实验结果表明，Inst-IT显著提升了模型在多种图像和视频理解基准上的表现。'}}}, {'id': 'https://huggingface.co/papers/2412.02980', 'title': 'Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models', 'url': 'https://huggingface.co/papers/2412.02980', 'abstract': 'Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.', 'score': 4, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '8055d4be8211be80', 'authors': ['Alex Havrilla', 'Andrew Dai', "Laura O'Mahony", 'Koen Oostermeijer', 'Vera Zisler', 'Alon Albalak', 'Fabrizio Milo', 'Sharath Chandra Raparthy', 'Kanishk Gandhi', 'Baber Abbasi', 'Duy Phung', 'Maia Iyer', 'Dakota Mahan', 'Chase Blagden', 'Srishti Gureja', 'Mohammed Hamdy', 'Wen-Ding Li', 'Giovanni Paolini', 'Pawan Sasanka Ammanamanchi', 'Elliot Meyerson'], 'affiliations': ['Aleph Alpha @ IPAI', 'Cognizant AI Labs', 'Cohere for AI Community', 'Cornell University', 'Eleuther AI', 'Georgia Tech', 'IBM', 'Independent', 'Reka AI', 'Sakana AI', 'Stanford University', 'SynthLabs', 'University of Bologna', 'University of Limerick'], 'pdf_title_img': 'assets/pdf/title_img/2412.02980.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#dataset', '#rl', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Баланс качества и разнообразия в синтетических данных для ИИ', 'desc': 'Статья рассматривает генерацию синтетических данных с помощью больших языковых моделей для расширения естественных данных в различных задачах. Авторы предлагают оценивать алгоритмы по качеству, разнообразию и сложности генерируемых данных. Исследование показывает важность баланса этих характеристик для эффективного обучения с подкреплением и алгоритмов самоулучшения. Авторы отмечают, что многие модели оптимизируются только по качеству выходных данных, ограничивая их разнообразие и потенциал самоулучшения.'}, 'en': {'title': 'Balancing Quality, Diversity, and Complexity in Synthetic Data Generation', 'desc': 'This paper discusses the generation of synthetic data using Large Language Models (LLMs) and its importance in enhancing natural data for various tasks. It evaluates synthetic data generation algorithms based on three key characteristics: quality, diversity, and complexity, which are crucial for the performance of downstream models. The authors highlight the trade-offs between these characteristics, noting that while quality is vital for in-distribution generalization, diversity is necessary for out-of-distribution generalization. The paper emphasizes the need for a balanced approach to these trade-offs to improve the effectiveness of reinforcement learning and self-improvement algorithms.'}, 'zh': {'title': '合成数据生成：平衡质量与多样性', 'desc': '本论文探讨了使用大型语言模型生成合成数据的潜力，强调了合成数据在自然数据增强中的重要性。我们提出通过数据质量、多样性和复杂性来评估合成数据生成算法，这三者对下游模型的能力有显著影响。研究发现，数据质量对模型的分布内泛化至关重要，而多样性则对分布外泛化至关重要，复杂性对两者都有益。我们还强调了训练数据中的质量-多样性权衡及其对模型性能的影响，认为在未来的自我改进算法中平衡这些权衡是至关重要的。'}}}, {'id': 'https://huggingface.co/papers/2412.03187', 'title': 'Weighted-Reward Preference Optimization for Implicit Model Fusion', 'url': 'https://huggingface.co/papers/2412.03187', 'abstract': 'While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.', 'score': 4, 'issue_id': 961, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': '6da11fbf4e1ea7d9', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Tianyuan Shi', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.03187.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#open_source', '#architecture', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'WRPO: Эффективное слияние языковых моделей без прямого объединения параметров', 'desc': 'В этой статье предлагается новый метод слияния разнородных языковых моделей с открытым исходным кодом - Weighted-Reward Preference Optimization (WRPO). WRPO использует оптимизацию предпочтений между исходными и целевой моделями для эффективного переноса их возможностей, устраняя необходимость в выравнивании словарей и слиянии матриц распределения. Метод вводит стратегию прогрессивной адаптации для решения проблемы различий в распределениях между моделями. Эксперименты показывают, что WRPO превосходит существующие методы слияния знаний и базовые подходы к дообучению на нескольких бенчмарках.'}, 'en': {'title': 'Effortless Fusion of LLMs with WRPO!', 'desc': 'This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods.'}, 'zh': {'title': '加权奖励偏好优化：高效融合多种大语言模型', 'desc': '本论文提出了一种隐式融合方法，称为加权奖励偏好优化（WRPO），旨在有效整合不同架构和规模的开源大语言模型（LLMs）。WRPO通过优化源模型与目标模型之间的偏好，避免了词汇对齐和矩阵融合的复杂性。该方法引入了渐进适应策略，逐步调整对目标模型和源模型的依赖，从而解决了分布偏差问题。实验结果表明，WRPO在多个基准测试中表现优于现有的知识融合方法和微调基线。'}}}, {'id': 'https://huggingface.co/papers/2412.00177', 'title': 'LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting', 'url': 'https://huggingface.co/papers/2412.00177', 'abstract': "We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.", 'score': 2, 'issue_id': 969, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '210b042d1a430116', 'authors': ['Xiaoyan Xing', 'Konrad Groh', 'Sezer Karaoglu', 'Theo Gevers', 'Anand Bhattad'], 'affiliations': ['BCAI-Bosch', 'Toyota Technological Institute at Chicago', 'UvA-Bosch Delta Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.00177.jpg', 'data': {'categories': ['#data', '#optimization', '#cv', '#diffusion', '#architecture'], 'emoji': '💡', 'ru': {'title': 'LumiNet: Реалистичный перенос освещения с помощью латентных представлений', 'desc': 'LumiNet - это новая архитектура для переноса освещения между изображениями, использующая генеративные модели и латентные представления. Она включает стратегию подготовки данных на основе StyleGAN и модифицированную версию ControlNet, обрабатывающую латентные свойства исходного и целевого изображений. LumiNet дополнительно улучшает перенос освещения с помощью обученного адаптера, внедряющего латентные внешние свойства целевого изображения. Эксперименты показывают, что метод успешно переносит сложные световые эффекты между сценами с различной геометрией и материалами.'}, 'en': {'title': 'LumiNet: Mastering Lighting Transfer with Generative Models', 'desc': 'LumiNet is a new machine learning architecture designed for transferring lighting effects from one image to another. It uses generative models to create a relit version of a source image by applying the lighting characteristics of a target image. The model incorporates a unique data curation strategy and a modified diffusion-based ControlNet that processes both intrinsic and extrinsic properties of the images. By utilizing cross-attention and fine-tuning, LumiNet effectively captures complex lighting phenomena, outperforming traditional methods in challenging indoor environments.'}, 'zh': {'title': 'LumiNet：高效光照转移的新方法', 'desc': 'LumiNet是一种新颖的架构，利用生成模型和潜在内在表示来实现有效的光照转移。该方法通过输入源图像和目标光照图像，合成出一个捕捉目标光照的重新照明版本。LumiNet的两个关键贡献包括基于StyleGAN的重新照明模型的数据整理策略，以及处理源图像的潜在内在属性和目标图像的潜在外在属性的改进扩散控制网络。通过交叉注意力和微调，LumiNet进一步通过学习适配器（MLP）注入目标的潜在外在属性，从而改善光照转移效果。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (15)', '#agents (5)', '#agi (1)', '#alignment (5)', '#architecture (31)', '#audio (4)', '#benchmark (38)', '#cv (31)', '#data (15)', '#dataset (40)', '#diffusion (31)', '#ethics (4)', '#games (11)', '#graphs', '#hallucinations (5)', '#healthcare (1)', '#inference (14)', '#interpretability (7)', '#leakage (1)', '#long_context (5)', '#low_resource (7)', '#machine_translation (4)', '#math (5)', '#multilingual (5)', '#multimodal (39)', '#open_source (19)', '#optimization (49)', '#plp', '#rag (2)', '#reasoning (12)', '#rl (2)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security (3)', '#small_models (3)', '#story_generation (1)', '#survey (3)', '#synthetic (14)', '#training (52)', '#transfer_learning (6)', '#video (25)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-06 19:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-06 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-06 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    